An intellectual is a person who engages in critical thinking, research, and reflection about society and proposes solutions for its normative problems. Some gain authority as public intellectuals.[1][2] Coming from the world of culture, either as a creator or as a mediator, the intellectual participates in politics either to defend a concrete proposition or to denounce an injustice, usually by rejecting, producing or extending an ideology, and by defending a system of values.[3]						Socially, intellectuals constitute the intelligentsia, a status class organised either by ideology (conservative, fascist, socialist, liberal, reactionary, revolutionary, democratic, communist intellectuals, et al.), or by nationality (American intellectuals, French intellectuals, Ibero–American intellectuals, et al.). The contemporary intellectual class originated from the intelligentsiya of Tsarist Russia (c. 1860s–1870s), the social stratum of those possessing intellectual formation (schooling, education, Enlightenment), and who were Russian society's counterpart to the German Bildungsbürgertum and to the French bourgeoisie éclairée, the enlightened middle classes of those realms.[4] [a]		In the late 19th century, amidst the Dreyfus affair (1894–1906), an identity crisis of anti-semitic nationalism for the French Third Republic (1870–1940), the reactionary anti–Dreyfusards (Maurice Barrès, Ferdinand Brunetière, et al.) used the terms intellectual and the intellectuals to deride the liberal Dreyfusards (Émile Zola, Octave Mirbeau, Anatole France, et al.) as political dilettantes from the realms of French culture, art, and science, who had become involved in politics, by publicly advocating for the exoneration and liberation of Alfred Dreyfus, a Jewish French artillery captain falsely accused of betraying France to Germany.[5]		In the 20th century, the term Intellectual acquired positive connotations of social prestige, derived from possessing intellect and intelligence, especially when the intellectual's activities exerted positive consequences in the public sphere and so increased the intellectual understanding of the public, by means of moral responsibility, altruism, and solidarity, without resorting to the manipulations of populism, paternalism, and incivility (condescension).[4][b] Hence, for the educated person of a society, participating in the public sphere—the political affairs of the city-state—is a civic responsibility dating from the Græco–Latin Classical era:		I am a man; I reckon nothing human to be foreign to me. (Homo sum: humani nihil a me alienum puto.)		The determining factor for a Thinker (historian, philosopher, scientist, writer, artist, et al.) to be considered a public intellectual is the degree to which he or she is implicated and engaged with the vital reality of the contemporary world; that is to say, participation in the public affairs of society. Consequently, being designated as a public intellectual is determined by the degree of influence of the designator's motivations, opinions, and options of action (social, political, ideological), and by affinity with the given thinker; therefore: [c]		The Intellectual is someone who meddles in what does not concern him. (L'intellectuel est quelqu'un qui se mêle de ce qui ne le regarde pas.)		Analogously, the application and the conceptual value of the terms Intellectual and the Intellectuals are socially negative when the practice of intellectuality is exclusively in service to The Establishment who wield power in a society, as such:		The Intellectuals are specialists in defamation, they are basically political commissars, they are the ideological administrators, the most threatened by dissidence.		Noam Chomsky’s negative view of the Establishment Intellectual suggests the existence of another kind of intellectual one might call "the public intellectual," which is:		. . . someone able to speak the truth, a . . . courageous and angry individual for whom no worldly power is too big and imposing to be criticised and pointedly taken to task. The real or true intellectual is therefore always an outsider, living in self-imposed exile, and on the margins of society. He or she speaks to, as well as for, a public, necessarily in public, and is properly on the side of the dispossessed, the un-represented and the forgotten.		The intellectual is a type of intelligent person, who is associated with reason and critical thinking. Many everyday roles require the application of intelligence to skills that may have a psychomotor component, for example, in the fields of medicine or the arts, but these do not necessarily involve the practitioner in the "world of ideas". The distinctive quality of the intellectual person is that the mental skills, which one demonstrates, are not simply intelligent, but even more, they focus on thinking about the abstract, philosophical and esoteric aspects of human inquiry and the value of their thinking.[citation needed]		The intellectual and the scholarly classes are related; the intellectual usually is not a teacher involved in the production of scholarship, but has an academic background, and works in a profession, practices an art, or a science. The intellectual person is one who applies critical thinking and reason in either a professional or a personal capacity, and so has authority in the public sphere of their society; the term intellectual identifies three types of person, one who:		Man of Letters		The English term "Man of Letters" derives from the French term belletrist, but is not synonymous with "An academic".[13][14] The term Man of Letters distinguished the literate man ("able to read and write") from the illiterate man ("unable to read and write"), in a time when literacy was a rare form of cultural capital. In the 17th and 18th centuries, the term Belletrist identified the literati, the French "citizens of the Republic of Letters", which evolved into the salon, a social institution, usually run by a hostess, meant for the edification, education, and cultural refinement of the participants.		In English, the term intellectual identifies a "literate thinker"; its earlier usage, as in the book title The Evolution of an Intellectual (1920), by John Middleton Murry, denotes literary activity, rather than the activities of the public intellectual.[15]		In the late 19th century, when literacy was relatively common in European countries such as the United Kingdom, the "Man of Letters" (littérateur)[16] denotation broadened to mean "specialized", a man who earned his living writing intellectually (not creatively) about literature: the essayist, the journalist, the critic, et al. In the 20th century, such an approach was gradually superseded by the academic method, and the term "Man of Letters" became disused, replaced by the generic term "intellectual", describing the intellectual person. In late 19th century, the term intellectual became common usage to denote the defenders of the falsely accused artillery officer Alfred Dreyfus.[17]		In early 19th century Britain, Samuel Taylor Coleridge coined the term clerisy, the intellectual class responsible for upholding and maintaining the national culture, the secular equivalent of the Anglican clergy. Likewise, in Tsarist Russia, there arose the intelligentsia (1860s–70s), who were the status class of white-collar workers. The theologian Alister McGrath said that "the emergence of a socially alienated, theologically literate, antiestablishment lay intelligentsia is one of the more significant phenomena of the social history of Germany in the 1830s", and that "three or four theological graduates in ten might hope to find employment" in a church post.[18] As such, politically radical thinkers already had participated in the French Revolution (1789–1799); Robert Darnton said that they were not societal outsiders, but "respectable, domesticated, and assimilated".[19]		Thenceforth, in Europe, an intellectual class was socially important, especially to self-styled intellectuals, whose participation in society’s arts, politics, journalism, and education—of either nationalist, internationalist, or ethnic sentiment—constitute "vocation of the intellectual". Moreover, some intellectuals were anti-academic, despite universities (the Academy) being synonymous with intellectualism.		In France, the Dreyfus affair marked the full emergence of the "intellectual in public life", especially Émile Zola, Octave Mirbeau, and Anatole France directly addressing the matter of French antisemitism to the public; thenceforward, "intellectual" became common, yet occasionally derogatory, usage; its French noun usage is attributed to Georges Clemenceau in 1898.		Habermas' Structural Transformation of Public Sphere (1963) made significant contribution to the notion of public intellectual by historically and conceptually delineating the idea of private and public.		In Imperial China, in the period from 206 BC until AD 1912, the intellectuals were the Scholar-officials ("Scholar-gentlemen"), who were civil servants appointed by the Emperor of China to perform the tasks of daily governance. Such civil servants earned academic degrees by means of imperial examination, and also were skilled calligraphers, and knew Confucian philosophy. Historian Wing-Tsit Chan concludes that:		Generally speaking, the record of these scholar-gentlemen has been a worthy one. It was good enough to be praised and imitated in 18th century Europe. Nevertheless, it has given China a tremendous handicap in their transition from government by men to government by law, and personal considerations in Chinese government have been a curse.[20]		In Joseon Korea (1392–1910), the intellectuals were the literati, who knew how to read and write, and had been designated, as the chungin (the "middle people"), in accordance with the Confucian system. Socially, they constituted the petite bourgeoisie, composed of scholar-bureaucrats (scholars, professionals, and technicians) who administered the dynastic rule of the Joseon dynasty.[21]		Addressing their role as a social class, Jean-Paul Sartre said that intellectuals are the moral conscience of their age; that their moral and ethical responsibilities are to observe the socio-political moment, and to freely speak to their society, in accordance with their consciences.[22] Like Sartre and Noam Chomsky, public intellectuals usually are polymaths, knowledgeable of the international order of the world, the political and economic organization of contemporary society, the institutions and laws that regulate the lives of the layman citizen, the educational systems, and the private networks of mass communication media that control the broadcasting of information to the public.[23]		Whereas, intellectuals (political scientists and sociologists), liberals, and democratic socialists usually hold, advocate, and support the principles of democracy (liberty, equality, fraternity, human rights, social justice, social welfare, environmental conservation), and the improvement of socio-political relations in domestic and international politics, the conservative public-intellectuals usually defend the social, economic, and political status quo as the realisation of the "perfect ideals" of Platonism, and present a static dominant ideology, in which utopias are unattainable and politically destabilizing of society.		In Marxist philosophy, the social-class function of the intellectuals (the intelligentsia) is to be the source of progressive ideas for the transformation of society; to provide advice and counsel to the political leaders; to interpret the country's politics to the mass of the population (urban workers and peasants); and, as required, to provide leaders.		The Italian Communist theoretician Antonio Gramsci (1891–1937) developed Karl Marx’s conception of the intelligentsia to include political leadership in the public sphere. That, because "all knowledge is existentially-based", the intellectuals, who create and preserve knowledge, are "spokesmen for different social groups, and articulate particular social interests". That intellectuals occur in each social class and throughout the right wing, the centre, and the left wing of the political spectrum. That, as a social class, the "intellectuals view themselves as autonomous from the ruling class" of their society. That, in the course of class struggle meant to achieve political power, every social class requires a native intelligentsia who shape the ideology (world view) particular to the social class from which they originated. Therefore, the leadership of intellectuals is required for effecting and realizing social change, because:		A human mass does not "distinguish" itself, does not become independent, in its own right, without, in the widest sense, organising itself; and there is no organisation without intellectuals, that is, without organisers and leaders, in other words, without ... a group of people "specialised" in [the] conceptual and philosophical elaboration of ideas.[24]		In the pamphlet What Is to Be Done? (1902), Lenin (1870–1924) said that vanguard-party revolution required the participation of the intellectuals to explain the complexities of socialist ideology to the uneducated proletariat and the urban industrial workers, in order to integrate them to the revolution; because "the history of all countries shows that the working class, exclusively by its own efforts, is able to develop only trade-union consciousness", and will settle for the limited, socio-economic gains so achieved. In Russia, as in Continental Europe, Socialist theory was the product of the "educated representatives of the propertied classes", of "revolutionary socialist intellectuals", such as were Karl Marx and Friedrich Engels.[25]		In the formal codification of Leninism, the Hungarian Marxist philosopher, György Lukács (1885–1971) identified the intelligentsia as the privileged social class who provide revolutionary leadership. By means of intelligible and accessible interpretation, the intellectuals explain to the workers and peasants the "Who?", the "How?", and the "Why?" of the social, economic, and political status quo—the ideological totality of society—and its practical, revolutionary application to the transformation of their society.		The term public intellectual describes the intellectual participating in the public-affairs discourse of society, in addition to an academic career.[26] Regardless of the academic field or the professional expertise, the public intellectual addresses and responds to the normative problems of society, and, as such, is expected to be an impartial critic who can "rise above the partial preoccupation of one’s own profession — and engage with the global issues of truth, judgment, and taste of the time."[27][28] In Representations of the Intellectual (1994), Edward Saïd said that the "… true intellectual is, therefore, always an outsider, living in self-imposed exile, and on the margins of society".[29]		An intellectual usually is associated with an ideology or with a philosophy; e.g., the Third Way centrism of Anthony Giddens in the Labour Government of Tony Blair.[30] The Czech intellectual Václav Havel said that politics and intellectuals can be linked, but that moral responsibility for the intellectual's ideas, even when advocated by a politician, remains with the intellectual. Therefore, it is best to avoid utopian intellectuals who offer ‘universal insights’ to resolve the problems of political economy with public policies that might harm and that have harmed civil society; that intellectuals be mindful of the social and cultural ties created with their words, insights, and ideas; and should be heard as social critics of politics and power.[31][32]		The American academic Peter H. Smith describes the intellectuals of Latin America as people from an identifiable social class, who have been conditioned by that common experience, and thus are inclined to share a set of common assumptions (values and ethics); that ninety-four per cent of intellectuals come either from the middle class or from the upper class, and that only six per cent come from the working class. In The Intellectual (2005), philosopher Steven Fuller said that, because cultural capital confers power and social status, as a status group, they must be autonomous in order to be credible as intellectuals:		It is relatively easy to demonstrate autonomy, if you come from a wealthy or [an] aristocratic background. You simply need to disown your status and champion the poor and [the] downtrodden . . . autonomy is much harder to demonstrate if you come from a poor or proletarian background . . . [thus] calls to join the wealthy in common cause appear to betray one’s class origins.		The political importance and effective consequence of Émile Zola in the Dreyfus affair (1894–1906) derived from being a leading French thinker; thus, J'accuse (I Accuse), his open letter to the French government and the nation proved critical to achieving the exoneration of Captain Alfred Dreyfus of the false charges of treason, which were facilitated by institutional anti-Semitism, among other ideological defects of the French Establishment.		In journalism, the term intellectual usually connotes "a university academic" of the humanities—especially a philosopher—who addresses important social and political matters of the day. Hence, such an academic functions as a public intellectual who explains the theoretic bases of said problems and communicates possible answers to the policy makers and executive leaders of society. The sociologist Frank Furedi said that "Intellectuals are not defined according to the jobs they do, but [by] the manner in which they act, the way they see themselves, and the [social and political] values that they uphold.[34] Public intellectuals usually arise from the educated élite of a society; although the North American usage of the term "intellectual" includes the university academics.[35] The difference between "intellectual" and "academic" is participation in the realm of public affairs.[36]		In the matters of public policy, the public intellectual connects scholarly research to the practical matters of solving societal problems. The British sociologist Michael Burawoy, an exponent of public sociology, said that professional sociology has failed, by giving insufficient attention to resolving social problems, and that a dialogue between the academic and the layman would bridge the gap.[37] An example is how Chilean intellectuals worked to reestablish democracy within the right-wing, neoliberal governments of the Military dictatorship of Chile (1973–90), the Pinochet régime allowed professional opportunities for some liberal and left-wing social scientists to work as politicians and as consultants in effort to realize the theoretical economics of the Chicago Boys, but their access to power was contingent upon political pragmatism, abandoning the political neutrality of the academic intellectual.[38]		In The Sociological Imagination (1959), C. Wright Mills said that academics had become ill-equipped for participating in public discourse, and that journalists usually are "more politically alert and knowledgeable than sociologists, economists, and especially . . . political scientists".[39] That, because the universities of the U.S. are bureaucratic, private businesses, they do not teach critical reasoning to the student, who then does not "how to gauge what is going on in the general struggle for power in modern society".[39] Likewise, Richard Rorty criticized the participation of intellectuals in public discourse as an example of the "civic irresponsibility of intellect, especially academic intellect".[40]		The American legal scholar Richard Posner said that the participation of academic public intellectuals in the public life of society is characterized by logically untidy and politically biased statements, of the kind that would be unacceptable academic work. That there are few ideologically and politically independent public intellectuals, and disapproves that public intellectuals limit themselves to practical matters of public policy, and not with values or public philosophy, or public ethics, or public theology, not with matters of moral and spiritual outrage.		In "An Interview with Milton Friedman" (1974), the neoliberal American economist Milton Friedman said that businessmen and the intellectuals are enemies of capitalism; the intellectuals, because most believed in socialism, while the businessman expected economic privileges;		The two, chief enemies of the free society or free enterprise are intellectuals, on the one hand, and businessmen, on the other, for opposite reasons. Every intellectual believes in freedom for himself, but he’s opposed to freedom for others. . . . He thinks . . . [that] there ought to be a central planning board that will establish social priorities. . . . The businessmen are just the opposite — every businessman is in favor of freedom for everybody else, but, when it comes to himself that’s a different question. He’s always “the special case”. He ought to get special privileges from the government, a tariff, this, that, and the other thing. . . .[41]		In "The Intellectuals and Socialism" (1949), the British economist Friedrich August von Hayek (1899–1992), a philosopher of Neoliberalism, said that "journalists, teachers, ministers, lecturers, publicists, radio commentators, writers of fiction, cartoonists, and artists", are the intellectual social-class whose function is to communicate the complex and specialized knowledge of the scientist to the general public. That, in the twentieth century, the intellectuals were attracted to socialism and to social democracy, because the socialists offered "broad visions; the spacious comprehension of the social order, as a whole, which a planned system promises" and that such broad-vision philosophies "succeeded in inspiring the imagination of the intellectuals" to change and improve their societies.[42]		In "The Heartless Lovers of Humankind" (1987), the journalist and popular historian Paul Johnson said:		It is not the formulation of ideas, however misguided, but the desire to impose them on others that is the deadly sin of the intellectuals. That is why they so incline, by temperament, to the Left. For capitalism merely occurs; if no-one does anything to stop it. It is socialism that has to be constructed, and, as a rule, forcibly imposed, thus providing a far bigger role for intellectuals in its genesis. The progressive intellectual habitually entertains Walter Mitty visions of exercising power.[43]		The public- and private-knowledge dichotomy originated in Ancient Greece, from Socrates's rejection of the Sophist concept that the pursuit of knowledge (Truth) is a "public market of ideas", open to all men of the city, not only to philosophers. In contradiction to the Sophist's public market of knowledge, Socrates proposed a knowledge monopoly for and by the philosophers; thus, "those who sought a more penetrating and rigorous intellectual life rejected, and withdrew from, the general culture of the city, in order to embrace a new model of professionalism"; the private market of ideas.[44]		In the 19th century, addressing the societal place, roles, and functions of intellectuals in American society, the Congregational theologian Edwards A. Park said, "We do wrong to our own minds, when we carry out scientific difficulties down to the arena of popular dissension".[44] That for the stability of society (social, economic, political) it is necessary "to separate the serious, technical role of professionals from their responsibility [for] supplying usable philosophies for the general public"; thus operated Socrate's cultural dichotomy of public-knowledge and private-knowledge, of "civic culture" and "professional culture", the social constructs that describe and establish the intellectual sphere of life as separate and apart from the civic sphere of life.[44][45]		The economist F. A. Hayek in the 20th century, said that intellectuals disproportionately support socialism for idealistic and utopian reasons that cannot be realized in practical terms. Nonetheless, in the article "Why Socialism?" (1949), Albert Einstein said that the economy of the world is not private property, because it is a "planetary community of production and consumption".[46][47] In U.S. society, the intellectual status class are demographically characterized as people who hold liberal-to-leftist political perspectives about guns-or-butter fiscal policy.[48]		The British historian Norman Stone said that the intellectual social class misunderstand the reality of society, and so are doomed to the errors of logical fallacy and [Ideological] stupidity; poor planning hampered by ideology.[49] In her memoirs, the Tory politician Margaret Thatcher said that the anti-monarchical French Revolution (1789–1799) was "a utopian attempt to overthrow a traditional order . . . in the name of abstract ideas, formulated by vain intellectuals".[50] Yet, as Prime Minister, Thatcher asked Britain's academics to help her government resolve the social problems of British society—whilst she retained the populist opinion of "The Intellectual" as being a man of un-British character, a thinker, not a doer; Thatcher's anti-intellectual perspective was shared by the mass media, especially The Spectator and The Sunday Telegraph newspapers, whose reportage documented a "lack of intellectuals" in Britain.[31][51]		In his essay "Why do intellectuals oppose capitalism?" (1998), Libertarian philosopher Robert Nozick of the Cato Institute argued that intellectuals become embittered leftists because their academic skills, much rewarded at school and at university, are under-valued and under-paid in the capitalist market economy; so, the intellectuals turned against capitalism—despite enjoying a more economically and financially comfortable life in a capitalist society than they might enjoy in either a socialist or a communist society.[52]		In post–Communist Europe, the social attitude perception of the intelligentsia became anti-intellectual; in the Netherlands, the word "intellectual" negatively connotes an overeducated person of "unrealistic visions of the World". In Hungary, the intellectual is perceived as an "egghead", a person who is "too-clever" for the good of society. In the Czech Republic, the intellectual is a cerebral person, aloof from reality. Such derogatory connotations of "intellectual" are not definitive, because, in the "case of English usage, positive, neutral, and pejorative uses can easily coexist"; the example is Václav Havel who, "to many outside observers, [became] a favoured instance of The Intellectual as National Icon" in the early history of the post–Communist Czech Republic.[53]		In the book, Intellectuals and Society (2010), the economist Thomas Sowell said that, lacking disincentives in professional life, the intellectual (producer of knowledge, not material goods) tends to speak outside his or her area of expertise, and expects social and professional benefits from the halo effect, derived from possessing professional expertise. That, in relation to other professions, the public intellectual is socially detached from the negative and unintended consequences of public policy derived from his or her ideas. As such, the philosopher and mathematician Bertrand Russell (1872–1970) advised the British government against national rearmament in the years before the First World War (1914–1918), while Imperial Germany prepared for war. Yet, the post-war intellectual reputation of Bertrand Russell remained almost immaculate, and his opinions respected by the general public, because of the halo effect.[54]		Effectively so, that is precisely the specific function of the intellectual: To treat everyone else as if they, too, were intellectuals. That is to say, to not attempt to hypnotise them, to intimidate them, or to seduce them, but to awaken in them the mechanism of intelligence that weighs, evaluates, and comprehends. One must start from the Socratic premise that everyone in the world reveals himself, herself intelligent when treated as if intelligent. Is that social function compatible with the offices of politicians? Because, more often than not, they tend to govern themselves by the cynical principle that: “One must not treat the public as if they were imbeciles, nor forget that they are imbeciles", which was established by the novelist Frédéric Beigbeder (who, not in vain, began his career as an advertising man); it is plainly obvious that those are opposite approaches. What is bad, is that the first approach demands effort from the interlocutors — attention, reflection, and dubious sizings-up, while the second approach flatters the primitive emotions of enthusiasm or revenge, and converts critical thinking to satire or to swearing curses, and social problems into notorious scandal. . . . Of course, the advocates of atavistic formulas periodically return to the charge, because those emotional formulas are easily assumed out of ignorance (populism, as you already know, is democracy for the mentally lazy), and, as such, are more necessary than ever; thus, if there be no intellectuals in politics, at the least, there should be intellectual ethos in public and in social discourse. Nonetheless, the lesson of personal experience often is negative, and the honest intellectuals whom I know always have returned crestfallen [from politics], like the pioneer Plato returned from Syracuse. . . ."[6]		
Philip Kindred Dick (December 16, 1928 – March 2, 1982) was an American writer notable for publishing works of science fiction. Dick explored philosophical, social, and political themes in novels with plots dominated by monopolistic corporations, authoritarian governments, alternate universes, and altered states of consciousness. His work reflected his personal interest in metaphysics and theology, and often drew upon his life experiences in addressing the nature of reality, identity, drug abuse, schizophrenia, and transcendental experiences.		Born in Illinois before moving to California, Dick began publishing science fiction stories in the 1950s, initially finding little commercial success.[1] His 1962 alternate history novel The Man in the High Castle earned Dick early acclaim, including a Hugo Award for Best Novel.[2] He followed with science fiction novels such as Do Androids Dream of Electric Sheep? (1968) and Ubik (1969). His 1974 novel Flow My Tears, the Policeman Said won the John W. Campbell Memorial Award for best novel.[3] Following a series of religious experiences in February–March 1974, Dick's work engaged more explicitly with issues of theology, philosophy, and the nature of reality, as in such novels as A Scanner Darkly (1977) and VALIS (1981).[4] A collection of his non-fiction writing on these themes was published posthumously as The Exegesis of Philip K. Dick (2011). He died in 1982 of a stroke, aged 53.		In addition to 44 published novels, Dick wrote approximately 121 short stories, most of which appeared in science fiction magazines during his lifetime.[5] A variety of popular films based on his works have been produced, including Blade Runner (1982), Total Recall (1990), Minority Report (2002), A Scanner Darkly (2006), Paycheck (2003), Next (2007), and The Adjustment Bureau (2011). In 2005, Time magazine named Ubik one of the hundred greatest English-language novels published since 1923.[6] In 2007, Dick became the first science fiction writer to be included in The Library of America series.[7][8][9][10]						Philip Kindred Dick and his twin sister, Jane Charlotte Dick, were born six weeks prematurely on December 16, 1928, in Chicago, Illinois, to Dorothy (née Kindred; 1900–1978) and Joseph Edgar Dick (1899–1985), who worked for the United States Department of Agriculture.[11][12] His paternal grandparents were Irish.[13] The death of Jane six weeks later, on January 26, 1929, profoundly affected Philip's life, leading to the recurrent motif of the "phantom twin" in his books.[11]		His family later moved to the San Francisco Bay Area. When Philip was five, his father was transferred to Reno, Nevada; when Dorothy refused to move, she and Joseph divorced. Both parents fought for custody of Philip, which was awarded to the mother. Dorothy, determined to raise Philip alone, took a job in Washington, D.C., and moved there with her son. Philip was enrolled at John Eaton Elementary School (1936–38), completing the second through fourth grades. His lowest grade was a "C" in Written Composition, although a teacher remarked that he "shows interest and ability in story telling." He was educated in Quaker schools.[14] In June 1938, Dorothy and Philip returned to California, and it was around this time that he became interested in science fiction.[15] Dick stated that he read his first science fiction magazine, Stirring Science Stories in 1940 at the age of twelve.[15]		Dick attended Berkeley High School in Berkeley, California. He and fellow science fiction author Ursula K. Le Guin were members of the same graduating class (1947) but did not know each other at the time. After graduation, he briefly attended the University of California, Berkeley, (September 1949 to November 11, 1949) with an honorable dismissal granted January 1, 1950. Dick did not declare a major and took classes in history, psychology, philosophy, and zoology. Through his studies in philosophy, he believed that existence is based on internal human perception, which does not necessarily correspond to external reality; he described himself as "an acosmic panentheist," believing in the universe only as an extension of God.[16] After reading the works of Plato and pondering the possibilities of metaphysical realms, Dick came to the conclusion that, in a certain sense, the world is not entirely real and there is no way to confirm whether it is truly there. This question from his early studies persisted as a theme in many of his novels. Dick dropped out because of ongoing anxiety problems, according to his third wife Anne's memoir. She also says he disliked the mandatory ROTC training. At Berkeley, Dick befriended poet Robert Duncan and poet and linguist Jack Spicer, who gave Dick ideas for a Martian language. Dick claimed to have been host of a classical music program on KSMO Radio in 1947.[17] From 1948 to 1952, Dick worked at Art Music Company, a record store on Telegraph Avenue.		Dick sold his first story in 1951, and from then on wrote full-time. During 1952 his first speculative fiction publications appeared in July and September numbers of Planet Stories, edited by Jack O'Sullivan, and in If and The Magazine of Fantasy and Science Fiction that fall.[18] His debut novel was Solar Lottery, published in 1955 as half of Ace Double #D-103 alongside The Big Jump by Leigh Brackett.[18] The 1950s were a difficult and impoverished time for Dick, who once lamented, "We couldn't even pay the late fees on a library book." He published almost exclusively within the science fiction genre, but dreamed of a career in mainstream fiction.[19] During the 1950s he produced a series of non-genre, relatively conventional novels. In 1960 he wrote that he was willing to "take twenty to thirty years to succeed as a literary writer." The dream of mainstream success formally died in January 1963 when the Scott Meredith Literary Agency returned all of his unsold mainstream novels. Only one of these works, Confessions of a Crap Artist, was published during Dick's lifetime.		In 1963, Dick won the Hugo Award for The Man in the High Castle.[2] Although he was hailed as a genius in the science fiction world, the mainstream literary world was unappreciative, and he could publish books only through low-paying science fiction publishers such as Ace. Even in his later years, he continued to have financial troubles. In the introduction to the 1980 short story collection The Golden Man, Dick wrote:		Several years ago, when I was ill, Heinlein offered his help, anything he could do, and we had never met; he would phone me to cheer me up and see how I was doing. He wanted to buy me an electric typewriter, God bless him—one of the few true gentlemen in this world. I don't agree with any ideas he puts forth in his writing, but that is neither here nor there. One time when I owed the IRS a lot of money and couldn't raise it, Heinlein loaned the money to me. I think a great deal of him and his wife; I dedicated a book to them in appreciation. Robert Heinlein is a fine-looking man, very impressive and very military in stance; you can tell he has a military background, even to the haircut. He knows I'm a flipped-out freak and still he helped me and my wife when we were in trouble. That is the best in humanity, there; that is who and what I love.		In 1972, Dick donated manuscripts, papers and other materials to the Special Collections Library at California State University, Fullerton where they are archived in the Philip K. Dick Science Fiction Collection in the Pollak Library. It was in Fullerton that Philip K. Dick befriended budding science-fiction writers K. W. Jeter, James Blaylock, and Tim Powers.		In 1971, Dick's marriage to Nancy Hackett broke down, and she moved out of their shared home. Dick descended into amphetamine abuse, eventually allowing a number of other drug users to move into the house with him.[20] One day in November of that year, Dick returned to his home in San Rafael to discover that it had been burgled, with his safe blown open and personal papers missing. The police were unable to determine the culprit, and even suspected Dick of having done so himself.[21] Shortly afterwards, he was invited to be guest of honor at the Vancouver Science Fiction Convention in February 1972. Within a day of arriving at the conference and giving his speech The Android and the Human, he informed people that he had fallen in love with a woman that he had met there, called Janis, and announced that he would be remaining in Vancouver.[21] An attendee of the conference, Michael Walsh, movie critic for local newspaper The Province, invited Dick to stay in his home, but asked him to leave two weeks later due to his erratic behavior. This was followed by Janis ending her and Dick's relationship and moving away. On March 23, 1972, Dick attempted to commit suicide by consuming an overdose of the sedative potassium bromide.[21] Subsequently, after deciding to seek help, Dick became a participant in X-Kalay (a Canadian Synanon-type recovery program), and was well enough by April that he was able to return to California.[21]		Dick returned to the events of these months while writing his 1977 novel A Scanner Darkly,[22] which contains fictionalized depictions of the burglary of his home, his time using amphetamines and living with addicts, and his experiences of X-Kalay (portrayed in the novel as "New-Path"). A factual account of Dick's recovery program participation was portrayed in his posthumously released book The Dark Haired Girl, a collection of letters and journals from the period.		On February 20, 1974, while recovering from the effects of sodium pentothal administered for the extraction of an impacted wisdom tooth, Dick received a home delivery of Darvon from a young woman. When he opened the door, he was struck by the beauty of the dark-haired girl and was especially drawn to her golden necklace. He asked her about its curious fish-shaped design. "This is a sign used by the early Christians," she said, and then left. Dick called the symbol the "vesicle pisces". This name seems to have been based on his conflation of two related symbols, the Christian ichthys symbol (two intersecting arcs delineating a fish in profile) which the woman was wearing, and the vesica piscis.[23]		Dick recounted that as the sun glinted off the gold pendant, the reflection caused the generation of a "pink beam" of light that mesmerized him. He came to believe the beam imparted wisdom and clairvoyance, and also believed it to be intelligent. On one occasion, Dick was startled by a separate recurrence of the pink beam. It imparted the information to him that his infant son was ill. The Dicks rushed the child to the hospital, where his suspicion was confirmed by professional diagnosis.[24][verification needed]		After the woman's departure, Dick began experiencing strange hallucinations. Although initially attributing them to side effects from medication, he considered this explanation implausible after weeks of continued hallucinations. "I experienced an invasion of my mind by a transcendentally rational mind, as if I had been insane all my life and suddenly I had become sane," Dick told Charles Platt.[25]		Throughout February and March 1974, Dick experienced a series of hallucinations, which he referred to as "2-3-74",[26] shorthand for February–March 1974. Aside from the "pink beam", Dick described the initial hallucinations as geometric patterns, and, occasionally, brief pictures of Jesus and ancient Rome. As the hallucinations increased in length and frequency, Dick claimed he began to live two parallel lives, one as himself, "Philip K. Dick", and one as "Thomas", a Christian persecuted by Romans in the first century AD. He referred to the "transcendentally rational mind" as "Zebra", "God" and "VALIS". Dick wrote about the experiences, first in the semi-autobiographical novel Radio Free Albemuth and then in VALIS, The Divine Invasion and the unfinished The Owl in Daylight (the VALIS trilogy).		In 1974, Dick wrote a letter to the FBI, accusing a number of people, including University of California, San Diego professor Frederic Jameson, of being foreign agents of Warsaw Pact powers.[27]		At one point Dick felt that he had been taken over by the spirit of the prophet Elijah. He believed that an episode in his novel Flow My Tears, the Policeman Said was a detailed retelling of a biblical story from the Book of Acts, which he had never read.[28] Dick documented and discussed his experiences and faith in a private journal he called his "exegesis", portions of which were later published as The Exegesis of Philip K. Dick. The last novel Dick wrote was The Transmigration of Timothy Archer; it was published shortly after his death in 1982.		Dick was married five times:		Dick had three children, Laura Archer (February 25, 1960), Isolde Freya (now Isa Dick Hackett) (March 15, 1967), and Christopher Kenneth (July 25, 1973).		In 1955, he and his second wife, Kleo Apostolides, received a visit from the FBI, which they believed to be the result of Kleo's socialist views and left-wing activities. The couple briefly befriended one of the FBI agents.[29]		Dick tried to stay out of the political scene because of high societal turmoil from the Vietnam War; however, he did show some anti-Vietnam War and anti-governmental sentiments. In 1968, he joined the "Writers and Editors War Tax Protest",[16][30] an anti-war pledge to pay no U.S. federal income tax, which resulted in the confiscation of his car by the IRS.		On February 17, 1982, after completing an interview, Dick contacted his therapist, complaining of failing eyesight, and was advised to go to a hospital immediately; but he did not. The next day, he was found unconscious on the floor of his Santa Ana, California, home, having suffered a stroke. In the hospital, he suffered another stroke, after which his brain activity ceased. Five days later, on March 2, 1982, he was disconnected from life support and died. After his death, Dick's father, Joseph, took his son's ashes to Riverside Cemetery in Fort Morgan, Colorado, (section K, block 1, lot 56), where they were buried next to his twin sister Jane, whose tombstone had been inscribed with both their names when she died 53 years earlier.[31][32][33]		Dick's stories typically focus on the fragile nature of what is real and the construction of personal identity. His stories often become surreal fantasies, as the main characters slowly discover that their everyday world is actually an illusion assembled by powerful external entities, such as the suspended animation in Ubik,[35] vast political conspiracies or the vicissitudes of an unreliable narrator. "All of his work starts with the basic assumption that there cannot be one, single, objective reality", writes science fiction author Charles Platt. "Everything is a matter of perception. The ground is liable to shift under your feet. A protagonist may find himself living out another person's dream, or he may enter a drug-induced state that actually makes better sense than the real world, or he may cross into a different universe completely."[25]		Alternate universes and simulacra are common plot devices, with fictional worlds inhabited by common, working people, rather than galactic elites. "There are no heroes in Dick's books", Ursula K. Le Guin wrote, "but there are heroics. One is reminded of Dickens: what counts is the honesty, constancy, kindness and patience of ordinary people."[35] Dick made no secret that much of his thinking and work was heavily influenced by the writings of Carl Jung.[31][36] The Jungian constructs and models that most concerned Dick seem to be the archetypes of the collective unconscious, group projection/hallucination, synchronicities, and personality theory.[31] Many of Dick's protagonists overtly analyze reality and their perceptions in Jungian terms (see Lies Inc.). Dick's self-named Exegesis also contained many notes on Jung in relation to theology and mysticism.[citation needed]		Dick identified one major theme of his work as the question, "What constitutes the authentic human being?"[37] In works such as Do Androids Dream of Electric Sheep?, beings can appear totally human in every respect while lacking soul or compassion, while completely alien beings such as Glimmung in Galactic Pot-Healer may be more humane and complex than their human peers.		Mental illness was a constant interest of Dick's, and themes of mental illness permeate his work. The character Jack Bohlen in the 1964 novel Martian Time-Slip is an "ex-schizophrenic". The novel Clans of the Alphane Moon centers on an entire society made up of descendants of lunatic asylum inmates. In 1965 he wrote the essay titled "Schizophrenia and the Book of Changes".[38]		Drug use (including religious, recreational, and abuse) was also a theme in many of Dick's works, such as A Scanner Darkly and The Three Stigmata of Palmer Eldritch. Dick himself was a drug user for much of his life. According to a 1975 interview in Rolling Stone,[39] Dick wrote all of his books published before 1970 while on amphetamines. "A Scanner Darkly (1977) was the first complete novel I had written without speed", said Dick in the interview. He also experimented briefly with psychedelics, but wrote The Three Stigmata of Palmer Eldritch, which Rolling Stone dubs "the classic LSD novel of all time", before he had ever tried them. Despite his heavy amphetamine use, however, Dick later said that doctors told him the amphetamines never actually affected him, that his liver had processed them before they reached his brain.[39]		Summing up all these themes in Understanding Philip K. Dick, Eric Carl Link discussed eight themes or 'ideas and motifs':[40] Epistemology and the Nature of Reality, Know Thyself, The Android and the Human, Entropy and Pot Healing, The Theodicy Problem, Warfare and Power Politics, The Evolved Human, and 'Technology, Media, Drugs and Madness'.[41]		Dick had two professional stories published under the pen names Richard Phillipps and Jack Dowland. "Some Kinds of Life" was published in October 1953 in Fantastic Universe under byline Richard Phillipps, apparently because the magazine had a policy against publishing multiple stories by the same author in the same issue; "Planet for Transients" was published in the same issue under his own name.[42]		The short story "Orpheus with Clay Feet" was published under the pen name Jack Dowland. The protagonist desires to be the muse for fictional author Jack Dowland, considered the greatest science fiction author of the 20th century. In the story, Dowland publishes a short story titled "Orpheus with Clay Feet" under the pen name Philip K. Dick.		The surname Dowland refers to Renaissance composer John Dowland, who is featured in several works. The title Flow My Tears, the Policeman Said directly refers to Dowland's best-known composition, "Flow My Tears". In the novel The Divine Invasion, the character Linda Fox, created specifically with Linda Ronstadt in mind, is an intergalactically famous singer whose entire body of work consists of recordings of John Dowland compositions.		The Man in the High Castle (1962) is set in an alternate history in which the United States is ruled by the victorious Axis powers. It is the only Dick novel to win a Hugo Award. Most recently this has been adapted into a television series provided by Amazon and available through Amazon Prime video.		The Three Stigmata of Palmer Eldritch (1965) utilizes an array of science fiction concepts and features several layers of reality and unreality. It is also one of Dick's first works to explore religious themes. The novel takes place in the 21st century, when, under UN authority, mankind has colonized the Solar System's every habitable planet and moon. Life is physically daunting and psychologically monotonous for most colonists, so the UN must draft people to go to the colonies. Most entertain themselves using "Perky Pat" dolls and accessories manufactured by Earth-based "P.P. Layouts". The company also secretly creates "Can-D", an illegal but widely available hallucinogenic drug allowing the user to "translate" into Perky Pat (if the drug user is a woman) or Pat's boyfriend, Walt (if the drug user is a man). This recreational use of Can-D allows colonists to experience a few minutes of an idealized life on Earth by participating in a collective hallucination.		Do Androids Dream of Electric Sheep? (1968) is the story of a bounty hunter policing the local android population. It occurs on a dying, poisoned Earth de-populated of almost all animals and all "successful" humans; the only remaining inhabitants of the planet are people with no prospects off-world. The 1968 novel is the literary source of the film Blade Runner (1982).[43] It is both a conflation and an intensification of the pivotally Dickian question: What is real, what is fake? What crucial factor defines humanity as distinctly "alive", versus those merely alive only in their outward appearance?		Ubik (1969) employs extensive psychic telepathy and a suspended state after death in creating a state of eroding reality. A group of psychics is sent to investigate a rival organisation, but several of them are apparently killed by a saboteur's bomb. Much of the following novel flicks between a number of equally plausible realities; the "real" reality, a state of half-life and psychically manipulated realities. In 2005, Time magazine listed it among the "All-TIME 100 Greatest Novels" published since 1923.[6]		Flow My Tears, the Policeman Said (1974) concerns Jason Taverner, a television star living in a dystopian near-future police state. After being attacked by an angry ex-girlfriend, Taverner awakens in a dingy Los Angeles hotel room. He still has his money in his wallet, but his identification cards are missing. This is no minor inconvenience, as security checkpoints (manned by "pols" and "nats", the police and National Guard) are set up throughout the city to stop and arrest anyone without valid ID. Jason at first thinks that he was robbed, but soon discovers that his entire identity has been erased. There is no record of him in any official database, and even his closest associates do not recognize or remember him. For the first time in many years, Jason has no fame or reputation to rely on. He has only his innate charm and social graces to help him as he tries to find out what happened to his past while avoiding the attention of the pols. The novel was Dick's first published novel after years of silence, during which time his critical reputation had grown, and this novel was awarded the John W. Campbell Memorial Award for Best Science Fiction Novel.[3] It is the only Philip K. Dick novel nominated for both a Hugo and a Nebula Award.		In an essay written two years before his death, Dick described how he learned from his Episcopalian priest that an important scene in Flow My Tears, the Policeman Said – involving its other main character, the eponymous Police General Felix Buckman, was very similar to a scene in Acts of the Apostles,[28] a book of the Christian New Testament. Film director Richard Linklater discusses this novel in his film Waking Life, which begins with a scene reminiscent of another Dick novel, Time Out of Joint.		A Scanner Darkly (1977) is a bleak mixture of science fiction and police procedural novels; in its story, an undercover narcotics police detective begins to lose touch with reality after falling victim to the same permanently mind-altering drug, Substance D, he was enlisted to help fight. Substance D is instantly addictive, beginning with a pleasant euphoria which is quickly replaced with increasing confusion, hallucinations and eventually total psychosis. In this novel, as with all Dick novels, there is an underlying thread of paranoia and dissociation with multiple realities perceived simultaneously. It was adapted to film by Richard Linklater.		The Philip K. Dick Reader[44] is an introduction to the variety of Dick's short fiction.		VALIS (1980) is perhaps Dick's most postmodern and autobiographical novel, examining his own unexplained experiences. It may also be his most academically studied work, and was adapted as an opera by Tod Machover.[45] Later works like the VALIS trilogy were heavily autobiographical, many with "two-three-seventy-four" (2-3-74) references and influences. The word VALIS is the acronym for Vast Active Living Intelligence System. Later, Dick theorized that VALIS was both a "reality generator" and a means of extraterrestrial communication. A fourth VALIS manuscript, Radio Free Albemuth, although composed in 1976, was posthumously published in 1985. This work is described by the publisher (Arbor House) as "an introduction and key to his magnificent VALIS trilogy."		Regardless of the feeling that he was somehow experiencing a divine communication, Dick was never fully able to rationalize the events. For the rest of his life, he struggled to comprehend what was occurring, questioning his own sanity and perception of reality. He transcribed what thoughts he could into an eight-thousand-page, one-million-word journal dubbed the Exegesis. From 1974 until his death in 1982, Dick spent many nights writing in this journal. A recurring theme in Exegesis is Dick's hypothesis that history had been stopped in the first century AD, and that "the Empire never ended". He saw Rome as the pinnacle of materialism and despotism, which, after forcing the Gnostics underground, had kept the population of Earth enslaved to worldly possessions. Dick believed that VALIS had communicated with him, and anonymous others, to induce the impeachment of U.S. President Richard Nixon, whom Dick believed to be the current Emperor of Rome incarnate.		In a 1968 essay titled "Self Portrait", collected in the 1995 book The Shifting Realities of Philip K. Dick, Dick reflects on his work and lists which books he feels "might escape World War Three": Eye in the Sky, The Man in the High Castle, Martian Time-Slip, Dr. Bloodmoney, or How We Got Along After the Bomb, The Zap Gun, The Penultimate Truth, The Simulacra, The Three Stigmata of Palmer Eldritch (which he refers to as "the most vital of them all"), Do Androids Dream of Electric Sheep?, and Ubik.[46] In a 1976 interview, Dick cited A Scanner Darkly as his best work, feeling that he "had finally written a true masterpiece, after 25 years of writing".[47]		A number of Dick's stories have been made into films. Dick himself wrote a screenplay for an intended film adaptation of Ubik in 1974, but the film was never made. Many film adaptations have not used Dick's original titles. When asked why this was, Dick's ex-wife Tessa said, "Actually, the books rarely carry Phil's original titles, as the editors usually wrote new titles after reading his manuscripts. Phil often commented that he couldn't write good titles. If he could, he would have been an advertising writer instead of a novelist."[48] Films based on Dick's writing had accumulated a total revenue of over US $1 billion by 2009.[49]		Future films based on Dick's writing include an animated adaptation of The King of the Elves from Walt Disney Animation Studios, set to be released in the spring of 2016; and a film adaptation of Ubik which, according to Dick's daughter, Isa Dick Hackett, is in advanced negotiation.[52] Ubik is set to be made into a film by Michel Gondry.[53] In 2014, however, writer/director Gondry told French outlet Telerama (via Jeux Actu), that he was no longer working on the project.		The Terminator series prominently features the theme of humanoid assassination machines first portrayed in Second Variety. The Halcyon Company, known for developing the Terminator franchise, acquired right of first refusal to film adaptations of the works of Philip K. Dick in 2007. In May 2009, they announced plans for an adaptation of Flow My Tears, the Policeman Said.[54]		It was reported in 2010 that Ridley Scott would produce an adaptation of The Man in the High Castle for the BBC, in the form of a mini-series.[55] A pilot episode was released on Amazon Prime in January 2015 and Season 1 was fully released in ten episodes of about 60 minutes each on 20 Nov 2015.[56]		In late 2015, Fox aired The Minority Report, a sequel adaptation to the 2002 film of the same name based on Dick's 1956 short story "The Minority Report".		In May 2016, it was announced that a 10-part anthology series was in the works. Titled Philip K. Dick’s Electric Dreams, the series will be distributed by Sony Pictures Television and will premiere on Channel 4 in the United Kingdom and Amazon Video in the United States.[57] It will be written by executive producers Ronald D. Moore and Michael Dinner and will star Bryan Cranston, also an executive producer.[58]		Four of Dick's works have been adapted for the stage.		One was the opera VALIS, composed and with libretto by Tod Machover, which premiered at the Pompidou Center in Paris on December 1, 1987, with a French libretto. It was subsequently revised and readapted into English, and was recorded and released on CD (Bridge Records BCD9007) in 1988.		Another was Flow My Tears, the Policeman Said, adapted by Linda Hartinian and produced by the New York-based avant-garde company Mabou Mines. It premiered in Boston at the Boston Shakespeare Theatre (June 18–30, 1985) and was subsequently staged in New York and Chicago. Productions of Flow My Tears, the Policeman Said were also staged by the Evidence Room [59] in Los Angeles in 1999[60] and by the Fifth Column Theatre Company at the Oval House Theatre in London in the same year.[61]		A play based on Radio Free Albemuth also had a brief run in the 1980s.		In November 2010, a production of Do Androids Dream of Electric Sheep?, adapted by Edward Einhorn, premiered at the 3LD Art and Technology Center in Manhattan.[62]		A radio drama adaptation of Dick's short story "Mr. Spaceship" was aired by the Finnish Broadcasting Company (Yleisradio) in 1996 under the name Menolippu Paratiisiin. Radio dramatizations of Dick's short stories Colony and The Defenders[63] were aired by NBC in 1956 as part of the series X Minus One.		In January 2006 The Three Stigmata of Palmer Eldritch (English for Trzy stygmaty Palmera Eldritcha) theatre adaptation premiered in polish Stary Teatr in Cracov, interestingly with an extensive use of lights and laser choreography.[64][65]		Marvel Comics adapted Dick's short story "The Electric Ant" as a limited series which was released in 2009. The comic was produced by writer David Mack (Daredevil) and artist Pascal Alixe (Ultimate X-Men), with covers provided by artist Paul Pope.[66] "The Electric Ant" had earlier been loosely adapted by Frank Miller and Geof Darrow in their 3-issue mini-series Hard Boiled published by Dark Horse Comics in 1990-1992.[67]		In 2009, BOOM! Studios started publishing a 24-issue miniseries comic book adaptation of Do Androids Dream of Electric Sheep?[68] Blade Runner, the 1982 film adapted from Do Androids Dream of Electric Sheep?, had previously been adapted to comics as A Marvel Comics Super Special: Blade Runner.		In 2011, Dynamite Entertainment published a 4-issue miniseries "Total Recall," a sequel to the 1990 film Total Recall, inspired by Philip K. Dick's short story "We Can Remember It for You Wholesale".[69] In 1990, DC Comics published the official adaptation of the original film as a DC Movie Special: Total Recall.[70]		In response to a 1975 request from the National Library for the Blind for permission to make use of The Man in the High Castle, Dick responded, "I also grant you a general permission to transcribe any of my former, present or future work, so indeed you can add my name to your 'general permission' list."[71] A number of his books and stories are available in braille and other specialized formats through the NLS.[72]		As of December 2012, thirteen of Philip K. Dick's early works in the public domain in the United States are available in ebook form from Project Gutenberg. As of April 4, 2012, Wikisource has one of Philip K. Dick's early works in the public domain in the United States available in ebook form which is not from Project Gutenberg.		Lawrence Sutin's 1989 biography of Dick, Divine Invasions: A Life of Philip K. Dick, is considered the standard biographical treatment of Dick's life.[38]		In 1993, French writer Emmanuel Carrère published Je suis vivant et vous êtes morts which was first translated and published in English in 2004 as I Am Alive and You Are Dead: A Journey Into the Mind of Philip K. Dick, which the author describes in his preface in this way:		The book you hold in your hands is a very peculiar book. I have tried to depict the life of Philip K. Dick from the inside, in other words, with the same freedom and empathy – indeed with the same truth – with which he depicted his own characters.[31]		Critics of the book have complained about the lack of fact checking, sourcing, notes and index, "the usual evidence of deep research that gives a biography the solid stamp of authority."[73][74][75] It can be considered a non-fiction novel about his life.		Dick has influenced many writers, including Jonathan Lethem,[76] and Ursula K. Le Guin.[77] The prominent literary critic Fredric Jameson proclaimed Dick the "Shakespeare of Science Fiction", and praised his work as "one of the most powerful expressions of the society of spectacle and pseudo-event".[78] The author Roberto Bolaño also praised Dick, describing him as "Thoreau plus the death of the American dream".[79] Dick has also influenced filmmakers, his work being compared to films such as the Wachowskis' The Matrix,[80] David Cronenberg's Videodrome,[81] eXistenZ,[80] and Spider,[81] Spike Jonze's Being John Malkovich,[81] Adaptation,[81] Michel Gondry's Eternal Sunshine of the Spotless Mind,[82][83] Alex Proyas's Dark City,[80] Peter Weir's The Truman Show,[80] Andrew Niccol's Gattaca,[81] In Time,[84] Terry Gilliam's 12 Monkeys,[81] Wes Craven's A Nightmare on Elm Street,[85] David Lynch's Mulholland Drive,[85] Alejandro Amenábar's Open Your Eyes,[86] David Fincher's Fight Club,[81] Cameron Crowe's Vanilla Sky,[80] Darren Aronofsky's Pi,[87] Richard Kelly's Donnie Darko[88] and Southland Tales,[89] Rian Johnson's Looper,[90] Duncan Jones' Source Code, and Christopher Nolan's Memento[91] and Inception.[92]		The Philip K. Dick Society was an organization dedicated to promoting the literary works of Dick and was led by Dick's longtime friend and music journalist Paul Williams. Williams also served as Dick's literary executor for several years after Dick's death and wrote one of the first biographies of Dick, entitled Only Apparently Real: The World of Philip K. Dick.		The Philip K. Dick estate owns and operates the production company Electric Shepherd Productions,[93] which has produced the films Adjustment Bureau (2011) and the upcoming Walt Disney Company film King of the Elves, the TV series The Man in the High Castle[94] and also a Marvel Comics 5-issue adaptation of Electric Ant.[95]		Dick was recreated by his fans in the form of a simulacrum or remote-controlled android designed in his likeness.[96][97][98] Such simulacra had been themes of many of Dick's works. The Philip K. Dick simulacrum was included on a discussion panel in a San Diego Comic Con presentation about the film adaptation of the novel, A Scanner Darkly. In February 2006, an America West Airlines employee misplaced the android's head, and it has not yet been found.[99] In January 2011, it was announced that Hanson Robotics had built a replacement.[100]		Postmodernists such as Jean Baudrillard, Fredric Jameson, Laurence Rickels and Slavoj Žižek have commented on Dick's writing's foreshadowing of postmodernity.[122] Jean Baudrillard offers this interpretation:		It is hyperreal. It is a universe of simulation, which is something altogether different. And this is so not because Dick speaks specifically of simulacra. SF has always done so, but it has always played upon the double, on artificial replication or imaginary duplication, whereas here the double has disappeared. There is no more double; one is always already in the other world, an other world which is not another, without mirrors or projection or utopias as means for reflection. The simulation is impassable, unsurpassable, checkmated, without exteriority. We can no longer move "through the mirror" to the other side, as we could during the golden age of transcendence.[123]		For his anti-government skepticism, Philip K. Dick was afforded minor mention in Mythmakers and Lawbreakers, a collection of interviews about fiction by anarchist authors. Noting his early authorship of "The Last of the Masters", an anarchist-themed novelette, author Margaret Killjoy expressed that while Dick never fully sided with anarchism, his opposition to government centralization and organized religion has influenced anarchist interpretations of gnosticism.[124]		The Science Fiction Hall of Fame inducted Dick in 2005.[125]		During his lifetime he received numerous annual literary awards and nominations for particular works.[126]		The Philip K. Dick Award is a science fiction award that annually recognizes the previous year's best SF paperback original published in the U.S.[132] It is conferred at Norwescon, sponsored by the Philadelphia Science Fiction Society, and since 2005 supported by the Philip K. Dick Trust. Winning works are identified on their covers as Best Original SF Paperback. It is currently administered by David G. Hartwell and Gordon Van Gelder.[132]		The award was inaugurated in 1983, the year after Dick's death. It was founded by Thomas Disch with assistance from David G. Hartwell, Paul S. Williams, and Charles N. Brown. Past administrators include Algis J. Budrys and David Alexander Smith.[citation needed]		
"White & Nerdy" is the second single from "Weird Al" Yankovic's album Straight Outta Lynwood, which was released on September 26, 2006. It parodies the song "Ridin'" by Chamillionaire and Krayzie Bone. The song both satirizes and celebrates nerd culture, as recited by the subject who cannot "roll with the gangstas" because he is "just too white and nerdy", and includes constant references to stereotypically nerdy activities, such as collecting comic books, playing Dungeons & Dragons (D&D), going to the Renaissance Fair, riding a Segway, and editing Wikipedia, as well as stereotypically Caucasian activities, such as watching Happy Days and eating "all [his] sandwiches with mayonnaise."		The song has won many accolades, including being Yankovic's first Billboard Hot 100 top 10 hit, which peaked at #9. It was also certified platinum by the RIAA, the first Yankovic single to achieve this feat. Chamillionaire himself also stated that he enjoyed the song.						While Yankovic usually records his songs together with his band, the backing tracks for "White and Nerdy" were completely recorded by guitarist Jim West—who handled the synthesizer production—and Jon "Bermuda" Schwartz—who was tasked with recording the drums. The two musicians recorded their specific tracks at their home studios. The finished audio tracks were then brought to Westlake Studio in Los Angeles, California, where Yankovic added his rap vocals.[1]		The single has an accompanying music video, which was recorded in high definition. The video, loosely parodying the video for Chamillionaire's Ridin' and following the song's lyrics, shows Yankovic, dressed as a stereotypical nerd with a buttoned-up polo shirt, dress slacks, and horn-rimmed glasses (see also Yuppie or Preppie), attempting to fit in with the "gangsters", but instead either scaring them away, causing them to flip him off (shown on the scene where Yankovic is riding on a Segway in the park), or to direct him away from their group and instead towards a herd of other nerds (shown when Yankovic is in a bowling alley). These scenes include shots that directly parody the "Ridin'" video, including similar outfits by both artists. Yankovic is seen at night dancing in front of a set of road flares arranged in the form of Pac-Man, similar to the shot of Chamillionaire in front of the figure of a lizard, his personal logo. Another repeated scene shows Yankovic along with Donny Osmond—the "whitest guy I could think of", according to Yankovic[2]—dancing in front of the Schrödinger equation, mimicking shots of Chamillionaire and Krayzie Bone from the "Ridin'" video. Whereas Chamillionaire's video ends with the artist throwing up a gang hand sign to the camera, "White & Nerdy" ends with Yankovic giving a mistaken version of the Vulcan salute (his thumb is against the side of his hand instead of perpendicular).		Further interspersed among these shots are additional shots of Yankovic demonstrating his "white and nerdy" life. One scene shows Yankovic vandalizing the Wikipedia page for Atlantic Records, replacing it with the words (in excessively large type) "YOU SUCK!", referencing his recent trouble with the company in getting permission to release "You're Pitiful", a parody of James Blunt's song "You're Beautiful".[3] Fans of the video have replicated the action depicted in the video several times. The video shows a fictional Trivial Pursuit card, with questions that include the location of "The Biggest Ball of Twine in Minnesota", on what page Harry Potter would die in the next book, and the number of Wicket men there are on a 43-Man Squamish team.[4] Yankovic is shown making a shady deal with a thug in a back alley to acquire a bootleg VHS copy of the Star Wars Holiday Special.[5] When Yankovic is on his Myspace page in the video, 'White and Nerdy has 27 friends' can be seen at the top of the computer screen (Bill Gates, Napoleon Dynamite, Mr. Peabody, Albert Einstein, Screech, Frodo, Pee-Wee Herman, and Tom). This is a continuation of his usual trend of putting the number 27 somewhere in his videos. The mayonnaise Al spreads on a sandwich in the video is labelled "Pest Foods" mayonnaise, parodying Best Foods mayonnaise. Also, when Yankovic says, "Even made a homepage for my dog, yo," the music video shows a website devoted to his dog, Foofie.		In addition to Osmond, there are other cameos in the video. Jordan Peele and Keegan-Michael Key of MADtv appear in the blue 1967 Chevrolet Impala convertible at the beginning of the video, two "gangsters" who are scared away by Yankovic's nerdiness. The license plate on the Impala reads "OG4LIFE", a reference to Ice-T's 1991 album Original Gangster. Seth Green appears in front of a wall display of action figures; Green has a huge personal collection of action figures (including Star Wars) that are stored in a large storage unit he shares with Hugh Sterbakov[6] (though they're not "cherry", i.e., unopened, as the song describes them). Judy Tenuta, a regular on The Weird Al Show and who previously appeared in "Headline News", appears as the woman who receives a surge protector as a present. Other actors were personally recruited by Yankovic through a post on his MySpace page, from which he received several interested extras that said they would appear for free.		The comic book store featured in the video is Golden Apple Comics, located in Los Angeles, California,[7] while the Gap store is located on Wilshire Boulevard in Santa Monica.		Also in the comic book store, he is wearing a yellow T-shirt that says "Carl Sagan is my homeboy," a parody of the "Jesus is my homeboy" T-shirt design.		The video was leaked onto YouTube on September 17, 2006, just one day before the planned official release date at 9 PM Pacific time at AOL.com. Due to the leak, the premiere was canceled, and AOL silently slipped the video onto their website hours before the premiere was set to take place.[8] VH1 started airing the video in "large rotation" on September 20, 2006, and it debuted at #5 on their Top 20 countdown.[9] It is available to watch on Yankovic's MySpace page, as well as on several other video hosting sites and P2P theaters.		Yahoo! Music solicited online members to make a fans-only video for "White & Nerdy". The video was released in late 2006.[10]		On November 2, 2006, Yankovic performed "White & Nerdy" live on The Tonight Show with Jay Leno. (He rode a Segway as he came on stage.) It was his first public live performance of the song; various background vocals from the original recording are played to allow him time to breathe during the fast-paced lyrics. Since then, however, his band has provided live background vocals. On December 3, he performed the song live at the VH1 "Big In '06" Awards. Since 2007, Yankovic has performed the song on each of his tours,[11] entering the stage on a Segway and wearing his hoodie and bandana from the video rather than using his "nerd" look. His band members maintain the "nerdy" outfits.[12]		The song became Yankovic's first career Top 10 hit on the Billboard Hot 100, a record 23 years after his first appearance on the Hot 100 chart with "Ricky" in 1983. It peaked at #9, beating his previous #12 peak for 1984's "Eat It". This was also his first Top 40 single since 1992's "Smells Like Nirvana".[13] It peaked on the Hot Digital Songs chart at #5. Both the song and the music video reached #1 at the U.S. and Australian iTunes Store, and peaked at #1 on VH1's top 20 video countdown. Both "White & Nerdy" and Straight Outta Lynwood were certified gold, and later platinum, by the RIAA.[9] This marks the first time any one of Yankovic's singles has been certified platinum.[14]		Chamillionaire himself put "White & Nerdy" on his official MySpace page, and commented that he enjoys the parody.[15] In an interview, he also stated he was pleasantly surprised by Yankovic's rapping ability, saying: "He's actually rapping pretty good on it, it's crazy ... I didn't know he could rap like that."[15] According to Yankovic, Chamillionaire approached him on the red carpet after "Ridin'" won the Grammy award for Best Rap Song at the 49th Annual Grammy Awards, and thanked him for creating the parody, as, in Yankovic's words, he had "made it undeniable that ["Ridin'"] was the rap song of the year".[16]		
In popular usage, eccentricity (also called quirkiness) refers to unusual or odd behavior on the part of an individual. This behavior would typically be perceived as unusual or unnecessary, without being demonstrably maladaptive. Eccentricity is contrasted with "normal" behavior, the nearly universal means by which individuals in society solve given problems and pursue certain priorities in everyday life. People who consistently display benignly eccentric behavior are labeled as "eccentrics".						From Medieval Latin eccentricus, derived from Greek ekkentros, "out of the center", from ek-, ex- "out of" + kentron, "center". Eccentric first appeared in English essays as a neologism in 1551 as an astronomical term meaning "a circle in which the earth, sun, etc. deviates from its center." Five years later, in 1556, an adjective form of the word was used. In 1685, the definition evolved from the literal to the figurative, and eccentric is noted to have begun being used to describe unconventional or odd behavior. A noun form of the word – a person who possesses and exhibits these unconventional or odd qualities and behaviors – appeared by 1832.		Eccentricity is often associated with genius, intellectual giftedness, or creativity. People may perceive the individual's eccentric behavior as the outward expression of their unique intelligence or creative impulse.[1] In this vein, the eccentric's habits are incomprehensible not because they are illogical or the result of madness, but because they stem from a mind so original that it cannot be conformed to societal norms. English utilitarian thinker John Stuart Mill (b. 1806) wrote that "the amount of eccentricity in a society has generally been proportional to the amount of genius, mental vigour, and moral courage which it contained,"[2][3] and mourned a lack of eccentricity as "the chief danger of the time".[4] Edith Sitwell (b. 1887) wrote that eccentricity is "often a kind of innocent pride", also saying that geniuses and aristocrats are called eccentrics because "they are entirely unafraid of and uninfluenced by the opinions and vagaries of the crowd".[5] Eccentricity is also associated with great wealth. What would be considered signs of insanity in a poor person, some may accept as eccentricity in these people.[6]		A person who is simply in a "fish out of water" situation is not, by the strictest definition, an eccentric since, presumably, he or she may be ordinary by the conventions of his or her native environment.		Eccentrics may or may not comprehend the standards for normal behavior in their culture. They are simply unconcerned by society's disapproval of their habits or beliefs. Many of history's most brilliant minds have displayed some unusual behaviors and habits.		Some eccentrics are pejoratively considered "cranks", rather than geniuses. Eccentric behavior is often considered whimsical or quirky, although it can also be strange and disturbing. Many individuals previously considered merely eccentric, such as aviation magnate Howard Hughes, have recently been retrospectively diagnosed as actually having suffered from mental disorders (obsessive–compulsive disorder in Hughes' case).		Other people may have eccentric taste in clothes, or have eccentric hobbies or collections they pursue with great vigor. They may have a pedantic and precise manner of speaking, intermingled with inventive wordplay.		Many individuals may even manifest eccentricities consciously and deliberately, in an attempt to differentiate themselves from societal norms or enhance a sense of inimitable identity; given the overwhelmingly positive stereotypes (at least in pop culture and especially with fictional characters) often associated with eccentricity, detailed above, certain individuals seek to be associated with this sort of character type. However, this is not always successful as eccentric individuals are not necessarily charismatic, and the individual in question may simply be dismissed by others as just seeking attention.		Extravagance is a kind of eccentricity, related to abundance and wastefulness; refer to description in hyperbole.		Psychologist David Weeks believes people with a mental illness "suffer" from their behavior while eccentrics are quite happy.[7][8] He even states eccentrics are less prone to mental illness than everyone else.		According to Weeks' study, there are several distinctive characteristics that often differentiate a healthy eccentric person from a regular person or someone who has a mental illness. The first five characteristics on Weeks' list are found in most people regarded as eccentric:[7]		
If I Ran the Zoo is a children's book written by Dr. Seuss in 1950.		The book is written in anapestic tetrameter, Seuss's usual verse type[citation needed], and illustrated in Seuss's trademark pen and ink style. The book is likely a tribute to a child's imagination[citation needed], because it ends with a reminder that all of the extraordinary creatures exist only in McGrew's head.		If I Ran the Zoo is often credited[1][2] with the first printed modern English appearance of the word "nerd," although the word is not used in its modern context, or even in a vaguely related context. It is simply the name of an otherwise un-characterized imaginary creature, appearing in the sentence "And then, just to show them, I'll sail to Ka-Troo/And Bring Back an It-Kutch, a Preep, and a Proo,/A Nerkle, a Nerd, and a Seersucker too!"		In the book, Gerald McGrew is a child who, when visiting a zoo, finds that the exotic animals are "not good enough". He says that if he ran the zoo, he would let all of the current animals free and find new, more bizarre and exotic ones. Throughout the book he lists these creatures, starting with a lion with ten feet and escalating to more imaginative (and imaginary) creatures, such as the Fizza-ma-Wizza-ma-Dill, "the world's biggest bird from the island of Gwark, who eats only pine trees, and spits out the bark." The illustrations also grow wilder as McGrew imagines going to increasingly remote and exotic habitats and capturing each fanciful creature, bringing them all back to a zoo now filled with his wild new animals. He also imagines the praise he receives from others, who are amazed at his "new Zoo, McGrew Zoo".		Some of the animals featured in "If I Ran the Zoo" have been featured in a segment of The Hoober-Bloob Highway, a 1975 CBS TV Special. In this segment, Hoober-Bloob babies don't have to be human if they don't choose to be, so Mr. Hoober-Bloob shows them a variety of different animals, including ones from "If I Ran The Zoo", such as Obsks, Wild Bippo-No-Bungus, a Tizzle-Topped Tufted Mazurka, a Big-Bug-Whos-Is-Very-Surprising, Chuggs, a Sort-Of-A-Hen, and an Elephant-Cat.						Dr. Seuss's "Zoo" book is also the main theme for one of the children's play areas at Universal Studios' Islands of Adventure. The small play area is located inside the area of the park known as Seuss Landing.		An animation short directed and produced by Ray Messecar and narrated by Brett Ambler.[3]		
Trousers (British English) or pants (American English) are an item of clothing worn from the waist to the ankles, covering both legs separately (rather than with cloth extending across both legs as in robes, skirts, and dresses).		In the UK, the word pants generally means underwear and not trousers.[1] Shorts are similar to trousers, but with legs that come down only to around the area of the knee, higher or lower depending on the style of the garment. To distinguish them from shorts, trousers may be called "long trousers" in certain contexts such as school uniform, where tailored shorts may be called "short trousers", especially in the UK.		The oldest known trousers are found at the Yanghai cemetery in Turpan, Xinjiang, western China, dated to the period between the 13th and the 10th centuries BC. Made of wool, the trousers had straight legs and wide crotches, and were likely made for horseback riding.[2][3]		In most of Europe, trousers have been worn since ancient times and throughout the Medieval period, becoming the most common form of lower-body clothing for adult males in the modern world, although shorts are also widely worn, and kilts and other garments may be worn in various regions and cultures. Breeches were worn instead of trousers in early modern Europe by some men in higher classes of society. Since the mid-20th century, trousers have increasingly been worn by women as well.		Jeans, made of denim, are a form of trousers for casual wear, now widely worn all over the world by both sexes. Shorts are often preferred in hot weather or for some sports and also often by children and adolescents. Trousers are worn on the hips or waist and may be held up by their own fastenings, a belt or suspenders (braces).						In Scotland, trousers are occasionally known as trews, which is the historic root of the word 'trousers'. Trousers are also known as breeks in Scots, a word related to breeches. The item of clothing worn under trousers is underpants. The standard form 'trousers' is also used, but it is sometimes pronounced in a manner approximately represented by "tru:zɨrz", possibly a throwback to the Gaelic word truis from which the English word originates.		In North America, Australia and South Africa pants is the general category term, whereas trousers (sometimes slacks in Australia and the United States) often refers more specifically to tailored garments with a waistband, belt-loops, and a fly-front. So informal elastic-waist knitted garments would be called pants, but not trousers (or slacks).		North Americans call undergarments underwear, underpants, undies, jockey shorts, shorts, long johns or panties (the last are women's garments specifically) to distinguish them from other pants that are worn on the outside. The term drawers normally refers to undergarments, but in some dialects, may be found as a synonym for "breeches", that is, trousers. In these dialects, the term underdrawers is used for undergarments. Many North Americans refer to their undergarments by their type, such as boxers or briefs.		In Australia, men's underwear also has various informal terms including under-dacks, undies, dacks or jocks. In New Zealand men's underwear is known as "undies", or "y-fronts".		Various people in the fashion industry use the words trouser or pant instead of trousers or pants. This is nonstandard usage. The words "trousers" and "pants" are pluralia tantum, nouns that generally only appear in plural form—much like the words "scissors" and "tongs". However, the singular form is used in some compound words, such as trouser-leg, trouser-press and trouser-bottoms.[4]		Jeans are trousers typically made from denim or dungaree cloth. Skin-tight leggings are commonly referred to as tights.		There is some evidence, from figurative art, of trousers being worn in the Upper Paleolithic, as seen on the figurines found at the Siberian sites of Mal'ta and Buret'.[5] The oldest known trousers are found at the Yanghai cemetery in Turpan, Xinjiang, western China, dated to the period between the 13th and the 10th century BC. Made of wool, the trousers had straight legs and wide crotches, and were likely made for horseback riding.[2][3]		Trousers enter recorded history in the 6th century BC, on the rock carvings and artworks of Persepolis,[6] and with the appearance of horse-riding Eurasian nomads in Greek ethnography. At this time, the Iranian People such as Scythians, Sarmatians, Sogdians and Bactrians among others, along with Armenians and Eastern and Central Asian peoples such as the Xiongnu and Hunnu, are known to have worn trousers.[7][8] Trousers are believed to have been worn by both sexes among these early users.[9]		The ancient Greeks used the term "ἀναξυρίδες" (anaxyrides) for the trousers worn by Eastern nations[10] and "σαράβαρα" (sarabara) for the loose trousers worn by the Scythians.[11] However, they did not wear trousers since they thought them ridiculous,[12][13] using the word "θύλακοι" (thulakoi), pl. of "θύλακος" (thulakos), "sack", as a slang term for the loose trousers of Persians and other Middle Easterners.[14]		Republican Rome viewed the draped clothing of Greek and Minoan (Cretan) culture as an emblem of civilisation and disdained trousers as the mark of barbarians.[15] As the Empire expanded beyond the Mediterranean basin, however, the greater warmth provided by trousers led to their adoption.[16] Two types of trousers eventually saw widespread use in Rome: the Feminalia, which fit snugly and usually fell to knee or mid-calf length,[17] and the Braccae, a loose-fitting trouser that was closed at the ankles.[18] Both garments were adopted originally from the Celts of Europe, although later familiarity with the Persian Near East and the Teutons increased acceptance. Feminalia and Braccae both began use as military garments, spreading to civilian dress later, and were eventually made in a variety of materials including leather, wool, cotton and silk.[19]		Trousers of various designs were worn throughout the Middle Ages in Europe, especially by men. Loose-fitting trousers were worn in Byzantium under long tunics,[20] and were worn by many tribes, such as the Germanic tribes that migrated to Western Roman Empire in the Late Antiquity and Early Middle Ages, as evidenced by both artistic sources and such relics as the 4th-century costumes recovered from the Thorsberg peat bog (see illustration).[21] Trousers in this period, generally called brais, varied in length and were often closed at the cuff or even had attached foot coverings, although open-legged pants were also seen.[22]		By the 8th century there is evidence of the wearing in Europe of two layers of trousers, especially among upper-class males.[23] The under layer is today referred to by costume historians as "drawers", although that usage did not emerge until the late 16th century. Over the drawers were worn trousers of wool or linen, which in the 10th century began to be referred to as breeches in many places. Tightness of fit and length of leg varied by period, class, and geography. (Open legged trousers can be seen on the Norman soldiers of the Bayeux Tapestry.)[24]		Although Charlemagne (742–814) is recorded to have habitually worn trousers, donning the Byzantine tunic only for ceremonial occasions,[25][26] the influence of the Roman past and the example of Byzantium led to the increasing use of long tunics by men, hiding most of the trousers from view and eventually rendering them an undergarment for many. As undergarments, these trousers became briefer or longer as the length of the various medieval outer garments changed, and were met by, and usually attached to, another garment variously called hose or stockings.		In the 14th century it became common among the men of the noble and knightly classes to connect the hose directly to their pourpoints[27] (the padded under jacket worn with armoured breastplates that would later evolve into the doublet) rather than to their drawers. In the 15th century, rising hemlines led to ever briefer drawers[28] until they were dispensed with altogether by the most fashionable elites who joined their skin-tight hose back into trousers.[29] These trousers, which we would today call tights but which were still called hose or sometimes joined hose at the time, emerged late in the 15th century and were conspicuous by their open crotch which was covered by an independently fastening front panel, the codpiece. The exposure of the hose to the waist was consistent with 15th-century trends, which also brought the pourpoint/doublet and the shirt, previously undergarments, into view,[30] but the most revealing of these fashions were only ever adopted at court and not by the general population.		Men's clothes in Hungary in the 15th century consisted of a shirt and trousers as underwear, and a dolman worn over them, as well as a short fur-lined or sheepskin coat. Hungarians generally wore simple trousers, only their colour being unusual; the dolman covered the greater part of the trousers.[31]		Around the turn of the 16th century it became conventional to separate hose into two pieces, one from the waist to the crotch which fastened around the top of the legs, called trunk hose, and the other running beneath it to the foot. The trunk hose soon reached down the thigh to fasten below the knee and were now usually called "breeches" to distinguish them from the lower-leg coverings still called hose or, sometimes stockings. By the end of the 16th century, the codpiece had also been incorporated into breeches which featured a fly or fall front opening.		As a modernisation measure, Tsar Peter the Great issued a decree in 1701 commanding every Russian man, other than clergy and peasant farmers, to wear trousers.[32]		During the French Revolution, male citizens of France adopted a working-class costume including ankle-length trousers, or pantaloons (from a Commedia dell'Arte character named Pantalone)[33] in place of the aristocratic knee-breeches. The new garment of the revolutionaries differed from that of the ancien regime upper classes in three ways: it was loose where the style for breeches had most recently been form-fitting, it was ankle length where breeches had generally been knee-length for more than two centuries, and they were open at the bottom while breeches were fastened. Pantaloons became fashionable in early 19th-century England and the Regency era. The style was introduced by Beau Brummell[34][35][36] and by mid-century had supplanted breeches as fashionable street wear.[37] At this point, even knee-length pants adopted the open bottoms of trousers (see shorts) and were worn by young boys, for sports, and in tropical climates. Breeches proper survived into the 20th century as court dress, and also in baggy mid-calf (or three-quarter length) versions known as plus-fours or knickers worn for active sports and by young schoolboys. Types of breeches are still worn today by baseball and American football players.		Sailors may have played a role in the worldwide dissemination of trousers as a fashion. In the 17th and 18th centuries, sailors wore baggy trousers known as galligaskins. Sailors also pioneered the wearing of jeans, trousers made of denim.[citation needed] These became more popular in the late 19th century in the American West because of their ruggedness and durability.		Starting around the mid-19th century, Wigan pit brow girls scandalised Victorian society by wearing trousers for their work at the local coal mines. They wore skirts over their trousers and rolled them up to their waists to keep them out of the way. Although pit brow lasses worked above ground at the pit-head, their task of sorting and shovelling coal involved hard manual labour, so wearing the usual long skirts of the time would have greatly hindered their movements.		The Korean word for trousers, baji (originally pajibaji) first appears in recorded history around the turn of the 15th century, but pants may have been in use by Korean society for some time. From at least this time pants were worn by both sexes in Korea. Men wore trousers either as outer garments or beneath skirts, while it was unusual for adult women to wear their pants (termed sokgot) without a covering skirt. As in Europe, a wide variety of styles came to define regions, time periods and age and gender groups, from the unlined gouei to the padded sombaji.[38]		See also the "Laws" section below in this article.		In Western society, it was Eastern culture that inspired French designer Paul Poiret (1879–1944) to be one of the first to design pants for women. In 1913, Poiret created loose-fitting, wide-leg trousers for women called harem pants, which were based on the costumes of the popular opera Sheherazade. Written by Nikolai Rimsky-Korsakov in 1888, Sheherazade was based on a collection of legends from the Middle East called 1001 Arabian Nights.[39]		In the early 20th century women air pilots and other working women often wore trousers. Frequent photographs from the 1930s of actresses Marlene Dietrich and Katharine Hepburn in trousers helped make trousers acceptable for women. During World War II, women working in factories and doing other forms of "men's work" on war service wore trousers when the work demanded it. In the post-war era, trousers became acceptable casual wear for gardening, the beach, and other leisurely pursuits. Further, in Britain during World War II, because of the rationing of clothing, many women took to wearing their husbands' civilian clothes, including their trousers, to work while their husbands were away from home serving in the armed forces. This was partly because they were seen as practical workwear and partly to allow women to keep their clothing allowance for other uses. As this practice of wearing trousers became more widespread and as the men's clothing wore out, replacements were needed. By the summer of 1944, it was reported that sales of women's trousers were five times more than they had been in the previous year.[40]		In 1919, Luisa Capetillo challenged mainstream society by becoming the first woman in Puerto Rico to wear trousers in public. Capetillo was sent to jail for what was then considered to be a crime, but, the judge later dropped the charges against her.		In the 1960s, André Courrèges introduced long trousers for women as a fashion item, leading to the era of the pantsuit and designer jeans and the gradual erosion of social prohibitions against girls and women wearing trousers in schools, the workplace and in fine restaurants.		In 1969, Rep. Charlotte Reid (R-Ill.) became the first woman to wear trousers in the US Congress.[41]		Pat Nixon was the first American First Lady to wear trousers in public.[42]		In 1989, California state senator Rebecca Morgan became the first woman to wear trousers in a US state senate.[43]		Hillary Clinton was the first woman to wear trousers in an official American First Lady portrait.[44]		In Rome in 1992, a 45-year-old driving instructor was accused of rape. When he picked up an 18-year-old girl for her first driving lesson, he allegedly raped her for an hour, then told her that if she was to tell anyone he would kill her. Later that night she told her parents and her parents agreed to help her press charges. While the alleged rapist was convicted and sentenced, the Italian Supreme Court overturned the conviction in 1998 because the victim wore tight jeans. It was argued that she must have necessarily have had to help her attacker remove her jeans, thus making the act consensual ("because the victim wore very, very tight jeans, she had to help him remove them...and by removing the jeans...it was no longer rape but consensual sex"). The Italian Supreme Court stated in its decision “it is a fact of common experience that it is nearly impossible to slip off tight jeans even partly without the active collaboration of the person who is wearing them.”[45] This ruling sparked widespread feminist protest. The day after the decision, women in the Italian Parliament protested by wearing jeans and holding placards that read “Jeans: An Alibi for Rape.” As a sign of support, the California Senate and Assembly followed suit. Soon Patricia Giggans, Executive Director of the Los Angeles Commission on Assaults Against Women, (now Peace Over Violence) made Denim Day an annual event. As of 2011 at least 20 U.S. states officially recognize Denim Day in April. Wearing jeans on this day has become an international symbol of protest. As of 2008 the Italian Supreme Court has overturned their findings, and there is no longer a "denim" defense to the charge of rape.		Women were not allowed to wear trousers on the US Senate floor until 1993.[46][47] In 1993, Senators Barbara Mikulski and Carol Moseley Braun wore trousers onto the floor in defiance of the rule, and female support staff followed soon after, with the rule being amended later that year by Senate Sergeant-at-Arms Martha Pope to allow women to wear trousers on the floor so long as they also wore a jacket.[46][47]		In Malawi women were not legally allowed to wear trousers under President Kamuzu Banda's rule until 1994.[48] This law was introduced in 1965.[49]		Since 2004 the International Skating Union has allowed women to wear trousers instead of skirts in competition.[50]		In 2009, journalist Lubna Hussein was fined the equivalent of $200 when a court found her guilty of violating Sudan's decency laws by wearing trousers.[51]		In 2012 the Royal Canadian Mounted Police began to allow women to wear trousers and boots with all their formal uniforms.[52]		In 2012 and 2013, some Mormon women participated in "Wear Pants to Church Day", in which they wore trousers to church instead of the customary dresses to encourage gender equality within The Church of Jesus Christ of Latter-day Saints.[53][54] Over one thousand women participated in this in 2012.[54]		In 2013, Turkey's parliament ended a ban on women lawmakers wearing trousers in its assembly.[55]		Also in 2013, an old bylaw requiring women in Paris, France to ask permission from city authorities before "dressing as men", including wearing trousers (with exceptions for those "holding a bicycle handlebar or the reins of a horse") was declared officially revoked by France's Women's Rights Minister, Najat Vallaud-Belkacem.[56] The bylaw was originally intended to prevent women from wearing the pantalons fashionable with Parisian rebels in the French Revolution.[56]		In 2014, an Indian family court in Mumbai ruled that a husband objecting to his wife wearing a kurta and jeans and forcing her to wear a sari amounts to cruelty inflicted by the husband and can be a ground to seek divorce.[57] The wife was thus granted a divorce on the ground of cruelty as defined under section 27(1)(d) of Special Marriage Act, 1954.[57]		Until 2016 some female crew members on British Airways were required to wear British Airways’ standard "ambassador" uniform, which has not traditionally included trousers.[58]		In 2017, The Church of Jesus Christ of Latter-day Saints announced that its female employees could wear "professional pantsuits and dress slacks" while at work; dresses and skirts had previously been required.[59]		Pleats just below the waistband on the front typify many styles of formal and casual trousers, including suit trousers and khakis. There may be one, two, three, or no pleats, which may face either direction. When the pleats open towards the pockets they are called reverse pleats (typical of most trousers today) and when they open toward the fly they are known as forward pleats.		Trouser-makers can finish the legs by hemming the bottom to prevent fraying.[citation needed] Trousers with turn-ups (cuffs in American English), after hemming, are rolled outward and sometimes pressed or stitched into place.		A fly is a covering over an opening join concealing the mechanism, such as a zipper, velcro or buttons, used to join the opening. In trousers, this is most commonly an opening covering the groin, which makes the pants easier to put on or take off. The opening also allows men to urinate without lowering their trousers.		Trousers have varied historically in whether or not they have a fly. Originally, hose did not cover the area between the legs. This was instead covered by a doublet or by a codpiece. When breeches were worn, during the Regency period for example, they were fall-fronted (or broad fall). Later, after trousers (pantaloons) were invented, the fly-front (split fall) emerged.[60] The panelled front returned as a sporting option, such as in riding breeches, but is now hardly ever used, a fly being by far the most common fastening. Most flies now use a zipper, though button-fly pants continue to be available.		At present, most trousers are held up through the assistance of a belt which is passed through the belt loops on the waistband of the trousers. However, this was traditionally a style acceptable only for casual trousers and work trousers; suit trousers and formal trousers were suspended by the use of braces (suspenders in American English) attached to buttons located on the interior or exterior of the waistband. Today, this remains the preferred method of trouser support amongst adherents of classical British tailoring. Many men claim this method is more effective and more comfortable because it requires no cinching of the waist or periodic adjustment.		In modern Western society, males customarily wear trousers and not skirts or dresses. There are exceptions, however, such as the ceremonial Scottish kilt and Greek fustanella, as well as robes or robe-like clothing like the cassocks of clergy and the academic robes, both rarely worn today in daily use. (See also Men's skirts.)		Based on Deuteronomy 22:5 in the Bible ("The woman shall not wear that which pertaineth unto a man"), some groups, including the Amish, Hutterites, some Mennonites, some Baptists, a few Church of Christ groups, and most Orthodox Jews, believe that women should not wear trousers, but only skirts and dresses. These groups do permit women to wear underpants as long as they are hidden. By contrast, many Muslim sects approve of pants as they are considered more modest than any skirt that is shorter than ankle length. However, some mosques require ankle length trousers for both Muslims and non-Muslims on the premises.[61]		Among certain groups, low-rise, baggy trousers exposing underwear became fashionable; for example, among skaters and in 1990s hip hop fashion. This fashion is called sagging or, alternatively, "busting slack."[62]		Cut-offs are homemade shorts made by cutting the legs off trousers, usually after holes have been worn in fabric around the knees. This extends the useful life of the trousers. The remaining leg fabric may be hemmed or left to fray after being cut.		In 2013, a bylaw requiring women in Paris, France to ask permission from city authorities before "dressing as men", including wearing trousers (with exceptions for those "holding a bicycle handlebar or the reins of a horse") was declared officially revoked by France's Women's Rights Minister, Najat Vallaud-Belkacem. [56] The bylaw was originally intended to prevent women from wearing the pantalons fashionable with Parisian rebels in the French Revolution.[56]		In 2014, an Indian family court in Mumbai ruled that a husband objecting to his wife wearing a kurta and jeans and forcing her to wear a sari amounts to cruelty inflicted by the husband and can be a ground to seek divorce.[57] The wife was thus granted a divorce on the ground of cruelty as defined under section 27(1)(d) of Special Marriage Act, 1954.[57]		In Rome in 1992, a 45-year-old driving instructor was accused of rape. When he picked up an 18-year-old girl for her first driving lesson, he allegedly raped her for an hour, then told her that if she was to tell anyone he would kill her. Later that night she told her parents and her parents agreed to help her press charges. While the alleged rapist was convicted and sentenced, the Italian Supreme Court overturned the conviction in 1998 because the victim wore tight jeans. It was argued that she must have necessarily have had to help her attacker remove her jeans, thus making the act consensual ("because the victim wore very, very tight jeans, she had to help him remove them...and by removing the jeans...it was no longer rape but consensual sex"). The Italian Supreme Court stated in its decision “it is a fact of common experience that it is nearly impossible to slip off tight jeans even partly without the active collaboration of the person who is wearing them.”[45] This ruling sparked widespread feminist protest. The day after the decision, women in the Italian Parliament protested by wearing jeans and holding placards that read “Jeans: An Alibi for Rape.” As a sign of support, the California Senate and Assembly followed suit. Soon Patricia Giggans, Executive Director of the Los Angeles Commission on Assaults Against Women, (now Peace Over Violence) made Denim Day an annual event. As of 2011 at least 20 U.S. states officially recognize Denim Day in April. Wearing jeans on this day has become an international symbol of protest. As of 2008 the Italian Supreme Court has overturned their findings, and there is no longer a "denim" defense to the charge of rape.		In Malawi women were not legally allowed to wear trousers under President Kamuzu Banda's rule until 1994.[48] This law was introduced in 1965.[49]		In 1919, Luisa Capetillo challenged mainstream society by becoming the first woman in Puerto Rico to wear trousers in public. Capetillo was sent to jail for what was then considered to be a crime, but, the judge later dropped the charges against her.		In 2013, Turkey's parliament ended a ban on women lawmakers wearing trousers in its assembly.[55]		In Sudan, Article 152 of the Memorandum to the 1991 Penal Code prohibits the wearing of "obscene outfits" in public. This law has been used to arrest and prosecute women wearing trousers. Thirteen women including journalist Lubna al-Hussein were arrested in Khartoum in July 2009 for wearing trousers; ten of the women pleaded guilty and were flogged with ten lashes and fined 250 Sudanese pounds apiece. Lubna al-Hussein considers herself a good Muslim and asserts "Islam does not say whether a woman can wear trousers or not. I'm not afraid of being flogged. It doesn't hurt. But it is insulting." She was eventually found guilty and fined the equivalent of $200 rather than being flogged.[51]		In May 2004, in Louisiana, Democrat and state legislator Derrick Shepherd proposed a bill that would make it a crime to appear in public wearing trousers below the waist and thereby exposing one's skin or "intimate clothing".[63] The Louisiana bill did not pass.		In February 2005, Virginia legislators tried to pass a similar law that would have made punishable by a $50 fine "any person who, while in a public place, intentionally wears and displays his below-waist undergarments, intended to cover a person's intimate parts, in a lewd or indecent manner". (It is not clear whether, with the same coverage by the trousers, exposing underwear was considered worse than exposing bare skin, or whether the latter was already covered by another law.) The law passed in the Virginia House of Delegates. However, various criticisms to it arose. For example, newspaper columnists and radio talk show hosts consistently said that since most people that would be penalised under the law would be young African-American men, the law would thus be a form of racial discrimination. Virginia's state senators voted against passing the law.[64][65]		In California, Government Code Section 12947.5 (part of the California Fair Employment and Housing Act (FEHA)) expressly protects the right to wear pants.[66] Thus, the standard California FEHA discrimination complaint form includes an option for "denied the right to wear pants."[67]		
Theodor Seuss Geisel (/ˈsɔɪs ˈɡaɪzəl/ ( listen); March 2, 1904 – September 24, 1991)[1] was an American author, political cartoonist, poet, animator, book publisher, and artist, best known for authoring children's books under the pen name Dr. Seuss (/suːs/). His work includes several of the most popular children's books of all time, selling over 600 million copies and being translated into more than 20 languages by the time of his death.[2]		Geisel adopted his "Dr. Seuss" pen name during his university studies at Dartmouth College and the University of Oxford. He left Oxford in 1927 to begin his career as an illustrator and cartoonist for Vanity Fair, Life, and various other publications. He also worked as an illustrator for advertising campaigns, most notably for Flit and Standard Oil, and as a political cartoonist for the New York newspaper PM. He published his first children's book And to Think That I Saw It on Mulberry Street in 1937. During World War II, he worked in an animation department of the United States Army where he produced several short films, including Design for Death, which later won the 1947 Academy Award for Documentary Feature.[3]		After the war, Geisel focused on children's books, writing classics such as If I Ran the Zoo (1950), Horton Hears a Who! (1955), If I Ran the Circus (1956), The Cat in the Hat (1957), How the Grinch Stole Christmas! (1957), and Green Eggs and Ham (1960). He published over 60 books during his career, which have spawned numerous adaptations, including 11 television specials, four feature films, a Broadway musical, and four television series. He won the Lewis Carroll Shelf Award in 1958 for Horton Hatches the Egg and again in 1961 for And to Think That I Saw It on Mulberry Street. Geisel's birthday, March 2, has been adopted as the annual date for National Read Across America Day, an initiative on reading created by the National Education Association.						Geisel was born and raised in Springfield, Massachusetts, the son of Henrietta (née Seuss) and Theodor Robert Geisel.[4][5] All four of his grandparents were German immigrants.[6] His father managed the family brewery and was later appointed to supervise Springfield's public park system by Mayor John A. Denison[7] after the brewery closed because of Prohibition.[8] Mulberry Street in Springfield, made famous in Dr. Seuss' first children's book And to Think That I Saw It on Mulberry Street, is less than a mile southwest of his boyhood home on Fairfield Street. Geisel was raised a Lutheran.[9] He enrolled at Springfield Central High School in 1917 and graduated in 1921. He took an art class as a freshman and later became manager of the school soccer team.[10]		Geisel attended Dartmouth College, graduating in 1925.[11] At Dartmouth, he joined the Sigma Phi Epsilon fraternity[4] and the humor magazine Dartmouth Jack-O-Lantern, eventually rising to the rank of editor-in-chief.[4] While at Dartmouth, he was caught drinking gin with nine friends in his room.[12] At the time, the possession and consumption of alcohol was illegal under Prohibition laws, which remained in place between 1920 and 1933. As a result of this infraction, Dean Craven Laycock insisted that Geisel resign from all extracurricular activities, including the college humor magazine.[13] To continue work on the Jack-O-Lantern without the administration's knowledge, Geisel began signing his work with the pen name "Seuss". He was encouraged in his writing by professor of rhetoric W. Benfield Pressey, whom he described as his "big inspiration for writing" at Dartmouth.[14]		Upon graduating from Dartmouth, he entered Lincoln College, Oxford intending to earn a PhD in English literature.[15] At Oxford, he met Helen Palmer, who encouraged him to give up becoming an English teacher in favor of pursuing drawing as a career.[15]		Geisel left Oxford without earning a degree and returned to the United States in February 1927,[16] where he immediately began submitting writings and drawings to magazines, book publishers, and advertising agencies.[17] Making use of his time in Europe, he pitched a series of cartoons called Eminent Europeans to Life magazine, but the magazine passed on it. His first nationally published cartoon appeared in the July 16, 1927 issue of The Saturday Evening Post.[18] This single $25 sale encouraged Geisel to move from Springfield to New York City.		Later that year, Geisel accepted a job as writer and illustrator at the humor magazine Judge, and he felt financially stable enough to marry Helen.[19] His first cartoon for Judge appeared on October 22, 1927, and the Geisels were married on November 29. Geisel's first work signed "Dr. Seuss" was published in Judge about six months after he started working there.[20]		In early 1928, one of Geisel's cartoons for Judge mentioned Flit, a common bug spray at the time manufactured by Standard Oil of New Jersey.[21] According to Geisel, the wife of an advertising executive in charge of advertising Flit saw Geisel's cartoon at a hairdresser's and urged her husband to sign him.[22] Geisel's first Flit ad appeared on May 31, 1928, and the campaign continued sporadically until 1941.[23] The campaign's catchphrase "Quick, Henry, the Flit!" became a part of popular culture. It spawned a song and was used as a punch line for comedians such as Fred Allen and Jack Benny. As Geisel gained notoriety for the Flit campaign, his work was sought after and began to appear regularly in magazines such as Life, Liberty, and Vanity Fair.		Geisel supported himself and his wife through the Great Depression by drawing advertising for General Electric, NBC, Standard Oil, Narragansett Brewing Company, and many other companies. In 1935, he wrote and drew a short-lived comic strip called Hejji.[24]		The increased income allowed the Geisels to move to better quarters and to socialize in higher social circles.[25] They became friends with the wealthy family of banker Frank A. Vanderlip. They also traveled extensively: by 1936, Geisel and his wife had visited 30 countries together. They did not have children, neither kept regular office hours, and they had ample money.[26] Geisel also felt that the traveling helped his creativity.		In 1936, the couple were returning from an ocean voyage to Europe when the rhythm of the ship's engines inspired the poem that became his first book: And to Think That I Saw It on Mulberry Street.[27] Based on Geisel's varied accounts, the book was rejected by between 20 and 43 publishers.[28][29] According to Geisel, he was walking home to burn the manuscript when a chance encounter with an old Dartmouth classmate led to its publication by Vanguard Press.[30] Geisel wrote four more books before the US entered World War II. This included The 500 Hats of Bartholomew Cubbins in 1938, as well as The King's Stilts and The Seven Lady Godivas in 1939, all of which were in prose, atypically for him. This was followed by Horton Hatches the Egg in 1940, in which Geisel returned to the use of poetry.		Geisel gained a significant public profile through a program for motor boat lubricants produced by Standard Oil under the brand name Essomarine[31] He later recounted that Harry Bruno, Ted Cook, and Verne Carrier worked with him at the National Motor Boat Show on exhibits referred to as the Seuss Navy.[32]		In 1934, Geisel produced a 30-page booklet titled Secrets of the Deep which was available by mail after June. At the January boat show for 1935, visitors filled out order cards to receive Secrets. Geisel drew up a Certificate of Commission for visitors in 1936. A mock ship deck called SS Essomarine provided the scene where photos of "Admirals" were taken. That summer, Geisel released a second volume of Secrets. For the 1937 show, he sculpted Marine Muggs and designed a flag for the Seuss Navy.[citation needed]		The following year featured "Little Dramas of the Deep", a six-act play with ten characters. According to Geisel's sister, "He plans the whole show with scenery and action and then, standing in a realistic bridge, reels off a speech which combines advertising with humor." For 1939, exhibitors made available the Nuzzlepuss ashtray and illustrated tide-table calendars.		A Seuss Navy Luncheon was held on January 11, 1940 at the Waldorf-Astoria Hotel. At that year's boat show, Geisel provided the Navigamarama exhibit and the Sea Lawyers Gazette.		The final contribution to the Essomarine project was the mermaid Essie Neptune and her pet whale in 1941. The exhibit offered photos for a Happy Cruising passport.[33]		As World War II began, Geisel turned to political cartoons, drawing over 400 in two years as editorial cartoonist for the left-leaning New York City daily newspaper, PM.[34] Geisel's political cartoons, later published in Dr. Seuss Goes to War, denounced Hitler and Mussolini and were highly critical of non-interventionists ("isolationists"), most notably Charles Lindbergh, who opposed US entry into the war.[35] One cartoon[36] depicted all Japanese Americans as latent traitors or fifth-columnists, while other cartoons simultaneously deplored the racism at home against Jews and blacks that harmed the war effort.[citation needed] His cartoons were strongly supportive of President Roosevelt's handling of the war, combining the usual exhortations to ration and contribute to the war effort with frequent attacks on Congress[37] (especially the Republican Party),[38] parts of the press (such as the New York Daily News, Chicago Tribune, and Washington Times-Herald),[39] and others for criticism of Roosevelt, criticism of aid to the Soviet Union,[40][41] investigation of suspected Communists,[42] and other offences that he depicted as leading to disunity and helping the Nazis, intentionally or inadvertently.		In 1942, Geisel turned his energies to direct support of the U.S. war effort. First, he worked drawing posters for the Treasury Department and the War Production Board. Then, in 1943, he joined the Army as a Captain and was commander of the Animation Department of the First Motion Picture Unit of the United States Army Air Forces, where he wrote films that included Your Job in Germany, a 1945 propaganda film about peace in Europe after World War II; Our Job in Japan; and the Private Snafu series of adult army training films. While in the Army, he was awarded the Legion of Merit.[43] Our Job in Japan became the basis for the commercially released film Design for Death (1947), a study of Japanese culture that won the Academy Award for Documentary Feature.[44] Gerald McBoing-Boing (1950) was based on an original story by Seuss and won the Academy Award for Animated Short Film.[45]		After the war, Geisel and his wife moved to La Jolla, California where he returned to writing children's books. He wrote many, including such favorites as If I Ran the Zoo (1950), Horton Hears a Who! (1955), If I Ran the Circus (1956), The Cat in the Hat (1957), How the Grinch Stole Christmas! (1957), and Green Eggs and Ham (1960). He received numerous awards throughout his career, but he won neither the Caldecott Medal nor the Newbery Medal. Three of his titles from this period were, however, chosen as Caldecott runners-up (now referred to as Caldecott Honor books): McElligot's Pool (1947), Bartholomew and the Oobleck (1949), and If I Ran the Zoo (1950). Dr Seuss also wrote the musical and fantasy film The 5,000 Fingers of Dr. T, which was released in 1953. The movie was a critical and financial failure, and Geisel never attempted another feature film. During the 1950s, he also published a number of illustrated short stories, mostly in Redbook Magazine. Some of these were later collected (in volumes such as The Sneetches and Other Stories) or reworked into independent books (If I Ran the Zoo). A number have never been reprinted since their original appearances.		In May 1954, Life magazine published a report on illiteracy among school children which concluded that children were not learning to read because their books were boring. William Ellsworth Spaulding was the director of the education division at Houghton Mifflin (he later became its chairman), and he compiled a list of 348 words that he felt were important for first-graders to recognize. He asked Geisel to cut the list to 250 words and to write a book using only those words.[46] Spaulding challenged Geisel to "bring back a book children can't put down".[47] Nine months later, Geisel completed The Cat in the Hat, using 236 of the words given to him. It retained the drawing style, verse rhythms, and all the imaginative power of Geisel's earlier works but, because of its simplified vocabulary, it could be read by beginning readers. The Cat in the Hat and subsequent books written for young children achieved significant international success and they remain very popular today. In 2009, Green Eggs and Ham sold 540,366 copies, The Cat in the Hat sold 452,258 copies, and One Fish, Two Fish, Red Fish, Blue Fish (1960) sold 409,068 copies—outselling the majority of newly published children's books.[48]		Geisel went on to write many other children's books, both in his new simplified-vocabulary manner (sold as Beginner Books) and in his older, more elaborate style.		In 1956, Dartmouth awarded Geisel with an honorary doctorate, finally justifying the "Dr." in his pen name.		On April 28, 1958, Geisel appeared on an episode of the panel game show To Tell the Truth.[49]		Geisel's wife Helen had a long struggle with illnesses, including cancer and emotional pain over Geisel's affair with Audrey Stone Dimond. On October 23, 1967, Helen committed suicide; Geisel married Dimond on June 21, 1968.[50] Though he devoted most of his life to writing children's books, Geisel had no children of his own, saying of children: "You have 'em; I'll entertain 'em."[50] Dimond added that Geisel "lived his whole life without children and he was very happy without children."[50]		Geisel received the Laura Ingalls Wilder Medal from the professional children's librarians in 1980, recognizing his "substantial and lasting contributions to children's literature". At the time, it was awarded every five years.[51] He won a special Pulitzer Prize in 1984 citing his "contribution over nearly half a century to the education and enjoyment of America's children and their parents".[52]		Geisel died of oral cancer on September 24, 1991 at his home in La Jolla at the age of 87.[53][54] He was cremated and his ashes were scattered. On December 1, 1995, four years after his death, University of California, San Diego's University Library Building was renamed Geisel Library in honor of Geisel and Audrey for the generous contributions that they made to the library and their devotion to improving literacy.[55]		While Geisel was living in La Jolla, the United States Postal Service and others frequently confused him with fellow La Jolla resident Dr. Hans Suess. Their names have been linked together posthumously: the personal papers of Hans Suess are housed in the Geisel Library.[56]		In 2002, the Dr. Seuss National Memorial Sculpture Garden opened in his birthplace of Springfield, Massachusetts, featuring sculptures of Geisel and of many of his characters. On May 28, 2008, California Governor Arnold Schwarzenegger and First Lady Maria Shriver announced that Geisel would be inducted into the California Hall of Fame located at The California Museum for History, Women and the Arts. The induction ceremony took place December 15 and Geisel's widow Audrey accepted the honor in his place. On March 2, 2009, the web search engine Google temporarily changed its logo to commemorate Geisel's birthday (a practice that it often follows for various holidays and events).[57]		In 2004, U.S. children's librarians established the annual Theodor Seuss Geisel Award to recognize "the most distinguished American book for beginning readers published in English in the United States during the preceding year". It should "demonstrate creativity and imagination to engage children in reading" during years pre-K to grade two.[58]		At Geisel's alma mater of Dartmouth, more than 90 percent of incoming first-year students participate in pre-registration Dartmouth Outing Club trips into the New Hampshire wilderness. It is traditional for students returning from the trips to stay overnight at Dartmouth's Moosilauke Ravine Lodge, where they are served green eggs and ham for breakfast in honor of Dr. Seuss. On April 4, 2012, the Dartmouth Medical School renamed itself the Audrey and Theodor Geisel School of Medicine in honor of their many years of generosity to the college.[59]		Dr. Seuss's honors include two Academy awards, two Emmy awards, a Peabody award, the Laura Ingalls Wilder Medal, and the Pulitzer Prize.		Dr. Seuss has a star on the Hollywood Walk of Fame at the 6500 block of Hollywood Boulevard.[60]		Geisel's most famous pen name is regularly pronounced /ˈsuːs/, an anglicized pronunciation inconsistent with his German surname (the standard German pronunciation is [ˈzɔʏ̯s]). He himself noted that it rhymed with "voice" (his own pronunciation being /ˈsɔɪs/). Alexander Laing, one of his collaborators on the Dartmouth Jack-O-Lantern,[61] wrote of it:		You're wrong as the deuce And you shouldn't rejoice If you're calling him Seuss. He pronounces it Soice[62] (or Zoice)[63]		Geisel switched to the anglicized pronunciation because it "evoked a figure advantageous for an author of children's books to be associated with—Mother Goose"[47] and because most people used this pronunciation. He added the "Dr." to his pen name because his father had always wanted him to practice medicine.[64]		For books that Geisel wrote and others illustrated, he used the pen name "Theo LeSieg", starting with I Wish That I Had Duck Feet published in 1965. "LeSieg" is "Geisel" spelled backward.[65] Geisel also published one book under the name Rosetta Stone, 1975's Because a Little Bug Went Ka-Choo!!, a collaboration with Michael K. Frith. Frith and Geisel chose the name in honor of Geisel's second wife Audrey, whose maiden name was Stone.[66]		Geisel was a liberal Democrat and a supporter of President Franklin D. Roosevelt and the New Deal. His early political cartoons show a passionate opposition to fascism, and he urged action against it both before and after the United States entered World War II. His cartoons portrayed the fear of communism as overstated, finding greater threats in the House Un-American Activities Committee and those who threatened to cut the US "life line"[41] to Stalin and the USSR, whom he once depicted as a porter carrying "our war load".[40]		Geisel supported the Japanese American internment during World War II. His treatment of the Japanese and of Japanese Americans (between whom he often failed to differentiate) has struck many readers as a moral blind spot.[67] On the issue of the Japanese, he is quoted as saying:		But right now, when the Japs are planting their hatchets in our skulls, it seems like a hell of a time for us to smile and warble: "Brothers!" It is a rather flabby battle cry. If we want to win, we've got to kill Japs, whether it depresses John Haynes Holmes or not. We can get palsy-walsy afterward with those that are left.[68]		After the war, though, Geisel overcame his feelings of animosity, using his book Horton Hears a Who! (1954) as an allegory for the Hiroshima bombing and the American post-war occupation of Japan, as well as dedicating the book to a Japanese friend.[69]		In 1948, after living and working in Hollywood for years, Geisel moved to La Jolla, California, a predominantly Republican town.[70]		Geisel converted a copy of one of his famous children's books into a polemic shortly before the end of the 1972–74 Watergate scandal, in which United States president Richard Nixon resigned, by replacing the name of the main character everywhere that it occurred.[71] "Richard M. Nixon, Will You Please Go Now!" was published in major newspapers through the column of his friend Art Buchwald.[71]		The line "a person's a person, no matter how small!!" from Horton Hears a Who! has been used widely as a slogan by the pro-life movement in the U.S., despite the objections of Geisel's widow. The line was first used in such a way in 1986; he demanded a retraction and received one.[72]		Geisel made a point of not beginning to write his stories with a moral in mind, stating that "kids can see a moral coming a mile off." He was not against writing about issues, however; he said that "there's an inherent moral in any story",[73] and he remarked that he was "subversive as hell."[74]		Many of Geisel's books express his views on a remarkable variety of social and political issues: The Lorax (1971), about environmentalism and anti-consumerism; "The Sneetches" (1961), about racial equality; The Butter Battle Book (1984), about the arms race; Yertle the Turtle (1958), about Hitler and anti-authoritarianism; How the Grinch Stole Christmas! (1957), criticizing the materialism and consumerism of the Christmas season; and Horton Hears a Who! (1954), about anti-isolationism and internationalism.[47][69]		Geisel wrote most of his books in anapestic tetrameter, a poetic meter employed by many poets of the English literary canon. This is often suggested as one of the reasons that Geisel's writing was so well received.[75][76]		Anapestic tetrameter consists of four rhythmic units called anapests, each composed of two weak syllables followed by one strong syllable (the beat); often, the first weak syllable is omitted, or an additional weak syllable is added at the end. An example of this meter can be found in Geisel's "Yertle the Turtle", from Yertle the Turtle and Other Stories:		And today the Great Yertle, that Marvelous he Is King of the Mud. That is all he can see.[77]		Some books by Geisel that are written mainly in anapestic tetrameter also contain many lines written in amphibrachic tetrameter, such as these from If I Ran the Circus:		All ready to put up the tents for my circus. I think I will call it the Circus McGurkus.		And NOW comes an act of Enormous Enormance! No former performer's performed this performance!		Geisel also wrote verse in trochaic tetrameter, an arrangement of a strong syllable followed by a weak syllable, with four units per line (for example, the title of One Fish Two Fish Red Fish Blue Fish). Traditionally, English trochaic meter permits the final weak position in the line to be omitted, which allows both masculine and feminine rhymes.		Geisel generally maintained trochaic meter for only brief passages, and for longer stretches typically mixed it with iambic tetrameter, which consists of a weak syllable followed by a strong, and is generally considered easier to write. Thus, for example, the magicians in Bartholomew and the Oobleck make their first appearance chanting in trochees (thus resembling the witches of Shakespeare's Macbeth):		Shuffle, duffle, muzzle, muff		They then switch to iambs for the oobleck spell:		Go make the Oobleck tumble down On every street, in every town![78]		Geisel's early artwork often employed the shaded texture of pencil drawings or watercolors, but in his children's books of the postwar period, he generally made use of a starker medium—pen and ink—normally using just black, white, and one or two colors. His later books, such as The Lorax, used more colors.		Geisel's style was unique – his figures are often "rounded" and somewhat droopy. This is true, for instance, of the faces of the Grinch and the Cat in the Hat. Almost all his buildings and machinery were devoid of straight lines when they were drawn, even when he was representing real objects. For example, If I Ran the Circus shows a droopy hoisting crane and a droopy steam calliope.		Geisel evidently enjoyed drawing architecturally elaborate objects. His endlessly varied but never rectilinear palaces, ramps, platforms, and free-standing stairways are among his most evocative creations. Geisel also drew complex imaginary machines, such as the Audio-Telly-O-Tally-O-Count, from Dr. Seuss's Sleep Book, or the "most peculiar machine" of Sylvester McMonkey McBean in The Sneetches. Geisel also liked drawing outlandish arrangements of feathers or fur: for example, the 500th hat of Bartholomew Cubbins, the tail of Gertrude McFuzz, and the pet for girls who like to brush and comb, in One Fish Two Fish.		Geisel's illustrations often convey motion vividly. He was fond of a sort of "voilà" gesture in which the hand flips outward and the fingers spread slightly backward with the thumb up. This motion is done by Ish in One Fish Two Fish when he creates fish (who perform the gesture themselves with their fins), in the introduction of the various acts of If I Ran the Circus, and in the introduction of the "Little Cats" in The Cat in the Hat Comes Back. He was also fond of drawing hands with interlocked fingers, making it look as though his characters were twiddling their thumbs.		Geisel also follows the cartoon tradition of showing motion with lines, like in the sweeping lines that accompany Sneelock's final dive in If I Ran the Circus. Cartoon lines are also used to illustrate the action of the senses—sight, smell, and hearing—in The Big Brag, and lines even illustrate "thought", as in the moment when the Grinch conceives his awful plan to ruin Christmas.		Geisel's early work in advertising and editorial cartooning helped him to produce "sketches" of things that received more perfect realization later in his children's books. Often, the expressive use to which Geisel put an image later on was quite different from the original.[79] Here are some examples:		Geisel wrote more than 60 books over the course of his long career. Most were published under his well-known pseudonym Dr. Seuss, though he also authored more than a dozen books as Theo LeSieg and one as Rosetta Stone. His books have topped many bestseller lists, sold over 600 million copies, and been translated into more than 20 languages.[2] In 2000, Publishers Weekly compiled a list of the best-selling children's books of all time; of the top 100 hardcover books, 16 were written by Geisel, including Green Eggs and Ham, at number 4, The Cat in the Hat, at number 9, and One Fish Two Fish Red Fish Blue Fish, at number 13.[88] In the years after his death in 1991, two additional books were published based on his sketches and notes: Hooray for Diffendoofer Day! and Daisy-Head Mayzie. My Many Colored Days was originally written in 1973 but was posthumously published in 1996. In September 2011, seven stories originally published in magazines during the 1950s were released in a collection titled The Bippolo Seed and Other Lost Stories.[89]		Geisel also wrote a pair of books for adults: The Seven Lady Godivas (1939; reprinted 1987), a retelling of the Lady Godiva legend that included nude depictions; and You're Only Old Once! (written in 1986 when Geisel was 82), which chronicles an old man's journey through a clinic. His last book was Oh, the Places You'll Go!, which published the year before his death and became a popular gift for graduating students.[90]		For most of his career, Geisel was reluctant to have his characters marketed in contexts outside of his own books. However, he did permit the creation of several animated cartoons, an art form in which he himself had gained experience during the Second World War, and he gradually relaxed his policy as he aged.		The first adaptation of one of Geisel's works was a cartoon version of Horton Hatches the Egg, animated at Warner Bros. in 1942 and directed by Robert Clampett. It was presented as part of the Merrie Melodies series and included a number of gags not present in the original narrative, including a fish committing suicide and a Katharine Hepburn imitation by Mayzie.		As part of the Puppetoon theatrical cartoon series for Paramount Pictures, two of Geisel's works were adapted into stop-motion films by George Pal. The first, "The 500 Hats of Bartholomew Cubbins", was released in 1943[91] and nominated for an Academy Award for "Short Subject (Cartoon)" the following year.[92] The second, "And to Think I Saw It On Mulberry Street", with a title slightly altered from the book's, was released in 1944.[93]		In 1959, Geisel authorized Revell, the well-known plastic model-making company, to make a series of "animals" that snapped together rather than being glued together, and could be assembled, disassembled, and re-assembled "in thousands" of ways. The series was called the "Dr. Seuss Zoo" and included Gowdy the Dowdy Grackle, Norval the Bashful Blinket, Tingo the Noodle Topped Stroodle, and Roscoe the Many Footed Lion. The basic body parts were the same and all were interchangeable, and so it was possible for children to combine parts from various characters in essentially unlimited ways in creating their own animal characters (Revell encouraged this by selling Gowdy, Norval, and Tingo together in a "Gift Set" as well as individually). Revell also made a conventional glue-together "beginner's kit" of The Cat in the Hat.		In 1966, Geisel authorized eminent cartoon artist Chuck Jones – his friend and former colleague from the war – to make a cartoon version of How the Grinch Stole Christmas!. Geisel was credited as a co-producer under his real name Ted Geisel, along with Jones. The cartoon was narrated by Boris Karloff, who also provided the voice of the Grinch. It was very faithful to the original book, and is considered a classic to this day by many. It is often broadcast as an annual Christmas television special. Jones directed an adaptation of Horton Hears a Who! in 1970 and produced an adaptation of The Cat in the Hat in 1971.		From 1972 to 1983, Geisel wrote six animated specials which were produced by DePatie-Freleng: The Lorax (1972); Dr. Seuss on the Loose (1973); The Hoober-Bloob Highway (1975); Halloween Is Grinch Night (1977); Pontoffel Pock, Where Are You? (1980); and The Grinch Grinches the Cat in the Hat (1982). Several of the specials won multiple Emmy Awards.		A Soviet paint-on-glass-animated short film was made in 1986 called Welcome, an adaptation of Thidwick the Big-Hearted Moose. The last adaptation of Geisel's work before he died was The Butter Battle Book, a television special based on the book of the same name, directed by adult animation legend Ralph Bakshi.		A television film titled In Search of Dr. Seuss was released in 1994 which adapted many of Seuss's stories. It uses both live-action versions and animated versions of the characters and stories featured; however, the animated portions were merely edited versions of previous animated television specials and, in some cases, re-dubbed as well.		After Geisel died of cancer at the age of 87 in 1991, his widow Audrey Geisel was placed in charge of all licensing matters. She approved a live-action feature-film version of How the Grinch Stole Christmas starring Jim Carrey, as well as a Seuss-themed Broadway musical called Seussical, and both premiered in 2000. The Grinch has had limited engagement runs on Broadway during the Christmas season, after premiering in 1998 (under the title How the Grinch Stole Christmas) at the Old Globe Theatre in San Diego, where it has become a Christmas tradition. In 2003, another live-action film was released, this time an adaptation of The Cat in the Hat that featured Mike Myers as the title character. Audrey Geisel has spoken critically of the film, especially the casting of Myers as the Cat in the Hat, and stated that she would not allow any further live-action adaptations of Geisel's books.[94] However, an animated CGI feature film adaptation of Horton Hears a Who! was approved, and was eventually released on March 14, 2008, to critical acclaim. A CGI-animated feature film adaptation of The Lorax was released by Universal on March 2, 2012 (on what would have been Seuss's 108th birthday).		Four television series have been adapted from Geisel's work. The first, Gerald McBoing-Boing, was an animated television adaptation of Geisel's 1951 cartoon of the same name and lasted three months between 1956 and 1957. The second, The Wubbulous World of Dr. Seuss, was a mix of live-action and puppetry by Jim Henson Television, the producers of The Muppets. It aired for one season on Nickelodeon in the United States, from 1996 to 1997. The third, Gerald McBoing-Boing, is a remake of the 1956 series.[95] Produced in Canada by Cookie Jar Entertainment (now DHX Media) and North America by Classic Media (now DreamWorks Classics), it ran from 2005 to 2007. The fourth, The Cat in the Hat Knows a Lot About That!, produced by Portfolio Entertainment Inc., began on August 7, 2010, in Canada and September 6, 2010, in the United States and is currently still showing.[when?]		Geisel's books and characters are also featured in Seuss Landing, one of many islands at the Islands of Adventure theme park in Orlando, Florida. In an attempt to match Geisel's visual style, there are reportedly "no straight lines" in Seuss Landing.[96]		The Hollywood Reporter has reported that Johnny Depp has agreed to produce and possibly star in a film based on Geisel's life. The film will be written by Keith Bunin, produced by Depp's Infinitum Nihil production company alongside Illumination Entertainment and distributed by Universal Pictures.[97]		
The 40-Year-Old Virgin is a 2005 American sex comedy film written, produced and directed by Judd Apatow, about a middle-aged man's journey to finally have sex. It was co-written by Steve Carell, although it features a great deal of improvised dialogue.[2] The film was released theatrically in North America on August 19, 2005, and was released on region 1 DVD on December 13, 2005.[3]		It is the directorial debut of Judd Apatow, who has since directed successful films such as Knocked Up, Funny People, This Is 40 and Trainwreck,[4] and it is Seth Rogen's first role in a comedy film since Anchorman: The Legend of Ron Burgundy (2004).						Andy Stitzer (Steve Carell) is a 40-year-old virgin who lives alone, his apartment filled with his collection of action figures and video games. At a poker game with his co-workers David (Paul Rudd), Cal (Seth Rogen), Mooj (Gerry Bednob), and Jay (Romany Malco), when conversation turns to past sexual exploits, the group learns that Andy is still a virgin, and resolve to help him lose his virginity.		The men give Andy different and sometimes contradictory advice, both on his appearance and how to interact with women. Cal advises Andy to simply "ask questions," which he practices on bookstore clerk Beth (Elizabeth Banks), who quickly becomes intrigued by him. David gives Andy his porn collection, encouraging him to masturbate. Mooj admonishes Andy that children are what give life meaning.		Andy begins to socialize and form friendships with his co-workers. David, after running into his ex-girlfriend Amy, has an emotional breakdown at work. Store manager Paula (Jane Lynch) promotes Andy to fill in for him.		Jay attempts to quicken the process by tricking Andy into meeting a prostitute. When Andy discovers the hooker is a male transvestite, he insists that his friends stop trying to help him. Andy lands a date with Trish Piedmont (Catherine Keener), a woman he met on the sales floor.		During Andy and Trish's first date, as they are about to have sex, they are interrupted by Trish's teenage daughter Marla (Kat Dennings). Trish suggests that they postpone having sex, to which Andy enthusiastically agrees.		Andy's friends begin to encounter the consequences of their lifestyles. David, obsessed with Amy, takes a vow of celibacy. Jay, who previously boasted of his promiscuity, gets into an argument with a customer after his girlfriend breaks up with him over his infidelity. Jay concedes to Andy that sex can ruin a relationship.		Andy and Trish's relationship grows. Trish encourages Andy's dream of starting a business, suggesting that fund it by selling his collectibles. Andy takes Marla to a group session at a sexual health clinic, where she reveals she is a virgin. Andy, trying to defend her against derision, admits that he is also a virgin but is disbelieved and ridiculed. Marla later says that she knows Andy is a virgin, but agrees to let him tell Trish himself.		On the couple's 20th date, the limit they agreed for their abstinence, Andy is still resistant, which upsets Trish. Trish demands he explain his reticence, and Andy accuses her of trying to change him against his will. He leaves for a nightclub where he meets his friends, gets drunk and praises them for encouraging him to have sex. Andy runs into Beth and the pair soon leave for her apartment. Marla convinces Trish to make up with Andy. By this time Andy has sobered up and is having second thoughts. His friends arrive and encourage him to go back to Trish.		Andy returns to his apartment, where he finds Trish waiting for him. He attempts to apologize, but Trish, having found various suspicious items in his apartment, is now afraid that Andy may be some sort of sexual deviant. Andy tries to defend himself and declares his love for her, but she leaves in alarm and disgust. Andy chases after her on his bike, but collides with her car and flies through the side of a truck. Trish rushes to his side, and Andy finally confesses that he is a virgin as explanation for his behavior. Trish is surprised but relieved, and they kiss.		Later, Andy and Trish are married in a lavish ceremony with everyone in attendance, with a sidelong mention that Andy's action figures sold for over half a million dollars. Afterwards, they consummate the marriage, the aftermath of which transitions into a musical scene where the characters sing and dance to "Aquarius/Let the Sunshine In".		Production on the film was nearly halted by Universal Pictures after five days of filming, allegedly due to concerns that the appearance of Carell's character resembled that of a serial killer.[5] Production was started on January 17, 2005, and wrapped on April 1, 2005.		The production used over a million feet of film, a milestone reached on the last day of filming and recognized with free champagne by Technicolor.[6] Using the conversion of 90 feet of film per minute, this means that the shooting ratio for the film is 96:1 for the theatrical (84:1 for the unrated version).		On Rotten Tomatoes, a review aggregator, the film has an approval rating of 85% based on 181 reviews and an average rating of 7.2/10. The site's critical consensus reads, "Steve Carell's first star turn scores big with a tender treatment of its titular underdog, using raunchy but realistically funny comedy to connect with adult audiences."[7] On Metacritic, the film has a score of 73 out of 100 based on 35 critics, indicating "generally favorable reviews".[8]		Audiences polled by CinemaScore gave the film an average grade of "A-" on an A+ to F scale.[9] Rotten Tomatoes declared it the "Best Reviewed Comedy of 2005."[10]		Ebert and Roeper gave the film a "two thumbs up" rating. Roger Ebert said, "I was surprised by how funny, how sweet, and how wise the movie really is" and "the more you think about it, the better The 40-Year-Old Virgin gets."[11] The pair gave minor criticisms, with Ebert describing "the way she (Catherine Keener as 'Trish') empathizes with Andy" as "almost too sweet to be funny" and Richard Roeper saying that the film was too long, and at times extremely frustrating.[11] Roeper later chose the film as the tenth best of 2005.[12]		Owen Gleiberman of Entertainment Weekly gave the movie an A-, saying that Carell "plays him [Andy] in the funniest and most surprising way possible: as a credible human being." Manohla Dargis of The New York Times called the film a "charmingly bent comedy," noting that Carell conveys a "sheer likability" and a "range as an actor" that was "crucial to making this film work as well as it does."[13]		The film was criticized by Harry Forbes of Catholic News Service for promoting "the false premise that there's something intrinsically wrong with an unmarried man being sexually inexperienced,"[14] and by conservative columnist Cal Thomas for not being a "tribute to self-control or purity."[15]		In December 2005, the film was chosen by the American Film Institute as one of the ten best movies of the year, the only comedy film to be so recognized (though the comedy-drama The Squid and the Whale was also chosen). The film was also ranked No. 30 on Bravo's 100 Funniest Movies.		The film was a summer hit, and opened at No. 1 at the box office, grossing $21,422,815 during its opening weekend, and stayed at No. 1 the following weekend. The film grossed a total of $109,449,237 at the domestic market, and $67,929,408 overseas, for a total of $177,378,645. The film was 25th in global gross, and 19th in the United States that year.[1] The film was released in the United Kingdom on September 2, 2005, and topped the country's box office that weekend.[16]		The film is recognized by American Film Institute in these lists:		The Film won the award for Best Comedy at The 11th Critic's Choice Movie Awards Ceremony, being the first movie to take the prize home.		On home video the film was released with an additional 17 minutes under the banner "unrated".[18] For the 100th Anniversary of Universal the theatrical edition was released on Blu-ray. This version also had a similar banner of "unrated".		The American Humane Association withheld its "no animals were harmed..." disclaimer due to the accidental deaths of several tropical fish used in the film.[19]		
Sir Christopher John Frayling (born 25 December 1946) is a British educationalist and writer, known for his study of popular culture.						Christopher Frayling was born in Hampton, a suburb of London,[1] in affluent circumstances.[2] After attending Repton School,[3] Frayling read history at Churchill College, Cambridge and gained a PhD in the study of Jean-Jacques Rousseau. He was appointed a Fellow of the college in 2009. He taught history at the University of Bath and was awarded an Honorary Degree (Doctor of Arts) from that University in 2003. In 1979 Frayling was appointed Professor of Cultural History at London's post-graduate art and design school, the Royal College of Art. Frayling was Rector in charge of the Royal College of Art from 1996 to 2009.[4]		In 2003 he was awarded the Sir Misha Black Award and was added to the College of Medallists.[5]		He was the Chairman of Arts Council England from 2005 until January 2009.[6] He also served as Chairman of the Design Council, Chairman of the Royal Mint Advisory Committee, and a Trustee of the Victoria and Albert Museum. He was a governor of the British Film Institute in the 1980s.[citation needed] In April 2014 he was appointed Chancellor of the Arts University Bournemouth.[7]		He has had a wide output as a writer and critic on subjects ranging from vampires to westerns. He has written and presented television series such as The Art of Persuasion on advertising and Strange Landscape on the Middle Ages. He has conducted a series of radio and television interviews with figures from the world of film, including Woody Allen, Deborah Kerr, Ken Adam, Francis Ford Coppola and Clint Eastwood. He has written and presented several television series, including The Face of Tutankhamun and Nightmare: Birth of Horror.[citation needed]		He studied spaghetti westerns and specifically director Sergio Leone. He has written a very popular biography of Leone, Something To Do With Death (2000); helped run the Los Angeles-based Gene Autry Museum's exhibit on Leone in 2005; and appeared in numerous documentaries about Leone and his films, particularly the DVD documentaries of Once Upon a Time in the West (1968). He also provided audio commentaries for the special edition DVD releases of A Fistful of Dollars, For a Few Dollars More, Once Upon a Time in the West and The Colossus of Rhodes.[citation needed]		His father, Major Arthur Frayling, was a furrier.[8] His mother, Betty Frayling, won the RAC Rally in 1952.[9] His brother, Nicholas, is Dean of Chichester Cathedral.		In 2001, he was awarded a knighthood for "Services to Art and Design Education" and chose as his motto "PERGE SCELUS MIHI DIEM PERFICIAS", which can be translated as "Proceed, varlet, and let the day be rendered perfect for my benefit".[10] That is, 'Go ahead, punk, make my day'.		
Seersucker is a thin, puckered, all-cotton fabric, commonly striped or chequered, used to make clothing for spring and summer wear. The word came into English from Persian, and originates from the words sheer and shakar, literally meaning "milk and sugar", probably from the resemblance of its smooth and rough stripes to the smooth texture of milk and the bumpy texture of sugar.[1] Seersucker is woven in such a way that some threads bunch together, giving the fabric a wrinkled appearance in places. This feature causes the fabric to be mostly held away from the skin when worn, facilitating heat dissipation and air circulation. It also means that pressing is not necessary.		Common items of made from seersucker include suits, shorts, shirts, curtains, dresses, and robes. The most common colors for it are white and blue; however, it is produced in a wide variety of colors, usually alternating colored stripes and puckered white stripes slightly wider than pin stripes.						During the British colonial period, seersucker was a popular material in Britain's warm weather colonies like British India. When seersucker was first introduced in the United States, it was used for a broad array of clothing items. For suits, the material was considered a mainstay of the summer wardrobe of gentlemen, especially in the South, who favored the light fabric in the high heat and humidity of the summer, especially prior to the arrival of air conditioning.[2]		From the mid Victorian era until the early 20th century, seersucker was also known as bed ticking due to its widespread use in mattresses, pillow cases and nightshirts during the hot summers in the Southern US[3] and Britain's overseas colonies.[4] During the American Civil War, this cheap but durable material was used to make haversacks and even the famous baggy pants of Confederate Zouaves such as the Louisiana Tigers.[5]		In the days of the Old West, a type of heavyweight dark blue seersucker known as "hickory stripe" was used to make the overalls, work jackets and peaked caps of train engineers and railroad workers such as George "Stormy" Kromer or Casey Jones.[6] It was later worn by butchers[7] and employees of the gasoline companies, most notably Standard Oil.[8] This cotton fabric was durable like denim, cheap to produce, kept the wearer cool in the hot cab of the steam locomotive,[9] and obscured oil or coal tar stains. Even today, the uniforms of American Union Pacific[10] train drivers include "railroad stripe" caps based on those from the steam age, and some rolling stock used for freight, shunting and maintenance work is painted with blue and white "zebra stripes" to improve visibility.[11]		The fabric was originally worn by the poor in the U.S. until preppy undergraduate students began wearing it in the 1920s in an air of reverse snobbery.[12]		Seersucker is comfortable and easily washed and was the choice for the summer service uniforms of the first female United States Marines. The decision was made by Captain Anne A. Lentz, one of the first female officers selected to run the Marine Corps Women's Reserve during the Second World War.[13] From the 1940s onwards, nurses and US hospital volunteers also wore uniforms made from a type of red and white seersucker known as candy stripe.[14]		About 1909, New Orleans clothier Joseph Haspel, Sr. started making men's suits out of seersucker fabric, which soon became regionally popular as more comfortable and practical than other types of suits during the area's hot and humid summers. [15][16]		During the 1950s, cheap railroad stripe overalls were worn by many young boys until they were old enough to wear jeans. This coincided with the popularity of train sets, and films such as The Great Locomotive Chase. At the same time, seersucker formal wear continued to be worn by many professional adults in the Southern and Southwestern US.[17] College professors were known to favor full suits with red bowties, although 1950s Ivy League and 21st century preppy[18] students usually restricted themselves to a single seersucker garment,[19] such as a blazer paired with khaki chino trousers.[20] Menswear brands famous for manufacturing seersucker at this time included Brooks Brothers, Macy's, Sears, and Joseph Haspel of New Orleans.[2][21]		In the 1970s, seersucker pants were popular among young urban African Americans seeking to connect to their rural heritage.[22] The fabric made a comeback among teenage girls in the 1990s, and again in the 2010s.[23]		Beginning in 1996, the US Senate held a Seersucker Thursday in June, where the participants dress in traditionally Southern clothing,[24] but the tradition was discontinued in June 2012. As of June 2014, it has been revived by members of the US Senate.[25] At the same time, however, some senators such as Ryan McKenna of Missouri have spoken against the wearing of seersucker due to its traditional use by small children.[26] The Republican Party has advised students at its Comms college not to wear seersucker when appearing before the cameras because of its old fashioned connotations,[2] plus the disruptive effect of the stripes.[27]		From 2012 onwards, seersucker blazers and pants made a comeback among American men[28] due to a resurgence of interest in preppy clothing[29] and the 1920s fashion showcased in The Great Gatsby. Although pale blue and dark blue stripes remained the most popular choice, alternative colors included green, red, black,[30] grey, beige, yellow, orange,[31] purple, pink, and brown.[32] The traditional two button blazer was updated with a slimmer cut and Edwardian inspired lapel piping,[33] and double breasted jackets became available during the mid 2010s.[34] Since 2010, "Seersucker Social" events have been held in major cities across the United States, where participants wear vintage clothes and ride vintage bicycles.[35] Such events are the summer equivalent of a Tweed Run, which is traditionally held in the fall.		In the 2016 Olympics hosted by Brazil, the Australian Olympic team received green and white seersucker blazers[36] and Toms shoes rather than the traditional dark green with gold trim.[37] At the same time, seersucker pants, skirts, espadrilles, blouses, and even bikinis were worn as casual attire by many fashion conscious young women in America.[38]		Seersucker is made by slack-tension weave. The threads are wound onto the two warp beams in groups of 10 to 16 for a narrow stripe. The stripes are always in the warp direction and ongrain. Today, seersucker is produced by a limited number of manufacturers. It is a low-profit, high-cost item because of its slow weaving speed.[citation needed]		Green/white checkered seersucker fabric.		Close up of green/white checkered seersucker, or "gingham", fabric showing the weave details.		Shirt from green/white seersucker fabric.		A blue and white seersucker jacket.		David Ferriero, speaking at Wikimania 2012, wearing a seersucker suit.		
African American Vernacular English (AAVE), also called African American English (AAE), or less precisely Black English, Black Vernacular, Black English Vernacular (BEV), or Black Vernacular English (BVE), is a variety (dialect, ethnolect and sociolect) of American English, spoken by urban working-class and (largely bidialectal) middle-class African Americans.[1] It is sometimes colloquially referred to as Ebonics, a term that is avoided by linguists because of its other meanings and connotations.[2]		It shares a large portion of its grammar and phonology with the rural dialects of the Southern United States,[3] and especially older Southern American English.[4] Several creolists, including William Stewart, John Dillard and John Rickford, argue that AAVE shares so many characteristics with African creole languages spoken around the world that AAVE itself may be an English-based creole language, separate from English;[5][6] however, mainstream linguists maintain that there are no significant parallels,[7][8][9] and that AAVE is a demonstrable variety of the English language,[10][11] having features that can be traced back mostly to the nonstandard British English of early settlers in the Southern United States.[12]		As with all linguistic forms, its usage is influenced by age, status, topic, and setting. There are many literary uses of AAVE, particularly in African-American literature.						AAVE shares several characteristics with Creole English language-forms spoken by people throughout much of the world. AAVE has pronunciation, grammatical structures, and vocabulary in common with various West African languages.[13]		Many features of AAVE are shared with English dialects spoken in the American South. While these are mostly regionalisms (i.e. originating from the dialect commonly spoken in the area, regardless of the speaker's color), a number of them—such as the deletion of is—are used much more frequently by black speakers, suggesting that they have their origins in black speech.[14] The traits of AAVE that distinguish it from the General American accent and other American English dialects include the following:		Early AAVE contributed a number of African-originated words to the American English mainstream, including gumbo,[15] goober,[16] yam, and banjo. AAVE has also contributed slang expressions such as cool and hip.[17]		Misconceptions about AAVE are, and have long been, common, and have stigmatized its use. One myth is that AAVE is grammatically simple or sloppy. Another is that AAVE is the native dialect (or even more inaccurately, a linguistic fad) employed by all African Americans. Wheeler (1999) warns that "AAVE should not be thought of as the language of Black people in America. Many African Americans neither speak it nor know much about it."[18]		While it is clear that there is a strong relationship between AAVE and Southern U.S. dialects, the unique characteristics of AAVE are not fully understood and its origins are still a matter of debate.		One theory is that AAVE arose from one or more slave creoles that arose from the trans-Atlantic African slave trade and the need for African captives to communicate among themselves and with their captors.[19] According to this theory, these captives developed what are called pidgins, simplified mixtures of two or more languages. As pidgins form from close contact between members of different language communities, the slave trade would have been exactly such a situation. Dillard quotes slave ship Captain William Smith:[20]		As for the languages of Gambia, they are so many and so different, that the Natives, on either Side of the River, cannot understand each other.... [T]he safest Way is to trade with the different Nations, on either Side of the River, and having some of every Sort on board, there will be no more Likelihood of their succeeding in a Plot, than of finishing the Tower of Babel.		By 1715, this African pidgin had made its way into novels by Daniel Defoe, in particular, The Life of Colonel Jacque. In 1721, Cotton Mather conducted the first attempt at recording the speech of slaves in his interviews regarding the practice of smallpox inoculation.[21]		By the time of the American Revolution, varieties among slave creoles were not quite mutually intelligible. Dillard quotes a recollection of "slave language" toward the latter part of the 18th century:[20]		Kay, massa, you just leave me, me sit here, great fish jump up into da canoe, here he be, massa, fine fish, massa; me den very grad; den me sit very still, until another great fish jump into de canoe; but me fall asleep, massa, and no wake 'til you come...		Not until the time of the American Civil War did the language of the slaves become familiar to a large number of educated whites. The abolitionist papers before the war form a rich corpus of examples of plantation creole. In Army Life in a Black Regiment (1870), Thomas Wentworth Higginson detailed many features of his soldiers' language.		In the early 2000s, Shana Poplack provided corpus-based evidence[8][9]—evidence from a body of writing—from isolated enclaves in Samaná and Nova Scotia peopled by descendants of migrations of early AAVE-speaking groups (see Samaná English), that suggests that the grammar of early AAVE was closer to that of contemporary British dialects than modern urban AAVE is to current American dialects, suggesting that the modern language is a result of divergence from mainstream varieties, rather than the result of decreolization from a widespread American creole.[22]		Linguist John McWhorter maintains that the contribution of West African languages to AAVE is minimal. In an interview on National Public Radio's Talk of the Nation, Dr. McWhorter characterized AAVE as a "hybrid of regional dialects of Great Britain that slaves in America were exposed to because they often worked alongside the indentured servants who spoke those dialects..." According to Dr. McWhorter, virtually all linguists who have carefully studied the origins of AAVE "agree that the West African connection is quite minor."[23]		Although the distinction between AAVE and General American accents is clear to speakers, some characteristics, notably double negatives and the omission of certain auxiliaries (see below) such as the has in has been are also characteristic of general colloquial American English.[citation needed] There is near uniformity of AAVE grammar, despite its vast geographic spread.[24] This may be due in part to relatively recent migrations of African Americans out of the American South (see Great Migration and Second Great Migration) as well as to long-term racial segregation.[25]		Phonological (or pronunciation) features that may set AAVE apart from other forms of American English (particularly, General American) include:		Although AAVE does not necessarily have the preterite marker of other English varieties (that is, the -ed of worked), it does have an optional tense system with four past and two future tenses or (because they indicate tense in degrees) phases.[47]		^a Syntactically, I bought it is grammatical, but done (always unstressed) is used to emphasize the completed nature of the action.[49]		As phase auxiliary verbs, been and done must occur as the first auxiliary; when they occur as the second, they carry additional aspects:[48]		The latter example shows one of the most distinctive features of AAVE: the use of be to indicate that performance of the verb is of a habitual nature. In most other American English dialects, this can only be expressed unambiguously by using adverbs such as usually.[50]		This aspect-marking form of been or BIN[51] is stressed and semantically distinct from the unstressed form: She BIN running ('She has been running for a long time') and She been running ('She has been running').[52] This aspect has been given several names, including perfect phase, remote past, and remote phase (this article uses the third).[53] As shown above, been places action in the distant past. However, when been is used with stative verbs or gerund forms, been shows that the action began in the distant past and that it is continuing now. Rickford (1999) suggests that a better translation when used with stative verbs is "for a long time". For instance, in response to "I like your new dress", one might hear Oh, I been had this dress, meaning that the speaker has had the dress for a long time and that it isn't new.[53]		To see the difference between the simple past and the gerund when used with been, consider the following expressions:		In addition to these, come (which may or may not be an auxiliary[58]) may be used to indicate speaker indignation, such as in Don't come acting like you don't know what happened and you started the whole thing ('Don't try to act as if you don't know what happened, because you started the whole thing').[59]		Negatives are formed differently from most other varieties of English:[60]		While AAVE shares these with Creole languages,[62] Howe & Walker (2000) use data from early recordings of African Nova Scotian English, Samaná English, and the recordings of former slaves to demonstrate that negation was inherited from nonstandard colonial English.[60]		According to John McWhorter, there is a continuum from "a 'deep' Black English through a 'light' Black English to standard English," and the sound on this continuum may vary from one African American speaker to the next or even in a single speaker from one situational context to the next.[68] McWhorter argues that what truly unites all varieties of AAVE is its unique intonation pattern or "melody," which characterizes even the most "neutral" light Black English. McWhorter regards the following as rarer features, characteristic only of a deep Black English but which speakers of light Black English may occasionally "dip into for humorous or emotive effect":[69]		AAVE shares much of its lexicon with other varieties of English, particularly that of informal and Southern dialects. For example, y'all is a feature. However, there are also some notable differences. It has been suggested that some of the vocabulary unique to AAVE has its origin in West African languages, but etymology is often difficult to trace and, without a trail of recorded usage, the suggestions below cannot be considered proven; in many cases, the postulated etymologies are not recognized by linguists or the Oxford English Dictionary.[71]		AAVE also has words that either are not part of most other American English dialects or have strikingly different meanings. For example, there are several words in AAVE referring to white people which are not part of mainstream American English; these include gray as an adjective for whites (as in gray dude), possibly from the color of Confederate uniforms; and paddy, an extension of the slang use for "Irish".[76] "Ofay," which is pejorative, is another general term for a white person; it might derive from the Ibibio word afia, which means "light-colored"; or from the Yoruba word ofe, spoken in hopes of disappearing from danger such as that posed by European traders; or via Pig Latin from "foe". However, most dictionaries simply say its etymology is unknown.[77] Kitchen refers to the particularly curly or kinky hair at the nape of the neck, and siditty or seddity means snobbish or bourgeois.[78]		AAVE has also contributed various words and phrases to other varieties of English; including chill out, main squeeze, soul, funky, and threads.[79]		Linguistically, that is, descriptively, there is nothing intrinsically "wrong" or "sloppy" about AAVE as a language variety since, like all dialects, AAVE shows consistent internal logic and grammatical complexity, and is used naturally to express thoughts and ideas.[80] Prescriptively, attitudes about AAVE are often less positive; since AAVE deviates from the standard, its use is commonly misinterpreted as a sign of ignorance, laziness, or both.[81][82] Perhaps because of this attitude (as well as similar attitudes among other Americans), most speakers of AAVE are bidialectal, being able to speak with a more General American accent as well as AAVE. Such linguistic adaptation in different environments is called code-switching[83][84]—though Linnes (1998) argues that the situation is actually one of diglossia:[85] each dialect, or code, is applied in different settings. Generally speaking, the degree of exclusive use of AAVE decreases with increasing socioeconomic status (although AAVE is still used by even well-educated African Americans).[86][87][88][89]		The United States courts are divided over how to admit statements made in AAVE under evidence. In United States v. Arnold, the United States Court of Appeals for the Sixth Circuit held that "he finna shoot me" was a statement made in the present tense, so it was admissible hearsay under the excited utterance exception; however, the dissent held that past or present tense could not be determined by the statement, so the statement should not have been admitted into evidence.[90]		Ogbu (1999) argues that the use of AAVE carries racially affirmative political undertones as its use allows African Americans to assert their cultural upbringing. Nevertheless, use of AAVE also carries strong social connotations; Sweetland (2002) presents a white female speaker of AAVE who is accepted as a member into African American social groups despite her race.		Amid related research in the 1960s and 1970s—including William Labov's groundbreaking thorough grammatical study, Language in the Inner City—there was doubt as to the existence of a distinct variety of English spoken by African Americans; Williamson (1970) noted that distinctive features of African American speech were present in the speech of Southerners and Farrison (1970) argued that there were really no substantial vocabulary or grammatical differences between the speech of blacks and that of other English dialects.[91]		There is a long tradition of representing the speech of blacks in American literature. A number of researchers[92] have looked into the ways that American authors have depicted the speech of black characters, investigating how black identity is established and how it connects to other characters. Brasch (1981:x) argues that early mass media portrayals of black speech are the strongest historical evidence of a separate variety of English for blacks.[93] Early popular works are also used to determine the similarities that historical varieties of black speech have in common with modern AAVE.[94][95]		The earliest depictions of black speech came from works written in the eighteenth century,[96] primarily by white authors. A notable exception is Clotel, the first novel written by an African American (William Wells Brown).[53][97] Depictions have largely been restricted to dialogue and the first novel written entirely in AAVE was June Jordan's His Own Where (1971),[98] though Alice Walker's epistolary novel The Color Purple is a much more widely known work written entirely in AAVE.[99] Lorraine Hansberry's 1959 play A Raisin in the Sun also has near exclusive use of AAVE.[100] The poetry of Langston Hughes uses AAVE extensively.[101][page needed]		Some other notable works that have incorporated representations of black speech (with varying degrees of perceived authenticity) include:[102]		As there is no established spelling system for AAVE,[106] depicting it in literature is instead often done through spelling changes to indicate its phonological features,[107] or to contribute to the impression that AAVE is being used (eye dialect).[108] More recently, authors have begun focusing on grammatical cues,[53] and even the use of certain rhetorical strategies.[109]		Portrayals of black characters in movies and television are also done with varying degrees of authenticity.[110] In Imitation of Life (1934), the speech and behavioral patterns of Delilah (an African American character) are reminiscent of minstrel performances that set out to exaggerate stereotypes, rather than depict black speech authentically.[111] More authentic performances, such as those in the following movies and TV shows, occur when certain speech events, vocabulary, and syntactic features are used to indicate AAVE usage, often with particular emphasis on young, urban African Americans:[112]		Spirituals, blues, jazz, R & B, and most recently, hip-hop are all genres associated with African American music; as such, AAVE appears in these musical forms. Examples of morphosyntactic features of AAVE in genres other than hip-hop are given below:		More recently, AAVE has been used heavily in hip-hop to show street cred.[113] Examples of morphosyntactic AAVE features used by black hip-hop artists are given below:		In addition to grammatical features, lexical items specific to AAVE are often used in hip-hop:		^a Lexical items taken from Smitherman (2000)		Because hip-hop is so intimately related to the African American oral tradition, non-black hip-hop artists also use certain features of AAVE; for example, in an MC battle, Eyedea said, "What that mean, yo?"[114] displaying lack of subject-verb inversion and also the auxiliary DO. However, they tend to avoid the term nigga, even as a marker of solidarity.[114] White hip-hop artists such as Eyedea can choose to accentuate their whiteness by hyper-articulating postvocalic r sounds (i.e. the retroflex approximant).[114]		AAVE is also used by non-black artists in genres other than hip-hop, if less frequently. For instance, in "Tonight, Tonight", Hot Chelle Rae uses the term dime to mean "an attractive woman".[115] Jewel's "Sometimes It Be That Way" employs habitual BE in the title to indicate habitual aspect. If they do not employ similar features of AAVE in their speech, then it can be argued that they are modeling their musical performance to evoke aspects of particular musical genres such as R & B or the blues (as British pop musicians of the 1960s and beyond did to evoke rock, pop, and the blues).[116]		Some research suggests that non-African American young adults learn AAVE vocabulary by listening to hip-hop music.[113]		On Twitter, AAVE is used as a framework from which sentences and words are constructed, in order to accurately express oneself. [117] Grammatical features and word pronunciations stemming from AAVE are preserved. [117] Spellings based on AAVE have become increasingly common, to the point where it has become a normalized practice. Some examples include, "you" (you're), "they" (their/they're), "gon/gone" (going to), and "yo" (your). [117]		AAVE has been the center of controversy about the education of African American youths, the role AAVE should play in public schools and education, and its place in broader society.[118] Educators have held that attempts should be made to eliminate AAVE usage through the public education system. Criticism from social commentators and educators has ranged from asserting that AAVE is an intrinsically deficient form of speech to arguments that its use, by being considered unacceptable in most cultural contexts, is socially limiting.[119] Some of the harshest criticism of AAVE or its use has come from African Americans.[120][121][122] A conspicuous example was the "Pound Cake speech", in which Bill Cosby criticized some African Americans for various social behaviors, including the way they talked.		Faced with such attitudes, the Conference on College Composition and Communication (CCCC), a division of National Council of Teachers of English (NCTE), issued a position statement on students' rights to their own language. This was adopted by CCCC members in April 1974 and appeared in a special issue of College Composition and Communication in Fall of 1974. The resolution was as follows:[123]		"We affirm the students' right to their own patterns and varieties of language—the dialects of their nurture or whatever dialects in which they find their own identity and style. Language scholars long ago denied that the myth of a standard American dialect has any validity. The claim that any one dialect is unacceptable amounts to an attempt of one social group to exert its dominance over another. Such a claim leads to false advice for speakers and writers and immoral advice for humans. A nation proud of its diverse heritage and its cultural and racial variety will preserve its heritage of dialects. We affirm strongly that teachers must have the experiences and training that will enable them to respect diversity and uphold the right of students to their own language."		Around this time, pedagogical techniques similar to those used to teach English to speakers of foreign languages were shown to hold promise for speakers of AAVE. William Stewart experimented with the use of dialect readers—sets of text in both AAVE and standard English.[124] The idea was that children could learn to read in their own dialect and then shift to "Standard English" with subsequent textbooks.[125] Simpkins, Holt & Simpkins (1977) developed a comprehensive set of dialect readers, called bridge readers, which included the same content in three different dialects: AAVE, a "bridge" version that was closer to "Standard American English" without being prohibitively formal, and a Standard English version.[126] Despite studies that showed promise for such "Standard English as a Second Dialect" (SESD) programs, reaction to them was largely hostile[127] and both Stewart's research and the Bridge Program were rejected for various political and social reasons, including strong resistance from parents.[125][128][129]		A more formal shift in the recognition of AAVE came in the "Ann Arbor Decision" of 1979 (Martin Luther King Junior Elementary School Children et al., v. Ann Arbor School District). In it, a federal judge of the Eastern District of Michigan ruled that in teaching black children to read, a school board must adjust to the children's dialect, not the children to the school,[125] and that, by not taking students’ language into consideration, teachers were contributing to the failure of such students to read and use mainstream English proficiently.[130]		National attitudes towards AAVE were revisited when a controversial resolution from the Oakland (California) school board (Oakland Unified School District) on December 18, 1996, called for "Ebonics" to be recognized as a language of African Americans.[131] In fact, ebonics would be classified as a "second language".[132][133] The proposal was to implement a program similar to the Language Development Program for African American Students (LDPAAS) in Los Angeles, which began in 1988 and uses methods from the SESD programs mentioned above.[134]		Like other similar programs,[135] the Oakland resolution was widely misunderstood as intended to teach AAVE and "elevate it to the status of a written language."[136] It gained national attention and was derided and criticized, most notably by Jesse Jackson and Kweisi Mfume who regarded it as an attempt to teach slang to children.[137] The statement that "African Language Systems are genetically based" also contributed to the negative reaction because "genetically" was popularly misunderstood to imply that African Americans had a biological predisposition to a particular language.[138] In an amended resolution, this phrase was removed and replaced with wording that states African American language systems "have origins in West [sic] and Niger–Congo languages and are not merely dialects of English. . . ."[139]		The Oakland proposal was explained as follows: that black students would perform better in school and more easily learn standard American English if textbooks and teachers incorporated AAVE in teaching black children to speak Standard English rather than mistakenly[30][140] equating nonstandard with substandard and dismissing AAVE as the latter. Baratz & Shuy (1969:93) point to these linguistic barriers, and common reactions by teachers, as a primary cause of reading difficulties and poor school performance.[141]		More recently, research has been conducted on the overrepresentation of African Americans in special education[142] Van Keulen, Weddington & DeBose (1998:112–113) argue that this is because AAVE speech characteristics are often erroneously considered to be signs of speech development problems, prompting teachers to refer children to speech pathologists.[143]		According to Smitherman, the controversy and debates concerning AAVE in public schools imply deeper deterministic attitudes towards the African-American community as a whole. Smitherman describes this as a reflection of the "power elite's perceived insignificance and hence rejection of Afro-American language and culture".[144] She also asserts that African Americans are forced to conform to European American society in order to succeed, and that conformity ultimately means the "eradication of black language . . . and the adoption of the linguistic norms of the white middle class." The necessity for "bi-dialectialism" (AAVE and General American) means "some blacks contend that being bi-dialectal not only causes a schism in the black personality, but it also implies such dialects are 'good enough' for blacks but not for whites."[145]		
The Massachusetts Institute of Technology (MIT) is a private research university in Cambridge, Massachusetts, often cited as one of the world's most prestigious universities.[10][11][12][13]		Founded in 1861 in response to the increasing industrialization of the United States, MIT adopted a European polytechnic university model and stressed laboratory instruction in applied science and engineering. Researchers worked on computers, radar, and inertial guidance during World War II and the Cold War. Post-war defense research contributed to the rapid expansion of the faculty and campus under James Killian. The current 168-acre (68.0 ha) campus opened in 1916 and extends over 1 mile (1.6 km) along the northern bank of the Charles River basin.		The Institute is traditionally known for its research and education in the physical sciences and engineering, and more recently in biology, economics, linguistics, and management as well. MIT is a member of the Association of American Universities (AAU) and founder of the Amsterdam Institute for Advanced Metropolitan Solutions (AMS Institute). For several years, MIT's School of Engineering has been ranked first in various international and national university rankings, and the Institute is also often ranked among the world's top universities overall.[10][11][12][13][14] The "Engineers" compete in 31 sports, most teams of which compete in the NCAA Division III's New England Women's and Men's Athletic Conference; the Division I rowing programs compete as part of the EARC and EAWRC.		As of 2015[update], 85 Nobel laureates, 52 National Medal of Science recipients, 65 Marshall Scholars, 45 Rhodes Scholars, 38 MacArthur Fellows, 34 astronauts, 19 Turing award winners, 16 Chief Scientists of the U.S. Air Force, and 6 Fields Medalists have been affiliated with MIT. The school has a strong entrepreneurial culture, and the aggregated revenues of companies founded by MIT alumni would rank as the eleventh-largest economy in the world.[15][16]						In 1859, a proposal was submitted to the Massachusetts General Court to use newly filled lands in Back Bay, Boston for a "Conservatory of Art and Science", but the proposal failed.[18][19] A charter for the incorporation of the Massachusetts Institute of Technology, proposed by William Barton Rogers, was signed by the governor of Massachusetts on April 10, 1861.[20]		Rogers, a professor from the University of Virginia, wanted to establish an institution to address rapid scientific and technological advances.[21][22] He did not wish to found a professional school, but a combination with elements of both professional and liberal education,[23] proposing that:		The true and only practicable object of a polytechnic school is, as I conceive, the teaching, not of the minute details and manipulations of the arts, which can be done only in the workshop, but the inculcation of those scientific principles which form the basis and explanation of them, and along with this, a full and methodical review of all their leading processes and operations in connection with physical laws.[24]		The Rogers Plan reflected the German research university model, emphasizing an independent faculty engaged in research, as well as instruction oriented around seminars and laboratories.[25][26]		Two days after the charter was issued, the first battle of the Civil War broke out. After a long delay through the war years, MIT's first classes were held in the Mercantile Building in Boston in 1865.[27] The new institute was founded as part of the Morrill Land-Grant Colleges Act to fund institutions "to promote the liberal and practical education of the industrial classes", and was a land-grant school.[28][29] In 1863 under the same act, the Commonwealth of Massachusetts founded the Massachusetts Agricultural College, which developed as the University of Massachusetts Amherst. In 1866, the proceeds from land sales went toward new buildings in the Back Bay.[30]		MIT was informally called "Boston Tech".[30] The institute adopted the European polytechnic university model and emphasized laboratory instruction from an early date.[31] Despite chronic financial problems, the institute saw growth in the last two decades of the 19th century under President Francis Amasa Walker.[32] Programs in electrical, chemical, marine, and sanitary engineering were introduced,[33][34] new buildings were built, and the size of the student body increased to more than one thousand.[32]		The curriculum drifted to a vocational emphasis, with less focus on theoretical science.[35] The fledgling school still suffered from chronic financial shortages which diverted the attention of the MIT leadership. During these "Boston Tech" years, MIT faculty and alumni rebuffed Harvard University president (and former MIT faculty) Charles W. Eliot's repeated attempts to merge MIT with Harvard College's Lawrence Scientific School.[36] There would be at least six attempts to absorb MIT into Harvard.[37] In its cramped Back Bay location, MIT could not afford to expand its overcrowded facilities, driving a desperate search for a new campus and funding. Eventually the MIT Corporation approved a formal agreement to merge with Harvard, over the vehement objections of MIT faculty, students, and alumni.[37] However, a 1917 decision by the Massachusetts Supreme Judicial Court effectively put an end to the merger scheme.[37]		In 1916, the MIT administration and the MIT charter crossed the Charles River on the ceremonial barge Bucentaur built for the occasion,[38][39] to signify MIT's move to a spacious new campus largely consisting of filled land on a mile-long tract along the Cambridge side of the Charles River.[40][41] The neoclassical "New Technology" campus was designed by William W. Bosworth[42] and had been funded largely by anonymous donations from a mysterious "Mr. Smith", starting in 1912. In January 1920, the donor was revealed to be the industrialist George Eastman of Rochester, New York, who had invented methods of film production and processing, and founded Eastman Kodak. Between 1912 and 1920, Eastman donated $20 million ($236.6 million in 2015 dollars) in cash and Kodak stock to MIT.[43]		In the 1930s, President Karl Taylor Compton and Vice-President (effectively Provost) Vannevar Bush emphasized the importance of pure sciences like physics and chemistry and reduced the vocational practice required in shops and drafting studios.[44] The Compton reforms "renewed confidence in the ability of the Institute to develop leadership in science as well as in engineering."[45] Unlike Ivy League schools, MIT catered more to middle-class families, and depended more on tuition than on endowments or grants for its funding.[46] The school was elected to the Association of American Universities in 1934.[47]		Still, as late as 1949, the Lewis Committee lamented in its report on the state of education at MIT that "the Institute is widely conceived as basically a vocational school", a "partly unjustified" perception the committee sought to change. The report comprehensively reviewed the undergraduate curriculum, recommended offering a broader education, and warned against letting engineering and government-sponsored research detract from the sciences and humanities.[48][49] The School of Humanities, Arts, and Social Sciences and the MIT Sloan School of Management were formed in 1950 to compete with the powerful Schools of Science and Engineering. Previously marginalized faculties in the areas of economics, management, political science, and linguistics emerged into cohesive and assertive departments by attracting respected professors and launching competitive graduate programs.[50][51] The School of Humanities, Arts, and Social Sciences continued to develop under the successive terms of the more humanistically oriented presidents Howard W. Johnson and Jerome Wiesner between 1966 and 1980.[52]		MIT's involvement in military science surged during World War II. In 1941, Vannevar Bush was appointed head of the federal Office of Scientific Research and Development and directed funding to only a select group of universities, including MIT.[53] Engineers and scientists from across the country gathered at MIT's Radiation Laboratory, established in 1940 to assist the British military in developing microwave radar. The work done there significantly affected both the war and subsequent research in the area.[54] Other defense projects included gyroscope-based and other complex control systems for gunsight, bombsight, and inertial navigation under Charles Stark Draper's Instrumentation Laboratory;[55][56] the development of a digital computer for flight simulations under Project Whirlwind;[57] and high-speed and high-altitude photography under Harold Edgerton.[58][59] By the end of the war, MIT became the nation's largest wartime R&D contractor (attracting some criticism of Bush),[53] employing nearly 4000 in the Radiation Laboratory alone[54] and receiving in excess of $100 million ($1.2 billion in 2015 dollars) before 1946.[45] Work on defense projects continued even after then. Post-war government-sponsored research at MIT included SAGE and guidance systems for ballistic missiles and Project Apollo.[60]		These activities affected MIT profoundly. A 1949 report noted the lack of "any great slackening in the pace of life at the Institute" to match the return to peacetime, remembering the "academic tranquility of the prewar years", though acknowledging the significant contributions of military research to the increased emphasis on graduate education and rapid growth of personnel and facilities.[62] The faculty doubled and the graduate student body quintupled during the terms of Karl Taylor Compton, president of MIT between 1930 and 1948; James Rhyne Killian, president from 1948 to 1957; and Julius Adams Stratton, chancellor from 1952 to 1957, whose institution-building strategies shaped the expanding university. By the 1950s, MIT no longer simply benefited the industries with which it had worked for three decades, and it had developed closer working relationships with new patrons, philanthropic foundations and the federal government.[63]		In late 1960s and early 1970s, student and faculty activists protested against the Vietnam War and MIT's defense research.[64][65] In this period MIT's various departments were researching helicopters, smart bombs and counterinsurgency techniques for the war in Vietnam as well as guidance systems for nuclear missiles.[66] The Union of Concerned Scientists was founded on March 4, 1969 during a meeting of faculty members and students seeking to shift the emphasis on military research toward environmental and social problems.[67] MIT ultimately divested itself from the Instrumentation Laboratory and moved all classified research off-campus to the MIT Lincoln Laboratory facility in 1973 in response to the protests.[68][69] The student body, faculty, and administration remained comparatively unpolarized during what was a tumultuous time for many other universities.[64] Johnson was seen to be highly successful in leading his institution to "greater strength and unity" after these times of turmoil.[70] However six MIT students were sentenced to prison terms at this time and some former student leaders, such as Michael Albert and George Katsiaficas, are still indignant about MIT's role in military research and its suppression of these protests.[71] (Richard Leacock's film, November Actions, records some of these tumultuous events.[72])		In the 1980s, there was more controversy at MIT over its involvement in SDI (space weaponry) and CBW (chemical and biological warfare) research.[73] More recently, MIT’s research for the military has included work on robots, drones and ‘battle suits’.[74]		MIT has kept pace with and helped to advance the digital age. In addition to developing the predecessors to modern computing and networking technologies,[75][76] students, staff, and faculty members at Project MAC, the Artificial Intelligence Laboratory, and the Tech Model Railroad Club wrote some of the earliest interactive computer video games like Spacewar! and created much of modern hacker slang and culture.[77] Several major computer-related organizations have originated at MIT since the 1980s: Richard Stallman's GNU Project and the subsequent Free Software Foundation were founded in the mid-1980s at the AI Lab; the MIT Media Lab was founded in 1985 by Nicholas Negroponte and Jerome Wiesner to promote research into novel uses of computer technology;[78] the World Wide Web Consortium standards organization was founded at the Laboratory for Computer Science in 1994 by Tim Berners-Lee;[79] the OpenCourseWare project has made course materials for over 2,000 MIT classes available online free of charge since 2002;[80] and the One Laptop per Child initiative to expand computer education and connectivity to children worldwide was launched in 2005.[81]		MIT was named a sea-grant college in 1976 to support its programs in oceanography and marine sciences and was named a space-grant college in 1989 to support its aeronautics and astronautics programs.[82][83] Despite diminishing government financial support over the past quarter century, MIT launched several successful development campaigns to significantly expand the campus: new dormitories and athletics buildings on west campus; the Tang Center for Management Education; several buildings in the northeast corner of campus supporting research into biology, brain and cognitive sciences, genomics, biotechnology, and cancer research; and a number of new "backlot" buildings on Vassar Street including the Stata Center.[84] Construction on campus in the 2000s included expansions of the Media Lab, the Sloan School's eastern campus, and graduate residences in the northwest.[85][86] In 2006, President Hockfield launched the MIT Energy Research Council to investigate the interdisciplinary challenges posed by increasing global energy consumption.[87]		In 2001, inspired by the open source and open access movements,[88] MIT launched OpenCourseWare to make the lecture notes, problem sets, syllabuses, exams, and lectures from the great majority of its courses available online for no charge, though without any formal accreditation for coursework completed.[89] While the cost of supporting and hosting the project is high,[90] OCW expanded in 2005 to include other universities as a part of the OpenCourseWare Consortium, which currently includes more than 250 academic institutions with content available in at least six languages.[91] In 2011, MIT announced it would offer formal certification (but not credits or degrees) to online participants completing coursework in its "MITx" program, for a modest fee.[92] The "edX" online platform supporting MITx was initially developed in partnership with Harvard and its analogous "Harvardx" initiative. The courseware platform is open source, and other universities have already joined and added their own course content.[93]		Three days after the Boston Marathon bombing of April 2013, MIT Police patrol officer Sean Collier was fatally shot by the suspects Dzhokhar and Tamerlan Tsarnaev, setting off a violent manhunt that shut down the campus and much of the Boston metropolitan area for a day.[94] One week later, Collier's memorial service was attended by more than 10,000 people, in a ceremony hosted by the MIT community with thousands of police officers from the New England region and Canada.[95][96][97] On November 25, 2013, MIT announced the creation of the Collier Medal, to be awarded annually to "an individual or group that embodies the character and qualities that Officer Collier exhibited as a member of the MIT community and in all aspects of his life". The announcement further stated that "Future recipients of the award will include those whose contributions exceed the boundaries of their profession, those who have contributed to building bridges across the community, and those who consistently and selflessly perform acts of kindness".[98][99][100]		MIT's 168-acre (68.0 ha) campus in the city of Cambridge spans approximately a mile along the north side of the Charles River basin.[6] The campus is divided roughly in half by Massachusetts Avenue, with most dormitories and student life facilities to the west and most academic buildings to the east. The bridge closest to MIT is the Harvard Bridge, which is known for being marked off in a non-standard unit of length – the smoot.[101][102]		The Kendall MBTA Red Line station is located on the northeastern edge of the campus, in Kendall Square. The Cambridge neighborhoods surrounding MIT are a mixture of high tech companies occupying both modern office and rehabilitated industrial buildings, as well as socio-economically diverse residential neighborhoods.[103][104] In early 2016, MIT presented its updated Kendall Square Initiative to the City of Cambridge, with plans for mixed-use educational, retail, residential, startup incubator, and office space in a dense high-rise transit-oriented development plan.[105][106] The MIT Museum will eventually be moved immediately adjacent to a Kendall Square subway entrance, joining the List Visual Arts Center on the eastern end of the campus.[106][107]		Each building at MIT has a number (possibly preceded by a W, N, E, or NW) designation and most have a name as well. Typically, academic and office buildings are referred to primarily by number while residence halls are referred to by name. The organization of building numbers roughly corresponds to the order in which the buildings were built and their location relative (north, west, and east) to the original center cluster of Maclaurin buildings.[108] Many of the buildings are connected above ground as well as through an extensive network of underground tunnels, providing protection from the Cambridge weather as well as a venue for roof and tunnel hacking.[109][110]		MIT's on-campus nuclear reactor[111] is one of the most powerful university-based nuclear reactors in the United States. The prominence of the reactor's containment building in a densely populated area has been controversial,[112] but MIT maintains that it is well-secured.[113] In 1999 Bill Gates donated US$20 million to MIT for the construction of a computer laboratory named the "William H. Gates Building", and designed by architect Frank Gehry. While Microsoft had previously given financial support to the institution, this was the first personal donation received from Gates.[114]		Other notable campus facilities include a pressurized wind tunnel and a towing tank for testing ship and ocean structure designs.[115][116] MIT's campus-wide wireless network was completed in the fall of 2005 and consists of nearly 3,000 access points covering 9,400,000 square feet (870,000 m2) of campus.[117]		In 2001, the Environmental Protection Agency sued MIT for violating the Clean Water Act and the Clean Air Act with regard to its hazardous waste storage and disposal procedures.[118] MIT settled the suit by paying a $155,000 fine and launching three environmental projects.[119] In connection with capital campaigns to expand the campus, the Institute has also extensively renovated existing buildings to improve their energy efficiency. MIT has also taken steps to reduce its environmental impact by running alternative fuel campus shuttles, subsidizing public transportation passes, and building a low-emission cogeneration plant that serves most of the campus electricity, heating, and cooling requirements.[120]		The MIT Police with state and local authorities, in the 2009-2011 period, have investigated reports of 12 forcible sex offenses, 6 robberies, 3 aggravated assaults, 164 burglaries, 1 case of arson, and 4 cases of motor vehicle theft on campus; affecting a community of around 22,000 students and employees.[121]		MIT has substantial commercial real estate holdings in Cambridge on which it pays property taxes, plus an additional voluntary payment in lieu of taxes (PILOT) on academic buildings which are legally tax-exempt. As of 2017[update], it is the largest taxpayer in the city, contributing approximately 14% of the city's annual revenues.[122] Holdings include Technology Square, parts of Kendall Square, and many properties in Cambridgeport and Area 4 neighboring the educational buildings.[123] The land is held for investment purposes and potential long-term expansion.		MIT's School of Architecture, now the School of Architecture and Planning, was the first in the United States,[124] and it has a history of commissioning progressive buildings.[125][126] The first buildings constructed on the Cambridge campus, completed in 1916, are sometimes called the "Maclaurin buildings" after Institute president Richard Maclaurin who oversaw their construction. Designed by William Welles Bosworth, these imposing buildings were built of reinforced concrete, a first for a non-industrial – much less university – building in the US.[127] Bosworth's design was influenced by the City Beautiful Movement of the early 1900s,[127] and features the Pantheon-esque Great Dome housing the Barker Engineering Library. The Great Dome overlooks Killian Court, where graduation ceremonies are held each year. The friezes of the limestone-clad buildings around Killian Court are engraved with the names of important scientists and philosophers.[a] The spacious Building 7 atrium at 77 Massachusetts Avenue is regarded as the entrance to the Infinite Corridor and the rest of the campus.[104]		Alvar Aalto's Baker House (1947), Eero Saarinen's MIT Chapel and Kresge Auditorium (1955), and I.M. Pei's Green, Dreyfus, Landau, and Wiesner buildings represent high forms of post-war modernist architecture.[130][131][132] More recent buildings like Frank Gehry's Stata Center (2004), Steven Holl's Simmons Hall (2002), Charles Correa's Building 46 (2005), and Fumihiko Maki's Media Lab Extension (2009) stand out among the Boston area's classical architecture and serve as examples of contemporary campus "starchitecture".[125][133] These buildings have not always been well received;[134][135] in 2010, The Princeton Review included MIT in a list of twenty schools whose campuses are "tiny, unsightly, or both".[136]		Undergraduates are guaranteed four-year housing in one of MIT's 12 undergraduate dormitories.[137] Those living on campus can receive support and mentoring from live-in graduate student tutors, resident advisors, and faculty housemasters.[138] Because housing assignments are made based on the preferences of the students themselves, diverse social atmospheres can be sustained in different living groups; for example, according to the Yale Daily News staff's The Insider's Guide to the Colleges, 2010, "The split between East Campus and West Campus is a significant characteristic of MIT. East Campus has gained a reputation as a thriving counterculture."[139] MIT also has 5 dormitories for single graduate students and 2 apartment buildings on campus for married student families.[140]		MIT has an active Greek and co-op housing system, including thirty-six fraternities, sororities, and independent living groups (FSILGs).[141] As of 2015[update], 98% of all undergraduates lived in MIT-affiliated housing; 54% of the men participated in fraternities and 20% of the women were involved in sororities.[142] Most FSILGs are located across the river in Back Bay near where MIT was founded, and there is also a cluster of fraternities on MIT's West Campus that face the Charles River Basin.[143] After the 1997 alcohol-related death of Scott Krueger, a new pledge at the Phi Gamma Delta fraternity, MIT required all freshmen to live in the dormitory system starting in 2002.[144] Because FSILGs had previously housed as many as 300 freshmen off-campus, the new policy could not be implemented until Simmons Hall opened in that year.[145]		MIT is chartered as a non-profit organization and is owned and governed by a privately appointed board of trustees known as the MIT Corporation.[146] The current board consists of 43 members elected to five-year terms,[147] 25 life members who vote until their 75th birthday,[148] 3 elected officers (President, Treasurer, and Secretary),[149] and 4 ex officio members (the president of the alumni association, the Governor of Massachusetts, the Massachusetts Secretary of Education, and the Chief Justice of the Massachusetts Supreme Judicial Court).[150][151] The board is chaired by Robert Millard, a co-founder of L-3 Communications Holdings.[152][153] The Corporation approves the budget, new programs, degrees and faculty appointments, and elects the President to serve as the chief executive officer of the university and preside over the Institute's faculty.[104][154] MIT's endowment and other financial assets are managed through a subsidiary called MIT Investment Management Company (MITIMCo).[155] Valued at $13.182 billion in 2016, MIT's endowment is the sixth-largest among American colleges and universities.[3]		MIT has five schools (Science, Engineering, Architecture and Planning, Management, and Humanities, Arts, and Social Sciences) and one college (Whitaker College of Health Sciences and Technology), but no schools of law or medicine.[156][b] While faculty committees assert substantial control over many areas of MIT's curriculum, research, student life, and administrative affairs,[158] the chair of each of MIT's 32 academic departments reports to the dean of that department's school, who in turn reports to the Provost under the President.[159] The current president is L. Rafael Reif, who formerly served as provost under President Susan Hockfield, the first woman to hold the post.[160][161]		MIT is a large, highly residential, research university with a majority of enrollments in graduate and professional programs.[162] The university has been accredited by the New England Association of Schools and Colleges since 1929.[163][164] MIT operates on a 4–1–4 academic calendar with the fall semester beginning after Labor Day and ending in mid-December, a 4-week "Independent Activities Period" in the month of January, and the spring semester beginning in early February and ending in late May.[165]		MIT students refer to both their majors and classes using numbers or acronyms alone.[166] Departments and their corresponding majors are numbered in the approximate order of their foundation; for example, Civil and Environmental Engineering is Course 1, while Linguistics and Philosophy is Course 24.[167] Students majoring in Electrical Engineering and Computer Science (EECS), the most popular department, collectively identify themselves as "Course 6". MIT students use a combination of the department's course number and the number assigned to the class to identify their subjects; the introductory calculus-based classical mechanics course is simply "8.01" at MIT.[168][c]		The four-year, full-time undergraduate program maintains a balance between professional majors and those in the arts and sciences, and has been dubbed "most selective" by U.S. News,[171] admitting few transfer students[162] and 8.0% of its applicants in the 2015 admissions cycle.[172] MIT offers 44 undergraduate degrees across its five schools.[173] In the 2010–2011 academic year, 1,161 bachelor of science degrees (abbreviated "SB") were granted, the only type of undergraduate degree MIT now awards.[needs update][174][175] In the 2011 fall term, among students who had designated a major, the School of Engineering was the most popular division, enrolling 63% of students in its 19 degree programs, followed by the School of Science (29%), School of Humanities, Arts, & Social Sciences (3.7%), Sloan School of Management (3.3%), and School of Architecture and Planning (2%).[needs update] The largest undergraduate degree programs were in Electrical Engineering and Computer Science (Course 6–2), Computer Science and Engineering (Course 6–3), Mechanical Engineering (Course 2), Physics (Course 8), and Mathematics (Course 18).[169]		All undergraduates are required to complete a core curriculum called the General Institute Requirements (GIRs).[176] The Science Requirement, generally completed during freshman year as prerequisites for classes in science and engineering majors, comprises two semesters of physics, two semesters of calculus, one semester of chemistry, and one semester of biology. There is a Laboratory Requirement, usually satisfied by an appropriate class in a course major. The Humanities, Arts, and Social Sciences (HASS) Requirement consists of eight semesters of classes in the humanities, arts, and social sciences, including at least one semester from each division as well as the courses required for a designated concentration in a HASS division. Under the Communication Requirement, two of the HASS classes, plus two of the classes taken in the designated major must be "communication-intensive",[177] including "substantial instruction and practice in oral presentation".[178] Finally, all students are required to complete a swimming test;[179] non-varsity athletes must also take four quarters of physical education classes.[176]		Most classes rely on a combination of lectures, recitations led by associate professors or graduate students, weekly problem sets ("p-sets"), and periodic quizzes or tests. While the pace and difficulty of MIT coursework has been compared to "drinking from a fire hose",[180][181] the freshmen retention rate at MIT is similar to other research universities.[171] The "pass/no-record" grading system relieves some pressure for first-year undergraduates. For each class taken in the fall term, freshmen transcripts will either report only that the class was passed, or otherwise not have any record of it. In the spring term, passing grades (A, B, C) appear on the transcript while non-passing grades are again not recorded.[182] (Grading had previously been "pass/no record" all freshman year, but was amended for the Class of 2006 to prevent students from gaming the system by completing required major classes in their freshman year.[183]) Also, freshmen may choose to join alternative learning communities, such as Experimental Study Group, Concourse, or Terrascope.[182]		In 1969, Margaret MacVicar founded the Undergraduate Research Opportunities Program (UROP) to enable undergraduates to collaborate directly with faculty members and researchers. Students join or initiate research projects ("UROPs") for academic credit, pay, or on a volunteer basis through postings on the UROP website or by contacting faculty members directly.[184] A substantial majority of undergraduates participate.[185][186] Students often become published, file patent applications, and/or launch start-up companies based upon their experience in UROPs.[187][188]		In 1970, the then-Dean of Institute Relations, Benson R. Snyder, published The Hidden Curriculum, arguing that education at MIT was often slighted in favor of following a set of unwritten expectations, and that graduating with good grades was more often the product of figuring out the system rather than a solid education. The successful student, according to Snyder, was the one who was able to discern which of the formal requirements were to be ignored in favor of which unstated norms. For example, organized student groups had compiled "course bibles"—collections of problem-set and examination questions and answers for later students to use as references. This sort of gamesmanship, Snyder argued, hindered development of a creative intellect and contributed to student discontent and unrest.[189][190]		MIT's graduate program has high coexistence with the undergraduate program, and many courses are taken by qualified students at both levels. MIT offers a comprehensive doctoral program with degrees in the humanities, social sciences, and STEM fields as well as professional degrees.[162] The Institute offers graduate programs leading to academic degrees such as the Master of Science (MS), various Engineer's Degrees, Doctor of Philosophy (PhD), and Doctor of Science (ScD) and interdisciplinary graduate programs such as the MD-PhD (with Harvard Medical School).[191][192]		Admission to graduate programs is decentralized; applicants apply directly to the department or degree program. More than 90% of doctoral students are supported by fellowships, research assistantships (RAs), or teaching assistantships (TAs).[193]		MIT awarded 1,547 master's degrees and 609 doctoral degrees in the academic year 2010–11.[needs update][174] In the 2011 fall term, the School of Engineering was the most popular academic division, enrolling 45.0% of graduate students, followed by the Sloan School of Management (19%), School of Science (16.9%), School of Architecture and Planning (9.2%), Whitaker College of Health Sciences (5.1%),[d] and School of Humanities, Arts, and Social Sciences (4.7%). The largest graduate degree programs were the Sloan MBA, Electrical Engineering and Computer Science, and Mechanical Engineering.[169]		MIT also places among the top ten in many overall rankings of universities (see right) and rankings based on students' revealed preferences.[202][203][204] For several years, U.S. News & World Report, the QS World University Rankings, and the Academic Ranking of World Universities have ranked MIT's School of Engineering first, as did the 1995 National Research Council report.[205] In the same lists, MIT's strongest showings apart from in engineering are in computer science, the natural sciences, business, architecture, economics, linguistics, mathematics, and, to a lesser extent, political science and philosophy.[10][11][12][13][14]		In 2014, Money magazine ranked MIT as third in the US "Best Colleges for Your Money", based on its assessment of "the most bang for your tuition buck", factoring in quality of education, affordability, and career outcomes.[206] As of 2014[update], Forbes magazine rated MIT as the second "Most Entrepreneurial University", based on the percentage of alumni and students self-identifying as founders or business owners on LinkedIn.[207] In 2015, Brookings Fellow Jonathan Rothwell issued a report "Beyond College Rankings", placing MIT as third in the US, with an estimated 45% value-added to mid-career salary.[208]		Times Higher Education has recognized MIT as one of the world's "six super brands" on its World Reputation Rankings, along with Berkeley, Cambridge, Harvard, Oxford and Stanford.[209]		The university historically pioneered research and training collaborations between academia, industry and government.[210][211]  In 1946, President Compton, Harvard Business School professor Georges Doriot, and Massachusetts Investor Trust chairman Merrill Grisswold founded American Research and Development Corporation, the first American venture-capital firm.[212][213]  In 1948, Compton established the MIT Industrial Liaison Program.[214] Throughout the late 1980s and early 1990s, American politicians and business leaders accused MIT and other universities of contributing to a declining economy by transferring taxpayer-funded research and technology to international – especially Japanese – firms that were competing with struggling American businesses.[215][216] On the other hand, MIT's extensive collaboration with the federal government on research projects has led to several MIT leaders serving as presidential scientific advisers since 1940.[e] MIT established a Washington Office in 1991 to continue effective lobbying for research funding and national science policy.[218][219]		The U.S. Justice Department began an investigation in 1989, and in 1991 filed an antitrust suit against MIT, the eight Ivy League colleges, and eleven other institutions for allegedly engaging in price-fixing during their annual "Overlap Meetings", which were held to prevent bidding wars over promising prospective students from consuming funds for need-based scholarships.[220][221] While the Ivy League institutions settled,[222] MIT contested the charges, arguing that the practice was not anti-competitive because it ensured the availability of aid for the greatest number of students.[223][224] MIT ultimately prevailed when the Justice Department dropped the case in 1994.[225][226]		MIT's proximity[f] to Harvard University ("the other school up the river") has led to a substantial number of research collaborations such as the Harvard-MIT Division of Health Sciences and Technology and the Broad Institute.[227] In addition, students at the two schools can cross-register for credits toward their own school's degrees without any additional fees.[227] A cross-registration program between MIT and Wellesley College has also existed since 1969, and in 2002 the Cambridge–MIT Institute launched an undergraduate exchange program between MIT and the University of Cambridge.[227] MIT has more modest cross-registration programs with Boston University, Brandeis University, Tufts University, Massachusetts College of Art, and the School of the Museum of Fine Arts, Boston.[227]		MIT maintains substantial research and faculty ties with independent research organizations in the Boston area, such as the Charles Stark Draper Laboratory, the Whitehead Institute for Biomedical Research, and the Woods Hole Oceanographic Institution. Ongoing international research and educational collaborations include the Amsterdam Institute for Advanced Metropolitan Solutions (AMS Institute), Singapore-MIT Alliance, MIT-Politecnico di Milano,[227][228] MIT-Zaragoza International Logistics Program, and projects in other countries through the MIT International Science and Technology Initiatives (MISTI) program.[227][229]		The mass-market magazine Technology Review is published by MIT through a subsidiary company, as is a special edition that also serves as an alumni magazine.[230][231] The MIT Press is a major university press, publishing over 200 books and 30 journals annually, emphasizing science and technology as well as arts, architecture, new media, current events, and social issues.[232]		The MIT library system consists of five subject libraries: Barker (Engineering), Dewey (Economics), Hayden (Humanities and Science), Lewis (Music), and Rotch (Arts and Architecture). There are also various specialized libraries and archives. The libraries contain more than 2.9 million printed volumes, 2.4 million microforms, 49,000 print or electronic journal subscriptions, and 670 reference databases. The past decade has seen a trend of increased focus on digital over print resources in the libraries.[233] Notable collections include the Lewis Music Library with an emphasis on 20th and 21st-century music and electronic music,[234] the List Visual Arts Center's rotating exhibitions of contemporary art,[235] and the Compton Gallery's cross-disciplinary exhibitions.[236] MIT allocates a percentage of the budget for all new construction and renovation to commission and support its extensive public art and outdoor sculpture collection.[237][238]		The MIT Museum was founded in 1971 and collects, preserves, and exhibits artifacts significant to the culture and history of MIT. The museum now engages in significant educational outreach programs for the general public, including the annual Cambridge Science Festival, the first celebration of this kind in the United States. Since 2005, its official mission has been, "to engage the wider community with MIT's science, technology and other areas of scholarship in ways that will best serve the nation and the world in the 21st century".[239]		MIT was elected to the Association of American Universities in 1934 and remains a research university with a very high level of research activity;[47][162] research expenditures totaled $718.2 million in 2009.[needs update][240] The federal government was the largest source of sponsored research, with the Department of Health and Human Services granting $255.9 million, Department of Defense $97.5 million, Department of Energy $65.8 million, National Science Foundation $61.4 million, and NASA $27.4 million.[240] MIT employs approximately 1300 researchers in addition to faculty.[241] In 2011, MIT faculty and researchers disclosed 632 inventions, were issued 153 patents, earned $85.4 million in cash income, and received $69.6 million in royalties.[242] Through programs like the Deshpande Center, MIT faculty leverage their research and discoveries into multi-million-dollar commercial ventures.[243]		In electronics, magnetic core memory, radar, single electron transistors, and inertial guidance controls were invented or substantially developed by MIT researchers.[244][245] Harold Eugene Edgerton was a pioneer in high speed photography and sonar.[246][247] Claude E. Shannon developed much of modern information theory and discovered the application of Boolean logic to digital circuit design theory.[248] In the domain of computer science, MIT faculty and researchers made fundamental contributions to cybernetics, artificial intelligence, computer languages, machine learning, robotics, and cryptography.[245][249] At least nine Turing Award laureates and seven recipients of the Draper Prize in engineering have been or are currently associated with MIT.[250][251]		Current and previous physics faculty have won eight Nobel Prizes,[252] four Dirac Medals,[253] and three Wolf Prizes predominantly for their contributions to subatomic and quantum theory.[254] Members of the chemistry department have been awarded three Nobel Prizes and one Wolf Prize for the discovery of novel syntheses and methods.[252] MIT biologists have been awarded six Nobel Prizes for their contributions to genetics, immunology, oncology, and molecular biology.[252] Professor Eric Lander was one of the principal leaders of the Human Genome Project.[255][256] Positronium atoms,[257] synthetic penicillin,[258] synthetic self-replicating molecules,[259] and the genetic bases for Amyotrophic lateral sclerosis (also known as ALS or Lou Gehrig's disease) and Huntington's disease were first discovered at MIT.[260] Jerome Lettvin transformed the study of cognitive science with his paper "What the frog's eye tells the frog's brain".[261] Researchers developed a system to convert MRI scans into 3D printed physical models.[262]		In the domain of humanities, arts, and social sciences, MIT economists have been awarded five Nobel Prizes and nine John Bates Clark Medals.[252][263] Linguists Noam Chomsky and Morris Halle authored seminal texts on generative grammar and phonology.[264][265] The MIT Media Lab, founded in 1985 within the School of Architecture and Planning and known for its unconventional research,[266][267] has been home to influential researchers such as constructivist educator and Logo creator Seymour Papert.[268]		Spanning many of the above fields, MacArthur Fellowships (the so-called "Genius Grants") have been awarded to 38 people associated with MIT.[269] Four Pulitzer Prize–winning writers currently work at or have retired from MIT.[270] Four current or former faculty are members of the American Academy of Arts and Letters.[271]		Allegations of research misconduct or improprieties have received substantial press coverage. Professor David Baltimore, a Nobel Laureate, became embroiled in a misconduct investigation starting in 1986 that led to Congressional hearings in 1991.[272][273] Professor Ted Postol has accused the MIT administration since 2000 of attempting to whitewash potential research misconduct at the Lincoln Lab facility involving a ballistic missile defense test, though a final investigation into the matter has not been completed.[274][275] Associate Professor Luk Van Parijs was dismissed in 2005 following allegations of scientific misconduct and found guilty of the same by the United States Office of Research Integrity in 2009.[276][277]		[290][291]		The faculty and student body place a high value on meritocracy and on technical proficiency.[292][293] MIT has never awarded an honorary degree, nor does it award athletic scholarships, ad eundem degrees, or Latin honors upon graduation.[294] However, MIT has twice awarded honorary professorships: to Winston Churchill in 1949 and Salman Rushdie in 1993.[295]		Many upperclass students and alumni wear a large, heavy, distinctive class ring known as the "Brass Rat".[296][297] Originally created in 1929, the ring's official name is the "Standard Technology Ring."[298] The undergraduate ring design (a separate graduate student version exists as well) varies slightly from year to year to reflect the unique character of the MIT experience for that class, but always features a three-piece design, with the MIT seal and the class year each appearing on a separate face, flanking a large rectangular bezel bearing an image of a beaver.[296] The initialism IHTFP, representing the informal school motto "I Hate This Fucking Place" and jocularly euphemized as "I Have Truly Found Paradise," "Institute Has The Finest Professors," "It's Hard to Fondle Penguins," and other variations, has occasionally been featured on the ring given its historical prominence in student culture.[299]		MIT has over 500 recognized student activity groups,[300] including a campus radio station, The Tech student newspaper, an annual entrepreneurship competition, and weekly screenings of popular films by the Lecture Series Committee. Less traditional activities include the "world's largest open-shelf collection of science fiction" in English, a model railroad club, and a vibrant folk dance scene. Students, faculty, and staff are involved in over 50 educational outreach and public service programs through the MIT Museum, Edgerton Center, and MIT Public Service Center.[301]		The Independent Activities Period is a four-week-long "term" offering hundreds of optional classes, lectures, demonstrations, and other activities throughout the month of January between the Fall and Spring semesters. Some of the most popular recurring IAP activities are the 6.270, 6.370, and MasLab competitions,[302] the annual "mystery hunt",[303] and Charm School.[304][305] More than 250 students pursue externships annually at companies in the US and abroad.[306][307]		Many MIT students also engage in "hacking", which encompasses both the physical exploration of areas that are generally off-limits (such as rooftops and steam tunnels), as well as elaborate practical jokes.[308][309] Recent high-profile hacks have included the abduction of Caltech's cannon,[310] reconstructing a Wright Flyer atop the Great Dome,[311] and adorning the John Harvard statue with the Master Chief's Mjölnir Helmet.[312]		MIT sponsors 31 varsity sports and has one of the three broadest NCAA Division III athletic programs.[313][314]  MIT participates in the NCAA's Division III, the New England Women's and Men's Athletic Conference, the New England Football Conference, NCAA's Division I Eastern Association of Women's Rowing Colleges (EAWRC) for women's crew, and the Collegiate Water Polo Association (CWPA) for Men's Water Polo. Men's crew competes outside the NCAA in the Eastern Association of Rowing Colleges (EARC). In April 2009, budget cuts led to MIT eliminating eight of its 41 sports, including the mixed men's and women's teams in alpine skiing and pistol; separate teams for men and women in ice hockey and gymnastics; and men's programs in golf and wrestling.[315][316]		MIT enrolled 4,384 undergraduates and 6,510 graduate students in 2011–2012.[needs update][169] Women constituted 45 percent of undergraduate students.[needs update][169][319] Undergraduate and graduate students were drawn from all 50 states as well as 115 foreign countries.[320]		MIT received 17,909 applications for admission to the undergraduate Class of 2015; 1,742 were admitted (9.7 percent) and 1128 enrolled (64.8 percent).[needs update][142] 19,446 applications were received for graduate and advanced degree program across all departments; 2,991 were admitted (15.4 percent) and 1,880 enrolled (62.8 percent).[needs update][321]		The interquartile range on the SAT was 2090–2340 and 97 percent of students ranked in the top tenth of their high school graduating class.[needs update][142] 97 percent of the Class of 2012 returned as sophomores; 82 percent of the Class of 2007 graduated within 4 years, and 93 percent (91 percent of the men and 95 percent of the women) graduated within 6 years.[142][322]		Undergraduate tuition and fees total $40,732 and annual expenses are estimated at $52,507 as of 2012.[needs update] 62 percent of students received need-based financial aid in the form of scholarships and grants from federal, state, institutional, and external sources averaging $38,964 per student.[needs update][323] Students were awarded a total of $102 million in scholarships and grants, primarily from institutional support ($84 million).[142] The annual increase in expenses has led to a student tradition (dating back to the 1960s) of tongue-in-cheek "tuition riots".[324]		MIT has been nominally co-educational since admitting Ellen Swallow Richards in 1870. Richards also became the first female member of MIT's faculty, specializing in sanitary chemistry.[325][326] Female students remained a minority prior to the completion of the first wing of a women's dormitory, McCormick Hall, in 1963.[327][328][329] Between 1993 and 2009, the proportion of women rose from 34 percent to 45 percent of undergraduates and from 20 percent to 31 percent of graduate students.[169][330] Women currently outnumber men in Biology, Brain & Cognitive Sciences, Architecture, Urban Planning, and Biological Engineering.[169][319]		A number of student deaths in the late 1990s and early 2000s resulted in considerable media attention to MIT's culture and student life.[331][332] After the alcohol-related death of Scott Krueger in September 1997 as a new member at the Phi Gamma Delta fraternity,[333] MIT began requiring all freshmen to live in the dormitory system.[333][334] The 2000 suicide of MIT undergraduate Elizabeth Shin drew attention to suicides at MIT and created a controversy over whether MIT had an unusually high suicide rate.[335][336] In late 2001 a task force's recommended improvements in student mental health services were implemented,[337][338] including expanding staff and operating hours at the mental health center.[339] These and later cases were significant as well because they sought to prove the negligence and liability of university administrators in loco parentis.[335]		As of 2013[update], MIT had 1,030 faculty members, of whom 225 were women.[4] Faculty are responsible for lecturing classes, advising both graduate and undergraduate students, and sitting on academic committees, as well as conducting original research. Between 1964 and 2009, a total of seventeen faculty and staff members affiliated with MIT were awarded Nobel Prizes (thirteen in the last 25 years).[340] MIT faculty members past or present have won a total of twenty-seven Nobel Prizes, the majority in Economics or Physics.[341] As of October 2013[update], among current faculty and teaching staff there are 67 Guggenheim Fellows, 6 Fulbright Scholars, and 22 MacArthur Fellows.[4] Faculty members who have made extraordinary contributions to their research field as well as the MIT community are granted appointments as Institute Professors for the remainder of their tenures.		A 1998 MIT study concluded that a systemic bias against female faculty existed in its School of Science,[342] although the study's methods were controversial.[343][344] Since the study, though, women have headed departments within the Schools of Science and of Engineering, and MIT has appointed several female vice presidents, although allegations of sexism continue to be made.[345] Susan Hockfield, a molecular neurobiologist, was MIT's president from 2004 to 2012 and was the first woman to hold the post.[161]		Tenure outcomes have vaulted MIT into the national spotlight on several occasions. The 1984 dismissal of David F. Noble, a historian of technology, became a cause célèbre about the extent to which academics are granted freedom of speech after he published several books and papers critical of MIT's and other research universities' reliance upon financial support from corporations and the military.[346] Former materials science professor Gretchen Kalonji sued MIT in 1994 alleging that she was denied tenure because of sexual discrimination. Several years later, the lawsuit was settled with undisclosed payments, and establishment of a project to encourage women and minorities to seek faculty positions.[345][347][348] In 1997, the Massachusetts Commission Against Discrimination issued a probable cause finding supporting UMass Boston Professor James Jennings' allegations of racial discrimination after a senior faculty search committee in the Department of Urban Studies and Planning did not offer him reciprocal tenure.[349]		In 2006–2007, MIT's denial of tenure to African-American stem cell scientist professor James Sherley reignited accusations of racism in the tenure process, eventually leading to a protracted public dispute with the administration, a brief hunger strike, and the resignation of Professor Frank L. Douglas in protest.[350][351] The Boston Globe reported on February 6, 2007: "Less than half of MIT's junior faculty members are granted tenure. After Sherley was initially denied tenure, his case was examined three times before the university established that neither racial discrimination nor conflict of interest affected the decision. Twenty-one of Sherley's colleagues later issued a statement saying that the professor was treated fairly in tenure review."[352]		MIT faculty members have often been recruited to lead other colleges and universities. Founding faculty member Charles W. Eliot was recruited in 1869 to become president of Harvard University, a post he would hold for 40 years, during which he wielded considerable influence on both American higher education and secondary education. MIT alumnus and faculty member George Ellery Hale played a central role in the development of the California Institute of Technology (Caltech), and other faculty members have been key founders of Franklin W. Olin College of Engineering in nearby Needham, Massachusetts.		As of 2014[update], former provost Robert A. Brown is president of Boston University; former provost Mark Wrighton is chancellor of Washington University in St. Louis; former associate provost Alice Gast is president of Lehigh University; and former professor Suh Nam-pyo is president of KAIST. Former dean of the School of Science Robert J. Birgeneau was the chancellor of the University of California, Berkeley (2004–2013); former professor John Maeda was president of Rhode Island School of Design (RISD, 2008–2013); former professor David Baltimore was president of Caltech (1997–2006); and MIT alumnus and former assistant professor Hans Mark served as chancellor of the University of Texas system (1984–1992).		In addition, faculty members have been recruited to lead governmental agencies; for example, former professor Marcia McNutt is president of the National Academy of Sciences,[353] urban studies professor Xavier de Souza Briggs is currently the associate director of the White House Office of Management and Budget,[354] and biology professor Eric Lander was a co-chair of the President's Council of Advisors on Science and Technology.[355] In 2013, faculty member Ernest Moniz was nominated by President Obama and later confirmed as United States Secretary of Energy.[356][357] Former professor Hans Mark served as Secretary of the Air Force from 1979 to 1981. Alumna and Institute Professor Sheila Widnall served as Secretary of the Air Force between 1993 and 1997, making her the first female Secretary of the Air Force and first woman to lead an entire branch of the US military in the Department of Defense.		As of 2017[update], MIT was the second-largest employer in the city of Cambridge.[122] Based on feedback from employees, MIT was ranked #7 as a place to work, among US colleges and universities as of 2013[update].[358] Surveys cited a "smart", "creative", "friendly" environment, noting that the work-life balance tilts towards a "strong work ethic" but complaining about "low pay" compared to an industry position.[359]		Many of MIT's over 120,000 alumni have had considerable success in scientific research, public service, education, and business. As of 2014[update], 27 MIT alumni have won the Nobel Prize, 47 have been selected as Rhodes Scholars, and 61 have been selected as Marshall Scholars.[360]		Alumni in American politics and public service include former Chairman of the Federal Reserve Ben Bernanke, former MA-1 Representative John Olver, former CA-13 Representative Pete Stark, former National Economic Council chairman Lawrence H. Summers, and former Council of Economic Advisors chairwoman Christina Romer. MIT alumni in international politics include Foreign Affairs Minister of Iran Ali Akbar Salehi, Israeli Prime Minister Benjamin Netanyahu, President of Colombia Virgilio Barco Vargas, President of the European Central Bank Mario Draghi, former Governor of the Reserve Bank of India Raghuram Rajan, former British Foreign Minister David Miliband, former Greek Prime Minister Lucas Papademos, former UN Secretary General Kofi Annan, former Iraqi Deputy Prime Minister Ahmed Chalabi, former Minister of Education and Culture of The Republic of Indonesia Yahya Muhaimin.		MIT alumni founded or co-founded many notable companies, such as Intel, McDonnell Douglas, Texas Instruments, 3Com, Qualcomm, Bose, Raytheon, Koch Industries, Rockwell International, Genentech, Dropbox, and Campbell Soup. According to the British newspaper, The Guardian, "a survey of living MIT alumni found that they have formed 25,800 companies, employing more than three million people including about a quarter of the workforce of Silicon Valley. Those firms collectively generate global revenues of about $1.9 trillion (£1.2 trillion) a year. If MIT were a country, it would have the 11th highest GDP of any nation in the world."[361][362][363]		Prominent institutions of higher education have been led by MIT alumni, including the University of California system, Harvard University, New York Institute of Technology, Johns Hopkins University, Carnegie Mellon University, Tufts University, Rochester Institute of Technology, Rhode Island School of Design (RISD), Northeastern University, Lahore University of Management Sciences, Rensselaer Polytechnic Institute, Tecnológico de Monterrey, Purdue University, Virginia Polytechnic Institute, KAIST, and Quaid-e-Azam University. Berklee College of Music, the largest independent college of contemporary music in the world, was founded and led by MIT alumnus Lawrence Berk for more than three decades.		More than one third of the United States' manned spaceflights have included MIT-educated astronauts (among them Apollo 11 Lunar Module Pilot Buzz Aldrin), more than any university excluding the United States service academies.[364] Alumnus and former faculty member Qian Xuesen was instrumental in the PRC rocket program.[365]		Noted alumni in non-scientific fields include author Hugh Lofting,[366] sculptor Daniel Chester French, guitarist Tom Scholz of the band Boston, the British BBC and ITN correspondent and political advisor David Walter, The New York Times columnist and Nobel Prize Winning economist Paul Krugman, The Bell Curve author Charles Murray, United States Supreme Court building architect Cass Gilbert,[367] Pritzker Prize-winning architects I.M. Pei and Gordon Bunshaft.		Apollo 11 astronaut Buzz Aldrin, ScD 1963 (Aero & Astro)		Former UN Secretary-General Kofi Annan, SM 1972 (Management)		President of Colombia 1986-1990 Virgilio Barco Vargas, SB 1943 (Civil Engineering)		Former Federal Reserve Bank chairman Ben Bernanke, PhD 1979 (Economics)		Physicist Nobel laureate Richard Feynman, SB 1939 (Physics)		Economics Nobel laureate Paul Krugman, PhD 1977 (Economics)		Biologist, suffragist, philanthropist Katherine Dexter McCormick (left), SB 1904 (Biology)		Challenger astronaut and physicist Ronald McNair, PhD 1976 (Physics)		Israeli Prime Minister Benjamin Netanyahu, SB 1975 (Architecture), SM 1976 (Management)		Architect I. M. Pei, BArch 1940 (Architecture)		CEO of General Motors Alfred P. Sloan, SB 1895 (Electrical Engineering)		"Boston" guitarist Tom Scholz, SB 1969, SM 1970 (Mechanical Engineering)				Coordinates: 42°21′35″N 71°05′32″W﻿ / ﻿42.35982°N 71.09211°W﻿ / 42.35982; -71.09211		
Science fiction (often shortened to SF, sci-fi or scifi) is a genre of speculative fiction, typically dealing with imaginative concepts such as futuristic science and technology, space travel, time travel, faster than light travel, parallel universes, and extraterrestrial life. Science fiction often explores the potential consequences of scientific and other innovations, and has been called a "literature of ideas".[1] It usually avoids the supernatural, and unlike the related genre of fantasy, historically, science-fiction stories were intended to have a grounding in science-based fact or theory at the time the story was created, but this connection is now limited to hard science fiction.[2]						Science fiction is difficult to define, as it includes a wide range of subgenres and themes. Author and editor Damon Knight summed up the difficulty, saying "science fiction is what we point to when we say it",[3] a definition echoed by author Mark C. Glassy, who argues that the definition of science fiction is like the definition of pornography: you do not know what it is, but you know it when you see it.[4]		Hugo Gernsback, who suggested the term "scientifiction" for his Amazing Stories magazine,[5] described his vision of the genre: "By 'scientifiction' I mean the Jules Verne, H. G. Wells and Edgar Allan Poe type of story—a charming romance intermingled with scientific fact and prophetic vision."[6]		In 1970 or 1971, William Atheling Jr. (James Blish) wrote about the English term "science fiction": "Wells used the term originally to cover what we would today call ‘hard’ science fiction, in which a conscientious attempt to be faithful to already known facts (as of the date of writing) was the substrate on which the story was to be built, and if the story was also to contain a miracle, it ought at least not to contain a whole arsenal of them."[7]		According to science fiction writer Robert A. Heinlein, "a handy short definition of almost all science fiction might read: realistic speculation about possible future events, based solidly on adequate knowledge of the real world, past and present, and on a thorough understanding of the nature and significance of the scientific method."[8] Rod Serling's definition is "fantasy is the impossible made probable. Science fiction is the improbable made possible."[9] Lester del Rey wrote, "Even the devoted aficionado—or fan—has a hard time trying to explain what science fiction is", and that the reason for there not being a "full satisfactory definition" is that "there are no easily delineated limits to science fiction."[10]		Science fiction is largely based on writing rationally about alternative possible worlds or futures.[11] It is related to, but different from fantasy in that, within the context of the story, its imaginary elements are largely possible within scientifically established or scientifically postulated physical laws (though some elements in a story might still be pure imaginative speculation).		The settings of science fiction are often contrary to those of consensus reality, but most science fiction relies on a considerable degree of suspension of disbelief, which is facilitated in the reader's mind by potential scientific explanations or solutions to various fictional elements. Science fiction elements include:		As a means of understanding the world through speculation and storytelling, science fiction has antecedents which go back to an era when the dividing line separating the mythological from the historical tended to become somewhat blurred. A True Story, written in the 2nd century AD by the Hellenized Syrian satirist Lucian, contains many themes and tropes that are characteristic of modern science fiction, including travel to other worlds, extraterrestrial lifeforms, interplanetary warfare, and artificial life, and is considered by some to be the first science fiction novel.[16] Some of the stories from The Arabian Nights,[17][18] along with the 10th century The Tale of the Bamboo Cutter[18] and Ibn al-Nafis's 13th century Theologus Autodidactus[19] also contain elements of science fiction.		A product of the budding Age of Reason and the development of modern science itself, Johannes Kepler's Somnium (1620–1630). Francis Bacon's The New Atlantis (1627),[20] Cyrano de Bergerac's Comical History of the States and Empires of the Moon (1657), his The States and Empires of the Sun (1662), Margaret Cavendish's "The Blazing World" (1666),[21] Jonathan Swift's Gulliver's Travels (1726), Ludvig Holberg's novel Nicolai Klimii Iter Subterraneum (1741) and Voltaire's Micromégas (1752) are some of the first true science fantasy works,[22][23] which often feature the adventures of the protagonist in fictional and fantastical places, or the moon. Isaac Asimov and Carl Sagan considered Kepler's work the first science fiction story.[24] It depicts a journey to the Moon and how the Earth's motion is seen from there.		Following the 18th-century development of the novel as a literary form, in the early 19th century, Mary Shelley's books Frankenstein (1818) and The Last Man (1826) helped define the form of the science fiction novel, and Brian Aldiss has argued that Frankenstein was the first work of science fiction.[25] Later, Edgar Allan Poe wrote a story about a flight to the moon.[26] More examples appeared throughout the 19th century.		Then with the dawn of new technologies such as electricity, the telegraph, and new forms of powered transportation, writers including H. G. Wells and Jules Verne created a body of work that became popular across broad cross-sections of society.[27] Wells' The War of the Worlds (1898) describes an invasion of late Victorian England by Martians using tripod fighting machines equipped with advanced weaponry. It is a seminal depiction of an alien invasion of Earth.		In the late 19th century, the term "scientific romance" was used in Britain to describe much of this fiction. This produced additional offshoots, such as the 1884 novella Flatland: A Romance of Many Dimensions by Edwin Abbott Abbott. The term would continue to be used into the early 20th century for writers such as Olaf Stapledon.		In the early 20th century, pulp magazines helped develop a new generation of mainly American SF writers, influenced by Hugo Gernsback, the founder of Amazing Stories magazine.[28] In 1912, Edgar Rice Burroughs published A Princess of Mars, the first of his three-decade-long series of Barsoom novels, situated on Mars and featuring John Carter as the hero. The 1928 publication of Philip Francis Nowlan's original Buck Rogers story, Armageddon 2419, in Amazing Stories was a landmark event. This story led to comic strips featuring Buck Rogers (1929), Brick Bradford (1933), and Flash Gordon (1934). The comic strips and derivative movie serials greatly popularized science fiction.		In the late 1930s, John W. Campbell became editor of Astounding Science Fiction, and a critical mass of new writers emerged in New York City in a group called the Futurians, including Isaac Asimov, Damon Knight, Donald A. Wollheim, Frederik Pohl, James Blish, Judith Merril, and others.[29] Other important writers during this period include E.E. (Doc) Smith, Robert A. Heinlein, Arthur C. Clarke, Olaf Stapledon, and A. E. van Vogt. Working outside the Campbell influence were Ray Bradbury and Stanisław Lem. Campbell's tenure at Astounding is considered to be the beginning of the Golden Age of science fiction, characterized by hard SF stories celebrating scientific achievement and progress.[28] This lasted until post-war technological advances, new magazines such as Galaxy, edited by H. L. Gold, and a new generation of writers began writing stories with less emphasis on the hard sciences and more on the social sciences.		In the 1950s, the Beat generation included speculative writers such as William S. Burroughs. In the 1960s and early 1970s, writers like Frank Herbert, Samuel R. Delany, Roger Zelazny, and Harlan Ellison explored new trends, ideas, and writing styles, while a group of writers, mainly in Britain, became known as the New Wave for their embrace of a high degree of experimentation, both in form and in content, and a highbrow and self-consciously "literary" or artistic sensibility.[22] In the 1970s, writers like Larry Niven brought new life to hard science fiction.[30] Ursula K. Le Guin and others pioneered soft science fiction.[31]		In the 1980s, cyberpunk authors like William Gibson turned away from the optimism and support for progress of traditional science fiction.[32] This dystopian vision of the near future is described in the work of Philip K. Dick, such as Do Androids Dream of Electric Sheep? and We Can Remember It for You Wholesale, which resulted in the films Blade Runner and Total Recall. The Star Wars franchise helped spark a new interest in space opera.[33] C. J. Cherryh's detailed explorations of alien life and complex scientific challenges influenced a generation of writers.[34]		Emerging themes in the 1990s included environmental issues, the implications of the global Internet and the expanding information universe, questions about biotechnology and nanotechnology, as well as a post-Cold War interest in post-scarcity societies; Neal Stephenson's The Diamond Age comprehensively explores these themes. Lois McMaster Bujold's Vorkosigan novels brought the character-driven story back into prominence.[35] The television series Star Trek: The Next Generation (1987) began a torrent of new SF shows, including three further Star Trek continuation shows (Deep Space 9, Voyager and Enterprise) and Babylon 5.[36] Stargate, a movie about an ancient portal to other gates across the galaxy, was released in 1994. Stargate SG-1, a TV series, premiered on July 27, 1997 and lasted 10 seasons with 214 episodes. Spin-offs include the animated television series Stargate Infinity, the TV series Stargate Atlantis and Stargate Universe, and the direct-to-DVD films Stargate: The Ark of Truth and Stargate: Continuum. Stargate SG-1 surpassed The X-Files as the longest-running North American science fiction television series, a record later broken by Smallville.[37]		Concern about the rapid pace of technological change crystallized around the concept of the technological singularity, popularized by Vernor Vinge's novel Marooned in Realtime and then taken up by other authors.[38]		Forrest J Ackerman is credited with first using the term sci-fi (analogous to the then-trendy "hi-fi") in 1954.[39] As science fiction entered popular culture, writers and fans active in the field came to associate the term with low-budget, low-tech "B-movies" and with low-quality pulp science fiction.[40] By the 1970s, critics within the field such as Terry Carr and Damon Knight were using sci-fi to distinguish hack-work from serious science fiction.[41]		Around 1978, critic Susan Wood and others introduced the use of the odd pronunciation "skiffy" which is intended to be self-deprecating humor but is inconsistent with the documented genesis of the term "sci-fi" (e.g., one would not pronounce "hi-fi" as "hiffy") and Ackerman's own words engraved on his crypt plaque which read "Sci-Fi was My High".[42]		Peter Nicholls writes that "SF" (or "sf") is "the preferred abbreviation within the community of sf writers and readers."[43] David Langford's monthly fanzine Ansible includes a regular section "As Others See Us" which offers numerous examples of "sci-fi" being used in a pejorative sense by people outside the genre.[44]		Science fiction has criticized developing and future technologies, but also initiates innovation and new technology. This topic has been more often discussed in literary and sociological than in scientific forums. Cinema and media theorist Vivian Sobchack examines the dialogue between science fiction films and the technological imagination. Technology impacts artists and how they portray their fictionalized subjects, but the fictional world gives back to science by broadening imagination. How William Shatner Changed the World is a documentary that gave a number of real-world examples of actualized technological imaginations. While more prevalent in the early years of science fiction with writers like Arthur C. Clarke, new authors still find ways to make currently impossible technologies seem closer to being realized.[45]		Hard science fiction, or "hard SF", is characterized by rigorous attention to accurate detail in the natural sciences, especially physics, astrophysics, and chemistry, or on accurately depicting worlds that more advanced technology may make possible. Some accurate predictions of the future come from the hard science fiction subgenre, but numerous inaccurate predictions have emerged as well.[citation needed] Some hard SF authors have distinguished themselves as working scientists, including Gregory Benford, Fred Hoyle, Geoffrey A. Landis, David Brin,[46] and Robert L. Forward, while mathematician authors include Rudy Rucker and Vernor Vinge. Other noteworthy hard SF authors include Isaac Asimov, Arthur C. Clarke, Hal Clement, Greg Bear, Larry Niven, Robert J. Sawyer, Stephen Baxter, Alastair Reynolds, Charles Sheffield, Ben Bova, Kim Stanley Robinson, Anne McCaffrey, Andy Weir and Greg Egan.		The description "soft" science fiction may describe works based on social sciences such as psychology, economics, political science, sociology, and anthropology. Noteworthy writers in this category include Ursula K. Le Guin and Philip K. Dick.[28][47] The term can describe stories focused primarily on character and emotion; SFWA Grand Master Ray Bradbury was an acknowledged master of this art.[48] The Eastern Bloc produced a large quantity of social science fiction, including works by Polish authors Stanislaw Lem and Janusz Zajdel, as well as Soviet authors such as the Strugatsky brothers, Kir Bulychov, Yevgeny Zamyatin and Ivan Yefremov.[49] Some writers blur the boundary between hard and soft science fiction.		Related to social SF and soft SF are utopian and dystopian stories; George Orwell's Nineteen Eighty-Four, Aldous Huxley's Brave New World, and Margaret Atwood's The Handmaid's Tale and Oryx and Crake are examples.		The cyberpunk genre emerged in the early 1980s; combining cybernetics and punk,[50] the term was coined by author Bruce Bethke for his 1980 short story Cyberpunk.[51] The time frame is usually near-future and the settings are often dystopian in nature and characterized by misery. Common themes in cyberpunk include advances in information technology and especially the Internet, visually abstracted as cyberspace, artificial intelligence, and cybernetics and post-democratic societal control where corporations have more influence than governments. Nihilism, post-modernism, and film noir techniques are common elements, and the protagonists may be disaffected or reluctant anti-heroes. Noteworthy authors in this genre are William Gibson, Bruce Sterling, Neal Stephenson, and Pat Cadigan. James O'Ehley has called the 1982 film Blade Runner a definitive example of the cyberpunk visual style.[52]		Time-travel stories have antecedents in the 18th and 19th centuries. The first major time-travel novel was Mark Twain's A Connecticut Yankee in King Arthur's Court. The most famous is H. G. Wells' 1895 novel The Time Machine, which uses a vehicle that allows an operator to travel purposefully and selectively, while Twain's time traveler is struck in the head. The term time machine, coined by Wells, is now universally used to refer to such a vehicle. Back to the Future is one of the most popular movie franchises in this category; Doctor Who is a similarly popular long-running television franchise. Stories of this type are complicated by logical problems such as the grandfather paradox,[53] as exemplified in the classic Robert Heinlein story "—All You Zombies—" and the Futurama episode "Roswell That Ends Well". Time travel continues to be a popular subject in modern science fiction, in print, movies, and television.		Alternative history stories are based on the premise that historical events might have turned out differently. These stories may use time travel to change the past, or may simply set a story in a universe with a different history from our own. Classics in the genre include Bring the Jubilee by Ward Moore, in which the South wins the American Civil War, and The Man in the High Castle by Philip K. Dick, in which Germany and Japan win World War II. The Sidewise Award acknowledges the best works in this subgenre; the name is taken from Murray Leinster's 1934 story Sidewise in Time. Harry Turtledove is one of the most prominent authors in the subgenre and is sometimes called the "master of alternate history".[54]		Military science fiction is set in the context of conflict between national, interplanetary, or interstellar armed forces; the primary viewpoint characters are usually soldiers. Stories include detail about military technology, procedure, ritual, and history; military stories may use parallels with historical conflicts. Heinlein's Starship Troopers is an early example, along with the Dorsai novels of Gordon Dickson. Joe Haldeman's The Forever War is a critique of the genre, a Vietnam-era response to the World War II–style stories of earlier authors.[55] Other military science fiction authors include John Scalzi, John Ringo, David Drake, David Weber, Tom Kratman, Michael Z. Williamson, S. M. Stirling, and John Carr. The publishing company Baen Books is known for cultivating several of these military science fiction authors.[56]		Superhuman stories deal with the emergence of humans who have abilities beyond the normal. This can stem either from natural causes such as in Olaf Stapledon's novel Odd John, Theodore Sturgeon's More Than Human, and Philip Wylie's Gladiator, or be the result of scientific advances, such as the intentional augmentation in A. E. van Vogt's novel Slan. These stories usually focus on the alienation that these beings feel as well as society's reaction to them. These stories have played a role in the real life discussion of human enhancement. Frederik Pohl's Man Plus also belongs to this category.		Apocalyptic fiction is concerned with the end of civilization through war (On the Beach), pandemic (The Last Man), astronomic impact (When Worlds Collide), ecological disaster (The Wind from Nowhere), or some other general disaster or with a world or civilization after such a disaster. Typical of the genre are George R. Stewart's novel Earth Abides and Pat Frank's novel Alas, Babylon. Apocalyptic fiction generally concerns the disaster itself and the direct aftermath, while post-apocalyptic fiction can deal with anything from the near aftermath (as in Cormac McCarthy's The Road) and Margaret Atwood's Oryx and Crake to centuries in the future (as in Stephen Vincent Benét's By the Waters of Babylon and Octavia Butler's Lilith's Brood) to hundreds or thousands of years in the future, as in Russell Hoban's novel Riddley Walker and Walter M. Miller, Jr.'s A Canticle for Leibowitz. Apocalyptic science-fiction is a popular genre in video games. The critically acclaimed, role-playing, action-adventure, video-game series Fallout is set on a post-apocalyptic Earth, where civilization is recovering from a nuclear war as survivors struggle to survive and seek to rebuild society.		Space opera is adventure science fiction set mainly or entirely in outer space or on sometimes distant planets. The conflict is heroic, and typically on a large scale. It is also used nostalgically, and modern space opera may be an attempt to recapture the sense of wonder of the golden age of science fiction. The pioneer of this subgenre is generally recognized to be Edward E. (Doc) Smith, with his Skylark and Lensman series.[citation needed] Isaac Asimov built his Foundation series along these lines. George Lucas's Star Wars series is among the most popular and famous franchises in cinematic space opera. It covers epic battles between good and evil throughout an entire galaxy. Alastair Reynolds's Revelation Space series, Peter F. Hamilton's Void, Night's Dawn, Pandora's Star series, Stephen Hunt's Sliding Void series, Vernor Vinge's A Fire Upon the Deep, A Deepness in the Sky are newer examples of this genre.		The space Western transposes themes of American Western books and films to a backdrop of futuristic space frontiers. These stories typically involve colony worlds that have only recently been terraformed and/or settled, serving as stand-ins for the backdrop of lawlessness and economic expansion that were predominant in the American west. Examples include the Sean Connery film Outland, Heinlein's Farmer in the Sky, Sparks Nevada: Marshall on Mars from the Thrilling Adventure Hour, the Firefly television series, and the film sequel Serenity by Joss Whedon, the animated series BraveStarr, the videogame series Borderlands as well as the manga and anime series Cowboy Bebop, Outlaw Star, and Trigun.		Social science fiction focuses on themes of society and human nature in a science-fiction setting. Since it usually focuses more on speculation about humanity and less on scientific accuracy, it is usually placed within soft science fiction.		Climate fiction is a genre based around themes of reaction to major climate change. It is sometimes called "cli-fi", much as "science fiction" is often shortened to "sci-fi". Cli-fi novels and films are often set in either the present or the near or distant future, but they can also be set in the past. Many cli-fi works raise awareness about the major threats that global warming and climate change present to life on Earth.		Mundane science fiction is a subgenre that is set on Earth and does not include outer space adventures or alien life. Because it focuses on modern day aspects, it is typically associated with hard science fiction.		Biopunk focuses on biotechnology and subversives. The main underlying theme within these stories is the attempt to change the human body and engineer humans for specific purposes through enhancements in genetic and molecular makeups. Many examples of this subgenre include subjects such as human experimentation, the misuse of biotechnology and synthetic biotechnology. This subgenre also includes works involving human cloning and how clones might exist within human society in the future.		The broader category of speculative fiction[63] includes science fiction, fantasy, alternate histories (which may have no particular scientific or futuristic component), and even literary stories that contain fantastic elements, such as the work of Jorge Luis Borges or John Barth. For some editors, magic realism is considered to be within the broad definition of speculative fiction.[64]		Afrofuturism is a genre of science fiction and speculative fiction which consists largely of the work produced by members of the African diaspora in the Americas. The term "afrofuturism" was coined in 1993 by Mark Dery in his essay "Black to the Future."[65] Afrofuturism largely concerns itself with the social positioning of African Americans in the United States, in the shadow of the legacy left by Slavery in the United States, the Civil Rights Movement, and forms of institutionalized racism which extend into the twenty-first century. Afrofuturism's critiques of European colonization and the epistemological frameworks to have come therefrom link it to other pieces of literature to have emerged from the postcolonialism movement.		The topics which Afrofuturistic works concern themselves include, but are not limited to: the understanding of and resistance to political systems; the importance of scientific and technological advancement to a civilization; and the redefinition of African and African-American histories (especially African-American claims of historical racial ties to the civilization developed in Ancient Egypt). The alienation of individuals (such as women) and non-humans (such as cyborgs) are also of particular interest.		Afrofuturistic works encompass a variety of media and a timespan longer than the coining of the term in 1993. "For better or worse, I am often spoken of as the first African-American science fiction writer," Samuel R. Delany said in an interview with Dark Matter (prose anthologies),[66] recognizing, too, writers of "proto-science fiction" such as black nationalist Martin Delany and M. P. Shiel, a British writer of Creole descent. The musical albums of Sun Ra display a particularly afrofuturist bend, as does his 1972 science fiction film Space Is the Place, an 85-minute chronicle of his attempts to settle the world's black population on his planet. The music of Janelle Monáe has been compared to Sun Ra's and that of George Clinton (musician).		Fantasy is commonly associated with science fiction, and a number of writers have worked in both genres, while writers such as Anne McCaffrey, Ursula K. Le Guin, and Marion Zimmer Bradley have written works that appear to blur the boundary between the two related genres.[67] The authors' professional organization is called the Science Fiction and Fantasy Writers of America (SFWA).[68] SF conventions routinely have programming on fantasy topics,[69] and fantasy authors such as J. K. Rowling have won the highest honor within the science fiction field, the Hugo Award.[70]		In general, science fiction differs from fantasy in that the former concerns things that might someday be possible or that at least embody the pretense of realism. Supernaturalism, usually absent in science fiction, is the distinctive characteristic of fantasy literature. A dictionary definition referring to fantasy literature is "fiction characterized by highly fanciful or supernatural elements." [71] Examples of fantasy supernaturalism include magic (spells, harm to opponents), magical places (Narnia, Oz, Middle Earth, Hogwarts), supernatural creatures (witches, vampires, orcs, trolls), supernatural transportation (flying broomsticks, ruby slippers, windows between worlds), and shapeshifting (beast into man, man into wolf or bear, lion into sheep). Such things are basic themes in fantasy.[72]		Literary critic Fredric Jameson has characterized the difference between the two genres by describing science fiction as turning "on a formal framework determined by concepts of the mode of production rather than those of religion" – that is, science fiction texts are bound by an inner logic based more on historical materialism than on magic or the forces of good and evil.[73] Some narratives are described as being essentially science fiction but "with fantasy elements." The term "science fantasy" is sometimes used to describe such material.[74]		Science fantasy is a genre where elements of science fiction and fantasy co-exist or combine. Stories and franchises that display fictional science as well as supernatural elements, sorcery or/and "magical technology" are considered science fantasy.		Horror fiction is the literature of the unnatural and supernatural, with the aim of unsettling or frightening the reader, sometimes with graphic violence. Historically it has also been known as weird fiction. Although horror is not per se a branch of science fiction, some works of horror literature incorporates science fictional elements. One of the defining classical works of horror, Mary Shelley's novel Frankenstein, is the first fully realized work of science fiction, where the manufacture of the monster is given a rigorous science-fictional grounding. The works of Edgar Allan Poe also helped define both the science fiction and the horror genres.[75] Today horror is one of the most popular categories of films.[76] Horror is often mistakenly categorized as science fiction at the point of distribution by libraries, video rental outlets, etc.		Supernatural fiction is a genre that features supernatural and other paranormal phenomenon in stories and settings.		A subgenre of spy fiction that includes significant elements of science fiction.		Works in which science and technology are a dominant theme, but based on current reality, may be considered mainstream fiction. Much of the thriller genre would be included, such as the novels of Tom Clancy or Michael Crichton, or the James Bond films.[77] Modernist works from writers like Kurt Vonnegut, Philip K. Dick, and Stanisław Lem have focused on speculative or existential perspectives on contemporary reality and are on the borderline between SF and the mainstream.[78] According to Robert J. Sawyer, "Science fiction and mystery have a great deal in common. Both prize the intellectual process of puzzle solving, and both require stories to be plausible and hinge on the way things really do work."[79] Isaac Asimov, Walter Mosley, and other writers incorporate mystery elements in their science fiction, and vice versa.		Distinct from the above, a full-fledged Science Fiction Mystery is one which is set in a completely different world from ours, in which the circumstances and motives of the crime committed and the identity of the detective(s) seeking to solve it are of an essentially science fictional character. An example is Isaac Asimov's The Caves of Steel and its sequels, set in a world thousands of years in the future and presenting the Robot detective R. Daneel Olivaw. An allied genre is the Fantasy Mystery, a detective mystery set in a world of fantasy - such as the Lord Darcy mysteries taking place in a world where magic works, or The Idylls of the Queen set in the mythical King Arthur's court.		Superhero fiction is a genre characterized by beings with much higher than usual capability and prowess, generally with a desire or need to help the citizens of their chosen country or world by using their powers to defeat natural or superpowered threats. A number of superhero fiction characters involve themselves (either intentionally or accidentally) with science fiction and fact, including advanced technologies, alien worlds, time travel, and interdimensional travel; but the standards of scientific plausibility are lower than with actual science fiction. Authors of this genre include Stan Lee (co-creator of Spider-Man, the Fantastic Four, the Iron Man, the X-Men, and the Hulk); Marv Wolfman, the creator of Blade for Marvel Comics, and The New Teen Titans for DC Comics; Dean Wesley Smith (Smallville, Spider-Man, and X-Men novels) and Superman writers Roger Stern and Elliot S! Maggin.		Science fiction fandom is the "community of the literature of ideas... the culture in which new ideas emerge and grow before being released into society at large."[80] Members of this community, "fans", are in contact with each other at conventions or clubs, through print or online fanzines, or on the Internet using web sites, mailing lists, and other resources.		SF fandom emerged from the letters column in Amazing Stories magazine. Soon fans began writing letters to each other, and then grouping their comments together in informal publications that became known as fanzines.[81] Once they were in regular contact, fans wanted to meet each other, and they organized local clubs. In the 1930s, the first science fiction conventions gathered fans from a wider area.[82] Conventions, clubs, and fanzines were the dominant form of fan activity, or "fanac", for decades, until the Internet facilitated communication among a much larger population of interested people.		Science fiction is being written worldwide by a diverse population of authors. According to 2013 statistics by the science fiction publisher Tor Books, men outnumber women by 78% to 22% among submissions to the publisher.[83] A controversy about voting slates in the 2015 Hugo Awards highlighted tensions in the science fiction community between a trend of increasingly diverse works and authors being honored by awards, and a backlash by groups of authors and fans who preferred what they considered more traditional science fiction.[84]		Among the most respected awards for science fiction are the Hugo Award, presented by the World Science Fiction Society at Worldcon; the Nebula Award, presented by SFWA and voted on by the community of authors; and the John W. Campbell Memorial Award for Best Science Fiction Novel and Theodore Sturgeon Memorial Award for short fiction. One notable award for science fiction films is the Saturn Award. It is presented annually by The Academy of Science Fiction, Fantasy, and Horror Films.		There are national awards, like Canada's Prix Aurora Awards, regional awards, like the Endeavour Award presented at Orycon for works from the Pacific Northwest, special interest or subgenre awards like the Chesley Award for art or the World Fantasy Award for fantasy. Magazines may organize reader polls, notably the Locus Award.		Conventions (in fandom, shortened as "cons"), are held in cities around the world, catering to a local, regional, national, or international membership. General-interest conventions cover all aspects of science fiction, while others focus on a particular interest like media fandom, filking, etc. Most are organized by volunteers in non-profit groups, though most media-oriented events are organized by commercial promoters. The convention's activities are called the "program", which may include panel discussions, readings, autograph sessions, costume masquerades, and other events. Activities that occur throughout the convention are not part of the program; these commonly include a dealer's room, art show, and hospitality lounge (or "con suites").[85]		Conventions may host award ceremonies; Worldcons present the Hugo Awards each year. SF societies, referred to as "clubs" except in formal contexts, form a year-round base of activities for science fiction fans. They may be associated with an ongoing science fiction convention, or have regular club meetings, or both. Most groups meet in libraries, schools and universities, community centers, pubs or restaurants, or the homes of individual members. Long-established groups like the New England Science Fiction Association and the Los Angeles Science Fantasy Society have clubhouses for meetings and storage of convention supplies and research materials.[86] The Science Fiction and Fantasy Writers of America (SFWA) was founded by Damon Knight in 1965 as a non-profit organization to serve the community of professional science fiction authors,[68] 24 years after his essay "Unite or Fie!" had led to the organization of the National Fantasy Fan Federation. Fandom has helped incubate related groups, including media fandom,[87] the Society for Creative Anachronism,[88] gaming,[89] filking, and furry fandom.[90]		The first science fiction fanzine, The Comet, was published in 1930.[91] Fanzine printing methods have changed over the decades, from the hectograph, the mimeograph, and the ditto machine, to modern photocopying. Distribution volumes rarely justify the cost of commercial printing. Modern fanzines are printed on computer printers or at local copy shops, or they may only be sent as email. The best known fanzine (or "'zine") today is Ansible, edited by David Langford, winner of numerous Hugo awards. Other fanzines to win awards in recent years include File 770, Mimosa, and Plokta.[92] Artists working for fanzines have risen to prominence in the field, including Brad W. Foster, Teddy Harvia, and Joe Mayhew; the Hugos include a category for Best Fan Artists.[92] The earliest organized fandom online was the SF Lovers community, originally a mailing list in the late 1970s with a text archive file that was updated regularly.[93] In the 1980s, Usenet groups greatly expanded the circle of fans online. In the 1990s, the development of the World-Wide Web exploded the community of online fandom by orders of magnitude, with thousands and then literally millions of web sites devoted to science fiction and related genres for all media.[86] Most such sites are small, ephemeral, and/or very narrowly focused, though sites like SF Site and SFcrowsnest offer a broad range of references and reviews about science fiction.		Fan fiction, known to aficionados as "fanfic", is non-commercial fiction created by fans in the setting of an established book, film, video game, or television series.[94] This modern meaning of the term should not be confused with the traditional (pre-1970s) meaning of "fan fiction" within the community of fandom, where the term meant original or parody fiction written by fans and published in fanzines, often with members of fandom as characters therein. Examples of this would include the Goon Defective Agency stories, written starting in 1956 by Irish fan John Berry and published in his and Arthur Thomson's fanzine Retribution. In the last few years, sites have appeared such as Orion's Arm and Galaxiki, which encourage collaborative development of science fiction universes. In some cases, the copyright owners of the books, films, or television series have instructed their lawyers to issue "cease and desist" letters to fans.		The study of science fiction, or science fiction studies, is the critical assessment, interpretation, and discussion of science fiction literature, film, new media, fandom, and fan fiction. Science fiction scholars take science fiction as an object of study in order to better understand it and its relationship to science, technology, politics, and culture-at-large. Science fiction studies has a long history dating back to the turn of the 20th century, but it was not until later that science fiction studies solidified as a discipline with the publication of the academic journals Extrapolation (1959), Foundation: The International Review of Science Fiction (1972), and Science Fiction Studies (1973), and the establishment of the oldest organizations devoted to the study of science fiction, the Science Fiction Research Association and the Science Fiction Foundation, in 1970. The field has grown considerably since the 1970s with the establishment of more journals, organizations, and conferences with ties to the science fiction scholarship community, and science fiction degree-granting programs such as those offered by the University of Liverpool and Kansas University.		The National Science Foundation has conducted surveys of "Public Attitudes and Public Understanding" of "Science Fiction and Pseudoscience."[96] They write that "Interest in science fiction may affect the way people think about or relate to science....one study found a strong relationship between preference for science fiction novels and support for the space program...The same study also found that students who read science fiction are much more likely than other students to believe that contacting extraterrestrial civilizations is both possible and desirable (Bainbridge 1982).[97]		Mary Shelley wrote a number of science fiction novels including Frankenstein, and is treated as a major Romantic writer.[98] A number of science fiction works have received critical acclaim including Childhood's End and Do Androids Dream of Electric Sheep? (the inspiration for the movie Blade Runner). A number of respected writers of mainstream literature have written science fiction, including Aldous Huxley's Brave New World, George Orwell's Nineteen Eighty-Four, Anthony Burgess' A Clockwork Orange and Margaret Atwood's The Handmaid's Tale. Nobel Laureate Doris Lessing wrote a series of SF novels, Canopus in Argos, and nearly all of Kurt Vonnegut's works contain science fiction premises or themes.		The scholar Tom Shippey asks a perennial question of science fiction: "What is its relationship to fantasy fiction, is its readership still dominated by male adolescents, is it a taste which will appeal to the mature but non-eccentric literary mind?"[99] In her much reprinted essay "Science Fiction and Mrs Brown,"[100] the science fiction writer Ursula K. Le Guin has approached an answer by first citing the essay written by the English author Virginia Woolf entitled Mr Bennett and Mrs Brown in which she states:		I believe that all novels, ... deal with character, and that it is to express character – not to preach doctrines, sing songs, or celebrate the glories of the British Empire, that the form of the novel, so clumsy, verbose, and undramatic, so rich, elastic, and alive, has been evolved ... The great novelists have brought us to see whatever they wish us to see through some character. Otherwise they would not be novelists, but poets, historians, or pamphleteers.		Le Guin argues that these criteria may be successfully applied to works of science fiction and so answers in the affirmative her rhetorical question posed at the beginning of her essay: "Can a science fiction writer write a novel?"		Tom Shippey[99] in his essay does not dispute this answer but identifies and discusses the essential differences that exists between a science fiction novel and one written outside the field. To this end, he compares George Orwell's Coming Up for Air with Frederik Pohl and C. M. Kornbluth's The Space Merchants and concludes that the basic building block and distinguishing feature of a science fiction novel is the presence of the novum, a term Darko Suvin adapts from Ernst Bloch and defines as "a discrete piece of information recognizable as not-true, but also as not-unlike-true, not-flatly- (and in the current state of knowledge) impossible."[101]		In science fiction the style of writing is often relatively clear and straightforward compared to classical literature. Orson Scott Card, an author of both science fiction and non-SF fiction, has postulated that in science fiction the message and intellectual significance of the work is contained within the story itself and, therefore, there need not be stylistic gimmicks or literary games; but that some writers and critics confuse clarity of language with lack of artistic merit. In Card's words:		...a great many writers and critics have based their entire careers on the premise that anything that the general public can understand without mediation is worthless drivel. [...] If everybody came to agree that stories should be told this clearly, the professors of literature would be out of a job, and the writers of obscure, encoded fiction would be, not honored, but pitied for their impenetrability.[102]		Science fiction author and physicist Gregory Benford has declared that: "SF is perhaps the defining genre of the twentieth century, although its conquering armies are still camped outside the Rome of the literary citadels."[103] This sense of exclusion was articulated by Jonathan Lethem in an essay published in the Village Voice entitled "Close Encounters: The Squandered Promise of Science Fiction."[104] Lethem suggests that the point in 1973 when Thomas Pynchon's Gravity's Rainbow was nominated for the Nebula Award, and was passed over in favor of Arthur C. Clarke's Rendezvous with Rama, stands as "a hidden tombstone marking the death of the hope that SF was about to merge with the mainstream." Among the responses to Lethem was one from the editor of the Magazine of Fantasy and Science Fiction who asked: "When is it [the SF genre] ever going to realize it can't win the game of trying to impress the mainstream?"[105] On this point the journalist and author David Barnett has remarked:[106]		The ongoing, endless war between "literary" fiction and "genre" fiction has well-defined lines in the sand. Genre's foot soldiers think that literary fiction is a collection of meaningless but prettily drawn pictures of the human condition. The literary guard consider genre fiction to be crass, commercial, whizz-bang potboilers. Or so it goes.		Barnett, in an earlier essay had pointed to a new development in this "endless war":[107]		What do novels about a journey across post-apocalyptic America, a clone waitress rebelling against a future society, a world-girdling pipe of special gas keeping mutant creatures at bay, a plan to rid a colonizable new world of dinosaurs, and genetic engineering in a collapsed civilization have in common?		They are all most definitely not science fiction. Literary readers will probably recognize The Road by Cormac McCarthy, one of the sections of Cloud Atlas by David Mitchell, The Gone-Away World by Nick Harkaway, The Stone Gods by Jeanette Winterson and Oryx and Crake by Margaret Atwood from their descriptions above. All of these novels use the tropes of what most people recognize as science fiction, but their authors or publishers have taken great pains to ensure that they are not categorized as such.		Although perhaps most developed as a genre and community in the United States, Canada, and the United Kingdom, science fiction is a worldwide phenomenon. Organisations devoted to promotion and even translation in particular countries are commonplace, as are country- or language-specific genre awards.		Mohammed Dib, an Algerian writer, wrote a science fiction allegory about his nation's politics, Qui se souvient de la mer (Who Remembers the Sea?) in 1962.[108] Masimba Musodza, a Zimbabwean author, published MunaHacha Maive Nei? the first science-fiction novel in the Shona language,[109] which also holds the distinction of being the first novel in the Shona language to appear as an ebook first before it came out in print. In South Africa, a movie titled District 9 came out in 2009, an apartheid allegory featuring extraterrestrial life forms, produced by Peter Jackson.		Science fiction examines society through shifting power structures (such as the shift of power from humanity to alien overlords). African science fiction often uses this genre norm to situate slavery and the slave trade as an alien abduction. Commonalities in experiences with unknown languages, customs, and culture lend themselves well to this comparison. The subgenre also commonly employs the mechanism of time travel to examine the effects of slavery and forced emigration on the individual and the family.[citation needed]		Indian science fiction, defined loosely as science fiction by writers of Indian descent, began with the English-language publication of Kylas Chundar Dutt's A Journal of Forty-Eight Hours of the Year 1945 in the Calcutta Literary Gazette (June 6, 1835). Since this story was intended as a political polemic, credit for the first science fiction story is often given to later Bengali authors such as Jagadananda Roy, Hemlal Dutta and the polymath Jagadish Chandra Bose. Eminent film maker and writer Satyajit Ray also enriched Bengali science fiction by writing many short stories as well as science fiction series, Professor Shonku (see Bengali science fiction). Similar traditions exist in Hindi, Marathi, Tamil and English.[110] In English, the modern era of Indian speculative fiction began with the works of authors such as Samit Basu, Payal Dhar, Vandana Singh and Anil Menon. Works such as Amitav Ghosh's The Calcutta Chromosome, Salman Rushdie's Grimus, and Boman Desai's The Memory of Elephants are generally classified as magic realist works but make essential use of SF tropes and techniques. In recent years authors in some other Indian languages have begun telling stories in this genre; for example in Punjabi IP Singh and Roop Dhillon have written stories that can clearly be defined as Punjabi science fiction. The latter has coined the term Vachitarvaad to describe such literature.[citation needed]		Bangladesh has a strong Science fiction literature in South Asia. After Qazi Abdul Halim's Mohasunner Kanna (Tears of the Cosmos) (1970), Humayun Ahmed wrote the first modern Bengali SF novel, Tomader Jonno Valobasa (Love For You All).[citation needed] It was published in 1973. This book is treated as the first full-fledged Bangladeshi science fiction novel.[citation needed] Then he wrote Tara Tinjon (They were Three), Irina, Anonto Nakshatra Bithi (Endless Galaxy), Fiha Somikoron (Fiha Equation) etc.[citation needed]		But Bengali science fiction leaves its cocoon phase holding the hands of Muhammed Zafar Iqbal.[citation needed] Mr. Iqbal wrote a story named Copotronic Sukh Dukho (Copotronic Emotions) when he was a student of Dhaka University.[citation needed] This story was later included in a compilation of Iqbal's work in a book by the same name. Muktodhara, a famous publishing house of Dhaka was the publisher of this book. This collection of science fiction stories gained huge popularity and the new trend of science fiction emerged among Bengali writers and readers. After his first collection, Mr. Iqbal transformed his own science fiction cartoon strip Mohakashe Mohatrash (Terror in the Cosmos) into a novel. All told, Muhammed Zafar Iqbal has written the greatest number of science fiction works in Bengali science fiction.[citation needed]		Following the footsteps of the ancestors, more and more writers, especially young writers started writing science fiction and a new era started in Bengali literature.		Modern science fiction in China mainly depends on the magazine Science Fiction World. A number of works were originally published in it in installments, including the highly successful novel The Three-Body Problem, written by Liu Cixin.		Until recently, there has been little domestic science fiction literature in Korea.[111] Within the small field, the author and critic writing under the nom de plume Djuna has been credited with being the major force.[112] Kim Boyoung, Bae Myunghoon and Kwak Jaesik are also often mentioned as the new generation of Korean science fiction writers of 2010s.[113] The upswing that began in 2009 has been attributed by Shin Junebong to a combination of factors.[114] Shin quotes Djuna as saying, "'It looks like the various literary awards established by one newspaper after another, with hefty sums of prize money, had a big impact.'" [114] Another factor cited was the active use of Web bulletin boards among the then-young writers brought up on translations of Western SF.[115] In spite of the increase, there were still no more than sixty or so authors writing in the field at that time.[114]		Chalomot Be'aspamia is an Israeli magazine of short science fiction and fantasy stories. The Prophecies Of Karma, published in 2011, is advertised as the first work of science fiction by an Arabic author, the Lebanese writer Nael Gharzeddine.		Jules Verne, a 19th-century French novelist known for his pioneering science fiction works (Twenty Thousand Leagues Under the Sea, Journey to the Center of the Earth, From the Earth to the Moon) is the prime representative of the French legacy of science fiction. French science fiction of the 19th century was also represented with such artists as Albert Robida and Isidore Grandville. In the 20th century, traditions of French science fiction were carried on by writers like Pierre Boulle (best known for his Planet of the Apes), Serge Brussolo, Bernard Werber, René Barjavel and Robert Merle, among others.		In Franco-Belgian comics, bande dessinée ("BD") science-fiction is a well established genre.[citation needed] Notable French science fiction comics include Valerian et Laureline by Pierre Christin and Jean-Claude Mézières, a space opera franchise that has lasted since 1967. Metal Hurlant magazine (known in US as Heavy Metal) was one of the largest contributors to francophone science-fiction comics. Its major authors include Jean "Moebius" Giraud, creator of Arzach; Chilean Alejandro Jodorowsky, who created a series of comics, including L'Incal and Les Metabarons, set in Jodoverse; and Enki Bilal with The Nikopol Trilogy. Giraud also contributed to French SF animation, collaborating with René Laloux on several animated features. A number of artists from neighboring countries, such as Spain and Italy, create science fiction and fantasy comics in French aimed at a Franco-Belgian market.[citation needed]		In French cinema, science fiction began with silent film director and visual effects pioneer George Méliès, whose most famous film was Voyage to the Moon, loosely based on books by Verne and Wells. In the 20th and 21st centuries, French science fiction films were represented by René Laloux's animated features, as well as Enki Bilal's adaptation of the Nikopol Trilogy, Immortal. Luc Besson filmed The Fifth Element as a joint Franco-American production.		In the French-speaking world, the colloquial use of the term sci-fi is an accepted Anglicism for the term science fiction.[116] This probably stems from the fact that science fiction writing never expanded there to the extent it did in the English-speaking world, particularly with the dominance of the United States. Nevertheless, France has made a tremendous contribution to science fiction in its seminal stages of development. Although the term "science fiction" is understood in France, their penchant for the "weird and wacky" has a long tradition and is sometimes called "le culte du merveilleux." This uniquely French tradition certainly encompasses what the anglophone world would call French science fiction but also ranges across fairies, Dadaism, and surrealism.[citation needed]		Italy has a vivid history in science fiction, though almost unknown outside its borders. The history of Italian science fiction recognizes a varied roadmap of this genre which spread to a popular level after World War Two, and in particular in the second half of the 1950s, on the wave of American and British literature.[117]		The earliest pioneers may be found in the literature of the fantastic voyage and of the Renaissance Utopia, even in previous masterpieces such as "The Million" of Marco Polo. In the second half of the 19th century stories and short novels of "scientific fantasies" (also known as "incredible stories" or "fantastic" or "adventuristic", "novels of the future times" or "utopic", "of the tomorrow") appeared in Sunday newspaper supplements, in literary magazines, and as booklets published in installments. Added to these, at the beginning of the 20th century, were the most futuristic masterpieces of the great Emilio Salgari, considered by most the father of Italian science fiction, and Yambo and Luigi Motta, well known authors of popular novels of the time, with extraordinary adventures in remote and exotic places, and even works of authors representing known figures of the "top" literature, among them Massimo Bontempelli, Luigi Capuana, Guido Gozzano, Ercole Luigi Morselli.[118]		The true birth of Italian science fiction is placed in 1952, with the publishing of the first specialized magazines, Scienza Fantastica (Fantastic Science) and Urania, and with the appearance of the term "fantascienza" which has become the usual translation of the English term "science fiction." The "Golden Years" span the period 1957-1960.		From the end of the 1950s science fiction became in Italy one of the most popular genres, although its popular success was not followed by critical success. In spite of an active and organized fandom there hasn't been an authentic sustained interest on the part of the Italian cultural élite towards science fiction.[119]		Popular Italian science fiction writers include Gianluigi Zuddas, Giampietro Stocco, Lino Aldani,[120] as well as comic artists, such as Milo Manara. Valerio Evangelisti is the best known modern author of Italian science fiction and fantasy.[121] Also, popular Italian children's writer Gianni Rodari often turned to science fiction aimed at children, most notably, in Gip in the Television.		The main German science fiction writer in the 19th century was Kurd Laßwitz.[122] According to Austrian SF critic Franz Rottensteiner, though significant German novels of a science-fiction nature were published in the first half of the 20th century, SF did not exist as a genre in the country until after World War II and the heavy importing and translation of American works. In the 20th century, during the years of divided Germany, both East and West spawned a number of successful writers. Top East German writers included Angela and Karlheinz Steinmüller, as well as Günther Krupkat. West German authors included Carl Amery, Gudrun Pausewang, Wolfgang Jeschke and Frank Schätzing, among others. A well known science fiction book series in the German language is Perry Rhodan, which started in 1961. Having sold over two billion copies (in pulp, paperback and hardcover formats), it is the most successful science fiction book series ever written, worldwide.[123] Current well-known SF authors from Germany are five-time Kurd-Laßwitz-Award winner Andreas Eschbach, whose books The Carpet Makers and Eine Billion Dollar are big successes, and Frank Schätzing, who in his book The Swarm mixes elements of the science thriller with SF elements to an apocalyptic scenario. The most prominent German-speaking author, according to Die Zeit, is[when?] Austrian Herbert W. Franke.[citation needed]		In the 1920s Germany produced a number of critically acclaimed high-budget science fiction and horror films. Metropolis by director Fritz Lang is credited as one of the most influential science fiction films ever made.[124] Other films of the era included Woman in the Moon, Alraune, Algol, Gold, Master of the World, among others. In the second half of the 20th century, East Germany also became a major science fiction film producer, often in a collaboration with fellow Eastern Bloc countries. Films of this era include Eolomea, First Spaceship on Venus and Hard to Be a God.		Russians made their first steps to science fiction in the mid-19th century, with utopias by Faddei Bulgarin and Vladimir Odoevsky.[125] However, it was the Soviet era that became the genre's golden age. Soviet writers were prolific,[126] despite limitations set up by state censorship. Early Soviet writers, such as Alexander Belayev, Alexey N. Tolstoy and Vladimir Obruchev, employed Vernian/Wellsian hard science fiction based on scientific predictions.[127] The most notable books of the era include Belayev's Amphibian Man, The Air Seller and Professor Dowell's Head; Tolstoy's Aelita and Engineer Garin's Death Ray. Early Soviet science fiction was influenced by communist ideology and often featured a leftist agenda or anti-capitalist satire. [128] Those few early Soviet books that challenged the communist worldview and satirized the Soviets, such as Yevgeny Zamyatin's dystopia We or Mikhail Bulgakov's Heart of a Dog and Fatal Eggs, were banned from publishing until the 1980s, although they still circulated in fan-made copies.		In the second half of the 20th century, a new generation of writers developed a more complex approach. Social science fiction, concerned with philosophy, ethics, utopian and dystopian ideas, became the prevalent subgenre.[129] The breakthrough was started by Ivan Yefremov's utopian novel Andromeda Nebula (1957). He was soon followed by brothers Arkady and Boris Strugatsky, who explored darker themes and social satire in their Noon Universe novels, such as Hard to be a God (1964) and Prisoners of Power (1969), as well as in their science fantasy trilogy Monday Begins on Saturday (1964). A good share of Soviet science fiction was aimed at children. Probably the best known[127][130] was Kir Bulychov, who created Alisa Selezneva (1965-2003), a children's space adventure series about a teenage girl from the future.		The Soviet film industry also contributed to the genre, starting from the 1924 film Aelita. Some of early Soviet films, namely Planet of the Storms (1962) and Battle Beyond the Sun (1959), were pirated, re-edited and released in the West under new titles.[131] Late Soviet science fiction films include Mystery of the Third Planet (1981), Ivan Vasilyevich (1973) and Kin-dza-dza! (1986), as well as Andrey Tarkovsky's Solaris and Stalker, among others.		After the fall of the Soviet Union, science fiction in the former Soviet republics is still written mostly in Russian, which allows an appeal to a broader audience. Aside from Russians themselves, especially notable are Ukrainian writers, who have greatly contributed to science fiction and fantasy in Russian language.[132] Among the most notable post-Soviet authors are H. L. Oldie, Sergey Lukyanenko, Alexander Zorich and Vadim Panov. Russia's film industry, however, has been less successful recently, producing only a few science fiction films, most of them are adaptations of books by the Strugatskys (The Inhabited Island, The Ugly Swans) or Bulychov (Alice's Birthday). Science fiction media in Russia is represented with such magazines as Mir Fantastiki and Esli.		Spanish science fiction starts mid 19th century; depending on how it is defined, Lunigrafía (1855) from M. Krotse or Una temporada en el más bello de los planetas from Tirso Aguimana de Veca — a trip to Saturn published in 1870-1871, but written in the 1840s — is the first science fiction novel.[133][134][135] As such, science fiction was very popular in the second half of the 19th century, but mainly produced alternate history and post-apocalyptic futures, written by some of the most important authors of the generations of '98 and '14.[136] The influence of Verne also produced some singular works, like Enrique Gaspar y Rimbau's El anacronópete (1887), a story about time travel that predates the publication of The Chronic Argonauts by H. G. Wells; Rafael Zamora y Pérez de Urría's Crímenes literarios (1906), that describes robots and a "brain machine" very similar to our modern laptops; or Frederich Pujulà i Vallès' Homes artificials (1912), the first in Spain about "artificial people".[136][137] But the most prolific were Coronel Ignotus, and Coronel Sirius, who published their adventures in the magazine Biblioteca Novelesco-Científica. The 19th century up to the Spanish Civil War saw no less than four fictional trips to the Moon, one to Venus, five to Mars, one to Jupiter, and one to Saturn.[136][138]		The Spanish Civil War devastated this rich literary landscape. With few exceptions, only the arrival of pulp science fiction in the 1950s would reintroduce the genre in Spanish literature.[139][136] The space opera series La Saga de los Aznar (1953-1958 and 1973-1978) by Pascual Enguídanos won the Hugo Award in 1978.[140] Also in the 1950s started the radio serial for children Diego Valor; inspired by Dan Dare, the serial produced 1200 episodes of 15 min., and spinned a comic (1954-1964), three theater plays (1956-1959) and the first Spanish sf TV series (1958), that has been lost.[141]		Modern, prospective, self-aware science fiction crystallized in the 1970s around the magazine Nueva Dimensión (1968-1983), and its editor Domingo Santos, one of the most important Spanish sf authors of the time.[138][136] Other important authors of the 70s and 80s are Manuel de Pedrolo (Mecanoscrit del segon origen, 1974), Carlos Saiz Cidoncha (La caída del Imperio galáctico, 1978), Rafael Marín (Lágrimas de luz, 1984), and Juan Miguel Aguilera (the Akasa-Puspa saga, 1988-2005).[136] In the 1990s the genre exploded with the creation many small dedicated fanzines, important SF prizes, and the convention HispaCon; Elia Barceló (El mundo de Yarek, 1992), became the most prolific.[136] Other recent authors are Eduardo Vaquerizo (Danza de tinieblas, 2005), Félix J. Palma (The Victorian trilogy, 2008-2014), and Carlos Sisí (Panteón, 2013).[142]		Spain has been continuously producing sf films since the 1960s, at a rate of 5 to 10 per decade. The 1970s was specially prolific; the director, and sreenwriter Juan Piquer Simón is the most important figure of fantaterror, producing some low budget sf films. La cabina (1972) is the most awarded Spanish TV production in history. In the 90s Acción mutante (1992), by Álex de la Iglesia, and Abre los ojos (1997), by Alejandro Amenábar, represent a watershed in Spanish sf filming, with a quality that would only be reached again by Los cronocrímenes (2007), by Nacho Vigalondo.[143][144] The most important sf TV series produced in Spain is El ministerio del tiempo (2015-), even though Mañana puede ser verdad (1964-1964) by Chicho Ibáñez Serrador, and Plutón BRB Nero (2008-2009), should also be mentioned.		Poland is a traditional producer of science fiction and fantasy. The country's most influential science fiction writer of all time is Stanisław Lem, who is probably best known for his science fiction books, such as Solaris and the stories involving Ijon Tichy, but who also wrote very successful hard sci-fi such as The Invincible and the stories involving Pilot Pirx. A number of Lem's books were adapted for screen, both in Poland and abroad. Other notable Polish writers of the genre include Jerzy Żuławski, Janusz A. Zajdel, Konrad Fiałkowski, Jacek Dukaj and Rafał A. Ziemkiewicz.		Czech writer and playwright Karel Čapek in his play R.U.R. (1920) introduced the word robot into science fiction. Čapek is also known for his satirical science fiction novels and plays, such as War with the Newts and The Absolute at Large. Traditions of Czech science fiction were carried on despite the general political climate by writers like Ludvík Souček, Josef Nesvadba, Ondřej Neff and Jaroslav Velinský. In the years 1980 - 2000 a new vawe of young writers appeared (J. W. Procházka, F. Novotný, E. Hauserová, V. Kadlečková, J. Rečková, E. Dufková).		Early writers of Yugoslav science fiction were mid-19th century Slovene writer Simon Jenko, late 19th century Slovene writers Josip Stritar, Janez Trdina and Janez Mencinger (who, in 1893, published a notable dystopian novel Abadon) and late 19th century Serbian writers Dragutin Ilić and Lazar Komarčić. Since the beginning of the 20th century, a large number of authors incorporated science fiction elements into their work. Scientist Milutin Milanković wrote the book Through Distant Worlds and Times (1928), which mixes elements of autobiography, scientific discussion and science fiction. In the 1930s, Vladimir Bartol wrote a series of science fiction novels. The period after World War II brought the appearance of a large number of writers, most notably the duo Zvonimir Furtinger and Mladen Bjažić, Vid Pečjak and Miha Remec, with some academically acclaimed writers, like Dobrica Ćosić, Erih Koš and Ivan Ivanji, occasionally turning towards science fiction.[145] Serbian writer Borislav Pekić published several science fiction works: Rabies (1983), 1999 (1984), The New Jerusalem (1988) and Atlantis (1988). Zoran Živković wrote a large number of essays on science fiction and one of the first encyclopedias of science fiction in the world. His early novels and stories featured elements of the genre. The films The Rat Savior (1977) by Krsto Papić and Visitors from the Galaxy (1981) by Dušan Vukotić won awards at international festivals. In the first half of the 20th century comic book authors such as Andrija Maurović and Đorđe Lobačev published a number of science fiction works, and since the 1980s comic book artists like Željko Pahek, Igor Kordej and Zoran Janjetov became internationally well known.[146]		Australia: American David G. Hartwell noted there is "nothing essentially Australian about Australian science-fiction." A number of Australian science-fiction (and fantasy and horror) writers are in fact international English language writers, and their work is published worldwide. This is further explainable by the fact that the Australian inner market is small (with Australian population being around 24 million), and sales abroad are crucial to most Australian writers.[147]		In Canadian Francophone province Québec, Élisabeth Vonarburg and other authors developed a tradition of French-Canadian SF, related to the European French literature. The Prix Boreal was established in 1979 to honor Canadian science fiction works in French. The Prix Aurora Awards (briefly preceded by the Casper Award) were founded in 1980 to recognize and promote the best works of Canadian science fiction in both French and English. Also, due to Canada's bilingualism and the US publishing almost exclusively in English, translation of science fiction prose into French thrives and runs nearly parallel upon a book's publishing in the original English. A sizeable market also exists within Québec for European-written Francophone science fiction literature.		Although there is still some controversy as to when science fiction began in Latin America, the earliest works date from the late 19th century. All published in 1875, O Doutor Benignus by the Portuguese Brazilian Augusto Emílio Zaluar, El Maravilloso Viaje del Sr. Nic-Nac by the Argentinian Eduardo Holmberg, and Historia de un Muerto by the Cuban Francisco Calcagno are three of the earliest novels which appeared in the region.[148]		Up to the 1960s, science fiction was the work of isolated writers who did not identify themselves with the genre, but rather used its elements to criticize society, promote their own agendas or tap into the public's interest in pseudo-sciences. It received a boost of respectability after authors such as Horacio Quiroga and Jorge Luis Borges used its elements in their writings. This, in turn, led to the permanent emergence of science fiction in the 1960s and mid-1970s, notably in Argentina, Brazil, Chile, Mexico, and Cuba. Magic realism enjoyed parallel growth in Latin America, with a strong regional emphasis on using the form to comment on social issues, similar to social science fiction and speculative fiction in the English world.		Economic turmoil and the suspicious eye of the dictatorial regimes in place reduced the genre's dynamism for the following decade. In the mid-1980s, it became increasingly popular once more. Although led by Argentina, Brazil and Mexico, Latin America now hosts dedicated communities and writers with an increasing use of regional elements to set them apart from English-language science-fiction.[149]		
Steven John Carell (/kəˈrɛl/; born August 16, 1962)[2] is an American actor, comedian, director, producer and writer. Carell is best known for playing Michael Scott on the American version of The Office (2005–2011), on which he also worked as an occasional writer, producer and director.		Carell was a correspondent on The Daily Show with Jon Stewart for about five years, and has since starred in Anchorman: The Legend of Ron Burgundy (2004), The 40-Year-Old Virgin (2005), Evan Almighty (2007), Get Smart (2008), Crazy, Stupid, Love (2011), The Incredible Burt Wonderstone and The Way, Way Back (both 2013). He has also voice-acted in Over the Hedge (2006), Horton Hears a Who! (2008), and the Despicable Me franchise (2010–2017).		Carell was nominated as "America's funniest man" in Life magazine,[3] and received the Golden Globe Award for Best Actor – Television Series Musical or Comedy for his work on the first season of The Office. His role as wrestling coach and convicted murderer John Eleuthère du Pont in the drama film Foxcatcher (2014) earned him, among various honors, nominations for both the Academy Award for Best Actor[4] and the BAFTA Award for Best Actor in a Supporting Role.[5] He also received acclaim for his roles in Little Miss Sunshine (2006) and The Big Short (2015), the latter earning him his eighth Golden Globe Award nomination.						The youngest of four brothers, Carell was born at Emerson Hospital in Concord, Massachusetts, and raised in nearby Acton, Massachusetts. His father, Edwin A. Carell, was an electrical engineer,[6][7] and his mother, Harriet Theresa (née Koch; 1925–2016),[8] was a psychiatric nurse. His maternal uncle, Stanley Koch, worked with scientist Allen B. DuMont to create cathode ray tubes.[9] His father is of Italian and German descent, and his mother was of Polish ancestry.[10] His father, born under the surname "Caroselli", later changed it to "Carell".[7]		Carell was raised Roman Catholic,[11] and was educated at Nashoba Brooks School, The Fenn School and Middlesex School. He also played ice hockey and lacrosse while in high school.[12] He played the fife, performing with other members of his family, and later joined a reenacting group portraying the 10th (North Lincoln) Regiment of Foot. He attributed his interest in history to this,[13] earning a degree in the subject from Denison University in Granville, Ohio, in 1984.[14][15]		While at Denison, Carell was a member of Burpee's Seedy Theatrical Company, a student-run improvisational comedy troupe and was a goalie on Big Red hockey team for four years.[16][17] He also spent time as a disc jockey under the name "Sapphire Steve Carell" at WDUB, the campus radio station.[18]		Carell states that he worked as a mail carrier in Littleton, Massachusetts. He later recounted that he quit after six months because his boss told him he wasn't very good at being a mail carrier and needed to be faster.[19] Early in his performing career, Carell acted on the stage in a touring children's theater company, later in the comedy musical, Knat Scatt Private Eye and in a television commercial for Brown's Chicken in 1989.[20] In 1991, Carell performed with Chicago troupe The Second City where Stephen Colbert was his understudy for a time. Carell made his film debut in a minor role in Curly Sue. In spring 1996, he was a cast member of The Dana Carvey Show, a small, short-lived sketch comedy program on ABC. Along with fellow cast member Stephen Colbert, Carell provided the voice of Gary, half of The Ambiguously Gay Duo, the Robert Smigel-produced animated short which continued on Saturday Night Live later that year. While the program lasted only seven episodes, The Dana Carvey Show has since been credited with forging Carell's career.[21] He starred in a few short-lived television series, including Come to Papa and Over the Top. He has made numerous guest appearances, including on an episode of Just Shoot Me! titled "Funny Girl." Carell's other early screen credits include Brad Hall's short-lived situation comedy Watching Ellie (2002–2003) and Woody Allen's Melinda and Melinda. Carell was a correspondent for The Daily Show from 1999 to 2005, with a number of regular segments including "Even Stevphen" with Stephen Colbert[22] and on The Daily Show'.[23][24]		In 2005, Carell signed a deal with NBC to star in The Office, a mockumentary about life at a mid-sized paper supply company, which was a remake of a successful British TV series. He played the role of Michael Scott, the idiosyncratic regional manager of Dunder Mifflin Inc, in Scranton, Pennsylvania. Although the first season of the adaptation suffered mediocre ratings, NBC renewed it for another season due to the anticipated success of Carell's film The 40-Year-Old Virgin,[25] and the series subsequently became a ratings success. Carell won a Golden Globe Award and Television Critics Association Award during 2006 for his role in The Office. He also received six Primetime Emmy Award nominations[26] for his work in the series (2006–2011). Carell earned approximately US$175,000 per episode of the third season of The Office, twice his salary for the previous two seasons. In an Entertainment Weekly interview, he commented on his salary, saying, "You don't want people to think you're a pampered jerk. Salaries can be ridiculous. On the other hand, a lot of people are making a lot of money off of these shows."[27]		Carell was allowed "flex time" during filming to work on theatrical films. Carell worked on Evan Almighty during a production hiatus during the second season of The Office.[28] Production ended during the middle of the fourth season of The Office because of Carell's and others' refusal to cross the picket line of the 2007 Writers Guild of America strike. Carell, a WGA member,[29] has written two episodes of The Office: "Casino Night" and "Survivor Man". Both episodes were praised, and Carell won a Writers Guild of America Award for "Casino Night".[30] On April 29, 2010, Carell stated he would be leaving the show when his contract expired at the conclusion of the 2010–2011 season because he wanted to focus on his film career.[31] His last episode as a main character, "Goodbye, Michael", aired April 28, 2011, with his final shot showing Carell walking to a Colorado-bound plane to join his fiancée, Holly Flax, in Boulder, Colorado. Although he was invited back for the series finale in 2013, Carell originally declined believing that it would go against his character's arc.[32] Ultimately in the final version of the finale Carell reprised the role.[33][34]		Carell's first major film role was as weatherman Brick Tamland in the 2004 hit comedy Anchorman: The Legend of Ron Burgundy. Struck by Carell's performance in the film, Anchorman producer Judd Apatow approached Carell about creating a film together, and Carell told him about an idea he had involving a middle-aged man who is still a virgin.[35] The result was the 2005 film The 40-Year-Old Virgin, which Carell and Apatow developed and wrote together, starring Carell as the title character. The film made $109 million in domestic box office sales[36] and established him as a leading man. It also earned Carell an MTV Movie Award for Best Comedic Performance[37] and a WGA Award nomination, along with Apatow, for Best Original Screenplay.[38]		Carell played Uncle Arthur, imitating the camp mannerisms of Paul Lynde's original character, in Bewitched, a TV adaptation co-starring Nicole Kidman and Will Ferrell. He also voiced Hammy the Squirrel in the 2006 computer-animated film, Over the Hedge and Ned McDodd, the mayor of Whoville, in the 2008 animated film Horton Hears a Who!. He starred in Little Miss Sunshine during 2006, as Uncle Frank.[39]		His work in the films Anchorman, The 40-Year-Old Virgin, and Bewitched established Carell as a member of Hollywood's so-called "Frat Pack", a group of actors who often appear in films together, that also includes Owen Wilson, Will Ferrell, Vince Vaughn and Luke Wilson. Carell acted as the title character of Evan Almighty, a sequel to Bruce Almighty, reprising his role as Evan Baxter, now a U.S. Congressman. The film received mostly negative reviews. Carell starred in the 2007 film Dan in Real Life, co-starring Dane Cook and Juliette Binoche.		Carell played Maxwell Smart in the 2008 film Get Smart, an adaptation of the TV series starring Don Adams. It was successful, grossing over $200 million worldwide.[40] During 2007, Carell was invited to join the Academy of Motion Picture Arts and Sciences.[41][42] Carell starred with Tina Fey in Date Night during late 2008 and was released on April 9, 2010 in the U.S. He voiced Gru, the main character in the Universal CGI film, Despicable Me along with Russell Brand, Miranda Cosgrove, and Kristen Wiig, The film was Carell's other 2010 film. He reprised the role in the 2013 sequel Despicable Me 2. He has several other projects in the works, including a remake of the 1967 Peter Sellers film The Bobo. He is currently doing voice-over work in commercials for Wrigley's Extra gum. Carell has launched a television division of his production company, Carousel Productions, which has contracted a three-year overall deal with Universal Media Studios, the studio behind his NBC comedy series. Thom Hinkle and Campbell Smith of North South Prods., former producers on Comedy Central's The Daily Show, were hired to manage Carousel's TV operations.[43]		Carell played millionaire E.I. du Pont family heir and convicted murderer John Eleuthère du Pont in the 2014 true crime drama film Foxcatcher. Since the film's premiere at the Cannes Film Festival, it has received widespread acclaim[44] and Carell was nominated for the Golden Globe Award for Best Actor in a Motion Picture – Drama and for the Academy Award for Best Actor, both of which he lost to Eddie Redmayne.[45]		Carell played activist Steven Goldstein in the gay rights drama Freeheld, replacing Zach Galifianakis, who dropped out due to scheduling conflicts.[46] The film co-stars Julianne Moore, Ellen Page and Michael Shannon, and was released in October 2015. He followed this with another biographical drama, The Big Short, in which he portrayed banker Steve Eisman, whose name was changed in the film to Mark Baum. Directed by Adam McKay, the film stars Christian Bale, Ryan Gosling and Brad Pitt, and it was released in December 2015,[47] The film also received widespread critical acclaim, earning Carell a Golden Globe Award nomination for Best Actor. The film was also nominated for the Academy Award for Best Picture, making it the second film starring Carell to be nominated for the award, with Little Miss Sunshine being the first. He next starred in Woody Allen's Café Society (2016), alongside Kristen Stewart and Jesse Eisenberg.		Carell will headline the biographical comedy-drama Battle of the Sexes, portraying tennis star Bobby Riggs and co-starring with Emma Stone as Billie Jean King.		On August 5, 1995, Carell married Saturday Night Live alumna Nancy Carell (née Walls), whom he met when she was a student in an improvisation class he was teaching at Second City.[48] The couple have two children, Elisabeth (born May 2001) and John (born June 2004).[49]		In addition to working with Carell as a fellow correspondent on The Daily Show, Nancy acted with him on The Office as his realtor and short-lived girlfriend Carol Stills. She also cameoed as a sex therapist in The 40-Year-Old Virgin and played Linda in Seeking a Friend for the End of the World.[50] They also created the TBS comedy series Angie Tribeca starring Rashida Jones, which premiered on January 17, 2016.		In February 2009, Carell bought the Marshfield Hills General Store in Marshfield, Massachusetts.[51][52] During 2011, Carell earned $17.5 million, making him the thirty-first highest paid actor (excluding television-related projects).[53]		In an interview with 60 minutes, Carell stated that his inspirations for acting and comedy are Steve Martin, Peter Sellers, John Cleese, Bill Cosby, and George Carlin.		
Fantasy is a fiction genre set in an fictional universe, often (but not always) without any locations, events, or people from the real world. Its roots are in oral traditions, which then developed into literature and drama. From the twentieth century it has expanded further into various media, including film, television, graphic novels and Video Games.		Most fantasy uses magic or other supernatural elements as a main plot element, theme, or setting. Magic and magical creatures are common in many of these worlds. Fantasy is a subgenre of speculative fiction and is distinguished from the genres of science fiction and horror by the absence of scientific or macabre themes respectively, though these genres overlap.		In popular culture, the fantasy genre is predominantly of the medievalist form. In its broadest sense, however, fantasy comprises works by many writers, artists, filmmakers, and musicians from ancient myths and legends to many recent and popular works.		Fantasy is studied in a number of disciplines including English and other language studies, cultural studies, comparative literature, history and medieval studies. Work in this area ranges widely from the structuralist theory of Tzvetan Todorov, which emphasizes the fantastic as a liminal space, to work on the connections (political, historical and literary) between medievalism and popular culture.[1]						The identifying trait of fantasy is the author's reliance on imagination to create narrative elements that do not have to rely on history or nature to be coherent.[2] This differs from realistic fiction in that whereas realistic fiction has to attend to the history and natural laws of reality, fantasy does not. An author applies his or her imagination to come up with characters, plots, and settings that are impossible in reality. Many fantasy authors use real-world folklore and mythology as inspiration;[3] and although for many the defining characteristic of the fantasy genre is the inclusion of supernatural elements, such as magic,[4] this does not have to be the case. For instance, a narrative that takes place in an imagined town in the northeastern United States could be considered realistic fiction as long as the plot and characters are consistent with the history of region and the natural characteristics that someone who has been to the northeastern United States expects; when, however, the narrative takes place in an imagined town, on an imagined continent, with an imagined history and an imagined ecosystem, the work becomes fantasy with or without supernatural elements.		Fantasy has often been compared with science fiction and horror because they are the major categories of speculative fiction. Fantasy is distinguished from science fiction by the plausibility of the narrative elements. A science fiction narrative is unlikely, though seeming possible through logical scientific or technological extrapolation, whereas fantasy narratives do not need to be scientifically possible.[5] The imagined elements of fantasy do not need a scientific explanation to be narratively functional. Authors have to rely on the readers' suspension of disbelief, an acceptance of the unbelievable or impossible for the sake of enjoyment, in order to write effective fantasies. Despite both genres' heavy reliance on the supernatural, fantasy and horror are distinguishable. Horror primarily evokes fear through the protagonists' weaknesses or inability to deal with the antagonists.[6]		Elements of the supernatural and the fantastic were an element of literature from its beginning.		There are many works where the boundary between fantasy and other works is not clear; the question of whether the writers believed in the possibilities of the marvels in A Midsummer Night's Dream or Sir Gawain and the Green Knight makes it difficult to distinguish when fantasy, in its modern sense, first began.[7]		Although pre-dated by John Ruskin's The King of the Golden River (1841), the history of modern fantasy literature is usually said to begin with George MacDonald, the Scottish author of such novels as The Princess and the Goblin and Phantastes (1858), the latter of which is widely considered to be the first fantasy novel ever written for adults. MacDonald was a major influence on both J. R. R. Tolkien and C. S. Lewis. The other major fantasy author of this era was William Morris, a popular English poet who wrote several novels in the latter part of the century, including The Well at the World's End.		Despite MacDonald's future influence with At the Back of the North Wind (1871), Morris's popularity with his contemporaries, and H. G. Wells's The Wonderful Visit (1895), it was not until the 20th century that fantasy fiction began to reach a large audience. Lord Dunsany established the genre's popularity in both the novel and the short story form. Many popular mainstream authors also began to write fantasy at this time, including H. Rider Haggard, Rudyard Kipling, and Edgar Rice Burroughs. These authors, along with Abraham Merritt, established what was known as the "lost world" subgenre, which was the most popular form of fantasy in the early decades of the 20th century, although several classic children's fantasies, such as Peter Pan and The Wonderful Wizard of Oz, were also published around this time.		Indeed, juvenile fantasy was considered more acceptable than fantasy intended for adults, with the effect that writers who wished to write fantasy had to fit their work in a work for children.[8] Nathaniel Hawthorne wrote fantasy in A Wonder-Book for Girls and Boys, intended for children,[9] though works for adults only verged on fantasy. For many years, this and successes such as Alice's Adventures in Wonderland (1865), created the circular effect that all fantasy works, even the later The Lord of the Rings, were therefore classified as children's literature.		Political and social trends can affect a society's reception towards fantasy. In the early 20th century, the New Culture Movement's enthusiasm for Westernization and science in China compelled them to condemn the fantastical shenmo genre of traditional Chinese literature. The spells and magical creatures of these novels were viewed as superstitious and backward, products of a feudal society hindering the modernization of China. Stories of the supernatural continued to be denounced once the Communists rose to power, and mainland China experienced a revival in fantasy only after the Cultural Revolution had ended.[10]		Fantasy was a staple genre of pulp magazines published in the West. In 1923, the first all-fantasy fiction magazine, Weird Tales, was created. Many other similar magazines eventually followed, most noticeably The Magazine of Fantasy and Science Fiction. The pulp magazine format was at the height of its popularity at this time and was instrumental in bringing fantasy fiction to a wide audience in both the U.S. and Britain. Such magazines were also instrumental in the rise of science fiction, and it was at this time the two genres began to be associated with each other.		By 1950, "sword and sorcery" fiction had begun to find a wide audience, with the success of Robert E. Howard's Conan the Barbarian and Fritz Leiber's Fafhrd and the Gray Mouser stories.[11] However, it was the advent of high fantasy, and most of all J. R. R. Tolkien's The Hobbit and The Lord of the Rings, which reached new heights of popularity in the late 1960s, that allowed fantasy to truly enter the mainstream.[12] Several other series, such as C. S. Lewis's Chronicles of Narnia and Ursula K. Le Guin's Earthsea books, helped cement the genre's popularity.		The popularity of the fantasy genre has continued to increase in the 21st century, as evidenced by the best-selling status of J. K. Rowling's Harry Potter series or of George R. R. Martin's Song of Ice and Fire sequence.		Several fantasy film adaptations have achieved blockbuster status, most notably The Lord of the Rings film trilogy directed by Peter Jackson, and the Harry Potter films, two of the highest-grossing film series in cinematic history. Meanwhile, David Benioff and D. B. Weiss would go on to produce the television drama series Game of Thrones for HBO, which has gone on to achieve unprecedented success for the fantasy genre on television.		Fantasy role-playing games cross several different media. Dungeons & Dragons was the first tabletop role-playing game and remains the most successful and influential.[13][14] The science fantasy role-playing game series Final Fantasy has been an icon of the role-playing video game genre (as of 2012[update] still among the top ten best-selling video game franchises). The first collectible card game, Magic: The Gathering, has a fantasy theme and is similarly dominant in the industry.[15]		Fantasy encompasses numerous subgenres characterized by particular themes or settings, or by an overlap with other literary genres or forms of speculative fiction. They include the following:		In her 2008 book Rhetorics of Fantasy,[16] Farah Mendlesohn proposes the following taxonomy of fantasy, as "determined by the means by which the fantastic enters the narrated world",[17] while noting that there are fantasies that fit neither pattern:		In a "portal-quest fantasy" or "portal fantasy", a fantastical world is entered through a portal, behind which the fantastic elements of the story remain contained. These tend to be quest-type narratives, whose main challenge is navigating the fantastical world.[18] Well-known portal fantasies include C. S. Lewis's novel The Lion, the Witch and the Wardrobe (1950) and L. Frank Baum's novel The Wonderful Wizard of Oz (1900).[19]		The "immersive fantasy" lets the reader perceive the fantastical world through the eyes and ears of the protagonist, without an explanatory narrative. The fictional world is seen as complete, and its fantastic elements are not questioned within the context of the story. If successfully done, this narrative mode "consciously negates the sense of wonder" often associated with speculative fiction. But, according to Mendlesohn, "a sufficiently effective immersive fantasy may be indistinguishable from science fiction" because, once assumed, the fantastic "acquires a scientific cohesion all of its own", which has led to disputes about how to classify novels such as Mary Gentle's Ash (2000) and China Miéville's Perdido Street Station (2000).[20]		In an "intrusion fantasy", the fantastic intrudes on reality (in contrast to portal fantasies, where the opposite happens), and the protagonists' engagement with that intrusion drives the story. Intrusion fantasies are normally realist in style, because they assume the normal world as their base, and rely heavily on explanation and description.[21] Immersive and portal fantasies may themselves host intrusions. Classic intrusion fantasies include Dracula by Bram Stoker (1897) and the works of H. P. Lovecraft.[22]		"Liminal fantasy", finally, is a relatively rare mode where the fantastic enters a world that appears to be our own, but this is not perceived as intrusive but rather as normal by the protagonists, and this disconcerts and estranges the reader. Such fantasies adopt an ironic, blasé tone, as opposed to the straight-faced mimesis of most other fantasy.[23] Examples include Joan Aiken's stories about the Armitage family, who are amazed that unicorns appear on their lawn on a Tuesday, rather than on a Monday.[22]		Professionals such as publishers, editors, authors, artists, and scholars within the fantasy genre get together yearly at the World Fantasy Convention. The World Fantasy Awards are presented at the convention. The first WFC was held in 1975 and it has occurred every year since. The convention is held at a different city each year.		Additionally, many science fiction conventions, such as Florida's FX Show and MegaCon, cater to fantasy and horror fans. Anime conventions, such as Ohayocon or Anime Expo frequently feature showings of fantasy, science fantasy, and dark fantasy series and films, such as Majutsushi Orphen (fantasy), Sailor Moon (urban fantasy), Berserk (dark fantasy), and Spirited Away (fantasy). Many science fiction/fantasy and anime conventions also strongly feature or cater to one or more of the several subcultures within the main subcultures, including the cosplay subculture (in which people make or wear costumes based on existing or self-created characters, sometimes also acting out skits or plays as well), the fan fiction subculture, and the fan video or AMV subculture, as well as the large internet subculture devoted to reading and writing prose fiction or doujinshi in or related to those genres.		According to 2013 statistics by the fantasy publisher Tor Books, men outnumber women by 67% to 33% among writers of historical, epic or high fantasy. But among writers of urban fantasy or paranormal romance, 57% are women and 43% are men.[24]		
Oral tradition, or oral lore, is a form of human communication where in knowledge, art, ideas and cultural material is received, preserved and transmitted orally from one generation to another.[1][2][3] The transmission is through speech or song and may include folktales, ballads, chants, prose or verses. In this way, it is possible for a society to transmit oral history, oral literature, oral law and other knowledge across generations without a writing system, or in parallel to a writing system. Indian religions such as Buddhism, Hinduism and Jainism, for example, have used an oral tradition, in parallel to a writing system, to transmit their canonical scriptures, secular knowledge such as Sushruta Samhita, hymns and mythologies from one generation to the next.[4][5][6]		Oral tradition is information, memories and knowledge held in common by a group of people, over many generations, and it is not same as testimony or oral history.[1][7] In a general sense, "oral tradition" refers to the recall and transmission of a specific, preserved textual and cultural knowledge through vocal utterance.[2][8] As an academic discipline, it refers both to a set of objects of study and a method by which they are studied.[9]		The study of oral tradition is distinct from the academic discipline of oral history,[10] which is the recording of personal memories and histories of those who experienced historical eras or events.[11] Oral tradition is also distinct from the study of orality defined as thought and its verbal expression in societies where the technologies of literacy (especially writing and print) are unfamiliar to most of the population.[12] A folklore is a type of oral tradition, but knowledge other than folklore has been orally transmitted and thus preserved in human history.[13][14]						According to John Foley, oral tradition has been an ancient human tradition found in "all corners of the world".[8] Modern archeology has been unveiling evidence of the human efforts to preserve and transmit arts and knowledge that depended completely or partially on an oral tradition, across various cultures:		The Judeo-Christian Bible reveals its oral traditional roots; medieval European manuscripts are penned by performing scribes; geometric vases from archaic Greece mirror Homer's oral style. (...) Indeed, if these final decades of the millennium have taught us anything, it must be that oral tradition never was the other we accused it of being; it never was the primitive, preliminary technology of communication we thought it to be. Rather, if the whole truth is told, oral tradition stands out as the single most dominant communicative technology of our species as both a historical fact and, in many areas still, a contemporary reality.		In Asia, the transmission of folklore, mythologies as well as scriptures in ancient India, in different Indian religions, was by oral tradition, preserved with precision with the help of elaborate mnemonic techniques.[15] Some scholars such as Jack Goody state that the Vedas are not the product strictly of an oral tradition, basing this view by comparing inconsistencies in the transmitted versions of literature from various oral societies such as the Greek, Serbia and other cultures, then noting that the Vedic literature is too consistent and vast to have been composed and transmitted orally across generations, without being written down.[4] According to Goody, the Vedic texts likely involved both a written and oral tradition, calling it a "parallel products of a literate society".[4][6]		In ancient Greece, the oral tradition was a dominant tradition. Homer's epic poetry, states Michael Gagarin, was largely composed, performed and transmitted orally.[16] As folklores and legends were performed in front of distant audiences, the singers would substitute the names in the stories with local characters or rulers to give the stories a local flavor and thus connect with the audience, but making the historicity embedded in the oral tradition as unreliable.[17] The lack of surviving texts about the Greek and Roman religious traditions have led scholars to presume that these were ritualistic and transmitted as oral traditions, but some scholars disagree that the complex rituals in the ancient Greek and Roman civilizations were an exclusive product of an oral tradition.[18] The Torah and other ancient Jewish literature, the Judeo-Christian Bible and texts of early centuries of Christianity are rooted in an oral tradition, and the term "People of the Book" is a medieval construct.[8][19][20] This is evidenced, for example, by the multiple scriptural statements by Paul admitting "previously remembered tradition which he received" orally.[21]		Oral traditions face the challenge of accurate transmission and verifiability of the accurate version, particularly when the culture lacks written language or has limited access to writing tools. Oral cultures have employed various strategies that achieve this without writing. For example, a heavily rhythmic speech filled with mnemonic devices enhances memory and recall. A few useful mnemonic devices include alliteration, repetition, assonance, and proverbial sayings. In addition, the verse is often metrically composed with an exact number of syllables or morae - such as with Greek and Latin prosody and in Chandas found in Hindu and Buddhist texts.[22][23] The verses of the epic or text are typically designed wherein the long and short syllables are repeated by certain rules, so that if an error or inadvertent change is made, an internal examination of the verse reveals the problem.[22] Such strategies help facilitate transmission of information from individual to individual without a written intermediate, and they can also be applied to oral governance.[24]		Rudyard Kipling's The Jungle Book provides an excellent demonstration of oral governance in the Law of the Jungle. Not only does grounding rules in oral proverbs allow for simple transmission and understanding, but it also legitimizes new rulings by allowing extrapolation. These stories, traditions, and proverbs are not static, but are often altered upon each transmission barring the overall meaning remains intact.[25] In this way, the rules that govern the people are modified by the whole and not authored by a single entity.		Ancient texts of Hinduism, Buddhism and Jainism were preserved and transmitted by an oral tradition.[26][27] For example, the śrutis of Hinduism called the Vedas, the oldest of which trace back to the second millennium BCE. Michael Witzel explains this oral tradition as follows:[5]		The Vedic texts were orally composed and transmitted, without the use of script, in an unbroken line of transmission from teacher to student that was formalized early on. This ensured an impeccable textual transmission superior to the classical texts of other cultures; it is, in fact, something like a tape-recording... Not just the actual words, but even the long-lost musical (tonal) accent (as in old Greek or in Japanese) has been preserved up to the present.		Ancient Indians developed techniques for listening, memorization and recitation of their knowledge, in schools called Gurukul, while maintaining exceptional accuracy of their knowledge across the generations.[28] Many forms of recitation or paths were designed to aid accuracy in recitation and the transmission of the Vedas and other knowledge texts from one generation to the next. All hymns in each Veda were recited in this way; for example, all 1,028 hymns with 10,600 verses of the Rigveda was preserved in this way; as were all other Vedas including the Principal Upanishads, as well as the Vedangas. Each text was recited in a number of ways, to ensure that the different methods of recitation acted as a cross check on the other. Pierre-Sylvain Filliozat summarizes this as follows:[29]		These extraordinary retention techniques guaranteed an accurate Śruti, fixed across the generations, not just in terms of unaltered word order but also in terms of sound.[28][30] That these methods have been effective, is testified to by the preservation of the most ancient Indian religious text, the Ṛgveda (ca. 1500 BCE).[29]		The following overview draws upon Oral-Formulaic Theory and Research: An Introduction and Annotated Bibliography, (NY: Garland Publishing, 1985, 1986, 1989); additional material is summarized from the overlapping prefaces to the following volumes: The Theory of Oral Composition: History and Methodology, (Indiana University Press, 1988, 1992); Immanent Art: From Structure to Meaning in Traditional Oral Epic (Bloomington: Indiana University Press, 1991); The Singer of Tales in Performance (Bloomington: Indiana University Press, 1995) and Comparative Research on Oral Traditions: A Memorial for Milman Parry (Columbus, Ohio: Slavica Publishers, 1987). in the work of the Serb scholar Vuk Stefanović Karadžić (1787–1864), a contemporary and friend of the Brothers Grimm. Vuk pursued similar projects of "salvage folklore" (similar to rescue archaeology) in the cognate traditions of the Southern Slavic regions which would later be gathered into Yugoslavia, and with the same admixture of romantic and nationalistic interests (he considered all those speaking the Eastern Herzegovinian dialect as Serbs). Somewhat later, but as part of the same scholarly enterprise of nationalist studies in folklore,[31] the turcologist Vasily Radlov (1837–1918) would study the songs of the Kara-Kirghiz in what would later become the Soviet Union; Karadzic and Radloff would provide models for the work of Parry.		In a separate development, the media theorist Marshall McLuhan (1911–1980) would begin to focus attention on the ways that communicative media shape the nature of the content conveyed.[32] He would serve as mentor to the Jesuit, Walter Ong (1912–2003), whose interests in cultural history, psychology and rhetoric would result in Orality and Literacy (Methuen, 1980) and the important but less-known Fighting for Life: Contest, Sexuality and Consciousness (Cornell, 1981)[33] These two works articulated the contrasts between cultures defined by primary orality, writing, print, and the secondary orality of the electronic age.[34]		Ong's works also made possible an integrated theory of oral tradition which accounted for both production of content (the chief concern of Parry-Lord theory) and its reception.[34] This approach, like McLuhan's, kept the field open not just to the study of aesthetic culture but to the way physical and behavioral artifacts of oral societies are used to store, manage and transmit knowledge, so that oral tradition provides methods for investigation of cultural differences, other than the purely verbal, between oral and literate societies.		The most-often studied section of Orality and Literacy concerns the "psychodynamics of orality" This chapter seeks to define the fundamental characteristics of 'primary' orality and summarizes a series of descriptors (including but not limited to verbal aspects of culture) which might be used to index the relative orality or literacy of a given text or society.[36]		In advance of Ong's synthesis, John Miles Foley began a series of papers based on his own fieldwork on South Slavic oral genres, emphasizing the dynamics of performers and audiences.[37] Foley effectively consolidated oral tradition as an academic field [4] when he compiled Oral-Formulaic Theory and Research in 1985. The bibliography gives a summary of the progress scholars made in evaluating the oral tradition up to that point, and includes a list of all relevant scholarly articles relating to the theory of Oral-Formulaic Composition. He also both established both the journal Oral Tradition and founded the Center for Studies in Oral Tradition (1986) at the University of Missouri. Foley developed Oral Theory beyond the somewhat mechanistic notions presented in earlier versions of Oral-Formulaic Theory, by extending Ong's interest in cultural features of oral societies beyond the verbal, by drawing attention to the agency of the bard and by describing how oral traditions bear meaning.		The bibliography would establish a clear underlying methodology which accounted for the findings of scholars working in the separate Linguistics fields (primarily Ancient Greek, Anglo-Saxon and Serbo-Croatian). Perhaps more importantly, it would stimulate conversation among these specialties, so that a network of independent but allied investigations and investigators could be established.[38]		Foley's key works include The Theory of Oral Composition (1988);[39] Immanent Art (1991); Traditional Oral Epic: The Odyssey, Beowulf and the Serbo-Croatian Return-Song (1993); The Singer of Tales in Performance (1995); Teaching Oral Traditions (1998); How to Read an Oral Poem (2002). His Pathways Project (2005-2012) draws parallels between the media dynamics of oral traditions and the Internet.		The theory of oral tradition would undergo elaboration and development as it grew in acceptance.[40] While the number of formulas documented for various traditions proliferated,[41] the concept of the formula remained lexically-bound. However, numerous innovations appeared, such as the "formulaic system"[42] with structural "substitution slots" for syntactic, morphological and narrative necessity (as well as for artistic invention).[43] Sophisticated models such as Foley's "word-type placement rules" followed.[44] Higher levels of formulaic composition were defined over the years, such as "ring composition",[45] "responsion"[46] and the "type-scene" (also called a "theme" or "typical scene"[47]). Examples include the "Beasts of Battle"[48] and the "Cliffs of Death".[49] Some of these characteristic patterns of narrative details, (like "the arming sequence;"[50] "the hero on the beach";[51] "the traveler recognizes his goal")[52] would show evidence of global distribution.[53]		At the same time, the fairly rigid division between oral and literate was replaced by recognition of transitional and compartmentalized texts and societies, including models of diglossia (Brian Stock[54] Franz Bäuml,[55] and Eric Havelock).[56] Perhaps most importantly, the terms and concepts of "orality" and "literacy" came to be replaced with the more useful and apt "traditionality" and "textuality".[57] Very large units would be defined (The Indo-European Return Song)[58] and areas outside of military epic would come under investigation: women's song,[59] riddles[57] and other genres.		The methodology of oral tradition now conditions a large variety of studies, not only in folklore, literature and literacy, but in philosophy,[60] communication theory,[61] Semiotics,[62] and including a very broad and continually expanding variety of languages and ethnic groups,[63][64][65][66][67] and perhaps most conspicuously in biblical studies,[68] in which Werner Kelber has been especially prominent.[69] The annual bibliography is indexed by 100 areas, most of which are ethnolinguistic divisions.[70]		Present developments explore the implications of the theory for rhetoric[71] and composition,[72] interpersonal communication,[73] cross-cultural communication,[74] postcolonial studies,[75] rural community development,[5] popular culture[76] and film studies,[6] and many other areas. The most significant areas of theoretical development at present may be the construction of systematic hermeneutics[77][78][79] and aesthetics[80][81] specific to oral traditions.		The theory of oral tradition encountered early resistance from scholars who perceived it as potentially supporting either one side or another in the controversy between what were known as "unitarians" and "analysts" – that is, scholars who believed Homer to have been a single, historical figure, and those who saw him as a conceptual "author function," a convenient name to assign to what was essentially a repertoire of traditional narrative.[82] A much more general dismissal of the theory and its implications simply described it as "unprovable"[83] Some scholars, mainly outside the field of oral tradition,[84][85][86][87] represent (either dismissively or with approval) this body of theoretical work as reducing the great epics to children's party games like "telephone" or "Chinese whispers". While games provide amusement by showing how messages distort content via uncontextualized transmission, Parry's supporters argue that the theory of oral tradition reveals how oral methods optimized the signal-to-noise ratio and thus improved the quality, stability and integrity of content transmission.[88]		There were disputes concerning particular findings of the theory. For example, those trying to support or refute Crowne's hypothesis found the "Hero on the Beach" formula in numerous Old English poems. Similarly, it was also discovered in other works of Germanic origin, Middle English poetry, and even an Icelandic prose saga. J.A. Dane, in an article[89] characterized as "polemics without rigor"[90] claimed that the appearance of the theme in Ancient Greek poetry, a tradition without known connection to the Germanic, invalidated the notion of "an autonomous theme in the baggage of an oral poet."		Within Homeric studies specifically, Lord's The Singer of Tales, which focused on problems and questions that arise in conjunction with applying oral-formulaic theory to problematic texts such as the Iliad, Odyssey, and even Beowulf, influenced nearly all of the articles written on Homer and oral-formulaic composition thereafter. However, in response to Lord, Geoffrey Kirk published The Songs of Homer, questioning Lord's extension of the oral-formulaic nature of Serbian and Croatian literature (the area from which the theory was first developed) to Homeric epic. Kirk argues that Homeric poems differ from those traditions in their "metrical strictness", "formular system[s]", and creativity. In other words, Kirk argued that Homeric poems were recited under a system that gave the reciter much more freedom to choose words and passages to get to the same end than the Serbo-Croatian poet, who was merely "reproductive".[91][92] Shortly thereafter, Eric Havelock's Preface to Plato revolutionized how scholars looked at Homeric epic by arguing not only that it was the product of an oral tradition, but also that the oral-formulas contained therein served as a way for ancient Greeks to preserve cultural knowledge across many different generations.[93] Adam Parry, in his 1966 work "Have we Homer's Iliad?", theorized the existence of the most fully developed oral poet to his time, a person who could (at his discretion) creatively and intellectually create nuanced characters in the context of the accepted, traditional story. In fact, he discounted the Serbo-Croatian tradition to an "unfortunate" extent, choosing to elevate the Greek model of oral-tradition above all others.[94][95] Lord reacted to Kirk's and Parry's essays with "Homer as Oral Poet", published in 1968, which reaffirmed Lord's belief in the relevance of Yugoslav poetry and its similarities to Homer and downplayed the intellectual and literary role of the reciters of Homeric epic.[96]		Many of the criticisms of the theory have been absorbed into the evolving field as useful refinements and modifications. For example, in what Foley called a "pivotal" contribution, Larry Benson introduced the concept of "written-formulaic" to describe the status of some Anglo-Saxon poetry which, while demonstrably written, contains evidence of oral influences, including heavy reliance on formulas and themes[97] A number of individual scholars in many areas continue to have misgivings about the applicability of the theory or the aptness of the South Slavic comparison,[98] and particularly what they regard as its implications for the creativity which may legitimately be attributed to the individual artist.[99] However, at present, there seems to be little systematic or theoretically coordinated challenge to the fundamental tenets of the theory; as Foley put it, ""there have been numerous suggestions for revisions or modifications of the theory, but the majority of controversies have generated further understanding.[100]		
She's All That is a 1999 American teen romantic comedy film directed by Robert Iscove and starring Freddie Prinze Jr., Rachael Leigh Cook, Paul Walker and Matthew Lillard. It is a modern adaptation of George Bernard Shaw's play Pygmalion and George Cukor's 1964 film My Fair Lady. It was one of the most popular teen films of the 1990s and reached No. 1 at the box office in its first week of release.[1] In April 2015, it was announced that the film will be remade by The Weinstein Company.[2]						Zack Siler (Freddie Prinze Jr.) is the big man on campus at his Southern California high school. His popular and narcissistic girlfriend, Taylor Vaughan (Jodi Lyn O'Keefe), ditches him for a faded reality TV star from The Real World, Brock Hudson (Matthew Lillard), whom she met on Spring Break in Florida. Although bitter over the break-up, Zack consoles himself by claiming that Taylor is replaceable by any girl in the school. Zack's friend, Dean Sampson, Jr. (Paul Walker), disagrees and challenges him to a bet on whether Zack can turn any random girl into the Prom Queen within six weeks, a coveted position held by the most popular girl in school. Dean picks out Laney Boggs (Rachael Leigh Cook), a dorky, solitary, unpopular art student, as his choice for Zack.		Zack approaches and attempts to befriend Laney in the hope of subsequently transforming her into prom queen material. His first encounter with her is a complete failure, when she pointedly ignores his advance and walks away from him. With help from Laney's friend, Jesse Jackson (Elden Henson), Zack eventually is successful in getting Laney to take him to a theater lounge frequented by artists and performers. Intending to deter him, Laney arranges for Zack to be called onto the stage and perform to his surprise. Zack manages to improvise a show with the Hacky Sack he happens to carry in his pocket. Laney is impressed by the performance, but rejects him again after he attempts to charm her.		Zack befriends her brother, Simon (Kieran Culkin), and in an attempt to stop this, Laney agrees to go to the beach with him once. She starts to make friends in the popular crowd as they get a chance to know her. Zack then successfully persuades her to attend a party at Preston's (Dulé Hill) house the same night, and he employs his sister Mac (Anna Paquin) to give her a makeover, transforming her into a stunning beauty. Laney's attendance at Preston's party sparks jealousy in Taylor, who then humiliates Laney, but Laney is consoled by Zack, who has by now developed a true affection for her.		As a result of her new appearance and Zack's interest, Laney is nominated for Prom Queen and begins an uneasy battle with Taylor for the crown. Taylor faces humiliation of her own when Brock informs her their relationship is over, and that he only used her to increase his own popularity (which proves successful with the producers of MTV offering him his own show). In the meantime, Dean begins to show an interest in Laney as her popularity begins to soar and Zack's victory becomes more imminent. Dean tries to invite Laney as his prom partner in an attempt to ruin Zack's attempt to boost Laney's winning chance with his own popularity, but Laney refuses. After falling out with Zack, Dean deliberately tells Laney about the bet and she forces a confession from Zack in public. Feeling objectified and betrayed, Laney refuses to see Zack again.		Unable to reconcile with Laney, Zack ends up attending the prom with his sister, while Taylor arrives alone, thinking that Zack is still interested in her despite his refusal of her advances. A disheartened Laney reluctantly dresses up after some persuasion from her father Wayne (Kevin Pollak) and goes to the dance with Dean when he shows up at her house in a tuxedo to invite her again to be his prom date.		At the prom, after a dance scene presided over by the school's resident DJ (Usher Raymond), Mac meets Jesse and they become friends. Dean boasts to Preston and others in the bathroom that he is succeeding in seducing Laney and has rented a hotel room with intention of having sex with her. Jesse overhears this, in a stall, and warns Mac and Zack. Taylor is then crowned Prom Queen with just over half the votes; she begins a long berating speech which is interrupted by the microphone being turned off by a teacher. By now, the students have seen Taylor for who she is, thanks to Mac revealing her actions against Laney and her involvement with Dean to sabotage Zack. As a result, Taylor is further humiliated when she loses her popularity and her friends. Laney leaves the prom with Dean, while Zack attempts to intervene but loses track of them.		When Laney returns to her home, Zack is there waiting for her, along with her father and Simon who are waiting up for her. Laney explains how she fought off Dean's advances by deafening him with an air horn. Zack confesses his true feelings to Laney, and asks for forgiveness as well as the chance to further their relationship, which she grants. Laney tells Zack that she is considering art school after graduation, and Zack jokingly tells her that she has inspired him to pursue a career in performance art. After their first dance and kiss, Laney asks Zack about his bet with Dean (which is now lost), and Zack responds that he will gracefully honor the terms.		At the graduation ceremony, the terms of the bet are revealed, Zack must appear nude on stage because he lost. After his name is called, Zack heads to the stage wearing only a graduation cap and strategically carrying a soccer ball. In the final shot he is not visible, but we see Laney with the soccer ball being thrown to her and the rest of the students reacting to Zack no longer having it for cover.		R. Lee Fleming, Jr. is officially credited as the sole screenwriter for the film, and in a 2002 interview, M. Night Shyamalan confirmed that he polished the screenplay while adapting Stuart Little and writing a spec script for The Sixth Sense. This was also confirmed in the film's audio commentary by director Robert Iscove.[3]		In 2013, Shyamalan claimed that instead of polishing Fleming, Jr.'s original script, he actually ghost-wrote the film.[4] This was disputed by someone who claimed to be Fleming, Jr.[5] in a message on Twitter that has since been deleted.[6]		On June 17, 2013, Jack Lechner (who served as Miramax's head of development in the late 1990s) confirmed that technically both Shyamalan and Fleming, Jr. contributed to the script: Fleming, Jr. wrote the initial script that Miramax bought while Shyamalan did an uncredited rewrite (doing more than "a polish") that got the film green-lit. Lechner reiterated that content from both writers was included in the final cut of the film.[7][8]		The film has received mixed reviews from different critics. Although Rotten Tomatoes gives the film a score of 38% based on reviews from 58 critics, with the sites consensus: "Despite its charming young leads, She's All That can't overcome its predictable, inconsistently funny script." IMDB set the rating at 32. The film won 8 awards, and was nominated for 5 others. The individual critics were also mixed. The film was the last to be reviewed by Gene Siskel before his death due to complications of brain surgery in 1999, who gave a favorable review and wrote that "Rachael Leigh Cook, as Laney, the plain Jane object of the makeover, is forced to demonstrate the biggest emotional range as a character, and she is equal to the assignment.".[9][10]		Generally regarded as a "feel-good movie", the film was a sleeper hit and reached No. 1 at the box office in the first week of its release in theaters, grossing $16,065,430 over the Super Bowl opening weekend.[1] It earned $63,366,989 in the United States and $39,800,000 at international box offices, totaling $103,166,989 worldwide against a production budget of $10,000,000.[1]		The film won and was nominated for several awards.[11]		
Fiction is the classification for any story or setting that is derived from imagination—in other words, not based strictly on history or fact.[1][2][3] Fiction can be expressed in a variety of formats, including writings, live performances, films, television programs, animations, video games, and role-playing games, though the term originally and most commonly refers to the narrative forms of literature (see literary fiction),[4] including novels, novellas, short stories, and plays. Fiction is occasionally used in its narrowest sense to mean simply any "literary narrative".[5]		A work of fiction is an act of creative imagination, so its total faithfulness to the real-world is not typically assumed by its audience.[6] Therefore, fiction is not expected to present only characters who are actual people or descriptions that are factually accurate. Instead, the context of fiction, not adhering precisely to the real world, is generally open to interpretation.[7] Characters and events within a fictional work may even be openly set in their own context entirely separate from the known universe: a fictional universe that stands on its own. Fiction is regarded as the traditional opposite of non-fiction, whose creators assume responsibility for presenting only the historical and factual truth; however, fictions place in reality is arguable, which has made the distinction between fiction and non-fiction unclear, for example, in postmodern literature.[8]						Traditionally, fiction includes novels, short stories, fables, legends, myths, fairy tales, epic and narrative poetry, plays (including operas, musicals, dramas, puppet plays, and various kinds of theatrical dances). However, fiction may also encompass comic books, and many animated cartoons, stop motions, anime, manga, films, video games, radio programs, television programs (comedies and dramas), etc.		The Internet has had a major impact on the creation and distribution of fiction, calling into question the feasibility of copyright as a means to ensure royalties are paid to copyright holders.[9] Also, digital libraries such as Project Gutenberg make public domain texts more readily available. The combination of inexpensive home computers, the Internet and the creativity of its users has also led to new forms of fiction, such as interactive computer games or computer-generated comics. Countless forums for fan fiction can be found online, where loyal followers of specific fictional realms create and distribute derivative stories. The Internet is also used for the development of blog fiction, where a story is delivered through a blog either as flash fiction or serial blog, and collaborative fiction, where a story is written sequentially by different authors, or the entire text can be revised by anyone using a wiki.		Types of literary fiction in prose:[10]		Fiction is commonly broken down into a variety of genres: subsets of fiction, each differentiated by a particular unifying tone or style, narrative technique, media content, or popularly defined criterion. Science fiction, for example, predicts or supposes technologies that are not realities at the time of the work's creation. For example, Jules Verne's novel From the Earth to the Moon was published in 1865 and only in 1969 did astronaut Neil Armstrong first land on the moon.		Historical fiction places imaginary characters into real historical events. In the early historical novel Waverley, Sir Walter Scott's fictional character Edward Waverley meets a figure from history, Bonnie Prince Charlie, and takes part in the Battle of Prestonpans. Some works of fiction are slightly or greatly re-imagined based on some originally true story, or a reconstructed biography.[13] Often, even when the author claims the fictional story is basically true, there may be artificial additions and subtractions from the true story to make it more interesting. One such example would be Tim O'Brien's The Things They Carried, a series of historical fiction short stories about the Vietnam War.		Fictional works that explicitly involve supernatural, magical, or scientifically impossible elements are often classified under the genre of fantasy, including Lewis Carroll's Alice In Wonderland, J. K. Rowling's Harry Potter series, and J. R. R. Tolkien's The Lord of the Rings. Creators of fantasy sometimes introduce entire imaginary creatures or beings such as dragons and fairies.[3]		Literary fiction is defined as fictional works that are deemed to be of literary merit, as distinguished from most commercial, or "genre" fiction. The distinction can be controversial among critics and scholars.		Neal Stephenson has suggested that while any definition will be simplistic there is today a general cultural difference between literary and genre fiction. On the one hand literary authors nowadays are frequently supported by patronage, with employment at a university or a similar institution, and with the continuation of such positions determined not by book sales but by critical acclaim by other established literary authors and critics. On the other hand, he suggests, genre fiction writers tend to support themselves by book sales.[14] However, in an interview, John Updike lamented that "the category of 'literary fiction' has sprung up recently to torment people like me who just set out to write books, and if anybody wanted to read them, terrific, the more the merrier. ... I'm a genre writer of a sort. I write literary fiction, which is like spy fiction or chick lit".[15] Likewise, on The Charlie Rose Show, he argued that this term, when applied to his work, greatly limited him and his expectations of what might come of his writing, so he does not really like it. He suggested that all his works are literary, simply because "they are written in words".[16]		Literary fiction often involves social commentary, political criticism, or reflection on the human condition.[17] In general it focuses on "introspective, in-depth character studies" of "interesting, complex and developed" characters.[17][18] This contrasts with genre fiction where plot is the central concern.[19] Usually in literary fiction the focus is on the "inner story" of the characters who drive the plot, with detailed motivations to elicit "emotional involvement" in the reader.[20][21] The style of literary fiction is often described as "elegantly written, lyrical, and ... layered".[22] The tone of literary fiction can be darker than genre fiction,[23] while the pacing of literary fiction may be slower than popular fiction.[23] As Terrence Rafferty notes, "literary fiction, by its nature, allows itself to dawdle, to linger on stray beauties even at the risk of losing its way".[24]		Realistic fiction typically involves a story whose basic setting (time and location in the world) is real and whose events could feasibly happen in a real-world setting; non-realistic fiction involves a story where the opposite is the case, often being set in an entirely imaginary universe, an alternative history of the world other than that currently understood as true, or some other non-existent location or time-period, sometimes even presenting impossible technology or a defiance of the currently understood laws of nature. However, all types of fiction arguably invite their audience to explore real ideas, issues, or possibilities in an otherwise imaginary setting, or using what is understood about reality to mentally construct something similar to reality, though still distinct from it.[note 1][note 2]		In terms of the traditional separation between fiction and non-fiction, the lines are now commonly understood as blurred, showing more overlap than mutual exclusion. Even fiction usually has elements of, or grounding in, truth. The distinction between the two may be best defined from the perspective of the audience, according to whom a work is regarded as non-fiction if its people, places, and events are all historically or factually real, while a work is regarded as fiction if it deviates from reality in any of those areas. The distinction between fiction and non-fiction is further obscured by an understanding, on the one hand, that the truth can be presented through imaginary channels and constructions, while, on the other hand, imagination can just as well bring about significant conclusions about truth and reality.		Literary critic James Wood, argues that "fiction is both artifice and verisimilitude", meaning that it requires both creative invention as well as some acceptable degree of believability,[25] a notion often encapsulated in poet Samuel Taylor Coleridge's term: willing suspension of disbelief. Also, infinite fictional possibilities themselves signal the impossibility of fully knowing reality, provocatively demonstrating that there is no criterion to measure constructs of reality.[26]		
A situation comedy, or sitcom, is a genre of comedy centered on a fixed set of characters who carry over from episode to episode. Sitcoms can be contrasted with sketch comedy, where a troupe may use new characters in each sketch, and stand-up comedy, where a comedian tells jokes and stories to an audience. Sitcoms originated in radio, but today are found mostly on television as one of its dominant narrative forms. This form can also include mockumentaries.		A situation comedy television program may be recorded in front of a studio audience, depending on the program's production format. The effect of a live studio audience can be imitated or enhanced by the use of a laugh track.						The terms "situational comedy" or "sitcom" were not commonly used until the 1950s.[1] There were prior examples on radio, but the first television sitcom is said to be Pinwright's Progress, ten episodes being broadcast on the BBC in the United Kingdom between 1946 and 1947.[2][3] In the United States, director and producer William Asher has been credited with being the "man who invented the sitcom",[4] having directed over two dozen of the leading sitcoms, including I Love Lucy, from the 1950s through the 1970s.		There have been few long-running Australian-made sitcoms, but many U.S. and UK sitcoms have been successful there. UK sitcoms are a staple of government broadcaster Australian Broadcasting Corporation (ABC); in the 1970s and 1980s many UK sitcoms also screened on the Seven Network. By 1986, UK comedies Bless This House and Are You Being Served? had been repeated by ABC Television several times, and were then acquired and screened by the Seven Network, in prime time.[5]		In 1981, Daily at Dawn was the first Australian comedy series to feature a regular gay character (Terry Bader as journalist Leslie).[6]		In 1987, Mother and Son was winner of the Television Drama Award presented by the Australian Human Rights Commission.[7][8]		In 2007, Kath & Kim The first episode of series four attracted an Australian audience of 2.521 million nationally,[9] the highest rating ever for a first episode in the history of Australian television,[9] until the series premiere of Underbelly: A Tale of Two Cities in 2009 with 2.58m viewers.[10]		In 2013, Please Like Me was praised by the critics,[11][12][13][14][15][16][17] receiving an invitation to screen at the Series Mania Television Festival in Paris.[18] and has garnered three awards and numerous nominations.[19][20][21][22][23][24][25][26] Also in 2013, At Home With Julia was criticised by several social commentators as inappropriately disrespectful to the office of Prime Minister,[27] the show nevertheless proved very popular both with television audiences — becoming the most watched Australian scripted comedy series of 2011[28] — and with television critics.[29] Nominated to the 2012 Australian Academy of Cinema and Television Arts Awards for Best Television Comedy Series.[30]		Although there have been a number of notable exceptions, Canadian television networks have generally fared poorly with their sitcom offerings, with relatively few Canadian sitcoms attaining notable success in Canada or internationally.[31] According to television critic Bill Brioux, there are a number of structural reasons for this: the shorter seasons typical of Canadian television production make it harder for audiences to connect with a program before its season has concluded, and put even successful shows at risk of losing their audience between seasons because of the longer waiting time before a show returns with new episodes; the more limited marketing budgets available to Canadian television networks mean that audiences are less likely to be aware that the show exists in the first place; and the shows tend to resemble American sitcoms, in the hope of securing a lucrative sale to an American television network, even though by and large the Canadian sitcoms that have been successful have been ones, such as Corner Gas or King of Kensington, that had a more distinctively Canadian flavour.[31] Conversely, however, Canadian television has had much greater success with sketch comedy and dramedy series.[31]		The popular show King of Kensington, aired from 1975 to 1980, prior to the start of the fourth season drew 1.5 to 1.8 million viewers weekly.[32]		Corner Gas, which ran for six seasons from 2004 to 2009, became an instant hit, averaging a million viewers per episode.[33] It has been the recipient of six Gemini Awards, and has been nominated almost 70 times for various awards.[34]		Between 2007 and 2012, the Little Mosque on the Prairie premiere drew an audience of 2.1 million,[35] but declined in its fourth season drawing 420,000 viewers a week, or twenty percent of its original audience.[36]		Other noteworthy recent sitcoms have included Call Me Fitz and Schitt's Creek[37] (both recent winners of the Canadian Screen Award for Best Comedy Series), Letterkenny and Kim's Convenience.[38]		Sitcoms started appearing on Indian television in the 1980s, with serials like Yeh Jo Hai Zindagi (1984) and Wagle Ki Duniya (1988) on the state-run Doordarshan channel. Gradually, as private channels were allowed, many more sitcoms followed in the 1990s, such as Zabaan Sambhalke (1993), Shrimaan Shrimati (1995), Office Office (2001), Khichdi (2002), Sarabhai vs Sarabhai (2005) to F.I.R. (2006- 2015), Taarak Mehta Ka Ooltah Chashmah, (2008–present) & "Uppum Mulakum" (2015–Present).[39][40]		El Chavo del Ocho, which ran from 1971 to 1980, was the most watched show in the Mexican television and had a Latin American audience of 350 million viewers per episode at its peak of popularity during the mid-1970s.[41] The show continues to be popular in Hispanic America as well as in Brazil, Spain, United States and other countries, with syndicated episodes averaging 91 million daily viewers in all of the markets where it is distributed in the Americas.[42][43] Since it ceased production in 1992, the show has earned an estimated billion in syndication fees alone for Televisa.[43]		Gliding On, a popular sit-com in New Zealand in the early 1980s, won multiple awards over the course of its run, including Best Comedy, Best Drama and Best Direction at the Feltex Awards.[44]		The first Russian sitcom series was "Strawberry" (resembled "Duty Pharmacy" in Spanish format), which was aired in 1996-1997 on the RTR channel. However, the "boom" of Russian sitcoms began only in the 2000s - when in 2004 the STS started very successful sitcom "My Fair Nanny" (an adaptation of the American sitcom "The Nanny"). Since that time sitcoms in Russia were produced by the two largest entertainment channels of the country - STS and TNT. In 2007 the STS released the first original domestic sitcom - "Daddy's Daughters" (there were only adaptation before), and in 2010 TNT released "Interns (sitcom)" - the first sitcom, filmed as a comedy (unlike dominated "conveyor" sitcoms).		Although styles of sitcom have changed over the years they tend to be based on a family, workplace or other institution, where the same group of contrasting characters is brought together in each episode. British sitcoms are typically produced in one or more series of six episodes. Most such series are conceived and developed by one or two writers. The majority of British sitcoms are 30 minutes long and are recorded on studio sets in a multiple-camera setup. A subset of British comedy consciously avoids traditional situation comedy themes and storylines to branch out into more unusual topics or narrative methods. Blackadder (1983–89) and Yes Minister/Yes Prime Minister (1980–88, 2013) moved what is often a domestic or workplace genre into the corridors of power. A later development was the mockumentary in such series as The Office (2001–3).		Most American sitcoms generally include episodes of 20 to 30 minutes in length, where the story is written to run a total of 22 minutes in length, leaving eight minutes for commercials.[45]		Some popular British shows have been successfully adapted for the U.S.[46]		The sitcom format was born in January 1926 with the initial broadcast of Sam 'n' Henry on WGN radio in Chicago, Illinois.[47] The 15-minute daily program was revamped in 1928, moved to another station, renamed Amos 'n' Andy, and became one of the most successful sitcoms of the period. It was also one of the earliest examples of radio syndication. Like many radio programs of the time, the two programs continued the American entertainment traditions of vaudeville and the minstrel show.		The Jack Benny Program, a radio-TV comedy series that ran for more than three decades, is generally regarded as a high-water mark in 20th-century American comedy.[48]		Fibber McGee and Molly was one of radio's most popular sitcoms of the 1940s.[49] The weekly half-hour domestic sitcom starring real-life husband and wife Jim Jordan and Marian Driscoll ran from 1935 to 1956 on NBC.		Mary Kay and Johnny, aired from 1947 to 1950, was the first sitcom broadcast on a network television in the United States and was the first program to show a couple sharing a bed, and the first series to show a woman's pregnancy on television.[50][51]		I Love Lucy, which originally ran from 1951 to 1957 on CBS, was the most watched show in the United States in four of its six seasons, and was the first to end its run at the top of the Nielsen ratings (an accomplishment later matched only by The Andy Griffith Show in 1968 and Seinfeld in 1998) . The show is still syndicated in dozens of languages across the world, and remains popular, with an American audience of 40 million each year.[52] Colorized edits of episodes from the original series have aired semi-annually on the network since 2013, six decades after the series aired.[53][54] It is often regarded as one of the greatest and most influential sitcoms in history. In 2012, it was voted the 'Best TV Show of All Time' in a survey conducted by ABC News and People Magazine.[55]		The Honeymooners debuted as a half-hour series on 1955 and was originally aired on the DuMont network's Cavalcade of Stars and subsequently on the CBS network's The Jackie Gleason Show,[56] which was filmed in front of a live audience. Although initially a ratings success—becoming the #2 show in the United States during its first season—it faced stiff competition from The Perry Como Show,[57][58] and eventually dropped to #19,[58][59] ending its production after only 39 episodes (now referred to as the "Classic 39"). The final episode of The Honeymooners aired on September 22, 1956. Creator/producer Jackie Gleason revived The Honeymooners sporadically until 1978. The Honeymooners was one of the first U.S. television shows to portray working-class married couples in a gritty, non-idyllic manner (the show is set mostly in the Kramdens' kitchen, in a neglected Brooklyn apartment building).[60] Steven Sheehan explains the popularity of The Honeymooners as the embodiment of working-class masculinity in the character of Ralph Kramden, and postwar ideals in American society regarding work, housing, consumerism, and consumer satisfaction. The series demonstrated visually the burdens of material obligations and participation in consumer culture, as well as the common use of threats of domestic violence in working class households.[61] Art Carney won five Emmy Awards for his portrayal of Ed Norton — two for the original Jackie Gleason Show, one for The Honeymooners, and two for the final version of The Jackie Gleason Show. He was nominated for another two (1957, 1966) but lost. Gleason and Audrey Meadows were both nominated in 1956 for their work on The Honeymooners. Meadows was also nominated for Emmys for her portrayal of Alice Kramden in 1954 and 1957.[62][63] In 1997, the episodes "The $99,000 Answer" and "TV or Not TV" were respectively ranked #6 and #26 on "TV Guide's 100 Greatest Episodes of All Time"[64] and in 1999, TV Guide published a list titled "TV's 100 Greatest Characters Ever!" Ed Norton was #20, and Ralph Kramden was #2.[65] In 2002, The Honeymooners was listed at #3 on TV Guide's 50 Greatest TV Shows of All Time and #13 on their list of the "60 Greatest Shows of All Time" in 2013.[66]		The Andy Griffith Show, first televised on CBS between 1960 and 1968, was consistently placed in the top ten during its run.[67] The show is one of only three shows to have its final season be the number one ranked show on television, the other two being I Love Lucy and Seinfeld. In 1998, more than 5 million people a day watched the show's re-runs on 120 stations.[68]		The Dick Van Dyke Show, initially aired on CBS from 1961 to 1966, won 15 Emmy Awards. In 1997, the episodes "Coast-to-Coast Big Mouth" and "It May Look Like a Walnut" were ranked at 8 and 15 respectively on TV Guide's 100 Greatest Episodes of All Time.[69] In 2002, it was ranked at 13 on TV Guide's 50 Greatest TV Shows of All Time[70] and in 2013, it was ranked at 20 on their list of the 60 Best Series.[71]		The series M*A*S*H, aired in the U.S. from 1972 to 1983, was honored with a Peabody Award in 1976 and was ranked number 25 on TV Guide's 50 Greatest TV Shows of All Time in 2002.[72][73] In 2013, the Writers Guild of America ranked it as the fifth-best written TV series ever[74] and TV Guide ranked it as the eighth-greatest show of all time.[75] The episodes "Abyssinia, Henry" and "The Interview" were ranked number 20 and number 80, respectively, on TV Guide's 100 Greatest Episodes of All Time in 1997.[76] And the finale, "Goodbye, Farewell and Amen", became the most-watched and highest-rated single television episode in the U.S. television history at the time, with a record-breaking of 125 million viewers (60.2 rating and 77 share),[77] according to The New York Times.[78]		Sanford and Son, which ran from 1972 to 1977, was included on the Time magazine's list of the "100 Best TV Shows of All Time" in 2007.[79]		All in the Family, premiered on January 1971, is often regarded in the United States as one of the greatest television series of all time.[80] Following a lackluster first season, the show became the most watched show in the United States during summer reruns[81] and afterwards ranked number one in the yearly Nielsen ratings from 1971 to 1976. It became the first television series to reach the milestone of having topped the Nielsen ratings for five consecutive years. The episode "Sammy's Visit" was ranked number 13 on TV Guide's 100 Greatest Episodes of All Time.[82] TV Guide's 50 Greatest TV Shows of All Time ranked All in the Family as number four. Bravo also named the show's protagonist, Archie Bunker, TV's greatest character of all time.[83] In 2013, the Writers Guild of America ranked All in the Family the fourth-best written TV series ever,[84] and TV Guide ranked it as the fourth-greatest show of all time.[85]		Cheers which ran for eleven seasons was one of the most successful sitcoms in the 80s, airing from 1982 to 1993. It was followed by a spin-off sitcom in the 90s, Frasier. During its run, Cheers became one of the most popular series of all time and has received critical acclaim. In 1997, the episodes "Thanksgiving Orphans" and "Home Is the Sailor", aired originally in 1987, were respectively ranked No. 7 and No. 45 on TV Guide's 100 Greatest Episodes of All-Time.[86] In 2002, Cheers was ranked No. 18 on TV Guide's 50 Greatest TV Shows of All Time.[87] In 2013, the Writers Guild of America ranked it as the eighth best written TV series[88] and TV Guide ranked it #11 on their list of the 60 Greatest Shows of All Time.[89]		The Cosby Show, airing from 1984 until 1992, spent five consecutive seasons as the number one rated show on television. The Cosby Show and All in the Family are the only sitcoms in the history of the Nielsen ratings, to be the number one show for five seasons. It spent all eight of its seasons in the Top 20.[90] According to TV Guide, the show "was TV's biggest hit in the 1980s, and almost single handedly revived the sitcom genre and NBC's ratings fortunes."[91] TV Guide also ranked it 28th on their list of 50 Greatest Shows. [92] In addition, Cliff Huxtable was named as the "Greatest Television Dad".[93] In May 1992, Entertainment Weekly stated that The Cosby Show helped to make possible a larger variety of shows with a predominantly African-American cast, from In Living Color to The Fresh Prince of Bel-Air.[94]		Seinfeld, which originally ran for nine seasons on NBC from 1989 to 1998, led the Nielsen ratings in seasons six and nine, and finished among the top two (with NBC's ER) every year from 1994 to 1998.[95] In 2002, TV Guide named Seinfeld the greatest television program of all time.[96] In 1997, the episodes "The Boyfriend" and "The Parking Garage" were respectively ranked numbers 4 and 33 on TV Guide's 100 Greatest Episodes of All Time,[97] and in 2009, "The Contest" was ranked #1 on the same magazine's list of TV's Top 100 Episodes of All Time.[98] E! named it the "number 1 reason the '90s ruled."[99] In 2013, the Writers Guild of America named Seinfeld the No. 2 Best Written TV Series of All Time (second to The Sopranos).[100] That same year, Entertainment Weekly named it the No. 3 best TV series of all time[101] and TV Guide ranked it at No. 2.[102]		The Nanny, aired on CBS from 1993 to 1999, earned a Rose d'Or and one Emmy Award, out of a total of twelve nominations.[103][104] The sitcom was the first new show delivered to CBS for the 1993 season and the highest-tested pilot at the network in years.[105] The series was also hugely successful internationally, especially in Australia.[106]		The Fresh Prince of Bel-Air was a sitcom which ran from 1990 to 1996. The series stars Will Smith as a fictionalized version of himself, a street-smart teenager from West Philadelphia who is sent to move in with his wealthy aunt and uncle in their Bel Air mansion after getting into a fight on a local basketball court. The became one of the popular sitcoms during the 90s, despite only one Emmy nomination and moderately positive critical reception.		Friends, which originally aired on NBC from 1994 to 2004, received acclaim throughout its run, becoming one of the most popular television shows of all time.[107] The series was nominated for 62 Primetime Emmy Awards, winning the Outstanding Comedy Series award in 2002 for its eighth season. The show ranked no. 21 on TV Guide's 50 Greatest TV Shows of All Time and no. 7 on Empire magazine's The 50 Greatest TV Shows of All Time.[108][109][110] In 1997, the episode "The One with the Prom Video" was ranked no. 100 on TV Guide's 100 Greatest Episodes of All-Time.[111] In 2013, Friends ranked no. 24 on the Writers Guild of America's 101 Best Written TV Series of All Time and no. 28 on TV Guide's 60 Best TV Series of All Time.[100][112] In 2014, the series was ranked by Mundo Estranho the Best TV Series of All Time.[113]		Frasier, with five wins in its first five seasons, set the record for most consecutive Emmy awards for Outstanding Comedy Series, a record that has since been matched by Modern Family. The series holds the record for most total Emmy wins, 37, shattering the record of 29 which had been set by The Mary Tyler Moore Show. Frasier is considered the most successful spin-off series in television history, beginning its run one season after Cheers went off the air, where the character of Frasier Crane had been appearing for nine years. Frasier ran from 1993-2004.		In early 2000s Curb Your Enthusiasm premiered on HBO. The series was created by Larry David, who stars as a semi-fictionalized version of himself following his life after the end of his work in Seinfeld. Curb Your Enthusiasm has received high critical acclaim and has grown in popularity since its debut. It has been nominated for 38 Primetime Emmy Awards, and Robert B. Weide received an Emmy for Outstanding Directing for a Comedy Series for the episode "Krazee Eyez Killa". The show won the 2002 Golden Globe Award for Best Television Series – Musical or Comedy.[114]		Two and a Half Men is a sitcom that originally aired on CBS for twelve seasons from September 22, 2003 to February 19, 2015. The success of the series led to it being the fourth-highest revenue-generating program for 2012, earning $3.24 million an episode. Arrested Development is a sitcom created by Mitchell Hurwitz, which originally aired on Fox for three seasons from November 2, 2003 to February 10, 2006. A fourth season of 15 episodes was released on Netflix on May 26, 2013. After its debut in 2003, the series received widespread critical acclaim, six Primetime Emmy Awards, and one Golden Globe Award, and has attracted a cult following, including several fan-based websites. In 2007, Time listed the show among its "All-TIME 100 TV Shows"; in 2008, it was ranked 16th on Entertainment Weekly's "New TV Classics" list. In 2011, IGN named Arrested Development the "funniest show of all time". Its humor has been cited as a key influence on later single-camera sitcoms such as 30 Rock and Community.		After its debut in 2003, Arrested Development gained a cult following and received widespread critical acclaim, six Primetime Emmy Awards, and one Golden Globe Award. In 2007, Time listed the show among its "All-TIME 100 TV Shows";[115] in 2008, it was ranked 16th on Entertainment Weekly's "New TV Classics" list.[116] In 2011, IGN named Arrested Development the "funniest show of all time".[117] Its humor has been cited as a key influence on later single-camera sitcoms such as 30 Rock and Community.[118]		How I Met Your Mother was a sitcom which aired from 2005 to 2014 on CBS, lasting 9 seasons. The show won 9 Emmy awards and 18 awards in general, while being nominated for 72 awards. It became successful in many places across the world.		The Big Bang Theory is a sitcom named after the scientific theory. It began airing in 2007 on CBS and is currently on Season 10. The show is set in Pasadena, California and focuses on five main characters (later on others get promoted to starring roles), Leonard Hofstadter (experimental physicist) and Sheldon Cooper (theoretical physicist) who live across the hall from aspiring actress Penny. Leonard and Sheldon are friends with Howard Wolowitz (aerospace engineer) and Rajesh "Raj" Koothrappali (astrophysicist). Later additions include Bernadette Rostenkowski (microbiologist), Amy Farrah Fowler (neurobiologist), Stuart Bloom (comic-book store owner) and Emily Sweeney (dermatologist). Season 7 had 19.96 million viewers, the highest rated and watched season to date.		30 Rock is a satirical sitcom created by Tina Fey that ran on NBC from October 11, 2006, to January 31, 2013. 30 Rock received critical acclaim throughout its run, winning several major awards (including Primetime Emmy Awards for Outstanding Comedy Series in 2007, 2008, and 2009 and nominations for every other year it ran), and appearing on many critics' year-end "best of" 2006-2013 lists. On July 14, 2009, the series was nominated for 22 Primetime Emmy Awards, the most in a single year for a comedy series.		The Office is a sitcom that aired on NBC from March 24, 2005 to May 16, 2013. It is an adaptation of the BBC series of the same name. The first season of The Office was met with mixed reviews, but the following four seasons received widespread acclaim from television critics. These seasons were included on several critics' year-end top TV series lists, winning several awards including four Primetime Emmy Awards, including Outstanding Comedy Series in 2006. While later seasons were criticized for a decline in quality, earlier writers oversaw the final season and ended the show's run with a positive reception.		Modern Family is a mockumentary sitcom that premiered on ABC on September 23, 2009. The series is presented in mockumentary style, with the fictional characters frequently talking directly into the camera. The show won the Emmy Award for Outstanding Comedy Series in each of its first five years and the Emmy Award for Outstanding Supporting Actor in a Comedy Series four times, twice for Eric Stonestreet and twice for Ty Burrell, as well as the Outstanding Supporting Actress in a Comedmy Series twice for Julie Bowen. It has so far won a total of 22 Emmy awards from 75 nominations. It also won the Golden Globe Award for Best Television Series – Musical or Comedy in 2011.		Parks and Recreation, originally running from 2009 until 2015, was part of NBC's "Comedy Night Done Right" programming during its Thursday night prime-time block. The series received mixed reviews during its first season, but after reworking its tone and format, the second and subsequent seasons were widely acclaimed. Throughout its run, Parks and Recreation received several awards and nominations, including two Primetime Emmy Award nominations for Outstanding Comedy Series, six Emmy nominations, a Golden Globe win for Poehler's performance, and a nomination for the Golden Globe Award for Best Television Series – Musical or Comedy. In TIME's 2012 year-end lists issue, Parks and Recreation was named the number one television series of that year.[119] In 2013, after receiving four consecutive nominations in the category, Parks and Recreation won the Television Critics Association Award for Outstanding Achievement in Comedy. It is widely considered one of the best sitcoms of all time.		Brooklyn Nine-Nine is a police sitcom set in the fictional 99th precinct in Brooklyn which premiered in 2013 on Fox. It has won two Creative Arts Emmy Awards, and two Golden Globe Awards: one for Best Television Series – Musical or Comedy and one for Andy Samberg for Best Actor – Television Series Musical or Comedy. Andre Braugher has also been nominated for three consecutive Primetime Emmy Awards for his performance.		Unbreakable Kimmy Schmidt is a web sitcom created by Tina Fey and Robert Carlock. Since its premiere, the show has received critical acclaim, with critic Scott Meslow calling it "the first great sitcom of the streaming era". As of July 14, 2016, the series has been nominated for eleven Primetime Emmy Awards, including two nominations for Outstanding Comedy Series.		Modern critics have disagreed over the utility of the term "sitcom" in classifying shows that have come into existence since the turn of the century. Many contemporary American sitcoms use the single-camera setup and do not feature a laugh track, thus often resembling the dramedy shows of the 1980s and 1990s rather than the traditional sitcom.[120] Other topics of debate have included whether or not cartoons, such as The Simpsons or Family Guy, can be classified as sitcoms.[121]		
In computing, a Digital Object Identifier or DOI is a persistent identifier or handle used to uniquely identify objects, standardized by the ISO.[1] An implementation of the Handle System,[2][3] DOIs are in wide use mainly to identify academic, professional, and government information, such as journal articles, research reports and data sets, and official publications though they also have been used to identify other types of information resources, such as commercial videos.		A DOI aims to be "resolvable", usually to some form of access to the information object to which the DOI refers. This is achieved by binding the DOI to metadata about the object, such as a URL, indicating where the object can be found. Thus, by being actionable and interoperable, a DOI differs from identifiers such as ISBNs and ISRCs which aim only to uniquely identify their referents. The DOI system uses the indecs Content Model for representing metadata.		The DOI for a document remains fixed over the lifetime of the document, whereas its location and other metadata may change. Referring to an online document by its DOI provides more stable linking than simply using its URL, because if its URL changes, the publisher only needs to update the metadata for the DOI to link to the new URL.[4][5][6]		The developer and administrator of the DOI system is the International DOI Foundation (IDF), which introduced it in 2000.[7] Organizations that meet the contractual obligations of the DOI system and are willing to pay to become a member of the system can assign DOIs.[8] The DOI system is implemented through a federation of registration agencies coordinated by the IDF.[9] By late April 2011 more than 50 million DOI names had been assigned by some 4,000 organizations,[10] and by April 2013 this number had grown to 85 million DOI names assigned through 9,500 organizations.						A DOI is a type of Handle System handle, which takes the form of a character string divided into two parts, a prefix and a suffix, separated by a slash. The prefix identifies the registrant of the identifier, and the suffix is chosen by the registrant and identifies the specific object associated with that DOI. Most legal Unicode characters are allowed in these strings, which are interpreted in a case-insensitive manner. The prefix usually takes the form 10.NNNN, where NNNN is a series of at least 4 numbers greater than or equal to 1000, whose limit depends only on the total number of registrants.[11][12] The prefix may be further subdivided with periods, like 10.NNNN.N.[13]		For example, in the DOI name 10.1000/182, the prefix is 10.1000 and the suffix is 182. The "10." part of the prefix distinguishes the handle as part of the DOI namespace, as opposed to some other Handle System namespace,[A] and the characters 1000 in the prefix identify the registrant; in this case the registrant is the International DOI Foundation itself. 182 is the suffix, or item ID, identifying a single object (in this case, the latest version of the DOI Handbook).		DOI names can identify creative works (such as texts, images, audio or video items, and software) in both electronic and physical forms, performances, and abstract works[14] such as licenses, parties to a transaction, etc.		The names can refer to objects at varying levels of detail: thus DOI names can identify a journal, an individual issue of a journal, an individual article in the journal, or a single table in that article. The choice of level of detail is left to the assigner, but in the DOI system it must be declared as part of the metadata that is associated with a DOI name, using a data dictionary based on the indecs Content Model.		The official DOI Handbook explicitly states that DOIs should display on screens and in print in the format "doi:10.1000/182".[15] Contrary to the DOI Handbook, CrossRef, a major DOI registration agency, recommends displaying a URL (for example, https://doi.org/10.1000/182) instead of the officially specified format (for example, doi:10.1000/182)[16][17] This URL provides the location of an HTTP proxy server which will redirect web accesses to the correct online location of the linked item.[8][18] This recommendation is primarily based on the assumption that the DOI is being displayed without being hyper-linked to its appropriate URL – the argument being that without the hyperlink it is not as easy to copy-and-paste the full URL to actually bring up the page for the DOI, thus the entire URL should be displayed, allowing people viewing the page containing the DOI to copy-and-paste the URL, by hand, into a new window/tab in their browser in order to go to the appropriate page for the document the DOI represents.		Major applications of the DOI system currently include:		In the Organisation for Economic Co-operation and Development's publication service OECD iLibrary, each table or graph in an OECD publication is shown with a DOI name that leads to an Excel file of data underlying the tables and graphs. Further development of such services is planned.[19]		A multilingual European DOI registration agency activity, mEDRA, and a Chinese registration agency, Wanfang Data, are active in non-English language markets.		The IDF designed the DOI system to provide a form of persistent identification, in which each DOI name permanently and unambiguously identifies the object to which it is associated. It also associates metadata with objects, allowing it to provide users with relevant pieces of information about the objects and their relationships. Included as part of this metadata are network actions that allow DOI names to be resolved to web locations where the objects they describe can be found. To achieve its goals, the DOI system combines the Handle System and the indecs Content Model with a social infrastructure.		The Handle System ensures that the DOI name for an object is not based on any changeable attributes of the object such as its physical location or ownership, that the attributes of the object are encoded in its metadata rather than in its DOI name, and that no two objects are assigned the same DOI name. Because DOI names are short character strings, they are human-readable, may be copied and pasted as text, and fit into the URI specification. The DOI name-resolution mechanism acts behind the scenes, so that users communicate with it in the same way as with any other web service; it is built on open architectures, incorporates trust mechanisms, and is engineered to operate reliably and flexibly so that it can be adapted to changing demands and new applications of the DOI system.[20] DOI name-resolution may be used with OpenURL to select the most appropriate among multiple locations for a given object, according to the location of the user making the request.[21] However, despite this ability, the DOI system has drawn criticism from librarians for directing users to non-free copies of documents that would have been available for no additional fee from alternative locations.[22]		The indecs Content Model as used within the DOI system associates metadata with objects. A small kernel of common metadata is shared by all DOI names and can be optionally extended with other relevant data, which may be public or restricted. Registrants may update the metadata for their DOI names at any time, such as when publication information changes or when an object moves to a different URL.		The International DOI Foundation (IDF) oversees the integration of these technologies and operation of the system through a technical and social infrastructure. The social infrastructure of a federation of independent registration agencies offering DOI services was modelled on existing successful federated deployments of identifiers such as GS1 and ISBN.		A DOI name differs from commonly used Internet pointers to material, such as the Uniform Resource Locator (URL), in that it identifies an object itself as a first-class entity, rather than the specific place where the object is located at a certain time. It implements the Uniform Resource Identifier (Uniform Resource Name) concept and adds to it a data model and social infrastructure.[23]		A DOI name also differs from standard identifier registries such as the ISBN, ISRC, etc. The purpose of an identifier registry is to manage a given collection of identifiers, whereas the primary purpose of the DOI system is to make a collection of identifiers actionable and interoperable, where that collection can include identifiers from many other controlled collections.[24]		The DOI system offers persistent, semantically-interoperable resolution to related current data and is best suited to material that will be used in services outside the direct control of the issuing assigner (e.g., public citation or managing content of value). It uses a managed registry (providing social and technical infrastructure). It does not assume any specific business model for the provision of identifiers or services and enables other existing services to link to it in defined ways. Several approaches for making identifiers persistent have been proposed. The comparison of persistent identifier approaches is difficult because they are not all doing the same thing. Imprecisely referring to a set of schemes as "identifiers" doesn't mean that they can be compared easily. Other "identifier systems" may be enabling technologies with low barriers to entry, providing an easy to use labeling mechanism that allows anyone to set up a new instance (examples include Persistent Uniform Resource Locator (PURL), URLs, Globally Unique Identifiers (GUIDs), etc.), but may lack some of the functionality of a registry-controlled scheme and will usually lack accompanying metadata in a controlled scheme. The DOI system does not have this approach and should not be compared directly to such identifier schemes. Various applications using such enabling technologies with added features have been devised that meet some of the features offered by the DOI system for specific sectors (e.g., ARK).		A DOI name does not depend on the object's location and, in this way, is similar to a Uniform Resource Name (URN) or PURL but differs from an ordinary URL. URLs are often used as substitute identifiers for documents on the Internet (better characterised as Uniform Resource Identifiers) although the same document at two different locations has two URLs. By contrast, persistent identifiers such as DOI names identify objects as first class entities: two instances of the same object would have the same DOI name.		DOI name resolution is provided through the Handle System, developed by Corporation for National Research Initiatives, and is freely available to any user encountering a DOI name. Resolution redirects the user from a DOI name to one or more pieces of typed data: URLs representing instances of the object, services such as e-mail, or one or more items of metadata. To the Handle System, a DOI name is a handle, and so has a set of values assigned to it and may be thought of as a record that consists of a group of fields. Each handle value must have a data type specified in its <type> field, which defines the syntax and semantics of its data. While a DOI persistently and uniquely identifies the object to which it is assigned, DOI resolution may not be persistent, due to technical and administrative issues.		To resolve a DOI name, it may be input to a DOI resolver, such as doi.org.		Another approach, which avoids typing or cutting-and-pasting into a resolver is to include the DOI in a document as a URL which uses the resolver as an HTTP proxy, such as http://doi.org/ (preferred)[25] or http://dx.doi.org/, both of which support HTTPS. For example, the DOI 10.1000/182 can be included in a reference or hyperlink as https://doi.org/10.1000/182. This approach allows users to click on the DOI as a normal hyperlink. Indeed, as previously mentioned, this is how CrossRef recommends that DOIs always be represented (preferring HTTPS over HTTP), so that if they are cut-and-pasted into other documents, emails, etc., they will be actionable.		Other DOI resolvers and HTTP Proxies include http://hdl.handle.net, http://doi.medra.org, https://doi.pangaea.de/. At the beginning of the year 2016, a new class of alternative DOI resolvers was started by http://doai.io. This service is unusual in that it tries to find a non-paywalled version of a title and redirects you to that instead of the publisher's version.[26][27] Since then, other open-access favoring DOI resolvers have been created, notably https://oadoi.org/ in October 2016.[28] While traditional DOI resolvers solely rely on the Handle System, alternative DOI resolvers first consult open access resources such as BASE (Bielefeld Academic Search Engine).[26][28]		An alternative to HTTP proxies is to use one of a number of add-ons and plug-ins for browsers, thereby avoiding the conversion of the DOIs to URLs,[29] which depend on domain names and may be subject to change, while still allowing the DOI to be treated as a normal hyperlink. For example. the CNRI Handle Extension for Firefox, enables the browser to access Handle System handles or DOIs like hdl:4263537/4000 or doi:10.1000/1 directly in the Firefox browser, using the native Handle System protocol. This plug-in can also replace references to web-to-handle proxy servers with native resolution. A disadvantage of this approach for publishers is that, at least at present, most users will be encountering the DOIs in a browser, mail reader, or other software which does not have one of these plug-ins installed.		The International DOI Foundation (IDF), a non-profit organisation created in 1998, is the governance body of the DOI system.[30] It safeguards all intellectual property rights relating to the DOI system, manages common operational features, and supports the development and promotion of the DOI system. The IDF ensures that any improvements made to the DOI system (including creation, maintenance, registration, resolution and policymaking of DOI names) are available to any DOI registrant. It also prevents third parties from imposing additional licensing requirements beyond those of the IDF on users of the DOI system.		The IDF is controlled by a Board elected by the members of the Foundation, with an appointed Managing Agent who is responsible for co-ordinating and planning its activities. Membership is open to all organizations with an interest in electronic publishing and related enabling technologies. The IDF holds annual open meetings on the topics of DOI and related issues.		Registration agencies, appointed by the IDF, provide services to DOI registrants: they allocate DOI prefixes, register DOI names, and provide the necessary infrastructure to allow registrants to declare and maintain metadata and state data. Registration agencies are also expected to actively promote the widespread adoption of the DOI system, to cooperate with the IDF in the development of the DOI system as a whole, and to provide services on behalf of their specific user community. A list of current RAs is maintained by the International DOI Foundation. The IDF is recognized as one of the federated registrars for the Handle System by the DONA Foundation (of which the IDF is a board member), and is responsible for assigning Handle System prefixes under the top-level 10 prefix.[31]		Registration agencies generally charge a fee to assign a new DOI name; parts of these fees are used to support the IDF. The DOI system overall, through the IDF, operates on a not-for-profit cost recovery basis.		The DOI system is an international standard developed by the International Organization for Standardization in its technical committee on identification and description, TC46/SC9.[32] The Draft International Standard ISO/DIS 26324, Information and documentation – Digital Object Identifier System met the ISO requirements for approval. The relevant ISO Working Group later submitted an edited version to ISO for distribution as an FDIS (Final Draft International Standard) ballot,[33] which was approved by 100% of those voting in a ballot closing on 15 November 2010.[34] The final standard was published on 23 April 2012.[1]		DOI is a registered URI under the info URI scheme specified by IETF RFC 4452. info:doi/ is the infoURI Namespace of Digital Object Identifiers.[35]		The DOI syntax is a NISO standard, first standardised in 2000, ANSI/NISO Z39.84-2005 Syntax for the Digital Object Identifier.[36]		The maintainers of the DOI system have deliberately not registered a DOI namespace for URNs, stating that:		URN architecture assumes a DNS-based Resolution Discovery Service (RDS) to find the service appropriate to the given URN scheme. However no such widely deployed RDS schemes currently exist.... DOI is not registered as a URN namespace, despite fulfilling all the functional requirements, since URN registration appears to offer no advantage to the DOI System. It requires an additional layer of administration for defining DOI as a URN namespace (the string urn:doi:10.1000/1 rather than the simpler doi:10.1000/1) and an additional step of unnecessary redirection to access the resolution service, already achieved through either http proxy or native resolution. If RDS mechanisms supporting URN specifications become widely available, DOI will be registered as a URN.		
Perth (/ˈpɜːrθ/ ( listen)) is the capital and largest city of the Australian state of Western Australia. It is the fourth-most populous city in Australia, with a population of around 1.94 million (as of 9 August 2016[update]) living in Greater Perth.[1] Perth is part of the South West Land Division of Western Australia, with the majority of the metropolitan area located on the Swan Coastal Plain, a narrow strip between the Indian Ocean and the Darling Scarp. The first areas settled were on the Swan River, with the city's central business district and port (Fremantle) both located on its shores.		Perth was founded by Captain James Stirling in 1829 as the administrative centre of the Swan River Colony. It gained city status (currently vested in the smaller City of Perth) in 1856, and was promoted to the status of a Lord Mayorality in 1929.[8] The city is named after Perth, Scotland, due to the influence of Sir George Murray, Member of Parliament for Perthshire and Secretary of State for War and the Colonies. The city's population increased substantially as a result of the Western Australian gold rushes in the late 19th century. During Australia's involvement in World War II, Fremantle served as a base for submarines operating in the Pacific Theatre, and a US Navy Catalina flying boat fleet was based at Matilda Bay.[9] An influx of immigrants after the war, predominantly from Britain, Greece, Italy and Yugoslavia, led to rapid population growth. This was followed by a surge in economic activity flowing from several mining booms in the late 20th and early 21st centuries that saw Perth become the regional headquarters for a number of large mining operations located around the state.		As part of Perth's role as the capital of Western Australia, the state's Parliament and Supreme Court are located within the city, as is Government House, the residence of the Governor of Western Australia. Perth came seventh in the Economist Intelligence Unit's August 2016 list of the world's most liveable cities,[10] and was classified by the Globalization and World Cities Research Network in 2010 as a world city.[11]		Perth is divided into 30 local government areas and 250 suburbs, stretching from Two Rocks in the north to Singleton in the south, and east inland to The Lake. Outside of the main CBD, important urban centres within Perth include Armadale, Fremantle, Joondalup, Midland, and Rockingham. Most of those were originally established as separate settlements, and retained a distinct identity after being subsumed into the wider metropolitan area. Mandurah, Western Australia's second-largest city, has in recent years formed a conurbation with Perth along the coast, though for most purposes it is still considered a separate city.						Aboriginal people have inhabited the Perth area for 38,000 years, as evidenced by archaeological remains at Upper Swan. The Noongar people occupied the southwest corner of Western Australia and lived as hunter-gatherers. The wetlands on the Swan Coastal Plain were particularly important to them, both spiritually (featuring in local mythology) and as a source of food.[12]		The Noongar people know the area where Perth now stands as Boorloo. Boorloo formed part of Mooro, the tribal lands of Yellagonga's group, one of several based around the Swan River and known collectively as the Whadjuk. The Whadjuk were part of a larger group of fourteen tribes that formed the south-west socio-linguistic block known as the Noongar (meaning "the people" in their language), also sometimes called the Bibbulmun.[13] On 19 September 2006 the Federal Court of Australia brought down a judgment recognising Noongar native title over the Perth metropolitan area in the case of Bennell v State of Western Australia [2006] FCA 1243.[14] The judgment was overturned on appeal.[15]		The first documented sighting of the region was made by the Dutch Captain Willem de Vlamingh and his crew on 10 January 1697.[16] Subsequent sightings between this date and 1829 were made by other Europeans, but as in the case of the sighting and observations made by Vlamingh, the area was considered to be inhospitable and unsuitable for the agriculture that would be needed to sustain a settlement.[17]		Although the Colony of New South Wales had established a convict-supported settlement at King George's Sound (later Albany) on the south coast of Western Australia in 1826 in response to rumours that the area would be annexed by France, Perth was the first full-scale settlement by Europeans in the western third of the continent. The British colony would be officially designated Western Australia in 1832, but was known informally for many years as the Swan River Colony after the area's major watercourse.[18]		On 4 June 1829, newly arriving British colonists had their first view of the mainland, and Western Australia's founding has since been recognised by a public holiday on the first Monday in June each year. Captain James Stirling, aboard Parmelia, said that Perth was "as beautiful as anything of this kind I had ever witnessed". On 12 August that year, Helen Dance, wife of the captain of the second ship, Sulphur, cut down a tree to mark the founding of the town.		It is clear that Stirling had already selected the name Perth for the capital well before the town was proclaimed, as his proclamation of the colony, read in Fremantle on 18 June 1829, ended "given under my hand and Seal at Perth this 18th Day of June 1829. James Stirling Lieutenant Governor".[19] The only contemporary information on the source of the name comes from Fremantle's diary entry for 12 August, which records that they "named the town Perth according to the wishes of Sir George Murray".[20] Murray was born in Perth, Scotland, and was in 1829 Secretary of State for the Colonies and Member for Perthshire in the British House of Commons. The town was named after the Scottish Perth,[21] in Murray's honour.[22][23][24]		Beginning in 1831, hostile encounters between the British settlers and the Noongar people – both large-scale land users with conflicting land value systems – increased considerably as the colony grew. The hostile encounters between the two groups of people resulted in a number of events, including the execution of the Whadjuk elder Midgegooroo, the death of his son Yagan in 1833, and the Pinjarra massacre in 1834.		The racial relations between the Noongar people and the Europeans were strained due to these happenings. Because of the large amount of building in and around Boorloo, the local Whadjuk Noongar people were slowly dispossessed of their country. They were forced to camp around prescribed areas, including the swamps and lakes north of the settlement area including Third Swamp, known to them as Boodjamooling. Boodjamooling continued to be a main camp-site for the remaining Noongar people in the Perth region, and was also used by travellers, itinerants, and homeless people. By the gold-rush days of the 1890s they were joined by miners who were en route to the goldfields.[25]		In 1850, Western Australia was opened to convicts at the request of farming and business people looking for cheap labour.[26] Queen Victoria announced the city status of Perth in 1856.[27]		After a referendum in 1900,[28] Western Australia joined the Federation of Australia in 1901.[27] It was the last of the Australian colonies to agree to join the Federation, and did so only after the other colonies had offered several concessions, including the construction of a transcontinental railway line from Port Augusta in South Australia to Kalgoorlie to link Perth with the eastern states.[29]		In 1933, Western Australia voted in a referendum to leave the Australian Federation, with a majority of two to one in favour of secession. However, the state general election held at the same time as the referendum had voted out the incumbent "pro-independence" government, replacing it with a government that did not support the independence movement. Respecting the result of the referendum, the new government nonetheless petitioned the Imperial Parliament at Westminster. The House of Commons established a select committee to consider the issue but after 18 months of negotiations and lobbying, finally refused to consider the matter, declaring that it could not legally grant secession.[28][30]		In 1962, Perth received global media attention when city residents lit their house lights and streetlights as American astronaut John Glenn passed overhead while orbiting the earth on Friendship 7. This led to it being nicknamed the "City of Light".[31][32] The city repeated the act as Glenn passed overhead on the Space Shuttle in 1998.[33][34]		Perth's development and relative prosperity, especially since the mid-1960s,[35] has resulted from its role as the main service centre for the state's resource industries, which extract gold, iron ore, nickel, alumina, diamonds, mineral sands, coal, oil, and natural gas.[36] Whilst most mineral and petroleum production takes place elsewhere in the state, the non-base services provide most of the employment and income to the people of Perth.[37]		The central business district of Perth is bounded by the Swan River to the south and east, with Kings Park on the western end, while the railway reserve formed a northern border. A state and federally funded project named Perth City Link sunk a section of the railway line, to link Northbridge and the CBD for the first time in 100 years. The Perth Arena is a building in the city link area that has received a number of architecture awards.[which?] St Georges Terrace is the prominent street of the area with 1.3 million m2 of office space in the CBD.[38] Hay Street and Murray Street have most of the retail and entertainment facilities. The tallest building in the city is Central Park, which is the seventh tallest building in Australia.[39] The CBD has recently been the centre of a mining-induced boom, with several commercial and residential projects being built, including Brookfield Place, a 244 m (801 ft) office building for Anglo-Australian mining company BHP Billiton.		Perth is set on the Swan River, named for the native black swans by Willem de Vlamingh, captain of a Dutch expedition and namer of WA's Rottnest Island who discovered the birds while exploring the area in 1697.[40] Traditionally, this water body had been known by Aboriginal inhabitants as Derbarl Yerrigan.[41] The city centre and most of the suburbs are located on the sandy and relatively flat Swan Coastal Plain, which lies between the Darling Scarp and the Indian Ocean. The soils of this area are quite infertile. The metropolitan area extends along the coast to Two Rocks in the north and Singleton to the south,[42] a total distance of approximately 125 kilometres (78 mi).[43] From the coast in the west to Mundaring in the east is a total distance of approximately 50 km (31 mi). The Perth metropolitan area covers 6,418 km2 (2,478 sq mi).[2]		Much of Perth was built on the Perth Wetlands, a series of freshwater wetlands running from Herdsman Lake in the west through to Claisebrook Cove in the east.[44]		To the east, the city is bordered by a low escarpment called the Darling Scarp. Perth is on generally flat, rolling land – largely due to the high amount of sandy soils and deep bedrock. The Perth metropolitan area has two major river systems: the first is made up of the Swan and Canning Rivers; the second is that of the Serpentine and Murray Rivers, which discharge into the Peel Inlet at Mandurah.		Perth receives moderate though highly seasonal rainfall, making it the fourth wettest Australian capital city after Darwin, Sydney and Brisbane. Summers are generally hot and dry, lasting from December to late March, with February generally being the hottest month of the year. Winters are relatively cool and wet, making Perth a classic example of a hot-summer Mediterranean climate (Köppen climate classification Csa).[45][46] Perth is a particularly sunny city for this type of climate; it has an average of 8.8 hours of sunshine per day, which equates to around 3200 hours of annual sunshine, and 138.7 clear days annually, making it the sunniest capital city in Australia.[47]		Summer is not completely devoid of rain and humidity, with sporadic rainfall in the form of short-lived thunderstorms, weak cold fronts and on occasions decaying tropical cyclones from Western Australia's north-west, which can bring significant rainfall. Winters are also known to be clear and sunny. The highest temperature recorded in Perth was 46.2 °C (115.2 °F) on 23 February 1991, although Perth Airport recorded 46.7 °C (116.1 °F) on the same day.[47][48] On most summer afternoons a sea breeze, known locally as the "Fremantle Doctor", blows from the southwest, providing relief from the hot north-easterly winds. Temperatures often fall below 30 °C (86 °F) a few hours after the arrival of the wind change.[49] In the summer, the 3 pm dewpoint averages at around 12 °C (54 °F).[47]		Winters are wet but mild, with most of Perth's annual rainfall being between May and September. The lowest temperature recorded in Perth was −0.7 °C (30.7 °F) on 17 June 2006.[48] The lowest temperature within the Perth metropolitan area was −3.4 °C (25.9 °F) on the same day at Jandakot Airport. However, temperatures at or below zero are very rare occurrences and it seldom gets cold enough for frost to form.[50]		The rainfall pattern has changed in Perth and southwest Western Australia since the mid-1970s. A significant reduction in winter rainfall has been observed with a greater number of extreme rainfall events in the summer months,[51] such as the slow-moving storms on 8 February 1992 that brought 120.6 millimetres (4.75 in) of rain,[48][49] and heavy rainfall associated with a tropical low on 10 February 2017, which brought 114.4 millimetres (4.50 in) of rain.[52] Perth was also impacted by a severe thunderstorm on 22 March 2010, which brought 40.2 mm (1.58 in) of rain and caused significant damage in the metropolitan area.[53]		The average temperature of the sea ranges from 18.9 °C (66.0 °F) in October to 23.4 °C (74.1 °F) in March.[54]		Perth is one of the most isolated major cities in the world. The nearest city with a population of more than 100,000 is Adelaide, 2,130 km (1,324 mi) away. Only Honolulu (population 374,660), 3,841 km (2,387 mi) from San Francisco, is more isolated.		Perth is geographically closer to both Dili, East Timor (2,785 km (1,731 mi)), and Jakarta, Indonesia (3,002 km (1,865 mi)), than to Sydney (3,291 km (2,045 mi)), Brisbane (3,604 km (2,239 mi)), or Canberra (3,106 km (1,930 mi)).		Perth is Australia's fourth-most populous city, having overtaken Adelaide's population in 1984.[65] In June 2015 there were approximately 2.04 million residents in the metropolitan area.[1]		In 2006, the largest ancestry groups in the Perth metropolitan areas were: English (534,555 or 28.6%), Australian (479,174 or 25.6%), Irish (115,384 or 6.2%), Scottish (113,846 or 6.1%), Italian (84,331 or 4.5%) and Chinese (53,390 or 2.9%). There were 26,486 Indigenous Australians in the city.[67]		Perth's population is notable for the high proportion of British and Irish born residents. At the 2006 Census, 142,424 England-born Perth residents were counted,[68] narrowly behind Sydney (145,261),[69] despite the fact that Perth had just 35% of the overall population of Sydney.		The ethnic make-up of Perth changed in the second part of the 20th century, when significant numbers of continental European immigrants arrived in the city. Prior to this, Perth's population had been almost completely Anglo-Celtic in ethnic origin. As Fremantle was the first landfall in Australia for many migrant ships coming from Europe in the 1950s and 1960s, Perth started to experience a diverse influx of people, including Italians, Greeks, Dutch, Germans, Croats. The Italian influence in the Perth and Fremantle area has been substantial, evident in places like the "Cappuccino strip" in Fremantle featuring many Italian eateries and shops. In Fremantle the traditional Italian blessing of the fleet festival is held every year at the start of the fishing season. In Northbridge every December is the San Nicola (Saint Nicholas) Festival, which involves a pageant followed by a concert, predominantly in Italian. Suburbs surrounding the Fremantle area, such as Spearwood and Hamilton Hill, also contain high concentrations of Italians, Croatians and Portuguese. Perth also has a small Jewish community – numbering 5,082 in 2006[66] – who have emigrated primarily from Eastern Europe and more recently from South Africa.		Another more recent wave of arrivals includes white minorities from Southern Africa. South African residents overtook those born in Italy as the fourth largest foreign group in 2001. By 2006, there were 18,825 South Africans residing in Perth, accounting for 1.3% of the city's population.[68] Many Afrikaners and Anglo-Africans emigrated to Perth during the 1980s and 1990s, with the phrase "packing for Perth" becoming associated with South Africans who choose to emigrate abroad, sometimes regardless of the destination.[70] As a result, the city has been described as "the Australian capital of South Africans in exile".[71] The reason for Perth being so popular among white South Africans has often been the location, the vast amount of land, and the slightly warmer climate compared to other large Australian cities – Perth has a Mediterranean climate reminiscent of Cape Town.		Since the late 1970s, Southeast Asia has become an increasingly important source of migrants, with communities from Vietnam, Malaysia, Indonesia, Thailand, Singapore, Hong Kong, Mainland China, and India all now well-established. There were 53,390 persons of Chinese descent in Perth in 2006 – 2.9% of the city's population.[72] These are supported by the Australian Eurasian Association of Western Australia,[73] which also serves a community of Portuguese-Malacca Eurasian or Kristang immigrants.[74]		The Indian community includes a substantial number of Parsees who emigrated from Bombay – Perth being the closest Australian city to India – and the India-born population of the city at the time of the 2006 census was 14,094 or 0.8%.[72] Perth is also home to the largest population of Anglo-Burmese in the world; many settled here following the independence of Burma in 1948 with immigration taking off after 1962. The city is now the cultural hub for Anglo-Burmese worldwide.[75] There is also a substantial Anglo-Indian population in Perth, who also settled in the city following the independence of India.		Protestants, predominantly Anglican, make up approximately 28% of the population.[76][77] Perth is the seat of the Anglican Diocese of Perth[78] and of the Roman Catholic Archdiocese of Perth.[79] Roman Catholics make up about 23% of the population,[76] and Catholicism is the most common single denomination.[76] Perth is also home to 12,000 Latter-day Saints[80] and the Perth Australia Temple of The Church of Jesus Christ of Latter-day Saints. Perth is also home of the seat of the Personal Ordinariate of Our Lady of the Southern Cross as the Church of St Ninian and St Chad in Perth was named the principal church of the ordinariate.[81]		Buddhism and Islam each claim more than 20,000 adherents.[76] Perth has the third largest Jewish population in Australia,[82] numbering approximately 20,000,[76] with both Orthodox and Progressive synagogues and a Jewish Day School.[83] The Bahá'í community in Perth numbers around 1,500.[76] Hinduism has over 20,000 adherents in Perth;[76] the Diwali (festival of lights) celebration in 2009 attracted over 20,000 visitors. There are Hindu temples in Canning Vale, Anketell and a Swaminarayan temple north of the Swan River.[84] Hinduism is the fastest growing religion in Australia.[85]		Approximately one in five people from Perth profess to having no religion, with 11% of people not specific as to their beliefs.[86] One hundred years ago this figure was one in 250 (0.4%).[86] Internationally this is not an isolated occurrence as other countries such as New Zealand and Great Britain are reporting similar increases.[86]		Perth houses the Parliament of Western Australia and the Governor of Western Australia. As of the 2008 state election[update], 42 of the Legislative Assembly's 59 seats and 18 of the Legislative Council's 36 seats are based in Perth's metropolitan area. Perth is represented by 9 full seats and significant parts of three others in the Federal House of Representatives, with the seats of Canning, Pearce and Brand including some areas outside the metropolitan area. The metropolitan area is divided into over 30 local government bodies, including the City of Perth which administers Perth's central business district.		The state's highest court, the Supreme Court, is located in Perth,[87] along with the District[88] and Family[89] Courts. The Magistrates' Court has six metropolitan locations.[90] The Federal Court of Australia and the Federal Circuit Court of Australia (previously the Federal Magistrates Court)[91][92] occupy the Commonwealth Law Courts building on Victoria Avenue,[93] which is also the location for annual Perth sittings of Australia's High Court.[94]		The administrative region of Perth includes 30 local governments, with the outer extent being the City of Wanneroo and the City of Swan to the north, the Shire of Mundaring, Shire of Kalamunda and the City of Armadale to the east, the Shire of Serpentine-Jarrahdale to the southeast and the City of Rockingham to the southwest, and including the islands of Rottnest Island and Garden Island off the west coast;[95] this also correlates with the Metropolitan Region Scheme. Perth can also be defined by its wider extent of Greater Perth.[96][97]		By virtue of its population and role as the administrative centre for business and government, Perth dominates the Western Australian economy, despite the major mining, petroleum, and agricultural export industries being located elsewhere in the state.[98] Perth's function as the state's capital city, its economic base and population size have also created development opportunities for many other businesses oriented to local or more diversified markets.		Perth's economy has been changing in favour of the service industries since the 1950s. Although one of the major sets of services it provides is related to the resources industry and, to a lesser extent, agriculture, most people in Perth are not connected to either; they have jobs that provide services to other people in Perth.[99]		As a result of Perth's relative geographical isolation, it has never had the necessary conditions to develop significant manufacturing industries other than those serving the immediate needs of its residents, mining, agriculture and some specialised areas, such as, in recent times, niche ship building and maintenance. It was simply cheaper to import all the needed manufactured goods from either the eastern states or overseas.		Industrial employment influenced the economic geography of Perth. After WWII, Perth experienced suburban expansion aided by high levels of car ownership. Workforce decentralisation and transport improvements made it possible for the establishment of small-scale manufacturing in the suburbs. Many firms took advantage of relatively cheap land to build spacious, single-storey plants in suburban locations with plentiful parking, easy access and minimal traffic congestion. "The former close ties of manufacturing with near-central and/or rail-side locations were loosened."[98]		Industrial estates such as Kwinana, Welshpool and Kewdale were post-war additions contributing to the growth of manufacturing south of the river. The establishment of the Kwinana industrial area was supported by standardisation of the east-west rail gauge linking Perth with eastern Australia. Since the 1950s the area has been dominated by heavy industry, including an oil refinery, steel-rolling mill with a blast furnace, alumina refinery, power station and a nickel refinery. Another development, also linked with rail standardisation, was in 1968 when the Kewdale Freight Terminal was developed adjacent to the Welshpool industrial area, replacing the former Perth railway yards.[98]		With significant population growth post-WWII,[100] employment growth occurred not in manufacturing but in retail and wholesale trade, business services, health, education, community and personal services and in public administration. Increasingly it was these services sectors, concentrated around the Perth metropolitan area, that provided jobs.[98]		Education is compulsory in Western Australia between the ages of six and seventeen, corresponding to primary and secondary school.[101] Tertiary education is available through a number of universities and technical and further education (TAFE) colleges.		Students may attend either public schools, run by the state government's Department of Education, or private schools, usually associated with a religion.		The Western Australian Certificate of Education (WACE) is the credential given to students who have completed Years 11 and 12 of their secondary schooling.[102]		In 2012 the minimum requirements for students to receive their WACE changed[how?].[103]		Perth is home to four public universities: the University of Western Australia, Curtin University, Murdoch University, and Edith Cowan University. There is also one private university, the University of Notre Dame.		The University of Western Australia, which was founded in 1911,[104] is renowned as one of Australia's leading research institutions.[105] The university's monumental neo-classical architecture, most of which is carved from white limestone, is a notable tourist destination in the city. It is the only university in the state to be a member of the Group of Eight, as well as the Sandstone universities. It is also the state's only university to have produced a Nobel Laureate[106] – Barry Marshall who graduated with a Bachelor of Medicine, Bachelor of Surgery in 1975 and was awarded a joint Nobel Prize for physiology or medicine in 2005, together with Robin Warren.		Curtin University (previously known as Western Australian Institute of Technology (1966-1986) and Curtin University of Technology (1986-2010) is Western Australia's largest university by student population.		Murdoch University was founded in the 1973, and incorporates Western Australia's only veterinary school.		Edith Cowan University was established in 1991 from the existing Western Australian College of Advanced Education (WACAE) which itself was formed in the 1970s from the existing Teachers Colleges at Claremont, Churchlands, and Mount Lawley. It incorporates the Western Australian Academy of Performing Arts (WAAPA).		The University of Notre Dame Australia was established in 1990. Notre Dame was established as a Catholic university with its lead campus in Fremantle and a large campus in Sydney. Its campus is set in the west end of Fremantle, using historic port buildings built in the 1890s, giving Notre Dame a distinct European university atmosphere. Though Notre Dame shares its name with the University of Notre Dame in Indiana USA, it is a separate institution, claiming only "strong ties" with its American namesake.[citation needed]		Colleges of TAFE provide trade and vocational training, including certificate- and diploma-level courses. TAFE began as a system of technical colleges and schools under the Education Department, from which they were separated in the 1980s and ultimately formed into regional colleges. Four exist in the Perth metropolitan area: Central Institute of Technology (formerly Central TAFE); West Coast Institute of Training (northern suburbs); Polytechnic West (eastern and south-eastern suburbs; formerly Swan TAFE); and Challenger Institute of Technology (Fremantle/Peel).		Perth is served by twenty-nine digital free-to-air television channels:		ABC, SBS, Seven, Nine and Ten were also broadcast in an analogue format until 16 April 2013, when the analogue transmission was switched off.[107] Community station Access 31 closed in August 2008. In April 2010 a new community station, West TV, began transmission (in digital format only).		Foxtel provides a subscription-based satellite and cable television service. Perth has its own local newsreaders on ABC (James McHale), Seven (Rick Ardon, Susannah Carr), Nine (Emmy Kubainski, Tim McMillan) and Ten (Narelda Jacobs).		Television shows produced in Perth include local editions of the current affair program Today Tonight, and other types of programming such as The Force. An annual telethon has been broadcast since 1968 to raise funds for charities including Princess Margaret Hospital for Children. The 24-hour Perth Telethon claims to be "the most successful fundraising event per capita in the world"[108] and raised more than A$20 million in 2013, with a combined total of over A$153 million since 1968.[109]		The main newspapers for Perth are The West Australian and The Sunday Times. Localised free community papers cater for each local government area. There are also many advertising newspapers, such as The Quokka. The local business paper is Western Australian Business News.		Radio stations are on AM, FM and DAB+ frequencies. ABC stations include ABC News (585AM), 720 ABC Perth, Radio National (810AM), Classic FM (97.7FM) and Triple J (99.3FM). The six local commercial stations are: 92.9, Nova 93.7, Mix 94.5, 96fm, on FM and 882 6PR and 1080 6IX on AM. DAB+ has mostly the same as both FM and AM plus national stations from the ABC/SBS, Radar Radio and Novanation, along with local stations My Perth Digital, HotCountry Perth, and 98five Christian radio. Major community radio stations include RTRFM (92.1FM), Sonshine FM (98.5FM),[110] SportFM (91.3FM)[111] and Curtin FM (100.1FM).[112]		Online news media covering the Perth area include TheWest.com.au backed by The West Australian, Perth Now from the newsroom of The Sunday Times, WAToday from Fairfax Media and other outlets like TweetPerth[113] on social media.		The Perth Cultural Centre is the location of the city's major arts, cultural and educational institutions, including the Art Gallery of Western Australia, Western Australian Museum, State Library of Western Australia, State Records Office, and Perth Institute of Contemporary Arts (PICA).[114] The State Theatre Centre of Western Australia is also located there,[114] and is the home of the Black Swan State Theatre Company[115] and the Perth Theatre Company.[116] Other performing arts companies based in Perth include the West Australian Ballet, the West Australian Opera and the West Australian Symphony Orchestra, all of which present regular programmes.[117][118][119] The Western Australian Youth Orchestras provide young musicians with performance opportunities in orchestral and other musical ensembles.[120]		Perth is also home to the internationally regarded Western Australian Academy of Performing Arts at Edith Cowan University, from which many successful actors and broadcasters have launched their careers.[121][122] The city's main performance venues include the Riverside Theatre within the Perth Convention Exhibition Centre,[123] the Perth Concert Hall,[124] the historic His Majesty's Theatre,[125] the Regal Theatre in Subiaco[126] and the Astor Theatre in Mount Lawley.[127] Perth Arena can be configured as an entertainment or sporting arena, and concerts are also hosted at other sporting venues, including Subiaco Oval, HBF Stadium, and nib Stadium. Outdoor concert venues include Quarry Amphitheatre, Supreme Court Gardens, Kings Park and Russell Square.		A number of annual events are held in Perth. The Perth International Arts Festival is a large cultural festival that has been held annually since 1953, and has since been joined by the Winter Arts festival, Perth Fringe Festival, and Perth Writers Festival. Perth also hosts annual music festivals including Listen Out, Origin and St Jerome's Laneway Festival. The Perth International Comedy Festival features a variety of local and international comedic talent, with performances held at the Astor Theatre and nearby venues in Mount Lawley, and regular night food markets throughout the summer months across Perth and its surrounding suburbs. Sculpture by the Sea showcases a range of local and international sculptors' creations along Cottesloe Beach. There is also a wide variety of public art and sculptures on display across the city, throughout the year.		Perth has featured in a variety of artistic works in various mediums. An early novel, Moondyne, set in the Swan River Colony, was written by a former Fenian convict, John Boyle O'Reilly, and a A Faithful Picture, edited by Peter Cowan, gives a good idea of the early days of the colony. Songs that refer to the city include "I Love Perth" (1996) by Pavement, and "Perth" (2011) by Bon Iver, while a number of films feature Perth: Last Train to Freo, Two Fists, One Heart, Thunderstruck, Bran Nue Dae, Japanese Story and Nickel Queen. The industrial metal band Fear Factory recorded the video for their single "Cyberwaste" in South Fremantle.		Because of Perth's relative isolation from other Australian cities, overseas performing artists often exclude it from their Australian tour schedules. This isolation, however, has developed a strong local music scene, and the development of local music groups such as The Dugites, Eurogliders, John Butler Trio, The Triffids, Pendulum, Eskimo Joe, Pond, Tame Impala, Karnivool, Gyroscope, Jebediah, Little Birdy, The Panics, Birds of Tokyo and Troye Sivan. Celebrity musical performers from Perth have included the late AC/DC lead singer Bon Scott, who has been remembered with a statue in Fremantle, and veteran performer and artist Rolf Harris, given the nickname "The Boy From Bassendean". The largest performance area within the State Theatre Centre, the Heath Ledger Theatre, is named in honour of Perth-born film actor Heath Ledger. Actor Isla Fisher was raised in Perth and Hugh Jackman studied at the Western Australian Academy of Performing Arts. 13 Reasons Why actress, Katherine Langford was born and raised in Perth.		Tourism in Perth is an important part of the state's economy, with approximately 2.8 million domestic visitors and 0.7 million international visitors in the year ending March 2012.[128] Tourist attractions are generally focused around the city centre, Fremantle, the coast, and the Swan River. In addition to the Perth Cultural Centre, there are a number of museums across the city. The Scitech Discovery Centre in West Perth is an interactive science museum, with regularly changing exhibitions on a large range of science and technology based subjects. Scitech also conducts live science demonstration shows, and operates the adjacent Horizon planetarium. The Western Australian Maritime Museum in Fremantle displays maritime objects from all eras. It houses Australia II, the yacht that won the 1983 America's Cup, as well as a former Royal Australian Navy submarine. Also located in Fremantle is the Army Museum of Western Australia, situated within a historic artillery barracks. The museum consists of several galleries which reflect the Army's involvement in Western Australia, and the military service of Western Australians.[129] The museum holds numerous items of significance, including three Victoria Crosses.[130] Aviation history is represented by the Aviation Heritage Museum in Bull Creek, with its significant collection of aircraft, including a Lancaster bomber and a Catalina of the type operated from the Swan River during WWII.[131] There are many heritage sites in Perth's CBD, Fremantle, and other parts of the metropolitan areas. Some of the oldest remaining buildings, dating back to the 1830s, include the Round House in Fremantle, the Old Mill in South Perth, and the Old Court House in the city centre. Registers of important buildings are maintained by the Heritage Council of Western Australia and local governments. A late heritage building is the Perth Mint.[132] Elizabeth Quay is also a notable attraction in Perth, featuring Swan Bells and a panoramic view of Swan River.		Retail shopping in the Perth CBD is focused around Murray Street and Hay Street. Both of these streets are pedestrian malls between William Street and Barrack Street. Forrest Place is another pedestrian mall, connecting the Murray Street mall to Wellington Street and the Perth railway station. A number of arcades run between Hay Street and Murray Street, including the Piccadilly Arcade, which housed the Piccadilly Cinema until it closed in late 2013. Other shopping precincts include Harbour Town in West Perth, featuring factory outlets for major brands, the historically significant Fremantle Markets, which date back to 1897, and the Midland townsite on Great Eastern Highway, combining historic development around the Town Hall and Post Office buildings with the modern Midland Gate shopping centre further east. Joondalup's central business district is largely a shopping and retail area lined with townhouses and apartments, and also features Lakeside Joondalup Shopping City. Joondalup was granted the status of "tourism precinct" by the State Government in 2009, allowing for extended retail trading hours. The Swan Valley, with fertile soil, uncommon in the Perth region, features numerous wineries such as the large complex at Houghtons, the state's biggest producer, Sandalfords and many smaller operators, including microbreweries and rum distilleries. The Swan Valley also contains specialised food producers, many restaurants and cafes, and roadside local-produce stalls that sell seasonal fruit throughout the year. Tourist Drive 203 is a circular route in the Swan Valley, passing by many attractions on West Swan Road and Great Northern Highway.		Kings Park, located in central Perth between the CBD and the University of Western Australia, is one of the world's largest inner-city parks,[133] at 400.6 hectares (990 acres).[134] There are many landmarks and attractions within Kings Park, including the State War Memorial Precinct on Mount Eliza, Western Australian Botanic Garden, and children's playgrounds. Other features include DNA Tower, a 15m high double helix staircase that resembles the deoxyribonucleic acid (DNA) molecule,[135] and Jacob's Ladder, comprising 242 steps that lead down to Mounts Bay Road. Hyde Park is another inner-city park located 2 km (1.2 mi) north of the CBD. It was gazetted as a public park in 1897, created from 15 ha (37 acres) of a chain of wetlands known as Third Swamp.[136] Avon Valley, John Forrest and Yanchep national parks are areas of protected bushland at the northern and eastern edges of the metropolitan area. Within the city's northern suburbs is Whiteman Park, a 4,000-hectare (9,900-acre) bushland area, with bushwalking trails, bike paths, sports facilities, playgrounds, a vintage tramway, a light railway on a 6-kilometre (3.7 mi) track, motor and tractor museums, and Caversham Wildlife Park.		Perth Zoo, located in South Perth, houses a variety of Australian and exotic animals from around the globe. The zoo is home to highly successful breeding programs for orangutans and giraffes, and participates in captive breeding and reintroduction efforts for a number of Western Australian species, including the numbat, the dibbler, the chuditch, and the western swamp tortoise.[137] More wildlife can be observed at the Aquarium of Western Australia in Hillarys, which is Australia's largest aquarium, specialising in marine animals that inhabit the 12,000-kilometre-long (7,500 mi) western coast of Australia. The northern Perth section of the coastline is known as Sunset Coast; it includes numerous beaches and the Marmion Marine Park, a protected area inhabited by tropical fish, Australian sea lions and bottlenose dolphins, and traversed by humpback whales. Tourist Drive 204, also known as Sunset Coast Tourist Drive, is a designated route from North Fremantle to Iluka along coastal roads.		The climate of Perth allows for extensive outdoor sporting activity, and this is reflected in the wide variety of sports available to residents of the city. Perth was host to the 1962 Commonwealth Games and the 1987 America's Cup defence (based at Fremantle). Australian rules football is the most popular spectator sport in Perth – nearly 23% of Western Australians attended a match at least once in 2009–2010.[138] The two Australian Football League teams located in Perth, the West Coast Eagles and the Fremantle Football Club, have two of the largest fan bases in the country. The Eagles, the older club, is one of the most successful teams in the league, and one of the largest sporting clubs in Australia.The next level of football is the Western Australian Football League, comprising nine clubs each having a League, Reserves and Colts team. Each of these clubs has a junior football system for all genders, and ages from 7 up to 17. The next level of football is the Western Australian Amateur Football League, comprising 68 clubs servicing senior footballers within the metropolitan area. Other popular sports include cricket, basketball, association football (soccer), and rugby union.[139]		Perth has hosted numerous state and international sporting events. Ongoing international events include the Hopman Cup during the first week of January at the Perth Arena, and the Perth International golf tournament at Lake Karrinyup Country Club. In addition to these Perth has hosted the Rally Australia of the World Rally Championships from 1989 to 2006, international Rugby Union games, including qualifying matches for 2003 Rugby World Cup. The 1991 and 1998 FINA World Championships were held in Perth.[140] Four races (2006, 2007, 2008 and 2010) in the Red Bull Air Race World Championship have been held on a stretch of the Swan River called Perth Water, using Langley Park as a temporary air field.[141] Several motorsport facilities exist in Perth including Perth Motorplex, catering to drag racing and speedway, and Barbagallo Raceway for circuit racing and drifting, which hosts a V8 Supercars round. Perth also has two thoroughbred racing facilities: Ascot, home of the Railway Stakes and Perth Cup; and Belmont Park.		The WACA Ground opened in the 1890s and has hosted Test cricket since 1970. The Western Australian Athletics Stadium opened in 2009.		Perth has ten large hospitals with emergency departments. As of 2013[update], Royal Perth Hospital in the city centre is the largest, with others spread around the metropolitan area: Armadale Kelmscott District Memorial Hospital, Joondalup Health Campus, King Edward Memorial Hospital for Women in Subiaco, Rockingham General Hospital, Sir Charles Gairdner Hospital in Nedlands, St John of God Murdoch Hospital, Midland Health Campus in Midland, and Fiona Stanley Hospital in Murdoch. Princess Margaret Hospital for Children is the state's only specialist children's hospital, and Graylands Hospital is the only public stand-alone psychiatric teaching hospital. Most of these are public hospitals, with some operating under public-private partnerships. St John of God Murdoch Hospital is privately owned and operated.		New hospitals are under construction to replace ageing facilities. A new children's hospital, due to open in 2015, is being constructed next to Sir Charles Gairdner Hospital, and will replace Princess Margaret Hospital.[142] Midland Health Campus, a public and a private hospital, is under construction in Midland. St John of God Health Care will build and operate the new hospitals under a public-private partnership with the state government. Midland Health Campus will open in late 2015, and replace the nearby Swan District Hospital.[143]		A number of other public and private hospitals operate in Perth.[144]		Perth is served by Perth Airport in the city's east for regional, domestic and international flights and Jandakot Airport in the city's southern suburbs for general aviation and charter flights.		Perth has a road network with three freeways and nine metropolitan highways. The Northbridge tunnel, part of the Graham Farmer Freeway, is the only significant road tunnel in Perth.		Perth metropolitan public transport, including trains, buses and ferries, are provided by Transperth, with links to rural areas provided by Transwa. There are 70 railway stations and 15 bus stations in the metropolitan area.		Perth provides zero-fare bus and train trips around the city centre (the "Free Transit Zone"), including four high-frequency CAT bus routes.		The Indian Pacific passenger rail service connects Perth with Adelaide and Sydney once per week in each direction. The Prospector passenger rail service connects Perth with Kalgoorlie via several Wheatbelt towns, while the Australind connects to Bunbury, and the AvonLink connects to Northam.		Rail freight terminates at the Kewdale Rail Terminal, 15 km (9 mi) south-east of the city centre.		Perth's main container and passenger port is at Fremantle, 19 km (12 mi) south west at the mouth of the Swan River.[145] The Fremantle Outer Harbour at Cockburn Sound is one of Australia’s major bulk cargo ports.[146]		Perth's electricity is predominantly generated, supplied, and retailed by three Western Australian Government corporations. Verve Energy operates coal and gas power generation stations, as well as wind farms and other power sources.[147] The physical network is maintained by Western Power,[148] while Synergy, the state's largest energy retailer, sells electricity to residential and business customers.[149]		Alinta Energy, which was previously a government owned company, had a monopoly in the domestic gas market since the 1990s. However, in 2013 Kleenheat Gas began operating in the market, allowing consumers to choose their gas retailer.[150]		The Water Corporation is the dominant supplier of water, as well as wastewater and drainage services, in Perth and throughout the Western Australia. It is also owned by the state government.[151]		Perth's water supply has traditionally relied on both groundwater and rain-fed dams. Reduced rainfall in the region over recent decades had greatly lowered inflow to reservoirs and affected groundwater levels. Coupled with the city's relatively high growth rate, this led to concerns that Perth could run out of water in the near future.[152] The Western Australian Government responded by building desalination plants, and introducing mandatory household sprinkler restrictions. The Kwinana Desalination Plant was opened in 2006,[153][154] and Southern Seawater Desalination Plant at Binningup (on the coast between Mandurah and Bunbury) began operating in 2011. A trial winter (1 June – 31 August) sprinkler ban was introduced in 2009 by the State Government, a move which the Government later announced would be made permanent.[155]		National and ACT Canberra		NSW Sydney		NT Darwin		QLD Brisbane		SA Adelaide		TAS Hobart		VIC Melbourne		WA Perth		
Family Matters is an American sitcom which originated on ABC from September 22, 1989 to May 9, 1997, before moving to CBS from September 19, 1997 to July 17, 1998. A spin-off of Perfect Strangers, the series revolves around the Winslow family, a middle-class African American family living in Chicago, Illinois.[1] Midway through the first season, the show introduced the Winslows' nerdy neighbor Steve Urkel (Jaleel White), who quickly became its breakout character and eventually the show's main character.[2] Having run for nine seasons, Family Matters became the second longest-running non-animated US sitcom with a predominantly African American cast, behind only The Jeffersons (11). Having aired 215 episodes, Family Matters is ranked third, behind only Tyler Perry's House of Payne (254), and The Jeffersons (253).						The series originally focused on the character of police officer Carl Winslow and his family: wife Harriette, son Eddie, elder daughter Laura, and younger daughter Judy (who appeared until the character was written out in season four).[3] In the pilot episode, "The Mama Who Came to Dinner," the family had also opened their home to Carl's street-wise mother, Estelle (Rosetta LeNoire), usually known as "Mother Winslow." Prior to the start of the series, Harriette's sister, Rachel Crawford and her infant son, Richie, had moved into the Winslow household after the death of Rachel's husband. The Winslows' nerdy teenage next-door neighbor, Steve Urkel (Jaleel White), was introduced midway through the first season in the episode "Laura's First Date" and quickly became the focus of the show.[4] The popular sitcom was a mainstay of ABC's TGIF lineup from 1989 until 1997, at which point it became part of the CBS Block Party lineup for its final season. Family Matters was produced by Bickley-Warren Productions and Miller-Boyett Productions, in association with Lorimar Television (1989–1993) and later Warner Bros. Television (1993–1998). As the show progressed, episodes began to center increasingly on Steve Urkel, and other original characters also played by White, including Steve's suave alter-ego, Stefan Urquelle, and his female cousin, Myrtle Urkel.		In early 1997, CBS picked up Family Matters and Step by Step in a $40 million deal to acquire the rights to the programs from ABC.[5] ABC then promised to pay Miller-Boyett Productions $1.5 million per episode for a ninth and tenth season of Family Matters. However, tensions had risen between Miller-Boyett Productions and ABC's corporate parent, The Walt Disney Company (which had bought the network in 1995 as part of its merger with ABC's then-parent Capital Cities/ABC, Inc.). Miller-Boyett thought that it would not be a big player on ABC after the network's recent purchase by Disney. In turn, Miller-Boyett Productions agreed to a $40 million offer from CBS for a 22-episode season for both Family Matters and Step By Step. CBS scheduled Family Matters along with Step By Step as a part of its new Friday lineup branded as the CBS Block Party and scheduled the family-oriented block against ABC's TGIF lineup, where the two series originated. Near the end of the ninth season, the cast was informed that a tenth and final season was planned, so scripts and plot synopses were written for the show. Ultimately, due to poor ratings, CBS cancelled Family Matters (as well as Step By Step) after one season, along with the rest of the "Block Party" lineup. CBS also pulled the show from its regular schedule in the winter. As a result, the series finale was broadcast with little fanfare during "burn off" summer TV time in 1998.		Family Matters was created by William Bickley and Michael Warren (who also wrote for, and were producers of parent series Perfect Strangers) and developed by Thomas L. Miller and Robert L. Boyett (who also served as producers on Perfect Strangers), all four also served as executive producers of the series. The series was produced by Miller-Boyett Productions, in association with Lorimar Television who co-produced the show until 1993, when Warner Bros. Television absorbed Lorimar (a sister company under the co-ownership of Time Warner). Starting with season three, the series was also produced by Bickley-Warren Productions. The series was filmed in front of a live studio audience; the Lorimar-produced episodes were shot at Lorimar Studios (later Sony Pictures Studios) in Culver City, California, while the Warner Bros.-produced episodes were filmed at Warner Bros. Studios in nearby Burbank.		The show's original theme was Louis Armstrong's "What a Wonderful World"; it was scrapped after the fifth episode of season one ("Straight A's"), though it was heard only in the pilot episode in syndicated reruns. The second theme, "As Days Go By," written by Jesse Frederick, Bennett Salvay and Scott Roeme and performed by Frederick, was the theme for the majority of the series until 1995; it was last used in the season seven episode "Fa La La La Laagghh," the only episode during the final three seasons to feature it (this was heard in season one episodes in ABC Family and syndicated airings). A longer version of "As Days Go By" was used during the first three seasons, though in syndicated reruns the short version is heard (in ABC Family airings, the long theme was used for all of the episodes during the first three seasons).		The opening sequence begins with a shot of the Chicago Lakefront (the John Hancock Center can be seen in the center), then a shot of the Winslow home. In the opening titles, the main characters were shown around the Winslow home (though in some shots featured some characters in other places as well, such as Rachel at the Rachel's Place restaurant during the seasons 2–4 version or Waldo at the Vanderbilt High School gym during the seasons 4–6 version). The opening credits during the first three seasons feature a scene showing the Winslow family riding their bicycles across the Irv Kupcinet Bridge over the Chicago River; an allusion to parent series Perfect Strangers, which featured a scene of Balki and Larry (played by Bronson Pinchot and Mark Linn-Baker), riding a tour boat underneath the same bridge, shot from the same angle, in its own opening credits from seasons 3–8 of that series. Clips of episodes were shown after the bike scene and before the house shot in the season one through three versions. The house shown at the beginning and the end of the opening credits (as well as in establishing shots for scenes set at the Winslow house) is located at 1516 West Wrightwood Avenue in Chicago (41°55′44″N 87°40′00″W﻿ / ﻿41.92891°N 87.666779°W﻿ / 41.92891; -87.666779). The closing shot at the end of the credits with the Winslow family at the piano (which also was shown during the closing credits when there was no tag scene), in which the shot pans outside the house and the camera zooms out showing neighborhoods and the northside Chicago skyline (Wrigleyville) in the background, was originally used in the pilot episode "The Mama Who Came to Dinner" (though the scene featuring the Winslows before the pan was redone twice in seasons two and five). The role of Richie as a baby was credited as being played by "Joseph [and] Julius Wright" in season 1, with Julius' name made to appear as Joseph's middle name in the titles—the duo was credited this way because the show's producers did not want audiences to know that Richie was then played by twins—the role of Richie as a baby was played by two children because California state law regulates the number of work hours for a young child, therefore it is common for the role of one baby in a television or film production to be played by twins (another Miller-Boyett series, Full House, credited Mary-Kate & Ashley Olsen in the same manner in its opening sequences until that show's seventh season, in which the Olsen twins were credited separately). In season five, after Hopkins left the show, White was given special billing in response to the popularity he earned as Steve Urkel. Appearing last in the credits, he was credited as "and Jaleel White as Steve Urkel," starting in the sixth season (Hopkins was credited similarly as "and Telma Hopkins as Rachel" prior to season five). In season seven, the opening theme song and credit sequence were dropped entirely—though it was brought back for one episode: "Fa La La La Laagghh," the eleventh episode of that same season—for all other episodes during seasons 7–9, the names of the show's main cast members, as well as co-executive producers and executive producers were shown during each episode's teaser scene.		Family Matters is set in the same "TV universe" as several other TV shows related to ABC's TGIF or CBS' Block Party:		In September 1993, Warner Bros. Domestic Television Distribution began distributing Family Matters for broadcast in off-network syndication; most television stations stopped carrying the show by around 2002, though some stations in larger markets such as WTOG in Tampa, Florida continued to air Family Matters until as recently as 2005. In 1995, reruns of the series began airing on TBS Superstation, where it ran until 2003. From 1997 to 2003, reruns of the series aired on WGN America. In 2003, ABC Family picked up the series and aired it for five years until February 29, 2008. From 2004 to 2006, UPN aired the show for 2 years. BET aired reruns briefly in December 2009 and began airing the series on a regular basis on March 1, 2013. MTV2 also began airing reruns on September 7, 2013. The show aired on Nick at Nite from June 29, 2008 to December 31, 2012. ABC Family and Nick at Nite airings cut the tag scenes at the end of all episodes, despite the fact that many episodes during the series have tag scenes during the closing credits. In 2015, the series now airs on a Viacom owned cable network Centric. In Canada, the series also aired on CTV and CBC for reruns.		Warner Home Video has released the first four seasons of Family Matters on DVD in Region 1[7][8][9] while the remaining five seasons were released by the Warner Archive Collection.[10][11][12][13][14] On February 4, 2014, Warner Home Video released season 4 on DVD, but consumers complained when it was found that the season 4 set contained syndication edits rather than the original broadcast masters. Warner Bros. responded to the complaints, offered a replacement program to receive corrected discs and reissuing the set with corrected broadcast copies on April 4. All episodes are the original broadcast form, except for the episode "Number One With a Bullet", disc 1, episode 6. The entire series is also available for digital download on Amazon.com and the iTunes Store, all but season 6 remastered in both SD and HD.[15]		WatchMojo.com rated Family Matters as the #8 African American TV show. Steve Urkel was rated as the #2 most annoying TV character and as the #4 TV neighbor. His catchphrase Did I do that? was rated as the #10 sitcom catchphrase.		T G I f		
Welcome to the Dollhouse is a 1995 American coming-of-age black comedy film.[1] An independent film, it launched the careers of Todd Solondz and Heather Matarazzo.[2]						Eleven-and-a-half-year-old Dawn Wiener is a shy, unattractive, unpopular seventh grader living in a middle-class suburban community in New Jersey. Her seventeen-year-old brother Mark is a nerdy high school student who plays clarinet in a garage band and shuns girls in order to prepare for college. Dawn's younger sister, eight-year-old Missy, is a spoiled, manipulative little girl who pesters Dawn and dances around the house in a tutu. Their mother dotes on Missy and sides with her in disputes with Dawn. Their father is a meek, immature, selfish man who sides with Dawn's mother in arguments with Dawn. Dawn's only friend is an effeminate fifth-grade boy named Ralphy, with whom she shares a dilapidated clubhouse in her backyard.		At school, Dawn is ridiculed and her locker is covered in graffiti. After her teacher unfairly keeps her after school, she is threatened with rape by a bully named Brandon McCarthy, who also has trouble socializing. At home Dawn's mother punishes her for calling Missy a lesbian and refusing to be nice to her. Dawn gets in trouble at school after she accidentally hits a teacher in the eye with a spitball. Brandon's first attempt to rape Dawn after school fails, but he orders her to meet him again. After she complies, he takes her to an abandoned field. He starts an earnest conversation with her and kisses her.		Mark's band is joined by Steve Rodgers, a charismatic and handsome aspiring teenage rock musician who agrees to play in the band in exchange for Mark's help in school. Dawn decides to pursue him romantically after he spends time with her, even though one of Steve's former girlfriends tells Dawn she has no chance of being with him.		Dawn and Brandon form an innocent romance, but Brandon is arrested and expelled for suspected drug dealing. Dawn visits his home and meets his father and mentally challenged brother who requires constant supervision. After kissing Dawn, Brandon runs away to avoid being sent to military school.		After angrily rejecting Ralphy, Dawn is left with no friends. When she refuses to tear down her clubhouse to make room for her parents' 20th wedding anniversary party, her mother has Mark and Missy destroy it and gives them her share of a cake. At the party, Dawn intends to proposition Steve, but gets cold feet and is contemptuously rebuffed. Steve plays with Missy, who pushes Dawn into a kiddie pool. That evening, the family watches a videotape of the party, laughing when Dawn falls into the water. That night, Dawn smashes the tape and briefly brandishes her hammer over Missy as she sleeps.		A few weeks later, Dawn's father's car breaks down and her mother has to pick him up from work. Dawn is supposed to tell Missy to find a ride home from ballet class but chooses not to do so after arguing with her; Missy is kidnapped while walking home. When Missy's tutu is found in Times Square, Dawn goes to New York City to find her. After a full day searching for Missy, Dawn phones home and Mark tells her that Missy was found by police after being abducted by a pedophile neighbor who lives on their street. Dawn returns home. Later, Dawn's classmates ridicule her as she presents a thank you speech. After the principal tells the unruly students to be quiet, Dawn musters the emotional strength to finish her speech and makes a quick exit.		Summer arrives and Dawn is relieved that school is over for the time-being. Mark tells Dawn that she cannot expect school life to get any better until she starts high school. As Dawn's parents continue mistreating and ignoring her, Dawn signs herself up to attend a summer camp in Florida. On a school trip to Walt Disney World, Dawn sits among other girls from her school and joins them in singing the school anthem. Unnoticed, her voice slowly trails off as she sits looking out a bus window.		The film was a surprise success, considering it was a relatively low budget, independently produced film. It garnered critical praise for its nail-biting view of a pre-teen outcast, and won the Grand Jury Prize for best dramatic feature at the 1996 Sundance Film Festival. Critic Roger Ebert was vocal about his love for the film, giving it four stars out of four and placing it at No. 5 on his "Best of 1996" list.[3]		The film currently holds a 90% "Fresh" rating on Rotten Tomatoes, which states, "Twelve-year-old Dawn Wiener (Heather Matarazzo) is perhaps the most put-upon adolescent in film history in Todd Solondz's bitterly hilarious black comedy Welcome to the Dollhouse."[4]		
Slashdot (sometimes abbreviated as /.) is a social news website that originally billed itself as "News for Nerds. Stuff that Matters". It features news stories on science and technology that are submitted and evaluated by site users. Each story has a comments section attached to it where users can add online comments. The website was founded in 1997 by Hope College students Rob Malda, also known as "CmdrTaco", and classmate Jeff Bates, also known as "Hemos". In 2012, it was acquired[4] by DHI Group, Inc. (i.e., Dice Holdings International, which created the Dice.com website for tech job seekers[5][6]). In January, 2016, BizX acquired Slashdot Media, including both slashdot.org and SourceForge.[1][7][8]		Summaries of stories and links to news articles are submitted by Slashdot's own users, and each story becomes the topic of a threaded discussion among users. Discussion is moderated by a user-based moderation system. Randomly selected moderators are assigned points (typically 5) which they can use to rate a comment. Moderation applies either −1 or +1 to the current rating, based on whether the comment is perceived as either "normal", "offtopic", "insightful", "redundant", "interesting", or "troll" (among others).		The site's comment and moderation system is administered by its own open source content management system, Slash, which is available under the GNU General Public License. In 2012, Slashdot had around 3.7 million unique visitors per month and received over 5300 comments per day.[5] The site has won more than 20 awards, including People's Voice Awards in 2000 for "Best Community Site" and "Best News Site". Occasionally, a news story posted to the site will link to a server causing a large surge of web traffic, which can overwhelm some smaller or independent sites. This phenomenon is known as the "Slashdot effect".						Slashdot was preceded by Rob Malda's personal website "Chips & Dips", which, launched in October 1997,[9] featured a single "rant" each day about something that interested its author – typically something to do with Linux or open source software. At the time, Malda was a student at Hope College in Holland, Michigan, majoring in computer science. The site became "Slashdot" in September 1997 under the slogan "News for Nerds. Stuff that Matters," and quickly became a hotspot on the Internet for news and information of interest to computer geeks.[10] The name "Slashdot" came from a somewhat "obnoxious parody of a URL" – when Malda registered the domain, he desired to make a name that was "silly and unpronounceable" – try pronouncing out, "h-t-t-p-colon-slash-slash-slashdot-dot-org".[11] By June 1998, the site was seeing as many as 100,000 page views per day and advertisers began to take notice.[10] Slashdot was co-founded by Rob Malda and Jeff Bates. By December 1998, Slashdot had net revenues of $18,000, yet its Internet profile was higher, and revenues were expected to increase.		On June 29, 1999, the site was sold to Linux megasite Andover.net for $1.5 million in cash and $7 million in Andover stock at the Initial public offering (IPO) price. Part of the deal was contingent upon the continued employment of Malda and Bates and on the achievement of certain "milestones". With the acquisition of Slashdot, Andover.net could now advertise itself as "the leading Linux/Open Source destination on the Internet".[12][13] Andover.net merged with VA Linux on February 3, 2000,[14] which changed its name to SourceForge, Inc. on May 24, 2007, and became Geeknet, Inc. on November 4, 2009.[15]		Slashdot's 10,000th article was posted after two and a half years on February 24, 2000,[16] and the 100,000th article was posted on December 11, 2009 after 12 years online.[17] During the first 12 years, the most active story with the most responses posted was the post-2004 US Presidential Election article "Kerry Concedes Election To Bush" with 5,687 posts. This followed the creation of a new article section, politics.slashdot.org, created at the start of the 2004 election on September 7, 2004.[18] Many of the most popular stories are political, with "Strike on Iraq" (March 19, 2003) the second-most-active article and "Barack Obama Wins US Presidency" (November 5, 2008) the third-most-active. The rest of the 10 most active articles are an article announcing the 2005 London bombings, and several articles about Evolution vs. Intelligent Design, Saddam Hussein's capture, and Fahrenheit 9/11. Articles about Microsoft and its Windows Operating System are popular. A thread posted in 2002 titled "What's Keeping You On Windows?" was the 10th-most-active story, and an article about Windows 2000/NT4 source-code leaks the most visited article with more than 680,000 hits.[19] Some controversy erupted on March 9, 2001 after an anonymous user posted the full text of Scientology's "Operating Thetan Level Three" (OT III) document in a comment attached to a Slashdot article. The Church of Scientology demanded that Slashdot remove the document under the Digital Millennium Copyright Act. A week later, in a long article, Slashdot editors explained their decision to remove the page while providing links and information on how to get the document from other sources.[20]		Slashdot Japan was launched on May 28, 2001 (although the first article was published April 5, 2001) and is an official offshoot of the US-based Web site. As of January 2010[update] the site was owned by OSDN-Japan, Inc., and carried some of the US-based Slashdot articles as well as localized stories.[21][22] An external site, New Media Services, has reported the importance of Online Moderation last December 1, 2011.[23] On Valentine's Day 2002, founder Rob Malda proposed to longtime girlfriend Kathleen Fent using the front page of Slashdot.[24][25] They were married on December 8, 2002, in Las Vegas, Nevada.[26] Slashdot implemented a paid subscription service on March 1, 2002. Slashdot's subscription model works by allowing users to pay a small fee to be able to view pages without banner ads, starting at a rate of $5 per 1,000 page views – non-subscribers may still view articles and respond to comments, with banner ads in place.[27] On March 6, 2003, subscribers were given the ability to see articles 10 to 20 minutes before they are released to the public.[28] Slashdot altered its threaded discussion forum display software to explicitly show domains for links in articles, as "users made a sport out of tricking unsuspecting readers into visiting [Goatse.cx]."[29]		In observance of April Fools' Day in 2006, Slashdot temporarily changed its signature teal color theme to a warm palette of bubblegum pink and changed its masthead from the usual, "News for Nerds" motto to, "OMG!!! Ponies!!!" Editors joked that this was done to increase female readership.[30] In another supposed April Fools' Day joke, User Achievement tags were introduced on April 1, 2009.[31] This system allowed users to be tagged with various achievements, such as "The Tagger" for tagging a story or "Member of the {1,2,3,4,5} Digit UID Club" for having a Slashdot UID consisting of a certain number of digits. While it was posted on April Fools' Day to allow for certain joke achievements, the system is real.[32] Slashdot unveiled its newly redesigned site on June 4, 2006, following a CSS Redesign Competition. The winner of the competition was Alex Bendiken, who built on the initial CSS framework of the site. The new site looks similar to the old one but is more polished with more rounded curves, collapsible menus, and updated fonts.[33] On November 9 that same year, Malda wrote that Slashdot attained 16,777,215 (or 224 − 1) comments, which broke the database for three hours until the administrators fixed the issue.[34]		On January 25, 2011, the site launched its third major redesign in its 13.5-year history, which gutted the HTML and CSS, and updated the graphics.[35] On August 25, 2011, Malda resigned as Editor-in-Chief with immediate effect. He did not mention any plans for the future, other than spending more time with his family, catching up on some reading, and possibly writing a book.[36][37] His final farewell message received over 1,400 comments within 24 hours on the site.[38] On December 7, 2011, Slashdot announced that it would start to push what the company described as "sponsored" Ask Slashdot questions.[39] On March 28, 2012, Slashdot launched Slashdot TV.[40] Two months later, in May 2012, Slashdot launched SlashBI, SlashCloud, and SlashDataCenter, three Websites dedicated to original journalistic content. The Websites proved controversial, with longtime Slashdot users commenting that the original content ran counter to the Website's longtime focus on user-generated submissions.[41] Nick Kolakowski, the editor of the three Websites, told The Next Web that the Websites were “meant to complement Slashdot with an added layer of insight into a very specific area of technology, without interfering with Slashdot’s longtime focus on tech-community interaction and discussion.” Despite the debate, articles published on SlashCloud and SlashBI attracted attention from io9,[42] NPR,[43] Nieman Lab,[44] Vanity Fair, and other publications.		In September 2012, Slashdot, SourceForge, and Freecode were acquired by online job site Dice.com for $20 million, and incorporated into a subsidiary known as Slashdot Media.[5][6] While initially stating that there were no plans for major changes to Slashdot,[6] in October 2013, Slashdot launched a "beta" for a significant redesign of the site, which featured a simpler appearance and commenting system.[45][46] While initially an opt-in beta, the site automatically began migrating selected users to the new design in February 2014; the rollout led to a negative response from many longtime users, upset by the added visual complexity, and the removal of features, such as comment viewing, that distinguished Slashdot from other news sites. An organized boycott of the site was held from February 10 to 17, 2014.[45] The "beta" site was eventually shelved. In July 2015, Dice announced that it planned to sell Slashdot and SourceForge; in particular, the company stated in a filing that it was unable to "successfully [leverage] the Slashdot user base to further Dice's digital recruitment business".[47]		On January 27, 2016, the two sites were sold to the San Diego-based BizX, LLC for an undisclosed amount.[8][47][48]		It was run by its founder, Rob "CmdrTaco" Malda, from 1998 until 2011. He shared editorial responsibilities with several other editors including Timothy Lord, Patrick "Scuttlemonkey" McGarry, Jeff "Soulskill" Boehm, Rob "Samzenpus" Rozeboom, and Keith Dawson.[49][50] Jonathan "cowboyneal" Pater is another popular editor of Slashdot, who came to work for Slashdot as a programmer and systems administrator. His online nickname (handle), CowboyNeal, is inspired by a Grateful Dead tribute to Neal Cassady in their song, "That's It for the Other One". He is best known as the target of the usual comic poll option,[51] a tradition started by Chris DiBona.[52]		Slashdot runs on Slash, a content management system available under the GNU General Public License.[53] Early versions of Slash were written by Rob Malda, co-founder of Slashdot, in the spring of 1998. After Andover.net bought Slashdot in June 1999,[54] several programmers were hired to structure the code and render it scalable, as its users had increased from a few hundred to tens of thousands. This work was done by Brian Aker, Patrick Galbraith and Chris Nandor, resulting in version 2 of the software, released in 2001. Slash remains Free software and anyone can contribute to development.		Slashdot's editors are primarily responsible for selecting and editing the primary stories that are posted daily by submitters. The editors provide a one-paragraph summary for each story and a link to an external website where the story originated. Each story becomes the topic for a threaded discussion among the site's users.[55] A user-based moderation system is employed to filter out abusive or offensive comments.[56] Every comment is initially given a score of −1 to +2, with a default score of +1 for registered users, 0 for anonymous users (Anonymous Coward), +2 for users with high "karma", or −1 for users with low "karma". As moderators read comments attached to articles, they click to moderate the comment, either up (+1) or down (−1). Moderators may choose to attach a particular descriptor to the comments as well, such as "normal", "offtopic", "flamebait", "troll", "redundant", "insightful", "interesting", "informative", "funny", "overrated", or "underrated", with each corresponding to a −1 or +1 rating. So a comment may be seen to have a rating of "+1 insightful" or "−1 troll".[50] Comments are very rarely deleted, even if they contain hateful remarks.[57][58]		Moderation points add to a user's rating, which is known as "karma" on Slashdot. Users with high "karma" are eligible to become moderators themselves. The system does not promote regular users as "moderators" and instead assigns five moderation points at a time to users based on the number of comments they have entered in the system – once a user's moderation points are used up, they can no longer moderate articles (though they can be assigned more moderation points at a later date). Paid staff editors have an unlimited number of moderation points.[50][55][59] A given comment can have any integer score from −1 to +5, and registered users of Slashdot can set a personal threshold so that no comments with a lesser score are displayed.[55][59] For instance, a user reading Slashdot at level +5 will only see the highest rated comments, while a user reading at level −1 will see a more "unfiltered, anarchic version".[50] A meta-moderation system was implemented on September 7, 1999,[60] to moderate the moderators and help contain abuses in the moderation system.[61][unreliable source?][page needed] Meta-moderators are presented with a set of moderations that they may rate as either fair or unfair. For each moderation, the meta-moderator sees the original comment and the reason assigned by the moderator (e.g. troll, funny), and the meta-moderator can click to see the context of comments surrounding the one that was moderated.[55][59]		Slashdot features discussion forums on a variety of technology- and science-related topics, or "News for Nerds", as its motto states. Articles are divided into the following sections:[62]		Slashdot uses a system of "tags" where users can categorize a story to group them together and sorting them. Tags are written in all lowercase, with no spaces, and limited to 64 characters. For example, articles could be tagged as being about "security" or "mozilla". Some articles are tagged with longer tags, such as "whatcouldpossiblygowrong" (expressing the perception of catastrophic risk), "suddenoutbreakofcommonsense" (used when the community feels that the subject has finally figured out something obvious), "correlationnotcausation" (used when scientific articles lack direct evidence; see correlation does not imply causation), or "getyourasstomars" (commonly seen in articles about Mars or space exploration).[64][65]		As an online community with primarily user-generated content, many in-jokes and internet memes have developed over the course of the site's history. A popular meme (based on an unscientific Slashdot user poll[66]) is, "In Soviet Russia, noun verb you!"[67] This type of joke has its roots in the 1960s or earlier, and is known as a "Russian reversal". Other popular memes usually pertain to computing or technology, such as "Imagine a Beowulf cluster of these",[68] "But does it run Linux?",[69] or "Netcraft now confirms: BSD (or some other software package or item) is dying."[70] Users will also typically refer to articles referring to data storage and data capacity by inquiring how much it is in units of Libraries of Congress.[71] Sometimes bandwidth speeds are referred to in units of Libraries of Congress per second. When numbers are quoted, people will comment that the number happens to be the "combination to their luggage" (a reference to the Mel Brooks film Spaceballs) and express false anger at the person who revealed it.		Slashdotters often use the abbreviation TFA which stands for The fucking article or RTFA ("Read the fucking article"), which itself is derived from the abbreviation RTFM.[72] Usage of this abbreviation often exposes comments from posters who have not read the article linked to in the main story. Slashdotters typically like to mock then United States Senator Ted Stevens' 2006 description of the Internet as a "series of tubes"[73][74] or former Microsoft CEO Steve Ballmer's chair-throwing incident from 2005.[75][76] Microsoft founder Bill Gates is a popular target of jokes by Slashdotters, and all stories about Microsoft were once identified with a graphic of Gates looking like a Borg from Star Trek: The Next Generation.[77] Many Slashdotters have long talked about the supposed release of Duke Nukem Forever, which was promised in 1997 but was delayed indefinitely (the game was eventually released in 2011).[78] References to the game are commonly brought up in other articles about software packages that are not yet in production even though the announced delivery date has long passed (see vaporware). Having a low Slashdot user identifier (user ID) is highly valued since they are assigned sequentially; having one is a sign that someone has an older account and has contributed to the site longer. For Slashdot's 10-year anniversary in 2007, one of the items auctioned off in the charity auction for the Electronic Frontier Foundation was a 3-digit Slashdot user ID.[32][79]		As of 2006, Slashdot had approximately 5.5 million users per month. As of January 2013, the site's Alexa rank is 2,000, with the average user spending 3 minutes and 18 seconds per day on the site and 82,665 sites linking in.[2] The primary stories on the site consist of a short synopsis paragraph, a link to the original story, and a lengthy discussion section, all contributed by users. Discussion on stories can get up to 10,000 posts per day. Slashdot has been considered a pioneer in user-driven content, influencing other sites such as Google News and Wikipedia.[80][81] There has been a dip in readership as of 2011, primarily due to the increase of technology-related blogs and Twitter feeds.[82] In 2002, approximately 50% of Slashdot's traffic consisted of people who simply check out the headlines and click through, while others participate in discussion boards and take part in the community.[83] Many links in Slashdot stories caused the linked site to get swamped by heavy traffic and its server to collapse. This is known as the "Slashdot effect",[80][83] a term which was first coined on February 15, 1999 that refers to an article about a "new generation of niche Web portals driving unprecedented amounts of traffic to sites of interest".[81][84] Today, most major websites can handle the surge of traffic, but the effect continues to occur on smaller or independent sites.[85] These sites are then said to have been "Slashdotted".		Slashdot has received over twenty awards, including People's Voice Awards in 2000 in both of the categories for which it was nominated (Best Community Site and Best News Site).[86] It was also voted as one of Newsweek's favorite technology Web sites and rated in Yahoo!'s Top 100 Web sites as the "Best Geek Hangout" (2001).[87] The main antagonists in the 2004 novel Century Rain, by Alastair Reynolds – The Slashers – are named after Slashdot users.[88] The site was mentioned briefly in the 2000 novel Cosmonaut Keep, written by Ken MacLeod.[89] Several celebrities have stated that they either checked the website regularly or participated in its discussion forums using an account. Some of these celebrities include: Apple co-founder Steve Wozniak,[90] writer and actor Wil Wheaton,[91] and id Software technical director John Carmack.[92]		
A caricature is a rendered image showing the features of its subject in a simplified or exaggerated way through sketching, pencil strokes, or through other artistic drawings.		In literature, a caricature is a description of a person using exaggeration of some characteristics and oversimplification of others.[1]		Caricatures can be insulting or complimentary and can serve a political purpose or be drawn solely for entertainment. Caricatures of politicians are commonly used in editorial cartoons, while caricatures of movie stars are often found in entertainment magazines.		The term is derived from the Italian caricare—to charge or load. An early definition occurs in the English doctor Thomas Browne's Christian Morals, published posthumously in 1716.		with the footnote:		Thus, the word "caricature" essentially means a "loaded portrait". Until the mid 19th century, it was commonly and mistakenly believed that the term shared the same root as the French 'charcuterie', likely owing to Parisian street artists using cured meats in their satirical portrayal of public figures.[2]						Some of the earliest caricatures are found in the works of Leonardo da Vinci, who actively sought people with deformities to use as models. The point was to offer an impression of the original which was more striking than a portrait.		Caricature took a road to its first successes in the closed aristocratic circles of France and Italy, where such portraits could be passed about for mutual enjoyment.		While the first book on caricature drawing to be published in England was Mary Darly's A Book of Caricaturas (c. 1762), the first known North American caricatures were drawn in 1759 during the battle for Quebec.[4] These caricatures were the work of Brig.-Gen. George Townshend whose caricatures of British General James Wolfe, depicted as "Deformed and crass and hideous" (Snell),[4] were drawn to amuse fellow officers.[4] Elsewhere, two great practitioners of the art of caricature in 18th-century Britain were Thomas Rowlandson (1756–1827) and James Gillray (1757–1815). Rowlandson was more of an artist and his work took its inspiration mostly from the public at large. Gillray was more concerned with the vicious visual satirisation of political life. They were, however, great friends and caroused together in the pubs of London.[5]		In a lecture titled The History and Art of Caricature, the British caricaturist Ted Harrison said that the caricaturist can choose to either mock or wound the subject with an effective caricature.[6] Drawing caricatures can simply be a form of entertainment and amusement – in which case gentle mockery is in order – or the art can be employed to make a serious social or political point. A caricaturist draws on (1) the natural characteristics of the subject (the big ears, long nose, etc.); (2) the acquired characteristics (stoop, scars, facial lines etc.); and (3) the vanities (choice of hair style, spectacles, clothes, expressions, and mannerisms).		Sir Max Beerbohm (1872–1956, British), created and published caricatures of the famous men of his own time and earlier. His style of single-figure caricatures in formalized groupings was established by 1896 and flourished until about 1930. His published works include Caricatures of Twenty-five Gentlemen (1896), The Poets' Corner (1904), and Rossetti and His Circle (1922). He published widely in fashionable magazines of the time, and his works were exhibited regularly in London at the Carfax Gallery (1901–18) and Leicester Galleries (1911–57).		George Cruikshank (1792–1878, British) created political prints that attacked the royal family and leading politicians. He went on to create social caricatures of British life for popular publications such as The Comic Almanack (1835–1853) and Omnibus (1842). Cruikshanks' New Union Club of 1819 is notable in the context of slavery.[7] He also earned fame as a book illustrator for Charles Dickens and many other authors.		Honoré Daumier (1808–1879, French) created over 4,000 lithographs, most of them caricatures on political, social, and everyday themes. They were published in the daily French newspapers (Le Charivari, La Caricature etc.)		Mort Drucker (1929-, American) joined Mad in 1957 and became well known for his parodies of movie satires. He combined a comic strip style with caricature likenesses of film actors for Md, and he also contributed covers to Time. He has been recognized for his work with the National Cartoonists Society Special Features Award for 1985, 1986, 1987, and 1988, and their Reuben Award for 1987.		Alex Gard (1900–1948, Russian) created more than 700 caricatures of show business celebrities and other notables for the walls of Sardi's Restaurant in the theater district of New York City: the first artist to do so. Today the images are part of the Billy Rose Theatre Collection of The New York Public Library for the Performing Arts.[8]		Al Hirschfeld (1903–2003, American) was best known for his simple black and white renditions of celebrities and Broadway stars which used flowing contour lines over heavy rendering. He was also known for depicting a variety of other famous people, from politicians, musicians, singers and even television stars like the cast of Star Trek: The Next Generation. He was even commissioned by the United States Postal Service to provide art for U.S. stamps. Permanent collections of Hirschfeld's work appear at the Metropolitan Museum of Art and the Museum of Modern Art in New York, and he boasts a star on the St. Louis Walk of Fame.		S. Jithesh (Indian) is best known for his super-speedy style of Celebrity Caricaturing Stage Shows.[9] He belongs to the genre of the odd and rare species of Performing Caricaturists. He is the first one who courageously and successfully experimented with and explored the performance dimensions of the 'Art of Caricaturing' as a perfect 'Stage Art' with consistency.[10] He evolved and gave a fine finish to the Infotainment 'Caricature Stage Show' or 'Caricature Concert' through more than two thousand stages. His 'Caricature Stage Show' is a blend of poetry, anecdotes and socio-political satire with super speedy drawing which explores the performing level possibilities of the 'Art of Caricaturing'.[11] Sketching of more than thousand celebrity caricatures relentlessly with a lightning pace and satirical commentary is the major attraction of his 'Caricature Stage Shows'. He is widely acclaimed as the 'World's Fastest Cartoonist' Since his amazing ability to sketch 50 celebrity caricatures within 5 minutes.[12]		Sebastian Krüger (1963, German) is known for his grotesque, yet hyper-realistic distortions of the facial features of celebrities, which he renders primarily in acrylic paint, and for which he has won praise from The Times. He is well known for his lifelike depictions of The Rolling Stones, in particular, Keith Richards. Krüger has published three collections of his works, and has a yearly art calendar from Morpheus International. Krüger's art can be seen frequently in Playboy magazine and has also been featured in the likes of Stern, L’Espresso, Penthouse, and Der Spiegel and USA Today. He has recently been working on select motion picture projects.		David Levine (1926–2009, American) is noted for his caricatures in The New York Review of Books and Playboy magazine. His first cartoons appeared in 1963. Since then he has drawn hundreds of pen-and-ink caricatures of famous writers and politicians for the newspaper.		Hermann Mejia (Venezuelan) is known for his frequent work for MAD Magazine. Mejia uses multiple techniques in his work, sometimes rendering his illustrations in black and white ink and copious amounts of cross-hatching, sometimes using watercolor, and sometimes combinations of both.		Thomas Nast (1840–1902, American) was a famous caricaturist and editorial cartoonist in the 19th century and is considered by some as written in 1908 by the New York Times to be the father of American political cartooning. He is often credited with creating the definitive caricature of Santa Claus, and often mistakenly credited with creating the definitive caricatures of the Democratic Donkey and the Republican Elephant.[13][14][15]		Gogu Neagoe (1976, Romanian) holds a Guinness World Record for the doing 131 caricatures through the phone, without ever seeing the subject.[16]		Vitali is known for his retro airbrush style. His work has appeared in Rolling Stone, Playboy, Vanity Fair, Esquire, and Interview.		Sanford Ross (1907-1954, American) was recognized by New York critics for his lithographic caricatures of New Jersey mansions and civic buildings in the 1930s.		Sam Viviano (1953, American) has done much work for corporations and in advertising, having contributed to Rolling Stone, Family Weekly, Reader's Digest, Consumer Reports, and Mad, of which he is currently the art director. Viviano’s caricatures are known for their wide jaws, which Viviano has explained is a result of his incorporation of side views as well as front views into his distortions of the human face. He has also developed a reputation for his ability to do crowd scenes. Explaining his twice-yearly covers for Institutional Investor magazine, Viviano has said that his upper limit is sixty caricatures in nine days.		There have been some efforts to produce caricatures automatically or semi-automatically using computer graphics techniques. For example, a system proposed by Akleman et al.[17] provides warping tools specifically designed toward rapidly producing caricatures. There are very few software programs designed specifically for automatically creating caricatures.		Computer graphic system requires quite different skill sets to design a caricature as compared to the caricatures created on paper. Thus using a computer in the digital production of caricatures requires advanced knowledge of the program's functionality. Rather than being a simpler method of caricature creation, it can be a more complex method of creating images that feature finer coloring textures than can be created using more traditional methods.		A milestone in formally defining caricature was Susan Brennan's master's thesis[18] in 1982. In her system, caricature was formalized as the process of exaggerating differences from an average face. For example, if Prince Charles has more prominent ears than the average person, in his caricature the ears will be much larger than normal. Brennan's system implemented this idea in a partially automated fashion as follows: the operator was required to input a frontal drawing of the desired person having a standardized topology (the number and ordering of lines for every face). She obtained a corresponding drawing of an average male face. Then, the particular face was caricatured simply by subtracting from the particular face the corresponding point on the mean face (the origin being placed in the middle of the face), scaling this difference by a factor larger than one, and adding the scaled difference back onto the mean face.		Though Brennan's formalization was introduced in the 1980s, it remains relevant in recent work. Mo et al.[19] refined the idea by noting that the population variance of the feature should be taken into account. For example, the distance between the eyes varies less than other features such as the size of the nose. Thus even a small variation in the eye spacing is unusual and should be exaggerated, whereas a correspondingly small change in the nose size relative to the mean would not be unusual enough to be worthy of exaggeration.		On the other hand, Liang et al.[20] argue that caricature varies depending on the artist and cannot be captured in a single definition. Their system uses machine learning techniques to automatically learn and mimic the style of a particular caricature artist, given training data in the form of a number of face photographs and the corresponding caricatures by that artist. The results produced by computer graphic systems are arguably not yet of the same quality as those produced by human artists. For example, most systems are restricted to exactly frontal poses, whereas many or even most manually produced caricatures (and face portraits in general) choose an off-center "three-quarters" view. Brennan's caricature drawings were frontal-pose line drawings. More recent systems can produce caricatures in a variety of styles, including direct geometric distortion of photographs.		Brennan's caricature generator was used to test recognition of caricatures. Rhodes, Brennan and Carey demonstrated that caricatures were recognised more accurately than the original images.[21] They used line drawn images but Benson and Perrett showed similar effects with photographic quality images.[22] Explanations for this advantage have been based on both norm-based theories of face recognition[21] and exemplar-based theories of face recognition.[23]		Beside the political and public-figure satire, most contemporary caricatures are used as gifts or souvenirs, often drawn by street vendors. For a small fee, a caricature can be drawn specifically (and quickly) for a patron. These are popular at street fairs, carnivals, and even weddings, often with humorous results.[24]		Caricature artists are also popular attractions at many places frequented by tourists, especially oceanfront boardwalks, where vacationers can have a humorous caricature sketched in a few minutes for a small fee. Caricature artists sometimes hire for parties, where they will draw caricatures of the guests for their entertainment.		There are numerous museums dedicated to caricature throughout the world, including the Museo de la Caricatura of Mexico City, the Muzeum Karykatury in Warsaw, the Caricatura Museum Frankfurt in Frankfurt, the Wilhelm Busch Museum in Hanover and the Cartoonmuseum in Basel. The first museum of caricature in the Arab world was opened in March, 2009, at Fayoum, Egypt.[25]		Taking selfie with the referee while he is showing a red card		Books surrounded by smart phones		Daesh chief Abu Bakr al-Baghdadi flees Mosul		
Physical attractiveness is the degree to which a person's physical features are considered aesthetically pleasing or beautiful. The term often implies sexual attractiveness or desirability, but can also be distinct from either. There are many factors which influence one person's attraction to another, with physical aspects being one of them. Physical attraction itself includes universal perceptions common to all human cultures, as well as aspects that are culturally and socially dependent, along with individual subjective preferences.		In many cases, humans subconsciously attribute positive characteristics, such as intelligence and honesty, to physically attractive people.[9] From research done in the United States and United Kingdom, it was found that the association between intelligence and physical attractiveness is stronger among men than among women.[10] Evolutionary psychologists have tried to answer why individuals who are more physically attractive should also, on average, be more intelligent, and have put forward the notion that both general intelligence and physical attractiveness may be indicators of underlying genetic fitness.[11] A person's physical characteristics can signal cues to fertility and health. Attending to these factors increases reproductive success, furthering the representation of one's genes in the population.[12]		Men, on average, tend to be attracted to women who are shorter than they are, have a youthful appearance, and exhibit features such as a symmetrical face,[13] full breasts, full lips, and a low waist-hip ratio.[14] Women, on average, tend to be attracted to men who are taller than they are, display a high degree of facial symmetry, masculine facial dimorphism, and who have broad shoulders, a relatively narrow waist, and a V-shaped torso.[15][16]						Generally, physical attractiveness can be viewed from a number of perspectives; with universal perceptions being common to all human cultures, cultural and social aspects, and individual subjective preferences. The perception of attractiveness can have a significant effect on how people are judged in terms of employment or social opportunities, friendship, sexual behavior, and marriage.[17]		Some physical features are attractive in both men and women, particularly bodily[18] and facial symmetry,[19][20][21][22] although one contrary report suggests that "absolute flawlessness" with perfect symmetry can be "disturbing".[23] Symmetry may be evolutionarily beneficial as a sign of health because asymmetry "signals past illness or injury".[24] One study suggested people were able to "gauge beauty at a subliminal level" by seeing only a glimpse of a picture for one-hundredth of a second.[24] Other important factors include youthfulness, skin clarity and smoothness of skin; and "vivid color" in the eyes and hair.[19] However, there are numerous differences based on gender.		A 1921 study of the reports of college students regarding those traits in individuals which make for attractiveness and repulsiveness argued that static traits, such as beauty or ugliness of features, hold a position subordinate to groups of physical elements like expressive behavior, affectionate disposition, grace of manner, aristocratic bearing, social accomplishments and personal habits.[25]		Grammer and colleagues have identified eight "pillars" of beauty: youthfulness, symmetry, averageness, sex-hormone markers, body odor, motion, skin complexion and hair texture.[26]		Most studies of the brain activations associated with the perception of attractiveness show photographs of faces to their participants and let them or a comparable group of people rate the attractiveness of these faces. Such studies consistently find that activity in certain parts of the orbitofrontal cortex increases with increasing attractiveness of faces.[27][28][29][30][31] This neural response has been interpreted as a reaction on the rewarding nature of attractiveness, as similar increases in activation in the medial orbitofrontal cortex can be seen in response to smiling faces[32] and to statements of morally good actions.[29][31] While most of these studies have not assessed participants of both genders or homosexual individuals, evidence from one study including male and female hetero- and homosexual individuals indicate that some of the aforementioned increases in brain activity are restricted to images of faces of the gender participants feel sexually attracted to.[33]		With regard to brain activation related to the perception of attractive bodies, one study with heterosexual participants suggests that activity in the nucleus accumbens and the anterior cingulate cortex increases with increasing attractiveness. The same study finds that for faces and bodies alike, the medial part of the orbitofrontal cortex responds with greater activity to both very attractive and very unattractive pictures.[34]		Women, on average, tend to be more attracted to men who have a relatively narrow waist, a V-shaped torso, and broad shoulders. Women also tend to be more attracted to men who are taller than they are, and display a high degree of facial symmetry, as well as relatively masculine facial dimorphism.[15][16]				Studies have shown that ovulating heterosexual women prefer faces with masculine traits associated with increased exposure to testosterone during key developmental stages, such as a broad forehead, relatively longer lower face, prominent chin and brow, chiseled jaw and defined cheekbones.[35] The degree of differences between male and female anatomical traits is called sexual dimorphism. Female respondents in the follicular phase of their menstrual cycle were significantly more likely to choose a masculine face than those in menses and luteal phases,[36] (or in those taking hormonal contraception).[15][16][37][38] This distinction supports the sexy son hypothesis, which posits that it is evolutionarily advantageous for women to select potential fathers who are more genetically attractive,[39] rather than the best caregivers.[40] However, women's likeliness to exert effort to view male faces does not seem to depend on their masculinity, but to a general increase with women's testosterone levels.[41]		It is suggested that the masculinity of facial features is a reliable indication of good health, or, alternatively, that masculine-looking males are more likely to achieve high status.[42] However, the correlation between attractive facial features and health has been questioned.[43] Sociocultural factors, such as self-perceived attractiveness, status in a relationship and degree of gender-conformity, have been reported to play a role in female preferences for male faces.[44] Studies have found that women who perceive themselves as physically attractive are more likely to choose men with masculine facial dimorphism, than are women who perceive themselves as physically unattractive.[45] In men, facial masculinity significantly correlates with facial symmetry—it has been suggested that both are signals of developmental stability and genetic health.[46] One study called into question the importance of facial masculinity in physical attractiveness in men arguing that when perceived health, which is factored into facial masculinity, is discounted it makes little difference in physical attractiveness.[47] In a cross-country study involving 4,794 women in their early twenties, a difference was found in women's average "masculinity preference" between countries.[48]		A study found that the same genetic factors cause facial masculinity in both males and females such that a male with a more masculine face would likely have a sister with a more masculine face due to the siblings having shared genes. The study also found that, although female faces that were more feminine were judged to be more attractive, there was no association between male facial masculinity and male facial attractiveness for female judges. With these findings, the study reasoned that if a woman were to reproduce with a man with a more masculine face, then her daughters would also inherit a more masculine face, making the daughters less attractive. The study concluded that there must be other factors that advantage the genetics for masculine male faces to offset their reproductive disadvantage in terms of "health", "fertility" and "facial attractiveness" when the same genetics are present in females. The study reasoned that the "selective advantage" for masculine male faces must "have (or had)" been due to some factor that is not directly tied to female perceptions of male facial attractiveness.[49]		In a study of 447 gay men in China, researchers said that tops preferred feminized male faces, bottoms preferred masculinized male faces and versatiles had no preference for either feminized or masculinized male faces.[50]		In pre-modern Chinese literature, the ideal man in caizi jiaren romances was said to have "rosy lips, sparkling white teeth" and a "jasper-like face" (Chinese: 唇 紅 齒 白 、 面 若 冠 玉).[51][52]		In Middle English literature, a beautiful man should have a long, broad and strong face.[53]		A study that used Chinese, Malay and Indian judges said that Chinese men with orthognathism where the mouth is flat and in-line with the rest of the face were judged to be the most attractive and Chinese men with a protruding mandible where the jaw projects outward were judged to be the least attractive.[54]		Symmetrical faces and bodies may be signs of good inheritance to women of child-bearing age seeking to create healthy offspring. Studies suggest women are less attracted to men with asymmetrical faces,[55] and symmetrical faces correlate with long term mental performance[56] and are an indication that a man has experienced "fewer genetic and environmental disturbances such as diseases, toxins, malnutrition or genetic mutations" while growing.[56] Since achieving symmetry is a difficult task during human growth, requiring billions of cell reproductions while maintaining a parallel structure, achieving symmetry is a visible signal of genetic health.		Studies have also suggested that women at peak fertility were more likely to fantasize about men with greater facial symmetry,[57] and other studies have found that male symmetry was the only factor that could significantly predict the likelihood of a woman experiencing orgasm during sex. Women with partners possessing greater symmetry reported significantly more copulatory female orgasms than were reported by women with partners possessing low symmetry, even with many potential confounding variables controlled.[58] This finding has been found to hold across different cultures. It has been argued that masculine facial dimorphism (in men) and symmetry in faces are signals advertising genetic quality in potential mates.[59] Low facial and body fluctuating asymmetry may indicate good health and intelligence, which are desirable features.[60] Studies have found that women who perceive themselves as being more physically attractive are more likely to favor men with a higher degree of facial symmetry, than are women who perceive themselves as being less physically attractive.[45] It has been found that symmetrical men (and women) have a tendency to begin to have sexual intercourse at an earlier age, to have more sexual partners, and to have more one-night stands. They are also more likely to be prone to infidelity.[61] A study of quarterbacks in the American National Football League found a positive correlation between facial symmetry and salaries.[20]		Double-blind studies found that women prefer the scent of men who are rated as facially attractive.[62] For example, both males and females were more attracted to the natural scent of individuals who had been rated by consensus as facially attractive.[63] Additionally, it has also been shown that women have a preference for the scent of men with more symmetrical faces, and that women's preference for the scent of more symmetrical men is strongest during the most fertile period of their menstrual cycle.[64] Within the set of normally cycling women, individual women's preference for the scent of men with high facial symmetry correlated with their probability of conception.[64]		Studies have explored the genetic basis behind such issues as facial symmetry and body scent and how they influence physical attraction. In one study in which women wore men's T-shirts, researchers found that women were more attracted to the bodily scents in shirts of men who had a different type of gene section within the DNA called Major histocompatibility complex (MHC).[65] MHC is a large gene area within the DNA of vertebrates which encodes proteins dealing with the immune system[66] and which influences individual bodily odors.[67] One hypothesis is that humans are naturally attracted by the sense of smell and taste to others with dissimilar MHC sections, perhaps to avoid subsequent inbreeding while increasing the genetic diversity of offspring.[66] Further, there are studies showing that women's natural attraction for men with dissimilar immune profiles can be distorted with use of birth control pills.[67] Other research findings involving the genetic foundations of attraction suggest that MHC heterozygosity positively correlates with male facial attractiveness. Women judge the faces of men who are heterozygous at all three MHC loci to be more attractive than the faces of men who are homozygous at one or more of these loci. Additionally, a second experiment with genotyped women raters, found these preferences were independent of the degree of MHC similarity between the men and the female rater. With MHC heterozygosity independently seen as a genetic advantage, the results suggest that facial attractiveness in men may be a measure of genetic quality.[68][69] General genetic heterozygosity has been demonstrated to be related to attractiveness in that people with mixed genetic backgrounds (i.e., mixed race people) as seen as more attractive than people with a more similar genetic parents (i.e., single race people).[70]		A 2010 OkCupid study on 200,000 of its male and female dating site users found that women are, except those during their early to mid-twenties, open to relationships with both somewhat older and somewhat younger men; they have a larger potential dating pool than men until age 26. At age 20, women, in a "dramatic change", begin sending private messages to significantly older men. At age 29 they become "even more open to older men". Male desirability to women peaks in the late 20s and does not fall below the average for all men until 36.[71] Other research indicates that women, irrespective of their own age, are attracted to men who are the same age or older.[72]		For the Romans especially, "beardlessness" and "smooth young bodies" were considered beautiful to both men and women.[73] For Greek and Roman men, the most desirable traits of boys were their "youth" and "hairlessness". Pubescent boys were considered a socially appropriate object of male desire, while post-pubescent boys were considered to be "ἔξωροι" or "past the prime".[73] This was largely in the context of pederasty (adult male interest in adolescent boys). Today, men and women's attitudes towards male beauty has changed. For example, body hair on men may even be preferred (see below).		A 1984 study said that gay men tend to prefer gay men of the same age as ideal partners, but there was a statistically significant effect (p < 0.05) of masculinity-femininity. The study said that more feminine men tended to prefer relatively older men than themselves and more masculine men tended to prefer relatively younger men than themselves.[74]		The physique of a slim waist, broad shoulders and muscular chest are often found to be attractive to females.[75] Further research has shown that, when choosing a mate, the traits females look for indicate higher social status, such as dominance, resources, and protection.[76] An indicator of health in males (a contributing factor to physical attractiveness) is the android fat distribution pattern which is categorized as more fat distributed on the upper body and abdomen, commonly referred to as the "V shape."[76] When asked to rate other men, both heterosexual and homosexual men found low waist-to-chest ratios (WCR) to be more attractive on other men, with the gay men showing a preference for lower WCR (more V-shaped) than the straight men.[77]		Other researchers found waist-to-chest ratio the largest determinant of male attractiveness, with body mass index and waist-to-hip ratio not as significant.[78]		Women focus primarily on the ratio waist to chest or more specifically waist to shoulder. This is analogous to the waist to hip ratio (WHR) that men prefer. Key body image for a man in the eyes of a woman would include big shoulders, chest, and upper back, and a slim waist area.[79] Research has additionally shown that college males had a better satisfaction with their body than college females. The research also found that when a college female's waist to hip ratio went up, their body image satisfaction decreased.[80] The results indicate that males had significantly greater body image satisfaction than did females.		Some research has shown that body weight may have a stronger effect than WHR when it comes to perceiving attractiveness of the opposite sex. It was found that waist to hip ratio played a smaller role in body preference than body weight in regards to both sexes.[81]		Psychologists Viren Swami and Martin J. Tovee compared female preference for male attractiveness cross culturally, between Britain and Malaysia. They found that females placed more importance on WCR (and therefore body shape) in urban areas of Britain and Malaysia, while females in rural areas placed more importance on BMI (therefore weight and body size). Both WCR and BMI are indicative of male status and ability to provide for offspring, as noted by evolutionary theory.[82]		Females have been found to desire males that are normal weight and have the average WHR for a male. Females view these males as attractive and healthy. Males who had the average WHR but were overweight or underweight are not perceived as attractive to females. This suggests that WHR is not a major factor in male attractiveness, but a combination of body weight and a typical male WHR seem to be the most attractive. Research has shown that men who have a higher waist to hip ratio and a higher salary are perceived as more attractive to women.[83]		A 1982 study, found that an abdomen that protrudes was the "least attractive" trait for men.[84]		In Middle English literature, a beautiful man should have a flat abdomen.[53]		Men's bodies portrayed in magazines marketed to men are more muscular than the men's bodies portrayed in magazines marketed to women. From this, some have concluded that men perceive a more muscular male body to be ideal, as distinct from a woman's ideal male, which is less muscular than what men perceive to be ideal.[85] This is due to the within-gender prestige granted by increased muscularity and within-gender competition for increased muscularity.[85] Men perceive the attractiveness of their own musculature by how closely their bodies resemble the "muscle man."[86] This "muscle man" ideal is characterized by large muscular arms, especially biceps, a large muscular chest that tapers to their waist and broad shoulders.[86]		In a study of stated profile preferences on Match.com, a greater percentage of gay men than lesbians selected their ideal partner's body type as "Athletic and Toned" as opposed to the other two options of "Average" or "Overweight".[87]		In pre-modern Chinese literature, such as in The Story of the Western Wing, a type of masculinity called "scholar masculinity" is depicted wherein the "ideal male lover" is "weak, vulnerable, feminine, and pedantic".[51]		In Middle English literature, a beautiful man should have thick, broad shoulders, a square and muscular chest, a muscular back, strong sides that taper to a small waist, large hands and arms and legs with huge muscles.[53]		A 2006 study, of 25,594 heterosexual men found that men who perceived themselves as having a large penis were more satisfied with their own appearance.[88]		A 2014 study criticized previous studies based on the fact that they relied on images and used terms such as "small", "medium", and "large" when asking for female preference. The new study used 3D models of penises from sizes of 4 inches (10 cm) long and 2.5 inches (6.4 cm) in circumference to 8.5 inches (22 cm) long and 7 inches (18 cm) in circumference and let the women "view and handle" them. It was found that women overestimated the actual size of the penises they have experimented with when asked in a follow-up survey. The study concluded that women on average preferred the 6.5-inch (17 cm) penis in length both for long-term and for one-time partners. Penises with larger girth were preferred for one-time partners.[89]		Females' sexual attraction towards males may be determined by the height of the man.[91] Height in men is associated with status or wealth in many cultures (in particular those where malnutrition is common),[92] which is beneficial to women romantically involved with them. One study conducted of women's personal ads support the existence of this preference; the study found that in ads requesting height in a mate, 80% requested a height of 6 feet (1.83 m) or taller.[92] The online dating Website eHarmony only matches women with taller men because of complaints from women matched with shorter men.[93]		Other studies have shown that heterosexual women often prefer men taller than they are rather than a man with above average height. While women usually desire men to be at least the same height as themselves or taller, several other factors also determine male attractiveness, and the male-taller norm is not universal.[94] For example, taller women are more likely to relax the "taller male" norm than shorter women.[95] Furthermore, professor Adam Eyre-Walker, from the University of Sussex, has stated that there is, as of yet, no evidence that these preferences are evolutionary preferences, as opposed to merely cultural preferences.[96] In a double-blind study by Graziano et al., it was found that, in person, using a sample of women of normal size, they were on average most attracted to men who were of medium height (5'9" – 5'11", 1.75 m – 1.80 m) and less attracted to both men of shorter height (5'5" – 5'7", 1.65 m – 1.70 m) and men of tallest height (6'2" – 6'4", 1.88 m – 1.93 m).[97]		Additionally, women seem more receptive to an erect posture than men, though both prefer it as an element within beauty.[92] According to one study (Yee N., 2002), gay men who identify as "only tops" tend to prefer shorter men, while gay men who identify as "only bottoms" tend to prefer taller men.[98]		In romances in Middle English literature, all of the "ideal" male heroes are tall, and the vast majority of the "valiant" male heroes are tall too.[53]		Studies based in the United States, New Zealand, and China have shown that women rate men with no trunk (chest and abdominal) hair as most attractive, and that attractiveness ratings decline as hairiness increases.[99][100] Another study, however, found that moderate amounts of trunk hair on men was most attractive, to the sample of British and Sri Lankan women.[101] Further, a degree of hirsuteness (hairiness) and a waist-to-shoulder ratio of 0.6 is often preferred when combined with a muscular physique.[101]		In a study using Finnish women, women with hairy fathers were more likely to prefer hairy men, suggesting that preference for hairy men is the result of either genetics or imprinting.[102] Among gay men, another study (Yee N., 2002) reported gay males who identify as "only tops" prefer less hairy men, while gay males who identify as "only bottoms" prefer hairier men.[98]		Testosterone has been shown to darken skin color in laboratory experiments.[103] Despite this, the aesthetics of skin tone varies from culture to culture. Manual laborers who spent extended periods of time outside developed a darker skin tone due to exposure to the sun. As a consequence, an association between dark skin and the lower classes developed. Light skin became an aesthetic ideal because it symbolized wealth. "Over time society attached various meanings to these colored differences. Including assumptions about a person's race, socioeconomic class, intelligence, and physical attractiveness."[104]		A scientific review published in 2011, identified from a vast body of empirical research that skin colour as well as skin tone tend to be preferred as they act as indicators of good health. More specifically, these indicators are thought to suggest to potential mates that the beholder has strong or good genes capable of fighting off disease.[105]		According to one study (Yee N., 2002), gay men who identify as "only tops" tend to prefer lighter-skinned men while gay men who identify as "only bottoms" tend to prefer darker-skinned men.[98]		More recent research has suggested that redder and yellower skin tones,[106] reflecting higher levels of oxygenated blood,[107] carotenoid and to a lesser extent melanin pigment, and net dietary intakes of fruit and vegetables,[108] appears healthier, and therefore more attractive.[109]		Research indicates that heterosexual men tend to be attracted to young[110] and beautiful women[111] with bodily symmetry.[112] Rather than decreasing it, modernity has only increased the emphasis men place on women's looks.[113] Evolutionary psychologists attribute such attraction to an evaluation of the fertility potential in a prospective mate.[110]		Research has attempted to determine which facial features communicate attractiveness. Facial symmetry has been shown to be considered attractive in women,[117][118] and men have been found to prefer full lips,[119] high forehead, broad face, small chin, small nose, short and narrow jaw, high cheekbones,[55][120] clear and smooth skin, and wide-set eyes.[110] The shape of the face in terms of "how everything hangs together" is an important determinant of beauty.[121] A University of Toronto study found correlations between facial measurements and attractiveness; researchers varied the distance between eyes, and between eyes and mouth, in different drawings of the same female face, and had the drawings evaluated; they found there were ideal proportions perceived as attractive (see photo).[114] These proportions (46% and 36%) were close to the average of all female profiles.[114] Women with thick, dark limbal rings in their eyes have also been found to be more attractive. The explanation given is that because the ring tends to fade with age and medical problems, a prominent limbal ring gives an honest indicator of youth.[122]		In a cross-cultural study, more neotenized (i.e., youthful looking) female faces were found to be most attractive to men while less neotenized female faces were found to be less attractive to men, regardless of the females' actual age.[123] One of these desired traits was a small jaw.[124] In a study of Italian women who have won beauty competitions, it was found that their faces had more "babyish" (pedomorphic) traits than those of the "normal" women used as a reference.[125]		In a cross-cultural study, Marcinkowska et al. said that 18- to 45-year-old heterosexual men in all 28 countries surveyed preferred photographs of 18- to 24-year-old Caucasian women whose faces were feminized using Psychomorph software over faces of 18- to 24-year-old Caucasian women that were masculinized using that software, but there were differences in preferences for femininity across countries. The higher the National Health Index of a country, the more were the feminized faces preferred over the masculinized faces. Among the countries surveyed, Japan had the highest femininity preference and Nepal had the lowest femininity preference.[126]		Michael R. Cunningham of the Department of Psychology at the University of Louisville found, using a panel of East Asian, Hispanic and White judges, that the Asian, Hispanic and White female faces found most attractive were those that had "neonate large eyes, greater distance between eyes, and small noses"[127] and his study led him to conclude that "large eyes" were the most "effective" of the "neonate cues".[127] Cunningham also said that "shiny" hair may be indicative of "neonate vitality".[127] Using a panel of blacks and whites as judges, Cunningham found more neotenous faces were perceived as having both higher "femininity" and "sociability".[127] In contrast, Cunningham found that faces that were "low in neoteny" were judged as "intimidating".[127] Cunningham noted a "difference" in the preferences of Asian and white judges with Asian judges preferring women with "less mature faces" and smaller mouths than the White judges.[127] Cunningham hypothesized that this difference in preference may stem from "ethnocentrism" since "Asian faces possess those qualities", so Cunningham re-analyzed the data with "11 Asian targets excluded" and concluded that "ethnocentrism was not a primary determinant of Asian preferences."[127] Rather than finding evidence for purely "neonate" faces being most appealing, Cunningham found faces with "sexually-mature" features at the "periphery" of the face combined with "neonate" features in the "center of the face" most appealing in men and women.[127] Upon analyzing the results of his study, Cunningham concluded that preference for "neonate features may display the least cross-cultural variability" in terms of "attractiveness ratings"[127] and, in another study, Cunningham concluded that there exists a large agreement on the characteristics of an attractive face.[128][129]		In computer face averaging tests, women with averaged faces have been shown to be considered more attractive.[22][130] This is possibly due to average features being more familiar and, therefore, more comfortable.[117]		Commenting on the prevalence of whiteness in supposed beauty ideals in his book White Lies: Race and the Myth of Whiteness, Maurice Berger states that the schematic rendering in the idealized face of a study conducted with American subjects had "straight hair," "light skin," "almond-shaped eyes," "thin, arched eyebrows," "a long, thin nose, closely set and tiny nostrils" and "a large mouth and thin lips",[131] though the author of the study stated that there was consistency between his results and those conducted on other races. Scholar Liu Jieyu says in the article White Collar Beauties, "The criterion of beauty is both arbitrary and gendered. The implicit consensus is that women who have fair skin and a slim figure with symmetrical facial features are pretty." He says that all of these requirements are socially constructed and force people to change themselves to fit these criteria.[132]		One psychologist speculated there were two opposing principles of female beauty: prettiness and rarity. So on average, symmetrical features are one ideal, while unusual, stand-out features are another.[133] A study performed by the University of Toronto found that the most attractive facial dimensions were those found in the average female face. However, that particular University of Toronto study looked only at white women.[134]		A study that used Chinese, Malay and Indian judges said that Chinese women with orthognathism where the mouth is flat and in-line with the rest of the face were judged to be the most attractive and Chinese women with a protruding mandible where the jaw projects outward were judged to be the least attractive.[54]		A 2011 study, by Wilkins, Chan and Kaiser found correlations between perceived femininity and attractiveness, that is, women's faces which were seen as more feminine were judged by both men and women to be more attractive.[135]		A component of the female beauty ideal in Persian literature is for women to have faces like a full moon.[136][137][138]		In Arabian society in the Middle Ages, a component of the female beauty ideal was for women to have round faces which were like a "full moon".[139]		In Japan, during the Edo period, a component of the female beauty ideal was for women to have long and narrow faces which were shaped like ovals.[140]		In Jewish Rabbinic literature, the Rabbis considered full lips to be the ideal type of lips for women.[141]		Historically, in Chinese and Japanese literature, the feminine ideal was said to include small lips.[142] Women would paint their lips thinner and narrower to align with this ideal.[143][144]		Classical Persian literature, paintings, and miniatures portrayed traits such as long black curly hair, a small mouth, long arched eyebrows, large almond shaped eyes, a small nose, and beauty spots as being beautiful for women.[145]		Evidence from various cultures suggests that heterosexual men tend to find the sight of women's genitalia to be sexually arousing.[citation needed]		Cross-cultural data shows that the reproductive success of women is tied to their youth and physical attractiveness[146] such as the pre-industrial Sami where the most reproductively successful women were 15 years younger than their man.[147] One study covering 37 cultures showed that, on average, a woman was 2.5 years younger than her male partner, with the age difference in Nigeria and Zambia being at the far extreme of 6.5 to 7.5 years. As men age, they tend to seek a mate who is ever younger.[110]		25% of eHarmony's male customers over the age of 50 request to only be matched with women younger than 40.[93] A 2010 OkCupid study, of 200,000 users found that female desirability to its male users peaks at age 21, and falls below the average for all women at 31. After age 26, men have a larger potential dating pool than women on the site; and by age 48, their pool is almost twice as large. The median 31-year-old male user searches for women aged 22 to 35, while the median 42-year-old male searches for women 27 to 45. The age skew is even greater with messages to other users; the median 30-year-old male messages teenage girls as often as women his own age, while mostly ignoring women a few years older than him. Excluding the 10% most and 10% least beautiful of women, however, women's attractiveness does not change between 18 and 40, but if extremes are not excluded "There's no doubt that younger [women] are more physically attractive—indeed in many ways beauty and youth are inextricable. That's why most of the models you see in magazines are teenagers".[71]		Pheromones (detected by female hormone markers) reflects female fertility and the reproductive value mean.[148] As females age, the estrogen-to-androgen production ratio changes and results in female faces to appear more and more masculine (thus appearing less "attractive").[148] In a small (n=148) study performed in the United States, using male college students at one university, the mean age expressed as ideal for a wife was found to be 16.87 years old, while 17.76 was the mean ideal age for a brief sexual encounter. However, the study sets up a framework where "taboos against sex with young girls" are purposely diminished, and biased their sample by removing any participant over the age of 30, with a mean participant age of 19.83.[149] In a study of penile tumescence, men were found most aroused by pictures of young adult females.[150]		Signals of fertility in women are often also seen as signals of youth. The evolutionary perspective proposes the idea that when it comes to sexual reproduction, the minimal parental investment required by men gives them the ability and want to simply reproduce 'as much as possible.'[151] It therefore makes sense that men are attracted to the features in women which signal youthfulness, and thus fertility.[151] Their chances of reproductive success are much higher than they would be should they pick someone older—and therefore less fertile.		This may explain why combating age declines in attractiveness occurs from a younger age in women than in men. For example, the removal of one's body hair is considered a very feminine thing to do.[152] This can be explained by the fact that aging results in raised levels of testosterone and thus, body hair growth. Shaving reverts one's appearance to a more youthful stage[152] and although this may not be an honest signal, men will interpret this as a reflection of increased fertile value. Research supports this, showing hairlessness to considered sexually attractive by men.[153]		Research has shown that most heterosexual men enjoy the sight of female breasts,[154] with a preference for large, firm breasts.[155] However, a contradictory study of British undergraduates found younger men preferred small breasts on women.[156] Smaller breasts were widely associated with youthfulness.[157] Cross-culturally, another study found "high variability" regarding the ideal breast size.[156] Some researchers in the United Kingdom have speculated that a preference for larger breasts may have developed in Western societies because women with larger breasts tend to have higher levels of the hormones estradiol and progesterone, which both promote fertility.[158]		A study by Groyecka et al., in which they examined Poles and Yali of New Guinea, demonstrated that men judgements of breast appearance is affected by the occurrence of breast ptosis (i.e., sagginess, droopiness).[159] Greater breast ptosis (more sagging breasts) is perceived as less attractive and attributed to a woman of older age. These findings are coherent with previous research that link breast attractiveness with female youthfulness. Unlike breast size, breast ptosis seems to be a universal marker of female breast attractiveness.		A study showed that men prefer symmetrical breasts.[112][160] Breast symmetry may be particularly sensitive to developmental disturbances and the symmetry differences for breasts are large compared to other body parts. Women who have more symmetrical breasts tend to have more children.[161]		Historical literature often includes specific features of individuals or a gender that are considered desirable. These have often become a matter of convention, and should be interpreted with caution. In Arabian society in the Middle Ages, a component of the female beauty ideal was for women to have small breasts.[139] In Persian literature, beautiful women are said to have breasts like pomegranates or lemons.[136] In the Chinese text "Jeweled Chamber Secrets" (Chinese: 玉 房 秘 訣) from the Six Dynasties period, the ideal woman was described as having firm breasts.[140] In Sanskrit literature, beautiful women are often said to have breasts so large that they cause the women to bend a little bit from their weight.[162] In Middle English literature, beautiful women should have small breasts that are round like an apple or a pear.[53]		Biological anthropologist Helen E. Fisher of the Center for Human Evolution Studies in the Department of Anthropology of Rutgers University said that, "perhaps, the fleshy, rounded buttocks... attracted males during rear-entry intercourse."[164] Bobbi S. Low et al. of the School of Natural Resources and Environment at the University of Michigan, said the female "buttocks evolved in the context of females competing for the attention and parental commitment of powerful resource-controlling males" as an "honest display of fat reserves" that could not be confused with another type of tissue,[165] although T. M. Caro, professor in the Center for Population Biology and the Department of Wildlife, Fish, and Conservation Biology, at University of California, Davis, rejected that as being a necessary conclusion, stating that female fatty deposits on the hips improve "individual fitness of the female", regardless of sexual selection.[165]		In a 1995 study, black men were more likely than white men to use the words "big" or "large" to describe their conception of an attractive woman's posterior.[166]		Body Mass Index (BMI) is an important determinant to the perception of beauty.[167] Even though the Western ideal is for a thin woman, some cultures prefer plumper women,[127][168] which has been argued to support that attraction for a particular BMI merely is a cultural artifact.[168] The attraction for a proportionate body also influences an appeal for erect posture.[169] One cross-cultural survey comparing body-mass preferences among 300 of the most thoroughly studied cultures in the world showed that 81% of cultures preferred a female body size that in English would be described as "plump".[170]		Availability of food influences which female body size is attractive which may have evolutionary reasons. Societies with food scarcities prefer larger female body size than societies that have plenty of food. In Western society males who are hungry prefer a larger female body size than they do when not hungry.[171]		In the United States, women overestimate men's preferences for thinness in a mate. In one study, American women were asked to choose what their ideal build was and what they thought the build most attractive to men was. Women chose slimmer than average figures for both choices. When American men were independently asked to choose the female build most attractive to them, the men chose figures of average build. This indicates that women may be misled as to how thin men prefer women to be.[168] Some speculate that thinness as a beauty standard is one way in which women judge each other[133] and that thinness is viewed as prestigious for within-gender evaluations of other women.[citation needed] A reporter surmised that thinness is prized among women as a "sign of independence, strength and achievement."[133] Some implicated the fashion industry for the promulgation of the notion of thinness as attractive.[172]		East Asians have historically preferred women whose bodies had small features. For example, during the Spring and Autumn period of Chinese history, women in Chinese harems wanted to have a thin body in order to be attractive for the Chinese emperor. Later, during the Tang Dynasty, a less thin body type was seen as most attractive for Chinese women.[173] In Arabian society in the Middle Ages, a component of the female beauty ideal was for women to be slender like a "cane" or a "twig".[139] In the Chinese text "Jeweled Chamber Secrets" (Chinese: 玉 房 秘 訣) from the Six Dynasties period, the ideal woman was described as not being "large-boned".[140]		In the Victorian era, women who adhered to Victorian ideals were expected to limit their food consumption to attain the ideal slim figure.[174] In Middle English literature, "slender" women are considered beautiful.[53]		A WHR of 0.7 for women has been shown to correlate strongly with general health and fertility. Women within the 0.7 range have optimal levels of estrogen and are less susceptible to major diseases such as diabetes, heart disease, and ovarian cancers.[176] Women with high WHR (0.80 or higher) have significantly lower pregnancy rates than women with lower WHRs (0.70–0.79), independent of their BMIs.[177][178] Female waist-to-hip ratio (WHR) has been proposed by evolutionary psychologists to be an important component of human male mate choice, because this trait is thought to provide a reliable cue to a woman's reproductive value.[179]		Both men and women judge women with smaller waist-to-hip ratios more attractive.[180] Ethnic groups vary with regard to their ideal waist-to-hip ratio for women,[181] ranging from 0.6 in China,[182] to 0.8 or 0.9 in parts of South America and Africa,[183][184][185] and divergent preferences based on ethnicity, rather than nationality, have also been noted.[186][187] A study found the Machiguenga people, an isolated indigenous South American ethnic group, prefer women with high WHR (0.9).[188] The preference for heavier women, has been interpreted to belong to societies where there is no risk of obesity.[189]		In Chinese, the phrase "willow waist" (Chinese: 柳 腰) is used to denote a beautiful woman by describing her waist as being slender like a willow branch.[140]		In the Victorian era, a small waist was considered the main trait of a beautiful woman.[174]		Most men tend to be taller than their female partner.[190] It has been found that, in Western societies, most men prefer shorter women. Having said this, height is a more important factor for a woman when choosing a man than it is for a man choosing a woman.[191] Men tend to view taller women as less attractive,[192] and people view heterosexual couples where the woman is taller to be less ideal.[192] Women who are 0.7 to 1.7 standard deviations below the mean female height have been reported to be the most reproductively successful,[193] since fewer tall women get married compared to shorter women.[192] However, in other ethnic groups, such as the Hadza, study has found that height is irrelevant in choosing a mate.[94]		In Middle English literature, 'tallness' is a characteristic of ideally beautiful women.[53]		A study using Polish participants by Sorokowski found 5% longer legs than average person leg to body ratio for both on man and woman was considered most attractive.[194] The study concluded this preference might stem from the influence of leggy runway models.[195] Another study using British and American participants, found "mid-ranging" leg-to-body ratios to be most ideal.[196]		A study by Swami et al. of British male and female undergraduates showed a preference for men with legs as long as the rest of their body and women with 40% longer legs than the rest of their body.[90] The researcher concluded that this preference might be influenced by American culture where long legged women are portrayed as more attractive.[90]		Marco Bertamini criticized the Swami et al. study for using a picture of the same person with digitally altered leg lengths which he felt would make the modified image appear unrealistic.[197] Bertamini also criticized the Swami study for only changing the leg length while keeping the arm length constant.[197] After accounting for these concerns in his own study, Bertamini's study which used stick figures also found a preference for women with proportionately longer legs than men.[197] When Bertamini investigated the issue of possible sexual dimorphism of leg length, he found two sources that indicated that men usually have slightly proportionately longer legs than women or that differences in leg length proportion may not exist between men and women.[197] Following this review of existing literature on the subject, he conducted his own calculations using data from 1774 men and 2208 women. Using this data, he similarly found that men usually have slightly proportionately longer legs than women or that differences in leg length proportion may not exist between men and women. These findings made him rule out the possibility that a preference for women with proportionately longer legs than men is due proportionately longer legs being a secondary sex characteristic of women.[197]		According to some studies, most men prefer women with small feet,[198][199] such as in ancient China where foot binding was practiced.[200]		In Jewish Rabbinic literature, the Rabbis considered small feet to be the ideal type of feet for women.[141]		Men have been found to prefer long-haired women.[110][201][202] An evolutionary psychology explanation for this is that malnutrition and deficiencies in minerals and vitamins causes loss of hair or hair changes. Hair therefore indicates health and nutrition during the last 2–3 years. Lustrous hair is also often a cross-cultural preference.[203] One study reported non-Asian men to prefer blondes and Asian men to prefer black-haired women.[202]		A component of the female beauty ideal in Persian literature is for women to have black hair,[136] which was also preferred in Arabian society in the Middle Ages.[139] In Middle English literature, curly hair is a necessary component of a beautiful woman.[53]		The way an individual moves can indicate health and even age and influence attractiveness.[203] A study reflecting the views of 700 individuals and that involved animated representations of people walking, found that the physical attractiveness of women increased by about 50 percent when they walked with a hip sway. Similarly, the perceived attractiveness of males doubled when they moved with a swagger in their shoulders.[204]		A preference for lighter-skinned women has remained prevalent over time, even in cultures without European contact, though exceptions have been found.[206] Anthropologist Peter Frost stated that since higher-ranking men were allowed to marry the perceived more attractive women, who tended to have fair skin, the upper classes of a society generally tended to develop a lighter complexion than the lower classes by sexual selection (see also Fisherian runaway).[206][207] In contrast, one study on men of the Bikosso tribe in Cameroon found no preference for attractiveness of females based on lighter skin color, bringing into question the universality of earlier studies that had exclusively focused on skin color preferences among non-African populations.[207]		Today, skin bleaching is not uncommon in parts of the world such as Africa,[208] and a preference for lighter-skinned women generally holds true for African Americans,[209] Latin Americans,[210] and Asians.[211] One exception to this has been in contemporary Western culture, where tanned skin used to be associated with the sun-exposed manual labor of the lower-class, but has generally been considered more attractive and healthier since the mid-20th century.[212][213][214][215][216]		More recent work has extended skin color research beyond preferences for lightness, arguing that redder (higher a* in the CIELab colour space) and yellower (higher b*) skin has healthier appearance.[106] These preferences have been attributed to higher levels of red oxygenated blood in the skin, which is associated with aerobic fitness and lack of cardiac and respiratory illnesses,[107] and to higher levels of yellow-red antioxidant carotenoids in the skin, indicative of more fruit and vegetables in the diet and, possibly more efficient immune and reproductive systems.[108]		Research has additionally shown that skin radiance or glowing skin indicates health, thus skin radiance influences perception of beauty and physical attractiveness.[217][218]		In Persian literature, beautiful women are said to have noses like hazelnuts.[136]		In Arabian society in the Middle Ages, a component of the female beauty ideal was for women to have straight and fine noses.[139]		In Jewish Rabbinic literature, the Rabbis considered a delicate nose to be the ideal type of nose for women.[141]		In Japan, during the Edo period, a component of the female beauty ideal was for women to have tall noses which were straight and not "too tall".[140]		A study where photographs of several women were manipulated (so that their faces would be shown with either the natural eye color of the model or with the other color) showed that, on average, brown-eyed men have no preference regarding eye color, but blue-eyed men prefer women of the same eye color.[219]		Through the East Asian blepharoplasty cosmetic surgery procedure, Asian women can permanently alter the structure of their eyelid. Some people have argued that this alteration is done to resemble the structure of a Western eyelid[220] while other people have argued that this is generally done solely as an improvement that "matches" an Asian face instead of being done to resemble the structure of a Western eyelid.[221]		A study that investigated whether or not an eyelid crease makes Chinese-descent women more attractive using photo-manipulated photographs of young Chinese-descent women's eyes found that the "medium upper eyelid crease" was considered most attractive by all three groups of both sexes: white people, Chinese and Taiwanese nationals together as a group, and Taiwanese and Chinese Americans together as a group. Similarly, all three groups of both genders found the absence of an eye crease to be least attractive on Chinese women.[222]		In the late sixteenth century, Japanese people considered epicanthic folds to be beautiful.[223]		A study that used Russian, American, Brazilian, Aché, and Hiwi raters, found that the only strong distinguisher between men and women's faces was wider eyes relative to facial height for women, and this trait consistently predicted attractiveness ratings for women.[224]		In Arabian society in the Middle Ages, a component of the female beauty ideal was for women to have dark black eyes which are large and long and in the shape of almonds. Furthermore, the eyes should be lustrous, and they should have long eyelashes.[139]		A source written in 1823, said that a component of the Persian female beauty ideal was for women to have large eyes which are black in color.[137] In Persian literature, beautiful women are said to have eyes that are shaped like almonds.[136]		In Chinese, the phrase "lucent irises, lustrous teeth" (Chinese: 明 眸 皓 齒) is used to describe a beautiful woman with "clear eyes" and "well-aligned, white teeth", and the phrase "moth-feeler eyebrows" (Chinese: 蛾眉) is used to denote a beautiful woman by describing her eyebrows as being thin and arched like moth antennae. In the Chinese text "The Grotto of the Immortals" (Chinese: 遊 仙 窟) written during the Tang dynasty period, narrow eyes were the preferred type of eyes for women, and, in the Chinese text "Jeweled Chamber Secrets" (Chinese: 玉 房 秘 訣) from the Six Dynasties period, the ideal woman was described as having small eyes.[140]		In Japan, during the Edo period, one piece of evidence, the appearance of the "formal wife" of Tokugawa Iesada as determined by "bone anthropologist" Suzuki Hisashi, indicates that large eyes were considered attractive for women, but, another piece of evidence, the 1813 Japanese text "Customs, Manners, and Fashions of the Capital" (Japanese: 都 風 俗 化 粧 伝), indicates that large eyes were not considered attractive for women.[140]		There are some subtle changes in women's perceived attractiveness across the menstrual cycle. During their most fertile phase, we can observe some changes in women's behavior and physiology. A study conducted by G. Miller (2007) examined the amount of tip earnings by lap dancers across the menstrual cycle. He found that dancers received nearly 15 USD more when they were near ovulation than during the rest of the month. This suggests that women either are more attractive during ovulation phase, or they experience a significant change in their behavior.[225] Some other studies have found that they are subtle differences in women’s faces when in their fertile phase. Bobst and Lobmaier (2012) created 20 prototyped photographs, some of a female during ovulation and some during the luteal phase. Men were asked to choose the more attractive, the more caring and the more flirtatious faces. They found a significant preference for the follicular phase (ovulation). This suggests that subtle shape differences in faces occurring during the female's ovulation phase are sufficient to attract men more.[226] This idea is supported by another study, where a similar experiment was done. Men and women had to judge photographs of women’s faces taken during their fertile phase. They were all rated more attractive than during non-fertile phase. They are some subtle visible cues to ovulation in women's faces, and they are perceived as more attractive, leading to the idea that it could be an adaptive mechanism to raise a female's mate value at that specific time (when probability of conception is at its highest).[225]		Women's attractiveness, as perceived by men and women, slightly differs across her menstrual cycle, being at peak when she is in her ovulation phase. Jones et al. (2008), focused on women’s preferences for masculinity, apparent health and self-resemblance and found that it varies across the cycle. They explained that the function of the effects of menstrual cycle phase on preferences for apparent health and self-resemblance in faces is to increase the likelihood of pregnancy.[227]		Similarly, female prefer the scent of symmetrical men and masculine faces during fertile phases as well as stereotypical male displays such as social presence, and direct intrasexual competitiveness.[228]		During the follicular phase (fertile), females prefer more male's traits (testosterone dependent traits such as face shape) than when in non-fertile phase. Those findings have been found in the voice, showing that females’ preferences for more masculine voices over feminine voices increase the fertile phase of the menstrual cycle.[229]		But not only females' preferences vary across cycle, their behaviours as well. Effectively, men respond differently to females when they are on ovulatory cycle,[225] because females act differently. Women in the ovulatory phase are flirtier with males showing genetic fitness markers than in low fertile phase.[230] It has been shown in some studies that women high in estrogen are generally perceived to be more attractive than women with low levels of estrogen, based on women not wearing make-up. High estrogen level women are also considered to have healthier and have a more feminine face.[231]		Similarly, a study investigated the capacity of women to select high quality males based on their facial attractiveness. They found that facial attractiveness correlated with semen quality (good, normal, or bad depending on sperm morphology and motility). The more attractive a man's face is, linked to his sperm being of better quality.[232]		Sexual ornaments are seen in many organisms; in humans, females have sexual ornamentation in the form of breasts and buttocks. The physical attraction to sexual ornaments is associated with gynoid fat, as opposed to android fat, which is considered unattractive.[233] In human females, proximate causes of the development of sexual ornaments are associated with the predominance of estrogren in puberty. The activation of estrogren receptors around the female skeletal tissue causes gynoid fat to be deposited in the breasts, buttocks, hips and thighs, producing an overall typical female body shape.[234] Specifically, female breasts are considered more attractive when symmetrical, rather than asymmetrical,[235] as this is thought to reflect good developmental stability.[236]		Sexual ornaments are considered attractive features as they are thought to indicate high mate value, fertility,[237] and the ability to provide good care to offspring. They are sexually selected traits present for the purpose of honest signalling and capturing the visual attention of the opposite sex, most commonly associated with females capturing the visual attention of males. It has been proposed that these ornaments have evolved in order to advertise personal quality and reproductive value.[238] Honest signalling with sexual ornaments is associated with ultimate causation of these evolved traits. The evolution of these ornaments is also associated with female-female competition in order to gain material benefits provided by resourceful and high status males.[29] In humans, once these sexual ornaments develop, they are permanent. It is thought that this is associated with the long-term pair bonding humans engage in; human females engage in extended sexual activity outside of their fertile period.[239] This relates to another ultimate cause of sexual ornaments with function in obtaining non-genetic material benefits from males. In other animal species, even other primate species, these advertisements of reproductive value are not permanent. Usually, it is the point at which the female is at her most fertile, she displays sexual swellings.[240]		Adolescence is the period of time whereby humans experience puberty, and experience anatomical changes to their bodies through the increase of sex hormones released in the body. Adolescent exaggeration is the period of time at which sexual ornaments are maximised, and peak gynoid fat content is reached.[29] In human females, the mean age for this is approximately 16 years. Female breasts develop at this stage not only to prepare for reproduction, but also due to competition with other females in displaying their reproductive value and quality to males.[29]		For both men and women, there appear to be universal criteria of attractiveness both within and across cultures and ethnic groups.[13][241] When considering long term relationships, some studies have found that men place a higher emphasis on physical attractiveness in a partner than women do.[242][243][244][245][246] On the other hand, some studies have found few differences between men and women in terms of the weight they place on physical characteristics when they are choosing partners for short-term relationships,[247][248][249][250] in particular with regard to their implicit, as opposed to explicitly articulated, preferences.[251] Other recent studies continue to find sex differences for long-term relationships.[252][253][254][255] There is also one study suggesting that only men, not women, place greater priority on bodily compared to facial attractiveness when looking for a short-term as compared to a long-term partner.[256]		Some evolutionary psychologists, including David Buss, have argued that this long-term relationship difference may be a consequence of ancestral humans who selected partners based on secondary sexual characteristics, as well as general indicators of fitness which allowed for greater reproductive success as a result of higher fertility in those partners,[257] although a male's ability to provide resources for offspring was likely signaled less by physical features.[244] It is argued that the most prominent indicator of fertility in women is youth,[258][259] while the traits in a man which enhance reproductive success are proxies for his ability to accrue resources and protect.[259]		Studies have shown that women pay greater attention to physical traits than they do directly to earning capability or potential to commit,[260] including muscularity, fitness and masculinity of features; the latter preference was observed to vary during a woman's period, with women preferring more masculine features during the late-follicular (fertile) phase of the menstrual cycle.[261][262] Additionally, women process physical attractiveness differently, paying attention to both individual features and the aesthetic effect of the whole face.[263] A 2003 study in the area concluded that heterosexual women are about equally aroused when viewing men or women. Heterosexual men were only aroused by women. This study verified arousal in the test subjects by connecting them to brain imaging devices.[264][265][266][267] Notably, the same study reported arousal for women upon viewing animals mating.		Bonnie Adrian's book, Framing the Bride, discusses the emphasis Taiwanese brides place on physical attractiveness for their wedding photographs. Globalization and western ideals of beauty have spread and have become more prevalent in Asian societies where brides go through hours of hair and makeup to "transform everyday women with their individual characteristics into generic look-alike beauties in three hours' time." These brides go through hours of makeup to transform themselves into socially constructed beauty.[268]		According to strategic pluralism theory, men may have correspondingly evolved to pursue reproductive strategies that are contingent on their own physical attractiveness. More physically attractive men accrue reproductive benefits from spending more time seeking multiple mating partners and relatively less time investing in offspring. In contrast, the reproductive effort of physically less attractive men, who therefore will not have the same mating opportunities, is better allocated either to investing heavily in accruing resources, or investing in their mates and offspring and spending relatively less time seeking additional mates.[269]		Several studies have suggested that people are generally attracted to people who look like them[270] and they generally evaluate faces that exhibit features of their own ethnic or racial group as being more attractive.[202] Although both men and women use children's "facial resemblance" to themselves in "attractiveness judgments," a greater percentage of women in one study (37% n=30) found hypothetical children whose faces were "self-morphs" of themselves as most attractive when compared to men (30% n=23).[271] The more similar a judged person is toward the judging person, the more the former is liked. However, this effect can be reversed. This might depend on how attractiveness is conceptualized: similar members (compared to dissimilar ones) of the opposite sex are judged as more likable in a prosocial sense. Again, findings are more ambiguous when looking for the desiring, pleasure related component of attractiveness.[272] This might be influenced by the measure one uses (subjective ratings can differ from the way one actually reacts) and by situational factors: while men usually prefer women whose face resembles their own, this effect can reverse under stress, when dissimilar females are preferred.[273]		A study by R. E. Hall in 2008, which examined determinations of physical attractiveness by having subjects look at the faces of women, found that race was sometimes a factor in these evaluations.[274] In 2011, two studies found evidence that the ethnicity of a face influenced how attractive it was judged to be.[275][276] A 2014 study by Tsunokai, McGrath and Kavanagh based on data from a dating website, the authors cited race as a factor in dating preferences by Asian-American men, both homosexual and heterosexual.[277]		Perceptions of physical attractiveness contribute to generalized assumptions based on those attractions. Individuals assume that when someone is beautiful, then they have many other positive attributes that make the attractive person more likeable.[12] This is referred to as the halo effect, also known as the 'beautiful-is-good' effect.[12] Across cultures, what is beautiful is assumed to be good; attractive people are assumed to be more extroverted, popular, and happy. This could lead to a self-fulfilling prophecy, as, from a young age, attractive people receive more attention that helps them develop these characteristics.[278][279] In one study, beautiful people were found to be generally happier than less beautiful or plain people, perhaps because these outgoing personality traits are linked to happiness, or perhaps because beauty led to increased economic benefits which partially explained the increased happiness.[121] In another study testing first impressions in 56 female and 17 male participants at University of British Columbia, personality traits of physically attractive people were identified more positively and more accurately than those who were less physically attractive. It was explained that people pay closer attention to those they find physically beautiful or attractive, and thus perceiving attractive individuals with greater distinctive accuracy. The study believes this accuracy to be subjective to the eye of the beholder.[280] Recent results from the Wisconsin Longitudinal Study confirmed the positive link between psychological well-being and attractiveness (higher facial attractiveness, lower BMI) and also found the complementary negative association with distress/depression. Even though connections and confounds with other variables could not be excluded, the effects of attractiveness in this study were the same size as the ones for other demographic variables.[281]		In developed western societies, women tend to be judged for their physical appearance over their other qualities and the pressure to engage in beauty work is much higher for women than men. Beauty work is defined as various beauty “practices individuals perform on themselves or others to elicit certain benefits from a specific social hierarchy.”[282] Being “beautiful” has individual, social and institutional rewards.[282] Although marketers have started to target the “metro-sexual” male and produce hygiene and beauty products geared towards men, the expectations placed on them is less than women[283] The time and money required for a man to achieve the same well-groomed appearance is much lower. Even in areas that men also face pressure to perform beauty work, such a haircuts/styling, the prices discrepancy for products and services are skewed. This phenomenon is called the “pink tax."[284][285]		However, attractiveness varies by society; in ancient China foot binding was practiced by confining young girls' feet in tightly bound shoes to prevent the feet from growing to normal size causing the women to have an attractive "lotus gait". In England, women used to wear corsets that severely constricted their breathing and damaged vital internal organs, in order to achieve a visual effect of an exaggeratedly low Waist-to-Hip ratio.		People make judgments of physical attractiveness based on what they see, but also on what they know about the person. Specifically, perceptions of beauty are malleable such that information about the person's personality traits can influence one's assessment of another person's physical beauty. A 2007 study had participants first rate pictures for attractiveness. After doing distracting math problems, participants saw the pictures again, but with information about the person's personality. When participants learned that a person had positive personality characteristics (e.g., smart, funny, kind), that person was seen as more physically attractive.[286] Conversely, a person with negative personality characteristics (e.g., materialistic, rude, untrustworthy) was seen as less physically attractive. This was true for both females and males. A person may be perceived as being more attractive if they are seen as part of a group of friends, rather than alone, according to one study.[287]		Physical attractiveness can have various effects. A survey conducted by London Guildhall University of 11,000 people showed that those who subjectively describe themselves as physically attractive earn more income than others who would describe themselves as less attractive.[288] People who described themselves as less attractive earned, on average, 13% less than those who described themselves as more attractive, while the penalty for being overweight was around 5%. According to further research done on the correlation between looks and earnings in men, the punishment for unattractiveness is greater than the benefits of being attractive. However, in women the punishment is found to be equal to the benefits.[289] Another study suggests that more physically attractive people are significantly more likely on average to earn considerably higher wages. Differences in income due to attractiveness was much more pronounced for men rather than women, and held true for all ranges of income.[290]		It is important to note that other factors such as self-confidence may explain or influence these findings as they are based on self-reported attractiveness as opposed to any sort of objective criteria; however, as one's self-confidence and self-esteem are largely learned from how one is regarded by his/her peers while maturing, even these considerations would suggest a significant role for physical appearance. One writer speculated that "the distress created in women by the spread of unattainable ideals of female beauty" might possibly be linked to increasing incidence of depression.[291]		Many have asserted that certain advantages tend to come to those who are perceived as being more attractive, including the ability to get better jobs and promotions; receiving better treatment from authorities and the legal system; having more choices in romantic partners and, therefore, more power in relationships; and marrying into families with more money.[21][121][278][279][292] Those who are attractive are treated and judged more positively than those who are considered unattractive, even by those who know them. Also, attractive individuals behave more positively than those who are unattractive.[293] One study found that teachers tend to expect that children who are attractive are more intelligent, and are more likely to progress further in school. They also consider these students to be more popular.[294] Voters choose political candidates who are more attractive over those who are less attractive.[295] Men and women use physical attractiveness as a measure of how "good" another person is.[296] In 1946, Soloman Asch coined the Implicit Personality Theory, meaning that the presence of one trait tends to imply the existence of other traits. This is also known as the halo effect. Research suggests that those who are physically attractive are thought to have more socially desirable personalities and lead better lives in general.[297] This is also known as the "what-is-beautiful-is-good effect." Discrimination against or prejudice towards others based on their appearance is sometimes referred to as lookism.		Some researchers conclude that little difference exists between men and women in terms of sexual behavior.[298][299] Other researchers disagree.[300] Symmetrical men and women have a tendency to begin to have sexual intercourse at an earlier age, to have more sexual partners, to engage in a wider variety of sexual activities, and to have more one-night stands. They are also prone to infidelity and are more likely to have open relationships.[61] Additionally, they have the most reproductive success. Therefore, their physical characteristics are most likely to be inherited by future generations.[301][302][303][304]		Concern for improving physical attractiveness has led many persons to consider alternatives such as cosmetic surgery. It has led scientists working with related disciplines such as computer imaging and mathematics to conduct research to suggest ways to surgically alter the distances between facial features in order to make a face conform more closely to the "agreed-upon standards of attractiveness" of an ideal face by using algorithms to suggest an alternative which still resembles the current face.[19] One research study found that cosmetic surgery as a way to "boost earnings" was "not profitable in a monetary sense."[121] Some research shows that physical attractiveness has a marginal effect on happiness.[305]		
Revenge of the Nerds is a 1984 American sex comedy film from 20th Century Fox, produced by Ted Field and Peter Samuelson, directed by Jeff Kanew, and starring Robert Carradine and Anthony Edwards, with Curtis Armstrong, Ted McGinley, Julia Montgomery, Brian Tochi, Larry B. Scott, Michelle Meyrink, John Goodman, and Donald Gibb.		The film's plot chronicles a group of nerds at the fictional Adams College trying to stop the ongoing harassment by the jock fraternity, the Alpha Betas, who are aided by their sister sorority Pi Delta Pi.						Best friends and nerds Lewis Skolnick and Gilbert Lowe enroll in Adams College to study computer science. They are kicked out of the freshmen dorms by the Alpha Betas, a fraternity composed primarily of football team members, after the Alphas carelessly burn down their own frat house. Dean Ulich sets up the freshmen in temporary quarters in the school's gymnasium, but allows them to rush the fraternities to alleviate their housing situation. Lewis, Gilbert, and other nerds fail to gain fraternity membership, but are able to rent and completely renovate a rundown two-story campus house.		Their success irks Stan Gable, the lead Alpha Beta and Adams' star quarterback; he sets his fellow fraternity members against the nerds, pulling several pranks. The nerds approach the campus police for help, but are bound by the fraternities' Greek Council that adjudicates all such pranks; the only way to appeal the Greek Council's inaction is to join a national fraternity. A black fraternity Lambda Lambda Lambda (Tri-Lambs) considers an Adams College chapter. The fraternity president, U.N. Jefferson, is not enthusiastic about a predominantly white group becoming a chapter, but is forced to grant an automatic 60-day probationary membership due to bylaws. The nerds invite Jefferson to a Lambda party with the Omega Mu sorority, which contain similar nerdy women including Gilbert's girlfriend Judy; their party is livened up when Booger supplies joints with high quality marijuana. The Alpha Betas, along with the Pi Delta Pis, to which Stan's head cheerleader girlfriend Betty Childs belongs, disrupt the party with a herd of pigs. U.N. Jefferson's attitude changes when he sees the discrimination the nerds face. The nerds later take their revenge on both groups by staging a panty raid on the Pi Delta Pis and pouring liquid heat into the Alpha Betas' jock straps. Jefferson is impressed by the nerds' willingness to stand up for themselves, and he commissions them the Adams College chapter of Lambda Lambda Lambda.		Although now a fraternity, the harrassment continues, and Lambda Lambda Lambda finds their prank charges stonewalled by Stan, who is also president of the Greek Council. The nerds realize they can only stop Stan by the Tri-Lambs winning the upcoming Greek Games during homecoming, the winning chapter gaining the presidency. Using their high intelligence, the nerds, working with the Omega Mus, win some sporting events and finish the athletic competition in second place. During the costume/charity sale events, the nerds use nude photos of Betty (taken during their earlier revenge pranks) to outsell the Alpha Betas. During this, Lewis, who has fallen in love with Betty, steals Stan's costume and tricks her into having sex with him. Betty is surprised when Lewis reveals his identity, but later admits to Stan that she is in love with a Tri-Lamb.		With the Alpha Betas and nerds in close running, the final event is a musical competition. The nerds readily outdo the Alpha Betas with a techno-computer-driven musical production and win the competition. Gilbert is nominated by the Tri-Lams are president-elect of the next Greek Council. Enraged, Coach Harris demands the Alpha Betas take revenge, and Stan, having just learned that Betty has fallen for Lewis, orders the Alpha Betas to trash the nerds' fraternity house.		The nerds are despondent at seeing their house wrecked, but Gilbert says Lambda Lambda Lambda is the first time he has been in an accepted group and will not let this stand. Attempting to speak at a pep rally, the Alpha Betas stop Gilbert, but Dean Ulich, U. N. Jefferson, and a group of national Tri-Lamb members arrive in force to ensure that Gilbert is allowed to speak. Gilbert speaks out on the discrimination the nerds have endured, causing Lewis to join him and finally to admit his nerd status. Judy and Betty then join their respective boyfriends on stage, and Gilbert asks all alumni who have ever been picked on or made to feel inferior to join them. Most of the audience does so. Dean Ulich then says that the Tri-Lams will occupy the Alpha Beta house until all damage to the Tri-Lam house is repaired. When Burke and Ogre protest, asking where they will live, Dean Ulich retorts, "You're jocks, go live in the gym". The nerds and alumni celebrate their victory.		Exterior scenes such as the arrival of the nerds at college and the fraternity houses were filmed at the University of Arizona in Tucson, Arizona. The original Nerds residence, from which they were ousted by the Alpha Betas, was actually Cochise Hall.[3] Their subsequent residence was U of A's Bear Down Gymnasium.[citation needed] The original Alpha Beta fraternity house that is burned down was filmed at the Alpha Gamma Rho house and the Beta Theta Pi house (on University Boulevard). The Pi Delta Pi sorority house was actually the Phi Delta Theta fraternity house.[citation needed]		Ollie E. Brown, of Ollie & Jerry fame, wrote and performed as Revenge the song "They're So Incredible" for the film. "They're So Incredible" is performed with different lyrics by the nerds in the film at the final event of the Greek Games.[citation needed]		The film holds a 70% approval rating and 5.9/10 average at the review aggregator website Rotten Tomatoes based on 43 critics' reviews. The consensus is: "Undeniably lowbrow but surprisingly sly, Revenge of the Nerds has enough big laughs to qualify as a minor classic in the slobs-vs.snobs subgenre."[5] It also holds a 41 out of 100 ratio on Metacritic based on five critics' reviews and signifying "mixed or average reviews".[6] Revenge of the Nerds is #91 on Bravo's "100 Funniest Movies".[7]		About three decades after the film's release, commentators have looked back at the film and considered some of the scenes, particularly when Lewis pretends to be Stan and has a sexual encounter with Betty, to be rape by deception, and a misogynistic remnant of a male-dominated culture of that time.[8] William Bradley of The Mary Sue stated that after viewing the film again as an adult he "was immediately struck by the way the film plays sexual exploitation and assault for laughs".[9] Amy Benfor of Salon wrote that the Revenge of the Nerds scene, and a similar scene in John Hughes' Sixteen Candles, were evidence that at the time of these films' productions, "people were stupid about date rape".[10]		Due to the influence of the film, several chapters of Lambda Lambda Lambda have sprung up in different locations around the United States. The real life fraternity has six chapters in Connecticut, Maryland, New York, and Washington.[11]		Three less successful sequels followed, two of which were television films.		A remake of the original Revenge of the Nerds was slated for release in 2007, the first project for the newly created Fox Atomic, but was canceled in November 2006 after two weeks of filming.[12] The cast included Adam Brody, Dan Byrd, Katie Cassidy, Kristin Cavallari, Jenna Dewan, Chris Marquette, Ryan Pinkston, Efren Ramirez, and Nick Zano. The film was to be directed by Kyle Newman, executive produced by McG, and written by Gabe Sachs and Jeff Judah, Adam Jay Epstein and Andrew Jacobson, and Adam F. Goldberg.[13]		Filming took place in Atlanta, Georgia at Agnes Scott College, the Georgia State Capitol, and Inman Park.[14] Filming was originally scheduled to take place at Emory University, but university officials changed their minds after reading the script.[15][16] The film was shelved after producers found the movie difficult to shoot on the smaller Agnes Scott campus and studio head Peter Rice was disappointed with the dailies.[12] 20th Century Fox personnel have stated that it's highly unlikely that a remake will be picked up in the future.[16]		
Happy Days is an American television sitcom that aired first-run from January 15, 1974, to September 24, 1984 on ABC, with a total of 255 half-hour episodes spanning over eleven seasons. Created by Garry Marshall, the series presented an idealized vision of life in the mid-1950s to mid-1960s Midwestern United States, and starred Ron Howard as teenager Richie Cunningham, Henry Winkler as his friend Arthur "Fonzie"/"The Fonz" Fonzarelli, and Tom Bosley and Marion Ross as Richie's parents, Howard and Marion Cunningham.[1] Happy Days became one of the biggest hits in television history and heavily influenced the television style of its time.[2]		The series began as an unsold pilot starring Howard, Ross and Anson Williams, which aired in 1972 as a segment entitled "Love and the Television Set" (later retitled "Love and the Happy Days" for syndication) on ABC's anthology show Love, American Style. Based on the pilot, director George Lucas cast Howard as the lead in his 1973 hit film American Graffiti, causing ABC to take a renewed interest in the pilot. The first two seasons of Happy Days focused on the experiences and dilemmas of "innocent teenager" Richie Cunningham, his family, and his high school friends, attempting to "honestly depict a wistful look back at adolescence".[2] Initially a moderate hit, the series' ratings began to fall during its second season, causing Marshall to retool it emphasizing broad comedy and spotlighting the previously minor character of Fonzie, a "cool" biker and high school dropout.[2] Following these changes, Happy Days became the number-one program in television in 1976-1977, Fonzie became one of the most merchandised characters of the 1970s, and Henry Winkler became a major star.[3][4] The series also spawned a number of spin-offs, including the hit shows Laverne & Shirley and Mork & Mindy.						Set in Milwaukee, Wisconsin, the series revolves around teenager Richie Cunningham and his family: his father, Howard, who owns a hardware store; traditional homemaker and mother, Marion; younger sister Joanie; Richie's older brother Chuck (seasons 1 and 2 only), and high school dropout, biker and suave ladies' man Arthur "Fonzie"/"The Fonz" Fonzarelli, who would eventually become Richie's best friend and the Cunninghams' upstairs tenant. The earlier episodes revolve around Richie and his friends, Potsie Weber and Ralph Malph, with Fonzie as a secondary character. However, as the series progressed, Fonzie proved to be a favorite with viewers and soon more story lines were written to reflect his growing popularity, and Winkler was eventually credited with top billing in the opening credits alongside Howard as a result.[5] Fonzie befriended Richie and the Cunningham family, and when Richie left the series for military service, Fonzie became the central figure of the show, with Winkler receiving sole top billing in the opening credits. In later seasons, other characters were introduced including Fonzie's young cousin, Charles "Chachi" Arcola, who became a love interest for Joanie Cunningham. The eleven seasons of the series roughly track the eleven years from 1955 to 1965, inclusive, in which the show was set.		The series' pilot was originally shown as Love and the Television Set, later retitled Love and the Happy Days for syndication, a one-episode teleplay on the anthology series Love, American Style, aired on February 25, 1972. Happy Days spawned the hit television shows Laverne & Shirley and Mork & Mindy as well as three failures, Joanie Loves Chachi, Blansky's Beauties featuring Nancy Walker as Howard's cousin,[6] and Out of the Blue. The show is the basis for the Happy Days musical touring the United States since 2008. The leather jacket worn by Winkler during the series was acquired by the Smithsonian Institution for the permanent collection at the National Museum of American History.[7]		With season four, Al Molinaro was added as Al Delvecchio, the new owner of Arnold's, after Pat Morita's character of Arnold moved on after his character got married. (Morita had left the program to star in a short-lived sitcom of his own, Mr. T and Tina, which was actually a spin-off of Welcome Back, Kotter. Morita also starred in a subsequent short lived Happy Days spin-off series titled Blansky's Beauties.) Al Molinaro also played Al's twin brother Father Anthony Delvecchio, a Catholic priest. Al eventually married Chachi's mother (played by Ellen Travolta) and Father Delvecchio served in the wedding of Joanie to Chachi in the series finale.		The most major character changes occurred after season five with the addition of Scott Baio as Fonzie's cousin, Charles "Chachi" Arcola. Originally, the character Spike (mentioned as Fonzie's nephew in the episode "Not With My Sister You Don't," but also claimed to be his cousin, as was stated in one episode) was supposed to be the character who became Chachi. Season five also saw the introduction of more outlandish and bizarre plots including Fonzie making a bet with the Devil, and the appearance of Mork (Robin Williams), an alien who wanted to take Richie back to his homeworld. Although when first aired this ended with it all simply being a dream Richie was having, this episode was retconned in subsequent airings by way of additional footage to have actually taken place, with Mork having wiped everyone's memory except Richie's and then deciding to time travel to the present day (the setting of Mork & Mindy).		Lynda Goodfriend joined the cast as semi-regular character Lori Beth Allen, Richie's steady girlfriend, in season five, and became a permanent member of the cast between seasons eight and nine, after Lori Beth married Richie.		After Ron Howard (Richie) left the series, Ted McGinley joined the cast as Roger Phillips, the new physical education teacher at Jefferson High and nephew to Howard and Marion. He took over from the departed Richie Cunningham character, acting as counterpoint to Fonzie. Cathy Silvers also joined the cast as Jenny Piccalo, Joanie's best friend who was previously referenced in various episodes from earlier seasons and remained as a main cast member until the final season. Both actors were originally credited as guest stars but were promoted to the main cast during season ten after several series regulars left the show. The real focus of the series was now on the Joanie and Chachi characters, and often finding ways to incorporate Fonzie into them as a shoulder to cry on, advice-giver, and savior as needed. The Potsie character, who had already been spun off from the devious best friend of Richie to Ralph's best friend and confidante, held little grist for the writers in this new age, and was now most often used as the occasional "dumb" foil for punchlines (most often from Mr. C., whom he later worked for at Cunningham Hardware, or Fonzie).		Billy Warlock joined the cast in season 10 as Roger's brother Flip, along with Crystal Bernard as Howard's niece K.C. They were intended as replacements for Erin Moran and Scott Baio (who departed for their own show, Joanie Loves Chachi) and were credited as part of the semi-regular cast. Both characters left with the return of Moran and Baio, following the cancellation of Joanie Loves Chachi. Al Molinaro also left Happy Days in season 10 for Joanie Loves Chachi. Pat Morita then returned to the cast as Arnold in his absence.		In season 11, the story line of Richie and Lori Beth is given closure with the two-part episode "Welcome Home." Richie returns home from the Army, but barely has time to unpack when he learns that his parents have lined up a job interview at the Milwaukee Journal for him. However, they are taken aback when he tells them he prefers to take his chances in California to become a Hollywood screenwriter. They remind him of his responsibilities and while Richie gives in, he becomes angry and discontented, torn between his obligations to his family and fulfilling his dream. After a confrontation that ends with a conversation with Fonzie, he decides to face his family and declare his intentions. While somewhat reluctant at first, they support him and bid Richie, Lori Beth, and Little Richie an emotional farewell.		Happy Days originated during a time of 1950s nostalgic interest as evident in 1970s film, television, and music. Beginning as an unsold pilot filmed in late 1971 called New Family in Town, with Harold Gould in the role of Howard Cunningham, Marion Ross as Marion, Ron Howard as Richie, Anson Williams as Potsie, Ric Carrott as Charles "Chuck" Cunningham, and Susan Neher as Joanie, Paramount passed on making it into a weekly series, and the pilot was recycled with the title Love and the Television Set (later retitled Love and the Happy Days for syndication), for presentation on the television anthology series Love, American Style. In 1972, George Lucas asked to view the pilot to determine if Ron Howard would be suitable to play a teenager in American Graffiti, then in pre-production. Lucas immediately cast Howard in the film, which became one of the top-grossing films of 1973. Show creator Garry Marshall and ABC recast the unsold pilot to turn Happy Days into a series. According to Marshall in an interview, executive producer Tom Miller said while developing the sitcom, "If we do a TV series that takes place in another era, and when it goes into reruns, then it won't look old." This made sense to Marshall while on the set of the show.		Gould had originally been tapped to reprise the role of Howard Cunningham on the show. However, during a delay before the start of production he found work doing a play abroad and when he was notified the show was ready to begin production, he declined to return because he wanted to honor his commitment.[22] Bosley was then offered the role.		The first two seasons of Happy Days (1974–75) were filmed using a single-camera setup and laugh track. One episode of season two ("Fonzie Gets Married") was filmed in front of a studio audience with three cameras as a test run. From the third season on (1975–84), the show was a three-camera production in front of a live audience (with a cast member, usually Tom Bosley, announcing in voice-over, "Happy Days is filmed before a live audience" at the start of most episodes), giving these later seasons a markedly different style. A laugh track was still used during post-production to smooth over live reactions.		Gary Marshall's earlier television series The Odd Couple had undergone an identical change in production style after its first season in 1970–71.		The show had two main sets: the Cunningham home and Arnold's/Al's Drive-in.		In seasons one and two, the Cunningham house was arranged with the front door on the left and the kitchen on the right of screen, in a triangular arrangement. From season three on, the house was rearranged to accommodate multiple cameras and a studio audience.		The Cunninghams' official address is 565 North Clinton Drive, Milwaukee, Wisconsin.[24] The house that served as the exterior of the Cunningham residence is actually located at 565 North Cahuenga Boulevard (south of Melrose Avenue) in Los Angeles, several blocks from the Paramount lot on Melrose Avenue.		The Milky Way Drive-In, located on Port Washington Road in the North Shore suburb of Glendale, Wisconsin (now Kopp's Frozen Custard Stand), was the inspiration for the original Arnold's Drive-In; it has since been demolished. The exterior of Arnold's was a standing set on the Paramount Studios lot that has since been demolished. This exterior was close to Stage 19, where the rest of the show's sets were located.		The set of the diner in the first season was a room with the same vague details of the later set, such as the paneling, and the college pennants. When the show changed to a studio production in 1975, the set was widened and the entrance was hidden, but allowed an upstage, central entrance for cast members. The barely-seen kitchen was also upstaged and seen only through a pass-through window. The diner had orange booths, downstage center for closeup conversation, as well as camera left. There were two restroom doors camera right, labeled "Guys" and "Dolls". A 1953 Seeburg Model G jukebox (with replaced metal pilasters from Wico Corp.) was positioned camera right, and an anachronistic "Nip-It" pinball machine (actually produced in 1972) was positioned far camera right.		College pennants adorned the walls, including Purdue and University of Wisconsin–Milwaukee, along with a blue and white sign reading "Jefferson High School". Milwaukee's Washington High School provided the inspiration for the exteriors of the fictional Jefferson.		In a two-part episode from the seventh season, the original Arnold's Drive-In was written out of the series as being destroyed by fire (see List of Happy Days episodes, episodes 159 and 160). In the last seasons that covered the 60s timeline, a new Arnold's Drive-In set (to portray the new Arnold's that replaced the original Arnold's destroyed by the fire) emerged in a 60s decor with wood paneling and stained glass.		In 2004, two decades after the first set was destroyed, the Happy Days 30th Anniversary Reunion requested that the reunion take place in Arnold's. The set was rebuilt by production designer James Yarnell based on the original floor plan. The reunion special was taped at CBS Television City's Bob Barker Studio in September 2004.[25]		Season one used a newly recorded version of "Rock Around the Clock" by Bill Haley & His Comets (recorded in the fall of 1973) as the opening theme song. This recording was not commercially released at the time, although the original 1954 recording returned to the American Billboard charts in 1974 as a result of the song's use on the show. The "Happy Days" recording had its first commercial release in 2005 by the German label Hydra Records. (When Happy Days entered syndication in 1979, the series was retitled Happy Days Again and used an edited version of the 1954 recording instead of the 1973 version). In some prints intended for reruns and overseas broadcasts, the original "Rock Around the Clock" opening theme is replaced by the more standard "Happy Days" theme.		The show's closing theme song in seasons one and two was a fragment from "Happy Days" (although in a different recording with different lyrics to that which would become the standard version), whose music was composed by Charles Fox and whose lyrics were written by Norman Gimbel. According to SAG, this version was performed by Jimmy Haas on lead vocals, Ron Hicklin of the Ron Hicklin Singers, Stan Farber, Jerry Whitman, and Gary Garrett on backing vocals, and studio musicians.		From seasons three to ten inclusive, a longer version of "Happy Days" replaced "Rock Around the Clock" at the beginning of the show. Released as a single in 1976 by Pratt & McClain, "Happy Days" cracked the Top 5. The show itself finished the 1976–77 television season at #1, ending the five-year Nielsen reign of All in the Family. On the Season 2 DVD set release & later re releases of the Season 1 DVD set, the song "Rock Around the Clock" was replaced with a reconstructed version of "Happy Days" because of music rights issues.		For the show's 11th and final season (1983–84), the theme was rerecorded in a more modern style. It featured Bobby Arvon on lead vocals, with several back-up vocalists. To accompany this new version, new opening credits were filmed, and the flashing Happy Days logo was reanimated to create an overall "new" feel which incorporated 1980s sensibilities with 1950s nostalgia (although by this time the show was set in 1965).		The idiom "jumping the shark" describes a point in a series where it resorts to outlandish or preposterous plot devices to maintain or regain good ratings. Specifically, the term arose from the season five episode "Hollywood (Part 3)" that first aired on September 20, 1977, in which a water-skiing Fonzie (clad in swim trunks and signature leather jacket) jumps over a confined shark. Despite the decline in ratings, Happy Days continued for several years until its cancellation in 1984. The program never received an Emmy nomination for writing during its entire run; comedy writing Emmy nominations during Happy Days broadcast history were routinely awarded to the writers of such shows as M*A*S*H, The Mary Tyler Moore Show, and All in the Family.[37][38]		Happy Days has been syndicated by many networks. It aired in the United States on TBS from 1989 to 1995, Nick at Nite from 1995 to 2000 (and again in 2002–03), Odyssey Network/Hallmark Channel from 1999 to 2002 (and again from January to April 2013), TV Land from 2002 to 2007, WGN America from 2002 until 2008, and FamilyNet from 2009 to 2010. It also aired on Me-TV from December 21, 2010, until early 2012, when it was removed from the network's lineup, where it aired on Sunday afternoons at 1pm Eastern and Pacific time. It returned to Me-TV on May 29, 2017. The series also joined INSP's line-up, airing in an hour block from 6 to 7 pm Eastern time, on January 2, 2012 to September 30, 2013. From October 11, 2010 through October 3, 2014, the show aired on Hub Network. The show returned to Me-TV on May 26, 2014. It also showed reruns on Cloo.		In the United Kingdom reruns aired on Five USA and on Channel 4 between the early 1990s and the early 2000s. Original-run episodes in the 1970s and 1980s were shown on various regions of the ITV network usually on a weekday afternoon at 17:15. It is currently (2015–16) being shown on the True Entertainment channel.		When reruns first went into syndication on local stations while the series was still producing new episodes, the reruns were re-titled Happy Days Again. The series went into off-network syndication in fall 1979, just as season seven began on ABC. There are also some episodes still aired with the Happy Days Again title.		On April 19, 2011, five Happy Days co-stars; Erin Moran, Don Most, Marion Ross, Anson Williams and the estate of Tom Bosley, who died in 2010, filed a $10 million breach-of-contract lawsuit against CBS, which owns the show, claiming they had not been paid for merchandising revenues owed under their contracts.[39] The cast members claimed they had not received revenues from show-related items, including comic books, T-shirts, scrapbooks, trading cards, games, lunch boxes, dolls, toy cars, magnets, greeting cards and DVDs where their images appear on the box covers. Under their contracts, they were supposed to be paid 5% of the net proceeds of merchandising if their sole image were used, and half that amount if they were in a group. CBS said it owed the actors $8,500 and $9,000 each, most of it from slot machine revenues, but the group said they were owed millions.[40] The lawsuit was initiated after Ross was informed by a friend playing slots at a casino of a "Happy Days" machine on which players win the jackpot when five Marion Rosses are rolled.		In October 2011, a judge rejected the group's fraud claim, which meant they could not receive millions of dollars in potential damages.[41] On June 5, 2012, a judge denied a motion filed by CBS to have the case thrown out, which meant it would go to trial on July 17 if the matter was not settled by then.[42] In July 2012, the actors settled their lawsuit with CBS. Each received a payment of $65,000 and a promise by CBS to continue honoring the terms of their contracts.[43][44]		Paramount Home Entertainment and CBS DVD have released the first six seasons of Happy Days on DVD in Region 1, as of December 2, 2014.[45] For the second season, CBS features music replacements due to copyright issues, including the theme song "Rock Around the Clock". (The Complete First Season retains the original opening, as it was released before CBS was involved). Each DVD release after season 2 has contained the original music. (except for seasons 5 and 6)[46] The Sixth Season was released on December 2, 2014. As of June 2016, no more seasons have been planned for release.[47]		Seasons 1 to 4 have also been released on DVD in the UK and in regions 2 and 4.		Happy Days, itself a spin-off from Love, American Style, resulted in seven different spin-off series, including two that were animated: Laverne & Shirley, Blansky's Beauties, Mork & Mindy, Out of the Blue, Joanie Loves Chachi, The Fonz and the Happy Days Gang (animated) and Laverne & Shirley with Special Guest Star the Fonz (animated).		A series of novels based on characters and dialog of the series was written by William Johnston and published by Tempo Books in the 1970s.		There are two animated series. Both were produced by Hanna-Barbera Productions in association with Paramount Television (now known as CBS Television Distribution). The Fonz and the Happy Days Gang ran from 1980 to 1982. There are also animated spin-offs of Laverne & Shirley (Laverne & Shirley in the Army) and Mork and Mindy (centering on a young Mork and Mindy in high school). The following season, they were connected together as The Mork & Mindy/Laverne & Shirley/Fonz Hour (1982).[48]		In the late 1990s, a touring arena show called Happy Days, The Arena Spectacular toured Australia's major cities.[49] The story featured a property developer, and former girlfriend of Fonzie called Miss Frost (Rebecca Gibney) wanting to buy the diner and redevelop it. It starred Craig McLachlan as Fonzie, Max Gillies and Wendy Hughes as Mr. and Mrs. Cunningham, Doug Parkinson as Al and Jo Beth Taylor as Richie's love interest Laura. Tom Bosley presented an introduction before each performance live on stage, and pop group Human Nature played a 1950s-style rock group.		Another stage show, Happy Days: A New Musical began touring in 2008.[50][51]		There have been two reunion shows which aired on ABC: the first was The Happy Days Reunion Special, originally aired in March 1992, followed by a second special in 2005 to commemorate the program's 30th anniversary. Both were set up in interview/clip format.		
Traceroute is a 2016 Austrian/American documentary film directed by Johannes Grenzfurthner. The autobiographical documentary and road movie deals with the history, politics and impact of nerd culture. Grenzfurthner calls his film a "personal journey into the uncharted depths of nerd culture, a realm full of dangers, creatures and more or less precarious working conditions",[1] an attempt to "chase the ghosts of nerddom's past, present and future."[2] The film was co-produced by art group monochrom and Reisenbauer Film. It features music by Kasson Crooker, Hans Nieswandt, and many others.						Artist and self-declared nerd Johannes Grenzfurthner is documenting his personal road trip from the West Coast to the East Coast of the USA, to introduce the audience to places and people that shaped and inspired his art and politics. Traceroute is a reflection on Grenzfurthner's own roots of nerddom, and "On the Road style romp across the United States as he visits icons of the counterculture, the outré, and the generally questionable."[3] Grenzfurthner summarizes the concept in an interview for Boing Boing: "It is a film on biographies and obsessions and spaces of possibility – in other words something between loving embrace and merciless vivisection. Maintaining a critical meta-outlook was just as important to me as abandoning myself to unfathomable stammerings of adoration. And that all works for one simple reason: because I take a step forward, introducing myself and confessing my guilt like in Alcoholics Anonymous, only to then take off and visit the best whiskey distilleries. In my case these destinations are not whiskey makers, but people and places and symbols of a very special pop culture."[4] On Film Threat he adds: "It was important for me to take nerddom apart, not only analyzing it, but also excavating its potential for greatness."[5]		The film incorporates art and illustrations by James Brothwell, Bonni Rambatan, Michael Marrak, Karin Frank, Ben Lawson, Michael Zeltner, Josh Ellingson and eSeL in a cinematic collage[6] that draws inspiration from 1990s fanzine and punk aesthetics, BBS culture, ANSI art and fantastic art.[7] UK Film Review's Hannah Sayer summarizes the artistic style of the film in her review: "There is a real sense that this is a collaborative exploration of creativity: of the old and the new, the past and the present, and the traditional and the digital. The use of photography and drawings interspersed between the interviews with various people associated with nerd culture shows an artistic approach to the material and these images act as reflective snapshots of moments in time, reinforcing the importance of looking back to the past as well as looking forward to the future of the digital age."[8]		Traceroute features interviews with Matt Winston, Sandy Stone, Bruce Sterling, Jason Scott, Christina Agapakis, Trevor Paglen, Ryan Finnigan, Kit Stubbs, V. Vale, Sean Bonner, Allison Cameron, Josh Ellingson, Maggie Mayhem, Paolo Pedercini, Steve Tolin, Dan Wilcox, Jon Lebkowsky, Jan "Varka" Mulders (of Bad Dragon), Adam Flynn, Abie Hadjitarkhani, Kelly Poots, and some special guest appearances (e.g. NPS spokesperson Vickie Carson, hacker Nick Farr and cultural curator Scott Beale).		Metronaut writes: "Traceroute offers deep insights into the world of nerddom. A woman who creates cheese from bacteria on sweaty feet. A man who wants to construct a giant network of independent Geiger counters. Or a sex worker who identifies as a nerd. The multitude of topics and people shows how being a nerd is not restricted to programming and computers."[9]		Grenzfurthner performed a stand-up show called Schicksalsjahre eines Nerds at Vienna's Rabenhof Theater in 2014. Parts of this show form the basis of Traceroute.[10] The basic script, although many scenes and interviews were improvised, was written by Johannes Grenzfurthner. The film's main language is English, but features archival footage in German.		Principal photography commenced on March 5, 2015 and ended March 27, 2015.[11] The film can be considered microfilmmaking and guerrilla filmmaking.		Daniel Hasibar and Christian Staudacher created the sound design and audioscape underlying Grenzfurthner's narration.		The film features music by Peter Barnett, Kasson Crooker, Damien Di Fede, Matthew Huffaker, Brady Leo, Vera Lynn, Kevin MacLeod, Hans Nieswandt, Roger Sandega, Eric Skiff, among others.		The film's World Premiere took place at NYC Independent Film Festival on April 28, 2016.[12] The European Premiere took place at DOK.fest Munich (Internationales Dokumentarfilmfestival München) on May 13, 2016.[13] Various film festivals and conventions (like Hackers on Planet Earth 2016, Gen Con 2016, the European Media Arts Festival 2016[14] in Osnabrück, the Dutch Design Week 2016, the NRW-Forum 2016, Print Screen Festival 2016[15] in Tel Aviv, FrackFest 2016, Norwich Radical Film Festival, the Chaos Communication Congress 2016, Guangzhou International Documentary Film Festival 2016) have screened the film.[16]		The film had a theatrical release in Austria in 2016.		Traceroute was digitally released on March 21, 2017.[17][18]		Reviews of the film have been positive. Film Threat's Bradley Gibson gives Traceroute 10/10 points and writes: "Traceroute is the most fun I've ever had watching a documentary. If you're a nerd (and you are) this is the road trip you've always wanted to take with your smartest, geekiest friend. You're not going to want to come home. It's Cosmos. It's DragonCon on wheels. It's your favorite sex fantasy. It's alcohol soaked nonviolent subversive protest mobile and WiFi linked. It's On The Road updated with tech, science, pseudoscience, sex, and fandom."[19] Mental Floss' Chris Higgins writes about Traceroute: "As many nerds have noticed, there's a glut of nerd-positive documentaries out there, but they tend to be either too self-serious, or too focused on the trappings of fandom to actually say much. Traceroute manages to be a real film, with humor and true insight (sometimes called out for us—and delightfully nullified—with a blinking 'INSIGHT' faux-HTML tag onscreen), primarily because it focuses on Grenzfurthner's personal journey, and he doesn't take himself too seriously. Let's put that another way: The director uses himself and a handful of subjects to create his story, and that specificity—coupled with his playfulness—makes it work. At one point he licks the chrome head of a Terminator prop. Then he licks a zombie head prop. Then he licks the propmaster himself. It's delightful."[20] Pop culture magazine Boing Boing calls Traceroute "a brilliantly careening biography of a highly enigmatic species. [...] Traceroute is radical individual empiricism, a narrative biographical puzzle and an experimental projection matrix. Despite continual stimulus satiation, it is wonderful fun: the film tickles the synapses with a perfectly mixed cocktail of collectively shared context and quirkiness."[21] Patrick Lichty of net culture magazine Furtherfield calls it "magical (...) After watching Traceroute, I was left with a real exhilaration and a deeply reflective feeling at once. (...) What Traceroute reveals is the tradition of alterity just beneath the surface of Western culture, and that it has a powerful effect on our mass consciousness, whether it is in plain sight or not."[22] Felix Knoke of Engadget Germany writes: "For me it is the best nerd documentary I've seen so far. Simply for the fact that it doesn't take itself as deadly serious as the other ones. (...) Traceroute deserves high praise because it represents an old-school definition of nerd culture, one that is never compatible with fintech, unicorn and iGod."[23] Pop culture critic Thomas Kaestle comments: "Traceroute is game, challenge, encyclopedia, and sentimental journey all in one. As a documentary it is most skillfully composed. And as a narration it is highly compatible. This film will, in passing, sweep proclaimed nerds off their feet. And it will touch the hearts of those who still need some explanation."[24] Diamond in the Rough Films praises Grenzfurthner's hosting and storytelling: "This doc works in some very unconventional ways, not the least of which is our plucky protagonist. Grenzfurthner is an absolute charmer as our host and his narration is note-perfect (the accent is so cool in that Werner Herzog kind of way). It's really rare in a documentary about a not-that-famous person where you become almost instantly won-over and invested in their personal journey."[25] Blogger and sci-fiction author Cory Doctorow states about Traceroute that "Johannes is a brilliant lunatic of surpassing and delightful weirdness."[26] MicroFilmmaker Magazine writes: "The different people Grenzfurthner chatted with were genuinely interesting and the organic way he moved from place to place was intriguing. [...] You'll find that you've learned an awful lot about technology, nerdiness, and America."[27] Richard Propes (The Independent Critic) writes: "It's challenging. It's thought-provoking. It's remarkably honest. It's well researched. [...] If you're expecting nothing more than your usual nerd doc with its cosplay cuteness and asocial gamers, you're going to be not just disappointed but probably traumatized. Grenzfurthner is clearly full-on willing to challenge culture, stereotypes, accepted thoughts and just about everything else. There's a healthy dose of sexuality in Traceroute, which one might expect, yet Grenzfurthner also immerses the film in politics, activism and social shifts. [...] Refreshingly devoid of the pretentiousness so often found amongst truly intellectual films, Traceroute is simultaneously a pretty wonderful personal journey and an immensely satisfying cinematic experience."[28]		The film won Best Documentary Feature at the 2016 Phuture Con Festival (Denver, USA/Sapporo, Japan),[29] won the Award of Merit for Documentary Feature at the 2016 Accolade Global Film Competition,[30] won the award for Best Documentary Feature Film at the 2016 Subversive Film Awards in Los Angeles,[31] won the Award of Merit at The Indie Fest Awards in several categories (Documentary Feature, History / Biographical, Tourism / Travel, Contemporary Issues / Awareness Raising)[32] and won a Honorable Mention at The Indie Gathering International Film Festival 2016.[33] It was nominated for the 2016 Austrian Documentary Award (ADA).[34] The hackerspace Voidwarranties in Antwerp, Belgium awarded Traceroute the title of "Nerd Movie Of The Year 2016".[35] Traceroute is a competition finalist at the 2016 Filmmatic Filmmaker Awards.[36] Diamond In The Rough Film Festival 2016 in Cupertino awarded Traceroute Best Feature Documentary,[37] Celludroid Film Festival 2016 in Cape Town awarded the film Best Documentary[38] and FrackFest 2016 in Oklahoma City gave Traceroute the Best of the Fest award and the Best Doc of the Fest Award.[39] The film is also semi-finalist for the Golden Kapok Award at Guangzhou International Documentary Film Festival 2016.[40]		
Alfred Matthew "Weird Al" Yankovic (/ˈjæŋkəvɪk/ YANG-kə-vik; born October 23, 1959)[1] is an American singer, songwriter, parodist, record producer, satirist, actor, voice actor, music video director, film producer, and author. He is known for his humorous songs that make light of popular culture and often parody specific songs by contemporary musical acts, original songs that are style pastiches of the work of other acts, and polka medleys of several popular songs, featuring his favored instrument, the accordion.		Since his first-aired comedy song in 1976, he has sold more than 12 million albums (as of 2007[update]),[2] recorded more than 150 parody and original songs,[3][4][5] and has performed more than 1,000 live shows.[6] His works have earned him four Grammy Awards and a further 11 nominations, four gold records, and six platinum records in the United States. Weird Al's first top ten Billboard album (Straight Outta Lynwood) and single ("White & Nerdy") were both released in 2006, nearly three decades into his career. His latest album, Mandatory Fun (2014), became his first number-one album during its debut week.		Weird Al's success comes in part from his effective use of music video to further parody popular culture, the song's original artist, and the original music videos themselves, scene-for-scene in some cases. He directed later videos himself and went on to direct for other artists including Ben Folds, Hanson, The Black Crowes, and The Presidents of the United States of America. With the decline of music television and the onset of social media, Weird Al used YouTube and other video sites to publish his videos; this strategy proved integral helping to boost sales of his later albums including Mandatory Fun. Weird Al has stated that he may forgo traditional albums in favor of timely releases of singles and EPs following on this success.		In addition to recording his albums, Weird Al wrote and starred in the film UHF (1989) and The Weird Al Show (1997). He has also made guest appearances and voice acting roles on many television shows and video web content, in addition to starring in Al TV specials on MTV.[1] He has also written two children's books, When I Grow Up and My New Teacher and Me!						Yankovic was born in Downey, California and raised in Lynwood, California. He is the only child of Mary Elizabeth (Vivalda) and Nick Yankovic.[8] His father was born in Kansas City, Kansas, of Yugoslavian[8][9] descent, and began living in California after serving during World War II;[10][11] he believed "the key to success" was "doing for a living whatever makes you happy" and often reminded his son of this philosophy.[10] Nick married Mary in 1949. Mary, who was of Italian and English descent, had come to California from Kentucky, and gave birth to Alfred ten years later.[10]		Al's first accordion lesson, which sparked his career in music, was on the day before his sixth birthday. A door-to-door salesman traveling through Lynwood offered the Yankovic parents a choice of accordion or guitar lessons at a local music school. Yankovic claims the reason his parents chose accordion over guitar was "they figured there should be at least one more accordion-playing Yankovic in the world", referring to Frankie Yankovic,[12] to whom he is not related.[9] Additionally, Yankovic said that "[his] parents chose the accordion because they were convinced it would revolutionize rock."[9] He continued lessons at the school for three years before continuing to learn on his own.[8] Yankovic's early accordion role models included Frankie Yankovic and Myron Floren.[citation needed]		In the 1970s, Yankovic was a big fan of Elton John and claims John's Goodbye Yellow Brick Road album "was partly how I learned to play rock 'n roll on the accordion."[10] As for his influences in comedic and parody music, Yankovic lists artists including Tom Lehrer, Stan Freberg, Spike Jones, Allan Sherman, Shel Silverstein and Frank Zappa "and all the other wonderfully sick and twisted artists that he was exposed to through the Dr. Demento Radio Show."[8][13] Other sources of inspiration for his comedy come from Mad magazine,[10] Monty Python,[14] and the Zucker, Abrahams and Zucker parody movies.[15]		Yankovic began kindergarten a year earlier than most children, and he skipped second grade. "My classmates seemed to think I was some kind of rocket scientist so I was labeled a nerd early on," he recalls.[10] As his unusual schooling left him two years younger than most of his classmates, Yankovic was not interested in sports or social events at school. He attended Lynwood High School. Yankovic was active in his school's extracurricular programs, including the National Forensic League sanctioned speech events, a play based upon Rebel Without a Cause, the yearbook (for which he wrote most of the captions), and the Volcano Worshippers club, "which did absolutely nothing. We started the club just to get an extra picture of ourselves in the yearbook."[10] Weird Al graduated in 1975[16] and was valedictorian of his senior class.[10]		Yankovic attended California Polytechnic State University in San Luis Obispo where he earned a bachelor's degree in architecture.[9]		Yankovic received his first exposure via Southern California and syndicated comedy radio personality Dr. Demento's radio show, saying "If there hadn't been a Dr. Demento, I'd probably have a real job now."[17] In 1976, Dr. Demento spoke at Yankovic's school where the then-16-year-old Yankovic gave him a homemade tape of original and parody songs performed on the accordion in Yankovic's bedroom into a "cheesy little tape recorder". The tape's first song, "Belvedere Cruisin'" - about his family's Plymouth Belvedere - was played on Demento's comedy radio show, launching Yankovic's career. Demento said, "'Belvedere Cruising' might not have been the very best song I ever heard, but it had some clever lines [...] I put the tape on the air immediately."[10][18] Yankovic also played at local coffeehouses, saying:		It was sort of like amateur music night, and a lot of people were like wannabe Dan Fogelbergs. They'd get up on stage with their acoustic guitar and do these lovely ballads. And I would get up with my accordion and play the theme from 2001. And people were kind of shocked that I would be disrupting their mellow Thursday night folk fest.[19]		During Yankovic's sophomore year as an architecture student at Cal Poly, he became a disc jockey at KCPR the university's radio station. Yankovic said he had originally been nicknamed Weird Al by fellow students and "took it on professionally" as his persona for the station.[10] In 1978, he released his first recording (as Alfred Yankovic), "Take Me Down", on the LP, Slo Grown, as a benefit for the Economic Opportunity Commission of San Luis Obispo County. The song mocked famous nearby landmarks such as Bubblegum Alley and the fountain toilets at the Madonna Inn.		In mid-1979, shortly before his senior year, "My Sharona" by The Knack was on the charts and Yankovic took his accordion into the restroom across the hall from the radio station to take advantage of the echo chamber acoustics and recorded a parody titled "My Bologna".[20] He sent it to Dr. Demento, who played it to good response from listeners. Yankovic met The Knack after a show at his college and introduced himself as the author of "My Bologna". The Knack's lead singer, Doug Fieger, said he liked the song and suggested that Capitol Records vice president Rupert Perry release it as a single.[10] "My Bologna" was released as a single with "School Cafeteria" as its B-side, and the label gave Yankovic a six-month recording contract. Yankovic, who was "only getting average grades" in his architecture degree, began to realize that he might make a career of comedic music.[10]		On September 14, 1980, Yankovic was a guest on the Dr. Demento Show, where he was to record a new parody live. The song was called "Another One Rides the Bus", a parody of Queen's hit, "Another One Bites the Dust". While practicing the song outside the sound booth, he met Jon "Bermuda" Schwartz, who told him he was a drummer and agreed to bang on Yankovic's accordion case to help Yankovic keep a steady beat during the song. They rehearsed the song just a few times before the show began.[10] "Another One Rides the Bus" became so popular that Yankovic's first television appearance was a performance of the song on The Tomorrow Show (April 21, 1981) with Tom Snyder.[21] On the show, Yankovic played his accordion, and again, Schwartz banged on the accordion case and provided comical sound effects. Yankovic's record label, TK Records, went bankrupt about two weeks after the single was released, so Yankovic received no royalties from its initial release.[20]		1981 brought Yankovic on tour for the first time as part of Dr. Demento's stage show. His stage act in a Phoenix, Arizona, nightclub caught the eye of manager Jay Levey, who was "blown away".[10] Levey asked Yankovic if he had considered creating a full band and doing his music as a career. Yankovic admitted that he had, so Levey held auditions. Steve Jay became Yankovic's bass player, and Jay's friend Jim West played guitar. Schwartz continued on drums. Yankovic's first show with his new band was on March 31, 1982.[6] Several days later, Yankovic and his band were the opening act for Missing Persons.		Yankovic recorded "I Love Rocky Road", (a parody of "I Love Rock 'n' Roll" as recorded by Joan Jett and The Blackhearts) which was produced by Rick Derringer, in 1982. The song was a hit on Top 40 radio, leading to Yankovic's signing with Scotti Brothers Records. In 1983, Yankovic's first self-titled album was released on Scotti Bros. The song "Ricky" was released as a single and the music video received exposure on the still-young MTV. Yankovic released his second album "Weird Al" Yankovic in 3-D in 1984. The first single "Eat It", a parody of the Michael Jackson song "Beat It", became popular, thanks in part to the music video, a shot-for-shot parody of Jackson's "Beat It" music video, and what Yankovic described as his "uncanny resemblance" to Jackson. Peaking at No. 12 on the Billboard Hot 100 on April 14, 1984,[22] "Eat It" remained Yankovic's highest-charting single until "White & Nerdy" placed at No. 9 in October 2006.[23]		In 1985, Yankovic co-wrote and starred in a mockumentary of his own life titled The Compleat Al, which intertwined the facts of his life up to that point with fiction. The movie also featured some clips from Yankovic's trip to Japan and some clips from the Al TV specials. The Compleat Al was co-directed by Jay Levey, who would direct UHF four years later. Also released around the same time as The Compleat Al was The Authorized Al, a biographical book based on the film. The book, resembling a scrapbook, included real and fictional humorous photographs and documents.		Yankovic and his band toured as the opening act for The Monkees in mid-1987 for their second reunion tour of North America. Yankovic claims to have enjoyed touring with The Monkees, even though "the promoter gypped us out of a bunch of money."[24]		In 1988 Yankovic was the narrator on the Wendy Carlos recording of Sergei Prokofiev's Peter and the Wolf.[25] The album also included a sequel to Camille Saint-Saëns's composition The Carnival of the Animals titled "The Carnival of the Animals Part II", with Yankovic providing humorous poems for each of the featured creatures in the style of Ogden Nash, who had written humorous poems for the original.		Rubén Valtierra joined the band on keyboards in 1991, allowing Yankovic to concentrate more on singing and increasing his use of the stage space during concerts.		A factual biographical booklet of Yankovic's life, written by Dr. Demento, was released with the 1994 box set compilation Permanent Record: Al in the Box.[10] The Dr. Demento Society, which issues yearly Christmas re-releases of material from Dr. Demento's Basement Tapes, often includes unreleased tracks from Yankovic's vaults, such as "Pacman", "It's Still Billy Joel To Me" or the live version of "School Cafeteria".		On January 24, 1998, Yankovic had LASIK eye surgery to correct his extreme myopia.[26] When Running with Scissors debuted in 1999, he unveiled a radically changed look. In addition to shedding his glasses, he had shaved off his mustache and grown out his hair. He had previously shaved his mustache in 1983 for the video of "Ricky" to resemble Desi Arnaz, and 1996 for the "Amish Paradise" video. Yankovic reasoned, "If Madonna's allowed to reinvent herself every 15 minutes, I figure I should be good for a change at least once every 20 years."[27] He parodied the reaction to this "new look" in a commercial for his nonexistent MTV Unplugged special. The commercial featured Yankovic in the short-haired wig from the music video for Hanson's "River", claiming his new look was an attempt to "get back to the core of what I'm all about", that being "the music".[28]		Yankovic has also started to explore digital distribution of his songs. On October 7, 2008, Yankovic released to the iTunes Store "Whatever You Like", a parody of the T.I. song of the same title, which Yankovic said he had come up with two weeks before. Yankovic said that the benefit of digital distribution is that "I don't have to wait around while my songs get old and dated—I can get them out on the Internet almost immediately."[29] In 2009, Yankovic released four more songs: "Craigslist" on June 16, "Skipper Dan" on July 14, "CNR" on August 4, and "Ringtone" on August 25. These five digitally released songs were packaged as a digital EP titled Internet Leaks, with "Whatever You Like" retroactively included in the set.[30]		In 2011, Yankovic completed his thirteenth studio album, titled Alpocalypse, which was released on June 21, 2011.[31] The album contains the five songs from the previous Internet Leaks digital download release, a polka medley called "Polka Face", a song called "TMZ" for which Bill Plympton created an animated music video, and five other new songs.[32][33]		Yankovic had reported an interest in parodying Lady Gaga's material,[34] and on April 20 announced that he had written and recorded a parody of "Born This Way" titled "Perform This Way", to be the lead single for his new album. However, upon first submitting it to Lady Gaga's manager for approval (which Yankovic does as a courtesy), he was not given permission to release it commercially. As he had previously done under similar circumstances (with his parody of James Blunt's "You're Beautiful", which was titled "You're Pitiful"), Yankovic then released the song for free on the internet. Soon afterwards, Gaga's manager admitted that he had denied the parody of his own accord without forwarding the song to his client, and upon seeing it online, Lady Gaga granted permission for the parody. Yankovic has stated that all of his proceeds from the parody and its music video will be donated to the Human Rights Campaign, to support the human rights themes of the original song.[35][36] Yankovic was also a judge for the 10th annual Independent Music Awards to support independent artists' careers.[37]		Yankovic stated in September 2013 that he was working on a new album, but gave no details.[38] In 2014, he used social media websites to hint at a July 15 release of this new album, as noted by Rolling Stone.[39] The album artwork and title, Mandatory Fun, were affirmed by his publisher.[40] Yankovic said in an interview promoting the album that, with the end of his recording contract, it is likely his last traditional album, in the sense of recording and releasing that many songs at a time; he said he will likely switch to releasing singles and EPs over the Internet, a method which offers more immediate release opportunities as Yankovic considers his parodies in particular as something that can become dated by the time of release.[41] Mandatory Fun was released to strong critical praise and was the No. 1 debut album on the Billboard charts the week of its release, buoyed by Yankovic's approach for releasing eight music videos over eight continuous days that drew viral attention to the album as described below.[42] It became Yankovic's first No. 1 album in his career. Additionally, the song "Word Crimes" (a parody of Robin Thicke's "Blurred Lines") reached No. 39 on the Top 100 singles for the same week; this is Yankovic's fourth Top 40 single, and makes him only the third artist, after Michael Jackson and Madonna, to have a Top 40 single in each decade since the 1980s.[43]		On November 24, 2017, Squeeze Box, a 15-album box set containing all of Yankovic's past albums as well as one new one, will be released. The set will be contained within a mock-up of Yankovic's very own accordion.[44]		On June 22, 2017, it was announced that Al, alongside such illustrious names as Minnie Mouse, Lin-Manuel Miranda, and Jeff Goldblum, would be receiving his own star on the Hollywood Walk of Fame in 2018.[45]		Yankovic changed his diet to become a vegan in 1992 after a former girlfriend gave him the book Diet for a New America and he felt "it made ... a very compelling argument for a strict vegetarian diet".[46][47] When asked how he can "rationalize" performing at events such as the Great American Rib Cook-Off when he is a vegan, he replied, "The same way I can rationalize playing at a college even though I'm not a student anymore."[48] In a 2011 interview with news website OnMilwaukee, Yankovic clarified his stance on his diet, saying, "I am still a vegetarian, and I try to be a vegan, but I occasionally cheat. If there's a cheese pizza on the band bus, I might sneak a piece."[49]		Yankovic married Suzanne Krajewski in 2001 after being introduced by their mutual friend Bill Mumy.[47] Their daughter, Nina, was born in 2003.[50] Yankovic identifies as Christian and has stated that a couple from his church appeared on the cover of Poodle Hat.[51][52] Yankovic's religious background is reflected in his abstinence from alcohol, tobacco, drugs, and profanity.[53]		On April 9, 2004, Yankovic's parents were found dead in their Fallbrook, California home, the victims of accidental carbon monoxide poisoning from their fireplace.[11][50] Several hours after his wife notified him of his parents' death, Yankovic went on with his concert in Appleton, Wisconsin,[54][55] saying that "since my music had helped many of my fans through tough times, maybe it would work for me as well"[56] and that it would "at least ... give me a break from sobbing all the time."[57] In a 2014 interview, Yankovic called his parents' death "the worst thing that ever happened to me." He added, "I knew intellectually, that at some point, probably, I'd have to, you know, live through the death of my parents, but I never thought it would be at the same time, and so abruptly."[58]		Yankovic is well known for creating parodies of contemporary radio hits, typically which make up about half of his studio releases. Unlike other parody artists such as Allan Sherman, Yankovic and his band strive to keep the backing music in his parodies the same as the original, transcribing the original song by ear and re-recording the song for the parody.[59] In some cases, in requesting the original band to allow for his parody, the band will offer to help out with the recreation: Dire Straits members Mark Knopfler and Guy Fletcher perform on "Money for Nothing/Beverly Hillbillies*", Yankovic's parody of Dire Straits' "Money for Nothing", while Imagine Dragons provided Yankovic with advice on how to recreate some of the electronic sounds they used for "Radioactive" in Yankovic's parody "Inactive".[60] Yankovic's career in novelty and comedy music has outlasted many of his "mainstream" parody targets, such as Toni Basil, MC Hammer, and Men Without Hats.[61][62] Yankovic's continued success (including the top 10 single "White & Nerdy" and album Straight Outta Lynwood in 2006) has enabled him to escape the one-hit wonder stigma often associated with novelty music.[63]		While Yankovic's song parodies (such as "Eat It") have resulted in success on the Billboard charts (see List of singles by "Weird Al" Yankovic), he has also recorded numerous original humorous songs ("You Don't Love Me Anymore" and "One More Minute").[8] Many of these songs are style pastiches of specific bands with allusions to specific songs. For example, "First World Problems" from Mandatory Fun is a style take on the Pixies, with the opening stanza reminiscent of the Pixies' "Debaser".[64] Other style parodies includes those of Rage Against the Machine with "I'll Sue Ya" (which features many aspects of the hit song "Killing in the Name"), Devo with "Dare to Be Stupid", Talking Heads with "Dog Eat Dog", Frank Zappa with "Genius in France", Nine Inch Nails with "Germs", and Queen with "Ringtone".[65] Some songs are pastiches of an overall genre of music, rather than a specific band (for example, country music with "Good Enough For Now", charity records with "Don't Download This Song") and college fight songs with "Sports Song". Yankovic stated that he does not have any unreleased original songs, instead coming up and committing to the song ideas he arrives at for his albums and other releases.[66]		Most of Yankovic's studio albums include a polka medley of about a dozen contemporary songs at the time of the album, with the choruses or memorable lines of various songs juxtaposed for humorous effect. Yankovic has been known to say that converting these songs to polka was "...the way God intended". Because the polkas have become a staple of Yankovic's albums, he has said he tries to include one on each album because "fans would be rioting in the streets, I think, if I didn't do a polka medley."[67]		Yankovic has contributed original songs to several films ("This Is the Life" from Johnny Dangerously; "Polkamon" from the movie Pokémon: The Movie 2000, and a parody of the James Bond title sequence in Spy Hard), in addition to his own film, UHF. Other songs of his have appeared in films or television series as well, such as "Dare to Be Stupid" in The Transformers: The Movie.		Although many of Yankovic's songs are parodies of contemporary radio hits, it is rare that the song's primary topic lampoons the original artist as a person, or the song itself. Most Yankovic songs consist of the original song's music, with a separate, unrelated set of amusing lyrics. Yankovic's humor normally lies more in creating unexpected incongruity between an artist's image and the topic of the song, contrasting the style of the song with its content (such as the songs "Amish Paradise", "White & Nerdy", and "You're Pitiful"), or in pointing out trends or works which have become pop culture clichés (such as "eBay" and "Don't Download This Song"). Yankovic's parodies are often satirical of popular culture, including television (see The TV Album), movies ("The Saga Begins"), and food (see The Food Album). Yankovic claims he has no intention of writing "serious" music. In his reasoning, "There's enough people that do unfunny music. I'll leave the serious stuff to Paris Hilton and Kevin Federline."[68]		Yankovic considered that his first true satirical song was "Smells Like Nirvana", which references unintelligible lyrics in Nirvana's "Smells Like Teen Spirit".[69] Other satirical songs include "Achy Breaky Song", which refers to the song "Achy Breaky Heart", "(This Song's Just) Six Words Long", which refers to the repetitious lyrics in "Got My Mind Set on You", and "Perform This Way", set to Lady Gaga's "Born This Way" that drew inspiration from Lady Gaga's outlandish but confident attitude.		Yankovic is the sole writer for all his songs and, for "legal and personal reasons", does not accept parody submissions or ideas from fans.[8] There exists, however, one exception to this rule: Madonna was reportedly talking with a friend and happened to wonder aloud when Yankovic was going to turn her "Like a Virgin" into "Like a Surgeon". Madonna's friend was a mutual friend of Yankovic's manager, Jay Levey, and eventually Yankovic himself heard the story from Levey.[10]		One of Yankovic's recurring jokes involves the number 27. It is mentioned in the lyrics of several songs, and seen on the covers for Running With Scissors, Poodle Hat[70] and Straight Outta Lynwood. He had originally just pulled the number 27 as a random figure to use in filling out lyrics, but as his fans started to notice the reuse of the number after the first few times, he began to purposely drop references to 27 within his lyrics, videos, and album covers. He explains that "It's just a number I started using that people started attaching a lot of importance to."[71] Other recurring jokes revolve around the names Bob (the Al TV interviews often mention the name,[72] David Bowe's character in UHF is named Bob, and a song called "Bob", done in the style of Bob Dylan, is featured on Poodle Hat), Frank (e.g. "Frank's 2000" TV"), and the surname "Finkelstein" (e.g. the music video for "I Lost on Jeopardy", or Fran Drescher's character, Pamela Finkelstein, in UHF). Also, a hamster called Harvey the Wonder Hamster is a recurring character in The Weird Al Show and the Al TV specials, as well as the subject of an original song on Alapalooza. Other recurring jokes include Yankovic borrowing, or being owed, $5. In a number of Al TV interviews, he often asks if he can borrow $5, being turned down every time. This motif also occurs in "Why Does This Always Happen to Me?", in which his deceased friend owes him $5. Another recurring joke is his attraction to female nostrils or nostrils in general. This also appears in numerous Al TV interviews as well as in several of his songs ("Albuquerque" and "Wanna B Ur Lovr" to name a few.) Yankovic also asks his celebrity guests if they could "shave his back for a nickel." This also appears in the song "Albuquerque". Yankovic has also put two backmasking messages into his songs. The first, in "Nature Trail to Hell", said "Satan Eats Cheez Whiz"; the second, in "I Remember Larry", said "Wow, you must have an awful lot of free time on your hands."[73]		While Yankovic's musical parodies generally do not include references to the songs or the artists of the original songs, Yankovic's music videos will sometimes parody the original song's music video in whole or in part.[74] Most notably, the video for "Smells Like Nirvana" uses an extremely similar set to Nirvana's "Smells Like Teen Spirit", including using several of the same actors. This video contended with "Smells like Teen Spirit" at the 1992 MTV Video Music Awards for Best Male Video. Other videos that draw directly from those of the original song include "Eat It", "Fat", "Money for Nothing/Beverly Hillbillies*", "Bedrock Anthem", "Headline News", "It's All About the Pentiums", "Amish Paradise", "Like a Surgeon", and "White & Nerdy". The video for "Dare to Be Stupid" is, as stated by Yankovic, a style parody in general of Devo videos.[75]		Several videos have included appearances by notable celebrities in addition to Yankovic and his band. Dr. Demento appeared in several of Yankovic's earlier videos, such as "I Love Rocky Road" and "Ricky". Actor Dick Van Patten is featured in both "Smells Like Nirvana" and "Bedrock Anthem"; Drew Carey, Emo Philips and Phil LaMarr appeared in "It's All About the Pentiums"; Keegan-Michael Key, Jordan Peele, Donny Osmond, Judy Tenuta and Seth Green appeared in "White & Nerdy"; and Ruth Buzzi and Pat Boone appeared in "Gump". The video for "I Lost on Jeopardy" includes an appearance by Greg Kihn, the artist whose song, "Jeopardy", was being parodied, along with Don Pardo and Art Fleming, Jeopardy's original announcer and host, as themselves. Florence Henderson plays an Amish seductress in "Amish Paradise".		While most videos that Yankovic creates are aired on music channels such as MTV and VH1, Yankovic worked with animation artists to create music videos for release with extended content albums. The DualDisc version of Straight Outta Lynwood features six videos set to songs from the release, including videos created by Bill Plympton and John Kricfalusi; one video, "Weasel Stomping Day" was created by the producers of the show Robot Chicken, and aired as a segment of that program. For the 2010 Alpocalypse, Yankovic produced videos for every song; four of those were previously released for each of the songs on the EP Internet Leaks, with the videos for the remaining songs released via social media sites and included in the deluxe edition of Alpocalypse. These live-action and animated videos were produced by both previous collaborators such as Plympton for "TMZ",[33] video content providers like Jib-Jab and SuperNews!, and other directors and animators.		To help promote his 2014 album Mandatory Fun in social media circles, Yankovic produced eight music videos for the album releasing them over eight consecutive days with release of the album, believing it "would make an impact because people would be talking about the album all week long".[76][77] RCA Records opted not to fund production of any of these videos, and Yankovic turned to various social media portals including Funny or Die and CollegeHumor which he had worked with in the past; these sites helped to cover the production cost of the videos with Yankovic foregoing any ad video revenue. He chose to distribute the videos to different portals to avoid burdening any single one with all of the costs and work needed to produce them. This approach proved to be successful, as the total collection of videos had acquired more than 20 million views in the first week.[78] This release strategy was considered by The Atlantic as a "web-enabled precision video delivery operation, and evidence of some serious digital distributional forethought" as it allows the videos to be seen by different sets of audiences for each site.[79] The approach was considered to be essential to promoting Mandatory Fun to reach the No. 1 position on the Billboard charts on its debut week.[42] Businessweek attributed the sales success of Mandatory Fun to the viral music video campaign.[80] ABC World News elaborated that Yankovic's success is in part due to the Internet's interest in viral and humorous videos catching up with what Yankovic has been doing for his entire career.[81] Yankovic himself was amazed with the response he got from the album and video releases, stating that "I've been doing the same thing for 30 years and all of a sudden I'm having the best week of my life"[81] and that he "kind of stumbled on my formula for the future".[78]		In October 2016, Yankovic collaborated with the Gregory Brothers to create a music video "Bad Hombres, Nasty Women" shortly after the third debate between Donald Trump and Hillary Clinton, with Yankovic singing between autotuned snippets from the candidates.[82][83]		Under the "fair use" provision of U.S. copyright law, affirmed by the United States Supreme Court in the 1994 case Campbell v. Acuff-Rose Music, Inc., artists such as Yankovic do not need permission to record a parody.[84] However, as a personal rule and as a means of maintaining good relationships, Yankovic has always sought permission from the original artist before commercially releasing a parody.[84] These communications are typically handled by his manager Jay Levey, but at times Yankovic has asked the artist directly, such as flying to Denver, Colorado, to attend an Iggy Azalea concert and speak to her personally about parodying her song "Fancy".[85] He claims that only about two to three percent of the artists he approaches for permission deny his requests,[86] while many of the rest who approve consider Yankovic's parodies to be a badge of honor and rite of passage in the music industry.		Michael Jackson was a big fan of Yankovic, and Yankovic claimed Jackson "had always been very supportive" of his work.[86] Jackson twice allowed him to parody his songs ("Beat It" and "Bad" became "Eat It" and "Fat", respectively). When Jackson granted Yankovic permission to do "Fat", Jackson allowed him to use the same set built for his own "Badder" video from the Moonwalker film.[87] Yankovic said that Jackson's support helped to gain approval from other artists he wanted to parody.[87] Though Jackson allowed "Eat It" and "Fat", he requested that Yankovic not record a parody of "Black or White", titled "Snack All Night", because he felt the message was too important. This refusal, coming shortly after the commercial failure of Yankovic's movie UHF in theaters, had initially set Yankovic back; he later recognized this as a critical time as, while searching for new parodies, he came across Nirvana, leading to a revitalization of his career with "Smells Like Nirvana".[86] Yankovic has performed a concert-only parody "Snack All Night" in some of his live shows.[88] Yankovic was one of several celebrities who appeared in the 1989 music video for Jackson's song "Liberian Girl".[89]		Dave Grohl of Nirvana said that the band felt they had "made it" after Yankovic recorded "Smells Like Nirvana", a parody of the grunge band's smash hit, "Smells Like Teen Spirit".[8] On his Behind the Music special, Yankovic stated that when he called Nirvana frontman Kurt Cobain to ask if he could parody the song, Cobain gave him permission, then paused and asked, "Um... it's not gonna be about food, is it?" Yankovic responded with, "No, it'll be about how no one can understand your lyrics." According to members of Nirvana interviewed for Behind the Music, when they saw the video of the song, they laughed hysterically. Additionally, Cobain described Yankovic as "a musical genius".[90]		Mark Knopfler approved Yankovic's parody of the Dire Straits song "Money for Nothing" for use in the film UHF on the provision that Knopfler himself be allowed to play lead guitar on the parody which was later titled "Money for Nothing/Beverly Hillbillies*".[91] Yankovic commented on the legal complications of the parody in the DVD audio commentary for UHF, explaining "We had to name that song 'Money for Nothing 'slash' Beverly Hillbillies 'asterisk' because the lawyers told us that had to be the name. Those wacky lawyers! What ya gonna do?"[92] The Permanent Record: Al in the Box booklet referred to the song's "compound fracture of a title."[10] When a fan asked about the song's title, Yankovic shared his feelings on the title, replying "That incredibly stupid name is what the lawyers insisted that the parody be listed as. I'm not sure why, and I've obviously never been very happy about it."[93]		The Presidents of the United States of America were so pleased with "Gump", Yankovic's parody of their song "Lump", that they ended the song with his last line instead of their own ("And that's all I have to say about that") on the live recording of "Lump" featured on the compilation album Pure Frosting. In 2008, Yankovic directed the music video for their song "Mixed Up S.O.B."		Don McLean was reportedly pleased with "The Saga Begins", a parody of "American Pie", and told Yankovic that the parody's lyrics sometimes enter his mind during live performances.[94] His parody not only replicates the music from the original Don McLean song, but it replicates the multi-layered rhyming structure in the verses and chorus. Additionally, George Lucas loved the song and a Lucasfilm representative told Yankovic, "You should have seen the smile on his face."[95]		Chamillionaire was also very pleased, even putting Yankovic's parody "White & Nerdy" (a parody of "Ridin'") on his official MySpace page before it was on Yankovic's own page. Chamillionaire stated in an interview, "He's actually rapping pretty good on it, it's crazy [...] I didn't know he could rap like that. It's really an honor when he does that. [...] Weird Al is not gonna do a parody of your song if you're not doing it big."[96] In September 2007, Chamillionaire credited "White & Nerdy" for his recent Grammy win, stating "That parody was the reason I won the Grammy, because it made the record so big it was undeniable. It was so big overseas that people were telling me they had heard my version of Weird Al's song."[97]		In 2011, Yankovic was initially denied permission to parody Lady Gaga's "Born This Way" for his song "Perform This Way" for release on a new album, but through his release of the song on YouTube and subsequent spread via Twitter, Lady Gaga and her staff asserted that her manager had made the decision without her input, and Gaga herself gave Yankovic permission to proceed with the parody's release.[86][98] Gaga considered herself "a huge Weird Al fan",[99] and she stated that the parody was a "rite of passage" for her musical career and considered the song "very empowering".[100]		Yankovic states that his style parodies have also been met with positive remarks by the original artist. He noted that his friends and fellow musicians Ben Folds and Taylor Hanson helped to support their respective style parodies "Why Does This Always Happen To Me?" and "If That Isn't Love". He also noted positive reactions he got through friends his band members have, such as from Frank Black of The Pixies for "First World Problems" and Southern Culture on the Skids for "Lame Claim to Fame", and a similar praise when he encountered Graham Nash of Crosby, Stills, and Nash on the street, and was able to play his recently completed "Mission Statement" for him.[66]		One of Yankovic's most controversial parodies was 1996's "Amish Paradise", based on "Gangsta's Paradise" by hip-hop artist Coolio, which, in turn, was based on "Pastime Paradise" by Stevie Wonder. Reportedly, Coolio's label gave Yankovic the impression that Coolio had granted permission to record the parody, but Coolio maintains that he never did. While Coolio claimed he was upset, legal action never materialized, and Coolio accepted royalty payments for the song. After this controversy, Yankovic has always made sure to speak directly with the artist of every song he parodied. At the XM Satellite Radio booth at the 2006 Consumer Electronics Show Yankovic and Coolio made peace. On his website, Yankovic wrote of this event, "I don't remember what we said to each other exactly, but it was all very friendly. I doubt I'll be invited to Coolio's next birthday party, but at least I can stop wearing that bulletproof vest to the mall."[101] In an interview in 2014, Coolio extended his apology for refusing his permission, stating that at the time "I was being cocky and shit and being stupid and I was wrong and I should've embraced that shit and went with it", and that he considered Yankovic's parody "actually funny as shit".[102]		In 2000, Red Hot Chili Peppers bassist Flea told Behind the Music that he was unimpressed and disappointed by Yankovic's 1993 song "Bedrock Anthem", which parodied two of the band's songs. He was quoted as stating "I didn't think it was very good. I enjoy Weird Al's things, but I found it unimaginative."[103][104]		In 2003, Yankovic was denied permission to make a video for "Couch Potato", his parody of Eminem's "Lose Yourself". Yankovic believes that Eminem thought that the video would be harmful to his image.[105]		For the Poodle Hat Al TV special, Yankovic raised the question of artistic expression in a fake interview with Eminem. As Yankovic has always done for his Al TV specials, he edited the footage of a previous Eminem interview and inserted himself asking questions for comic effect.[106]		On numerous occasions, Prince refused Yankovic permission to record parodies of his songs. Yankovic has stated in interviews that he "approached him every few years [to] see if he's lightened up."[107] Yankovic related one story where, before the American Music Awards where he and Prince were assigned to sit in the same row, he got a telegram from Prince's management company, demanding he not make eye contact with the artist.[86] Among parodies that Yankovic had ideas for included one based on "Let's Go Crazy" about The Beverly Hillbillies, "1999" as an infomercial with a call-in number ending in -1999, and parodies of "Kiss" and "When Doves Cry".[87]		Led Zeppelin guitarist Jimmy Page is a self-proclaimed Yankovic fan, but when Yankovic wished to create a polka medley of Led Zeppelin songs, Page refused.[108] Yankovic was, however, allowed the opportunity to re-record a sample of "Black Dog" for a segment of "Trapped in the Drive-Thru".[109]		Paul McCartney, also a Yankovic fan, refused Yankovic permission to record a parody of Wings' "Live and Let Die", titled "Chicken Pot Pie", because, according to Yankovic, McCartney is "a strict vegetarian and he didn't want a parody that condoned the consumption of animal flesh".[84] Though McCartney suggested possibly changing the parody to "Tofu Pot Pie", Yankovic found this wouldn't fit around the chorus of the parody, based on making the sound of a chicken throughout it. While never recorded for an album, Yankovic did play parts of "Chicken Pot Pie" as part of a larger medley in several tours during the 1990s.[84]		In 2006, Yankovic gained James Blunt's permission to record a parody of "You're Beautiful". However, after Yankovic had recorded "You're Pitiful", Blunt's label, Atlantic Records, rescinded this permission, despite Blunt's personal approval of the song.[86] The parody was pulled from Yankovic's Straight Outta Lynwood because of his label's unwillingness to "go to war" with Atlantic. Yankovic released the song as a free download on his MySpace profile, as well as his official website, and plays it in concert, since it was not Blunt himself objecting to the parody.[110] Yankovic referenced the incident in his video for "White & Nerdy" when he depicts himself vandalizing Atlantic Records' Wikipedia page.		Yankovic often describes his live concert performances as "a rock and comedy multimedia extravaganza"[111] with an audience that "ranges from toddlers to geriatrics."[68] Apart from Yankovic and his band performing his classic and contemporary hits, staples of Yankovic's live performances include a medley of parodies, many costume changes between songs, and a video screen on which various clips are played during the costume changes.[111] A concert from Yankovic's 1999 tour, "Touring with Scissors", for the Running with Scissors album was released on VHS in 1999 and on DVD in 2000.[4] Titled "Weird Al" Yankovic Live!, the concert was recorded at the Marin County Civic Center in San Rafael, California, on October 2, 1999.[112] For legal reasons, video clips (apart from those for Yankovic's own music videos) could not be shown for the home release, and unreleased parodies were removed from the parody medley for the performance.[113]		In 2003, Yankovic toured overseas for the first time. Before 2003, Yankovic and his band had toured only the United States and parts of Canada.[6] Following the success of Poodle Hat in Australia, Yankovic performed eleven shows in Australia's major capital cities and regional areas in October of that year.[114] Yankovic returned to Australia and toured New Zealand for the first time in 2007 to support the Straight Outta Lynwood album. On September 8, 2007, Yankovic performed his 1,000th live show at Idaho Falls, Idaho.[6]		Yankovic has invited members of the 501st Legion on stage during performances of his Star Wars-themed songs "Yoda" and "The Saga Begins", recruiting members of local garrisons (club chapters) while on tour. In appreciation, the 501st inducted Yankovic as a "Friend of the Legion", in September 2007.[115]		He performed his first ever European mini-tour, including an appearance at the All Tomorrow's Parties music festival in Minehead, England in December 2010. Yankovic was picked to perform by the Canadian band Godspeed You! Black Emperor, who curated the festival's lineup. Yankovic played three other dates in the UK around his festival appearance before performing a single date in the Netherlands.[116]		A second concert film, "Weird Al" Yankovic Live!: The Alpocalypse Tour, aired on Comedy Central on October 1, 2011, and was released on Blu-ray and DVD three days later. The concert was filmed at Massey Hall in Toronto, Ontario, Canada, during Yankovic's tour supporting the album Alpocalypse. As before, video clips (apart from those for his own videos) and unreleased songs were edited out for legal reasons.[117]		Yankovic performed George Harrison's What Is Life at the live-recorded George Fest (Los Angeles, 2014). DVD and Blu-Ray CD combos of the concert honoring George Harrison became available in early 2016.		In 1989, Yankovic starred in a full-length feature film, co-written by himself and manager Jay Levey, and filmed in Tulsa, Oklahoma called UHF. A satire of the television and film industries, also starring Michael Richards, Fran Drescher, and Victoria Jackson, it brought floundering studio Orion their highest test scores since the movie RoboCop.[118] However, it was unsuccessful in theaters due to both poor critical reception and competition from other summer blockbusters at the time such as Indiana Jones and the Last Crusade, Lethal Weapon 2, Batman and Licence to Kill.[119] The failure of the film left Yankovic in a three-year slump, which was later broken by his inspiration to compose "Smells Like Nirvana".[120]		The film has since become a cult classic, with out-of-print copies of the VHS version selling for up to $100 on eBay until the release of the DVD in 2002.[citation needed] Yankovic occasionally shows clips from the film at his concerts (to which MGM, the film's current owner, initially objected in the form of a cease and desist letter).[121] In an apparent attempt to make it more accessible to overseas audiences, where the term UHF is used less frequently to describe TV broadcasts, the film was titled The Vidiot From UHF in Australia and parts of Europe.[122]		UHF shows the creation of Yankovic's signature food—the Twinkie Wiener Sandwich. The snack consists of an overturned Twinkie split open as a makeshift bun, a hot dog, and Easy Cheese put together and dipped in milk before eating. Yankovic has stated that he has switched to using tofu hot dogs since becoming a vegetarian, but still enjoys the occasional Twinkie Wiener Sandwich.[123]		Yankovic has hosted Al TV on MTV and Al Music on MuchMusic many times, generally coinciding with the release of each new album. For Poodle Hat, Al TV appeared on VH1 for the first time. A recurring segment of Al TV involves Yankovic manipulating interviews for comic effect. He inserts himself into a previously conducted interview with a musician, and then manipulates his questions, resulting in bizarre and comic responses from the celebrity.		Yankovic had a TV series called The Weird Al Show, which aired from September to December 1997 on CBS. Though the show appeared to be geared at children, the humor was really more for his adult fans (as such, it is often compared to Pee-wee's Playhouse). The entire series was released on DVD by Shout! Factory on August 15, 2006.		VH1 produced a Behind the Music episode on Yankovic. His two commercial failures (his film UHF and his 1986 album Polka Party!) were presented as having a larger impact on the direction of his career than they really had. Also, Coolio's later disapproval of "Amish Paradise" was played up as a large feud. Much was also made over his apparent lack of a love life, though he got married shortly after the program aired. The episode was updated and re-released in early 2012 as part of the "Behind the Music Remastered" series.		Yankovic has done voice-overs for several animated series. He appeared in a 2003 episode of The Simpsons, singing "The Ballad of Homer & Marge" (a parody of John Mellencamp's "Jack & Diane") with his band. The episode, "Three Gays of the Condo", in which Marge hires Yankovic to sing the aforementioned song to Homer in an attempt to reconcile their marriage, later won an Emmy Award for "Outstanding Animated Program (For Programming Less Than One Hour)". Yankovic also had a cameo in a 2008 episode, titled "That '90s Show", during which he records a parody of Homer's grunge hit "Shave Me" titled "Brain Freeze" (Homer's song, "Shave Me", was itself a parody of Nirvana's "Rape Me") making Yankovic one of only a handful of celebrities to appear twice on the show playing themselves.		He appeared in the animated Adult Swim show Robot Chicken, which provided him with a music video for the song "Weasel Stomping Day".[124][125] Yankovic is the voice for Squid Hat on the Cartoon Network show, The Grim Adventures of Billy & Mandy. He is also the announcer of the cartoon's eponymous video game adaptation.		Yankovic had a guest appearance voicing Wreck-Gar, a waste collection vehicle Transformer in the Transformers: Animated cartoon series;[126] previously, Yankovic's "Dare to Be Stupid" song was featured in the 1986 animated film The Transformers: The Movie, during the sequence in which the Wreck-Gar character was first introduced; as such, the song is referenced in the episode. He also plays local TV talent show host Uncle Muscles on several episodes of Tim and Eric Awesome Show, Great Job! along with other appearances on the show. Weird Al has also supplied the voice of one-shot character 'Petroleum Joe' on The Brak Show. He also voiced himself on a Back at the Barnyard episode, and he appeared as a ringmaster who helps the regular characters of Yo Gabba Gabba! organize a circus in a 2007 episode of the children's show.		In 2011, Al appeared as himself in the Batman: The Brave and the Bold episode "Bat-Mite Presents: Batman's Strangest Cases!"[127] In 2012, Al was extensively featured in the sixth season episode of 30 Rock called "Kidnapped by Danger", where Jenna tries to come up with a "Weird Al-proof" song,[128] as well as appearing on two episodes of The Aquabats! Super Show!, playing two different characters as the superhero SuperMagic PowerMan and as the President of the United States. In 2014, he appeared in the fourth season My Little Pony: Friendship Is Magic episode "Pinkie Pride" as Cheese Sandwich, a rival party planner to Pinkie Pie.[129] In 2016, "Weird Al" Yankovic was hired to voice the lead role in the 2016 Disney XD series Milo Murphy's Law.[130] Yankovic guest voiced as Papa Kotassium in a 2016 episode of Cartoon Network's animated series, Mighty Magiswords, which was created by fellow Weird Al-fan, musician and accordionist, Kyle Carrozza.[131] Carrozza not only sent an FAQ to Weird Al when he was in college in 1999,[132] but was also a contributor to a Weird Al-tribute album called Twenty Six-and-a-Half[133] and got a picture taken with him with the autographed album.[134] Also in 2016, Yankovic became the bandleader on the IFC series Comedy Bang! Bang!, on which he had previously guest starred.[135]		Yankovic performed at the 66th Primetime Emmy Awards singing a comedic medley of songs based on the themes of several Emmy-nominated shows such as Mad Men and Game of Thrones.[136]		A brief list of television shows on which Yankovic has appeared is available on his official website.[137]		"Weird Al" Yankovic has directed many of his own music videos; he has directed all of his music videos from 1993's "Bedrock Anthem" to 2006's "White & Nerdy". He also directed the end sequence of 1986's "Christmas at Ground Zero" (an original piece juxtaposing Christmas with nuclear warfare) from his Polka Party! album and the title sequence to Spy Hard, for which he sang the title song.[138] Yankovic wrote, directed and starred in the short 3-D movie attraction "Al's Brain: A 3-D Journey Through The Human Brain", a $2.5 million project which was sponsored by and premiered at the Orange County Fair in Costa Mesa, California, in 2009.[139] The project included a brief cameo by Sir Paul McCartney, which Yankovic directed during McCartney's appearance at the 2009 Coachella Valley Music and Arts Festival.[140] Fair CEO Steve Beazley, who supported the project, considered the project a success and explored leasing the exhibit to other fairs; the second appearance of the exhibit was at the 2009 Puyallup Fair in Washington.[141]		He has also directed several videos for other artists, including Hanson (the Titanic sequences in "River"), The Black Crowes ("Only a Fool"), Ben Folds ("Rockin' the Suburbs"), Jeff Foxworthy ("Redneck Stomp" and "Party All Night"), Jon Spencer Blues Explosion ("Wail"), and The Presidents of the United States of America ("Mixed Up S.O.B").[138] He has cameo appearances in his videos for Jon Spencer Blues Explosion, Hanson (as the interviewer), and Ben Folds (as the producer fixing Folds' "shitty tracks").		On January 25, 2010, Yankovic announced that he had signed a production deal with Warner Bros. to write and direct a live-action feature film for Cartoon Network.[142] Although Yankovic previously wrote the script for UHF, this was to be the first movie Yankovic directed.[142] Yankovic stated that he would not be starring in the movie, as Cartoon Network wanted a younger protagonist. During an interview on Comedy Death-Ray Radio, Yankovic revealed that though Cartoon Network "loved" his script, the network decided that they were no longer intending to produce feature films. Yankovic initially stated that he would instead shop the script around to other potential studios,[143] but in 2013 revealed that the project had been scrapped as "it was really geared for Cartoon Network" and that he had "cannibalized jokes from that script to use for other projects."[144]		Yankovic wrote When I Grow Up, a children's book released on February 1, 2011 and published by HarperCollins.[145] The book features 8-year-old Billy presenting to his class the wide variety of imaginative career possibilities that he is considering. Yankovic stated that the idea for the book was based on his own "circuitous" career path.[146] The book allows Yankovic to apply the humorous writing style found in his music in another medium, allowing him to use puns and rhymes.[146] Yankovic worked with Harper Collins' editor Anne Hoppe—the first time that Yankovic has had an editor—and found her help to be a positive experience.[146] The book is illustrated by Wes Hargis, who, according to Yankovic, has "a childlike quality and a very fun quality and a very imaginative quality" that matched well with Yankovic's writing.[146] The book reached the No. 4 position on The New York Times Best Seller list for Children's Picture Books for the week of February 20, 2011.[147]		Yankovic also wrote a sequel to When I Grow Up, 2013's My New Teacher and Me!		Yankovic became the first guest editor for Mad Magazine for their 533rd issue, published in April 2015.[148]		In 2008, Weird Al joined Michael J. Nelson as a guest on the RiffTrax treatment of Jurassic Park.		On November 10, 2009, Weird Al was a guest "internet scientist" on Rocketboom's "Know Your Meme" video series, in the installment on the topic of Auto-Tune, hosted by Jamie Wilkinson.		Eric Appel produced a Funny or Die movie trailer for Weird: The Al Yankovic Story, a fictional biographical film that parodies other films based on musicians; Yankovic (played by Aaron Paul) is seen hiding his "weirdness" from his parents (Gary Cole and Mary Steenburgen), making it big using song parodies with the help of Dr. Demento (Patton Oswalt), falling in and out of love with Madonna (Olivia Wilde), and fading into alcoholism and being arrested, at which point his father finally admits he is "weird" as well. Yankovic himself plays a music producer in the short.[32][149][150][151] Yankovic later appeared in another Funny or Die short alongside Huey Lewis which parodied the ax murder scene in the movie American Psycho, in which Christian Bale's character Patrick Bateman discusses the nature of Lewis's musical work before killing his victim.[152][153]		For The Nerdist Podcast, Weird Al began hosting a new comedic celebrity interview web series, Face to Face with 'Weird Al' Yankovic, on April 3, 2012. The series features Al TV-esque fake interviews with movie stars.		Al has appeared on numerous other webshows, including CollegeHumor Originals, LearningTown, Some Jerk with a Camera, Team Unicorn, and Epic Rap Battles of History appearing as Sir Isaac Newton in a battle against actors portraying Bill Nye, the Science Guy (YouTube star Nice Peter), and Neil DeGrasse Tyson (Chali 2na of the group Jurassic 5).		Yankovic competed on a week of Wheel of Fortune taped at Disney's Hollywood Studios in March 1994.[154] He also competed on Rock & Roll Jeopardy!		Weird Al joined the band Hanson in their music video for "Thinking 'bout Somethin'" in which he plays the tambourine.		Yankovic contributes backing vocals for the song "Time" on Ben Folds' album Songs for Silverman.		Yankovic also appeared in Halloween II as himself on a news channel.		Yankovic was also one of many celebrities who took part in the NOH8 Campaign against Proposition 8, which banned same-sex marriage in California.[155]		Yankovic was approached by a beer company to endorse their product. Yankovic had turned it down because he believed that "a lot of my fans were young and impressionable."[156] Yankovic later posted on his Twitter account that he never regretted the decision.[157]		In 2009, Yankovic was a special guest on an episode of G4's Web Soup where he came as Mark Gormley at first.[158]		In 2011, Yankovic guest starred as the character "Banana Man" in an episode of Adventure Time. The same year, he appeared as himself in the How I Met Your Mother episode "Noretta".		In 2012, he appeared as himself along with Alice Cooper, Bret Michaels, and Maria Menounos in The High Fructose Adventures of Annoying Orange for the Christmas special, and sung with Alice, Bret, and Orange.		On May 31, 2014, Yankovic won the ACE Award (Amateur Cartoonist Extraordinaire) from the National Cartoonists Society at its awards banquet in San Diego.[159]		Songs posted to file sharing networks are often misattributed to him because of their humorous subject matter. Often, his surname is misspelled (and thus mispronounced) as "Yankovich", among other variations. Much to the disdain of Yankovic, these misattributed files include songs that are racist, sexually explicit, or otherwise offensive. A young listener who had heard several of these offensive tracks by way of a file sharing service confronted Yankovic online, threatening a boycott because of his supposedly explicit lyrics.[160] Quite a few of the songs, such as "Star Wars Cantina" by Mark Jonathan Davis (not, in a double misattribution, his lounge-singer character Richard Cheese), "Star Wars Gangsta Rap", "Yoda Smokes Weed", "Chewbacca", "The Devil Went To Jamaica", "The Twelve Pains of Christmas" by Bob Rivers and several more have a Star Wars motif.[161] Some songs misattributed to him are not songs, but spoken skits, such as "Sesame Street on crack", which is also widely misattributed to Adam Sandler. A list of songs frequently misattributed to Yankovic can be found at The Not Al Page[161] and a list of all commercially released songs recorded by Yankovic can be found on his website.[162]		Yankovic cites these misattributions as "his only real beef with peer-to-peer file sharing sites":		If you do a search for my name on any one of those sites, I guarantee you that about half of the songs that come up will be songs I had absolutely nothing to do with. That particularly bothers me, because I really try to do quality work, and I also try to maintain a more-or-less family-friendly image—and some of these songs that are supposedly by me are just, well, vulgar and awful. I truly think my reputation has suffered in a lot of people's minds because of all those fake Weird Al songs floating around the Internet.[163]		In an episode of HBO's Mr. Show with Bob and David called "Rudy Will Await Your Foundation", Bob Odenkirk plays a character called Daffy "Mal" Yinkleyankle, a parody of Weird Al. Al, who claims it was the only genuine parody act on himself he has ever seen, told Odenkirk in an email that he was "flattered, in a weird way" and "found it very funny".[164][165]		The Weird Al Star Fund is a campaign started by Yankovic's fans to get him a star on the Hollywood Walk of Fame. Their mission is to "solicit, collect, and raise the necessary money, and to compile the information needed for the application to nominate "Weird Al" Yankovic for a star on the Hollywood Walk of Fame."[166] Fans worldwide have sent donations to raise the US$15,000 needed for a nomination. In addition to the preferred method of cash donations, many methods were used to raise money for the cause, such as a live benefit show held April 11, 2006, and selling merchandise on the official website and eBay, including T-shirts, calendars, and cookbooks.[167] On May 26, 2006, the campaign hit the then-$15,000 target, just five days before the May 31 deadline to submit the necessary paperwork.[166] However, Yankovic was not included on the list of inductees for 2007.[168] On February 9, 2007, the Hollywood Chamber Of Commerce raised the price to sponsor a new star to $25,000[166] and as such the Fund is accepting donations again. Yankovic's application was resubmitted for consideration in 2007, but he was not included among 2008's inductees.[169] The Hollywood Chamber of Commerce announced in June 2017 that Yankovic will receive a star on the Walk of Fame as part of the 2018 inductees.[170]		Similar to the Weird Al Star Fund, a second fan-driven campaign called "Make the Rock Hall 'Weird'" has tried to enshrine him into Rock and Roll Hall of Fame in Cleveland, Ohio, for which he has been eligible since 2004.[171] Previous attempts to raise awareness for the campaign and support Yankovic's nomination included a petition drive from 2006 to 2007, which raised over 9000 signatures; an art competition in 2005; additionally, a documentary film about the campaign is currently being developed.[172][173] In addition to these efforts, an ongoing campaign is underway in which supporters of Yankovic's nomination are requested to send "sincere, thoughtful" letters to the Rock Hall Foundation's headquarters in New York.[173] The Hall has not considered Yankovic for nomination since the campaign started in 2004.[171] A 2009 Rolling Stone poll named Weird Al as the top artist that should be nominated for the Hall of Fame, followed by Rush (who were inducted in 2013) and The Moody Blues in the top ten."[174]		A smaller ongoing effort has been made by fans to have Yankovic perform at the halftime show of a Super Bowl game.[175] This inspired Yankovic to write the fight song parody "Sports Song" for Mandatory Fun to help round out his repertoire.[176] Subsequent to the success of Mandatory Fun, another fan-driven campaign pushed for Yankovic to headline the then-upcoming Super Bowl XLIX at the highlight of the artist's career, which was noticed by many media outlets, including CNN and Wired, though the decision for this selection would reside within the management of the NFL (who instead chose Katy Perry for that position).[177][178][179]		Grammy Awards[180]		
A social skill is any skill facilitating interaction and communication with others. Social rules and relations are created, communicated, and changed in verbal and nonverbal ways. The process of learning these skills is called socialization. For socialization, Interpersonal skills are essential to relate one another. Interpersonal skills are the interpersonal acts a person uses to interact with others which are related to dominance vs. submission, love vs. hate, affiliation vs. aggression, control vs. autonomy categories (Leary, 1957). Positive interpersonal skills include persuasion, active listening, delegation, and stewardship among others. Social psychology is the academic discipline that does research related to social skills, the discipline studies how skills are learned by an individual through changes in attitude, thinking, and behavior.[citation needed]						Social skills are the tools that enable people to communicate, learn, ask for help, get their needs met in appropriate ways, get along with others, make friends and develop healthy relationships, protect themselves, and generally be able to interact with the society harmoniously.[1] Social skill builds essential character traits like trustworthiness, respectfulness, responsibility, fairness, caring, and citizenship. These traits help to build an internal moral compass, allowing individuals to make good choices in thinking and behavior, resulting in social competence.		The important social skills identified by the Employment and Training Administration are:[citation needed]		Social skills are goal oriented with both main- and sub-goals. For example, a workplace interaction initiated by a new employee with a senior employee at first will be with the main goal to gather information and the sub goal will be to establishing rapport in order to obtain the main goal.[citation needed]		Deficits in social skills were categorized by Gresham in 1998 as failure to recognize and reflect social skills, failure to model appropriate models, failure to perform acceptable behavior in particular situations in relation to developmental and transitional stages.[2]		Social skills are significantly impaired in people suffering from alcoholism due to the neurotoxic effects of alcohol on the brain, especially the prefrontal cortex area of the brain. The social skills that are impaired by alcohol abuse include impairments in perceiving facial emotions, prosody perception problems, and theory of mind deficits; the ability to understand humor is also impaired in alcohol abusers.[3] Impairments in social skills also occur in individuals who suffer from fetal alcohol spectrum disorders; these deficits persist throughout affected people's lives and may worsen over time due to the effects of aging on the brain.[4]		People with ADHD and hyperkinetic disorder[5] (a more severe form of ADHD) more often have difficulties with social skills, such as social interaction and forming and maintaining friendships. Approximately half of ADHD children and adolescents will experience peer rejection compared to 10-15 percent of non-ADHD youth. Adolescents with ADHD are less likely to develop close friendships, although it might be easier by the time adolescents age into adulthood and enter the workplace. Difficulties in sustaining romantic relationships may also occur in high school and college aged individuals with ADHD. Training in social skills, behavioural modification and medication may have some limited beneficial effects; the most important factor in reducing emergence of later psychopathology is the ADHD individual forming friendships with people who are not involved in deviant/delinquent activities. Poor peer relationships can contribute to major depression, criminality, school failure, and substance use disorders.[6] Adolescents with ADHD are more likely to find it difficult in making and keeping friends due to their attention deficits causing impairments in processing verbal and nonverbal language which is important for social skills and adolescent interaction; this may result in such adolescents being regarded by their peers as immature or as social outcasts.[7] Romantic relationships are usually difficult in the adolescent and college age because of the lack of attention of non verbal cues such as flirting gestures, tone of voice, which may include misinterpretation if whether the person is romantically attracted to that person, along with the impulsiveness of "jumping into" relationships.[citation needed]		As a rule, people with autistic spectrum disorders such as Asperger's syndrome have a deficit within social skills. This is most likely the result of the lack of theory of mind, which enables the person to understand other people's emotions. Many people in the spectrum have many social idiosyncrasies such as obsessive interests and routines, lack of eye contact, one sided conversations, abnormal body language and non-verbal communication.[citation needed] The concept of social skills has been questioned in terms of autistic spectrum.[8] In response for the needs of children with autism, Romanczyk has suggested for adapting comprehensive model of social acquisitions with behavioral modification rather than specific response's tailored for social contexts.[9]		Individuals with few opportunities to socialize with others often struggle with social skills. This can often create a downward spiral for people with conditions like anxiety or depression. Due to anxiety experienced from concerns with interpersonal evaluation and from fear of negative reaction by others, surfeit expectations of failure or social rejection in socialization's leads to avoiding or shutting themselves from social interactions.[citation needed] Due to depression, people avoid opportunities to socialize, which impairs their social skills, which makes socialization even more unattractive.[10]		The authors of the book Snakes in Suits: When Psychopaths Go to Work explore psychopathy in workplace. The FBI consultants describe a five phase model of how a typical workplace psychopath climbs to and maintains power. They conclude many traits exhibited by these individuals were consistent with psychopathy: superficial charm, insincerity, egocentricity, manipulativeness, grandiosity, lack of empathy, low on agreeableness, exploitativeness, independence, rigidity, stubbornness and dictatorial tendencies. Babiak and Hare say for corporate psychopaths, success is defined as the best revenge and their problem behaviors are repeated "ad infinitum" due to little insight and their proto-emotions such as "anger, frustration, and rage" is refracted as irresistible charm. The authors note that lack of emotional literacy and moral conscience is often confused with toughness, the ability to make hard decisions, and effective crisis management. Babiak and Hare also emphasizes a reality they identified with psychopaths from studies that "there is no evidence that psychopaths can benefit from treatment or management programs positively".[11][12]		Emily Grijalva at the University at Buffalo, part of the State University of New York, investigating narcissism in business found there are two forms of narcissism: "vulnerable" and "grandiose".[13] It is her finding that "moderate" level of grandiose narcissism is linked to becoming an effective manager. Grandiose narcissists are characterized as confident; They possess unshakable belief that they are superior, even when it's unwarranted. They can be charming, pompous show-offs, and can also be selfish, exploitative and entitled.[14] Jens Lange and Jan Crusius at the University of Cologne, Germany associates "malicious-benign" envy within narcissistic social climbers in workplace. It is their finding that grandiose narcissists are less prone to low self-esteem and neuroticism and are less susceptible to the anxiety and depression that can affect vulnerable narcissists when coupled with envy. They characterize vulnerable narcissists as those who "believe they are special, and want to be seen that way–but are just not that competent, or charming. As a result, their self-esteem fluctuates a lot. They tend to be self-conscious and passive, but also prone to outbursts of potentially violent aggression if their inflated self-image is threatened."[15] Richard Boyatzis says this is an unproductive form of expression of emotions that the person can’t share constructively, which reflects lack of appropriate skills.[16] Eddie Brummelman, a social and behavioral scientist at the University of Amsterdam in the Netherlands and Brad Bushman at Ohio State University in Columbus says studies show that in western culture narcissism is on the rise from shifting focus on the self rather than on relationships and concludes that all narcissism to be socially undesirable ("unhealthy feelings of superiority"). David Kealy at the University of British Columbia in Canada states that narcissism might aid temporarily but in the long run it is better to be true to oneself, having personal integrity and being kind to others.[17]		To behaviorists, social skills are learned behavior that allow people to achieve social reinforcement. According to Schneider & Byrne (1985), who conducted a meta-analysis of social skills training procedures (51 studies), operant conditioning procedures for training social skills had the largest effect size, followed by modeling, coaching, and social cognitive techniques.[18] Behavior analysts prefer to use the term behavioral skills to social skills.[19] Behavioral skills training to build social and other skills is used with a variety of populations including in packages to treat addictions as in the community reinforcement approach and family training (CRAFT).[20]		Training of behavioral skills is also used for people who suffer from borderline personality disorder,[21] depression,[22] and developmental disabilities.[19][23] Typically behaviorists try to develop what are considered cusp skills,[24] which are critical skills to open access to a variety of environments. The rationale for this type of an approach to treatment is that people meet a variety of social problems and can reduce the stress and punishment from the encounter in a safe environment as well as increase their reinforcement by having the correct skills.[25]		
The Woodstock Music & Art Fair—informally, the Woodstock Festival or simply Woodstock—was a music festival attracting an audience of over 400,000 people, scheduled over three days on a dairy farm in New York from August 15 to 17, 1969, but ultimately ran four days long, ending August 18, 1969.[2]		Billed as "An Aquarian Exposition: 3 Days of Peace & Music", it was held at Max Yasgur's 600-acre (240 ha; 0.94 sq mi) dairy farm in the Catskills near the hamlet of White Lake in the town of Bethel.[2] Bethel, in Sullivan County, is 43 miles (69 km) southwest of the town of Woodstock, New York, in adjoining Ulster County.		During the sometimes rainy weekend, 32 acts performed outdoors before an audience of more than 400,000 people.[3] It is widely regarded as a pivotal moment in popular music history, as well as the definitive nexus for the larger counterculture generation.[4][5]		Rolling Stone listed it as one of the 50 Moments That Changed the History of Rock and Roll.[6]		The event was captured in the Academy Award winning 1970 documentary movie Woodstock, an accompanying soundtrack album, and Joni Mitchell's song "Woodstock", which commemorated the event and became a major hit for both Crosby, Stills, Nash & Young and Matthews Southern Comfort. In 2017 the festival site was listed on the National Register of Historic Places.[7]						Woodstock was initiated through the efforts of Michael Lang, John Roberts, Joel Rosenman, and Artie Kornfeld. Roberts and Rosenman financed the project. Lang had some experience as a promoter, having co-organized a small festival on the East Coast the prior year, the Miami Pop Festival, where an estimated 25,000 people attended the two-day event. Early in 1969, Roberts and Rosenman were New York City entrepreneurs, in the process of building Media Sound, a large audio recording studio complex in Manhattan. Lang and Kornfeld's lawyer, Miles Lourie, who had done legal work on the Media Sound project, suggested that they contact Roberts and Rosenman about financing a similar, but much smaller, studio Kornfeld and Lang hoped to build in Woodstock, New York. Unpersuaded by this Studio-in-the-Woods proposal, Roberts and Rosenman counter-proposed a concert featuring the kind of artists known to frequent the Woodstock area (such as Bob Dylan and The Band). Kornfeld and Lang agreed to the new plan, and Woodstock Ventures was formed in January 1969.[8] The company offices were located in an oddly decorated floor of 47 West 57th Street in Manhattan. Burt Cohen, and his design group, Curtain Call Productions, oversaw the psychedelic transformation of the office.[9]		From the start, there were differences in approach among the four: Roberts was disciplined and knew what was needed for the venture to succeed, while the laid-back Lang saw Woodstock as a new, "relaxed" way of bringing entrepreneurs together.[10] When Lang was unable to find a site for the concert, Roberts and Rosenman, growing increasingly concerned, took to the road and eventually came up with a venue. Similar differences about financial discipline made Roberts and Rosenman wonder whether to pull the plug or to continue pumping money into the project.[10]		In April 1969, newly minted superstars Creedence Clearwater Revival became the first act to sign a contract for the event, agreeing to play for $10,000. The promoters had experienced difficulty landing big-name groups prior to Creedence committing to play. Creedence drummer Doug Clifford later commented, "Once Creedence signed, everyone else jumped in line and all the other big acts came on." Given their 3:00 a.m. start time and omission from the Woodstock film (at Creedence frontman John Fogerty's insistence), Creedence members have expressed bitterness over their experiences at the famed festival.[11]		Woodstock was designed as a profit-making venture, aptly titled "Woodstock Ventures". It famously became a "free concert" only after the event drew hundreds of thousands more people than the organizers had prepared for. Tickets for the three-day event cost $18 in advance and $24 at the gate (equivalent to about $120 and $160 today[12]). Ticket sales were limited to record stores in the greater New York City area, or by mail via a post office box at the Radio City Station Post Office located in Midtown Manhattan. Around 186,000 advance tickets were sold, and the organizers anticipated approximately 200,000 festival-goers would turn up.[13]		The original venue plan was for the festival to take place in Wallkill, New York, possibly near the proposed recording studio site owned by Alexander Tapooz. After local residents quickly shot down that idea, Lang and Kornfeld thought they had found another possible location in Saugerties, New York. But they had misunderstood, as the landowner's attorney made clear, in a brief meeting with Roberts and Rosenman.[8] Growing alarmed at the lack of progress, Roberts and Rosenman took over the search for a venue, and discovered the 300-acre (120 ha) Mills Industrial Park (41°28′39″N 74°21′49″W﻿ / ﻿41.477525°N 74.36358°W﻿ / 41.477525; -74.36358﻿ (Mills Industrial Park)) in the town of Wallkill, New York, which Woodstock Ventures leased for $10,000 in the Spring of 1969.[1] Town officials were assured that no more than 50,000 would attend. Town residents immediately opposed the project. In early July, the Town Board passed a law requiring a permit for any gathering over 5,000 people. On July 15, 1969, the Wallkill Zoning Board of Appeals officially banned the concert on the basis that the planned portable toilets would not meet town code.[14] Reports of the ban, however, turned out to be a publicity bonanza for the festival.[15]		In his 2007 book Taking Woodstock, Elliot Tiber relates that he offered to host the event on his 15 acres (6.1 ha) motel grounds, and had a permit for such an event. He claims to have introduced the promoters to dairy farmer Max Yasgur.[16] Lang, however, disputes Tiber's account and says that Tiber introduced him to a realtor, who drove him to Yasgur's farm without Tiber. Sam Yasgur, Max's son, agrees with Lang's account.[17] Yasgur's land formed a natural bowl sloping down to Filippini Pond on the land's north side. The stage would be set up at the bottom of the hill with Filippini Pond forming a backdrop. The pond would become a popular skinny dipping destination.		The organizers once again told Bethel authorities they expected no more than 50,000 people.		Despite resident opposition and signs proclaiming, "Buy No Milk. Stop Max's Hippy Music Festival",[18] Bethel Town Attorney Frederick W. V. Schadt and building inspector Donald Clark approved the permits, but the Bethel Town Board refused to issue them formally. Clark was ordered to post stop-work orders.		The late change in venue did not give the festival organizers enough time to prepare. At a meeting three days before the event, organizers felt they had two options: one was to complete the fencing and ticket booths, without which the promoters were almost certain to lose their shirts; the other option involved putting their remaining available resources into building the stage, without which the promoters feared they would have a disappointed and disgruntled audience. When the audience began arriving by the tens of thousands, the next day, on Wednesday before the weekend, the decision had been made for them.[8] "The fences at Woodstock" became an oxymoron, while the stage at Woodstock gave birth to a legend.		The influx of attendees to the rural concert site in Bethel created a massive traffic jam. Fearing chaos as thousands began descending on the community, Bethel did not enforce its codes.[14] Eventually, announcements on radio stations as far away as WNEW-FM in Manhattan and descriptions of the traffic jams on television news discouraged people from setting off to the festival.[19][20] Arlo Guthrie made an announcement that was included in the film saying that the New York State Thruway was closed.[21] The director of the Woodstock museum discussed below said this never occurred.[22] To add to the problems and difficulty in dealing with the large crowds, recent rains had caused muddy roads and fields. The facilities were not equipped to provide sanitation or first aid for the number of people attending; hundreds of thousands found themselves in a struggle against bad weather, food shortages, and poor sanitation.[23]		On the morning of Sunday, August 17, New York Governor Nelson Rockefeller called festival organizer John Roberts and told him he was thinking of ordering 10,000 New York State National Guard troops to the festival. Roberts was successful in persuading Rockefeller not to do this. Sullivan County declared a state of emergency.[19] During the festival, personnel from nearby Stewart Air Force Base assisted in helping to ensure order and airlifting performers in and out of the concert venue.[24]		Jimi Hendrix was the last act to perform at the festival. Because of the rain delays that Sunday, when Hendrix finally took the stage it was 8:30 Monday morning. The audience, which had peaked at an estimated 400,000 during the festival, was now reduced to about 30,000 by that point; many of them merely waited to catch a glimpse of Hendrix before leaving during his performance.[25]		Hendrix and his new band, Gypsy Sun and Rainbows (introduced as The Experience, but corrected by Hendrix) [26] performed a two-hour set. His psychedelic rendition of the U.S. national anthem, "The Star-Spangled Banner" occurred about three-quarters into the set (after which he segued into "Purple Haze"). The song would become "part of the sixties Zeitgeist" as it was captured forever in the Woodstock film;[27] Hendrix's image performing this number wearing a blue-beaded white leather jacket with fringe and a red head scarf has since been regarded as a defining moment of the 1960s.[25][28]		And this is the moment I will never forget as long as I live: A quarter mile away in the darkness, on the other edge of this bowl, there was some guy flicking his Bic, and in the night I hear, 'Don't worry about it, John. We're with you.' I played the rest of the show for that guy.		Although the festival was remarkably peaceful given the number of people and the conditions involved, there were two recorded fatalities: one from what was believed to be a heroin overdose, and another caused in an accident when a tractor ran over an attendee sleeping in a nearby hayfield. There also were two births recorded at the event (one in a car caught in traffic and another in a hospital after an airlift by helicopter) and four miscarriages.[29] Oral testimony in the film supports the overdose and run-over deaths and at least one birth, along with many logistical headaches.		Yet, in tune with the idealistic hopes of the 1960s, Woodstock satisfied most attendees. There was a sense of social harmony, which, with the quality of music, and the overwhelming mass of people, many sporting bohemian dress, behavior, and attitudes, helped to make it one of the enduring events of the century.[30]		After the concert, Max Yasgur, who owned the site of the event, saw it as a victory of peace and love. He spoke of how nearly half a million people filled with potential for disaster, riot, looting, and catastrophe spent the three days with music and peace on their minds. He stated, "If we join them, we can turn those adversities that are the problems of America today into a hope for a brighter and more peaceful future..."[10]		Sound for the concert was engineered by sound engineer Bill Hanley. "It worked very well," he says of the event. "I built special speaker columns on the hills and had 16 loudspeaker arrays in a square platform going up to the hill on 70-foot (21 m) towers. We set it up for 150,000 to 200,000 people. Of course, 500,000 showed up."[31] ALTEC designed marine plywood cabinets that weighed half a ton apiece and stood 6 feet (1.8 m) tall, almost 4 feet (1.2 m) deep, and 3 feet (0.91 m) wide. Each of these enclosures carried four 15-inch (380 mm) JBL D140 loudspeakers. The tweeters consisted of 4×2-Cell & 2×10-Cell Altec Horns. Behind the stage were three transformers providing 2,000 amperes of current to power the amplification setup.[32] For many years this system was collectively referred to as the Woodstock Bins.[33]		Thirty-two acts performed over the course of the four days:[34]		Very few reporters from outside the immediate area were on the scene. During the first few days of the festival, national media coverage emphasized the problems. Front page headlines in the Daily News read "Traffic Uptight at Hippiefest" and "Hippies Mired in a Sea of Mud". Coverage became more positive by the end of the festival, in part because the parents of concertgoers called the media and told them, based on their children's phone calls, that their reporting was misleading.[19][61]		The New York Times covered the prelude to the festival and the move from Wallkill to Bethel.[18] Barnard Collier, who reported from the event for The New York Times, asserts that he was pressured by on-duty editors at the paper to write a misleadingly negative article about the event. According to Collier, this led to acrimonious discussions and his threat to refuse to write the article until the paper's executive editor, James Reston, agreed to let him write the article as he saw fit. The eventual article dealt with issues of traffic jams and minor lawbreaking, but went on to emphasize cooperation, generosity, and the good nature of the festival goers.[19][61] When the festival was over, Collier wrote another article about the exodus of fans from the festival site and the lack of violence at the event. The chief medical officer for the event and several local residents were quoted as praising the festival goers.[29][62]		Middletown, New York's Times Herald-Record, the only local daily newspaper, editorialized against the law that banned the festival from Wallkill. During the festival a rare Saturday edition was published. The paper had the only phone line running out of the site, and it used a motorcyclist to get stories and pictures from the impassable crowd to the newspaper's office 35 miles (56 km) away in Middletown.[1][63][64][65]		The documentary film Woodstock, directed by Michael Wadleigh and edited by Thelma Schoonmaker and Martin Scorsese, was released in 1970. Artie Kornfeld (one of the promoters of the festival) went to Fred Weintraub, an executive at Warner Bros., and asked for money to film the festival. Artie had been turned down everywhere else, but against the express wishes of other Warner Bros. executives, Weintraub put his job on the line and gave Kornfeld $100,000 to make the film. Woodstock helped to save Warner Bros at a time when the company was on the verge of going out of business. The book Easy Riders, Raging Bulls details the making of the film.		Wadleigh rounded up a crew of about 100 from the New York film scene. With no money to pay the crew, he agreed to a double-or-nothing scheme, in which the crew would receive double pay if the film succeeded and nothing if it bombed. Wadleigh strove to make the film as much about the hippies as the music, listening to their feelings about compelling events contemporaneous with the festival (such as the Vietnam War), as well as the views of the townspeople.[66]		Woodstock received the Academy Award for Documentary Feature.[67] The film has been deemed culturally significant by the United States Library of Congress. In 1994, Woodstock: The Director's Cut was released and expanded to include Janis Joplin as well as additional performances by Jefferson Airplane, Jimi Hendrix, and Canned Heat not seen in the original version of the film. In 2009, the expanded 40th Anniversary Edition was released on DVD. This release marks the film's first availability on Blu-ray disc.		Another film on Woodstock named Taking Woodstock was produced in 2009 by Taiwanese American filmmaker Ang Lee.[68] Lee practically rented out the entire town of New Lebanon, New York, to shoot the film. He was concerned with angering the locals, but they ended up being incredibly welcoming and excited to help with the film.[69] The movie is based on Elliot Tiber, played by Demetri Martin, and his role in bringing Woodstock to Bethel, New York. The film also stars Jonathan Groff as Michael Lang and Henry Goodman and Imelda Staunton as Jake and Sonia Teichberg.[70]		Two soundtrack albums were released. The first, Woodstock: Music from the Original Soundtrack and More, was a 3-LP (later 2-CD) album containing a sampling of one or two songs by most of the acts who performed. A year later, Woodstock 2 was released as a 2-LP album. Both albums included recordings of stage announcements (e.g., "[We're told] that the brown acid is not specifically too good", "Hey, if you think really hard, maybe we can stop this rain") and crowd noises (i.e., the rain chant) between songs. In 1994, a third album, Woodstock Diary was released. Tracks from all three albums, as well as numerous additional, previously unreleased performances from the festival but not the stage announcements and crowd noises, were reissued by Atlantic as a 4-CD box set titled Woodstock: Three Days of Peace and Music.		An album titled Jimi Hendrix: Woodstock also was released in 1994, featuring only selected recordings of Jimi Hendrix at the festival. A longer double-disc set, Live at Woodstock (1999) features nearly every song of Hendrix's performance, omitting just two pieces that were sung by his rhythm guitarist.		In 2009, Joe Cocker released Live at Woodstock, a live album of his entire Woodstock set. The album contained eleven tracks, ten of which were previously unreleased.		In 2009, complete performances from Woodstock by Santana, Janis Joplin, Sly & the Family Stone, Jefferson Airplane, and Johnny Winter were released separately by Legacy/SME Records, and were also collected in a box set titled The Woodstock Experience. Also, in 2009, Rhino/Atlantic Records issued a 6-CD box set titled Woodstock: 40 Years On: Back to Yasgur's Farm, which included further musical performances as well as stage announcements and other ancillary material.[71]		Max Yasgur refused to rent out his farm for a 1970 revival of the festival, saying, "As far as I know, I'm going back to running a dairy farm." Yasgur died in 1973.[74]		Bethel voters tossed out their supervisor in an election held in November 1969 because of his role in bringing the festival to the town. New York State and the town of Bethel passed mass gathering laws designed to prevent any more festivals from occurring.		In 1984, at the original festival site, land owners Louis Nicky and June Gelish put up a monument marker with plaques called "Peace and Music" by a local sculptor from nearby Bloomingburg, Wayne C. Saward (1957–2009).[72][75]		Attempts were made to prevent people from visiting the site, its owners spread chicken manure, and during one anniversary, tractors and state police cars formed roadblocks. Twenty thousand people gathered at the site in 1989 during an impromptu 20th anniversary celebration. In 1997 a community group put up a welcoming sign for visitors. Unlike Bethel, the town of Woodstock made several efforts to cash in on its notoriety. Bethel's stance changed in recent years, and the town now embraces the festival. Efforts have begun to forge a link between Bethel and Woodstock.[76]		Approximately 80 lawsuits were filed against Woodstock Ventures, primarily by farmers in the area. The movie financed settlements and paid off the $1.4 million of debt Woodstock Ventures had incurred from the festival.[19]		In 1984, a plaque was placed at the original site commemorating the festival.[77] The field and the stage area remain preserved in their rural setting and the fields of the Yasgur farm are still visited by people of all generations.[78]		In 1996, the site of the concert and 1,400 acres (5.7 km2) surrounding was purchased by cable television pioneer Alan Gerry for the purpose of creating the Bethel Woods Center for the Arts.[79] The Center opened on July 1, 2006, with a performance by the New York Philharmonic.[80] On August 13, 2006, Crosby, Stills, Nash & Young performed before 16,000 fans at the new Center—37 years after their historic performance at Woodstock.[81]		The Museum at Bethel Woods opened on June 2, 2008.[82] The Museum contains film and interactive displays, text panels, and artifacts that explore the unique experience of the Woodstock festival, its significance as the culminating event of a decade of radical cultural transformation, and the legacy of the Sixties and Woodstock today.[82]		The ashes of the late Richie Havens were scattered across the site on August 18, 2013.[83]		In late 2016 New York's State Historic Preservation Office applied to the National Park Service to have 600 acres (240 ha) including the site of the festival and adjacent areas used for campgrounds, all of which still appear mostly as they did in 1969 as they were not redeveloped when Bethel Woods was built, listed on the National Register of Historic Places.[84]		There was worldwide media interest in the 40th anniversary of Woodstock in 2009.[85] A number of activities to commemorate the festival took place around the world. On August 15, at the Bethel Woods Center for the Arts overlooking the original site, the largest assembly of Woodstock performing alumni since the original 1969 festival performed in an eight-hour concert in front of a sold-out crowd. Hosted by Country Joe McDonald, the concert featured Big Brother and the Holding Company performing Janis Joplin's hits (she actually appeared with the Kozmic Blues Band at Woodstock, although that band did feature former Big Brother guitarist Sam Andrew), Canned Heat, Ten Years After, Jefferson Starship, Mountain, and the headliners, The Levon Helm Band. At Woodstock, Levon Helm played drums and was one of the lead vocalists with The Band. Paul Kantner was the only member of the 1969 Jefferson Airplane line-up to appear with Jefferson Starship. Tom Constanten, who played keyboard with the Grateful Dead at Woodstock, joined Jefferson Starship on stage for several numbers. Jocko Marcellino from Sha Na Na also appeared, backed up by Canned Heat.[86] Richie Havens, who opened the Woodstock festival in 1969, appeared at a separate event the previous night.[87] Crosby, Stills & Nash and Arlo Guthrie also marked the anniversary with live performances at Bethel earlier in August 2009.		Another event occurred in Hawkhurst, Kent (UK), at a Summer of Love party, with acts including two of the participants at the original Woodstock, Barry Melton of Country Joe and the Fish and Robin Williamson of The Incredible String Band, plus Santana and Grateful Dead cover bands.[88] On August 14 and 15, 2009, a 40th anniversary tribute concert was held in Woodstock, IL and was the only festival to receive the official blessing of the "Father of Woodstock", Artie Kornfeld.[89] Kornfeld later made an appearance in Woodstock[clarification needed] with the event's promoters.		Also in 2009, Michael Lang and Holly George-Warren published The Road to Woodstock, which describes Lang's involvement in the creation of the Woodstock Music & Arts Festival, and includes personal stories and quotes from central figures involved in the event.		As one of the biggest rock festivals of all time and a cultural touchstone for the late sixties, Woodstock has been referenced in many different ways in popular culture. The phrase "the Woodstock generation" became part of the common lexicon.[90] Tributes and parodies of the festival began almost as soon as the final chords sounded. Cartoonist Charles Schulz named his recurring Peanuts bird character - which began appearing in 1966 but was yet unnamed -Woodstock in tribute to the festival.[91] In April 1970, Mad magazine published a poem by Frank Jacobs and illustrated by Sergio Aragonés titled "I Remember, I Remember The Wondrous Woodstock Music Fair" that parodies the traffic jams and the challenges of getting close enough to actually hear the music.[92] Keith Robertson's 1970 children's book Henry Reed's Big Show has the title character attempting to emulate the success of the festival by mounting his own concert at his uncle's farm. In 1973, the stage show National Lampoon's Lemmings portrayed the "Woodchuck" festival, featuring parodies of many Woodstock performers.[93]		More recent culture continues to remember Woodstock, with Time magazine naming "The Who at Woodstock – 1969" to the magazine's "Top 10 Music-Festival Moments" list on March 18, 2010.[94]		In 2005, Argentine writer Edgar Brau published Woodstock, a long poem commemorating the festival. An English translation of the poem was published in January 2007 by Words Without Borders.[95]		In 2017, the singer Lana Del Rey released a song, Coachella - Woodstock in My Mind, in order to show her worries about the tensions between North Korea and the United States, while she was at Coachella, expressing nostalgia by reminding the Woodstock festival as a symbol of peace.[96]		Wadham College, one of the constituent colleges of the University of Oxford, hosts an annual music and arts festival in its gardens, named 'Wadstock', after the Woodstock festival.[97]		Opening ceremony at Woodstock. Swami Satchidananda giving the opening speech		A rainy day (August 15, 1969)		Two hippies at Woodstock		Joe Cocker and the Grease Band performing at Woodstock		Photo taken near Woodstock on August 18, 1969		Richie Havens at the Woodstock Festival		Tents at the Woodstock Festival		Coordinates: 41°42′05″N 74°52′49″W﻿ / ﻿41.70139°N 74.88028°W﻿ / 41.70139; -74.88028		
Spider-Man is a fictional superhero appearing in American comic books published by Marvel Comics. The character was created by writer-editor Stan Lee and writer-artist Steve Ditko, and first appeared in the anthology comic book Amazing Fantasy #15 (August 1962) in the Silver Age of Comic Books. Lee and Ditko conceived the character as an orphan being raised by his Aunt May and Uncle Ben in New York City after his parents Richard and Mary Parker were killed in a plane crash, and as a teenager, having to deal with the normal struggles of adolescence in addition to those of a costumed crime-fighter. Spider-Man's creators gave him super strength and agility, the ability to cling to most surfaces, shoot spider-webs using wrist-mounted devices of his own invention, which he calls "web-shooters", and react to danger quickly with his "spider-sense", enabling him to combat his foes.		When Spider-Man first appeared in the early 1960s, teenagers in superhero comic books were usually relegated to the role of sidekick to the protagonist. The Spider-Man series broke ground by featuring Peter Parker, the high school student from Queens behind Spider-Man's secret identity and with whose "self-obsessions with rejection, inadequacy, and loneliness" young readers could relate.[8] While Spider-Man had all the makings of a sidekick, unlike previous teen heroes such as Bucky and Robin, Spider-Man had no superhero mentor like Captain America and Batman; he thus had to learn for himself that "with great power there must also come great responsibility"—a line included in a text box in the final panel of the first Spider-Man story but later retroactively attributed to his guardian, the late Uncle Ben.		Marvel has featured Spider-Man in several comic book series, the first and longest-lasting of which is titled The Amazing Spider-Man. Over the years, the Peter Parker character has developed from shy, nerdy New York City high school student to troubled but outgoing college student, to married high school teacher to, in the late 2000s, a single freelance photographer. In the 2010s, he joins the Avengers, Marvel's flagship superhero team. Spider-Man's nemesis Doctor Octopus also took on the identity for a story arc spanning 2012–2014, following a body swap plot in which Peter appears to die.[9] Separately, Marvel has also published books featuring alternate versions of Spider-Man, including Spider-Man 2099, which features the adventures of Miguel O'Hara, the Spider-Man of the future; Ultimate Spider-Man, which features the adventures of a teenaged Peter Parker in an alternate universe; and Ultimate Comics Spider-Man, which depicts the teenager Miles Morales, who takes up the mantle of Spider-Man after Ultimate Peter Parker's supposed death. Miles is later brought into mainstream continuity, where he works alongside Peter.		Spider-Man is one of the most popular and commercially successful superheroes.[10] As Marvel's flagship character and company mascot, he has appeared in countless forms of media, including several animated and live action television series, syndicated newspaper comic strips, and in a series of films. The character was first portrayed in live action by Danny Seagren in Spidey Super Stories, a The Electric Company skit which ran from 1974 to 1977.[11] In films, Spider-Man has been portrayed by actors Tobey Maguire (2002–2007), Andrew Garfield (2012–2014),[12] and Tom Holland, who has portrayed the character in the Marvel Cinematic Universe since 2016. Reeve Carney starred as Spider-Man in the 2010 Broadway musical Spider-Man: Turn Off the Dark.[13] Spider-Man has been well received as a superhero and comic book character, and he is often ranked as one of the most popular comic book characters of all time, alongside DC Comics' most famous superheroes, Batman, Superman and Wonder Woman.						In 1962, with the success of the Fantastic Four, Marvel Comics editor and head writer Stan Lee was casting about for a new superhero idea. He said the idea for Spider-Man arose from a surge in teenage demand for comic books, and the desire to create a character with whom teens could identify.[15]:1 In his autobiography, Lee cites the non-superhuman pulp magazine crime fighter the Spider (see also The Spider's Web and The Spider Returns) as a great influence,[14]:130 and in a multitude of print and video interviews, Lee stated he was further inspired by seeing a spider climb up a wall—adding in his autobiography that he has told that story so often he has become unsure of whether or not this is true.[note 1] Although at the time teenage superheroes were usually given names ending with "boy", Lee says he chose "Spider-Man" because he wanted the character to age as the series progressed, and moreover felt the name "Spider-Boy" would have made the character sound inferior to other superheroes.[16] At that time Lee had to get only the consent of Marvel publisher Martin Goodman for the character's approval. In a 1986 interview, Lee described in detail his arguments to overcome Goodman's objections.[note 2] Goodman eventually agreed to a Spider-Man tryout in what Lee in numerous interviews recalled as what would be the final issue of the science-fiction and supernatural anthology series Amazing Adult Fantasy, which was renamed Amazing Fantasy for that single issue, #15 (cover-dated August 1962, on sale June 5, 1962).[17] In particular, Lee stated that the fact that it had already been decided that Amazing Fantasy would be cancelled after issue #15 was the only reason Goodman allowed him to use Spider-Man.[16] While this was indeed the final issue, its editorial page anticipated the comic continuing and that "The Spiderman [sic] ... will appear every month in Amazing."[17][18]		Regardless, Lee received Goodman's approval for the name Spider-Man and the "ordinary teen" concept, and approached artist Jack Kirby. As comics historian Greg Theakston recounts, Kirby told Lee about an unpublished character on which he had collaborated with Joe Simon in the 1950s, in which an orphaned boy living with an old couple finds a magic ring that granted him superhuman powers. Lee and Kirby "immediately sat down for a story conference", Theakston writes, and Lee afterward directed Kirby to flesh out the character and draw some pages.[19] Steve Ditko would be the inker.[note 3] When Kirby showed Lee the first six pages, Lee recalled, "I hated the way he was doing it! Not that he did it badly—it just wasn't the character I wanted; it was too heroic".[19]:12 Lee turned to Ditko, who developed a visual style Lee found satisfactory. Ditko recalled:		One of the first things I did was to work up a costume. A vital, visual part of the character. I had to know how he looked ... before I did any breakdowns. For example: A clinging power so he wouldn't have hard shoes or boots, a hidden wrist-shooter versus a web gun and holster, etc. ... I wasn't sure Stan would like the idea of covering the character's face but I did it because it hid an obviously boyish face. It would also add mystery to the character....[20]		Although the interior artwork was by Ditko alone, Lee rejected Ditko's cover art and commissioned Kirby to pencil a cover that Ditko inked.[17] As Lee explained in 2010, "I think I had Jack sketch out a cover for it because I always had a lot of confidence in Jack's covers."[21]		In an early recollection of the character's creation, Ditko described his and Lee's contributions in a mail interview with Gary Martin published in Comic Fan #2 (Summer 1965): "Stan Lee thought the name up. I did costume, web gimmick on wrist & spider signal."[22] At the time, Ditko shared a Manhattan studio with noted fetish artist Eric Stanton, an art-school classmate who, in a 1988 interview with Theakston, recalled that although his contribution to Spider-Man was "almost nil", he and Ditko had "worked on storyboards together and I added a few ideas. But the whole thing was created by Steve on his own... I think I added the business about the webs coming out of his hands."[19]:14		Kirby disputed Lee's version of the story, and claimed Lee had minimal involvement in the character's creation. According to Kirby, the idea for Spider-Man had originated with Kirby and Joe Simon, who in the 1950s had developed a character called the Silver Spider for the Crestwood Publications comic Black Magic, who was subsequently not used.[note 4] Simon, in his 1990 autobiography, disputed Kirby's account, asserting that Black Magic was not a factor, and that he (Simon) devised the name "Spider-Man" (later changed to "The Silver Spider"), while Kirby outlined the character's story and powers. Simon later elaborated that his and Kirby's character conception became the basis for Simon's Archie Comics superhero the Fly.[23] Artist Steve Ditko stated that Lee liked the name Hawkman from DC Comics, and that "Spider-Man" was an outgrowth of that interest.[20]		Simon concurred that Kirby had shown the original Spider-Man version to Lee, who liked the idea and assigned Kirby to draw sample pages of the new character but disliked the results—in Simon's description, "Captain America with cobwebs".[note 5] Writer Mark Evanier notes that Lee's reasoning that Kirby's character was too heroic seems unlikely—Kirby still drew the covers for Amazing Fantasy #15 and the first issue of The Amazing Spider-Man. Evanier also disputes Kirby's given reason that he was "too busy" to draw Spider-Man in addition to his other duties since Kirby was, said Evanier, "always busy".[24]:127 Neither Lee's nor Kirby's explanation explains why key story elements like the magic ring were dropped; Evanier states that the most plausible explanation for the sudden change was that Goodman, or one of his assistants, decided that Spider-Man as drawn and envisioned by Kirby was too similar to the Fly.[24]:127		Author and Ditko scholar Blake Bell writes that it was Ditko who noted the similarities to the Fly. Ditko recalled that, "Stan called Jack about the Fly", adding that "[d]ays later, Stan told me I would be penciling the story panel breakdowns from Stan's synopsis". It was at this point that the nature of the strip changed. "Out went the magic ring, adult Spider-Man and whatever legend ideas that Spider-Man story would have contained". Lee gave Ditko the premise of a teenager bitten by a spider and developing powers, a premise Ditko would expand upon to the point he became what Bell describes as "the first work for hire artist of his generation to create and control the narrative arc of his series". On the issue of the initial creation, Ditko states, "I still don't know whose idea was Spider-Man".[25] Kirby noted in a 1971 interview that it was Ditko who "got Spider-Man to roll, and the thing caught on because of what he did".[26] Lee, while claiming credit for the initial idea, has acknowledged Ditko's role, stating, "If Steve wants to be called co-creator, I think he deserves [it]".[27] He has further commented that Ditko's costume design was key to the character's success; since the costume completely covers Spider-Man's body, people of all races could visualize themselves inside the costume and thus more easily identify with the character.[16] Writer Al Nickerson believes "that Stan Lee and Steve Ditko created the Spider-Man that we are familiar with today [but that] ultimately, Spider-Man came into existence, and prospered, through the efforts of not just one or two, but many, comic book creators".[28]		A few months after Spider-Man's introduction, publisher Goodman reviewed the sales figures for that issue and was shocked to find it was one of the nascent Marvel's highest-selling comics.[29]:97 A solo ongoing series followed, beginning with The Amazing Spider-Man #1 (cover-dated March 1963). The title eventually became Marvel's top-selling series[8]:211 with the character swiftly becoming a cultural icon; a 1965 Esquire poll of college campuses found that college students ranked Spider-Man and fellow Marvel hero the Hulk alongside Bob Dylan and Che Guevara as their favorite revolutionary icons. One interviewee selected Spider-Man because he was "beset by woes, money problems, and the question of existence. In short, he is one of us."[8]:223 Following Ditko's departure after issue #38 (July 1966), John Romita, Sr. replaced him as penciler and would draw the series for the next several years. In 1968, Romita would also draw the character's extra-length stories in the comics magazine The Spectacular Spider-Man, a proto-graphic novel designed to appeal to older readers. It only lasted for two issues, but it represented the first Spider-Man spin-off publication, aside from the original series' summer annuals that began in 1964.[30]		An early 1970s Spider-Man story led to the revision of the Comics Code. Previously, the Code forbade the depiction of the use of illegal drugs, even negatively. However, in 1970, the Nixon administration's Department of Health, Education, and Welfare asked Stan Lee to publish an anti-drug message in one of Marvel's top-selling titles.[8]:239 Lee chose the top-selling The Amazing Spider-Man; issues #96–98 (May–July 1971) feature a story arc depicting the negative effects of drug use. In the story, Peter Parker's friend Harry Osborn becomes addicted to pills. When Spider-Man fights the Green Goblin (Norman Osborn, Harry's father), Spider-Man defeats the Green Goblin, by revealing Harry's drug addiction. While the story had a clear anti-drug message, the Comics Code Authority refused to issue its seal of approval. Marvel nevertheless published the three issues without the Comics Code Authority's approval or seal. The issues sold so well that the industry's self-censorship was undercut and the Code was subsequently revised.[8]:239		In 1972, a second monthly ongoing series starring Spider-Man began: Marvel Team-Up, in which Spider-Man was paired with other superheroes and villains.[31] From that point on there have generally been at least two ongoing Spider-Man series at any time. In 1976, his second solo series, Peter Parker, the Spectacular Spider-Man began running parallel to the main series.[32] A third series featuring Spider-Man, Web of Spider-Man, launched in 1985 to replace Marvel Team-Up.[33] The launch of a fourth monthly title in 1990, the "adjectiveless" Spider-Man (with the storyline "Torment"), written and drawn by popular artist Todd McFarlane, debuted with several different covers, all with the same interior content. The various versions combined sold over 3 million copies, an industry record at the time. Several limited series, one-shots, and loosely related comics have also been published, and Spider-Man makes frequent cameos and guest appearances in other comic series.[32][34] In 1996 The Sensational Spider-Man was created to replace Web of Spider-Man.[35]		In 1998 writer-artist John Byrne revamped the origin of Spider-Man in the 13-issue limited series Spider-Man: Chapter One (December 1998 – October 1999), similar to Byrne's adding details and some revisions to Superman's origin in DC Comics' The Man of Steel.[36] At the same time the original The Amazing Spider-Man was ended with issue #441 (November 1998), and The Amazing Spider-Man was restarted with vol. 2, #1 (January 1999).[37] In 2003 Marvel reintroduced the original numbering for The Amazing Spider-Man and what would have been vol. 2, #59 became issue #500 (December 2003).[37]		When primary series The Amazing Spider-Man reached issue #545 (December 2007), Marvel dropped its spin-off ongoing series and instead began publishing The Amazing Spider-Man three times monthly, beginning with #546–548 (all January 2008).[38] The three times monthly scheduling of The Amazing Spider-Man lasted until November 2010 when the comic book was increased from 22 pages to 30 pages each issue and published only twice a month, beginning with #648–649 (both November 2010).[39][40] The following year, Marvel launched Avenging Spider-Man as the first spinoff ongoing series in addition to the still twice monthly The Amazing Spider-Man since the previous ones were cancelled at the end of 2007.[38] The Amazing series temporarily ended with issue #700 in December 2012, and was replaced by The Superior Spider-Man, which had Doctor Octopus serve as the new Spider-Man, having taken over Peter Parker's body. Superior was an enormous commercial success for Marvel,[41] and ran for 31-issue before the real Peter Parker returned in a newly relaunched The Amazing Spider-Man #1 in April 2014.[42]		In Forest Hills, Queens, New York,[43] Midtown High School student Peter Parker is a science-whiz orphan living with his Uncle Ben and Aunt May. As depicted in Amazing Fantasy #15 (August 1962), he is bitten by a radioactive spider (erroneously classified as an insect in the panel) at a science exhibit and "acquires the agility and proportionate strength of an arachnid".[44] Along with super strength, Parker gains the ability to adhere to walls and ceilings. Through his native knack for science, he develops a gadget that lets him fire adhesive webbing of his own design through small, wrist-mounted barrels. Initially seeking to capitalize on his new abilities, Parker dons a costume and, as "Spider-Man", becomes a novelty television star. However, "He blithely ignores the chance to stop a fleeing thief, [and] his indifference ironically catches up with him when the same criminal later robs and kills his Uncle Ben." Spider-Man tracks and subdues the killer and learns, in the story's next-to-last caption, "With great power there must also come—great responsibility!"[45]		Despite his superpowers, Parker struggles to help his widowed aunt pay rent, is taunted by his peers—particularly football star Flash Thompson—and, as Spider-Man, engenders the editorial wrath of newspaper publisher J. Jonah Jameson.[46][47] As he battles his enemies for the first time,[48] Parker finds juggling his personal life and costumed adventures difficult. In time, Peter graduates from high school,[49] and enrolls at Empire State University (a fictional institution evoking the real-life Columbia University and New York University),[50] where he meets roommate and best friend Harry Osborn, and girlfriend Gwen Stacy,[51] and Aunt May introduces him to Mary Jane Watson.[48][52][53] As Peter deals with Harry's drug problems, and Harry's father is revealed to be Spider-Man's nemesis the Green Goblin, Peter even attempts to give up his costumed identity for a while.[54][55] Gwen Stacy's father, New York City Police detective captain George Stacy is accidentally killed during a battle between Spider-Man and Doctor Octopus (#90, November 1970).[56]		In issue #121 (June 1973),[48] the Green Goblin throws Gwen Stacy from a tower of either the Brooklyn Bridge (as depicted in the art) or the George Washington Bridge (as given in the text).[57][58] She dies during Spider-Man's rescue attempt; a note on the letters page of issue #125 states: "It saddens us to say that the whiplash effect she underwent when Spidey's webbing stopped her so suddenly was, in fact, what killed her."[59] The following issue, the Goblin appears to kill himself accidentally in the ensuing battle with Spider-Man.[60]		Working through his grief, Parker eventually develops tentative feelings toward Watson, and the two "become confidants rather than lovers".[61] A romantic relationship eventually develops, with Parker proposing to her in issue #182 (July 1978), and being turned down an issue later.[62] Parker went on to graduate from college in issue #185,[48] and becomes involved with the shy Debra Whitman and the extroverted, flirtatious costumed thief Felicia Hardy, the Black Cat,[63] whom he meets in issue #194 (July 1979).[48]		From 1984 to 1988, Spider-Man wore a black costume with a white spider design on his chest. The new costume originated in the Secret Wars limited series, on an alien planet where Spider-Man participates in a battle between Earth's major superheroes and villains.[64] He continues wearing the costume when he returns, starting in The Amazing Spider-Man #252. The change to a longstanding character's design met with controversy, "with many hardcore comics fans decrying it as tantamount to sacrilege. Spider-Man's traditional red and blue costume was iconic, they argued, on par with those of his D.C. rivals Superman and Batman."[65] The creators then revealed the costume was an alien symbiote which Spider-Man is able to reject after a difficult struggle,[66] though the symbiote returns several times as Venom for revenge.[48]		Parker proposes to Watson a second time in The Amazing Spider-Man #290 (July 1987), and she accepts two issues later, with the wedding taking place in The Amazing Spider-Man Annual #21 (1987). It was promoted with a real-life mock wedding using models, including Tara Shannon as Watson,[67] with Stan Lee officiating at the June 5, 1987, event at Shea Stadium.[68][69] However, David Michelinie, who scripted based on a plot by editor-in-chief Jim Shooter, said in 2007, "I didn't think they actually should [have gotten] married. ... I had actually planned another version, one that wasn't used."[68]		In a controversial storyline, Peter becomes convinced that Ben Reilly, the Scarlet Spider (a clone of Peter created by his college professor Miles Warren) is the real Peter Parker, and that he, Peter, is the clone. Peter gives up the Spider-Man identity to Reilly for a time, until Reilly is killed by the returning Green Goblin and revealed to be the clone after all.[70] In stories published in 2005 and 2006 (such as "The Other"), he develops additional spider-like abilities including biological web-shooters, toxic stingers that extend from his forearms, the ability to stick individuals to his back, enhanced Spider-sense and night vision, and increased strength and speed. Peter later becomes a member of the New Avengers, and reveals his civilian identity to the world,[71] increasing his already numerous problems. His marriage to Mary Jane and public unmasking are later erased in another controversial[72] storyline "One More Day", in a Faustian bargain with the demon Mephisto that results in several other adjustments to the timeline, including the resurrection of Harry Osborn and the return of Spider-Man's traditional tools and powers.[73]		That storyline came at the behest of editor-in-chief Joe Quesada, who said, "Peter being single is an intrinsic part of the very foundation of the world of Spider-Man".[72] It caused unusual public friction between Quesada and writer J. Michael Straczynski, who "told Joe that I was going to take my name off the last two issues of the [story] arc" but was talked out of doing so.[74] At issue with Straczynski's climax to the arc, Quesada said, was		...that we didn't receive the story and methodology to the resolution that we were all expecting. What made that very problematic is that we had four writers and artists well underway on [the sequel arc] "Brand New Day" that were expecting and needed "One More Day" to end in the way that we had all agreed it would. ... The fact that we had to ask for the story to move back to its original intent understandably made Joe upset and caused some major delays and page increases in the series. Also, the science that Joe was going to apply to the retcon of the marriage would have made over 30 years of Spider-Man books worthless, because they never would have had happened. ...[I]t would have reset way too many things outside of the Spider-Man titles. We just couldn't go there....[74]		Following the "reboot", Parker's identity was no longer known to the general public; however, he revealed it to other superheroes.[75] and others have deduced it. Parker's Aunt May marries J. Jonah Jameson's father, Jay Jameson.[76] Parker became an employee of the think-tank Horizon Labs.[77] In issue #700, the dying supervillain Doctor Octopus swaps bodies with Parker, who remains as a presence in Doctor Octopus's mind,[78] prompting a two-year storyline in the series The Superior Spider-Man in which Peter Parker is absent and Doctor Octopus is Spider-Man. Peter eventually regains control of his body.[79] Following Peter Parker's return, The Amazing Spider-Man was relaunched in April 2014, with Peter Parker becoming a billionaire after the formation of Parker Industries.[80][81] In December 2014, following the Death of Wolverine comic book, Spider-Man became the new headmaster of the Jean Grey School and began appearing more prominently in X-Men stories, taking Wolverine's role in the comic Wolverine and the X-Men.[82]		As one contemporaneous journalist observed, "Spider-Man has a terrible identity problem, a marked inferiority complex, and a fear of women. He is anti-social, [sic] castration-ridden, racked with Oedipal guilt, and accident-prone ... [a] functioning neurotic".[43] Agonizing over his choices, always attempting to do right, he is nonetheless viewed with suspicion by the authorities, who seem unsure as to whether he is a helpful vigilante or a clever criminal.[84]		Notes cultural historian Bradford W. Wright,		Spider-Man's plight was to be misunderstood and persecuted by the very public that he swore to protect. In the first issue of The Amazing Spider-Man, J. Jonah Jameson, publisher of the Daily Bugle, launches an editorial campaign against the "Spider-Man menace." The resulting negative publicity exacerbates popular suspicions about the mysterious Spider-Man and makes it impossible for him to earn any more money by performing. Eventually, the bad press leads the authorities to brand him an outlaw. Ironically, Peter finally lands a job as a photographer for Jameson's Daily Bugle.[8]:212		The mid-1960s stories reflected the political tensions of the time, as early 1960s Marvel stories had often dealt with the Cold War and Communism.[8]:220–223 As Wright observes,		From his high-school beginnings to his entry into college life, Spider-Man remained the superhero most relevant to the world of young people. Fittingly, then, his comic book also contained some of the earliest references to the politics of young people. In 1968, in the wake of actual militant student demonstrations at Columbia University, Peter Parker finds himself in the midst of similar unrest at his Empire State University.... Peter has to reconcile his natural sympathy for the students with his assumed obligation to combat lawlessness as Spider-Man. As a law-upholding liberal, he finds himself caught between militant leftism and angry conservatives.[8]:234–235		A bite from a radioactive spider triggers mutations in Peter Parker's body, granting him superpowers.[85] In the original Lee-Ditko stories, Spider-Man has the ability to cling to walls, superhuman strength, a sixth sense ("spider-sense") that alerts him to danger, perfect balance and equilibrium, as well as superhuman speed and agility.[85] The character was originally conceived by Stan Lee and Steve Ditko as intellectually gifted, but later writers have depicted his intellect at genius level.[86] Academically brilliant, Parker has expertise in the fields of applied science, chemistry, physics, biology, engineering, mathematics, and mechanics. With his talents, he sews his own costume to conceal his identity, and he constructs many devices that complement his powers, most notably mechanical web-shooters.[85] This mechanism ejects an advanced adhesive, releasing web-fluid in a variety of configurations, including a single rope-like strand to swing from, a net to snare or bind enemies, and a simple glob to foul machinery or blind an opponent. He can also weave the web material into simple forms like a shield, a spherical protection or hemispherical barrier, a club, or a hang-glider wing. Other equipment include a light beacon that can either be used as a flashlight or project a "Spider-Signal" design, a specially modified camera that can take pictures automatically, and spider-tracers, which are spider-shaped adhesive homing beacons keyed to his own spider-sense.		Due to Spider-Man's popularity in the mainstream Marvel Universe, publishers have been able to introduce different variations of Spider-Man outside of mainstream comics as well as reimagined stories in many other multiversed spinoffs such as Ultimate Spider-Man, Spider-Man 2099, and Spider-Man: India. Marvel has also made its own parodies of Spider-Man in comics such as Not Brand Echh, which was published in the late 1960s and featured such characters as Peter Pooper alias Spidey-Man,[87] and Peter Porker, the Spectacular Spider-Ham, who appeared in the 1980s. The fictional character has inspired a number of deratives such as a manga version of Spider-Man drawn by Japanese artist Ryoichi Ikegami as well as Hideshi Hino's The Bug Boy, which has been cited as inspired by Spider-Man.[88] Also the French comic Télé-Junior, which published strips based on popular TV series, produced original Spider-Man adventures in the late 1970s; artists included Gérald Forton, who later moved to America and worked for Marvel.[89]		Spider-Man has had a large range of supporting characters introduced in the comics that are essential in the issues and storylines that star him. After his parents died, Peter Parker was raised by his loving aunt, May Parker, and his uncle and father figure, Ben Parker. After Uncle Ben is murdered by a burglar, Aunt May is virtually Peter's only family, and she and Peter are very close.[44]		J. Jonah Jameson is depicted as the publisher of the Daily Bugle and is Peter Parker's boss and as a harsh critic of Spider-Man, always saying negative things about the superhero in the newspaper. Despite his role as Jameson's publishing editor and confidant Robbie Robertson is always depicted as a supporter of both Peter Parker and Spider-Man.[46]		Eugene "Flash" Thompson is commonly depicted as Parker's high school tormentor and bully, but in later comic issues he becomes a friend to Peter.[46] Meanwhile, Harry Osborn, son of Norman Osborn, is most commonly recognized as Peter's best friend but has also been depicted sometimes as his rival in the comics.[48]		Peter Parker's romantic interests range between his first crush, the fellow high-school student Liz Allan,[46] to having his first date with Betty Brant,[90] the secretary to the Daily Bugle newspaper publisher J. Jonah Jameson. After his breakup with Betty Brant, Parker eventually falls in love with his college girlfriend Gwen Stacy,[48][51] daughter of New York City Police Department detective captain George Stacy, both of whom are later killed by supervillain enemies of Spider-Man.[56] Mary Jane Watson eventually became Peter's best friend and then his wife.[68] Felicia Hardy, the Black Cat, is a reformed cat burglar who had been Spider-Man's sole superhuman girlfriend and partner at one point.[63]		Writers and artists over the years have established a rogues gallery of supervillains to face Spider-Man. In comics and in other media. As with the hero, the majority of the villains' powers originate with scientific accidents or the misuse of scientific technology, and many have animal-themed costumes or powers.[note 6] Examples are listed down below in the ordering of their original chronological appearance:      Indicates a group team.		Todd McFarlane[137]		Unlike a lot of well known rivalries in comics book depictions. Spider-Man is cited to have more than one archenemy and it can be debated or disputed as to which one is worse:[142]		In The Creation of Spider-Man, comic book writer-editor and historian Paul Kupperberg calls the character's superpowers "nothing too original"; what was original was that outside his secret identity, he was a "nerdy high school student".[151]:5 Going against typical superhero fare, Spider-Man included "heavy doses of soap-opera and elements of melodrama". Kupperberg feels that Lee and Ditko had created something new in the world of comics: "the flawed superhero with everyday problems". This idea spawned a "comics revolution".[151]:6 The insecurity and anxieties in Marvel's early 1960s comic books such as The Amazing Spider-Man, The Incredible Hulk, and X-Men ushered in a new type of superhero, very different from the certain and all-powerful superheroes before them, and changed the public's perception of them.[152] Spider-Man has become one of the most recognizable fictional characters in the world, and has been used to sell toys, games, cereal, candy, soap, and many other products.[153]		Spider-Man has become Marvel's flagship character, and has often been used as the company mascot. When Marvel became the first comic book company to be listed on the New York Stock Exchange in 1991, the Wall Street Journal announced "Spider-Man is coming to Wall Street"; the event was in turn promoted with an actor in a Spider-Man costume accompanying Stan Lee to the Stock Exchange.[8]:254 Since 1962, hundreds of millions of comics featuring the character have been sold around the world.[154] Spider-Man is the world's most profitable superhero.[155] In 2014, global retail sales of licensed products related to Spider-Man reached approximately $1.3 billion.[156] Comparatively, this amount exceeds the global licensing revenue of Batman, Superman, and the Avengers combined.[155]		Spider-Man joined the Macy's Thanksgiving Day Parade from 1987 to 1998 as one of the balloon floats,[157] designed by John Romita Sr.,[158] one of the character's signature artists. A new, different Spider-Man balloon float is scheduled to appear from at least 2009 to 2011.[157]		When Marvel wanted to issue a story dealing with the immediate aftermath of the September 11 attacks, the company chose the December 2001 issue of The Amazing Spider-Man.[159] In 2006, Spider-Man garnered major media coverage with the revelation of the character's secret identity,[160] an event detailed in a full page story in the New York Post before the issue containing the story was even released.[161]		In 2008, Marvel announced plans to release a series of educational comics the following year in partnership with the United Nations, depicting Spider-Man alongside UN Peacekeeping Forces to highlight UN peacekeeping missions.[162] A BusinessWeek article listed Spider-Man as one of the top ten most intelligent fictional characters in American comics.[163]		Rapper Eminem has cited Spider-Man as one of his favorite comic book superheroes.[164][165]		In 2015, the Supreme Court of the United States decided Kimble v. Marvel Entertainment, LLC, a case concerning royalties on a patent for an imitation web-shooter. The opinion for the Court, by Justice Elena Kagan, included several Spider-Man references, concluding with the statement that "with great power there must also come—great responsibility".[166]		Spider-Man was declared the number one superhero on Bravo's Ultimate Super Heroes, Vixens, and Villains TV series in 2005.[168] Empire magazine placed him as the fifth-greatest comic book character of all time.[169] Wizard magazine placed Spider-Man as the third greatest comic book character on their website.[170] In 2011, Spider-Man placed third on IGN's Top 100 Comic Book Heroes of All Time, behind DC Comics characters Superman and Batman.[167] and sixth in their 2012 list of "The Top 50 Avengers".[171] In 2014, IGN identified Spider-Man the greatest Marvel Comics character of all time.[172] A 2015 poll at Comic Book Resources named Spider-Man the greatest Marvel character of all time.[173] IGN described him as the common everyman that represents many normal people but also noting his uniqueness compared to many top-tiered superheroes with his many depicted flaws as a superhero. IGN noted that despite being one of the most tragic superheroes of all time that he is "one of the most fun and snarky superheroes in existence."[167] Empire noted and praised that despite the many tragedies that Spider-Man faces that he retains his sense of humour at all times with his witty wisecracks. The magazine website aspraised the depiction of his "iconic" superhero poses describing it as "a top artist's dream".[170]		George Marston of Newsarama placed Spider-Man's origin story as the greatest origin story of all time opining that "Spider-Man's origin combines all of the most classic aspects of pathos, tragedy and scientific wonder into the perfect blend for a superhero origin."[174]		Real-life people who have been compared to Spider-Man for their climbing feats include:		From the character's inception, Spider-Man stories have won numerous awards, including:		Spider-Man has appeared in comics, cartoons, films, video games, coloring books, novels, records, and children's books.[153] On television, he first starred in the ABC animated series Spider-Man (1967–1970);[177] Spidey Super Stories (1974-1977) on PBS; and the CBS live action series The Amazing Spider-Man (1978–1979), starring Nicholas Hammond. Other animated series featuring the superhero include the syndicated Spider-Man (1981–1982), Spider-Man and His Amazing Friends (1981–1983), Fox Kids' Spider-Man (1994–1998), Spider-Man Unlimited (1999–2000), Spider-Man: The New Animated Series (2003), The Spectacular Spider-Man (2008–2009), and Ultimate Spider-Man (2012-2017).[178]		A tokusatsu series featuring Spider-Man was produced by Toei and aired in Japan. It is commonly referred to by its Japanese pronunciation "Supaidā-Man".[179] Spider-Man also appeared in other print forms besides the comics, including novels, children's books, and the daily newspaper comic strip The Amazing Spider-Man, which debuted in January 1977, with the earliest installments written by Stan Lee and drawn by John Romita, Sr.[180] Spider-Man has been adapted to other media including games, toys, collectibles, and miscellaneous memorabilia, and has appeared as the main character in numerous computer and video games on over 15 gaming platforms.		Spider-Man was featured in a trilogy of live-action films directed by Sam Raimi and starring Tobey Maguire as the titular superhero. The first Spider-Man film of the trilogy was released on May 3, 2002; followed by Spider-Man 2 (2004) and Spider-Man 3 (2007). A third sequel was originally scheduled to be released in 2011, however Sony later decided to reboot the franchise with a new director and cast. The reboot, titled The Amazing Spider-Man, was released on July 3, 2012; directed by Marc Webb and starring Andrew Garfield as the new Spider-Man.[181][182][183] It was followed by The Amazing Spider-Man 2 (2014).[184][185] In 2015, Sony and Disney made a deal for Spider-Man to appear in the Marvel Cinematic Universe.[186] Tom Holland made his debut as Spider-Man in the MCU film Captain America: Civil War (2016), before later starring in Spider-Man: Homecoming (2017); directed by Jon Watts.[187][188] Holland has been confirmed to reprise his role as Spider-Man for the upcoming Avengers: Infinity War (2018).[189]		A Broadway musical, Spider-Man: Turn Off the Dark, began previews on November 14, 2010 at the Foxwoods Theatre on Broadway, with the official opening night on June 14, 2011.[190][191] The music and lyrics were written by Bono and The Edge of the rock group U2, with a book by Julie Taymor, Glen Berger, Roberto Aguirre-Sacasa.[192] Turn Off the Dark is currently the most expensive musical in Broadway history, costing an estimated $70 million.[193] In addition, the show's unusually high running costs are reported to be about $1.2 million per week.[194]		
Silicon Valley is a nickname for the southern portion of the San Francisco Bay Area, in the northern part of the U.S. state of California. The "valley" in its name refers to the Santa Clara Valley in Santa Clara County, which includes the city of San Jose and surrounding cities and towns, where the region has been traditionally centered. The region has expanded to include the southern half of the San Francisco Peninsula in San Mateo County, and southern portions of the East Bay in Alameda County.		The word "silicon" originally referred to the large number of silicon chip innovators and manufacturers in the region, but the area is now the home to many of the world's largest high-tech corporations, including the headquarters of 39 businesses in the Fortune 1000, and thousands of startup companies. Silicon Valley also accounts for one-third of all of the venture capital investment in the United States, which has helped it to become a leading hub and startup ecosystem for high-tech innovation and scientific development. It was in the Valley that the silicon-based integrated circuit, the microprocessor, and the microcomputer, among other key technologies, were developed. As of 2013, the region employed about a quarter of a million information technology workers.[1]		As more high-tech companies were established across the Santa Clara Valley, and then north towards the Bay Area's two other major cities, San Francisco and Oakland, the "Silicon Valley" has come to have two definitions: a geographic one, referring to Santa Clara County, and a metonymical one, referring to all high-tech businesses in the Bay Area or even in the United States. The term is now generally used as a synecdoche for the American high-technology economic sector. The name also became a global synonym for leading high-tech research and enterprises, and thus inspired similar named locations, as well as research parks and technology centers with a comparable structure all around the world.						"Perhaps the strongest thread that runs through the Valley's past and present is the drive to 'play' with novel technology, which, when bolstered by an advanced engineering degree and channeled by astute management, has done much to create the industrial powerhouse we see in the Valley today." (Timothy J. Sturgeon)[2]:44		The first published use of Silicon Valley is credited to Don Hoefler, a friend of local entrepreneur Ralph Vaerst's who suggested the phrase to him. Hoefler used the phrase as the title of a series of articles in the weekly trade newspaper Electronic News.[3] The series, titled "Silicon Valley in the USA", began in the paper's January 11, 1971, issue. The term gained widespread use in the early 1980s, at the time of the introduction of the IBM PC and numerous related hardware and software products to the consumer market. The silicon part of the name refers to the high concentration of companies involved in the making of semiconductors (silicon is used to create most semiconductors commercially) and computer industries that were concentrated in the area. These firms slowly replaced the orchards and the fruits which gave the area its initial nickname—the "Valley of Heart's Delight."[citation needed]		Silicon Valley was born through several contributing factors intersecting, including a skilled STEM research base housed in area universities, plentiful venture capital, and steady U.S. Department of Defense spending. Stanford University leadership was especially important in the valley's early development. Together these elements formed the basis of its growth and success.[4]		The first ship-to-shore wireless telegraph message to be received in the US was from the San Francisco lightship outside the Golden Gate, signaling the return of the American fleet from the Philippines after their victory in the Spanish–American War.[when?] The ship had been outfitted with a wireless telegraph transmitter by a local newspaper, so that they could prepare a celebration on the return of the American sailors.[5] Local historian Clyde Arbuckle states in Clyde Arbuckle's History of San Jose[6] that "California first heard the click of a telegraph key on September 11, 1853. It marked completion of an enterprise begun by a couple of San Francisco Merchants' Exchange members named George Sweeney and Theodore E. Baugh…" He says, "In 1849, the gentleman established a wigwag telegraph station a top a high hill overlooking Portsmouth Squares for signaling arriving ships… The operator at the first station caught these signals by telescope and relayed them to the Merchant's Exchange for the waiting business community." Arbuckle points to the historic significance the Merchants Exchange Building (San Francisco) and Telegraph Hill, San Francisco when he goes on to say "The first station gave the name Telegraph to the hill on which it was located. It was known as the Inner Station; the second, as the Outer Station. Both used their primitive mode of communication until Messrs. Sweeney and Baugh connected the Outer Station directly with the Merchants's Exchange by electric telegraph Wire."		According to Arbuckle (p. 380-381) Sweeney and Baugh's line was strictly an intra-city, San Francisco-based service; that is until California State Telegraph Company enfranchised on May 3, 1852; whereas, O.E. Allen and C. Burnham led the way to "build a line from San Francisco to Marysville via San Jose, Stockton, and Sacramento." Delays to construction occurred until September 1853; but, "…San Jose became the first station on the line when the wire arrived here on October 15. The line was completed when [James] Gamble's northbound crew met a similar crew working southward from Marysville on October 24."		The Bay Area had long been a major site of United States Navy research and technology. In 1909, Charles Herrold started the first radio station in the United States with regularly scheduled programming in San Jose. Later that year, Stanford University graduate Cyril Elwell purchased the U.S. patents for Poulsen arc radio transmission technology and founded the Federal Telegraph Corporation (FTC) in Palo Alto. Over the next decade, the FTC created the world's first global radio communication system, and signed a contract with the Navy in 1912.[2]		In 1933, Air Base Sunnyvale, California, was commissioned by the United States Government for use as a Naval Air Station (NAS) to house the airship USS Macon in Hangar One. The station was renamed NAS Moffett Field, and between 1933 and 1947, U.S. Navy blimps were based there.[7] A number of technology firms had set up shop in the area around Moffett Field to serve the Navy. When the Navy gave up its airship ambitions and moved most of its west coast operations to San Diego, the National Advisory Committee for Aeronautics (NACA, forerunner of NASA) took over portions of Moffett Field for aeronautics research. Many of the original companies stayed, while new ones moved in. The immediate area was soon filled with aerospace firms, such as Lockheed.		The Bay Area was an early center of ham radio with about 10% of the operators in the United States. William Eitel, Jack McCullough, and Charles Litton, who together pioneered vacuum tube manufacturing in the Bay Area, were hobbyists with training in technology gained locally who participated in development of shortwave radio by the ham radio hobby. High frequency, and especially, Very high frequency, VHF, transmission in the 10 meter band, required higher quality power tubes than were manufactured by the consortium of RCA, Western Electric, General Electric, Westinghouse which controlled vacuum tube manufacture. Litton, founder of Litton Industries, pioneered manufacturing techniques which resulted in award of wartime contracts to manufacture transmitting tubes for radar to Eitel-McCullough, a San Bruno firm, which manufactured power-grid tubes for radio amateurs and aircraft radio equipment.[8]		A union organizing drive in 1939–40 at Eitel-McCullough by the strong Bay Area labor movement was fought off by adoption of a strategy of welfare capitalism which included pensions and other generous benefits, profit sharing, and such extras as a medical clinic and a cafeteria. An atmosphere of cooperation and collaboration was established.[9] Successes have been few and far between[10] for union organizing drives by UE and others in subsequent years.[11]		On October 4, 1957 the Soviet Union launched the first space satellite, Sputnik, which sparked fear that the Soviet Union was pulling ahead technologically. After President Eisenhower signed the National Aeronautics and Space Act (NASA), he turned to Fairchild Semiconductor, then the only company in the world that was able to make transistors. The president funded Fairchild's project, which was highly successful.[12]		Stanford University, its affiliates, and graduates have played a major role in the development of this area.[13] Some examples include the work of Lee De Forest with his invention of a pioneering vacuum tube called the Audion and the oscilloscopes of Hewlett-Packard.		A very powerful sense of regional solidarity accompanied the rise of Silicon Valley. From the 1890s, Stanford University's leaders saw its mission as service to the West and shaped the school accordingly. At the same time, the perceived exploitation of the West at the hands of eastern interests fueled booster-like attempts to build self-sufficient indigenous local industry. Thus, regionalism helped align Stanford's interests with those of the area's high-tech firms for the first fifty years of Silicon Valley's development.[14]		During the 1940s and 1950s, Frederick Terman, as Stanford's dean of engineering and provost, encouraged faculty and graduates to start their own companies. He is credited with nurturing Hewlett-Packard, Varian Associates, and other high-tech firms, until what would become Silicon Valley grew up around the Stanford campus. Terman is often called "the father of Silicon Valley".[15]		In 1956, William Shockley, the creator of the transistor, moved from New Jersey to Mountain View, California, to start Shockley Semiconductor Laboratory to live closer to his ailing mother in Palo Alto. Shockley's work served as the basis for many electronic developments for decades.[16][17]		During 1955–85, solid state technology research and development at Stanford University followed three waves of industrial innovation made possible by support from private corporations, mainly Bell Telephone Laboratories, Shockley Semiconductor, Fairchild Semiconductor, and Xerox PARC. In 1969, the Stanford Research Institute (now SRI International), operated one of the four original nodes that comprised ARPANET, predecessor to the Internet.[18]		After World War II, universities were experiencing enormous demand due to returning students. To address the financial demands of Stanford's growth requirements, and to provide local employment opportunities for graduating students, Frederick Terman proposed the leasing of Stanford's lands for use as an office park, named the Stanford Industrial Park (later Stanford Research Park) in the year 1951. Leases were limited to high technology companies. Its first tenant was Varian Associates, founded by Stanford alumni in the 1930s to build military radar components. However, Terman also found venture capital for civilian technology start-ups. One of the major success stories was Hewlett-Packard. Founded in Packard's garage by Stanford graduates William Hewlett and David Packard, Hewlett-Packard moved its offices into the Stanford Research Park shortly after 1953. In 1954, Stanford created the Honors Cooperative Program to allow full-time employees of the companies to pursue graduate degrees from the University on a part-time basis. The initial companies signed five-year agreements in which they would pay double the tuition for each student in order to cover the costs. Hewlett-Packard has become the largest personal computer manufacturer in the world, and transformed the home printing market when it released the first thermal drop-on-demand ink jet printer in 1984.[19] Other early tenants included Eastman Kodak, General Electric, and Lockheed.[20]		In 1953, William Shockley left Bell Labs in a disagreement over the handling of the invention of the transistor. After returning to California Institute of Technology for a short while, Shockley moved to Mountain View, California, in 1956, and founded Shockley Semiconductor Laboratory. Unlike many other researchers who used germanium as the semiconductor material, Shockley believed that silicon was the better material for making transistors. Shockley intended to replace the current transistor with a new three-element design (today known as the Shockley diode), but the design was considerably more difficult to build than the "simple" transistor. In 1957, Shockley decided to end research on the silicon transistor. As a result of Shockley's abusive management style, eight engineers left the company to form Fairchild Semiconductor; Shockley referred to them as the "traitorous eight". Two of the original employees of Fairchild Semiconductor, Robert Noyce and Gordon Moore, would go on to found Intel.[21][22]		April 23, 1963 J.C.R. Licklider, the first director of the Information Processing Techniques Office (IPTO) at The Pentagon's ARPA issued an office memorandum rescheduling a meeting in Palo Alto addressed to "Members and Affiliates of the Intergalactic Computer Network".[23][24] regarding his vision of a computer network which he “imagined as an electronic commons open to all, ‘the main and essential medium of informational interaction for governments, institutions, corporations, and individuals.’”[25][26] As head of IPTO from 1962 to 1964, “Licklider initiated three of the most important developments in information technology: the creation of computer science departments at several major universities, time-sharing, and networking.”[26] By the late 1960s, his promotion of the concept had inspired a primitive version of his vision called ARPANET, which expanded into a network of networks in the 1970s that became the Internet.[25]		The Immigration and Nationality Act of 1965 and other factors such as the mass exodus by Vietnamese boat people resulted in significant immigration, particularly by Asians, Latinos, and Portuguese, to Silicon Valley where they contributed to both the high-tech and production workforce.[27] The Asian-American population in Santa Clara County rose from 43,000 in 1970 to 430,000 in 2000. During the same period the Latino population grew to 24% in the county and 30% in San Jose. The African-American population in the county remained steady but grew slightly to about 5%.[28] Expansion of the H-1B visa in 1990 also played a role.[29]		In April 1974, Intel released the Intel 8080,[30] a "computer on a chip", "the first truly usable microprocessor". A microprocessor incorporates the functions of a computer's central processing unit (CPU) on a single integrated circuit (IC).[31]		The Homebrew Computer Club was an informal group of electronic enthusiasts and technically minded hobbyists who gathered to trade parts, circuits, and information pertaining to DIY construction of computing devices.[32] It was started by Gordon French and Fred Moore who met at the Community Computer Center in Menlo Park. They both were interested in maintaining a regular, open forum for people to get together to work on making computers more accessible to everyone.[33]		The first meeting was held as of March 1975 at French's garage in Menlo Park, San Mateo County, California; which was on occasion of the arrival of the MITS Altair microcomputer, the first unit sent to the area for review by People's Computer Company. Steve Wozniak and Steve Jobs credit that first meeting with inspiring them to design the original Apple I and (successor) Apple II computers. As a result, the first preview of the Apple I was given at the Homebrew Computer Club.[34] Subsequent meetings were held at an auditorium at the Stanford Linear Accelerator Center.[35]		By the early 1970s, there were many semiconductor companies in the area, computer firms using their devices, and programming and service companies serving both. Industrial space was plentiful and housing was still inexpensive. The growth was fueled by the emergence of the venture capital industry on Sand Hill Road, beginning with Kleiner Perkins Caufield & Byers and Sequoia Capital in 1972; the availability of venture capital exploded after the successful $1.3 billion IPO of Apple Computer in December 1980.		In 1980, Intelligent Machines Journal -a hobbyist journal- changed its name to InfoWorld, and, with offices in Palo Alto, began covering the explosive emergence of the microcomputer industry in the valley.[36]		Although semiconductors are still a major component of the area's economy, Silicon Valley has been most famous in recent years for innovations in software and Internet services. Silicon Valley has significantly influenced computer operating systems, software, and user interfaces.		Using money from NASA, the US Air Force, and ARPA, Doug Engelbart invented the mouse and hypertext-based collaboration tools in the mid-1960s and 1970s while at Stanford Research Institute (now SRI International), first publicly demonstrated in 1968 in what is now known as The Mother of All Demos. Engelbart's Augmentation Research Center at SRI was also involved in launching the ARPANET (precursor to the Internet) and starting the Network Information Center (now InterNIC). Xerox hired some of Engelbart's best researchers beginning in the early 1970s. In turn, in the 1970s and 1980s, Xerox's Palo Alto Research Center (PARC) played a pivotal role in object-oriented programming, graphical user interfaces (GUIs), Ethernet, PostScript, and laser printers.		While Xerox marketed equipment using its technologies, for the most part its technologies flourished elsewhere. The diaspora of Xerox inventions led directly to 3Com and Adobe Systems, and indirectly to Cisco, Apple Computer, and Microsoft. Apple's Macintosh GUI was largely a result of Steve Jobs' visit to PARC and the subsequent hiring of key personnel.[37] Cisco's impetus stemmed from the need to route a variety of protocols over Stanford's campus Ethernet.		Commercial use of the Internet became practical and and grew slowly throughout the early 1990s.		In 1995, commercial use of the Internet grew substantially and the initial wave of internet startups, Amazon.com, eBay, and the predecessor to Craigslist began operations.[38]		Silicon Valley is generally considered to have been the center of the dot-com bubble, which started in the mid-1990s and collapsed after the NASDAQ stock market began to decline dramatically in April 2000. During the bubble era, real estate prices reached unprecedented levels. For a brief time, Sand Hill Road was home to the most expensive commercial real estate in the world, and the booming economy resulted in severe traffic congestion.		After the dot-com crash, Silicon Valley continues to maintain its status as one of the top research and development centers in the world. A 2006 The Wall Street Journal story found that 12 of the 20 most inventive towns in America were in California, and 10 of those were in Silicon Valley.[39] San Jose led the list with 3,867 utility patents filed in 2005, and number two was Sunnyvale, at 1,881 utility patents.[40] Silicon Valley is also home to a significant number of "Unicorn" ventures, referring to startup companies whose valuation has exceeded $1 billion dollars.[41]		Silicon Valley has a social and business ethos that supports innovation and entrepreneurship. Attempts to create "Silicon Valleys" in environments where disruptive innovation does not go over well have a poor track record.[42]		According to a 2008 study by AeA in 2006, Silicon Valley was the third largest high-tech center (cybercity) in the United States, behind the New York metropolitan area and Washington metropolitan area, with 225,300 high-tech jobs. The Bay Area as a whole however, of which Silicon Valley is a part, would rank first with 387,000 high-tech jobs. Silicon Valley has the highest concentration of high-tech workers of any metropolitan area, with 285.9 out of every 1,000 private-sector workers. Silicon Valley has the highest average high-tech salary at $144,800.[43] Largely a result of the high technology sector, the San Jose-Sunnyvale-Santa Clara, CA Metropolitan Statistical Area has the most millionaires and the most billionaires in the United States per capita.[44]		The region is the biggest high-tech manufacturing center in the United States.[45][46] The unemployment rate of the region was 9.4% in January 2009, up from 7.8% in the previous month.[47] Silicon Valley received 41% of all U.S. venture investment in 2011, and 46% in 2012.[48] More traditional industries also recognize the potential of high-tech development, and several car manufacturers have opened offices in Silicon Valley to capitalize on its entrepreneurial ecosystem.[49]		Manufacture of transistors is, or was, the core industry in Silicon Valley. The production workforce[50] was for the most part composed of Asian and Latina immigrants who were paid low wages and worked in hazardous conditions due to the chemicals used in the manufacture of integrated circuits. Technical, engineering, design, and administrative staffs were in large part [51] well compensated.[52]		Many more jobs (400,000 during the period 2010 to 2015) are created in Silicon Valley than housing built (60,000 units during the period 2010 to 2015).[53] Housing prices are extremely high, far out of the range of production workers.[54] As of 2016 a two-bedroom apartment rented for about $2,500 while the median home price was about $1 million.[53] The Financial Post called Silicon Valley the most expensive U.S. housing region.[55] Homelessness is a problem with housing beyond the reach of middle-income residents; there is little shelter space other than in San Jose which, as of 2015, was making an effort to develop shelters by renovating old hotels.[56]		Thousands of high technology companies are headquartered in Silicon Valley. Among those, the following 39 are in the Fortune 1000:		Additional notable companies headquartered (or with a significant presence) in Silicon Valley include (some defunct or subsumed):		Silicon Valley is also home to the high-tech superstore retail chain Fry's Electronics.		Depending on what geographic regions are included in the meaning of the term, the population of Silicon Valley is between 3.5 and 4 million. A 1999 study by AnnaLee Saxenian for the Public Policy Institute of California reported that a third of Silicon Valley scientists and engineers were immigrants and that nearly a quarter of Silicon Valley's high-technology firms since 1980 were run by Chinese (17 percent) or Indian CEOs (7 percent).[57] There is a stratum of well-compensated technical employees and managers, including 10s of thousands of "single-digit millionaires." This income and range of assets will support a middle-class lifestyle in Silicon Valley.[58]		In November 2006, the University of California, Davis released a report analyzing business leadership by women within the state.[59] The report showed that although 103 of the 400 largest public companies headquartered in California were located in Santa Clara County (the most of all counties), only 8.8% of Silicon Valley companies had women CEOs.[60]:4,7 This was the lowest percentage in the state.[61] (San Francisco County had 19.2% and Marin County had 18.5%.)[60]		Silicon Valley tech leadership positions are occupied almost exclusively by men.[62] This is also represented in the number of new companies founded by women as well as the number of women-lead startups that receive venture capital funding. Wadhwa said he believes that a contributing factor is a lack of parental encouragement to study science and engineering.[63] He also cited a lack of women role models and noted that most famous tech leaders — like Bill Gates, Steve Jobs, and Mark Zuckerberg — are men.[62]		In 2014, tech companies Google, Yahoo!, Facebook, Apple, and others, released corporate transparency reports that offered detailed employee breakdowns. In May, Google said 17% of its tech employees worldwide were women, and, in the U.S., 1% of its tech workers were black and 2% were Hispanic.[64] June 2014 brought reports from Yahoo! and Facebook. Yahoo! said that 15% of its tech jobs were held by women, 2% of its tech employees were black and 4% Hispanic.[65] Facebook reported that 15% of its tech workforce was female, and 3% was Hispanic and 1% was black.[66] In August, Apple reported that 80% of its global tech staff was male and that, in the U.S., 54% of its tech jobs were staffed by Caucasians and 23% by Asians.[67] Soon after, USA Today published an article about Silicon Valley's lack of tech-industry diversity, pointing out that it is largely white or Asian, and male. "Blacks and Hispanics are largely absent," it reported, "and women are underrepresented in Silicon Valley — from giant companies to start-ups to venture capital firms."[68] Civil rights activist Jesse Jackson said of improving diversity in the tech industry, "This is the next step in the civil rights movement"[69] while T.J. Rodgers has argued against Jackson's assertions.		As of October 2014, some high-profile Silicon Valley firms were working actively to prepare and recruit women. Bloomberg reported that Apple, Facebook, Google, and Microsoft attended the 20th annual Grace Hopper Celebration of Women in Computing conference to actively recruit and potentially hire female engineers and technology experts.[70] The same month, the second annual Platform Summit was held to discuss increasing racial and gender diversity in tech.[71] As of April 2015 experienced women were engaged in creation of venture capital firms which leveraged women's perspectives in funding of startups.[72]		After UC Davis published its Study of California Women Business Leaders in November 2006,[60] some San Jose Mercury News readers dismissed the possibility that sexism contributed in making Silicon Valley's leadership gender gap the highest in the state. A January 2015 issue of Newsweek magazine featured an article detailing reports of sexism and misogyny in Silicon Valley.[73] The article's author, Nina Burleigh, asked, "Where were all these offended people when women like Heidi Roizen published accounts of having a venture capitalist stick her hand in his pants under a table while a deal was being discussed?"[74]		Silicon Valley firms' board of directors are composed of 15.7% women compared with 20.9% in the S&P 100.[75]		The 2012 lawsuit Pao v. Kleiner Perkins was filed in San Francisco County Superior Court by executive Ellen Pao for gender discrimination against her employer, Kleiner Perkins Caufield & Byers.[76] The case went to trial in February 2015. On March 27, 2015 the jury found in favor of Kleiner Perkins on all counts.[77] Nevertheless, the case, which had wide press coverage, resulted in major advances in consciousness of gender discrimination on the part of venture capital and technology firms and their women employees.[78][79] Two other cases have been filed against Facebook and Twitter.[80]		Funding for public schools in upscale Silicon Valley communities such as Woodside, California is often supplemented by grants from private foundations set up for that purpose and funded by local residents. Schools in less favorable demographics such as East Palo Alto, California must depend on state funding.[81]		The following Santa Clara County cities are actually located in the Santa Clara Valley and based on that status are traditionally considered to be in Silicon Valley (in alphabetical order):		In 2015, MIT researchers developed a novel method for measuring which towns are home to startups with higher growth potential. This defines Silicon Valley to center on the municipalities of Menlo Park, Mountain View, Palo Alto, and Sunnyvale.[82][83]		The following Bay Area cities are (or were) home to various high-tech companies (or related firms like venture capital firms) and have thereby become associated with Silicon Valley:		Silicon Valley's first internationally known art gallery, Pace Art and Technology Gallery in Menlo Park, opened on February 6, 2016.[84]		In 1928, the Allied Arts Guild was formed in Menlo Park and is a complex of artist studios, shops, restaurant, and gardens.[85][86]		Some museums in Silicon Valley include;		Local and national media cover Silicon Valley and its companies. CNN, The Wall Street Journal, and Bloomberg News operate Silicon Valley bureaus out of Palo Alto. Public broadcaster KQED (TV) and KQED-FM, as well as the Bay Area's local ABC station KGO-TV, operate bureaus in San Jose. KNTV, NBC's local Bay Area affiliate "NBC Bay Area", is located in San Jose. Produced from this location is the nationally distributed TV Show "Tech Now" as well as the CNBC Silicon Valley bureau. San Jose-based media serving Silicon Valley include the San Jose Mercury News daily and the Metro Silicon Valley weekly. Specialty media include El Observador and the San Jose / Silicon Valley Business Journal. Most of the Bay Area's other major TV stations, newspapers, and media operate in San Francisco or Oakland. Patch.com operates various web portals, providing local news, discussion and events for residents of Silicon Valley. Mountain View has a public nonprofit station, KMVT-15. KMVT-15's shows include Silicon Valley Education News (EdNews)-Edward Tico Producer.		Coordinates: 37°24′N 122°00′W﻿ / ﻿37.4°N 122.0°W﻿ / 37.4; -122.0		
Victoria (abbreviated as Vic) is a state in southeastern Australia. Victoria is Australia's most densely-populated state and its second-most populous state overall. Most of its population lives concentrated in the area surrounding Port Phillip Bay, which includes the metropolitan area of its state capital and largest city, Melbourne, Australia's second-largest city. Geographically the smallest state on the Australian mainland, Victoria is bordered by Bass Strait and Tasmania to the south,[note 1] New South Wales to the north, the Tasman Sea to the east, and South Australia to the west.		Prior to British European settlement, a large[quantify] number of Aboriginal peoples, collectively known[by whom?] as the Koori, lived in the area now constituting Victoria. With Great Britain having claimed the entire Australian continent east of the 135th meridian east in 1788, Victoria formed part of the wider colony of New South Wales. The first European settlement in the area occurred in 1803 at Sullivan Bay, and much of what is now Victoria was included in 1836 in the Port Phillip District, an administrative division of New South Wales. Named in honour of Queen Victoria who signed the division's separation from New South Wales, the colony was officially established in 1851 and achieved self-government in 1855.[6] The Victorian gold rush in the 1850s and 1860s significantly increased both the population and wealth of the colony, and by the time of the Federation of Australia in 1901, Melbourne had become the largest city and leading financial centre in Australasia. Melbourne served as federal capital of Australia until the construction of Canberra in 1927, with the Federal Parliament meeting in Melbourne's Parliament House and all principal offices of the federal government being based in Melbourne.		Politically, Victoria has 37 seats in the Australian House of Representatives and 12 seats in the Australian Senate. At state level, the Parliament of Victoria consists of the Legislative Assembly (the lower house) and the Legislative Council (the upper house). As of 2017[update] the Labor Party governs in Victoria, with Daniel Andrews serving as Premier since 2014. The personal representative of the Queen of Australia in the state is the Governor of Victoria, currently Linda Dessau (in office since 2015). Local government is concentrated in 79 municipal districts, including 33 cities, although a number of unincorporated areas still exist, which the state administers directly.		The economy of Victoria is highly diversified: service sectors including financial and property services, health, education, wholesale, retail, hospitality and manufacturing constitute the majority of employment. Victoria's total gross state product (GSP) ranks second in Australia, although Victoria ranks fourth in terms of GSP per capita because of its limited mining activity. Culturally, Melbourne hosts a number of museums, art galleries and theatres and is also described as the "sporting capital of World" [7][8][9]. The Melbourne Cricket Ground, home of the largest stadium in Australia, hosted the 1956 Summer Olympics and the 2006 Commonwealth Games. The ground is also considered[by whom?] the "spiritual home" of Australian cricket and Australian rules football [10], and hosts the grand final of the Australian Football League (AFL) each year, drawing crowds of approximately 100,000. Victoria has eight public universities, with the oldest, the University of Melbourne, dating from 1853.						Victoria, like Queensland, was named after Queen Victoria, who had been on the British throne for 14 years when the colony was established in 1851.[11]		After the founding of the colony of New South Wales in 1788, Australia was divided into an eastern half named New South Wales and a western half named New Holland, under the administration of the colonial government in Sydney. The first British settlement in the area later known as Victoria was established in October 1803 under Lieutenant-Governor David Collins at Sullivan Bay on Port Phillip. It consisted of 402 people (5 Government officials, 9 officers of marines, 2 drummers, and 39 privates, 5 soldiers' wives, and a child, 307 convicts, 17 convicts' wives, and 7 children).[12] They had been sent from England in HMS Calcutta under the command of Captain Daniel Woodriff, principally out of fear that the French, who had been exploring the area, might establish their own settlement and thereby challenge British rights to the continent.		In the year 1826 Colonel Stewart, Captain S. Wright, and Lieutenant Burchell were sent in HMS Fly (Captain Wetherall) and the brigs Dragon and Amity, took a number of convicts and a small force composed of detachments of the 3rd and 93rd regiments. The expedition landed at Settlement Point (now Corinella), on the eastern side of the bay, which was the headquarters until the abandonment of Western Port at the insistence of Governor Darling about twelve months afterwards.[13][14]		Victoria's next settlement was at Portland, on the south west coast of what is now Victoria. Edward Henty settled Portland Bay in 1834.[15]		Melbourne was founded in 1835 by John Batman, who set up a base in Indented Head, and John Pascoe Fawkner. From settlement the region around Melbourne was known as the Port Phillip District, a separately administered part of New South Wales. Shortly after the site now known as Geelong was surveyed by Assistant Surveyor W. H. Smythe, three weeks after Melbourne. And in 1838 Geelong was officially declared a town, despite earlier white settlements dating back to 1826.				flag (1870-1901)		On 1 July 1851, writs were issued for the election of the first Victorian Legislative Council, and the absolute independence of Victoria from New South Wales was established proclaiming a new Colony of Victoria.[16] Days later, still in 1851 gold was discovered near Ballarat, and subsequently at Bendigo. Later discoveries occurred at many sites across Victoria. This triggered one of the largest gold rushes the world has ever seen. The colony grew rapidly in both population and economic power. In ten years the population of Victoria increased sevenfold from 76,000 to 540,000. All sorts of gold records were produced including the "richest shallow alluvial goldfield in the world" and the largest gold nugget. Victoria produced in the decade 1851–1860 20 million ounces of gold, one third of the world's output[citation needed].		Immigrants arrived from all over the world to search for gold, especially from Ireland and China. Many Chinese miners worked in Victoria, and their legacy is particularly strong in Bendigo and its environs. Although there was some racism directed at them, there was not the level of anti-Chinese violence that was seen at the Lambing Flat riots in New South Wales. However, there was a riot at Buckland Valley near Bright in 1857. Conditions on the gold fields were cramped and unsanitary; an outbreak of typhoid at Buckland Valley in 1854 killed over 1,000 miners.		In 1854 at Ballarat there was an armed rebellion against the government of Victoria by miners protesting against mining taxes (the "Eureka Stockade"). This was crushed by British troops, but the discontents prompted colonial authorities to reform the administration (particularly reducing the hated mining licence fees) and extend the franchise. Within a short time, the Imperial Parliament granted Victoria responsible government with the passage of the Colony of Victoria Act 1855. Some of the leaders of the Eureka rebellion went on to become members of the Victorian Parliament.		The first foreign military action by the colony of Victoria was to send troops and a warship to New Zealand as part of the Māori Wars. Troops from New South Wales had previously participated in the Crimean War.		In 1901 Victoria became a state in the Commonwealth of Australia. As a result of the gold rush, Melbourne had by then become the financial centre of Australia and New Zealand. Between 1901 and 1927, Melbourne was the capital of Australia while Canberra was under construction. It was also the largest city in Australia at the time.[citation needed].		Victoria has a parliamentary form of government based on the Westminster System. Legislative power resides in the Parliament consisting of the Governor (the representative of the Queen), the executive (the Government), and two legislative chambers. The Parliament of Victoria consists of the lower house Legislative Assembly, the upper house Legislative Council and the Queen of Australia.		Eighty-eight members of the Legislative Assembly are elected to four-year terms from single-member electorates.		In November 2006, the Victorian Legislative Council elections were held under a new multi-member proportional representation system. The State of Victoria was divided into eight electorates with each electorate represented by five representatives elected by Single Transferable Vote. The total number of upper house members was reduced from 44 to 40 and their term of office is now the same as the lower house members—four years. Elections for the Victorian Parliament are now fixed and occur in November every four years. Prior to the 2006 election, the Legislative Council consisted of 44 members elected to eight-year terms from 22 two-member electorates.		The Premier of Victoria is the leader of the political party or coalition with the most seats in the Legislative Assembly. The Premier is the public face of government and, with cabinet, sets the legislative and political agenda. Cabinet consists of representatives elected to either house of parliament. It is responsible for managing areas of government that are not exclusively the Commonwealth's, by the Australian Constitution, such as education, health and law enforcement. The current Premier of Victoria is Daniel Andrews.		Executive authority is vested in the Governor of Victoria who represents and is appointed by Queen Elizabeth II. The post is usually filled by a retired prominent Victorian. The governor acts on the advice of the premier and cabinet. The current Governor of Victoria is Linda Dessau.		Victoria has a written constitution enacted in 1975,[17] but based on the 1855 colonial constitution, passed by the United Kingdom Parliament as the Victoria Constitution Act 1855, which establishes the Parliament as the state's law-making body for matters coming under state responsibility. The Victorian Constitution can be amended by the Parliament of Victoria, except for certain "entrenched" provisions that require either an absolute majority in both houses, a three-fifths majority in both houses, or the approval of the Victorian people in a referendum, depending on the provision.		Victorians, and Melburnians in particular, are said to be "more progressive than other Australians", and although "most Australians support gay marriage", it is supported "nowhere more strongly than in Victoria." At the republic referendum in 1999, the state with the highest yes vote was Victoria. Victorians are also said to be "generally socially progressive, supportive of multiculturalism, wary of extremes of any kind."[18]		Premier Daniel Andrews leads the Australian Labor Party that won the November 2014 Victorian state election.		The centre-left Australian Labor Party (ALP), the centre-right Liberal Party of Australia, the rural-based National Party of Australia, and the environmentalist Australian Greens are Victoria's main political parties. Traditionally, Labor is strongest in Melbourne's working class western and northern suburbs, and the regional cities of Ballarat, Bendigo and Geelong. The Liberals' main support lies in Melbourne's more affluent eastern and outer suburbs, and some rural and regional centres. The Nationals are strongest in Victoria's North Western and Eastern rural regional areas. The Greens, who won their first lower house seats in 2014, are strongest in inner Melbourne.		Victorian voters elect 49 representatives to the Parliament of Australia, including 37 members of the House of Representatives and 12 members of the Senate. Since 2013, the ALP has held 19 Victorian house seats, the Liberals 14, the Nationals two, the Greens one, and one held by an Independent. As of 1 July 2014, the Liberals have held three senate seats, the Nationals one, the ALP four, the Greens two, and the Democratic Labor Party one.		Victoria is incorporated into 79 municipalities for the purposes of local government, including 39 shires, 32 cities, seven rural cities and one borough. Shire and city councils are responsible for functions delegated by the Victorian parliament, such as city planning, road infrastructure and waste management. Council revenue comes mostly from property taxes and government grants.[19]		The 2011 Australian census reported that Victoria had 5,354,042 people resident at the time of the census.[20] The Australian Bureau of Statistics estimates that the population may well reach 7.2 million by 2050.		Victoria's founding Anglo-Celtic population has been supplemented by successive waves of migrants from southern and eastern Europe, Southeast Asia and, most recently, the Horn of Africa and the Middle East. Victoria's population is ageing in proportion with the average of the remainder of the Australian population.		About 72% of Victorians are Australian-born. This figure falls to around 66% in Melbourne but rises to higher than 95% in some rural areas in the north west of the state. Around two-thirds of Victorians claim Scottish, English or Irish ancestry. Less than 1% of Victorians identify themselves as Aboriginal. The largest groups of people born outside Australia came from the British Isles, China, Italy, Vietnam, Greece and New Zealand.		More than 75% of Victorians live in Melbourne, located in the state's south. The greater Melbourne metropolitan area is home to an estimated 4.17 million people.[21] Leading urban centres outside Melbourne include Geelong, Ballarat, Bendigo, Shepparton, Mildura, Warrnambool, Wodonga and the Latrobe Valley.		Victoria is Australia's most urbanised state: nearly 90% of residents living in cities and towns. State Government efforts to decentralise population have included an official campaign run since 2003 to encourage Victorians to settle in regional areas,[22] however Melbourne continues to rapidly outpace these areas in terms of population growth.[23]		The government predicts that nearly a quarter of Victorians will be aged over 60 by 2021. The 2011 census reveals that Australian median age has crept upward from 35 to 37 since 2001, which reflects the population growth peak of 1969–72.[24] In 2011, Victoria recorded a TFR of 1.88, the highest after 1978.[25]		In 2011–2012 there were 173 homicides.[26]		In 2015, the average male prisoner in Victoria is:[27]		In 2015, the average female prisoner in Victoria is:[27]		About 61.1% of Victorians describe themselves as Christian. Roman Catholics form the single largest religious group in the state with 26.7% of the Victorian population, followed by Anglicans and members of the Uniting Church. Buddhism is the state's largest non-Christian religion, with 168,637 members as of the most recent census. Victoria is also home of 152,775 Muslims and 45,150 Jews. Hinduism is the fastest growing religion. Around 20% of Victorians claim no religion. Amongst those who declare a religious affiliation, church attendance is low.[28]		In 2012 the proportion of couples marrying in a church had dropped to 28.2%; the other 71.7% registered their marriage with a civil celebrant.[29]		Victoria's state school system dates back to 1872, when the colonial government legislated to make schooling both free and compulsory. The state's public secondary school system began in 1905. Before then, only private secondary schooling was available. Today, a Victorian school education consists of seven years of primary schooling (including one preparatory year) and six years of secondary schooling.		The final years of secondary school are optional for children aged over 17. Victorian children generally begin school at age five or six. On completing secondary school, students earn the Victorian Certificate of Education. Students who successfully complete their secondary education also receive a tertiary entrance ranking, or ATAR score, to determine university admittance.		Victorian schools are either publicly or privately funded. Public schools, also known as state or government schools, are funded and run directly by the Victoria Department of Education [3]. Students do not pay tuition fees, but some extra costs are levied. Private fee-paying schools include parish schools run by the Roman Catholic Church and independent schools similar to British public schools. Independent schools are usually affiliated with Protestant churches. Victoria also has several private Jewish and Islamic primary and secondary schools. Private schools also receive some public funding. All schools must comply with government-set curriculum standards. In addition, Victoria has four government selective schools, Melbourne High School for boys, MacRobertson Girls' High School for girls, the coeducational schools John Monash Science School, Nossal High School and Suzanne Cory High School, and The Victorian College of the Arts Secondary School. Students at these schools are exclusively admitted on the basis of an academic selective entry test.		As of August 2010, Victoria had 1,548 public schools, 489 Catholic schools and 214 independent schools. Just under 540,800 students were enrolled in public schools, and just over 311,800 in private schools. Over 61 per cent of private students attend Catholic schools. More than 462,000 students were enrolled in primary schools and more than 390,000 in secondary schools. Retention rates for the final two years of secondary school were 77 per cent for public school students and 90 per cent for private school students. Victoria has about 63,519 full-time teachers.[30]		Victoria has nine universities. The first to offer degrees, the University of Melbourne, enrolled its first student in 1855. The largest, Monash University, has an enrolment of over 70,000 students—more than any other Australian university.		The number of students enrolled in Victorian universities was 241,755 at 2004, an increase of 2% on the previous year. International students made up 30% of enrolments and account for the highest percentage of pre-paid university tuition fees. The largest number of enrolments were recorded in the fields of business, administration and economics, with nearly a third of all students, followed by arts, humanities, and social science, with 20% of enrolments.		Victoria has 18 government-run institutions of “technical and further education” (TAFE). The first vocational institution in the state was the Melbourne Mechanics' Institute (established in 1839), which is now the Melbourne Athenaeum. More than 1,000 adult education organisations are registered to provide recognised TAFE programs. In 2004, there were about 480,700 students enrolled in vocational education programs in the state.[31]		The State Library of Victoria is the State's research and reference library. It is responsible for collecting and preserving Victoria's documentary heritage and making it available through a range of services and programs. Material in the collection includes books, newspapers, magazines, journals, manuscripts, maps, pictures, objects, sound and video recordings and databases.		In addition, local governments maintain local lending libraries, typically with multiple branches in their respective municipal areas.		The state of Victoria is the second largest economy in Australia after New South Wales, accounting for a quarter of the nation's gross domestic product. The total gross state product (GSP) at current prices for Victoria was at just over A$293 billion, with a GSP per capita of A$52,872. The economy grew by 2.0 per cent in 2010, less than the Australian average of 2.3 per cent.		Finance, insurance and property services form Victoria's largest income producing sector, while the community, social and personal services sector is the state's biggest employer. Despite the shift towards service industries, the manufacturing sector remains Victoria's single largest employer and income producer.		During 2003–04, the gross value of Victorian agricultural production increased by 17% to $8.7 billion. This represented 24% of national agricultural production total gross value. As of 2004, an estimated 32,463 farms occupied around 136,000 square kilometres (52,500 sq mi) of Victorian land. This comprises more than 60% of the state's total land surface. Victorian farms range from small horticultural outfits to large-scale livestock and grain productions. A quarter of farmland is used to grow consumable crops.		More than 26,000 square kilometres (10,000 sq mi) of Victorian farmland are sown for grain, mostly in the state's west. More than 50% of this area is sown for wheat, 33% for barley and 7% for oats. A further 6,000 square kilometres (2,300 sq mi) is sown for hay. In 2003–04, Victorian farmers produced more than 3 million tonnes of wheat and 2 million tonnes of barley. Victorian farms produce nearly 90% of Australian pears and third of apples. It is also a leader in stone fruit production. The main vegetable crops include asparagus, broccoli, carrots, potatoes and tomatoes. Last year, 121,200 tonnes of pears and 270,000 tonnes of tomatoes were produced.		More than 14 million sheep and 5 million lambs graze over 10% of Victorian farms, mostly in the state's north and west. In 2004, nearly 10 million lambs and sheep were slaughtered for local consumption and export. Victoria also exports live sheep to the Middle East for meat and to the rest of the world for breeding. More than 108,000 tonnes of wool clip was also produced—one-fifth of the Australian total.		Victoria is the centre of dairy farming in Australia. It is home to 60% of Australia's 3 million dairy cattle and produces nearly two-thirds of the nation's milk, almost 6.4 billion litres. The state also has 2.4 million beef cattle, with more than 2.2 million cattle and calves slaughtered each year. In 2003–04, Victorian commercial fishing crews and aquaculture industry produced 11,634 tonnes of seafood valued at nearly A$109 million. Blacklipped abalone is the mainstay of the catch, bringing in A$46 million, followed by southern rock lobster worth A$13.7 million. Most abalone and rock lobster is exported to Asia.		Victoria has a diverse range of manufacturing enterprises and Melbourne is Victoria's (and Australia's) most important industrial city, followed by Geelong.[citation needed] Additionally, energy production has aided industrial growth in the Latrobe Valley.[citation needed]		Machinery and equipment manufacturing is the state's most valuable manufacturing activity, followed by food and beverage products, petrochemicals and chemicals.[citation needed] More than 15% of Victorian workers, are employed directly in manufacturing, the highest percentage in Australia.[citation needed] The state is marginally behind New South Wales in the total value of manufacturing output.[citation needed]		Prominent manufacturing plants in the state include the Portland and Point Henry aluminium smelters, owned by Alcoa; oil refineries at Geelong and Altona; a major petrochemical facility at Laverton; and Victorian-based CSL, a global biotechnology company that produces vaccines and plasma products, among others. Victoria also plays an important role in providing goods for the defence industry.		Historically, Victoria has been the base for the manufacturing plants of the major car brands Ford, Toyota and Holden; however, closure announcements by all three companies in the 21st century will mean that Australia will no longer be a base for the global car industry, with Toyota's statement in February 2014 outlining a closure year of 2017. Holden's announcement occurred in May 2013, followed by Ford's decision in December of the same year (Ford's Victorian plants—in Broadmeadows and Geelong—will close in October 2016).[32][33]		The Victorian Government will sponsor and support industry and research, for example the Victorian Photonics Network from 2002 to 2015.[34]		Crown land held in Victoria is managed under the Crown Land (Reserves) Act 1978 and the Land Act 1958		Mining in Victoria contributes around A$3 billion to the gross state product (~1%) but employs less than 1% of workers. The Victorian mining industry is concentrated on energy producing minerals, with brown coal, petroleum and gas accounting for nearly 90% of local production. The oil and gas industries are centred off the coast of Gippsland in the state's east, while brown coal mining and power generation is based in the Latrobe Valley.		In the 2005/2006 fiscal year, the average gas production was over 700 million cubic feet (20,000,000 m3) per day (M cuft/d) and represented 18% of the total national gas sales, with demand growing at 2% per year.[35]		In 1985, oil production from the offshore Gippsland Basin peaked to an annual average of 450,000 barrels (72,000 m3) per day. In 2005–2006, the average daily oil production declined to 83,000 bbl (13,200 m3)/d, but despite the decline Victoria still produces almost 19.5% of crude oil in Australia.[35]		Brown coal is Victoria's leading mineral, with 66 million tonnes mined each year for electricity generation in the Latrobe Valley, Gippsland.[36] The region is home to the world's largest known reserves of brown coal.		Despite being the historic centre of Australia's gold rush, Victoria today contributes a mere 1% of national gold production. Victoria also produces limited amounts of gypsum and kaolin.		The service industries sector is the fastest growing component of the Victorian economy. It includes the wide range of activities generally classified as community, social and personal services; finances, insurance and property services, government services, transportation and communication, and wholesale and retail trade. Most service industries are located in Melbourne and the state's larger regional centres.		As of 2004–05, service industries employed nearly three-quarters of Victorian workers and generated three-quarters of the state's GSP. Finance, insurance and property services, as a group, provide a larger share of GSP than any other economic activity in Victoria. More than a quarter of Victorian workers are employed by the community, social and personal services sector.[37]		Some major tourist destinations in Victoria are:		Other popular tourism activities are gliding, hang-gliding, hot air ballooning and scuba diving.		Major events also play a big part in tourism in Victoria, particularly cultural tourism and sports tourism. Most of these events are centred on Melbourne, but others occur in regional cities, such as the V8 Supercars and Australian Motorcycle Grand Prix at Phillip Island, the Grand Annual Steeplechase at Warrnambool and the Australian International Airshow at Avalon and numerous local festivals such as the popular Port Fairy Folk Festival, Queenscliff Music Festival, Bells Beach Surf Classic and the Bright Autumn Festival.		Victoria's northern border follows a straight line from Cape Howe to the start of the Murray River and then follows the Murray River as the remainder of the northern border. On the Murray River, the border is the southern bank of the river. This precise definition was not established until 1980, when a ruling by Justice Ninian Stephen of the High Court of Australia settled the question as to which state had jurisdiction in the unlawful death of a man on an island in the middle of the river. The ruling clarified that no part of the watercourse is in Victoria.[39] The border also rests at the southern end of the Great Dividing Range, which stretches along the east coast and terminates west of Ballarat. It is bordered by South Australia to the west and shares Australia's shortest land border with Tasmania. The official border between Victoria and Tasmania is at 39°12' S, which passes through Boundary Islet in the Bass Strait for 85 metres.[40][41][42]		Victoria contains many topographically, geologically and climatically diverse areas, ranging from the wet, temperate climate of Gippsland in the southeast to the snow-covered Victorian alpine areas which rise to almost 2,000 m (6,600 ft), with Mount Bogong the highest peak at 1,986 m (6,516 ft). There are extensive semi-arid plains to the west and northwest. There is an extensive series of river systems in Victoria. Most notable is the Murray River system. Other rivers include: Ovens River, Goulburn River, Patterson River, King River, Campaspe River, Loddon River, Wimmera River, Elgin River, Barwon River, Thomson River, Snowy River, Latrobe River, Yarra River, Maribyrnong River, Mitta River, Hopkins River, Merri River and Kiewa River. The state symbols include the pink heath (state flower), Leadbeater's possum (state animal) and the helmeted honeyeater (state bird).		The state's capital, Melbourne, contains about 70% of the state's population and dominates its economy, media, and culture. For other cities and towns, see list of localities (Victoria) and local government areas of Victoria.		Victoria has a varied climate despite its small size. It ranges from semi-arid temperate with hot summers in the north-west, to temperate and cool along the coast. Victoria's main land feature, the Great Dividing Range, produces a cooler, mountain climate in the centre of the state. Winters along the coast of the state, particularly around Melbourne, are relatively mild (see chart at right).		Victoria's southernmost position on the Australian mainland means it is cooler and wetter than other mainland states and territories. The coastal plain south of the Great Dividing Range has Victoria's mildest climate. Air from the Southern Ocean helps reduce the heat of summer and the cold of winter. Melbourne and other large cities are located in this temperate region. The autumn months of April/May are mild and bring some of Australia's colourful foliage across many parts of the state.		The Mallee and upper Wimmera are Victoria's warmest regions with hot winds blowing from nearby semi-deserts. Average temperatures exceed 32 °C (90 °F) during summer and 15 °C (59 °F) in winter. Except at cool mountain elevations, the inland monthly temperatures are 2–7 °C (4–13 °F) warmer than around Melbourne (see chart). Victoria's highest maximum temperature since World War II, of 48.8 °C (119.8 °F) was recorded in Hopetoun on 7 February 2009, during the 2009 southeastern Australia heat wave.[43]		The Victorian Alps in the northeast are the coldest part of Victoria. The Alps are part of the Great Dividing Range mountain system extending east–west through the centre of Victoria. Average temperatures are less than 9 °C (48 °F) in winter and below 0 °C (32 °F) in the highest parts of the ranges. The state's lowest minimum temperature of −11.7 °C (10.9 °F) was recorded at Omeo on 15 June 1965, and again at Falls Creek on 3 July 1970.[43] Temperature extremes for the state are listed in the table below:		Victoria is the wettest Australian state after Tasmania. Rainfall in Victoria increases from south to the northeast, with higher averages in areas of high altitude. Mean annual rainfall exceeds 1,800 millimetres (71 inches) in some parts of the northeast but is less than 280 mm (11 in) in the Mallee.		Rain is heaviest in the Otway Ranges and Gippsland in southern Victoria and in the mountainous northeast. Snow generally falls only in the mountains and hills in the centre of the state. Rain falls most frequently in winter, but summer precipitation is heavier. Rainfall is most reliable in Gippsland and the Western District, making them both leading farming areas. Victoria's highest recorded daily rainfall was 375 mm (14.8 in) at Tanybryn in the Otway Ranges on 22 March 1983.[43]		Average January maximum temperatures: Victoria's north is almost always hotter than coastal and mountainous areas.		Average July maximum temperatures: Victoria's hills and ranges are coolest during winter. Snow also falls there.		Average yearly precipitation: Victoria's rainfall is concentrated in the mountainous north-east and coast.		Victoria has the highest population density in any state in Australia, with population centres spread out over most of the state; only the far northwest and the Victorian Alps lack permanent settlement.		The Victorian road network services the population centres, with highways generally radiating from Melbourne and other major cities and rural centres with secondary roads interconnecting the highways to each other. Many of the highways are built to freeway standard ("M" freeways), while most are generally sealed and of reasonable quality.		Rail transport in Victoria is provided by several private and public railway operators who operate over government-owned lines. Major operators include: Metro Trains Melbourne which runs an extensive, electrified, passenger system throughout Melbourne and suburbs; V/Line which is now owned by the Victorian Government, operates a concentrated service to major regional centres, as well as long distance services on other lines; Pacific National, CFCL Australia which operate freight services; Great Southern Rail which operates The Overland Melbourne—Adelaide; and NSW TrainLink which operates XPTs Melbourne—Sydney.		There are also several smaller freight operators and numerous tourist railways operating over lines which were once parts of a state-owned system. Victorian lines mainly use the 1,600 mm (5 ft 3 in) broad gauge. However, the interstate trunk routes, as well as a number of branch lines in the west of the state have been converted to 1,435 mm (4 ft 8 1⁄2 in) standard gauge. Two tourist railways operate over 760 mm (2 ft 6 in) narrow gauge lines, which are the remnants of five formerly government-owned lines which were built in mountainous areas.		Melbourne has the world's largest tram network,[45] currently operated by Yarra Trams. As well as being a popular form of public transport, over the last few decades trams have become one of Melbourne's major tourist attractions. There are also tourist trams operating over portions of the former Ballarat and Bendigo systems. There are also tramway museums at Bylands and Haddon.		Melbourne Airport is the major domestic and international gateway for the state. Avalon Airport is the state's second busiest airport, which complements Essendon and Moorabbin Airports to see the remainder of Melbourne's air traffic. Hamilton Airport, Mildura Airport, Mount Hotham and Portland Airport are the remaining airports with scheduled domestic flights. There are no fewer than 27 other airports in the state with no scheduled flights.		The Port of Melbourne is the largest port for containerised and general cargo in Australia,[46] and is located in Melbourne on the mouth of the Yarra River, which is at the head of Port Phillip. Additional seaports are at Westernport, Geelong, and Portland.		As of October 2013, smoking tobacco is prohibited in the sheltered areas of train stations, and tram and bus stops—between 2012 and 2013, 2002 people were issued with infringement notices. The state government announced a plan in October 2013 to prohibit smoking on all Victorian railway station platforms and raised tram stops.[47]		Victoria's major utilities include a collection of brown-coal-fired power stations, particularly in the Latrobe Valley. One of these is Hazelwood Power Station, which is number 1 in the worldwide List of least carbon efficient power stations.		Victoria's water infrastructure includes a series of dams and reservoirs, predominantly in Central Victoria, that hold and collect water for much of the state. The water collected is of a very high quality and requires little chlorination treatment, giving the water a taste more like water collected in a rainwater tank. In regional areas however, such as in the west of the state, chlorination levels are much higher.		The Victorian Water Grid consists of a number of new connections and pipelines being built across the State. This allows water to be moved around Victoria to where it is needed most and reduces the impact of localised droughts in an era thought to be influenced by climate change. Major projects already completed as part of the Grid include the Wimmera Mallee Pipeline and the Goldfields Superpipe.[48]		Victoria is the home of Australian rules football, with ten of the eighteen Australian Football League (AFL) clubs based in the state. The AFL Grand Final is traditionally held at the Melbourne Cricket Ground on the last Saturday of September. Victoria's newest public holiday is Grand Final Friday. The holiday is celebrated the day before the AFL Grand Final and was designated in 2015 "to celebrate Australia’s national game."		Victoria's cricket team, the Victorian Bushrangers play in the national Sheffield Shield cricket competition. Victoria is represented in the National Rugby League by the Melbourne Storm and in Super Rugby by the Melbourne Rebels. It is represented in the National Basketball League by Melbourne United. It is also represented in soccer by Melbourne Victory and Melbourne City in the A-League.		Melbourne has held the 1956 Summer Olympics, 2006 Commonwealth Games and the FINA World Swimming Championship.		Melbourne is also home to the Australian Open tennis tournament in January each year, the first of the world's four Grand Slam tennis tournaments, and the Australian Formula One Australian Grand Prix in March. It hosted the Australian Masters golf tournament from 1979 to 2015.		Victoria's Bells Beach hosts one of the world's longest-running surfing competition, the Bells Beach SurfClassic, which is part of The ASP World Tour.		Netball is a big part of sport in Victoria.[citation needed] The Melbourne Vixens represent Victoria in the ANZ Championship. Some of the world's best netballers such as Sharelle McMahon, Renae Hallinan, Madison Browne, Julie Corletto and Bianca Chatfield come from Victoria.		Possibly Victoria's most famous island, Phillip Island, is home of the Phillip Island Grand Prix Circuit which hosts the Australian motorcycle Grand Prix which features MotoGP (the world's premier motorcycling class), as well as the Australian round of the World Superbike Championship and the domestic V8 Supercar racing, which also visits Sandown Raceway and the rural Winton Motor Raceway circuit.		Australia's most prestigious footrace, the Stawell Gift, is an annual event.		Victoria is also home to the Aussie Millions poker tournament, the richest in the Southern Hemisphere.		The main horse racing tracks in Victoria are Caulfield Racecourse, Flemington Racecourse and Sandown Racecourse. The Melbourne Spring Racing Carnival is one of the biggest horse racing events in the world and is one of the world's largest sporting events. The main race is for the $6 million Melbourne Cup, and crowds for the carnival exceed 700,000.		Major professional teams include:		Geography:		Lists:		
Coordinates: 40°N 100°W﻿ / ﻿40°N 100°W﻿ / 40; -100		The United States of America (/əˈmɛrɪkə/; USA), commonly known as the United States (U.S.) or America, is a federal republic[19][20] composed of 50 states, a federal district, five major self-governing territories, and various possessions.[fn 6] Forty-eight of the fifty states and the federal district are contiguous and located in North America between Canada and Mexico. The state of Alaska is in the northwest corner of North America, bordered by Canada to the east and across the Bering Strait from Russia to the west. The state of Hawaii is an archipelago in the mid-Pacific Ocean. The U.S. territories are scattered about the Pacific Ocean and the Caribbean Sea, stretching across nine time zones. The extremely diverse geography, climate and wildlife of the United States make it one of the world's 17 megadiverse countries.[22][23]		At 3.8 million square miles (9.8 million km2)[11] and with over 324 million people, the United States is the world's third- or fourth-largest country by total area,[fn 7] third-largest by land area, and the third-most populous. It is one of the world's most ethnically diverse and multicultural nations, and is home to the world's largest immigrant population.[28] The capital is Washington, D.C., and the largest city is New York City; nine other major metropolitan areas—each with at least 4.5 million inhabitants—are Los Angeles, Chicago, Dallas, Houston, Philadelphia, Miami, Atlanta, Boston, and San Francisco.		Paleo-Indians migrated from Asia to the North American mainland at least 15,000 years ago.[29] European colonization began in the 16th century. The United States emerged from 13 British colonies along the East Coast. Numerous disputes between Great Britain and the colonies following the Seven Years' War led to the American Revolution, which began in 1775. On July 4, 1776, during the course of the American Revolutionary War, the colonies unanimously adopted the Declaration of Independence. The war ended in 1783 with recognition of the independence of the United States by Great Britain, representing the first successful war of independence against a European power.[30] The current constitution was adopted in 1788, after the Articles of Confederation, adopted in 1781, were felt to have provided inadequate federal powers. The first ten amendments, collectively named the Bill of Rights, were ratified in 1791 and designed to guarantee many fundamental civil liberties.		The United States embarked on a vigorous expansion across North America throughout the 19th century,[31] displacing Native American tribes, acquiring new territories, and gradually admitting new states until it spanned the continent by 1848.[31] During the second half of the 19th century, the American Civil War led to the end of legal slavery in the country.[32][33] By the end of that century, the United States extended into the Pacific Ocean,[34] and its economy, driven in large part by the Industrial Revolution, began to soar.[35] The Spanish–American War and World War I confirmed the country's status as a global military power. The United States emerged from World War II as a global superpower, the first country to develop nuclear weapons, the only country to use them in warfare, and a permanent member of the United Nations Security Council. The end of the Cold War and the dissolution of the Soviet Union in 1991 left the United States as the world's sole superpower.[36] The U.S. is a founding member of the United Nations, World Bank, International Monetary Fund, Organization of American States (OAS), and other international organizations.		The United States is a highly developed country, with the world's largest economy by nominal GDP and second-largest economy by PPP. Though its population is only 4.3% of the world total,[37] Americans hold nearly 40% of the total wealth in the world.[38] The United States ranks among the highest in several measures of socioeconomic performance, including average wage,[39] human development, per capita GDP, and productivity per person.[40] While the U.S. economy is considered post-industrial, characterized by the dominance of services and knowledge economy, the manufacturing sector remains the second-largest in the world.[41] Accounting for approximately a quarter of global GDP[42] and a third of global military spending,[43] the United States is the world's foremost economic and military power. The United States is a prominent political and cultural force internationally, and a leader in scientific research and technological innovations.[44]						In 1507, the German cartographer Martin Waldseemüller produced a world map on which he named the lands of the Western Hemisphere "America" in honor of the Italian explorer and cartographer Amerigo Vespucci (Latin: Americus Vespucius).[45] The first documentary evidence of the phrase "United States of America" is from a letter dated January 2, 1776, written by Stephen Moylan, Esq., George Washington's aide-de-camp and Muster-Master General of the Continental Army. Addressed to Lt. Col. Joseph Reed, Moylan expressed his wish to carry the "full and ample powers of the United States of America" to Spain to assist in the revolutionary war effort.[47][48][49]		The first known publication of the phrase "United States of America" was in an anonymous essay in The Virginia Gazette newspaper in Williamsburg, Virginia, on April 6, 1776.[50][51] The second draft of the Articles of Confederation, prepared by John Dickinson and completed by June 17, 1776, at the latest, declared "The name of this Confederation shall be the 'United States of America.'"[52] The final version of the Articles sent to the states for ratification in late 1777 contains the sentence "The Stile of this Confederacy shall be 'The United States of America'".[53] In June 1776, Thomas Jefferson wrote the phrase "UNITED STATES OF AMERICA" in all capitalized letters in the headline of his "original Rough draught" of the Declaration of Independence.[54][55] This draft of the document did not surface until June 21, 1776, and it is unclear whether it was written before or after Dickinson used the term in his June 17 draft of the Articles of Confederation.[52] In the final Fourth of July version of the Declaration, the title was changed to read, "The unanimous Declaration of the thirteen united States of America".[56] The preamble of the Constitution states "...establish this Constitution for the United States of America."		The short form "United States" is also standard. Other common forms are the "U.S.", the "USA", and "America". Colloquial names are the "U.S. of A." and, internationally, the "States". "Columbia", a name popular in poetry and songs of the late 18th century, derives its origin from Christopher Columbus; it appears in the name "District of Columbia".[57] In non-English languages, the name is frequently the translation of either the "United States" or "United States of America", and colloquially as "America". In addition, an abbreviation (e.g. USA) is sometimes used.[58]		The phrase "United States" was originally plural, a description of a collection of independent states—e.g., "the United States are"—including in the Thirteenth Amendment to the United States Constitution, ratified in 1865. The singular form—e.g., "the United States is"—became popular after the end of the American Civil War. The singular form is now standard; the plural form is retained in the idiom "these United States".[59] The difference is more significant than usage; it is a difference between a collection of states and a unit.[60]		A citizen of the United States is an "American". "United States", "American" and "U.S." refer to the country adjectivally ("American values", "U.S. forces"). In English, the word "American" rarely refers to topics or subjects not connected with the United States.[61]		The first inhabitants of North America migrated from Siberia by way of the Bering land bridge and arrived at least 15,000 years ago, though increasing evidence suggests an even earlier arrival.[29] Some, such as the pre-Columbian Mississippian culture, developed advanced agriculture, grand architecture, and state-level societies.[62] The first Europeans to arrive in territory of the modern United States were Spanish conquistadors such as Juan Ponce de León, who made his first visit to Florida in 1513.		In the Hawaiian Islands, the earliest indigenous inhabitants arrived around 1 AD from Polynesia. Europeans under the British explorer Captain James Cook arrived in the Hawaiian Islands in 1778.		After Spain sent Columbus on his first voyage to the New World in 1492, other explorers followed. The Spanish set up the first settlements in Florida and New Mexico such as Saint Augustine[63] and Santa Fe. The French established their own as well along the Mississippi River. Successful English settlement on the eastern coast of North America began with the Virginia Colony in 1607 at Jamestown and the Pilgrims' Plymouth Colony in 1620. Many settlers were dissenting Christian groups who came seeking religious freedom. The continent's first elected legislative assembly, Virginia's House of Burgesses created in 1619, the Mayflower Compact, signed by the Pilgrims before disembarking, and the Fundamental Orders of Connecticut, established precedents for the pattern of representative self-government and constitutionalism that would develop throughout the American colonies.[64][65]		Most settlers in every colony were small farmers, but other industries developed within a few decades as varied as the settlements. Cash crops included tobacco, rice and wheat. Extraction industries grew up in furs, fishing and lumber. Manufacturers produced rum and ships, and by the late colonial period Americans were producing one-seventh of the world's iron supply.[66] Cities eventually dotted the coast to support local economies and serve as trade hubs. English colonists were supplemented by waves of Scotch-Irish and other groups. As coastal land grew more expensive freed indentured servants pushed further west.[67]		A large-scale slave trade with English privateers was begun.[68] The life expectancy of slaves was much higher in North America than further south, because of less disease and better food and treatment, leading to a rapid increase in the numbers of slaves.[69][70] Colonial society was largely divided over the religious and moral implications of slavery and colonies passed acts for and against the practice.[71][72] But by the turn of the 18th century, African slaves were replacing indentured servants for cash crop labor, especially in southern regions.[73]		With the British colonization of Georgia in 1732, the 13 colonies that would become the United States of America were established.[74] All had local governments with elections open to most free men, with a growing devotion to the ancient rights of Englishmen and a sense of self-government stimulating support for republicanism.[75] With extremely high birth rates, low death rates, and steady settlement, the colonial population grew rapidly. Relatively small Native American populations were eclipsed.[76] The Christian revivalist movement of the 1730s and 1740s known as the Great Awakening fueled interest in both religion and religious liberty.[77]		During the Seven Years' War (in America, known as the French and Indian War), British forces seized Canada from the French, but the francophone population remained politically isolated from the southern colonies. Excluding the Native Americans, who were being conquered and displaced, the 13 British colonies had a population of over 2.1 million in 1770, about one-third that of Britain. Despite continuing new arrivals, the rate of natural increase was such that by the 1770s only a small minority of Americans had been born overseas.[78] The colonies' distance from Britain had allowed the development of self-government, but their success motivated monarchs to periodically seek to reassert royal authority.[79]		With the progress of European colonization in the territories of the contemporary United States, the Native Americans were often conquered and displaced.[80] The native population of America declined after Europeans arrived, and for various reasons, primarily diseases such as smallpox and measles. Violence was not a significant factor in the overall decline among Native Americans, though conflict among themselves and with Europeans affected specific tribes and various colonial settlements.[81][82][83][84][85][86]		In the early days of colonization, many European settlers were subject to food shortages, disease, and attacks from Native Americans. Native Americans were also often at war with neighboring tribes and allied with Europeans in their colonial wars. At the same time, however, many natives and settlers came to depend on each other. Settlers traded for food and animal pelts, natives for guns, ammunition and other European wares.[87] Natives taught many settlers where, when and how to cultivate corn, beans and squash. European missionaries and others felt it was important to "civilize" the Native Americans and urged them to adopt European agricultural techniques and lifestyles.[88][89]		The American Revolutionary War was the first successful colonial war of independence against a European power. Americans had developed an ideology of "republicanism" asserting that government rested on the will of the people as expressed in their local legislatures. They demanded their rights as Englishmen and "no taxation without representation". The British insisted on administering the empire through Parliament, and the conflict escalated into war.[90]		Following the passage of the Lee Resolution, on July 2, 1776, which was the actual vote for independence, the Second Continental Congress adopted the Declaration of Independence on July 4, which proclaimed, in a long preamble, that humanity is created equal in their unalienable rights and that those rights were not being protected by Great Britain, and declared, in the words of the resolution, that the Thirteen Colonies were independent states and had no allegiance to the British crown in the United States. The fourth day of July is celebrated annually as Independence Day. In 1777, the Articles of Confederation established a weak government that operated until 1789.[91]		Britain recognized the independence of the United States following their defeat at Yorktown in 1781.[92] In the peace treaty of 1783, American sovereignty was recognized from the Atlantic coast west to the Mississippi River. Nationalists led the Philadelphia Convention of 1787 in writing the United States Constitution, ratified in state conventions in 1788. The federal government was reorganized into three branches, on the principle of creating salutary checks and balances, in 1789. George Washington, who had led the revolutionary army to victory, was the first president elected under the new constitution. The Bill of Rights, forbidding federal restriction of personal freedoms and guaranteeing a range of legal protections, was adopted in 1791.[93]		Although the federal government criminalized the international slave trade in 1808, after 1820, cultivation of the highly profitable cotton crop exploded in the Deep South, and along with it, the slave population.[94][95][96] The Second Great Awakening, especially 1800–1840, converted millions to evangelical Protestantism. In the North, it energized multiple social reform movements, including abolitionism;[97] in the South, Methodists and Baptists proselytized among slave populations.[98]		Americans' eagerness to expand westward prompted a long series of American Indian Wars.[99] The Louisiana Purchase of French-claimed territory in 1803 almost doubled the nation's area.[100] The War of 1812, declared against Britain over various grievances and fought to a draw, strengthened U.S. nationalism.[101] A series of military incursions into Florida led Spain to cede it and other Gulf Coast territory in 1819.[102] Expansion was aided by steam power, when steamboats began traveling along America's large water systems, which were connected by new canals, such as the Erie and the I&M; then, even faster railroads began their stretch across the nation's land.[103]		From 1820 to 1850, Jacksonian democracy began a set of reforms which included wider white male suffrage; it led to the rise of the Second Party System of Democrats and Whigs as the dominant parties from 1828 to 1854. The Trail of Tears in the 1830s exemplified the Indian removal policy that resettled Indians into the west on Indian reservations. The U.S. annexed the Republic of Texas in 1845 during a period of expansionist Manifest destiny.[104] The 1846 Oregon Treaty with Britain led to U.S. control of the present-day American Northwest.[105] Victory in the Mexican–American War resulted in the 1848 Mexican Cession of California and much of the present-day American Southwest.[106]		The California Gold Rush of 1848–49 spurred western migration and the creation of additional western states.[107] After the American Civil War, new transcontinental railways made relocation easier for settlers, expanded internal trade and increased conflicts with Native Americans.[108] Over a half-century, the loss of the American bison (sometimes called "buffalo") was an existential blow to many Plains Indians cultures.[109] In 1869, a new Peace Policy sought to protect Native-Americans from abuses, avoid further war, and secure their eventual U.S. citizenship, although conflicts, including several of the largest Indian Wars, continued throughout the West into the 1900s.[110]		Differences of opinion and social order between northern and southern states in early United States society, particularly regarding Black slavery, ultimately led to the American Civil War.[111] Initially, states entering the Union alternated between slave and free states, keeping a sectional balance in the Senate, while free states outstripped slave states in population and in the House of Representatives. But with additional western territory and more free-soil states, tensions between slave and free states mounted with arguments over federalism and disposition of the territories, whether and how to expand or restrict slavery.[112]		With the 1860 election of Abraham Lincoln, the first president from the largely anti-slavery Republican Party, conventions in thirteen slave states ultimately declared secession and formed the Confederate States of America, while the federal government maintained that secession was illegal.[112] The ensuing war was at first for Union, then after 1863 as casualties mounted and Lincoln delivered his Emancipation Proclamation, a second war aim became abolition of slavery. The war remains the deadliest military conflict in American history, resulting in the deaths of approximately 618,000 soldiers as well as many civilians.[113]		Following the Union victory in 1865, three amendments were added to the U.S. Constitution: the Thirteenth Amendment prohibited slavery, the Fourteenth Amendment provided citizenship to the nearly four million African Americans who had been slaves,[114] and the Fifteenth Amendment ensured that they had the right to vote. The war and its resolution led to a substantial increase in federal power[115] aimed at reintegrating and rebuilding the Southern states while ensuring the rights of the newly freed slaves.		Southern white conservatives, calling themselves "Redeemers" took control after the end of Reconstruction. By the 1890–1910 period Jim Crow laws disenfranchised most blacks and some poor whites. Blacks faced racial segregation, especially in the South.[116] Racial minorities occasionally experienced vigilante violence.[117]		In the North, urbanization and an unprecedented influx of immigrants from Southern and Eastern Europe supplied a surplus of labor for the country's industrialization and transformed its culture.[118] National infrastructure including telegraph and transcontinental railroads spurred economic growth and greater settlement and development of the American Old West. The later invention of electric light and the telephone would also affect communication and urban life.[119]		The end of the Indian Wars further expanded acreage under mechanical cultivation, increasing surpluses for international markets.[120] Mainland expansion was completed by the purchase of Alaska from Russia in 1867.[121] In 1893, pro-American elements in Hawaii overthrew the monarchy and formed the Republic of Hawaii, which the U.S. annexed in 1898. Puerto Rico, Guam, and the Philippines were ceded by Spain in the same year, following the Spanish–American War.[122]		Rapid economic development during the late 19th and early 20th centuries fostered the rise of many prominent industrialists. Tycoons like Cornelius Vanderbilt, John D. Rockefeller, and Andrew Carnegie led the nation's progress in railroad, petroleum, and steel industries. Banking became a major part of the economy, with J. P. Morgan playing a notable role. Edison and Tesla undertook the widespread distribution of electricity to industry, homes, and for street lighting. Henry Ford revolutionized the automotive industry. The American economy boomed, becoming the world's largest, and the United States achieved great power status.[123] These dramatic changes were accompanied by social unrest and the rise of populist, socialist, and anarchist movements.[124] This period eventually ended with the advent of the Progressive Era, which saw significant reforms in many societal areas, including women's suffrage, alcohol prohibition, regulation of consumer goods, greater antitrust measures to ensure competition and attention to worker conditions.[125][126][127][128]		The United States remained neutral from the outbreak of World War I, in 1914, until 1917 when it joined the war as an "associated power", alongside the formal Allies of World War I, helping to turn the tide against the Central Powers. In 1919, President Woodrow Wilson took a leading diplomatic role at the Paris Peace Conference and advocated strongly for the U.S. to join the League of Nations. However, the Senate refused to approve this, and did not ratify the Treaty of Versailles that established the League of Nations.[129]		In 1920, the women's rights movement won passage of a constitutional amendment granting women's suffrage.[130] The 1920s and 1930s saw the rise of radio for mass communication and the invention of early television.[131] The prosperity of the Roaring Twenties ended with the Wall Street Crash of 1929 and the onset of the Great Depression. After his election as president in 1932, Franklin D. Roosevelt responded with the New Deal, which included the establishment of the Social Security system.[132] The Great Migration of millions of African Americans out of the American South began before World War I and extended through the 1960s;[133] whereas the Dust Bowl of the mid-1930s impoverished many farming communities and spurred a new wave of western migration.[134]		At first effectively neutral during World War II while Germany conquered much of continental Europe, the United States began supplying material to the Allies in March 1941 through the Lend-Lease program. On December 7, 1941, the Empire of Japan launched a surprise attack on Pearl Harbor, prompting the United States to join the Allies against the Axis powers.[135] During the war, the United States was referred as one of the "Four Policemen"[136] of Allies power who met to plan the postwar world, along with Britain, the Soviet Union and China.[137][138] Though the nation lost more than 400,000 soldiers,[139] it emerged relatively undamaged from the war with even greater economic and military influence.[140]		The United States played a leading role in the Bretton Woods and Yalta conferences with the United Kingdom, the Soviet Union and other Allies, which signed agreements on new international financial institutions and Europe's postwar reorganization. As an Allied victory was won in Europe, a 1945 international conference held in San Francisco produced the United Nations Charter, which became active after the war.[141] The United States developed the first nuclear weapons and used them on Japan in the cities of Hiroshima and Nagasaki; causing the Japanese to surrender on September 2, ending World War II.[142][143] Parades and celebrations followed in what is known as Victory Day, or V-J Day.[144]		After World War II the United States and the Soviet Union jockeyed for power during what became known as the Cold War, driven by an ideological divide between capitalism and communism[145] and, according to the school of geopolitics, a divide between the maritime Atlantic and the continental Eurasian camps. They dominated the military affairs of Europe, with the U.S. and its NATO allies on one side and the USSR and its Warsaw Pact allies on the other. The U.S. developed a policy of containment towards the expansion of communist influence. While the U.S. and Soviet Union engaged in proxy wars and developed powerful nuclear arsenals, the two countries avoided direct military conflict.		The United States often opposed Third World movements that it viewed as Soviet-sponsored. American troops fought communist Chinese and North Korean forces in the Korean War of 1950–53.[146] The Soviet Union's 1957 launch of the first artificial satellite and its 1961 launch of the first manned spaceflight initiated a "Space Race" in which the United States became the first nation to land a man on the moon in 1969.[146] A proxy war in Southeast Asia eventually evolved into full American participation, as the Vietnam War.		At home, the U.S. experienced sustained economic expansion and a rapid growth of its population and middle class. Construction of an Interstate Highway System transformed the nation's infrastructure over the following decades. Millions moved from farms and inner cities to large suburban housing developments.[147][148] In 1959 Hawaii became the 50th and last U.S. state added to the country.[149] The growing Civil Rights Movement used nonviolence to confront segregation and discrimination, with Martin Luther King Jr. becoming a prominent leader and figurehead. A combination of court decisions and legislation, culminating in the Civil Rights Act of 1968, sought to end racial discrimination.[150][151][152] Meanwhile, a counterculture movement grew which was fueled by opposition to the Vietnam war, black nationalism, and the sexual revolution.		The launch of a "War on Poverty" expanded entitlements and welfare spending, including the creation of Medicare and Medicaid, two programs that provide health coverage to the elderly and poor, respectively, and the means-tested Food Stamp Program and Aid to Families with Dependent Children.[153]		The 1970s and early 1980s saw the onset of stagflation. After his election in 1980, President Ronald Reagan responded to economic stagnation with free-market oriented reforms. Following the collapse of détente, he abandoned "containment" and initiated the more aggressive "rollback" strategy towards the USSR.[154][155][156][157][158] After a surge in female labor participation over the previous decade, by 1985 the majority of women aged 16 and over were employed.[159]		The late 1980s brought a "thaw" in relations with the USSR, and its collapse in 1991 finally ended the Cold War.[160][161][162][163] This brought about unipolarity[164] with the U.S. unchallenged as the world's dominant superpower. The concept of Pax Americana, which had appeared in the post-World War II period, gained wide popularity as a term for the post-Cold War new world order.		After the Cold War, the conflict in the Middle East triggered a crisis in 1990, when Iraq under Sadaam Hussein invaded and attempted to annex Kuwait, an ally of the United States. Fearing that the instability would spread to other regions, President George H.W. Bush launched Operation Desert Shield, a defensive force buildup in Saudi Arabia, and Operation Desert Storm, in a staging titled the Gulf War; waged by coalition forces from 34 nations, led by the United States against Iraq ending in the successful expulsion of Iraqi forces from Kuwait, restoring the former monarchy.[165]		Originating in U.S. defense networks, the Internet spread to international academic networks, and then to the public in the 1990s, greatly affecting the global economy, society, and culture.[166]		Due to the dot-com boom, stable monetary policy under Alan Greenspan, and reduced social welfare spending, the 1990s saw the longest economic expansion in modern U.S. history, ending in 2001.[167] Beginning in 1994, the U.S. entered into the North American Free Trade Agreement (NAFTA), linking 450 million people producing $17 trillion worth of goods and services. The goal of the agreement was to eliminate trade and investment barriers among the U.S., Canada, and Mexico by January 1, 2008. Trade among the three partners has soared since NAFTA went into force.[168]		On September 11, 2001, Al-Qaeda terrorists struck the World Trade Center in New York City and the Pentagon near Washington, D.C., killing nearly 3,000 people.[169] In response, the United States launched the War on Terror, which included war in Afghanistan and the 2003–11 Iraq War.[170][171] In 2007, the Bush administration ordered a major troop surge in the Iraq War,[172] which successfully reduced violence and led to greater stability in the region.[173][174]		Government policy designed to promote affordable housing,[175] widespread failures in corporate and regulatory governance,[176] and historically low interest rates set by the Federal Reserve[177] led to the mid-2000s housing bubble, which culminated with the 2008 financial crisis, the largest economic contraction in the nation's history since the Great Depression.[178] Barack Obama, the first African American[179] and multiracial[180] president, was elected in 2008 amid the crisis,[181] and subsequently passed stimulus measures and the Dodd-Frank Wall Street Reform and Consumer Protection Act in an attempt to mitigate its negative effects. While the stimulus facilitated infrastructure improvements[182] and a relative decline in unemployment,[183] Dodd-Frank has had a negative impact on business investment and small banks.[184]		In 2010, the Obama administration passed the Affordable Care Act, which made the most sweeping reforms to the nation's healthcare system in nearly five decades, including mandates, subsidies and insurance exchanges. The law caused a significant reduction in the number and percentage of people without health insurance, with 24 million covered during 2016,[185] but remains controversial due to its impact on healthcare costs, insurance premiums, and economic performance.[186] Although the recession reached its trough in June 2009, voters remained frustrated with the slow pace of the economic recovery. The Republicans, who stood in opposition to Obama's policies, won control of the House of Representatives with a landslide in 2010 and control of the Senate in 2014.[187]		American forces in Iraq were withdrawn in large numbers in 2009 and 2010, and the war in the region was declared formally over in December 2011.[188] The withdrawal caused an escalation of sectarian insurgency,[189] leading to the rise of the Islamic State of Iraq and the Levant, the successor of al-Qaeda in the region.[190] In 2014, Obama announced a restoration of full diplomatic relations with Cuba for the first time since 1961.[needs update][191] The next year, the United States as a member of the P5+1 countries signed the Joint Comprehensive Plan of Action, an agreement aimed to slow the development of Iran's nuclear program.[192]		The land area of the contiguous United States is 2,959,064 square miles (7,663,940.6 km2). Alaska, separated from the contiguous United States by Canada, is the largest state at 663,268 square miles (1,717,856.2 km2). Hawaii, occupying an archipelago in the central Pacific, southwest of North America, is 10,931 square miles (28,311 km2) in area. The populated territories of Puerto Rico, American Samoa, Guam, Northern Mariana Islands, and U.S. Virgin Islands together cover 9,185 square miles (23,789 km2).[193]		The United States is the world's third- or fourth-largest nation by total area (land and water), ranking behind Russia and Canada and just above or below China. The ranking varies depending on how two territories disputed by China and India are counted and how the total size of the United States is measured: calculations range from 3,676,486 square miles (9,522,055.0 km2)[194] to 3,717,813 square miles (9,629,091.5 km2)[195] to 3,796,742 square miles (9,833,516.6 km2)[10] to 3,805,927 square miles (9,857,306 km2).[11] Measured by only land area, the United States is third in size behind Russia and China, just ahead of Canada.[196]		The coastal plain of the Atlantic seaboard gives way further inland to deciduous forests and the rolling hills of the Piedmont.[197] The Appalachian Mountains divide the eastern seaboard from the Great Lakes and the grasslands of the Midwest.[198] The Mississippi–Missouri River, the world's fourth longest river system, runs mainly north–south through the heart of the country. The flat, fertile prairie of the Great Plains stretches to the west, interrupted by a highland region in the southeast.[198]		The Rocky Mountains, at the western edge of the Great Plains, extend north to south across the country, reaching altitudes higher than 14,000 feet (4,300 m) in Colorado.[199] Farther west are the rocky Great Basin and deserts such as the Chihuahua and Mojave.[200] The Sierra Nevada and Cascade mountain ranges run close to the Pacific coast, both ranges reaching altitudes higher than 14,000 feet (4,300 m). The lowest and highest points in the contiguous United States are in the state of California,[201] and only about 84 miles (135 km) apart.[202] At an elevation of 20,310 feet (6,190.5 m), Alaska's Denali (Mount McKinley) is the highest peak in the country and North America.[203] Active volcanoes are common throughout Alaska's Alexander and Aleutian Islands, and Hawaii consists of volcanic islands. The supervolcano underlying Yellowstone National Park in the Rockies is the continent's largest volcanic feature.[204]		The United States, with its large size and geographic variety, includes most climate types. To the east of the 100th meridian, the climate ranges from humid continental in the north to humid subtropical in the south.[205] The Great Plains west of the 100th meridian are semi-arid. Much of the Western mountains have an alpine climate. The climate is arid in the Great Basin, desert in the Southwest, Mediterranean in coastal California, and oceanic in coastal Oregon and Washington and southern Alaska. Most of Alaska is subarctic or polar. Hawaii and the southern tip of Florida are tropical, as are the populated territories in the Caribbean and the Pacific.[206] Extreme weather is not uncommon—the states bordering the Gulf of Mexico are prone to hurricanes, and most of the world's tornadoes occur within the country, mainly in Tornado Alley areas in the Midwest and South.[207]		The U.S. ecology is megadiverse: about 17,000 species of vascular plants occur in the contiguous United States and Alaska, and over 1,800 species of flowering plants are found in Hawaii, few of which occur on the mainland.[209] The United States is home to 428 mammal species, 784 bird species, 311 reptile species, and 295 amphibian species.[210] About 91,000 insect species have been described.[211] The bald eagle is both the national bird and national animal of the United States, and is an enduring symbol of the country itself.[212]		There are 59 national parks and hundreds of other federally managed parks, forests, and wilderness areas.[213] Altogether, the government owns about 28% of the country's land area.[214] Most of this is protected, though some is leased for oil and gas drilling, mining, logging, or cattle ranching; about .86% is used for military purposes.[215][216]		Environmental issues have been on the national agenda since 1970. Environmental controversies include debates on oil and nuclear energy, dealing with air and water pollution, the economic costs of protecting wildlife, logging and deforestation,[217][218] and international responses to global warming.[219][220] Many federal and state agencies are involved. The most prominent is the Environmental Protection Agency (EPA), created by presidential order in 1970.[221] The idea of wilderness has shaped the management of public lands since 1964, with the Wilderness Act.[222] The Endangered Species Act of 1973 is intended to protect threatened and endangered species and their habitats, which are monitored by the United States Fish and Wildlife Service.[223]		The U.S. Census Bureau estimated the country's population to be 323,425,550 as of April 25, 2016, and to be adding 1 person (net gain) every 13 seconds, or about 6,646 people per day.[228] The U.S. population almost quadrupled during the 20th century, from about 76 million in 1900.[229] The third most populous nation in the world, after China and India, the United States is the only major industrialized nation in which large population increases are projected.[230] In the 1800s the average woman had 7.04 children, by the 1900s this number had decreased to 3.56.[231] Since the early 1970s the birth rate has been below the replacement rate of 2.1 with 1.86 children per woman in 2014. Foreign born immigration has caused the US population to continue its rapid increase with the foreign born population doubling from almost 20 million in 1990 to over 40 million in 2010, representing one third of the population increase.[232] The foreign born population reached 45 million in 2015.[233][fn 8]		The United States has a birth rate of 13 per 1,000, which is 5 births below the world average.[237] Its population growth rate is positive at 0.7%, higher than that of many developed nations.[238] In fiscal year 2012, over one million immigrants (most of whom entered through family reunification) were granted legal residence.[239] Mexico has been the leading source of new residents since the 1965 Immigration Act. China, India, and the Philippines have been in the top four sending countries every year since the 1990s.[240] As of 2012[update], approximately 11.4 million residents are illegal immigrants.[241] As of 2015, 47% of all immigrants are Hispanic, 26% are Asian, 18% are white and 8% are black. The percentage of immigrants who are Asian is increasing while the percentage who are Hispanic is decreasing.[233]		According to a survey conducted by the Williams Institute, nine million Americans, or roughly 3.4% of the adult population identify themselves as homosexual, bisexual, or transgender.[242][243] A 2016 Gallup poll also concluded that 4.1% of adult Americans identified as LGBT. The highest percentage came from the District of Columbia (10%), while the lowest state was North Dakota at 1.7%.[244] In a 2013 survey, the Centers for Disease Control and Prevention found that 96.6% of Americans identify as straight, while 1.6% identify as gay or lesbian, and 0.7% identify as being bisexual.[245]		In 2010, the U.S. population included an estimated 5.2 million people with some American Indian or Alaska Native ancestry (2.9 million exclusively of such ancestry) and 1.2 million with some native Hawaiian or Pacific island ancestry (0.5 million exclusively).[246] The census counted more than 19 million people of "Some Other Race" who were "unable to identify with any" of its five official race categories in 2010, over 18.5 million (97%) of whom are of Hispanic ethnicity.[246]		The population growth of Hispanic and Latino Americans (the terms are officially interchangeable) is a major demographic trend. The 50.5 million Americans of Hispanic descent[246] are identified as sharing a distinct "ethnicity" by the Census Bureau; 64% of Hispanic Americans are of Mexican descent.[247] Between 2000 and 2010, the country's Hispanic population increased 43% while the non-Hispanic population rose just 4.9%.[248] Much of this growth is from immigration; in 2007, 12.6% of the U.S. population was foreign-born, with 54% of that figure born in Latin America.[249][fn 9]		About 82% of Americans live in urban areas (including suburbs);[10] about half of those reside in cities with populations over 50,000.[255] The US has numerous clusters of cities known as megaregions, the largest being the Great Lakes Megalopolis followed by the Northeast Megalopolis and Southern California. In 2008, 273 incorporated municipalities had populations over 100,000, nine cities had more than one million residents, and four global cities had over two million (New York, Los Angeles, Chicago, and Houston).[256] There are 52 metropolitan areas with populations greater than one million.[257] Of the 50 fastest-growing metro areas, 47 are in the West or South.[258] The metro areas of San Bernardino, Dallas, Houston, Atlanta, and Phoenix all grew by more than a million people between 2000 and 2008.[257]				English (American English) is the de facto national language. Although there is no official language at the federal level, some laws—such as U.S. naturalization requirements—standardize English. In 2010, about 230 million, or 80% of the population aged five years and older, spoke only English at home. Spanish, spoken by 12% of the population at home, is the second most common language and the most widely taught second language.[261][262] Some Americans advocate making English the country's official language, as it is in 32 states.[263]		Both Hawaiian and English are official languages in Hawaii, by state law.[264] Alaska recognizes twenty Native languages as well as English.[265] While neither has an official language, New Mexico has laws providing for the use of both English and Spanish, as Louisiana does for English and French.[266] Other states, such as California, mandate the publication of Spanish versions of certain government documents including court forms.[267] Many jurisdictions with large numbers of non-English speakers produce government materials, especially voting information, in the most commonly spoken languages in those jurisdictions.		Several insular territories grant official recognition to their native languages, along with English: Samoan[268] and Chamorro[269] are recognized by American Samoa and Guam, respectively; Carolinian and Chamorro are recognized by the Northern Mariana Islands;[270] Cherokee is officially recognized by the Cherokee Nation within the Cherokee tribal jurisdiction area in eastern Oklahoma;[271] Spanish is an official language of Puerto Rico and is more widely spoken than English there.[272]		The most widely taught foreign languages in the United States, in terms of enrollment numbers from kindergarten through university undergraduate studies, are: Spanish (around 7.2 million students), French (1.5 million), and German (500,000). Other commonly taught languages (with 100,000 to 250,000 learners) include Latin, Japanese, ASL, Italian, and Chinese.[273][274] 18% of all Americans claim to speak at least one language in addition to English.[275]		The First Amendment of the U.S. Constitution guarantees the free exercise of religion and forbids Congress from passing laws respecting its establishment.		Christianity is by far the most common religion practiced in the U.S., but other religions are followed, too. In a 2013 survey, 56% of Americans said that religion played a "very important role in their lives", a far higher figure than that of any other wealthy nation.[278] In a 2009 Gallup poll, 42% of Americans said that they attended church weekly or almost weekly; the figures ranged from a low of 23% in Vermont to a high of 63% in Mississippi.[279] Experts, researchers and authors have referred to the United States as a "Protestant nation" or "founded on Protestant principles,"[280][281][282][283] specifically emphasizing its Calvinist heritage.[284][285][286]		As with other Western countries, the U.S. is becoming less religious. Irreligion is growing rapidly among Americans under 30.[287] Polls show that overall American confidence in organized religion has been declining since the mid to late 1980s,[288] and that younger Americans in particular are becoming increasingly irreligious.[9][289] According to a 2012 study, the Protestant share of the U.S. population had dropped to 48%, thus ending its status as religious category of the majority for the first time.[290][291] Americans with no religion have 1.7 children compared to 2.2 among Christians. The unaffiliated are less likely to get married with 37% marrying compared to 52% of Christians.[292]		According to a 2014 survey, 70.6% of adults identified themselves as Christian,[293] Protestant denominations accounted for 46.5%, while Roman Catholicism, at 20.8%, was the largest individual denomination.[294] The total reporting non-Christian religions in 2014 was 5.9%.[294] Other religions include Judaism (1.9%), Islam (0.9%), Buddhism (0.7%), Hinduism (0.7%).[294] The survey also reported that 22.8% of Americans described themselves as agnostic, atheist or simply having no religion, up from 8.2% in 1990.[294][295][296] There are also Unitarian Universalist, Baha'i, Sikh, Jain, Shinto, Confucian, Taoist, Druid, Native American, Wiccan, humanist and deist communities.[297]		Protestantism is the largest Christian religious grouping in the United States. Baptists collectively form the largest branch of Protestantism, and the Southern Baptist Convention is the largest individual Protestant denomination. About 26% of Americans identify as Evangelical Protestants, while 15% are Mainline and 7% belong to a traditionally Black church. Roman Catholicism in the United States has its origin in the Spanish and French colonization of the Americas, and later grew because of Irish, Italian, Polish, German and Hispanic immigration. Rhode Island has the highest percentage of Catholics with 40 percent of the total population.[298] Lutheranism in the U.S. has its origin in immigration from Northern Europe and Germany. North and South Dakota are the only states in which a plurality of the population is Lutheran. Presbyterianism was introduced in North America by Scottish and Ulster Scots immigrants. Although it has spread across the United States, it is heavily concentrated on the East Coast. Dutch Reformed congregations were founded first in New Amsterdam (New York City) before spreading westward. Utah is the only state where Mormonism is the religion of the majority of the population. The Mormon Corridor also extends to parts of Idaho, Nevada and Wyoming.[299]		The Bible Belt is an informal term for a region in the Southern United States in which socially conservative Evangelical Protestantism is a significant part of the culture and Christian church attendance across the denominations is generally higher than the nation's average. By contrast, religion plays the least important role in New England and in the Western United States.[279]		As of 2007[update], 58% of Americans age 18 and over were married, 6% were widowed, 10% were divorced, and 25% had never been married.[300] Women now work mostly outside the home and receive a majority of bachelor's degrees.[301]		The U.S. teenage pregnancy rate is 26.5 per 1,000 women. The rate has declined by 57% since 1991.[302] In 2013, the highest teenage birth rate was in Alabama, and the lowest in Wyoming.[302][303] Abortion is legal throughout the U.S., owing to Roe v. Wade, a 1973 landmark decision by the Supreme Court of the United States. While the abortion rate is falling, the abortion ratio of 241 per 1,000 live births and abortion rate of 15 per 1,000 women aged 15–44 remain higher than those of most Western nations.[304] In 2013, the average age at first birth was 26 and 40.6% of births were to unmarried women.[305]		The total fertility rate (TFR) was estimated for 2013 at 1.86 births per woman.[306] Adoption in the United States is common and relatively easy from a legal point of view (compared to other Western countries).[307] In 2001, with over 127,000 adoptions, the U.S. accounted for nearly half of the total number of adoptions worldwide.[308] Same-sex marriage is legal nationwide and it is legal for same-sex couples to adopt. Polygamy is illegal throughout the U.S.[309]		The United States is the world's oldest surviving federation. It is a representative democracy, "in which majority rule is tempered by minority rights protected by law".[310] The government is regulated by a system of checks and balances defined by the U.S. Constitution, which serves as the country's supreme legal document.[311] For 2016, the U.S. ranked 21st on the Democracy Index[312] (tied with Italy) and 18th on the Corruption Perceptions Index.[313]		In the American federalist system, citizens are usually subject to three levels of government: federal, state, and local. The local government's duties are commonly split between county and municipal governments. In almost all cases, executive and legislative officials are elected by a plurality vote of citizens by district. There is no proportional representation at the federal level, and it is rare at lower levels.[314]		The federal government is composed of three branches:		The House of Representatives has 435 voting members, each representing a congressional district for a two-year term. House seats are apportioned among the states by population every tenth year. At the 2010 census, seven states had the minimum of one representative, while California, the most populous state, had 53.[319]		The Senate has 100 members with each state having two senators, elected at-large to six-year terms; one third of Senate seats are up for election every other year. The President serves a four-year term and may be elected to the office no more than twice. The President is not elected by direct vote, but by an indirect electoral college system in which the determining votes are apportioned to the states and the District of Columbia.[320] The Supreme Court, led by the Chief Justice of the United States, has nine members, who serve for life.[321]		The state governments are structured in roughly similar fashion; Nebraska uniquely has a unicameral legislature.[323] The governor (chief executive) of each state is directly elected. Some state judges and cabinet officers are appointed by the governors of the respective states, while others are elected by popular vote.		The original text of the Constitution establishes the structure and responsibilities of the federal government and its relationship with the individual states. Article One protects the right to the "great writ" of habeas corpus. The Constitution has been amended 27 times;[324] the first ten amendments, which make up the Bill of Rights, and the Fourteenth Amendment form the central basis of Americans' individual rights. All laws and governmental procedures are subject to judicial review and any law ruled by the courts to be in violation of the Constitution is voided. The principle of judicial review, not explicitly mentioned in the Constitution, was established by the Supreme Court in Marbury v. Madison (1803)[325] in a decision handed down by Chief Justice John Marshall.[326]		The United States is a federal republic of 50 states, a federal district, five territories and eleven uninhabited island possessions.[328] The states and territories are the principal administrative districts in the country. These are divided into subdivisions of counties and independent cities. The District of Columbia is a federal district that contains the capital of the United States, Washington DC.[329] The states and the District of Columbia choose the President of the United States. Each state has presidential electors equal to the number of their Representatives and Senators in Congress; the District of Columbia has three.[330]		Congressional Districts are reapportioned among the states following each decennial Census of Population. Each state then draws single member districts to conform with the census apportionment. The total number of Representatives is 435, and delegate Members of Congress represent the District of Columbia and the five major U.S. territories.[331]		The United States also observes tribal sovereignty of the American Indian nations to a limited degree, as it does with the states' sovereignty. American Indians are U.S. citizens and tribal lands are subject to the jurisdiction of the U.S. Congress and the federal courts. Like the states they have a great deal of autonomy, but also like the states tribes are not allowed to make war, engage in their own foreign relations, or print and issue currency.[332]		The United States has operated under a two-party system for most of its history.[334] For elective offices at most levels, state-administered primary elections choose the major party nominees for subsequent general elections. Since the general election of 1856, the major parties have been the Democratic Party, founded in 1824, and the Republican Party, founded in 1854. Since the Civil War, only one third-party presidential candidate—former president Theodore Roosevelt, running as a Progressive in 1912—has won as much as 20% of the popular vote. The President and Vice-president are elected through the Electoral College system.[335]		Within American political culture, the center-right Republican Party is considered "conservative" and the center-left Democratic Party is considered "liberal".[336][337] The states of the Northeast and West Coast and some of the Great Lakes states, known as "blue states", are relatively liberal. The "red states" of the South and parts of the Great Plains and Rocky Mountains are relatively conservative.		Republican Donald Trump, the winner of the 2016 presidential election, is currently serving as the 45th President of the United States.[338] Current leadership in the Senate includes Republican Vice President Mike Pence, Republican President Pro Tempore Orrin Hatch, Majority Leader Mitch McConnell, and Minority Leader Chuck Schumer.[339] Leadership in the House includes Speaker of the House Paul Ryan, Majority Leader Kevin McCarthy, and Minority Leader Nancy Pelosi.[340]		In the 115th United States Congress, both the House of Representatives and the Senate are controlled by the Republican Party. The Senate currently consists of 52 Republicans, and 46 Democrats with 2 Independents who caucus with the Democrats; the House consists of 241 Republicans and 194 Democrats.[341] In state governorships, there are 33 Republicans, 16 Democrats, and 1 Independent.[342] Among the DC mayor and the 5 territorial governors, there are 2 Republicans, 1 Democrat, 1 New Progressive, and 2 Independents.[343]		The United States has an established structure of foreign relations. It is a permanent member of the United Nations Security Council, and New York City is home to the United Nations Headquarters. It is a member of the G7,[345] G20, and Organisation for Economic Co-operation and Development. Almost all countries have embassies in Washington, D.C., and many have consulates around the country. Likewise, nearly all nations host American diplomatic missions. However, Iran, North Korea, Bhutan, and the Republic of China (Taiwan) do not have formal diplomatic relations with the United States (although the U.S. still maintains relations with Taiwan and supplies it with military equipment).[346]		The United States has a "Special Relationship" with the United Kingdom[347] and strong ties with Canada,[348] Australia,[349] New Zealand,[350] the Philippines,[351] Japan,[352] South Korea,[353] Israel,[354] and several European Union countries, including France, Italy, Germany, and Spain. It works closely with fellow NATO members on military and security issues and with its neighbors through the Organization of American States and free trade agreements such as the trilateral North American Free Trade Agreement with Canada and Mexico. In 2008, the United States spent a net $25.4 billion on official development assistance, the most in the world. As a share of America's large gross national income (GNI), however, the U.S. contribution of 0.18% ranked last among 22 donor states. By contrast, private overseas giving by Americans is relatively generous.[355]		The U.S. exercises full international defense authority and responsibility for three sovereign nations through Compact of Free Association with Micronesia, the Marshall Islands and Palau. These are Pacific island nations, once part of the U.S.-administered Trust Territory of the Pacific Islands after World War II, which gained independence in subsequent years.[356]		Taxes in the United States are levied at the federal, state, and local government levels. These include taxes on income, payroll, property, sales, imports, estates and gifts, as well as various fees. In 2010 taxes collected by federal, state and municipal governments amounted to 24.8% of GDP.[358] During FY2012, the federal government collected approximately $2.45 trillion in tax revenue, up $147 billion or 6% versus FY2011 revenues of $2.30 trillion. Primary receipt categories included individual income taxes ($1,132B or 47%), Social Security/Social Insurance taxes ($845B or 35%), and corporate taxes ($242B or 10%).[359] Based on CBO estimates,[360] under 2013 tax law the top 1% will be paying the highest average tax rates since 1979, while other income groups will remain at historic lows.[361]		U.S. taxation is generally progressive, especially the federal income taxes, and is among the most progressive in the developed world.[362][363][364][365][366] The highest 10% of income earners pay a majority of federal taxes,[367] and about half of all taxes.[368] Payroll taxes for Social Security are a flat regressive tax, with no tax charged on income above $118,500 (for 2015 and 2016) and no tax at all paid on unearned income from things such as stocks and capital gains.[369][370] The historic reasoning for the regressive nature of the payroll tax is that entitlement programs have not been viewed as welfare transfers.[371][372] However, according to the Congressional Budget Office the net effect of Social Security is that the benefit to tax ratio ranges from roughly 70% for the top earnings quintile to about 170% for the lowest earning quintile, making the system progressive.[373]		The top 10% paid 51.8% of total federal taxes in 2009, and the top 1%, with 13.4% of pre-tax national income, paid 22.3% of federal taxes.[374] In 2013 the Tax Policy Center projected total federal effective tax rates of 35.5% for the top 1%, 27.2% for the top quintile, 13.8% for the middle quintile, and −2.7% for the bottom quintile.[375][376] The incidence of corporate income tax has been a matter of considerable ongoing controversy for decades.[365][377] State and local taxes vary widely, but are generally less progressive than federal taxes as they rely heavily on broadly borne regressive sales and property taxes that yield less volatile revenue streams, though their consideration does not eliminate the progressive nature of overall taxation.[365][378]		During FY 2012, the federal government spent $3.54 trillion on a budget or cash basis, down $60 billion or 1.7% vs. FY 2011 spending of $3.60 trillion. Major categories of FY 2012 spending included: Medicare & Medicaid ($802B or 23% of spending), Social Security ($768B or 22%), Defense Department ($670B or 19%), non-defense discretionary ($615B or 17%), other mandatory ($461B or 13%) and interest ($223B or 6%).[359]		The total national debt of the United States in the United States was $18.527 trillion (106% of the GDP) in 2014.[379][fn 11]		The President holds the title of commander-in-chief of the nation's armed forces and appoints its leaders, the Secretary of Defense and the Joint Chiefs of Staff. The United States Department of Defense administers the armed forces, including the Army, Marine Corps, Navy, and Air Force. The Coast Guard is run by the Department of Homeland Security in peacetime and by the Department of the Navy during times of war. In 2008, the armed forces had 1.4 million personnel on active duty. The Reserves and National Guard brought the total number of troops to 2.3 million. The Department of Defense also employed about 700,000 civilians, not including contractors.[384]		Military service is voluntary, though conscription may occur in wartime through the Selective Service System.[385] American forces can be rapidly deployed by the Air Force's large fleet of transport aircraft, the Navy's 10 active aircraft carriers, and Marine expeditionary units at sea with the Navy's Atlantic and Pacific fleets. The military operates 865 bases and facilities abroad,[386] and maintains deployments greater than 100 active duty personnel in 25 foreign countries.[387]		The military budget of the United States in 2011 was more than $700 billion, 41% of global military spending and equal to the next 14 largest national military expenditures combined. At 4.7% of GDP, the rate was the second-highest among the top 15 military spenders, after Saudi Arabia.[388] U.S. defense spending as a percentage of GDP ranked 23rd globally in 2012 according to the CIA.[389] Defense's share of U.S. spending has generally declined in recent decades, from Cold War peaks of 14.2% of GDP in 1953 and 69.5% of federal outlays in 1954 to 4.7% of GDP and 18.8% of federal outlays in 2011.[390]		The proposed base Department of Defense budget for 2012, $553 billion, was a 4.2% increase over 2011; an additional $118 billion was proposed for the military campaigns in Iraq and Afghanistan.[391] The last American troops serving in Iraq departed in December 2011;[392] 4,484 service members were killed during the Iraq War.[393] Approximately 90,000 U.S. troops were serving in Afghanistan in April 2012;[394] by November 8, 2013 2,285 had been killed during the War in Afghanistan.[395]		Law enforcement in the United States is primarily the responsibility of local police and sheriff's departments, with state police providing broader services. The New York City Police Department (NYPD) is the largest in the country. Federal agencies such as the Federal Bureau of Investigation (FBI) and the U.S. Marshals Service have specialized duties, including protecting civil rights, national security and enforcing U.S. federal courts' rulings and federal laws.[397] At the federal level and in almost every state, a legal system operates on a common law. State courts conduct most criminal trials; federal courts handle certain designated crimes as well as certain appeals from the state criminal courts. Plea bargaining in the United States is very common; the vast majority of criminal cases in the country are settled by plea bargain rather than jury trial.[398]		In 2015, there were 15,696 murders which was 1,532 more than in 2014, a 10.8 per cent increase, the largest since 1971.[399] The murder rate in 2015 was 4.9 per 100,000 people.[400] The national clearance rate for homicides in 2015 was 64.1%, compared to 90% in 1965.[401] In 2012 there were 4.7 murders per 100,000 persons in the United States, a 54% decline from the modern peak of 10.2 in 1980.[402] In 2001–2, the United States had above-average levels of violent crime and particularly high levels of gun violence compared to other developed nations.[403] A cross-sectional analysis of the World Health Organization Mortality Database from 2010 showed that United States "homicide rates were 7.0 times higher than in other high-income countries, driven by a gun homicide rate that was 25.2 times higher."[404] Gun ownership rights continue to be the subject of contentious political debate.		From 1980 through 2008 males represented 77% of homicide victims and 90% of offenders. Blacks committed 52.5% of all homicides during that span, at a rate almost eight times that of whites ("whites" includes most Hispanics), and were victimized at a rate six times that of whites. Most homicides were intraracial, with 93% of black victims killed by blacks and 84% of white victims killed by whites.[405] In 2012, Louisiana had the highest rate of murder and non-negligent manslaughter in the U.S., and New Hampshire the lowest.[406] The FBI's Uniform Crime Reports estimates that there were 3,246 violent and property crimes per 100,000 residents in 2012, for a total of over 9 million total crimes.[407]		Capital punishment is sanctioned in the United States for certain federal and military crimes, and used in 31 states.[408][409] No executions took place from 1967 to 1977, owing in part to a U.S. Supreme Court ruling striking down arbitrary imposition of the death penalty. In 1976, that Court ruled that, under appropriate circumstances, capital punishment may constitutionally be imposed. Since the decision there have been more than 1,300 executions, a majority of these taking place in three states: Texas, Virginia, and Oklahoma.[410] Meanwhile, several states have either abolished or struck down death penalty laws. In 2015, the country had the fifth-highest number of executions in the world, following China, Iran, Pakistan and Saudi Arabia.[411]		The United States has the highest documented incarceration rate and total prison population in the world.[412] At the start of 2008, more than 2.3 million people were incarcerated, more than one in every 100 adults.[413] In December 2012, the combined U.S. adult correctional systems supervised about 6,937,600 offenders. About 1 in every 35 adult residents in the United States was under some form of correctional supervision in December 2012, the lowest rate observed since 1997.[414] The prison population has quadrupled since 1980,[415] and state and local spending on prisons and jails has grown three times as much as that spent on public education during the same period.[416] However, the imprisonment rate for all prisoners sentenced to more than a year in state or federal facilities is 478 per 100,000 in 2013[417] and the rate for pre-trial/remand prisoners is 153 per 100,000 residents in 2012.[418] The country's high rate of incarceration is largely due to changes in sentencing guidelines and drug policies.[419] According to the Federal Bureau of Prisons, the majority of inmates held in federal prisons are convicted of drug offenses.[420] The privatization of prisons and prison services which began in the 1980s has been a subject of debate.[421][422] In 2008, Louisiana had the highest incarceration rate,[423] and Maine the lowest.[424]		The United States has a capitalist mixed economy[433] which is fueled by abundant natural resources and high productivity.[434] According to the International Monetary Fund, the U.S. GDP of $16.8 trillion constitutes 24% of the gross world product at market exchange rates and over 19% of the gross world product at purchasing power parity (PPP).[435]		The US's nominal GDP is estimated to be $17.528 trillion as of 2014[update][436] From 1983 to 2008, U.S. real compounded annual GDP growth was 3.3%, compared to a 2.3% weighted average for the rest of the G7.[437] The country ranks ninth in the world in nominal GDP per capita and sixth in GDP per capita at PPP.[435] The U.S. dollar is the world's primary reserve currency.[438]		The United States is the largest importer of goods and second-largest exporter, though exports per capita are relatively low. In 2010, the total U.S. trade deficit was $635 billion.[439] Canada, China, Mexico, Japan, and Germany are its top trading partners.[440] In 2010, oil was the largest import commodity, while transportation equipment was the country's largest export.[439] Japan is the largest foreign holder of U.S. public debt.[441] The largest holder of the U.S. debt are American entities, including federal government accounts and the Federal Reserve, who hold the majority of the debt.[442][443][444][445][fn 12]		In 2009, the private sector was estimated to constitute 86.4% of the economy, with federal government activity accounting for 4.3% and state and local government activity (including federal transfers) the remaining 9.3%.[448] The number of employees at all levels of government outnumber those in manufacturing by 1.7 to 1.[449] While its economy has reached a postindustrial level of development and its service sector constitutes 67.8% of GDP, the United States remains an industrial power.[450] The leading business field by gross business receipts is wholesale and retail trade; by net income it is manufacturing.[451] In the franchising business model, McDonald's and Subway are the two most recognized brands in the world. Coca-Cola is the most recognized soft drink company in the world.[452]		Chemical products are the leading manufacturing field.[453] The United States is the largest producer of oil in the world, as well as its second-largest importer.[454] It is the world's number one producer of electrical and nuclear energy, as well as liquid natural gas, sulfur, phosphates, and salt. The National Mining Association provides data pertaining to coal and minerals that include beryllium, copper, lead, magnesium, zinc, titanium and others.[455][456]		Agriculture accounts for just under 1% of GDP,[450] yet the United States is the world's top producer of corn[457] and soybeans.[458] The National Agricultural Statistics Service maintains agricultural statistics for products that include peanuts, oats, rye, wheat, rice, cotton, corn, barley, hay, sunflowers, and oilseeds. In addition, the United States Department of Agriculture (USDA) provides livestock statistics regarding beef, poultry, pork, and dairy products. The country is the primary developer and grower of genetically modified food, representing half of the world's biotech crops.[459]		Consumer spending comprises 68% of the U.S. economy in 2015.[460] In August 2010, the American labor force consisted of 154.1 million people. With 21.2 million people, government is the leading field of employment. The largest private employment sector is health care and social assistance, with 16.4 million people. About 12% of workers are unionized, compared to 30% in Western Europe.[461] The World Bank ranks the United States first in the ease of hiring and firing workers.[462] The United States is ranked among the top three in the Global Competitiveness Report as well. It has a smaller welfare state and redistributes less income through government action than European nations tend to.[463]		The United States is the only advanced economy that does not guarantee its workers paid vacation[464] and is one of just a few countries in the world without paid family leave as a legal right, with the others being Papua New Guinea, Suriname and Liberia.[465] While federal law currently does not require sick leave, it is a common benefit for government workers and full-time employees at corporations.[466] 74% of full-time American workers get paid sick leave, according to the Bureau of Labor Statistics, although only 24% of part-time workers get the same benefits.[466] In 2009, the United States had the third-highest workforce productivity per person in the world, behind Luxembourg and Norway. It was fourth in productivity per hour, behind those two countries and the Netherlands.[467]		The 2008–2012 global recession significantly affected the United States, with output still below potential according to the Congressional Budget Office.[468] It brought high unemployment (which has been decreasing but remains above pre-recession levels), along with low consumer confidence, the continuing decline in home values and increase in foreclosures and personal bankruptcies, an escalating federal debt crisis, inflation, and rising petroleum and food prices. There remains a record proportion of long-term unemployed, continued decreasing household income, and tax and federal budget increases.[469][470][471]		Americans have the highest average household and employee income among OECD nations, and in 2007 had the second-highest median household income.[472][473][474] According to the Census Bureau, median household income was $53,657 in 2014.[475] Despite accounting for only 4.4% of the global population, Americans collectively possess 41.6% of the world's total wealth,[476] and Americans make up roughly half of the world's population of millionaires.[477] The Global Food Security Index ranked the U.S. number one for food affordability and overall food security in March 2013.[478] Americans on average have over twice as much living space per dwelling and per person as European Union residents, and more than every EU nation.[479] For 2013 the United Nations Development Programme ranked the United States 5th among 187 countries in its Human Development Index and 28th in its inequality-adjusted HDI (IHDI).[480]		There has been a widening gap between productivity and median incomes since the 1970s.[481] However, the gap between total compensation and productivity is not as wide because of increased employee benefits such as health insurance.[482] While inflation-adjusted ("real") household income had been increasing almost every year from 1947 to 1999, it has since been flat on balance and has even decreased recently.[483] According to Congressional Research Service, during this same period, immigration to the United States increased, while the lower 90% of tax filers incomes became stagnant, and eventually decreasing since 2000.[484] The rise in the share of total annual income received by the top 1 percent, which has more than doubled from 9 percent in 1976 to 20 percent in 2011, has significantly affected income inequality,[485] leaving the United States with one of the widest income distributions among OECD nations.[486] The post-recession income gains have been very uneven, with the top 1 percent capturing 95 percent of the income gains from 2009 to 2012.[487] The extent and relevance of income inequality is a matter of debate.[488][disputed – discuss][489]		Wealth, like income and taxes, is highly concentrated; the richest 10% of the adult population possess 72% of the country's household wealth, while the bottom half claim only 2%.[491] Between June 2007 and November 2008 the global recession led to falling asset prices around the world. Assets owned by Americans lost about a quarter of their value.[492] Since peaking in the second quarter of 2007, household wealth was down $14 trillion, but has since increased $14 trillion over 2006 levels.[493][494] At the end of 2014, household debt amounted to $11.8 trillion,[495] down from $13.8 trillion at the end of 2008.[496]		There were about 578,424 sheltered and unsheltered homeless persons in the U.S. in January 2014, with almost two-thirds staying in an emergency shelter or transitional housing program.[497] In 2011 16.7 million children lived in food-insecure households, about 35% more than 2007 levels, though only 1.1% of U.S. children, or 845,000, saw reduced food intake or disrupted eating patterns at some point during the year, and most cases were not chronic.[498] According to a 2014 report by the Census Bureau, one in five young adults lives in poverty today, up from one in seven in 1980.[499]		Personal transportation is dominated by automobiles, which operate on a network of 4 million miles (6.4 million km) of public roads,[501] including one of the world's longest highway systems at 57,000 miles (91700 km).[502] The world's second-largest automobile market,[503] the United States has the highest rate of per-capita vehicle ownership in the world, with 765 vehicles per 1,000 Americans.[504] About 40% of personal vehicles are vans, SUVs, or light trucks.[505] The average American adult (accounting for all drivers and non-drivers) spends 55 minutes driving every day, traveling 29 miles (47 km).[506]		Mass transit accounts for 9% of total U.S. work trips.[508][509] Transport of goods by rail is extensive, though relatively low numbers of passengers (approximately 31 million annually) use intercity rail to travel, partly because of the low population density throughout much of the U.S. interior.[510][511] However, ridership on Amtrak, the national intercity passenger rail system, grew by almost 37% between 2000 and 2010.[512] Also, light rail development has increased in recent years.[513] Bicycle usage for work commutes is minimal.[514]		The civil airline industry is entirely privately owned and has been largely deregulated since 1978, while most major airports are publicly owned.[515] The three largest airlines in the world by passengers carried are U.S.-based; American Airlines is number one after its 2013 acquisition by US Airways.[516] Of the world's 50 busiest passenger airports, 16 are in the United States, including the busiest, Hartsfield–Jackson Atlanta International Airport, and the fourth-busiest, O'Hare International Airport in Chicago.[517] In the aftermath of the 9/11 attacks of 2001, the Transportation Security Administration was created to police airports and commercial airliners.		The United States energy market is about 29,000 terawatt hours per year.[518] Energy consumption per capita is 7.8 tons (7076 kg) of oil equivalent per year, the 10th-highest rate in the world. In 2005, 40% of this energy came from petroleum, 23% from coal, and 22% from natural gas. The remainder was supplied by nuclear power and renewable energy sources.[519] The United States is the world's largest consumer of petroleum.[520]		For decades, nuclear power has played a limited role relative to many other developed countries, in part because of public perception in the wake of a 1979 accident. In 2007, several applications for new nuclear plants were filed.[521] The United States has 27% of global coal reserves.[522] It is the world's largest producer of natural gas and crude oil.[523]		Issues that affect water supply in the United States include droughts in the West, water scarcity, pollution, a backlog of investment, concerns about the affordability of water for the poorest, and a rapidly retiring workforce. Increased variability and intensity of rainfall as a result of climate change is expected to produce both more severe droughts and flooding, with potentially serious consequences for water supply and for pollution from combined sewer overflows.[524][525][fn 13]		American public education is operated by state and local governments, regulated by the United States Department of Education through restrictions on federal grants. In most states, children are required to attend school from the age of six or seven (generally, kindergarten or first grade) until they turn 18 (generally bringing them through twelfth grade, the end of high school); some states allow students to leave school at 16 or 17.[528]		About 12% of children are enrolled in parochial or nonsectarian private schools. Just over 2% of children are homeschooled.[529] The U.S. spends more on education per student than any nation in the world, spending more than $11,000 per elementary student in 2010 and more than $12,000 per high school student.[530] Some 80% of U.S. college students attend public universities.[531]		The United States has many competitive private and public institutions of higher education. The majority of the world's top universities listed by different ranking organizations are in the U.S.[532][533][534] There are also local community colleges with generally more open admission policies, shorter academic programs, and lower tuition. Of Americans 25 and older, 84.6% graduated from high school, 52.6% attended some college, 27.2% earned a bachelor's degree, and 9.6% earned graduate degrees.[535] The basic literacy rate is approximately 99%.[10][536] The United Nations assigns the United States an Education Index of 0.97, tying it for 12th in the world.[537]		As for public expenditures on higher education, the U.S. trails some other OECD nations but spends more per student than the OECD average, and more than all nations in combined public and private spending.[530][538] As of 2012[update], student loan debt exceeded one trillion dollars, more than Americans owe on credit cards.[539]		The United States is home to many cultures and a wide variety of ethnic groups, traditions, and values.[28][540] Aside from the Native American, Native Hawaiian, and Native Alaskan populations, nearly all Americans or their ancestors settled or immigrated within the past five centuries.[541] Mainstream American culture is a Western culture largely derived from the traditions of European immigrants with influences from many other sources, such as traditions brought by slaves from Africa.[28][542] More recent immigration from Asia and especially Latin America has added to a cultural mix that has been described as both a homogenizing melting pot, and a heterogeneous salad bowl in which immigrants and their descendants retain distinctive cultural characteristics.[28]		Core American culture was established by Protestant British colonists and shaped by the frontier settlement process, with the traits derived passed down to descendants and transmitted to immigrants through assimilation. Americans have traditionally been characterized by a strong work ethic, competitiveness, and individualism,[543] as well as a unifying belief in an "American creed" emphasizing liberty, equality, private property, democracy, rule of law, and a preference for limited government.[544] Americans are extremely charitable by global standards. According to a 2006 British study, Americans gave 1.67% of GDP to charity, more than any other nation studied, more than twice the second place British figure of 0.73%, and around twelve times the French figure of 0.14%.[545][546]		The American Dream, or the perception that Americans enjoy high social mobility, plays a key role in attracting immigrants.[547] Whether this perception is realistic has been a topic of debate.[548][549][550][551][437][552] While mainstream culture holds that the United States is a classless society,[553] scholars identify significant differences between the country's social classes, affecting socialization, language, and values.[554] Americans' self-images, social viewpoints, and cultural expectations are associated with their occupations to an unusually close degree.[555] While Americans tend greatly to value socioeconomic achievement, being ordinary or average is generally seen as a positive attribute.[556]		Mainstream American cuisine is similar to that in other Western countries. Wheat is the primary cereal grain with about three-quarters of grain products made of wheat flour[557] and many dishes use indigenous ingredients, such as turkey, venison, potatoes, sweet potatoes, corn, squash, and maple syrup which were consumed by Native Americans and early European settlers.[558] These home grown foods are part of a shared national menu on one of America's most popular holidays; Thanksgiving, when some Americans make traditional foods to celebrate the occasion.[559]		Characteristic dishes such as apple pie, fried chicken, pizza, hamburgers, and hot dogs derive from the recipes of various immigrants. French fries, Mexican dishes such as burritos and tacos, and pasta dishes freely adapted from Italian sources are widely consumed.[561] Americans drink three times as much coffee as tea.[562] Marketing by U.S. industries is largely responsible for making orange juice and milk ubiquitous breakfast beverages.[563][564]		American eating habits owe a great deal to that of their British culinary roots with some variations. Although American lands could grow newer vegetables that Britain could not, most colonists would not eat these new foods until accepted by Europeans.[565] Over time American foods changed to a point that food critic, John L. Hess stated in 1972: "Our founding fathers were as far superior to our present political leaders in the quality of their food as they were in the quality of their prose and intelligence".[566]		The American fast food industry, the world's largest,[567] pioneered the drive-through format in the 1940s.[568] Fast food consumption has sparked health concerns. During the 1980s and 1990s, Americans' caloric intake rose 24%;[561] frequent dining at fast food outlets is associated with what public health officials call the American "obesity epidemic".[569] Highly sweetened soft drinks are widely popular, and sugared beverages account for nine percent of American caloric intake.[570]		In the 18th and early 19th centuries, American art and literature took most of its cues from Europe. Writers such as Nathaniel Hawthorne, Edgar Allan Poe, and Henry David Thoreau established a distinctive American literary voice by the middle of the 19th century. Mark Twain and poet Walt Whitman were major figures in the century's second half; Emily Dickinson, virtually unknown during her lifetime, is now recognized as an essential American poet.[571] A work seen as capturing fundamental aspects of the national experience and character—such as Herman Melville's Moby-Dick (1851), Twain's The Adventures of Huckleberry Finn (1885), F. Scott Fitzgerald's The Great Gatsby (1925) and Harper Lee's To Kill a Mockingbird (1960)—may be dubbed the "Great American Novel".[572]		Twelve U.S. citizens have won the Nobel Prize in Literature, most recently Bob Dylan in 2016. William Faulkner, Ernest Hemingway and John Steinbeck are often named among the most influential writers of the 20th century.[573] Popular literary genres such as the Western and hardboiled crime fiction developed in the United States. The Beat Generation writers opened up new literary approaches, as have postmodernist authors such as John Barth, Thomas Pynchon, and Don DeLillo.[574]		The transcendentalists, led by Thoreau and Ralph Waldo Emerson, established the first major American philosophical movement. After the Civil War, Charles Sanders Peirce and then William James and John Dewey were leaders in the development of pragmatism. In the 20th century, the work of W. V. O. Quine and Richard Rorty, and later Noam Chomsky, brought analytic philosophy to the fore of American philosophical academia. John Rawls and Robert Nozick led a revival of political philosophy. Cornel West and Judith Butler have led a continental tradition in American philosophical academia. Chicago school economists like Milton Friedman, James M. Buchanan, and Thomas Sowell have affected various fields in social and political philosophy.[575][576]		In the visual arts, the Hudson River School was a mid-19th-century movement in the tradition of European naturalism. The realist paintings of Thomas Eakins are now widely celebrated. The 1913 Armory Show in New York City, an exhibition of European modernist art, shocked the public and transformed the U.S. art scene.[577] Georgia O'Keeffe, Marsden Hartley, and others experimented with new, individualistic styles. Major artistic movements such as the abstract expressionism of Jackson Pollock and Willem de Kooning and the pop art of Andy Warhol and Roy Lichtenstein developed largely in the United States. The tide of modernism and then postmodernism has brought fame to American architects such as Frank Lloyd Wright, Philip Johnson, and Frank Gehry.[578] Americans have long been important in the modern artistic medium of photography, with major photographers including Alfred Stieglitz, Edward Steichen, and Ansel Adams.[579]		One of the first major promoters of American theater was impresario P. T. Barnum, who began operating a lower Manhattan entertainment complex in 1841. The team of Harrigan and Hart produced a series of popular musical comedies in New York starting in the late 1870s. In the 20th century, the modern musical form emerged on Broadway; the songs of musical theater composers such as Irving Berlin, Cole Porter, and Stephen Sondheim have become pop standards. Playwright Eugene O'Neill won the Nobel literature prize in 1936; other acclaimed U.S. dramatists include multiple Pulitzer Prize winners Tennessee Williams, Edward Albee, and August Wilson.[581]		Though little known at the time, Charles Ives's work of the 1910s established him as the first major U.S. composer in the classical tradition, while experimentalists such as Henry Cowell and John Cage created a distinctive American approach to classical composition. Aaron Copland and George Gershwin developed a new synthesis of popular and classical music.		Choreographers Isadora Duncan and Martha Graham helped create modern dance, while George Balanchine and Jerome Robbins were leaders in 20th-century ballet.		The rhythmic and lyrical styles of African-American music have deeply influenced American music at large, distinguishing it from European traditions. Elements from folk idioms such as the blues and what is now known as old-time music were adopted and transformed into popular genres with global audiences. Jazz was developed by innovators such as Louis Armstrong and Duke Ellington early in the 20th century. Country music developed in the 1920s, and rhythm and blues in the 1940s.[582]		Elvis Presley and Chuck Berry were among the mid-1950s pioneers of rock and roll. In the 1960s, Bob Dylan emerged from the folk revival to become one of America's most celebrated songwriters and James Brown led the development of funk. More recent American creations include hip hop and house music. American pop stars such as Presley, Michael Jackson, and Madonna have become global celebrities,[582] as have contemporary musical artists such as Taylor Swift, Britney Spears, Katy Perry, and Beyoncé as well as hip hop artists Jay-Z, Eminem and Kanye West.[583] Rock bands such as Metallica, the Eagles, and Aerosmith are among the highest grossing in worldwide sales.[584][585][586]		Hollywood, a northern district of Los Angeles, California, is one of the leaders in motion picture production.[587] The world's first commercial motion picture exhibition was given in New York City in 1894, using Thomas Edison's Kinetoscope.[588] The next year saw the first commercial screening of a projected film, also in New York, and the United States was in the forefront of sound film's development in the following decades. Since the early 20th century, the U.S. film industry has largely been based in and around Hollywood, although in the 21st century an increasing number of films are not made there, and film companies have been subject to the forces of globalization.[589]		Director D. W. Griffith, the top American filmmaker during the silent film period, was central to the development of film grammar, and producer/entrepreneur Walt Disney was a leader in both animated film and movie merchandising.[590] Directors such as John Ford redefined the image of the American Old West and history, and, like others such as John Huston, broadened the possibilities of cinema with location shooting, with great influence on subsequent directors. The industry enjoyed its golden years, in what is commonly referred to as the "Golden Age of Hollywood", from the early sound period until the early 1960s,[591] with screen actors such as John Wayne and Marilyn Monroe becoming iconic figures.[592][593] In the 1970s, film directors such as Martin Scorsese, Francis Ford Coppola and Robert Altman were a vital component in what became known as "New Hollywood" or the "Hollywood Renaissance",[594] grittier films influenced by French and Italian realist pictures of the post-war period.[595] Since, directors such as Steven Spielberg, George Lucas and James Cameron have gained renown for their blockbuster films, often characterized by high production costs, and in return, high earnings at the box office, with Cameron's Avatar (2009) earning more than $2 billion.[596]		Notable films topping the American Film Institute's AFI 100 list include Orson Welles's Citizen Kane (1941), which is frequently cited as the greatest film of all time,[597][598] Casablanca (1942), The Godfather (1972), Gone with the Wind (1939), Lawrence of Arabia (1962), The Wizard of Oz (1939), The Graduate (1967), On the Waterfront (1954), Schindler's List (1993), Singin' in the Rain (1952), It's a Wonderful Life (1946) and Sunset Boulevard (1950).[599] The Academy Awards, popularly known as the Oscars, have been held annually by the Academy of Motion Picture Arts and Sciences since 1929,[600] and the Golden Globe Awards have been held annually since January 1944.[601]		American football is by several measures the most popular spectator sport;[603] the National Football League (NFL) has the highest average attendance of any sports league in the world, and the Super Bowl is watched by millions globally. Baseball has been regarded as the U.S. national sport since the late 19th century, with Major League Baseball (MLB) being the top league. Basketball and ice hockey are the country's next two leading professional team sports, with the top leagues being the National Basketball Association (NBA) and the National Hockey League (NHL). These four major sports, when played professionally, each occupy a season at different, but overlapping, times of the year. College football and basketball attract large audiences.[604] In soccer, the country hosted the 1994 FIFA World Cup, the men's national soccer team qualified for ten World Cups and the women's team has won the FIFA Women's World Cup three times; Major League Soccer is the sport's highest league in the United States (featuring 19 American and 3 Canadian teams). The market for professional sports in the United States is roughly $69 billion, roughly 50% larger than that of all of Europe, the Middle East, and Africa combined.[605]		Eight Olympic Games have taken place in the United States. As of 2014, the United States has won 2,400 medals at the Summer Olympic Games, more than any other country, and 281 in the Winter Olympic Games, the second most behind Norway.[606] While most major U.S. sports have evolved out of European practices, basketball, volleyball, skateboarding, and snowboarding are American inventions, some of which have become popular in other countries. Lacrosse and surfing arose from Native American and Native Hawaiian activities that predate Western contact.[607] The most watched individual sports are golf and auto racing, particularly NASCAR.[608][609] Rugby union is considered the fastest growing sport in the U.S., with registered players numbered at 115,000+ and a further 1.2 million participants.[610]		The four major broadcasters in the U.S. are the National Broadcasting Company (NBC), Columbia Broadcasting System (CBS), the American Broadcasting Company (ABC), and Fox. The four major broadcast television networks are all commercial entities. Cable television offers hundreds of channels catering to a variety of niches.[611] Americans listen to radio programming, also largely commercial, on average just over two-and-a-half hours a day.[612]		In 1998, the number of U.S. commercial radio stations had grown to 4,793 AM stations and 5,662 FM stations. In addition, there are 1,460 public radio stations. Most of these stations are run by universities and public authorities for educational purposes and are financed by public or private funds, subscriptions and corporate underwriting. Much public-radio broadcasting is supplied by NPR (formerly National Public Radio). NPR was incorporated in February 1970 under the Public Broadcasting Act of 1967; its television counterpart, PBS, was also created by the same legislation. (NPR and PBS are operated separately from each other.) As of September 30, 2014[update], there are 15,433 licensed full-power radio stations in the U.S. according to the U.S. Federal Communications Commission (FCC).[613]		Well-known newspapers are The Wall Street Journal, The New York Times and USA Today. Although the cost of publishing has increased over the years, the price of newspapers has generally remained low, forcing newspapers to rely more on advertising revenue and on articles provided by a major wire service, such as the Associated Press or Reuters, for their national and world coverage. With very few exceptions, all the newspapers in the U.S. are privately owned, either by large chains such as Gannett or McClatchy, which own dozens or even hundreds of newspapers; by small chains that own a handful of papers; or in a situation that is increasingly rare, by individuals or families. Major cities often have "alternative weeklies" to complement the mainstream daily papers, for example, New York City's The Village Voice or Los Angeles' LA Weekly, to name two of the best-known. Major cities may also support a local business journal, trade papers relating to local industries, and papers for local ethnic and social groups. Early versions of the American newspaper comic strip and the American comic book began appearing in the 19th century. In 1938, Superman, the comic book superhero of DC Comics, developed into an American icon.[614] Aside from web portals and search engines, the most popular websites are Facebook, YouTube, Wikipedia, Yahoo!, eBay, Amazon, and Twitter.[615]		More than 800 publications are produced in Spanish, the second most commonly used language in the United States behind English.[616][617]		The United States has been a leader in technological innovation since the late 19th century and scientific research since the mid-20th century. Methods for producing interchangeable parts were developed by the U.S. War Department by the Federal Armories during the first half of the 19th century. This technology, along with the establishment of a machine tool industry, enabled the U.S. to have large scale manufacturing of sewing machines, bicycles and other items in the late 19th century and became known as the American system of manufacturing. Factory electrification in the early 20th century and introduction of the assembly line and other labor saving techniques created the system called mass production.[618]		In 1876, Alexander Graham Bell was awarded the first U.S. patent for the telephone. Thomas Edison's research laboratory, one of the first of its kind, developed the phonograph, the first long-lasting light bulb, and the first viable movie camera.[619] The latter lead to emergence of the worldwide entertainment industry. In the early 20th century, the automobile companies of Ransom E. Olds and Henry Ford popularized the assembly line. The Wright brothers, in 1903, made the first sustained and controlled heavier-than-air powered flight.[620]		The rise of Fascism and Nazism in the 1920s and 1930s led many European scientists, including Albert Einstein, Enrico Fermi, and John von Neumann, to immigrate to the United States.[621] During World War II, the Manhattan Project developed nuclear weapons, ushering in the Atomic Age, while the Space Race produced rapid advances in rocketry, materials science, and aeronautics.[622][623]		The invention of the transistor in the 1950s, a key active component in practically all modern electronics, led to many technological developments and a significant expansion of the U.S. technology industry.[624][625][626] This in turn led to the establishment of many new technology companies and regions around the country such as in Silicon Valley in California. Advancements by American microprocessor companies such as Advanced Micro Devices (AMD), and Intel along with both computer software and hardware companies that include Adobe Systems, Apple Inc., IBM, Microsoft, and Sun Microsystems created and popularized the personal computer. The ARPANET was developed in the 1960s to meet Defense Department requirements, and became the first of a series of networks which evolved into the Internet.[627]		These advancements then lead to greater personalization of technology for individual use.[628] As of 2013[update], 83.8% of American households owned at least one computer, and 73.3% had high-speed Internet service.[629] 91% of Americans also own a mobile phone as of May 2013[update].[630] The United States ranks highly with regard to freedom of use of the internet.[631]		In the 21st century, approximately two-thirds of research and development funding comes from the private sector.[632] The United States leads the world in scientific research papers and impact factor.[633]		The United States has a life expectancy of 79.8 years at birth, up from 75.2 years in 1990.[634][635][636] The infant mortality rate of 6.17 per thousand places the United States 56th-lowest out of 224 countries.[637]		Increasing obesity in the United States and health improvements elsewhere contributed to lowering the country's rank in life expectancy from 11th in the world in 1987, to 42nd in 2007.[638] Obesity rates have more than doubled in the last 30 years, are the highest in the industrialized world, and are among the highest anywhere.[639][640] Approximately one-third of the adult population is obese and an additional third is overweight.[641] Obesity-related type 2 diabetes is considered epidemic by health care professionals.[642]		In 2010, coronary artery disease, lung cancer, stroke, chronic obstructive pulmonary diseases, and traffic accidents caused the most years of life lost in the U.S. Low back pain, depression, musculoskeletal disorders, neck pain, and anxiety caused the most years lost to disability. The most deleterious risk factors were poor diet, tobacco smoking, obesity, high blood pressure, high blood sugar, physical inactivity, and alcohol use. Alzheimer's disease, drug abuse, kidney disease and cancer, and falls caused the most additional years of life lost over their age-adjusted 1990 per-capita rates.[636] U.S. teenage pregnancy and abortion rates are substantially higher than in other Western nations, especially among blacks and Hispanics.[643]		The U.S. is a global leader in medical innovation. America solely developed or contributed significantly to 9 of the top 10 most important medical innovations since 1975 as ranked by a 2001 poll of physicians, while the European Union and Switzerland together contributed to five.[644] Since 1966, more Americans have received the Nobel Prize in Medicine than the rest of the world combined. From 1989 to 2002, four times more money was invested in private biotechnology companies in America than in Europe.[645] The U.S. health-care system far outspends any other nation, measured in both per capita spending and percentage of GDP.[646]		Health-care coverage in the United States is a combination of public and private efforts and is not universal. In 2014, 13.4% of the population did not carry health insurance.[647] The subject of uninsured and underinsured Americans is a major political issue.[648][649] In 2006, Massachusetts became the first state to mandate universal health insurance.[650] Federal legislation passed in early 2010 would ostensibly create a near-universal health insurance system around the country by 2014, though the bill and its ultimate effect are issues of controversy.[651][652]		
Cleveland (/ˈkliːvlənd/ KLEEV-lənd) is a city in the U.S. state of Ohio and the county seat of Cuyahoga County,[7] the state's second most populous county.[8][9] The city proper has a population of 388,072, making Cleveland the 51st largest city in the United States,[5] and the second-largest city in Ohio after Columbus.[10][11] Greater Cleveland ranked as the 32nd largest metropolitan area in the United States, with 2,055,612 people in 2016.[12] The city anchors the Cleveland–Akron–Canton Combined Statistical Area, which had a population of 3,515,646 in 2010 and ranks 15th in the United States.		The city is located on the southern shore of Lake Erie, approximately 60 miles (100 kilometers) west of the Pennsylvania border. It was founded in 1796 near the mouth of the Cuyahoga River, and became a manufacturing center owing to its location on the lake shore, as well as being connected to numerous canals and railroad lines. Cleveland's economy has diversified sectors that include manufacturing, financial services, healthcare, and biomedical. Cleveland is also home to the Rock and Roll Hall of Fame.		Residents of Cleveland are called "Clevelanders". Cleveland has many nicknames, the oldest of which in contemporary use being "The Forest City".[13]						Cleveland obtained its name on July 22, 1796 when surveyors of the Connecticut Land Company laid out Connecticut's Western Reserve into townships and a capital city they named "Cleaveland" after their leader, General Moses Cleaveland. Cleaveland oversaw the plan for what would become the modern downtown area, centered on Public Square, before returning home, never again to visit Ohio. The first settler in Cleaveland was Lorenzo Carter, who built a cabin on the banks of the Cuyahoga River. The Village of Cleaveland was incorporated on December 23, 1814.[11] In spite of the nearby swampy lowlands and harsh winters, its waterfront location proved to be an advantage. The area began rapid growth after the 1832 completion of the Ohio and Erie Canal. This key link between the Ohio River and the Great Lakes connected the city to the Atlantic Ocean via the Erie Canal and later via the St. Lawrence Seaway and the Gulf of Mexico via the Mississippi River. Growth continued with added railroad links.[14] Cleveland incorporated as a city in 1836.[11]		In 1836, the city, then located only on the eastern banks of the Cuyahoga River, nearly erupted into open warfare with neighboring Ohio City over a bridge connecting the two.[15] Ohio City remained an independent municipality until its annexation by Cleveland in 1854.[11]		The city's prime geographic location as a transportation hub on the Great Lakes has played an important role in its development as a commercial center. Cleveland serves as a destination point for iron ore shipped from Minnesota, along with coal transported by rail. In 1870, John D. Rockefeller founded Standard Oil in Cleveland, and moved its headquarters to New York City in 1885.[16] Cleveland emerged in the early 20th Century as an important American manufacturing center, which included automotive companies such as Peerless, People's,[17] Jordan, Chandler, and Winton, maker of the first car driven across the U.S.[18] Other manufacturers located in Cleveland produced steam-powered cars, which included White and Gaeth, as well as the electric car company Baker. Because of the significant growth, Cleveland was known as the "Sixth City" during this period.[19][20]		By 1920, due in large part to the city's economic prosperity, Cleveland became the nation's fifth largest city.[11] The city counted Progressive Era politicians such as the populist Mayor Tom L. Johnson among its leaders. Many prominent Clevelanders from this era are buried in the historic Lake View Cemetery, including President James A. Garfield,[21] and John D. Rockefeller.		In commemoration of the centennial of Cleveland's incorporation as a city, the Great Lakes Exposition debuted in June 1936 along the Lake Erie shore north of downtown. Conceived as a way to energize a city after the Great Depression, it drew four million visitors in its first season, and seven million by the end of its second and final season in September 1937.[22] The exposition was housed on grounds that are now used by the Great Lakes Science Center, the Rock and Roll Hall of Fame and Burke Lakefront Airport, among others.[23] Following World War II, the city experienced a prosperous economy. In sports, the Indians won the 1948 World Series, the hockey Barons became champions of the American Hockey League, and the Browns dominated professional football in the 1950s. As a result, along with track and boxing champions produced, Cleveland was dubbed "City of Champions" in sports at this time. Businesses proclaimed that Cleveland was the "best location in the nation".[24][25][26] In 1940, non-Hispanic whites represented 90.2% of Cleveland's population.[27] The city's population reached its peak of 914,808, and in 1949 Cleveland was named an All-America City for the first time.[28] By the 1960s, the economy slowed, and residents sought new housing in the suburbs, reflecting the national trends of urban flight and suburban growth.[29]		In the 1950s and 1960s, social and racial unrest occurred in Cleveland, resulting in the Hough Riots from July 18 to 23, 1966 and the Glenville Shootout from July 23 to 25, 1968. In November 1967, Cleveland became the first major American city to elect a black mayor, Carl Stokes (who served from 1968 to 1971).		In December 1978, Cleveland became the first major American city since the Great Depression to enter into a financial default on federal loans.[11] By the beginning of the 1980s, several factors, including changes in international free trade policies, inflation and the Savings and Loans Crisis contributed to the recession that impacted cities like Cleveland.[30] While unemployment during the period peaked in 1983,[31] Cleveland's rate of 13.8% was higher than the national average due to the closure of several production centers.[32][33][34]		The metropolitan area began a gradual economic recovery under mayors George Voinovich and Michael R. White. Redevelopment within the city limits has been strongest in the downtown area near the Gateway Sports and Entertainment Complex—consisting of Progressive Field and Quicken Loans Arena—and near North Coast Harbor, including the Rock and Roll Hall of Fame, FirstEnergy Stadium, and the Great Lakes Science Center. Cleveland has been hailed by local media as the "Comeback City",[35] while economic development of the inner-city neighborhoods and improvement of the school systems are municipal priorities.[36] In 1999, Cleveland was identified as an emerging global city.[37]		In the 21st century, the city has improved infrastructure, is more diversified, has gained a national reputation in medical fields, and has invested in the arts. Cleveland is generally considered an example of revitalization. The city's goals include additional neighborhood revitalization and increased funding for public education.[38] In 2009, it was announced that Cleveland was chosen to host the 2014 Gay Games, the fourth city in the United States to host this international event.[39] On July 8, 2014, it was announced that Cleveland was chosen to be the host city of the 2016 Republican National Convention.[40]		According to the United States Census Bureau, the city has a total area of 82.47 square miles (213.60 km2), of which 77.70 square miles (201.24 km2) is land and 4.77 square miles (12.35 km2) is water.[2] The shore of Lake Erie is 569 feet (173 m) above sea level; however, the city lies on a series of irregular bluffs lying roughly parallel to the lake. In Cleveland these bluffs are cut principally by the Cuyahoga River, Big Creek, and Euclid Creek. The land rises quickly from the lakeshore. Public Square, less than one mile (1.6 km) inland, sits at an elevation of 650 feet (198 m), and Hopkins Airport, 5 miles (8 km) inland from the lake, is at an elevation of 791 feet (241 m).[41]		Cleveland's downtown architecture is diverse. Many of the city's government and civic buildings, including City Hall, the Cuyahoga County Courthouse, the Cleveland Public Library, and Public Auditorium, are clustered around an open mall and share a common neoclassical architecture. Built in the early 20th century, they are the result of the 1903 Group Plan, and constitute one of the most complete examples of City Beautiful design in the United States.[42] The Terminal Tower, dedicated in 1930, was the tallest building in North America outside New York City until 1964 and the tallest in the city until 1991.[43] It is a prototypical Beaux-Arts skyscraper. The two newer skyscrapers on Public Square, Key Tower (currently the tallest building in Ohio) and the 200 Public Square, combine elements of Art Deco architecture with postmodern designs. Another of Cleveland's architectural treasures is The Arcade (sometimes called the Old Arcade), a five-story arcade built in 1890 and renovated in 2001 as a Hyatt Regency Hotel.[44] Cleveland's landmark ecclesiastical architecture includes the historic Old Stone Church in downtown Cleveland and the onion domed St. Theodosius Russian Orthodox Cathedral in Tremont, along with myriad ethnically inspired Roman Catholic churches.[45] Running east from Public Square through University Circle is Euclid Avenue, which was known for its prestige and elegance. In the late 1880s, writer Bayard Taylor described it as "the most beautiful street in the world".[46] Known as "Millionaire's Row", Euclid Avenue was world-renowned as the home of such internationally known names as Rockefeller, Hanna, and Hay.[47]		Downtown Cleveland is centered on Public Square and includes a wide range of diversified districts. Downtown Cleveland is home to the traditional Financial District and Civic Center, as well as the distinct Cleveland Theater District, which is home to Playhouse Square Center. Mixed-use neighborhoods such as the Flats and the Warehouse District are occupied by industrial and office buildings as well as restaurants and bars. The number of downtown housing units in the form of condominiums, lofts, and apartments has been on the increase since 2000. Recent developments include the revival of the Flats, the Euclid Corridor Project, and the developments along East 4th Street.[48][49] Cleveland residents geographically define themselves in terms of whether they live on the east or west side of the Cuyahoga River.[50] The east side includes the neighborhoods of Buckeye-Shaker, Central, Collinwood, Corlett, Euclid-Green, Fairfax, Forest Hills, Glenville, Payne/Goodrich-Kirtland Park, Hough, Kinsman, Lee Harvard/Seville-Miles, Mount Pleasant, Nottingham, St. Clair-Superior, Union-Miles Park, University Circle, Little Italy, and Woodland Hills. The west side includes the neighborhoods of Brooklyn Centre, Clark-Fulton, Detroit-Shoreway, Cudell, Edgewater, Ohio City, Tremont, Old Brooklyn, Stockyards, West Boulevard, and the four neighborhoods colloquially known as West Park: Kamm's Corners, Jefferson, Puritas-Longmead, and Riverside. Three neighborhoods in the Cuyahoga Valley are sometimes referred to as the south side: Industrial Valley/Duck Island, Slavic Village (North and South Broadway), and Tremont.		Several inner-city neighborhoods have begun to gentrify in recent years. Areas on both the west side (Ohio City, Tremont, Detroit-Shoreway, and Edgewater) and the east side (Collinwood, Hough, Fairfax, and Little Italy) have been successful in attracting increasing numbers of creative class members, which in turn is spurring new residential development.[51] Furthermore, a live-work zoning overlay for the city's near east side has facilitated the transformation of old industrial buildings into loft spaces for artists.[52]		Cleveland's older, inner-ring suburbs include Bedford, Bedford Heights, Brook Park, Brooklyn, Brooklyn Heights, Cleveland Heights, Cuyahoga Heights, East Cleveland, Euclid, Fairview Park, Garfield Heights, Lakewood, Linndale, Maple Heights, Newburgh Heights, Parma, Parma Heights, Shaker Heights, Solon, South Euclid, University Heights, and Warrensville Heights. Many of the suburbs are members of the Northeast Ohio First Suburbs Consortium.[53]		Typical of the Great Lakes region, Cleveland exhibits a continental climate with four distinct seasons, which lies in the humid continental (Köppen Dfa)[54] zone. Summers are warm to hot and humid while winters are cold and snowy. The Lake Erie shoreline is very close to due east–west from the mouth of the Cuyahoga west to Sandusky, but at the mouth of the Cuyahoga it turns sharply northeast. This feature is the principal contributor to the lake effect snow that is typical in Cleveland (especially on the city's East Side) from mid-November until the surface of Lake Erie freezes, usually in late January or early February. The lake effect also causes a relative differential in geographical snowfall totals across the city: while Hopkins Airport, on the city's far West Side, has only reached 100 inches (254 cm) of snowfall in a season three times since record-keeping for snow began in 1893,[55] seasonal totals approaching or exceeding 100 inches (254 cm) are not uncommon as the city ascends into the Heights on the east, where the region known as the 'Snow Belt' begins. Extending from the city's East Side and its suburbs, the Snow Belt reaches up the Lake Erie shore as far as Buffalo.[56]		The all-time record high in Cleveland of 104 °F (40 °C) was established on June 25, 1988,[57] and the all-time record low of −20 °F (−29 °C) was set on January 19, 1994.[58] On average, July is the warmest month with a mean temperature of 73.5 °F (23.1 °C), and January, with a mean temperature of 28.1 °F (−2.2 °C), is the coldest. Normal yearly precipitation based on the 30-year average from 1981 to 2010 is 39.1 inches (990 mm).[59] The least precipitation occurs on the western side and directly along the lake, and the most occurs in the eastern suburbs. Parts of Geauga County to the east receive over 44 inches (1,100 mm) of liquid precipitation annually.[60]		As of the census[4] of 2010, there were 396,815 people, 167,490 households, and 89,821 families residing in the city. The population density was 5,107.0 inhabitants per square mile (1,971.8/km2). There were 207,536 housing units at an average density of 2,671.0 per square mile (1,031.3/km2). The racial makeup of the city was 53.3% African American, 37.3% White, 0.3% Native American, 1.8% Asian, 4.4% from other races, and 2.8% from two or more races. Hispanic or Latino of any race were 10.0% of the population.[68]		There were 167,490 households of which 29.7% had children under the age of 18 living with them, 22.4% were married couples living together, 25.3% had a female householder with no husband present, 6.0% had a male householder with no wife present, and 46.4% were non-families. 39.5% of all households were made up of individuals and 10.7% had someone living alone who was 65 years of age or older. The average household size was 2.29 and the average family size was 3.11.		The median age in the city was 35.7 years. 24.6% of residents were under the age of 18; 11% were between the ages of 18 and 24; 26.1% were from 25 to 44; 26.3% were from 45 to 64; and 12% were 65 years of age or older. The gender makeup of the city was 48.0% male and 52.0% female.		As of the census of 2000, there were 478,403 people, 190,638 households, and 111,904 families residing in the city. The population density was 6,166.5 inhabitants per square mile (2,380.9/km2). There were 215,856 housing units at an average density of 2,782.4 per square mile (1,074.3/km2). The racial makeup of the city was 51.0% African American, 41.5% White, 0.3% Native American, 1.3% Asian, 0.0% Pacific Islander, 3.6% from other races, and 2.2% from two or more races. Hispanic or Latinos of any race were 7.3% of the population.[69] Ethnic groups include Germans (15.2%), Irish (10.9%), English (8.7%), Italian (5.6%), Poles (3.2%), and French (3.0%). Out of the total population, 4.5% were foreign born; of which 41.2% were born in Europe, 29.1% Asia, 22.4% Latin American, 5.0% Africa, and 1.9% Northern America.[70]		There are also substantial communities of Slovaks, Hungarians, French, Slovenes,[71] Czechs, Ukrainians, Arabs, Dutch, Scottish, Russian, Scotch Irish, Croats, Macedonians, Puerto Ricans, West Indians, Romanians, Lithuanians, and Greeks.[72] The presence of Hungarians within Cleveland proper was, at one time, so great that the city boasted the highest concentration of Hungarians in the world outside of Budapest.[73] The availability of jobs attracted African Americans from the South. Between 1920 and 1960, the black population of Cleveland increased from 35,000 to 251,000.[74]		Out of 190,638 households, 29.9% have children under the age of 18 living with them, 28.5% were married couples living together, 24.8% had a female householder with no husband present, and 41.3% were nonfamilies. 35.2% of all households were made up of individuals and 11.1% had someone living alone who is 65 years of age or older. The average household size was 2.44 and the average family size was 3.19. The age distribution of the population shows 28.5% under the age of 18, 9.5% from 18 to 24, 30.4% from 25 to 44, 19.0% from 45 to 64, and 12.5% who are 65 years of age or older. The median age was 33 years. For every 100 females there were 90.0 males. For every 100 females age 18 and over, there were 85.2 males.		The median income for a household in the city was US$25,928, and the median income for a family was $30,286. Males had a median income of $30,610 versus $24,214 for females. The per capita income for the city was $14,291. 26.3% of the population and 22.9% of families were below the poverty line. Out of the total population, 37.6% of those under the age of 18 and 16.8% of those 65 and older were living below the poverty line.[75]		As of 2010[update], 88.4% (337,658) of Cleveland residents age 5 and older spoke English at home as a primary language, while 7.1% (27,262) spoke Spanish, 0.6% (2,200) Arabic, and 0.5% (1,960) Chinese. In addition 0.9% (3,364) spoke a Slavic language (1,279 – Polish, 679 Serbo-Croatian, and 485 Russian). In total, 11.6% (44,148) of Cleveland's population age 5 and older spoke another language other than English.[76]		Cleveland's location on the Cuyahoga River and Lake Erie has been key to its growth. The Ohio and Erie Canal coupled with rail links helped establish the city as an important business center. Steel and many other manufactured goods emerged as leading industries.[77]		The city diversified its economy in addition to its manufacturing sector. Cleveland is home to the corporate headquarters of many large companies such as Applied Industrial Technologies, Cliffs Natural Resources, Forest City Enterprises, NACCO Industries, Sherwin-Williams Company and KeyCorp. NASA maintains a facility in Cleveland, the Glenn Research Center. Jones Day, one of the largest law firms in the US, began in Cleveland.[78] In 2007, Cleveland's commercial real estate market experienced rebound with a record pace of purchases,[79][80] with a housing vacancy of 10%.[81][82]		The Cleveland Clinic is the city's largest private employer with a workforce of over 37,000 as of 2008[update].[83] It carries the distinction as being among America's best hospitals with top ratings published in U.S. News & World Report.[84] Cleveland's healthcare sector also includes University Hospitals of Cleveland, a renowned center for cancer treatment,[85] MetroHealth medical center, and the insurance company Medical Mutual of Ohio. Cleveland is also noted in the fields of biotechnology and fuel cell research, led by Case Western Reserve University, the Cleveland Clinic, and University Hospitals of Cleveland. Cleveland is among the top recipients of investment for biotech start-ups and research.[86] Case Western Reserve, the Clinic, and University Hospitals have recently announced plans to build a large biotechnology research center and incubator on the site of the former Mt. Sinai Medical Center, creating a research campus to stimulate biotech startup companies that can be spun off from research conducted in the city.[87]		City leaders promoted growth of the technology sector in the first decade of the 21st century. Mayor Jane L. Campbell appointed a "tech czar" to recruit technology companies to the downtown office market, offering connections to the high-speed fiber networks that run underneath downtown streets in several "high-tech offices" focused on the Euclid Avenue area. Cleveland State University hired a technology transfer officer to cultivate technology transfers from CSU research to marketable ideas and companies in the Cleveland area, and appointed a vice president for economic development. Case Western Reserve University participated in technology initiatives such as the OneCommunity project,[88] a high-speed fiber optic network linking the area's research centers intended to stimulate growth. In mid-2005, Cleveland was named an Intel "Worldwide Digital Community" along with Corpus Christi, Texas, Philadelphia, and Taipei. This added about $12 million for marketing to expand regional technology partnerships, created a city-wide Wi-Fi network, and developed a tech economy.		In addition to this Intel initiative, in January 2006 a New York-based think tank, the Intelligent Community Forum, selected Cleveland as the sole American city among its seven finalists for the "Intelligent Community of the Year" award. The group announced it nominated the city for its OneCommunity network with potential broadband applications.[89] OneCommunity collaborated with Cisco Systems to deploy a wireless network starting in September 2006.[90]		Cleveland is home to Playhouse Square Center, the second largest performing arts center in the United States behind New York City's Lincoln Center.[91] Playhouse Square includes the State, Palace, Allen, Hanna, and Ohio theaters within what is known as the Cleveland Theater District.[92] Playhouse Square's resident performing arts companies include Cleveland Play House, Cleveland State University Department of Theatre and Dance, and Great Lakes Theater Festival. The center hosts various Broadway musicals, special concerts, speaking engagements, and other events throughout the year.[92]		One Playhouse Square, now the headquarters for Cleveland's public broadcasters, was originally used as the broadcast studios of WJW (AM), where disc jockey Alan Freed first popularized the term "rock and roll".[93] Cleveland gained a strong reputation in rock music in the 1960s and 70s as a key breakout market for nationally promoted acts and performers. The city hosted the " World Series of Rock" at Cleveland Municipal Stadium, which were notable high-attendance events. Located between Playhouse Square and University Circle is Karamu House, a well-known African American performing and fine arts center, founded in the 1920s.[94]		Cleveland is home to the Cleveland Orchestra, widely considered one of the world's finest orchestras, and often referred to as the finest in the United States.[95] It is one of the "Big Five" major orchestras in the United States. The Orchestra plays at Severance Hall in University Circle during the winter and at Blossom Music Center in Cuyahoga Falls during the summer.[96] The city is also home to the Cleveland Pops Orchestra, the Cleveland Youth Orchestra, and the Cleveland Youth Wind Symphony.		The city also has a history of polka music being popular both past and present, even having a subgenre called Cleveland-style polka named after the city, and is home to the Polka Hall of Fame. This is due in part to the success of Frankie Yankovic who was a Cleveland native and was considered the America's Polka King and the square at the intersection of Waterloo Rd. and East 152nd St. in Cleveland (41°34′08″N 81°34′31″W﻿ / ﻿41.569°N 81.5752°W﻿ / 41.569; -81.5752), not far from where Yankovic grew up, was named in his honor.[97]		There are two main art museums in Cleveland. The Cleveland Museum of Art is a major American art museum,[98] with a collection that includes more than 40,000 works of art ranging over 6,000 years, from ancient masterpieces to contemporary pieces. Museum of Contemporary Art Cleveland showcases established and emerging artists, particularly from the Cleveland area, through hosting and producing temporary exhibitions.[99]		The Gordon Square Arts District on Detroit Ave., in the Detroit-Shoreway neighborhood, features a movie theater called the Capitol Theatre and an Off-Off-Broadway playhouse, the Cleveland Public Theatre.		Cleveland has served as the setting for several major studio and independent films. Players from the 1948 Cleveland Indians, winners of the World Series, appear in The Kid from Cleveland (1949). Cleveland Municipal Stadium features prominently in both that film and The Fortune Cookie (1966); written and directed by Billy Wilder, the picture marked Walter Matthau and Jack Lemmon's first on-screen collaboration and features gameday footage of the 1965 Cleveland Browns. Director Jules Dassin's first American film in nearly twenty years, Up Tight! (1968) is set in Cleveland immediately following the assassination of Martin Luther King, Jr. Set in 1930s Cleveland, Sylvester Stallone leads a local labor union in F.I.S.T. (1978). Paul Simon chose Cleveland as the opening for his only venture into filmmaking, One-Trick Pony (1980); Simon spent six weeks filming concert scenes at the Cleveland Agora. The boxing-match-turned-riot near the start of Raging Bull (1980) takes place at the Cleveland Arena in 1941. Clevelander Jim Jarmusch's critically acclaimed and independently produced Stranger Than Paradise (1984)—a deadpan comedy about two New Yorkers who travel to Florida by way of Cleveland—was a favorite of the Cannes Film Festival, winning the Caméra d'Or. The cult-classic mockumentary This Is Spinal Tap (1984) includes a memorable scene where the parody band gets lost backstage just before performing at a Cleveland rock concert (origin of the phrase "Hello, Cleveland!"). Howard the Duck (1986), George Lucas' heavily criticized adaptation of the Marvel comic of the same name, begins with the title character crashing into Cleveland after drifting in outer space. Michael J. Fox and Joan Jett play the sibling leads of a Cleveland rock group in Light of Day (1987); directed by Paul Schrader, much of the film was shot in the city. Both Major League (1989) and Major League II (1994) reflected the actual perennial struggles of the Cleveland Indians during the 1960s, 1970s, and 1980s. Kevin Bacon stars in Telling Lies in America (1997), the semi-autobiographical tale of Clevelander Joe Eszterhas, a former reporter for The Plain Dealer. Cleveland serves as the setting for fictitious insurance giant Great Benefit in The Rainmaker (1997); in the film, Key Tower doubles as the firm's main headquarters. A group of Cleveland teenagers try to scam their way into a Kiss concert in Detroit Rock City (1999), and several key scenes from director Cameron Crowe's Almost Famous (2000) are set in Cleveland. Antwone Fisher (2002) recounts the real-life story of the Cleveland native. Brothers Joe and Anthony Russo—native Clevelanders and Case Western Reserve University alumni—filmed their comedy Welcome to Collinwood (2002) entirely on location in the city. American Splendor (2003)—the biographical film of Harvey Pekar, author of the autobiographical comic of the same name—was also filmed on location throughout Cleveland, as was The Oh in Ohio (2006). Much of The Rocker (2008) is set in the city, and Cleveland native Nathaniel Ayers' life story is told in The Soloist (2009). Kill the Irishman (2011) follows the real-life turf war in 1970s Cleveland between Irish mobster Danny Greene and the Cleveland crime family. More recently, the teenage comedy Fun Size (2012) takes place in and around Cleveland on Halloween night, and the film Draft Day (2014) followed Kevin Costner as general manager for the Cleveland Browns.[100][101][102][103][104]		Cleveland has often doubled for other locations in film. The wedding and reception scenes in The Deer Hunter (1978), while set in the small Pittsburgh suburb of Clairton, were actually shot in the Cleveland neighborhood of Tremont; U.S. Steel also permitted the production to film in one of its Cleveland mills. Francis Ford Coppola produced The Escape Artist (1982), much of which was shot in Downtown Cleveland near City Hall and the Cuyahoga County Courthouse, as well as the Flats. A Christmas Story (1983) was set in Indiana, but drew many of its external shots—including the Parker family home, the downtown Christmas parade and Higbee's department store Santa scenes —from Cleveland. Much of Double Dragon (1994) and Happy Gilmore (1996) were also shot in Cleveland, and the opening shots of Air Force One (1997) were filmed in and above Severance Hall. A complex chase scene in Spider-Man 3 (2007), though set in New York City, was actually filmed along Cleveland's Euclid Avenue. Downtown's East 9th Street also doubled for New York in the climax of The Avengers (2012); in addition, the production shot on Cleveland's Public Square as a fill-in for Stuttgart, Germany. More recently, Jackass Presents: Bad Grandpa (2013), Miss Meadows (2014) and Captain America: The Winter Soldier (2014) each filmed in Cleveland. Future productions in the Cleveland area are the responsibility of the Greater Cleveland Film Commission.[100][101][102][105]		In television, the city is the setting for the popular network sitcom The Drew Carey Show, starring Cleveland native Drew Carey. Real-life crime series Cops, Crime 360, and The First 48 regularly film in Cleveland and other U.S. cities. Hot in Cleveland, a comedy airing on TV Land, premiered on June 16, 2010.[106][107][108]		The American modernist poet Hart Crane was born in nearby Garrettsville, Ohio in 1899. His adolescence was divided between Cleveland and Akron before he moved to New York City in 1916. Aside from factory work during the first world war, he served as reporter to The Plain Dealer for a short period, before achieving recognition in the Modernist literary scene. A diminutive memorial park is dedicated to Crane along the left bank of the Cuyahoga in Cleveland. In University Circle, a historical marker sits at the location of his Cleveland childhood house on E. 115 near the Euclid Ave intersection. On Case Western Reserve University campus, a statue of him stands behind the Kelvin Smith Library.		Langston Hughes, preeminent poet of the Harlem Renaissance and child of an itinerant couple, lived in Cleveland as a teenager and attended Central High School in Cleveland in the 1910s. He wrote for the school newspaper and started writing his earlier plays, poems and short stories while living in Cleveland.[109] The African-American avant garde poet Russell Atkins also lived in Cleveland.[110]		Cleveland was the home of Joe Shuster and Jerry Siegel, who created the comic book character Superman in 1932.[111] Both attended Glenville High School, and their early collaborations resulted in the creation of "The Man of Steel".[112] D. A. Levy wrote: "Cleveland: The Rectal Eye Visions". Mystery author Richard Montanari's first three novels, Deviant Way, The Violet Hour, and Kiss of Evil are set in Cleveland. Mystery writer, Les Roberts's Milan Jacovich series is also set in Cleveland. Author and Ohio resident, James Renner set his debut novel, The Man from Primrose Lane in present-day Cleveland.		Harlan Ellison, noted author of speculative fiction, was born in Cleveland in 1934; his family subsequently moved to the nearby suburb of Painesville, though Ellison moved back to Cleveland in 1949. As a youngster, he published a series of short stories appearing in the Cleveland News; he also performed in a number of productions for the Cleveland Play House.		The Cleveland State University Poetry Center serves as an academic center for poetry. Cleveland continues to have a thriving literary and poetry community,[113][114] with regular poetry readings at bookstores, coffee shops, and various other venues.[115]		Cleveland is the site of the Anisfield-Wolf Book Award, established by poet and philanthropist Edith Anisfield Wolf in 1935, which recognizes books that have made important contributions to understanding of racism and human diversity.[116] Presented by the Cleveland Foundation, it remains the only American book prize focusing on works that address racism and diversity.[117] In an early Gay and Lesbian Studies anthology titled Lavender Culture,[118] a short piece by John Kelsey "The Cleveland Bar Scene in the Forties" discusses the gay and lesbian culture in Cleveland and the unique experiences of amateur female impersonators that existed alongside the New York and San Francisco LGBT subcultures.[119]		Cleveland's melting pot of immigrant groups and their various culinary traditions have long played an important role in defining the local cuisine. Examples of these can particularly be found in neighborhoods such as Little Italy, Slavic Village, and Tremont.		Local mainstays of Cleveland's cuisine include an abundance of Polish and Central European contributions, such as kielbasa, stuffed cabbage and pierogies.[120] Cleveland also has plenty of corned beef, with nationally renowned Slyman's, on the near East Side, a perennial winner of various accolades from Esquire Magazine, including being named the best corned beef sandwich in America in 2008.[121] Other famed sandwiches include the Cleveland original, Polish Boy, a local favorite found at many BBQ and Soul food restaurants.[120][122] With its blue-collar roots well intact, and plenty of Lake Erie perch available, the tradition of Friday night fish fries remains alive and thriving in Cleveland, particularly in church-based settings and during the season of Lent.[123] Ohio City is home to a growing brewery district, which includes Great Lakes Brewing Company (Ohio's oldest microbrewery); Market Garden Brewery next to the historic West Side Market and Platform Beer Company.[124]		Cleveland is noted in the world of celebrity food culture. Famous local figures include chef Michael Symon and food writer Michael Ruhlman, both of whom achieved local and national attentions for their contributions in the culinary world. On November 11, 2007, Symon helped gain the spotlight when he was named "The Next Iron Chef" on the Food Network. In 2007, Ruhlman collaborated with Anthony Bourdain, to do an episode of his Anthony Bourdain: No Reservations focusing on Cleveland's restaurant scene.[125]		The national food press—including publications Gourmet, Food & Wine, Esquire and Playboy—has heaped praise on several Cleveland spots for awards including 'best new restaurant', 'best steakhouse', 'best farm-to-table programs' and 'great new neighborhood eateries'. In early 2008, the Chicago Tribune ran a feature article in its 'Travel' section proclaiming Cleveland, America's "hot new dining city".[125]		Five miles (8.0 km) east of downtown Cleveland is University Circle, a 550-acre (2.2 km2) concentration of cultural, educational, and medical institutions, including the Cleveland Botanical Garden, Case Western Reserve University, University Hospitals, Severance Hall, the Cleveland Museum of Art, the Cleveland Museum of Natural History, and the Western Reserve Historical Society. A 2011 study by Walk Score ranked Cleveland 17th most walkable of fifty largest U.S. cities.[126] Cleveland is home to the I. M. Pei-designed Rock and Roll Hall of Fame on the Lake Erie waterfront at North Coast Harbor downtown. Neighboring attractions include Cleveland Browns Stadium, the Great Lakes Science Center, the Steamship Mather Museum, and the USS Cod, a World War II submarine.[127] Cleveland has an attraction for visitors and fans of A Christmas Story: A Christmas Story House and Museum to see props, costumes, rooms, photos and other materials related to the Jean Shepherd film. Cleveland is home to many festivals throughout the year. Cultural festivals such as the annual Feast of the Assumption in the Little Italy neighborhood, the Harvest Festival in the Slavic Village neighborhood, and the more recent Cleveland Asian Festival in the Asia Town neighborhood are popular events. Vendors at the West Side Market in Ohio City offer many different ethnic foods for sale. Cleveland hosts an annual parade on Saint Patrick's Day that brings hundreds of thousands to the streets of downtown.[128]		Fashion Week Cleveland, the city's annual fashion event, is the third-largest fashion show of its kind in the United States.[129] In addition to the cultural festivals, Cleveland hosted the CMJ Rock Hall Music Fest, which featured national and local acts, including both established artists and up-and-coming acts, but the festival was discontinued in 2007 due to financial and manpower costs to the Rock Hall.[130] The annual Ingenuity Fest, Notacon and TEDxCLE conference focus on the combination of art and technology.[131][132] The Cleveland International Film Festival has been held annually since 1977, and it drew a record 66,476 people in March 2009.[133] Cleveland also hosts an annual holiday display lighting and celebration, dubbed Winterfest, which is held downtown at the city's historic hub, Public Square.[134]		Cleveland also has the Jack Cleveland Casino. Phase I opened on May 14, 2012, on Public Square, in the historic former Higbee's Building at Tower City Center. Phase II will open along the bend of the Cuyahoga River behind Tower City Center.		The new Greater Cleveland Aquarium is on the west bank of the Cuyahoga River near Downtown.[135]		Cleveland's current major professional sports teams include the Cleveland Indians (Major League Baseball), Cleveland Browns (National Football League), and Cleveland Cavaliers (National Basketball Association). Local sporting facilities include Progressive Field, FirstEnergy Stadium, Quicken Loans Arena and the Wolstein Center.		The Cleveland Indians won the World Series in 1920 and 1948. They also won the American League pennant, making the World Series in the 1954, 1995, 1997, and 2016 seasons. Between 1995 and 2001, Progressive Field (then known as Jacobs Field) sold out 455 consecutive games, a Major League Baseball record until it was broken in 2008.[136]		The Cavaliers won the Eastern Conference in 2007, 2015, 2016 and 2017 but were defeated in the NBA Finals by the San Antonio Spurs and then by the Golden State Warriors, respectively. The Cavs won the Conference again in 2016 and won their first NBA Championship, finally defeating the Golden State Warriors. Afterwards, an estimated 1.3 million people attended a parade held in the Cavs honor on June 22, 2016.		Historically, the Browns have been among the winningest franchises in American football history winning eight titles during a short period of time—1946, 1947, 1948, 1949, 1950, 1954, 1955, and 1964. The Browns have never played in a Super Bowl, getting close five times by making it to the NFL/AFC Championship Game in 1968, 1969, 1986, 1987, and 1989. Former owner Art Modell's relocation of the Browns after the 1995 season (to Baltimore creating the Ravens), caused tremendous heartbreak and resentment among local fans.[137] Cleveland mayor, Michael R. White, worked with the NFL and Commissioner Paul Tagliabue to bring back the Browns beginning in 1999 season, retaining all team history.[138] The city has had previous champions as well, and has a rich history in professional sports. In professional basketball, the Cleveland Rosenblums dominated the American Basketball League in the 1920s, and the Pipers were a pro champion in 1962. The Cleveland Rams won the NFL title in 1945 before relocating to Los Angeles and conceding the city to the Browns. A notable Cleveland athlete is Jesse Owens, who grew up in the city after moving from Alabama when he was nine. He participated in the 1936 Summer Olympics in Berlin, where he achieved international fame by winning four gold medals. A statue commemorating his achievement can be found in Downtown Cleveland at Fort Washington Park.[139]		Cleveland State University alum and area native, Stipe Miocic, won the UFC World Heavyweight Championship at UFC 198 in 2016. With the first ever UFC World Championship fight in the city of Cleveland held September 2016, Miocic defended his title to remain World Heavyweight Champion at UFC 203.[140]		The AHL Cleveland Monsters won the 2016 Calder Cup, becoming the first Cleveland pro sports team to do so since the 1964 Cleveland Barons.[141]		The city is also host to the Cleveland Gladiators of the Arena Football League, Cleveland Fusion of the Women's Football Alliance and AFC Cleveland Royals of the National Premier Soccer League, who won the championship in 2016.		Collegiately, NCAA Division I Cleveland State Vikings have 16 varsity sports, nationally known for their Cleveland State Vikings men's basketball team. NCAA Division III Case Western Reserve Spartans have 19 varsity sports, most known for their Case Western Reserve Spartans football team. The headquarters of the Mid-American Conference (MAC) are located in Cleveland. The conference also stages both its men's and women's basketball tournaments at Quicken Loans Arena.		Several chess championships have taken place in Cleveland. The second American Chess Congress, a predecessor the current U.S. Championship, was held in 1871, and won by George Henry Mackenzie. The 1921 and 1957 U.S. Open Chess Championship also took place in the city, and were won by Edward Lasker and Bobby Fischer, respectively. The Cleveland Open is currently held annually.		The Cleveland Marathon has been hosted annually since 1978.		Cleveland is home to four of the parks in the countywide Cleveland Metroparks system, as well as the: Washington Park, Brookside Park and parts of the Rocky River and Washington Reservations. Known locally as the "Emerald Necklace", the Olmsted-inspired Metroparks encircle Cuyahoga county. Included in the system is the Cleveland Metroparks Zoo. Located in Big Creek valley, the zoo contains one of the largest collection of primates in North America.[142] In addition to the Metroparks system, the Cleveland Lakefront State Park district provides public access to Lake Erie.[143] This cooperative between the City of Cleveland and the State of Ohio contains six parks: Edgewater Park, located on the city's near west side between the Shoreway and the lake; East 55th Street Marina, Euclid Beach Park and Gordon Park. The Cleveland Public Parks District is the municipal body that oversees the city's neighborhood parks, the largest of which is the historic Rockefeller Park, notable for its late-19th century historical landmark bridges and Cultural Gardens.[144]		Cleveland's position as a center of manufacturing established it as a hotbed of union activity early in its history. While other parts of Ohio, particularly Cincinnati and the southern portion of the state, have historically supported the Republican Party, Cleveland commonly breeds the strongest support in the state for the Democrats.[145] At the local level, elections are nonpartisan. However, Democrats still dominate every level of government. Cleveland is split between two congressional districts. Most of the western part of the city is in the 9th District, represented by Marcy Kaptur. Most of the eastern part of the city, as well as most of downtown, is in the 11th District, represented by Marcia Fudge. Both are Democrats.		During the 2004 Presidential election, although George W. Bush carried Ohio by 2.1%, John Kerry carried Cuyahoga County 66.6%–32.9%,[146] his largest margin in any Ohio county. The city of Cleveland supported Kerry over Bush by the even larger margin of 83.3%–15.8%.[147]		The city of Cleveland operates on the mayor–council (strong mayor) form of government.[148] The mayor is the chief executive of the city, and the office is held in 2010 by Frank G. Jackson. Previous mayors of Cleveland include progressive Democrat Tom L. Johnson, World War I era War Secretary and founder of BakerHostetler law firm Newton D. Baker, United States Supreme Court Justice Harold Hitz Burton, Republican Senator George V. Voinovich, two-term Ohio Governor and Senator, former United States Representative Dennis Kucinich of Ohio's 10th congressional district, Frank J. Lausche, and Carl B. Stokes, the first African American mayor of a major American city.[149] The state of Ohio lost two Congressional seats as a result of the 2010 Census, which affects Cleveland's districts in the northeast part of the state.[150]		Between about 1935 to 1938, the Cleveland Torso Murderer killed and dismembered at least a dozen and perhaps twenty people in the area. No arrest was ever made.		From 2002 to 2014, Ariel Castro held three women as sex slaves in his home in Cleveland. Police became aware of the crime when one of the women escaped. Castro was sentenced to one thousand years in jail, but committed suicide.		Based on the Morgan Quitno Press 2008 national crime rankings, Cleveland ranked as the 7th most dangerous city in the nation among US cities with a population of 100,000 to 500,000 and the 11th most dangerous overall.[151] Violent crime from 2005 to 2006 was mostly unchanged nationwide, but increased more than 10% in Cleveland. The murder rate dropped 30% in Cleveland, but was still far above the national average. Property crime from 2005 to 2006 was virtually unchanged across the country and in Cleveland, with larceny-theft down by 7% but burglaries up almost 14%.[152]		In September 2009, the local police arrested Anthony Sowell, who was known in press reports as the Cleveland Strangler. He was convicted of eleven murders as well as other crimes and sentenced to death.		In October 2010, Cleveland had two neighborhoods appear on ABC News's list of 'America's 25 Most Dangerous Neighborhoods': both in sections just blocks apart in the city's Central neighborhood on the East Side. Ranked 21st was in the vicinity of Quincy Avenue and E. 40th Streets, while an area near E. 55th and Scovill Avenue ranked 2nd in the nation, just behind a section of the Englewood neighborhood in Chicago, which ranked 1st.[153][154]		A study in 1971–72 found that although Cleveland's crime rate was significantly lower than other large urban areas, most Cleveland residents feared crime.[155] In the 1980s, gang activity was on the rise, associated with crack cocaine. A task force was formed and was partially successful at reducing gang activity by a combination of removing gang-related graffiti and educating news sources to not name gangs in news reporting.[156]		The distribution of crime in Cleveland is highly heterogeneous. Relatively few crimes take place in downtown Cleveland's business district, but the perception of crime in the downtown has been pointed to by the Greater Cleveland Growth Association[157] as damaging to the city's economy.[158] More affluent areas of Cleveland and its suburbs have lower rates of violent crime than areas of lower socioeconomic status. Statistically speaking, higher incidences of violent crimes have been noted in some parts of Cleveland with higher populations of African Americans.[159] A study of the relationship between employment access and crime in Cleveland found a strong inverse relationship, with the highest crime rates in areas of the city that had the lowest access to jobs. Furthermore, this relationship was found to be strongest with respect to economic crimes.[160] A study of public housing in Cleveland found that criminals tend to live in areas of higher affluence and move into areas of lower affluence to commit crimes.[161]		In 2012, Cleveland's crime rate were 84 murders, 3,252 robberies, and 9,740 burglaries.[162] In 2014, the United States Department of Justice (DOJ) published a report that investigated the use of force by the Cleveland Police Department from 2010–2013. The Justice Department found a pattern of excessive force including the use of firearms, tasers, fists, and chemical spray that unnecessarily escalated nonviolent situations, including against the mentally ill and people who were already restrained. As a result of the Justice Department report, the city of Cleveland has agreed to a consent decree to revise its policies and implement new independent oversight over the police force.[163]		On May 26, 2015, the City of Cleveland and the DOJ released a 105-page agreement addressing concerns about Cleveland Division of Police (CDP) use-of-force policies and practices.		The agreement follows a two-year Department of Justice investigation, prompted by a request from Cleveland Mayor Frank Jackson,[164] to determine whether the CDP engaged in a pattern or practice of the use of excessive force in violation of the Fourth Amendment of the United States Constitution and the Violent Crime Control and Law Enforcement Act (1994), 42 U.S.C § 14141 (Section 14141"). Under Section 14141, the Department of Justice is granted authority to seek declaratory or equitable relief to remedy a pattern or practice of conduct by law enforcement officers that deprives individuals of rights, privileges, or immunities secured by the Constitution or federal law.		U.S. Attorney General Eric Holder and U.S. Attorney Steven Dettelbach announced the findings of the DOJ investigation in Cleveland on December 4, 2014.[165] After reviewing nearly 600 use-of-force incidents from 2010 to 2013 and conducting thousands of interviews, the investigators found systemic patterns insufficient accountability mechanisms, inadequate training, ineffective policies, and inadequate community engagement.[165][166]		At the same time as the announcement of the investigation findings, the City of Cleveland and the Department of Justice issued a Joint Statement of Principles agreeing to begin negotiations with the intention of reaching a court-enforceable settlement agreement.		The details of the settlement agreement, or consent decree, were released on May 26, 2015. The agreement mandates sweeping changes in training for recruits and seasoned officers, developing programs to identify and support troubled officers, updating technology and data management practices, and an independent monitor to ensure that the goals of the decree are met. The agreement is not an admission or evidence of liability, nor is it an admission by the city, CDP, or its officers and employees that they have engaged in unconstitutional, illegal, or otherwise improper activities or conduct. Pending approval from a federal judge,[167] the consent decree will be implemented and the agreement is binding.		The Cleveland Consent Decree is divided into 15 divisions, with 462 enumerated items.[168] At least some of the provisions have been identified as unique to Cleveland:		On June 12, 2015, Chief U.S. District Judge Solomon Oliver Jr. approved and signed the consent decree.[171] The signing of the agreement starts the clock for numerous deadlines that must be met in an effort to improve the department's handling of use-of-force incidents.[172]		Cleveland is served by the firefighters of the Cleveland Division of Fire.[173] The fire department operates out of 22 active fire stations, located throughout the city in five battalions. Each Battalion is commanded by a Battalion Chief, who reports to an on-duty Assistant Chief.[173][174]		The Division of Fire operates a fire apparatus fleet of twenty-two engine companies, eight ladder companies, three tower companies, two task force rescue squad companies, hazardous materials ("haz-mat") unit, and numerous other special, support, and reserve units. The current Chief of Department is Patrick Kelly.[175]		Cleveland EMS is operated by the city as its own department; however, a merger between the fire and EMS departments is in progress. Cleveland EMS units are now based out of most of the city's fire stations as of 2013[update]. City officials are currently negotiating with Cleveland Fire and EMS to form a new union contract that will merge the two systems entirely. No set projection for a full merger has been established. Neither the Fire nor EMS unions have been able to come to an agreement with city officials on fair terms of merger as of yet.[176]		The Cleveland Metropolitan School District is the largest K–12 district in the state of Ohio, with 127 schools and an enrollment of 55,567 students during the 2006–2007 academic year.[177] It is the only district in Ohio that is under direct control of the mayor, who appoints a school board.[178]		Approximately 1 square mile (2.6 km2) of Cleveland, adjacent the Shaker Square neighborhood, is part of the Shaker Heights City School District. The area, which has been a part of the Shaker school district since the 1920s, permits these Cleveland residents to pay the same school taxes as the Shaker residents, as well as vote in the Shaker school board elections.[179]		Cleveland is home to a number of colleges and universities. Most prominent among these is Case Western Reserve University, a world-renowned research and teaching institution located in University Circle. A private university with several prominent graduate programs, CWRU was ranked 37th in the nation in 2012 by U.S. News & World Report.[181] University Circle also contains Cleveland Institute of Art and the Cleveland Institute of Music. Cleveland State University (CSU), based in Downtown Cleveland, is the city's public four-year university. In addition to CSU, downtown hosts the metropolitan campus of Cuyahoga Community College, the county's two-year higher education institution. Ohio Technical College is also based in Cleveland.[182]		Cleveland's primary daily newspaper is The Plain Dealer. Defunct major newspapers include the Cleveland Press, an afternoon publication which printed its last edition on June 17, 1982; and the Cleveland News, which ceased publication in 1960. Additional newspaper coverage includes: the News-Herald which serves the smaller suburbs in the east side, the Thursdays-only Sun Post-Herald, which serves a few neighborhoods on the city's west side; and the Call and Post, a weekly newspaper that primarily serves the city's African-American community. The city is also served by Cleveland Magazine, a regional culture magazine published monthly; Crain's Cleveland Business, a weekly business newspaper; Cleveland Jewish News, a weekly Jewish newspaper; and Cleveland Scene, a free alternative weekly paper which absorbed its competitor, the Cleveland Free Times, in 2008. In addition, nationally distributed rock magazine Alternative Press was founded in Cleveland in 1985, and the publication's headquarters remain in the city.[183][184][185]		Combined with nearby Akron and Canton, Cleveland is ranked as the 19th-largest television market by Nielsen Media Research (as of 2013[update]–14).[186] The market is served by 10 stations affiliated with major American networks, including: WEWS-TV (ABC), WJW (Fox), WKYC (NBC), WOIO (CBS), WVIZ (PBS), WBNX-TV (The CW), WUAB (MyNetworkTV), WVPX-TV (Ion), WQHS-DT (Univision), and WDLI-TV (TBN). The Mike Douglas Show, a nationally syndicated daytime talk show, began in Cleveland in 1961 on KYW-TV (now WKYC), while The Morning Exchange on WEWS-TV served as the model for Good Morning America. Tim Conway and Ernie Anderson first established themselves in Cleveland while working together at KYW-TV and later WJW-TV (now WJW). Anderson both created and performed as the immensely popular Cleveland horror host Ghoulardi on WJW-TV's Shock Theater, and was later succeeded by the long-running late night duo Big Chuck and Lil' John.[187][188][189][190]		Cleveland is directly served by 31 AM and FM radio stations, 22 of which are licensed to the city. Commercial FM music stations are frequently the highest rated stations in the market: WAKS (contemporary hit radio), WDOK (adult contemporary), WENZ (mainstream urban), WHLK (adult hits), WGAR-FM (country), WMJI (classic hits), WMMS (active rock/hot talk; Indians and Cavaliers FM flagship), WNCX (classic rock; Browns co-flagship), WQAL (hot adult contemporary), and WZAK (urban adult contemporary). WCPN public radio functions as the local NPR affiliate, and sister station WCLV airs a classical music format. College radio stations include WBWC (Baldwin Wallace University), WCSB (Cleveland State University), WJCU (John Carroll University), and WRUW-FM (Case Western Reserve University).		News/talk station WTAM serves as the AM flagship for both the Cleveland Cavaliers and Cleveland Indians. WKNR and WWGK cover sports via ESPN Radio, while WKRK-FM covers sports via CBS Sports Radio (WKNR and WKRK-FM are also co-flagship stations for the Cleveland Browns). As WJW (AM), WKNR was once the home of Alan Freed − the Cleveland disc jockey credited with first using and popularizing the term "rock and roll" to describe the music genre. News/talk station WHK was one of the first radio stations to broadcast in the United States and the first in Ohio; its former sister station, rock station WMMS, dominated Cleveland radio in the 1970s and 1980s and was at that time one of the highest rated radio stations in the country. In 1972, WMMS program director Billy Bass coined the phrase "The Rock and Roll Capital of the World" to describe Cleveland. In 1987, Playboy named WMMS DJ Kid Leo (Lawrence Travagliante) "The Best Disc Jockey in the Country".[191][192][193][194][195][196]		Cleveland is home to several major hospital systems, two of which are in University Circle. Most notable is the world renowned Cleveland Clinic, which is supplemented by University Hospitals and its Rainbow Babies & Children's Hospital. Additionally MetroHealth System, which operates the level one trauma center for northeast Ohio, has various locations throughout greater Cleveland. Cleveland's Global Center for Health Innovation opened with 235,000 square feet (21,800 m2) of display space for healthcare companies across the world.		Cleveland Hopkins International Airport is the city's major airport and an international airport that formerly served as a main hub for United Airlines. It holds the distinction of having the first airport-to-downtown rapid transit connection in North America, established in 1968. In 1930, the airport was the site of the first airfield lighting system and the first air traffic control tower. Originally known as Cleveland Municipal Airport, it was the first municipally owned airport in the country. Cleveland Hopkins is a significant regional air freight hub hosting FedEx Express, UPS Airlines, United States Postal Service, and major commercial freight carriers. In addition to Hopkins, Cleveland is served by Burke Lakefront Airport, on the north shore of downtown between Lake Erie and the Shoreway. Burke is primarily a commuter and business airport.[197]		The Port of Cleveland, located at the Cuyahoga River's mouth, is a major bulk freight terminal on Lake Erie, receiving much of the raw materials used by the region's manufacturing industries.[198]		Amtrak, the national passenger rail system, provides service to Cleveland, via the Capitol Limited and Lake Shore Limited routes, which stop at Cleveland Lakefront Station. Cleveland has also been identified as a hub for the proposed Ohio Hub project, which would bring high-speed rail to Ohio.[199] Cleveland hosts several inter-modal freight railroad terminals.[200][201] There have been several proposals for commuter rail in Cleveland, including an ongoing (as of January 2011[202]) study into a Sandusky–Cleveland line.[203]		Cleveland has a bus and rail mass transit system operated by the Greater Cleveland Regional Transit Authority (RTA). The rail portion is officially called the RTA Rapid Transit, but local residents refer to it as The Rapid. It consists of two light rail lines, known as the Green and Blue Lines, and a heavy rail line, the Red Line. In 2008, RTA completed the HealthLine, a bus rapid transit line, for which naming rights were purchased by the Cleveland Clinic and University Hospitals. It runs along Euclid Avenue from downtown through University Circle, ending at the Louis Stokes Station at Windermere in East Cleveland.[204] In 2007, the American Public Transportation Association named Cleveland's mass transit system the best in North America.[205] Cleveland is the only metropolitan area in the Western Hemisphere with its rail rapid transit system having only one center-city area rapid transit station (Tower City-Public Square). During construction of the Red Line rapid transit line in the 1950's the citizens of Cleveland voted to build the Downtown Distributor Subway which would have provided a number of Center City stations. The plan was quashed by highway promoting County Engineer Albert S. Porter and the full development and growth of center city Cleveland has since been significantly impeded due to the resulting inaccessibility.		National intercity bus service is provided at a Greyhound station, located just behind the Playhouse Square theater district. Megabus provides service to Cleveland and has a stop at the Stephanie Tubbs Jones Transit Center on the east side of downtown.[206] Akron Metro, Brunswick Transit Alternative, Laketran, Lorain County Transit, and Medina County Transit provide connecting bus service to the Greater Cleveland Regional Transit Authority. Geauga County Transit and Portage Area Regional Transportation Authority (PARTA) also offer connecting bus service in their neighboring areas.[207]		Cleveland's road system consists of numbered streets running roughly north–south, and named avenues, which run roughly east–west. The numbered streets are designated "east" or "west", depending where they lie in relation to Ontario Street, which bisects Public Square.[208] The numbered street system extends beyond the city limits into some suburbs on both the west and east sides. The named avenues that lie both on the east side of the Cuyahoga River and west of Ontario Street receive a "west" designation on street signage. The two downtown avenues which span the Cuyahoga change names on the west side of the river. Superior Avenue becomes Detroit Avenue on the west side, and Carnegie Avenue becomes Lorain Avenue. The bridges that make these connections are often called the Detroit–Superior Bridge and the Lorain–Carnegie Bridge.		Three two-digit Interstate highways serve Cleveland directly. Interstate 71 begins just southwest of downtown and is the major route from downtown Cleveland to the airport. I-71 runs through the southwestern suburbs and eventually connects Cleveland with Columbus and Cincinnati. Interstate 77 begins in downtown Cleveland and runs almost due south through the southern suburbs. I-77 sees the least traffic of the three interstates, although it does connect Cleveland to Akron. Interstate 90 connects the two sides of Cleveland, and is the northern terminus for both I-71 and I-77. Running due east–west through the west side suburbs, I-90 turns northeast at the junction with and I-490, and is known as the Innerbelt through downtown. At the junction with the Shoreway, I-90 makes a 90-degree turn known in the area as Dead Man's Curve, then continues northeast, entering Lake County near the eastern split with Ohio State Route 2. Cleveland is also served by two three-digit interstates, Interstate 480, which enters Cleveland briefly at a few points and Interstate 490, which connects I-77 with the junction of I-90 and I-71 just south of downtown.[209]		Two other limited-access highways serve Cleveland. The Cleveland Memorial Shoreway carries State Route 2 along its length, and at varying points also carries US 6, US 20 and I-90. The Jennings Freeway (State Route 176) connects I-71 just south of I-90 to I-480 near the suburbs of Parma and Brooklyn Heights. A third highway, the Berea Freeway (State Route 237 in part), connects I-71 to the airport, and forms part of the boundary between Cleveland and Brook Park.[210]		In 2011, Walk Score ranked Cleveland the seventeenth most walkable of the fifty largest cities in the United States.[211] As of 2014[update], Walk Score increased Cleveland's rank to being the sixteenth most walkable US city, with a Walk Score of 57, a Transit Score of 47, and a Bike Score of 51. Cleveland's most walkable and transient areas can be found in the Downtown, Ohio City, Detroit-Shoreway, University Circle, and Buckeye-Shaker Square neighborhoods.[212]		Cleveland is home to the Consulate General of the Republic of Slovenia.[213]		As of 2015[update], Cleveland has twenty-two sister cities:[214][215]		In addition, Northeast Ohio's Jewish community has an unofficial supportive relationship with the State of Israel.[222]		Notes		General references		
Superbad is a 2007 American teen comedy film directed by Greg Mottola and produced by Judd Apatow. The film stars Jonah Hill and Michael Cera as Seth and Evan, two teenagers about to graduate high-school. Before graduating, the boys want to go to a party and lose their virginities. However, their plan proves harder than expected. Written by Seth Rogen and Evan Goldberg, the script began development when Rogen and Goldberg were 13 years old, and was loosely based on their experience in Grade 12 in Vancouver during the 1990s. The main characters have the same given names as the two writers. In fact, Rogen initially intended to play Seth, but due to age and physical size this was changed, and Hill went on to portray Seth, while Rogen portrayed the irresponsible Officer Michaels, opposite Saturday Night Live star Bill Hader as Officer Slater.		The film received favorable reviews, with critics praising the dialogue and the chemistry between the two leads. The film also proved financially successful, grossing a total of US$169 million from only a US$20 million budget.						Seth (Jonah Hill) and Evan (Michael Cera) are two high school seniors who lament their virginity and poor social standing. Best friends since childhood, the two are about to go off to different colleges, as Seth did not get accepted into Dartmouth like Evan. After Seth is paired with Jules (Emma Stone) during Home-Ec class, she invites him to a party at her house later that night. Later, their friend Fogell (Christopher Mintz-Plasse) comes up to the two and reveals his plans to obtain a fake ID during lunch. Seth uses this to his advantage and promises to bring alcohol to Jules' party. Meanwhile, Evan runs into his crush Becca (Martha MacIsaac) and he offers to get her some vodka for the party. Fogell's fake ID is met with derision by Seth and Evan, as it states that Fogell's name is simply "McLovin". After contemplating their options, Seth decides they have no choice but to have Fogell buy the alcohol with his fake ID. Fogell goes in and successfully buys the alcohol, but is interrupted when a robber enters the store, punches him in the face, and takes money from the cash register.		When police officers Slater (Bill Hader) and Michaels (Seth Rogen) arrive to investigate the robbery, Seth and Evan believe that Fogell will be arrested for possessing a fake ID. Inside the store, Slater and Michaels are apparently fooled by Fogell's ID and give him a ride to the party. While arguing over what to do, Seth is hit by a car being driven by Francis (Joe Lo Truglio), who promises to take them to a party he is attending in exchange for them not telling the police. During Fogell's time with the police, they exhibit very irresponsible behavior such as drinking on the job, shooting their firearms at a stop sign, and improper use of their sirens to run red lights. All the while, the three develop a strong friendship. When Seth and Evan arrive at the party, they quickly discover that Francis is not welcome there. Francis is brutally beaten by Mark (Kevin Corrigan), the party host, while Seth fills detergent bottles from the basement with alcohol he finds in the fridge. Just as he was about to leave, Mark angrily confronts him for dancing with his fiancée earlier, who was on her period. Mark was about to beat up Seth until an angry party guest attacks him for throwing a glass bottle at his face, which was meant for Seth. Mark's fiancée is angry at Seth for embarrassing her and calls the cops. Seth and Evan escape with the alcohol.		After running away, Evan and Seth begin to argue, with Seth angrily asking why Evan is going to Dartmouth when he knew Seth would not get accepted. Evan angrily responds that Seth has been holding him back for years and he does not want to miss out because of him. During the argument, Evan pushes Seth in front of the police cruiser driven by Slater and Michaels. Afraid of losing their jobs, the cops decide to frame Seth and Evan by arresting them, but when Fogell comes out of the car, Evan makes a run for it, and Seth and Fogell escape with the alcohol. While on a bus, a drifter attempts to steal the vodka that Becca wanted, causing it to fall out of Evan's hand and smash on the floor, after which the trio is kicked off. They run to the party, but on the way, Fogell accidentally clues Seth in on his plans to room with Evan the next year. Hurt, Seth angrily takes the alcohol into the party by himself. At the party, Seth becomes popular and Evan tries to hook up with Becca, but she is too drunk.		Becca drags Evan upstairs to have sex with him, but he declines and leaves after she vomits next to him on the bed. Meanwhile, Fogell impresses Nicola (Aviva Farber) and plans to have sex with her upstairs as well. Seth drunkenly attempts to kiss Jules, but she turns him down because she neither drinks nor wants anything to do with Seth while he is drunk. Seth then confesses to Jules his plan to hook up with her while they were both intoxicated and become her boyfriend over the summer before they both departed for college and that he has effectively ruined any chance of that happening. Jules tries to reassure him otherwise, but before she can continue, Seth passes out and accidentally headbutts her, leaving her with a black eye. Slater and Michaels bust the party and Seth saves an intoxicated Evan by carrying him out. Fogell's lovemaking lasts only a moment, before he is interrupted by Slater, who scares Nicola away. Michaels calms down Slater, who is angry at Fogell for ditching them.		After they apologize for "cock-blocking" him, they reconcile and reveal they knew Fogell was not 25 the whole time—they had played along, wanting to prove cops can have fun as well. To make it up to Fogell, they pretend to arrest him to boost his social standing, then proceed to drive recklessly and destroy their car with a Molotov cocktail while Fogell shoots it with the cop's firearm. At Evan's house, Seth reveals to Evan that he knew all along that Fogell was Evan's roommate in college after looking at papers Evan had lying around with that information. Evan reveals he didn't want to room with Fogell and only did it because he didn't want to live with strangers. He then confesses his true brotherly love feelings to Seth who in return shows the same feelings. Seth and Evan patch things up and declare their friendship for each other. The next morning, they go to the mall to buy stuff for college, where they meet Jules and Becca, and they all reconcile. Seth takes Jules to buy concealer for her bruise, while Evan and Becca leave their friends to go buy a new comforter to replace the one that Becca vomited on. Evan and Seth go their separate ways.		The film was written by Goldberg and Rogen during their teen years. It is loosely based on their own experience as seniors in Vancouver in the late 1990s, hence the character names Seth and Evan. According to an interview at an event panel in 2009 Fogell was also a real friend of Rogen and Goldberg. Rogen was initially slated to play Hill's character Seth, but due to his physical size and age, he played one of the police officers. The film took over seven years to complete from early scripting in 2000 and filming in 2006/2007.[citation needed] Mintz-Plasse was only 17 at the time of filming Superbad, and as a result, his mother was required to be present on set during his sex scene.[4]		The film was primarily shot in Los Angeles.[5]		The high school is actually the exterior of El Segundo High School.[6] The mall scenes were shot at the old Fox Hills Mall (which became the Westfield Mall) in Culver City, California.[7]		Other notable filming locations include the convenience store at the beginning of the film, also in Culver City,[8] the liquor store where "McLovin" gets IDed in Glendale, California,[9] and the bar where the cops take McLovin for a drink is neighboring Los Angeles International Airport (LAX).[10]		The scene where McLovin and the cops do donuts in the cop car was filmed in a parking lot on the California State University, Northridge campus.		Superbad opened at number one at the United States box office, grossing US$33,052,411 in its weekend from 2,948 theaters for an average of US$11,212 per theater.[11] The film stayed at #1 the second week, grossing US$18,044,369.[11]		The film grossed US$121.5 million in the United States and Canada and US$48.4 million in other countries, for a total of US$169.9 million worldwide. Compared to the budget of US$20 million, the film earned a huge financial profit,[2] making it the highest domestic grossing high school comedy at the time (it was surpassed by 21 Jump Street, a film also starring Hill, in 2012).[12]		Superbad received favorable reviews from critics. The review aggregator website Rotten Tomatoes reported an 88% approval rating with an average rating of 7.5/10 based on 204 reviews. The website's consensus reads, "Deftly balancing vulgarity and sincerity while placing its protagonists in excessive situations, Superbad is an authentic take on friendship and the overarching awkwardness of the high school experience."[13] On Metacritic, the film has a score of 76/100 based on 36 reviews, indicating "generally favorable reviews".[14]		Mick LaSalle of the San Francisco Chronicle called it 2007's most successful comedy. Roger Ebert of the Chicago Sun-Times had the headline of his review read "McLovin It," and gave the film 3 1⁄2 stars (out of 4) and said "The movie reminded me a little of National Lampoon's Animal House, except that it's more mature, as all movies are."[15] Carina Chocano of the Los Angeles Times said "Physically, Hill and Cera recall the classic comic duos—Laurel and Hardy, Abbott and Costello, Aykroyd and Belushi. But they are contemporary kids, sophisticated and sensitive to nuance"; she added, "I hope it's not damning the movie with the wrong kind of praise to say that for a film so deliriously smutty, Superbad is supercute".[16] Sean Burns of Philadelphia Weekly said "2007: the year Judd Apatow and Seth Rogen saved movie comedy", a reference to Knocked Up which was released in June.[17] Devin Gordon of Newsweek said "As a Revenge of the Nerds redux, Superbad isn't perfect. But it's super close."[18]		In a more critical vein, Stephen Farber of The Hollywood Reporter, compared the film to other films with a single-day structure, such as American Graffiti and Dazed and Confused, but said that Superbad "doesn't have the smarts or the depths of those ensemble comedies".[19] The Hollywood Reporter review was referenced in the film's DVD audio commentary, particularly the review's suggestion that the two main characters have a homoerotic experience similar to the film Y Tu Mamá También.[20] Adam Graham of The Detroit News said, "the cops belong in a bad Police Academy sequel, not this movie", and also that the film "falls short of teen-classic status."[21] Roger Moore of the Orlando Sentinel called the film "super-derivative", "super-raunchy", and "Freaks and Geeks: Uncensored". Moore went on to say the film shamelessly plagiarizes from films such as Can't Hardly Wait and American Graffiti. He also said, "Like Knocked Up, this is a comedy they don't know how to end. The energy flags as it overstays its welcome." Wesley Morris of The Boston Globe said the film "has a degree more sophistication than Revenge of the Nerds and American Pie, and less than the underrated House Party". Morris also said, "the few smart observations could have come from an episode of one of Apatow's TV shows" and "I wanted to find this as funny as audiences did".[22]		It was listed as #487 on Empire's 500 Greatest films of all time.[23]		Superbad was released via DVD and Blu-ray on December 4, 2007, in two versions: theatrical (113 minutes) and unrated (118 minutes). Special features include deleted scenes, an audio commentary on the unrated version with cast and crew, line-o-ramas (a feature most associated with Apatow films), a making-of, and a number of featurettes.		Two tie-in books to the film were published by Newmarket Press:		
Emotional intelligence (EI) is the capability of individuals to recognize their own and other people's emotions, discern between different feelings and label them appropriately, use emotional information to guide thinking and behavior, and manage and/or adjust emotions to adapt to environments or achieve one's goal(s).[1]		Although the term first appeared in a 1964 paper by Michael Beldoch, it gained popularity in the 1995 book by that title, written by the author, psychologist, and science journalist Daniel Goleman. Since this time, Goleman's 1995 analysis of EI has been criticized within the scientific community,[2] despite prolific reports of its usefulness in the popular press.[3][4][5][6]		There are currently several models of EI. Goleman's original model may now be considered a mixed model that combines what have subsequently been modeled separately as ability EI and trait EI. Goleman defined EI as the array of skills and characteristics that drive leadership performance.[7] The trait model was developed by Konstantin Vasily Petrides in 2001. It "encompasses behavioral dispositions and self perceived abilities and is measured through self report".[8] The ability model, developed by Peter Salovey and John Mayer in 2004, focuses on the individual's ability to process emotional information and use it to navigate the social environment.[9]		Studies have shown that people with high EI have greater mental health, job performance, and leadership skills although no causal relationships have been shown and such findings are likely to be attributable to general intelligence and specific personality traits rather than emotional intelligence as a construct. For example, Goleman indicated that EI accounted for 67% of the abilities deemed necessary for superior performance in leaders, and mattered twice as much as technical expertise or IQ.[10] Other research finds that the effect of EI on leadership and managerial performance is non-significant when ability and personality are controlled for,[11] and that general intelligence correlates very closely with leadership.[12] Markers of EI and methods of developing it have become more widely coveted in the past decade. In addition, studies have begun to provide evidence to help characterize the neural mechanisms of emotional intelligence.[13][14][15]		Criticisms have centered on whether EI is a real intelligence and whether it has incremental validity over IQ and the Big Five personality traits.[16] Review finds that, in most studies, poor research methodology has exaggerated the significance of EI.[17]						The term "emotional intelligence" seems first to have appeared in a 1964 paper by Michael Beldoch,[18][19] and in the 1966 paper by B. Leuner entitled Emotional intelligence and emancipation which appeared in the psychotherapeutic journal: Practice of child psychology and child psychiatry.[20]		In 1983, Howard Gardner's Frames of Mind: The Theory of Multiple Intelligences[21] introduced the idea that traditional types of intelligence, such as IQ, fail to fully explain cognitive ability. He introduced the idea of multiple intelligences which included both interpersonal intelligence (the capacity to understand the intentions, motivations and desires of other people) and intrapersonal intelligence (the capacity to understand oneself, to appreciate one's feelings, fears and motivations).[22]		The term subsequently appeared in Wayne Payne's doctoral thesis, A Study of Emotion: Developing Emotional Intelligence from 1985.[23]		The first published use of the term 'EQ' (Emotional Quotient) is an article by Keith Beasley in 1987 in the British Mensa magazine.[24][non-primary source needed][verification needed][citation needed]		In 1989 Stanley Greenspan put forward a model to describe EI, followed by another by Peter Salovey and John Mayer published in the same year.[25]		However, the term became widely known with the publication of Goleman's book: Emotional Intelligence – Why it can matter more than IQ[26] (1995). It is to this book's best-selling status that the term can attribute its popularity.[27][28] Goleman has followed up with several further popular publications of a similar theme that reinforce use of the term.[29][30][31][32][33] To date, tests measuring EI have not replaced IQ tests as a standard metric of intelligence. Emotional Intelligence has also received criticism on its role in leadership and business success.[34]		The distinction between trait emotional intelligence and ability emotional intelligence was introduced in 2000.[35]		Emotional intelligence can be defined as the ability to monitor one's own and other people's emotions, to discriminate between different emotions and label them appropriately, and to use emotional information to guide thinking and behavior.[1] Emotional intelligence also reflects abilities to join intelligence, empathy and emotions to enhance thought and understanding of interpersonal dynamics.[36] However, substantial disagreement exists regarding the definition of EI, with respect to both terminology and operationalizations. Currently, there are three main models of EI:		Different models of EI have led to the development of various instruments for the assessment of the construct. While some of these measures may overlap, most researchers agree that they tap different constructs.		Specific ability models address the ways in which emotions facilitate thought and understanding. For example, emotions may interact with thinking and allow people to be better decision makers (Lyubomirsky et al. 2005).[36] A person who is more responsive emotionally to crucial issues will attend to the more crucial aspects of his or her life.[36] Aspects of emotional facilitation factor is to also know how to include or exclude emotions from thought depending on context and situation.[36] This is also related to emotional reasoning and understanding in response to the people, environment and circumstances one encounters in his or her day-to-day life.[36]		Salovey and Mayer's conception of EI strives to define EI within the confines of the standard criteria for a new intelligence.[39][40] Following their continuing research, their initial definition of EI was revised to "The ability to perceive emotion, integrate emotion to facilitate thought, understand emotions and to regulate emotions to promote personal growth." However, after pursuing further research, their definition of EI evolved into "the capacity to reason about emotions, and of emotions, to enhance thinking. It includes the abilities to accurately perceive emotions, to access and generate emotions so as to assist thought, to understand emotions and emotional knowledge, and to reflectively regulate emotions so as to promote emotional and intellectual growth." [9]		The ability-based model views emotions as useful sources of information that help one to make sense of and navigate the social environment.[41][42] The model proposes that individuals vary in their ability to process information of an emotional nature and in their ability to relate emotional processing to a wider cognition. This ability is seen to manifest itself in certain adaptive behaviors. The model claims that EI includes four types of abilities:		The ability EI model has been criticized in the research for lacking face and predictive validity in the workplace.[43] However, in terms of construct validity, ability EI tests have great advantage over self-report scales of EI because they compare individual maximal performance to standard performance scales and do not rely on individuals' endorsement of descriptive statements about themselves.[44]		The current measure of Mayer and Salovey's model of EI, the Mayer-Salovey-Caruso Emotional Intelligence Test (MSCEIT) is based on a series of emotion-based problem-solving items.[42][45] Consistent with the model's claim of EI as a type of intelligence, the test is modeled on ability-based IQ tests. By testing a person's abilities on each of the four branches of emotional intelligence, it generates scores for each of the branches as well as a total score.		Central to the four-branch model is the idea that EI requires attunement to social norms. Therefore, the MSCEIT is scored in a consensus fashion, with higher scores indicating higher overlap between an individual's answers and those provided by a worldwide sample of respondents. The MSCEIT can also be expert-scored, so that the amount of overlap is calculated between an individual's answers and those provided by a group of 21 emotion researchers.[42]		Although promoted as an ability test, the MSCEIT is unlike standard IQ tests in that its items do not have objectively correct responses. Among other challenges, the consensus scoring criterion means that it is impossible to create items (questions) that only a minority of respondents can solve, because, by definition, responses are deemed emotionally "intelligent" only if the majority of the sample has endorsed them. This and other similar problems have led some cognitive ability experts to question the definition of EI as a genuine intelligence.[citation needed]		In a study by Føllesdal,[46] the MSCEIT test results of 111 business leaders were compared with how their employees described their leader. It was found that there were no correlations between a leader's test results and how he or she was rated by the employees, with regard to empathy, ability to motivate, and leader effectiveness. Føllesdal also criticized the Canadian company Multi-Health Systems, which administers the MSCEIT test. The test contains 141 questions but it was found after publishing the test that 19 of these did not give the expected answers. This has led Multi-Health Systems to remove answers to these 19 questions before scoring but without stating this officially.		Various other specific measures have also been used to assess ability in emotional intelligence. These measures include:		The model introduced by Daniel Goleman[47] focuses on EI as a wide array of competencies and skills that drive leadership performance. Goleman's model outlines five main EI constructs (for more details see "What Makes A Leader" by Daniel Goleman, best of Harvard Business Review 1998):		Goleman includes a set of emotional competencies within each construct of EI. Emotional competencies are not innate talents, but rather learned capabilities that must be worked on and can be developed to achieve outstanding performance. Goleman posits that individuals are born with a general emotional intelligence that determines their potential for learning emotional competencies.[48] Goleman's model of EI has been criticized in the research literature as mere "pop psychology" (Mayer, Roberts, & Barsade, 2008).		Two measurement tools are based on the Goleman model:		Konstantinos Vasilis Petrides ("K. V. Petrides") proposed a conceptual distinction between the ability based model and a trait based model of EI and has been developing the latter over many years in numerous publications.[35][50] Trait EI is "a constellation of emotional self-perceptions located at the lower levels of personality."[50] In lay terms, trait EI refers to an individual's self-perceptions of their emotional abilities. This definition of EI encompasses behavioral dispositions and self-perceived abilities and is measured by self report, as opposed to the ability based model which refers to actual abilities, which have proven highly resistant to scientific measurement. Trait EI should be investigated within a personality framework.[51] An alternative label for the same construct is trait emotional self-efficacy.		The trait EI model is general and subsumes the Goleman model discussed above. The conceptualization of EI as a personality trait leads to a construct that lies outside the taxonomy of human cognitive ability. This is an important distinction in as much as it bears directly on the operationalization of the construct and the theories and hypotheses that are formulated about it.[35]		There are many self-report measures of EI,[52] including the EQ-i, the Swinburne University Emotional Intelligence Test (SUEIT), and the Schutte EI model. None of these assess intelligence, abilities, or skills (as their authors often claim), but rather, they are limited measures of trait emotional intelligence.[50] The most widely used and widely researched measure of self-report or self-schema (as it is currently referred to) emotional intelligence is the EQ-i 2.0. Originally known as the BarOn EQ-i, it was the first self-report measure of emotional intelligence available, the only measure predating Goleman's best-selling book. There are over 200 studies that have used the EQ-i or EQ-i 2.0. It has the best norms, reliability, and validity of any self-report instrument and was the first one reviewed in Buros Mental Measures Book. The EQ-i 2.0 is available in many different languages as it is used worldwide.		The TEIQue provides an operationalization for the model of Petrides and colleagues, that conceptualizes EI in terms of personality.[53] The test encompasses 15 subscales organized under four factors: well-being, self-control, emotionality, and sociability. The psychometric properties of the TEIQue were investigated in a study on a French-speaking population, where it was reported that TEIQue scores were globally normally distributed and reliable.[54]		The researchers also found TEIQue scores were unrelated to nonverbal reasoning (Raven's matrices), which they interpreted as support for the personality trait view of EI (as opposed to a form of intelligence). As expected, TEIQue scores were positively related to some of the Big Five personality traits (extraversion, agreeableness, openness, conscientiousness) as well as inversely related to others (alexithymia, neuroticism). A number of quantitative genetic studies have been carried out within the trait EI model, which have revealed significant genetic effects and heritabilities for all trait EI scores.[55] Two recent studies (one a meta-analysis) involving direct comparisons of multiple EI tests yielded very favorable results for the TEIQue.[38][56]		A review published in the journal of Annual Psychology found that higher emotional intelligence is positively correlated with:[36]		Goleman's early work has been criticized for assuming from the beginning that EI is a type of intelligence or cognitive ability. Eysenck (2000)[57] writes that Goleman's description of EI contains unsubstantiated assumptions about intelligence in general, and that it even runs contrary to what researchers have come to expect when studying types of intelligence:		"[Goleman] exemplifies more clearly than most the fundamental absurdity of the tendency to class almost any type of behavior as an 'intelligence'... If these five 'abilities' define 'emotional intelligence', we would expect some evidence that they are highly correlated; Goleman admits that they might be quite uncorrelated, and in any case if we cannot measure them, how do we know they are related? So the whole theory is built on quicksand: there is no sound scientific basis."		Similarly, Locke (2005)[58] claims that the concept of EI is in itself a misinterpretation of the intelligence construct, and he offers an alternative interpretation: it is not another form or type of intelligence, but intelligence—the ability to grasp abstractions—applied to a particular life domain: emotions. He suggests the concept should be re-labeled and referred to as a skill.		The essence of this criticism is that scientific inquiry depends on valid and consistent construct utilization, and that before the introduction of the term EI, psychologists had established theoretical distinctions between factors such as abilities and achievements, skills and habits, attitudes and values, and personality traits and emotional states.[59] Thus, some scholars believe that the term EI merges and conflates such accepted concepts and definitions.		Adam Grant warned of the common but mistaken perception of EI as a desirable moral quality rather than a skill, Grant asserting that a well-developed EI is not only an instrumental tool for accomplishing goals, but has a dark side as a weapon for manipulating others by robbing them of their capacity to reason.[60]		Landy (2005)[61] claimed that the few incremental validity studies conducted on EI have shown that it adds little or nothing to the explanation or prediction of some common outcomes (most notably academic and work success). Landy suggested that the reason why some studies have found a small increase in predictive validity is a methodological fallacy, namely, that alternative explanations have not been completely considered:		"EI is compared and contrasted with a measure of abstract intelligence but not with a personality measure, or with a personality measure but not with a measure of academic intelligence." Landy (2005)		Similarly, other researchers have raised concerns about the extent to which self-report EI measures correlate with established personality dimensions. Generally, self-report EI measures and personality measures have been said to converge because they both purport to measure personality traits.[50] Specifically, there appear to be two dimensions of the Big Five that stand out as most related to self-report EI – neuroticism and extroversion. In particular, neuroticism has been said to relate to negative emotionality and anxiety. Intuitively, individuals scoring high on neuroticism are likely to score low on self-report EI measures.		The interpretations of the correlations between EI questionnaires and personality have been varied. The prominent view in the scientific literature is the Trait EI view, which re-interprets EI as a collection of personality traits.[62][63][64]		One criticism of the works of Mayer and Salovey comes from a study by Roberts et al. (2001),[65] which suggests that the EI, as measured by the MSCEIT, may only be measuring conformity. This argument is rooted in the MSCEIT's use of consensus-based assessment, and in the fact that scores on the MSCEIT are negatively distributed (meaning that its scores differentiate between people with low EI better than people with high EI).		Further criticism has been leveled by Brody (2004),[66] who claimed that unlike tests of cognitive ability, the MSCEIT "tests knowledge of emotions but not necessarily the ability to perform tasks that are related to the knowledge that is assessed". The main argument is that even though someone knows how he should behave in an emotionally laden situation, it doesn't necessarily follow that the person could actually carry out the reported behavior.		New research is surfacing that suggests that ability EI measures might be measuring personality in addition to general intelligence. These studies examined the multivariate effects of personality and intelligence on EI and also corrected estimates for measurement error (which is often not done in some validation studies). For example, a study by Schulte, Ree, Carretta (2004),[67] showed that general intelligence (measured with the Wonderlic Personnel Test), agreeableness (measured by the NEO-PI), as well as gender could reliably be used to predict the measure of EI ability.		They gave a multiple correlation (R) of .81 with the MSCEIT (perfect prediction would be 1). This result has been replicated by Fiori and Antonakis (2011),;[68] they found a multiple R of .76 using Cattell’s "Culture Fair" intelligence test and the Big Five Inventory (BFI); significant covariates were intelligence (standardized beta = .39), agreeableness (standardized beta = .54), and openness (standardized beta = .46). Antonakis and Dietz (2011a),[69] who investigated the Ability Emotional Intelligence Measure found similar results (Multiple R = .69), with significant predictors being intelligence, standardized beta = .69 (using the Swaps Test and a Wechsler scales subtest, the 40-item General Knowledge Task) and empathy, standardized beta = .26 (using the Questionnaire Measure of Empathic Tendency)--see also Antonakis and Dietz (2011b),[70] who show how including or excluding important controls variables can fundamentally change results—thus, it is important to always include important controls like personality and intelligence when examining the predictive validity of ability and trait EI models. §		More formally termed socially desirable responding (SDR), faking good is defined as a response pattern in which test-takers systematically represent themselves with an excessive positive bias (Paulhus, 2002). This bias has long been known to contaminate responses on personality inventories (Holtgraves, 2004; McFarland & Ryan, 2000; Peebles & Moore, 1998; Nichols & Greene, 1997; Zerbe & Paulhus, 1987), acting as a mediator of the relationships between self-report measures (Nichols & Greene, 1997; Gangster et al., 1983).[full citation needed]		It has been suggested that responding in a desirable way is a response set, which is a situational and temporary response pattern (Pauls & Crost, 2004; Paulhus, 1991). This is contrasted with a response style, which is a more long-term trait-like quality. Considering the contexts some self-report EI inventories are used in (e.g., employment settings), the problems of response sets in high-stakes scenarios become clear (Paulhus & Reid, 2001).		There are a few methods to prevent socially desirable responding on behavior inventories. Some researchers believe it is necessary to warn test-takers not to fake good before taking a personality test (e.g., McFarland, 2003). Some inventories use validity scales in order to determine the likelihood or consistency of the responses across all items.		Landy[61] distinguishes between the "commercial wing" and "the academic wing" of the EI movement, basing this distinction on the alleged predictive power of EI as seen by the two currents. According to Landy, the former makes expansive claims on the applied value of EI, while the latter is trying to warn users against these claims. As an example, Goleman (1998) asserts that "the most effective leaders are alike in one crucial way: they all have a high degree of what has come to be known as emotional intelligence. ...emotional intelligence is the sine qua non of leadership". In contrast, Mayer (1999) cautions "the popular literature's implication—that highly emotionally intelligent people possess an unqualified advantage in life—appears overly enthusiastic at present and unsubstantiated by reasonable scientific standards." Landy further reinforces this argument by noting that the data upon which these claims are based are held in "proprietary databases", which means they are unavailable to independent researchers for reanalysis, replication, or verification.[61] Thus, the credibility of the findings cannot be substantiated in a scientific way, unless those datasets are made public and available for independent analysis.		In an academic exchange, Antonakis and Ashkanasy/Dasborough mostly agreed that researchers testing whether EI matters for leadership have not done so using robust research designs; therefore, currently there is no strong evidence showing that EI predicts leadership outcomes when accounting for personality and IQ.[71] Antonakis argued that EI might not be needed for leadership effectiveness (he referred to this as the "curse of emotion" phenomenon, because leaders who are too sensitive to their and others' emotional states might have difficulty making decisions that would result in emotional labor for the leader or followers). A recently published meta-analysis seems to support the Antonakis position: In fact, Harms and Credé found that overall (and using data free from problems of common source and common methods), EI measures correlated only ρ = 0.11 with measures of transformational leadership.[72] Interestingly, ability-measures of EI fared worst (i.e., ρ = 0.04); the WLEIS (Wong-Law measure) did a bit better (ρ = 0.08), and the Bar-On[73] measure better still (ρ = 0.18). However, the validity of these estimates does not include the effects of IQ or the big five personality, which correlate both with EI measures and leadership.[74] In a subsequent paper analyzing the impact of EI on both job performance and leadership, Harms and Credé[75] found that the meta-analytic validity estimates for EI dropped to zero when Big Five traits and IQ were controlled for. Joseph and Newman[76] meta-analytically showed the same result for Ability EI.		However, it is important to note that self-reported and Trait EI measures retain a fair amount of predictive validity for job performance after controlling Big Five traits and IQ.[76] Newman, Joseph, and MacCann[77] contend that the greater predictive validity of Trait EI measures is due to their inclusion of content related to achievement motivation, self efficacy, and self-rated performance. Meta-analytic evidence confirms that self-reported emotional intelligence predicting job performance is due to mixed EI and trait EI measures' tapping into self-efficacy and self-rated performance, in addition to the domains of Neuroticism, Extraversion, Conscientiousness, and IQ. As such, the predictive ability of mixed EI to job performance drops to nil when controlling for these factors.[78]		The National Institute of Child Health and Human Development has recognized that because there are divisions about the topic of emotional intelligence, the mental health community needs to agree on some guidelines to describe good mental health and positive mental living conditions. In their section, "Positive Psychology and the Concept of Health", they explain. "Currently there are six competing models of positive health, which are based on concepts such as being above normal, character strengths and core virtues, developmental maturity, social-emotional intelligence, subjective well-being, and resilience. But these concepts define health in philosophical rather than empirical terms. Dr. [Lawrence] Becker suggested the need for a consensus on the concept of positive psychological health...".[79]		Bullying is abusive social interaction between peers which can include aggression, harassment, and violence. Bullying is typically repetitive and enacted by those who are in a position of power over the victim. A growing body of research illustrates a significant relationship between bullying and emotional intelligence.[80][81][82] Emotional intelligence (EI) is a set of abilities related to the understanding, use and management of emotion as it relates to one's self and others. Mayer et al., (2008) defines the dimensions of overall EI as: "accurately perceiving emotion, using emotions to facilitate thought, understanding emotion, and managing emotion".[83] The concept combines emotional and intellectual processes.[84] Lower emotional intelligence appears to be related to involvement in bullying, as the bully and/or the victim of bullying. EI seems to play an important role in both bullying behavior and victimization in bullying; given that EI is illustrated to be malleable, EI education could greatly improve bullying prevention and intervention initiatives.[85]		Research of EI and job performance shows mixed results: a positive relation has been found in some of the studies, while in others there was no relation or an inconsistent one.[78] This led researchers Cote and Miners (2006)[86] to offer a compensatory model between EI and IQ, that posits that the association between EI and job performance becomes more positive as cognitive intelligence decreases, an idea first proposed in the context of academic performance (Petrides, Frederickson, & Furnham, 2004). The results of the former study supported the compensatory model: employees with low IQ get higher task performance and organizational citizenship behavior directed at the organization, the higher their EI. It has also been observed that there is no significant link between emotional intelligence and work attitude behavior.[87]		A more recent study suggests that EI is not necessarily a universally positive trait.[88] They found a negative correlation between EI and managerial work demands; while under low levels of managerial work demands, they found a negative relationship between EI and teamwork effectiveness. An explanation for this may suggest gender differences in EI, as women tend to score higher levels than men.[76] This furthers the idea that job context plays a role in the relationships between EI, teamwork effectiveness, and job performance. Another interesting find was discussed in a study that assessed a possible link between EI and entrepreneurial behaviors and success.[89]		Although studies between emotional intelligence (EI) and job performance has shown mixed results of high and low correlations, EI is undeniably better predictor than most of the hiring methods commonly used in companies, such as letter of references, cover letter, among others. By 2008, 147 companies and consulting firms in U.S had developed programmes that involved EI for training and hiring employees.[90] Van Rooy and Viswesvaran (2004)[91] showed that EI correlated significantly with different domains in performance, ranging from .24 for job performance to .10 for academic performance. These findings may contribute organisations in different ways. For instance, employees high on EI would be more aware of their own emotions and from others, which in turn, could lead companies to better profits and less unnecessary expenses. This is especially important for expatriate managers, who have to deal with mixed emotions and feelings, while adapting to a new working culture.[91] Moreover, employees high in EI show more confidence in their roles, which allow them to face demanding tasks positively.[92]		Emotional Intelligence accounted for more career success than IQ.[93] Similarly, other studies argued that employees high on EI perform substantially better than employees low in EI. This measured by self-reports and different work performance indicators, such as wages, promotions and salary increase.[94] According to Lopes and his colleagues (2006),[95] EI contributes to develop strong and positive relationships with co-workers and perform efficiently in work teams. This benefits performance of workers by providing emotional support and instrumental resources needed to succeed in their roles.[96] Also, emotional intelligent employees have better resources to cope with stressing situations and demanding tasks, which enable them to outperform in those situations.[95] For instance, Law et al. (2004)[94] found that EI was the best predictor of job performance beyond general cognitive ability among IT scientist in computer company in China. Similarly, Sy, Tram, and O’Hara (2006)[92] found that EI was associated positively with job performance in employees from a food service company.[97]		In the job performance – emotional intelligence correlation is important to consider the effects of managing up, which refers to the good and positive relationship between the employee and his/her supervisor.[98] Previous research found that quality of this relationship could interfere in the results of the subjective rating of job performance evaluation.[99] Emotional intelligent employees devote more of their working time on managing their relationship with supervisors. Hence, the likelihood of obtaining better results on performance evaluation is greater for employees high in EI than for employees with low EI.[92] Based on theoretical and methodological approaches, EI measures are categorized in three main streams: (1) stream 1: ability-based measures (e.g. MSCEIT), (2) stream 2: self-reports of abilities measures (e.g. SREIT, SUEIT and WLEIS) and (3) stream 3: mixed-models (e.g. AES, ECI, EI questionnaire, EIS, EQ-I and GENOS), which include measures of EI and traditional social skills.[100] O’Boyle Jr. and his colleagues (2011)[101] found that the three EI streams together had a positive correlation of 0.28 with job performance. Similarly, each of EI streams independently obtained a positive correlation of 0.24, 0.30 and 0.28, respectively. Stream 2 and 3 showed an incremental validity for predicting job performance over and above personality (Five Factor model) and general cognitive ability. Both, stream 2 and 3 were the second most important predictor of job performance below general cognitive ability. Stream 2 explained 13.6% of the total variance; whereas stream 3, explained 13.2%. In order to examine the reliability of these findings, a publication bias analysis was developed. Results indicated that studies on EI-job performance correlation prior to 2010 do not present substantial evidences to suggest the presence of publication bias.		Despite the validity of previous findings, some researchers still question whether EI – job performance correlation makes a real impact on business strategies. They argue that popularity of emotional intelligence’s studies is due to media advertising, rather than objective scientific findings.[102] Also, it is mentioned that relationship between job performance and EI is not as strong as suggested. This relationship requires the presence of other constructs to rise important outcomes. For instance, previous studies found that EI is positively associated with teamwork effectiveness under job contexts of high managerial work demands, which improves job performance. This is due to activation of strong emotions during performance on this job context. In this scenario, emotional intelligent individuals show a better set of resources to succeed on their roles. However, individuals with high EI show a similar level of performance than non-emotional intelligent employees under different job contexts.[103] Moreover, Joseph and Newman (2010)[104] suggested that emotional perception and emotional regulation components of EI highly contribute to job performance under job contexts of high emotional demands. Moon and Hur (2011)[105] found that emotional exhaustion (“burn-out”) significantly influences the job performance – EI relationship. Emotional exhaustion showed a negative association with two components of EI (optimism and social skills). This association impacted negatively to job performance, as well. Hence, job performance – EI relationship is stronger under contexts of high emotional exhaustion or burn-out; in other words, employees with high levels of optimism and social skills possess better resources to outperform when facing high emotional exhaustion contexts.		A 2007 meta-analysis of 44 effect sizes by Schutte found that emotional intelligence was associated with better mental and physical health. Particularly, trait EI had the stronger association with mental and physical health.[106] This was replicated again in 2010 by researcher Alexandra Martin who found trait EI as a strong predictor for health after conducting a meta-analysis based on 105 effect sizes and 19,815 participants. This meta-analysis also indicated that this line of research reached enough sufficiency and stability in concluding EI as a positive predictor for health.[107]		A small 2004 study by Ellen Paek empirically examined the extent to which religiosity, operationalized as religious orientation and religious behavior, is related to the controversial[57][58][59] idea of emotional intelligence (EI). The study examined the extent to which religious orientation and behavior were related to self-reported EI in 148 church attending adult Christians.[108] Non-religious individuals were not part of the study. The study found that the individuals' self-reported religious orientation was positively correlated with their perceiving themselves to have greater EI. While the number of religious group activities was positively associated with perceived EI, number of years of church attendance was unrelated. Significant positive correlations were also found between level of religious commitment and perceived EI. Thus, the Christian volunteers were more likely to consider themselves emotionally intelligent if they spent more time in group activities and had more commitment to their beliefs.		Tischler, Biberman and McKeage warn that there is still ambiguity in the above concepts. In their 2002 article, entitled "Linking emotional intelligence, spirituality and workplace performance: Definitions, models and ideas for research", they reviewed literature on both EI and various aspects of spirituality. They found that both EI and spirituality appear to lead to similar attitudes, behaviors and skills, and that there often seems to be confusion, intersection and linking between the two constructs.[109]		A 2012 study cross examined emotional intelligence, self-esteem and marijuana dependence.[110] Out of a sample of 200, 100 of whom were dependent on cannabis and the other 100 emotionally healthy, the dependent group scored exceptionally low on EI when compared to the control group. They also found that the dependent group also scored low on self-esteem when compared to the control.		Another study in 2010 examined whether or not low levels of EI had a relationship with the degree of drug and alcohol addiction.[111] In the assessment of 103 residents in a drug rehabilitation center, they examined their EI along with other psychosocial factors in a one-month interval of treatment. They found that participants' EI scores improved as their levels of addiction lessened as part of their treatment.		
The Wayback Machine is a digital archive of the World Wide Web and other information on the Internet created by the Internet Archive, a nonprofit organization, based in San Francisco, California, United States.		The Internet Archive launched the Wayback Machine in October 2001.[4][5] It was set up by Brewster Kahle and Bruce Gilliat, and is maintained with content from Alexa Internet.[citation needed] The service enables users to see archived versions of web pages across time, which the archive calls a "three dimensional index".[citation needed]		Since 1996, the Wayback Machine has been archiving cached pages of websites onto its large cluster of Linux nodes.[citation needed] It revisits sites every few weeks or months and archives a new version.[citation needed] Sites can also be captured on the fly by visitors who enter the site's URL into a search box.[citation needed] The intent is to capture and archive content that otherwise would be lost whenever a site is changed or closed down.[citation needed] The overall vision of the machine's creators is to archive the entire Internet.[citation needed]		The name Wayback Machine was chosen as a reference to the "WABAC machine" (pronounced way-back), a time-traveling device used by the characters Mr. Peabody and Sherman in The Rocky and Bullwinkle Show, an animated cartoon.[6][7] In one of the animated cartoon's component segments, Peabody's Improbable History, the characters routinely used the machine to witness, participate in, and, more often than not, alter famous events in history.[citation needed]		Software has been developed to "crawl" the web and download all publicly accessible World Wide Web pages, the Gopher hierarchy, the Netnews (Usenet) bulletin board system, and downloadable software.[8] The information collected by these "crawlers" does not include all the information available on the Internet, since much of the data is restricted by the publisher or stored in databases that are not accessible. To overcome inconsistencies in partially cached websites, Archive-It.org was developed in 2005 by the Internet Archive as a means of allowing institutions and content creators to voluntarily harvest and preserve collections of digital content, and create digital archives.[citation needed]		Information had been kept on digital tape for five years, with Kahle occasionally allowing researchers and scientists to tap into the clunky database.[9] When the archive reached its fifth anniversary, it was unveiled and opened to the public in a ceremony at the University of California, Berkeley.[citation needed]		Snapshots usually become available more than six months after they are archived or, in some cases, even later; it can take twenty-four months or longer.[10] The frequency of snapshots is variable, so not all tracked website updates are recorded. Sometimes there are intervals of several weeks or years between snapshots.[citation needed]		After August 2008 sites had to be listed on the Open Directory in order to be included.[11] According to Jeff Kaplan of the Internet Archive in November 2010, other sites were still being archived,[12] but more recent captures would become visible only after the next major indexing, an infrequent operation.[citation needed]		As of 2009[update], the Wayback Machine contained approximately three petabytes of data and was growing at a rate of 100 terabytes each month;[13] the growth rate reported in 2003 was 12 terabytes/month. The data is stored on PetaBox rack systems manufactured by Capricorn Technologies.[14]		In 2009, the Internet Archive migrated its customized storage architecture to Sun Open Storage, and hosts a new data center in a Sun Modular Datacenter on Sun Microsystems' California campus.[15]		In 2011 a new, improved version of the Wayback Machine, with an updated interface and fresher index of archived content, was made available for public testing.[16]		In March 2011, it was said on the Wayback Machine forum that "The Beta of the new Wayback Machine has a more complete and up-to-date index of all crawled materials into 2010, and will continue to be updated regularly. The index driving the classic Wayback Machine only has a little bit of material past 2008, and no further index updates are planned, as it will be phased out this year".[17]		In January 2013, the company announced a ground-breaking milestone of 240 billion URLs.[18]		In October 2013, the company announced the "Save a Page" feature[19] which allows any Internet user to archive the contents of a URL. This became a threat of abuse by the service for hosting malicious binaries.[20][21]		As of December 2014[update], the Wayback Machine contained almost nine petabytes of data and was growing at a rate of about 20 terabytes each week.[22]		As of July 2016[update], the Wayback Machine reportedly contained around 15 petabytes of data.[23]		Between October 2013 and March 2015 the website's global Alexa rank changed from 162[24] to 208.[25]		Historically, Wayback Machine respected the robots exclusion standard (robots.txt) in determining if a website would be crawled or not; or if already crawled, if its archives would be publicly viewable. Website owners had the option to opt-out of Wayback Machine through the use of robots.txt. It applied robots.txt rules retroactively; if a site blocked the Internet Archive, any previously archived pages from the domain were immediately rendered unavailable as well. In addition the Internet Archive stated, "Sometimes a website owner will contact us directly and ask us to stop crawling or archiving a site. We comply with these requests."[37] In addition, the website says: "The Internet Archive is not interested in preserving or offering access to Web sites or other Internet documents of persons who do not want their materials in the collection."[38]		This policy began to relax in 2017, when it stopped honoring robots.txt on U.S. government and military web sites for both crawling and displaying web pages. As of April 2017, Wayback is exploring ignoring robots.txt more broadly, not just for U.S. government websites.[39][40][41][42]		The site is frequently used by journalists and citizens to review dead websites, dated news reports or changes to website contents. Its content has been used to hold politicians accountable and expose battlefield lies.[43]		In 2014 an archived social media page of separatist rebel leader in Ukraine Igor Girkin showed him boasting about his troops having shot down a suspected Ukrainian military airplane before it became known that the plane actually was a civilian Malaysian Airlines jet after which he deleted the post and blamed Ukraine's military.[43][44]		In 2017 the March for Science originated from a discussion on reddit that indicated someone had visited Archive.org and discovered that all references to climate change had been deleted from the White House website. In response, a user commented, "There needs to be a Scientists' March on Washington".[45][46][47]		Furthermore, the site is used heavily for verification, providing access to references and content creation by Wikipedia editors.[citation needed]		In a 2009 case, Netbula, LLC v. Chordiant Software Inc., defendant Chordiant filed a motion to compel Netbula to disable the robots.txt file on its website that was causing the Wayback Machine to retroactively remove access to previous versions of pages it had archived from Netbula's site, pages that Chordiant believed would support its case.[48]		Netbula objected to the motion on the ground that defendants were asking to alter Netbula's website and that they should have subpoenaed Internet Archive for the pages directly.[49] An employee of Internet Archive filed a sworn statement supporting Chordiant's motion, however, stating that it could not produce the web pages by any other means "without considerable burden, expense and disruption to its operations."[48]		Magistrate Judge Howard Lloyd in the Northern District of California, San Jose Division, rejected Netbula's arguments and ordered them to disable the robots.txt blockage temporarily in order to allow Chordiant to retrieve the archived pages that they sought.[48]		In an October 2004 case, Telewizja Polska USA, Inc. v. Echostar Satellite, No. 02 C 3293, 65 Fed. R. Evid. Serv. 673 (N.D. Ill. Oct. 15, 2004), a litigant attempted to use the Wayback Machine archives as a source of admissible evidence, perhaps for the first time. Telewizja Polska is the provider of TVP Polonia and EchoStar operates the Dish Network. Prior to the trial proceedings, EchoStar indicated that it intended to offer Wayback Machine snapshots as proof of the past content of Telewizja Polska's website. Telewizja Polska brought a motion in limine to suppress the snapshots on the grounds of hearsay and unauthenticated source, but Magistrate Judge Arlander Keys rejected Telewizja Polska's assertion of hearsay and denied TVP's motion in limine to exclude the evidence at trial.[50][51] At the trial, however, district Court Judge Ronald Guzman, the trial judge, overruled Magistrate Keys' findings,[citation needed] and held that neither the affidavit of the Internet Archive employee nor the underlying pages (i.e., the Telewizja Polska website) were admissible as evidence. Judge Guzman reasoned that the employee's affidavit contained both hearsay and inconclusive supporting statements, and the purported web page printouts were not self-authenticating.[citation needed]		Provided some additional requirements are met (e.g., providing an authoritative statement of the archivist), the United States patent office and the European Patent Office will accept date stamps from the Internet Archive as evidence of when a given Web page was accessible to the public. These dates are used to determine if a Web page is available as prior art for instance in examining a patent application.[52]		There are technical limitations to archiving a website, and as a consequence, it is possible for opposing parties in litigation to misuse the results provided by website archives. This problem can be exacerbated by the practice of submitting screen shots of web pages in complaints, answers, or expert witness reports, when the underlying links are not exposed and therefore, can contain errors. For example, archives such as the Wayback Machine do not fill out forms and therefore, do not include the contents of non-RESTful e-commerce databases in their archives.[53]		In Europe the Wayback Machine could be interpreted as violating copyright laws. Only the content creator can decide where their content is published or duplicated, so the Archive would have to delete pages from its system upon request of the creator.[54] The exclusion policies for the Wayback Machine may be found in the FAQ section of the site.[citation needed]		A number of cases have been brought against the Internet Archive specifically for its Wayback Machine archiving efforts.		In late 2002, the Internet Archive removed various sites that were critical of Scientology from the Wayback Machine.[55] An error message stated that this was in response to a "request by the site owner".[56] Later, it was clarified that lawyers from the Church of Scientology had demanded the removal and that the site owners did not want their material removed.[57]		In 2003, Harding Earley Follmer & Frailey defended a client from a trademark dispute using the Archive's Wayback Machine. The attorneys were able to demonstrate that the claims made by the plaintiff were invalid, based on the content of their website from several years prior. The plaintiff, Healthcare Advocates, then amended their complaint to include the Internet Archive, accusing the organization of copyright infringement as well as violations of the DMCA and the Computer Fraud and Abuse Act. Healthcare Advocates claimed that, since they had installed a robots.txt file on their website, even if after the initial lawsuit was filed, the Archive should have removed all previous copies of the plaintiff website from the Wayback Machine.[58] The lawsuit was settled out of court.[59]		In December 2005, activist Suzanne Shell filed suit demanding Internet Archive pay her US $100,000 for archiving her website profane-justice.org between 1999 and 2004.[60][61] Internet Archive filed a declaratory judgment action in the United States District Court for the Northern District of California on January 20, 2006, seeking a judicial determination that Internet Archive did not violate Shell's copyright. Shell responded and brought a countersuit against Internet Archive for archiving her site, which she alleges is in violation of her terms of service.[62] On February 13, 2007, a judge for the United States District Court for the District of Colorado dismissed all counterclaims except breach of contract.[61] The Internet Archive did not move to dismiss copyright infringement claims Shell asserted arising out of its copying activities, which would also go forward.[63]		On April 25, 2007, Internet Archive and Suzanne Shell jointly announced the settlement of their lawsuit.[60] The Internet Archive said it "...has no interest in including materials in the Wayback Machine of persons who do not wish to have their Web content archived. We recognize that Ms. Shell has a valid and enforceable copyright in her Web site and we regret that the inclusion of her Web site in the Wayback Machine resulted in this litigation." Shell said, "I respect the historical value of Internet Archive's goal. I never intended to interfere with that goal nor cause it any harm."[64]		In 2013–14, a pornographic actor tried to remove archived images of himself from the WayBack Machine's archive, first by sending multiple DMCA requests to the archive, and then by appealing to the Federal Court of Canada.[65][66]		In 2005, Yahoo! Search began to provide links to other versions of pages archived on the Wayback Machine.[67]		Archive.org is currently blocked in China.[68][69] After the site enabled the encrypted HTTPS protocol, the Internet Archive was blocked in its entirety in Russia in 2015.[70][71][43][needs update?]		Alison Macrina, director of the Library Freedom Project, notes that "while librarians deeply value individual privacy, we also strongly oppose censorship".[43]		There are known rare cases where online access to content which "for nothing" has put people in danger was disabled.[43]		In April 2017, emails of French presidential candidate Emmanuel Macron were leaked to the site and elsewhere.[72][73] As archive.org is not a website for publishing original leaks, uploading this data to the site first may have been intended to or may effectively cause harm to the site.[citation needed]		Other threats include natural disasters,[74] destruction (remote or physical),[citation needed] manipulation of the archive's contents (see also: cyberattack, backup), problematic copyright laws[75] and surveillance of the site's users.[76]		Kevin Vaughan suspects that in the long-term of multiple generations "next to nothing" will survive in a useful way besides "if we have continuity in our technological civilization" by which "a lot of the bare data will remain findable and searchable".[77]		Some find the Internet Archive, which describes itself to be built for the long-term,.[78] to be working furiously to capture data before it disappears without any long-term infrastructure to speak of.[79][copyright violation?]		
Steven Quincy Urkel is a fictional character on the ABC/CBS sitcom Family Matters who was portrayed by Jaleel White.[1] Originally slated to have been a one-time only character on the show, he soon became its most popular character and gradually became its protagonist.[2][3][4][5]		Steve is the epitome of a geek/nerd, with large, thick eyeglasses, flood pants held up by suspenders, multi-colored cardigan sweaters, and a high-pitched voice.[6] He professes an unrequited love for his neighbor Laura Winslow, perpetually annoys her father, Carl, and tries to befriend her brother, Eddie. Amongst the rest of the family, Harriette Winslow, Rachel Crawford and Estelle Winslow are more accepting and caring of Urkel.		Throughout the series' run, Steve is central to many of its recurring gags, primarily property damage and/or personal injury as a result of his inventions going awry or his outright clumsiness.[7] He becomes known for several catchphrases uttered after some humorous misfortune occurs, including "I've fallen, and I can't get up!" after he accidentally became drunk and fell off the edge of a building in the episode "Life of the Party," "Did I do that?" (previously used by Curly in the 1934 Three Stooges short Punch Drunks), "Whoa, Mama!" and "Look what you did" (on those rare occasions when someone else caused the damage). Additionally, he frequently insinuates "You love me, don't you?" and "I'm wearing you down, baby. I'm wearing you down" to Laura Winslow, the usual object of his affection.						Steve Urkel first appeared on the 4th episode of the first season, "Rachel's First Date". On the 12th episode of the first season, "Laura's First Date", he reappears as a nerdy young boy who was supposed to take Laura Winslow out on a date, where he is depicted as being madly in love with her. But, in an example of unrequited love, Laura did not return these feelings because of Steve's nerdy, infuriating personality. Although intended to only appear once, White's portrayal became very popular with the show's studio audience during the live taping of the episode—his popularity later crossing over with the home viewing audience after the episode aired—for his humorous, geeky antics. After appearing in other episodes, he joined the main cast beginning with the season two premiere "Rachel's Place".[8] Throughout the course of the series, Steve maintains his extreme infatuation with Laura and regularly invites himself over for unwanted visits to her house, much to the annoyance of the Winslows. Among Steve's other famed character traits are his exceptional scientific skills, crafting devices that would be impossible to construct in reality, his absurdly destructive clumsiness, and his kind heart.		Steve is commonly known and respected by other characters for his kindness to others, his never-ending love and loyalty for those he holds dear, and, alongside with Harriette, his position as a voice of reason and source of wisdom for the often bickering members of the Winslow family, all of which serve as redeeming qualities for his generally unwelcome or tolerated presence. He always cares for and means well for other people, but is often the misunderstood victim of the Winslows' anger and rejection, especially of Carl, Eddie and Laura, who all struggle to see through his clumsiness and annoying behavior and to understand and appreciate him for his positive traits.		Although he is often portrayed positively for being a kind and virtuous person compared to other repentant characters, Steve is not a flawless character. While he is indeed kind and good-natured, Steve tends to be needy, overbearing, and often demonstrates very poor empathy, understanding, and consideration of other people's feelings, especially when they feel negatively towards him (whether those feelings are justifiable or not). He shows little regard or respect for Laura's choice to not reciprocate Steve's affections. One example of Steve's lack of consideration for others is in the season four episode "Surely You Joust," when he accidentally caused Carl and Eddie to fall off of the roof while they were installing a satellite dish, only to demand an apology from Carl for unintentionally landing on top of Steve, and even going so far as to fight back when Carl subsequently banished him from the Winslow house, leading Waldo to arrange for Carl and Steve to settle their issues on American Gladiators. Steve's random awareness of his own overbearing presence is addressed multiple times throughout the series, yet even when he is aware of the agony he causes to others, his overbearing need for attention is easily greater than any desire to correct his own flaws for the comfort of other people.		Steve can also be quite selfish and self-centered at times. He lives very strictly the way he feels most comfortable, forces people around him to tolerate and adapt around his lifestyle, and his attitude towards these people is quite often negligent and callous. One of his strongest positive traits throughout the show is also his most antagonistic one: his persistence. No matter how angry, miserable, or hurt many people get because of him, his immediate response is always to keep doing what he's doing until they give up demanding him to stop, and just accept him for who he is, for good or ill. He's not often bothered by what he does to others, but instead, he would rather just brush those people off as being grouchy, or assume they're just kidding. This is especially evident towards the Winslows, since they don't always allow him into their home, accept him as family, or offer him Laura's romantic love, and he has never given up challenging or outright fighting them to get what he wants; what they don't want to give to him. This behavior later came back to haunt Steve in Season 5, when Myra became similarly overbearing and persistent in her pursuit in winning his love. Over time, he found out what life is like as the Winslows when she stalked him in seasons 5 and 9. In turn, this helped Urkel finally respect people's privacy and stop taking them for granted.		Because his feelings are too sensitive to rejection by the people he loves most, the Winslows are very hesitant to leave him heartbroken. Although easily prone to having his feelings hurt, he remains rarely ever concerned about why people get so angry at him. So while he is considerably vulnerable to emotional pain, he is almost completely immune to guilt, regret, and any chance to learn from his mistakes, the way the Winslows do. In fact, he displays an ability to block out all negative criticism, and embrace only the meager amounts of redeeming qualities said about him out of pity, which allows him to completely disregard all mistakes he made and deem himself apparently faultless. And usually when he is harshly criticized by the Winslows, instead of reflecting on their reasoning, he would rather fall into depression (sometimes even pretend to), and simply wait for them to change their minds and apologize for hurting him. Steve appears to possess a martyr complex, which drives his self-delusion that he is always a misunderstood victim, and that reasons for being angry at him are always unfair. Instead of accepting responsibility for his accidents and learning from mistakes, his main priority is largely to stay the same way he always is.		Nonetheless, Steve is kind, bears no hatred for anyone, and is extremely loyal to the Winslow family, whom he is always ready to be there for. He would always be ready to forgive the Winslows when they take advantage of him, behave coldly towards him, harshly reject him and then learn that they hurt his feelings. Steve loves the Winslows like they were his real family, and whether through kindness, persistence or force, he is seen working tirelessly to become as much a part of their lives as a genuine family member.		Despite being poorly conscious of his social life, Steve has often demonstrated good judgment with the law and acute suspicions of law-breaking. Early on, Steve has gotten into some scrapes along with Eddie. This is evident in "Busted" when he takes Eddie to a secret illegal gambling den to help get enough funds to pay for the repairs of Carl's beloved station wagon. Even though Steve won $32,000, he and Eddie got caught by the police who arrested them along with the other gamblers. Steve's integrity with the law became more firm, and was concerned about Eddie a few times when he made poor judgment calls. This was first shown in "Jailhouse Blues" when Steve suspected Clarence had stolen the Porsche and warned Eddie not to go on the joyride, only to watch him get in the car anyway and regret it when the cops put an end to the ride. In "Hot Stuff", he learned about the expensive stereo system that Eddie bought from Weasel's friend and quickly proved it was stolen. In "Money Out The Window", Steve warned Eddie not to gamble again, only to watch him lose himself in debt with Weasel and Waldo. He later warned Eddie not to sell off Carl's prized stamp to pay the debt and confess to his gambling. Once again the latter ignored Urkel and it lead to him telling on Eddie to Carl. And in "Scammed", Urkel suspected the owner of the Ace Lounge being a con-artist and warned his friends not to enter, only to once again watch Eddie proceed anyways and regret it later. In "The Jury" while serving jury duty, Urkel was the only one who believed in the defendant, Mr. Hayes. By using his laptop, he successfully proved that the incriminating security tape was perjured, and revealed the true perpetrator.		Interestingly, Steve does have somewhat of a spiritual side despite his vast knowledge in science, such as his belief in Santa Claus. When Laura asks him how Santa Claus can deliver presents all over the world in just one night, Steve replies, "Well, it's a miracle! You don't analyze miracles—you just believe in them." In season three he joins a church and became a Christian. He tells Estelle that his father asked him how, with his scientific training, he could believe in God when he can't see, hear or touch Him. Steve replied that he can't see, hear or touch an atom either, but he believes it exists. As Estelle points out, "That is what we call faith!" And Steve definitely holds both to his scientific training and spiritual faith in Christ.		By late in the series run, actor Jaleel White was a prominent off-air representative of the show itself, and often talked on behalf of its producers about the show's direction and Urkel's role in it.[9]		Family Matters co-creator Michael Warren named the character after his friend, writer and director Steve Erkel. Due to the show and the character's tremendous popularity during the early 1990s, Erkel encountered difficulties using his own name; he received many prank phone calls from "Laura" asking for "Steve", and businesses found his name to be suspicious. Warren stated that had he known that the character would reappear for years he would not have named him after his friend.[8]		The Urkels are very intelligent people; Steve and his parents, Herb and Diane, were known to do the Sunday crossword puzzles in pen in about 20 minutes. In fact, Steve himself was known for considering his teachers and school officials as equals, calling them by their given names instead of Mr., Mrs., or Ms. He is also fluent in Japanese and Korean, has studied French in high school, and would sometimes speak Japanese to Principal Shimata, Vanderbilt High School's Japanese-American principal.		However, on many occasions, it is said or referenced that Steve's parents have nothing but total contempt for him, and do their best to avoid him at all times. For example, when he was born his father, Dr. Urkel, tried to push him back in (as revealed in season three's "Words Hurt," while Steve was recalling his birth as he was under hypnosis to find out why he had been hitting Carl with a newspaper while sleepwalking), and his parents do not own a car because he was born in one. In the season three episode "Choir Trouble", Steve says that his dad questioned him for going to church with the Winslows and becoming a born-again Christian, implying that his parents are not religious. He also once mentioned that his parents do not feed him every day. This perhaps explains why he spends his time at the Winslows and not at his own home. However, when Steve created his new persona, Stefan, in the season five episode "Dr. Urkel and Mr. Cool," they started to show their love for him and finally introduced him to his grandparents as well as the other relatives. It may be assumed that his parents, although nerds, are very shallow people and prefer Stefan's company rather than Steve's.		The viewers see that Steve has at least four relatives who do care about him. In the season one episode "The Big Fix (a.k.a. Mercy Date)," his uncle Ernie drives him on his date with Laura, and takes a picture of the two. He also has a good relationship with his "Aunt Oona from Altoona" (played by Donna Summer). Oona visits Steve in two episodes (season five's "Aunt Oona" and season eight's "Pound Foolish") and acts as a mother figure to him, in addition to Estelle. He also mentions his Uncle Cecil throughout the series, and implies that Uncle Cecil (who has a police record) cares for Steve. His cousin, Myrtle Urkel, is very close to him and treats him like her brother. Her infatuation with Laura's brother, Eddie, mirrors Steve's love for Laura. However, Eddie never grew to like Myrtle—unlike Laura, who did grow to like Steve (thanks in some part to Myra). In Myrtle's final appearance in season nine's "Don't Make Me Over", she realized that she is acting like a prostitute (due to shopping and a makeover with Laura and Maxine) and decided to end her infatuation with Eddie to pursue some other men.		Other relatives included Myrtle's father, "Big Daddy" (played by Reginald VelJohnson in a dual role), who did not think Eddie was a good choice for his daughter except when bribed with cash. It is mentioned in season two that Big Daddy and the other relatives (excluding Oona, Cecil, Ernie and Myrtle) pay Steve a lot of money not to visit them. However, they tolerated him after he got a makeover and became less clumsy. Steve has a gangster cousin from Detroit named Cornelius Eugene Urkel (also played by White), who went by the moniker "Original Gangsta Dawg"; Steve does not have a good relationship with him and tries to avoid him at all costs. This is because whenever he comes to visit, Cornelius wants money from Urkel and which he refuses to give. Steve also has another cousin named Julie, who is a friend of D.J. Tanner and lives in San Francisco (as seen in the Full House episode "Stephanie Gets Framed," in which Jaleel White guest starred as Steve Urkel). In his first episode, "Laura's First Date", Steve's father is mentioned in the line: "Did I mention my dad knows Wayne Newton?" It was also once mentioned that his uncle Ernie owns a hearse.		In the season six episode "Home Sweet Home," after Steve's father is invited to showcase a microscopic camera for doctors to use to detect brain damage, Steve's parents move to Russia without him as he did not want to leave Chicago and effectively, the Winslows. Steve was then allowed to live with the Winslows; however, in the season seven episode "She's Back," a subplot has Steve visiting his parents.		The Urkel family, along with Steve, have very good luck in gambling; in season two's "Busted," Eddie damaged Carl's car and needed $800 to pay for the damage without Carl noticing, but he could not afford it. Steve offered to help Eddie by using his abilities at an illegal casino. Eddie was glad Steve used his mathematical abilities in this manner, as Urkel managed to win them $32,000, only to have the gambling den be raided by the police. Eddie ends up having to face the music in front of Carl, who before the tirade said not to concern himself with Steve, as he is facing commesurate punishment from Dr. Urkel.		In one episode Steve alludes to Kenyan American descent, describing the Urkel surname as meaning "a benign cyst on the foreleg of a wildebeest" in Kenya.		During the season five episode "Dr. Urkel and Mr. Cool," in a takeoff on the Nutty Professor films, Urkel devised the ultimate plan to win Laura's heart: transforming his DNA using a serum he dubbed "Cool Juice" to suppress his "nerd" genes and bring out his "cool" genes. This resulted in the alter ego known as Stefan Urquelle, played by White in more casual attire and with a smoother delivery. Initially, Laura is enamored with Stefan, but asks that he turn back into Steve towards the end of the episode when Stefan's self-centered, narcissistic traits come out.		Steve later improved the formula in the season five episode "Stefan Returns" to limit the effects it had on his new personality, and renamed the formula "Boss Sauce." He also invented a "transformation chamber," which allowed him to turn into Stefan for extended periods of time (as well as to avoid developing rashes in "personal areas" from direct consumption of the formula). He changed into Stefan several times – even while dating Myra – but some circumstance caused Steve to change back into his regular persona each time. During these sporadic appearances, Stefan's narcissism seemed to improve and Laura fell deeply in love with Steve in this persona. In the season six two-part episode "To Be or Not To Be," Steve ended up stuck as Stefan temporarily, after Myra had tampered with the transformation chamber. Unbeknownst to her, Stefan had invited Carl to use his transformation chamber to cure his farsightedness over insecurity about wearing glasses, since Stefan did not need glasses. Only when he starts the chamber and realizing her mistake, Myra frantically tells Stefan to stop the chamber because she tampered with it and reset the dial to nerd, rather than suave. Stefan attempts to stop it, but the broken transformation chamber ends up creating "Carl Urkel", resulting in Stefan having to help fix the chamber while Myra explains herself to him. However, it took a week for Stefan to fix the chamber because his scientific genius from his time as Steve was in Carl. After fixing the chamber and getting Carl back to normal, Stefan decides to return to being Steve for the time being.		The transformation chamber had other uses for Steve besides turning him into Stefan, but similar unpredictable results, such as when Urkel attempted to fuse his DNA with Albert Einstein, he accidentally got the DNA and personality of Elvis Presley. He used the chamber and DNA to become a martial arts master, Bruce Lee, to deal with bad guys since he knows that Stefan is not enough.		Late in the sixth season in "We're Going to Disney World," Steve transformed into Stefan as part of an inventor's competition at Walt Disney World; however, Laura sabotaged the transformation chamber to prevent him from turning back into Steve. During his extended stint as Stefan, he proposed to Laura in front of Cinderella Castle. Laura accepted, but their engagement was broken off when Myra appealed to Stefan and Laura revealed her sabotage. While just about everyone liked Stefan, Myra disliked him immensely, believing he was a joke and knows that her "Steviekins" was perfect already.		In the seventh-season finale "Send in the Clones," Steve created a cloning machine and wound up cloning himself. Myra was initially excited, but eventually realized that two Steves were just too much. To clear up the situation, Laura proposed that one of the Steves be permanently turned into Stefan, so that she and Myra can both be with the one they love.		The permanent Stefan made several more appearances throughout the series, pursuing a career as a model in season eight's "Paris Vacation," and proposed to Laura again in the ninth season. After weighing her choices in the flashback episode "Pop Goes the Question," Laura chose Steve over Stefan. Stefan departed and was never seen again.		Steve had been in love with Laura since they were in kindergarten. However, she did not always reciprocate the feelings that he had for her and would date other guys more to her taste. Unlike Greta's successful warding off of Myrtle, Laura's would-be boyfriends would often bully Steve, but it always ends with him warding them off, especially when he knows most of them would end up hurting her in the end and refused to allow them to take advantage of her. Steve has shown that, unlike Myrtle, he does respect Laura's personal space. However, she has shown appreciation for Steve when he selflessly risked himself to not only protect her, but also has saved her father's life. In the ninth-season episode "Out With the Old," he finally gets himself a makeover in time for Laura's charity bachelor auction, and, though it was not a success, her feelings for him finally changed, and they eventually become engaged to be married by "Pop Goes the Question" later that season.		Steve had a short and brief relationship with Susie in season two, but their relationship ended when he introduced her to the president of the chess club. In the season three episode "The Love God," Vonda Mahoney also became interested in him after he tutored her for a class. Fearing that she wants to make herself "easy" for Eddie to date, Steve teaches her the dangers of it and helps her have love and self-respect for herself. Although Vonda felt better about herself, this did not sit well with Eddie, who wanted to pound Steve for it.		In season four's "A Thought In The Dark", Steve was set up with Laura's boyfriend Ted's cousin, Myra Monkhouse (played by Michelle Thomas) so Laura could have some space from his infatuation. Although he was initially attracted to Myra because of her intelligence and sweet personality, he was soon disgusted by her jealous and possessive nature by the end of "Buds 'N Buns". This was explored in both seasons five and nine, when Steve finally saw what life was like in Laura's shoes when he pursued her relentlessly. Though in love with him, Myra was also frustrated with Steve and asked him why he even loves Laura. When he agrees to go steady with her in season six's "Paradise Bluff", she did somewhat revert to the sweet, intelligent girl he was originally attracted to. By season nine, he grows apart from her as he matures and Laura started to return the feelings he had for her. In "Breaking Up Is Hard To Do," during their date at the restaurant Amour, it became clear to Steve that Myra hated his makeover and demanded that he return to the sexy nerd she loved so much. When he refused, she broke up with him first, although it later turned out to be a ploy so she could get him to renounce his love for Laura and stay with her. When that failed, Myra resorted to stalking and spying on him with illegal surveillance gear in her room with a spy cam attached to his glasses. In "Crazy For You," she teamed up with Stefan to stop Steve and Laura's date. Although initially successful, Laura still wanted to date him and Myra vowed never to stop stalking him until he took her back. In the following episode "Crazier For You," Steve has learned that Myra was spying on him and confronted her about it. He orders her to stop stalking him and renounce her love for him because he will never take her back and reaffirms his love for Laura. She refused and got Laura falsely arrested for stealing her watch. However, Steve was able to drop the charges when he exposed Myra for stalking him with illegal spy gear and the police pursued criminal charges against her. When she learned of his engagement to Laura, she continued to carry the torch even when he went into space.		In syndication, Steve is incorporated into the teaser scene of "Rachel's First Date"; his first appearance in the original broadcasts is in the 1989 episode "Laura's First Date", in which Carl and Eddie separately set up dates for Laura for a dance or party (both terms are used in the episode), and the first thing known about him is that he allegedly ate a mouse, and he later makes reference to a mouse when speaking to Carl, implying that it might be true. Prior to Steve Urkel's introduction, the show was on the brink of cancellation due to mediocre ratings. After Urkel was introduced, several scripts had to be hastily rewritten to accommodate the character, while several first-season episodes that had been completed had new opening gag sequences filmed featuring Steve trying to push open the Winslows' front door while the family holds it shut. The addition of Steve immediately helped the show boost its modest ratings. White was credited as a guest star in the first season and became a regular member of the cast in season two.		The Urkel Dance was a novelty dance that originated in the season two episode "Life of the Party". It was based around the character of Steve Urkel and essentially incorporated movements which made the dancer's posture more like his. The lyrics instructed the dancer how to pose:		"If you want to do the Steve Urkel dance, All you have to do is hitch up your pants, Bend your knees, and stick out your pelvis; (I'm telling you, baby, it's better than Elvis!)".		The dance was popular enough to appear on another show, Step by Step, when the Steve Urkel character appeared in a crossover in the season one episode "The Dance".		Jaleel White also performed the song, in character as Steve Urkel, on the 5th Annual American Comedy Awards. Bea Arthur (from Maude & The Golden Girls) joined him on stage to "Do The Urkel,[10]" after which she said, "Hey MC Hammer, try and touch that!"[11]		A promotional cassette single of the song that accompanies the dance was pressed and distributed in limited numbers. A T-shirt was also produced featuring lyrics and Urkel's likeness.		In 2010, Westside Middle School in Memphis, Tennessee outlined its dress code policy on sagging pants, asking students to pull them up or get "Urkeled", a reference to the character. In this practice, teachers would forcibly pull students' pants up and attach them to their waist using zip ties. Students would also have their photo taken and posted on a board in the hallway, for all of their classmates to view. In an interview with NBC affiliate WMC-TV, Principal Bobby White stated that the general idea is to fight pop culture with pop culture.[12] One teacher at the school claimed to have "Urkeled" up to 80 students per week, although after five weeks the number dropped to 18.[13]		At the height of his popularity, Urkel's name was branded to several products including a short-lived fruit flavored cereal known as Urkel-Os and a Steve Urkel pullstring doll. There was also a T-shirt line that was created in 2002, but was discontinued shortly after its inception.[14]		In 1999, TV Guide ranked Urkel #27 on its list of the "50 Greatest TV Characters of All Time".[15] In 2004, he was listed at #98 in Bravo's 100 Greatest TV Characters.[16]		
Dental braces (also known as braces, orthodontic cases, or cases) are devices used in orthodontics that align and straighten teeth and help position them with regard to a person's bite, while also aiming to improve dental health. They are often used to correct underbites, as well as malocclusions, overbites, open bites, deep bites, cross bites, crooked teeth, and various other flaws of the teeth and jaw. Braces can be either cosmetic or structural. Dental braces are often used in conjunction with other orthodontic appliances to help widen the palate or jaws and to otherwise assist in shaping the teeth and jaws.						According to scholars and historians, braces date back to ancient times. Around 400-300 BCE, Hippocrates and Aristotle contemplated ways to straighten teeth and fix various dental conditions. Archaeologists have discovered numerous mummified ancient individuals with what appear to be metal bands wrapped around their teeth. Catgut, a type of cord made from the natural fibers of an animal's intestines, performed a similar role to today’s orthodontic wire in closing gaps in the teeth and mouth.[1] The Etruscans buried their dead with dental appliances in place to maintain space and prevent collapse of the teeth during the afterlife. A Roman tomb was found with a number of teeth bound with gold wire documented as a ligature wire, a small elastic wire that is used to affix the arch wire to the bracket. Even Cleopatra wore a pair. Roman philosopher and physician Aulus Cornelius Celsus first recorded the treatment of teeth by finger pressure.[1] Unfortunately, due to lack of evidence, poor preservation of bodies, and primitive technology, little research was carried out on dental braces until around the 17th century, although dentistry was making great advancements as a profession by then.		Orthodontics truly began developing in the 18th and 19th centuries. In 1728, French dentist Pierre Fauchard, who is often credited with inventing modern orthodontics, published a book entitled "The Surgeon Dentist" on methods of straightening teeth. Fauchard, in his practice, used a device called a "Bandeau", a horseshoe-shaped piece of iron that helped expand the arch. In 1754, another French dentist, Louis Bourdet, dentist to the King of France, followed Fauchard's book with The Dentist's Art, which also dedicated a chapter to tooth alignment and application. He perfected the "Bandeau" and was the first dentist on record to recommend extraction of the premolar teeth to alleviate crowding and to improve jaw growth.		Although teeth and palate straightening and/or pulling was used to improve alignment of remaining teeth and had been practiced since early times, orthodontics, as a science of its own, did not really exist until the mid-19th century. Several important dentists helped to advance dental braces with specific instruments and tools that allowed braces to be improved.		In 1819, Delabarre introduced the wire crib, which marked the birth of contemporary orthodontics, and gum elastics were first employed by Maynard in 1843. Tucker was the first to cut rubber bands from rubber tubing in 1850. Dentist, writer, artist, and sculptor Norman William Kingsley in 1858 wrote the first article on orthodontics and in 1880, his book, Treatise on Oral Deformities, was published. A dentist named John Nutting Farrar is credited for writing two volumes entitled, A Treatise on the Irregularities of the Teeth and Their Corrections and was the first to suggest the use of mild force at timed intervals to move teeth.		In the early 20th century, Edward Angle devised the first simple classification system for malocclusions, such as Class I, Class II, and so on. His classification system is still used today as a way for dentists to describe how crooked teeth are, what way teeth are pointing, and how teeth fit together. Angle contributed greatly to the design of orthodontic and dental appliances, making many simplifications. He founded the first school and college of orthodontics, organized the American Society of Orthodontia in 1901 which became the American Association of Orthodontists (AAO) in the 1930s, and founded the first orthodontic journal in 1907. Other innovations in orthodontics in the late 19th and early 20th centuries included the first textbook on orthodontics for children, published by J.J. Guilford in 1889, and the use of rubber elastics, pioneered by Calvin S. Case, along with Henry Albert Baker.		The application of braces moves the teeth as a result of force and pressure on the teeth. There are traditionally four basic elements that are used: brackets, bonding material, arch wire, and ligature elastic (also called an “O-ring”). The teeth move when the arch wire puts pressure on the brackets and teeth. Sometimes springs or rubber bands are used to put more force in a specific direction.[2]		Braces have constant pressure which, over time, move teeth into the desired positions. The process loosens the tooth after which new bone grows in to support the tooth in its new position. This is called bone remodeling. Bone remodeling is a biomechanical process responsible for making bones stronger in response to sustained load-bearing activity and weaker in the absence of carrying a load. Bones are made of cells called osteoclasts and osteoblasts. Two different kinds of bone resorption are possible: direct resorption, which starts from the lining cells of the alveolar bone, and indirect or retrograde resorption, which takes place when the periodontal ligament has been subjected to an excessive amount and duration of compressive stress.[2][3] Another important factor associated with tooth movement is bone deposition. Bone deposition occurs in the distracted periodontal ligament. Without bone deposition, the tooth will loosen and voids will occur distal to the direction of tooth movement.[4]		When braces put pressure on teeth, the periodontal membrane stretches on one side and is compressed on the other. If this movement is not done slowly then the patient risks losing his or her teeth. This is why braces are commonly worn for a year or more and adjustments are only made every few weeks. A tooth will usually move about a millimeter per month during orthodontic movement, but there is high individual variability. Orthodontic mechanics can vary in efficiency, which partly explains the wide range of response to orthodontic treatment.		Traditional braces consist of a small bracket that is glued to the front of each tooth and the molars are adjusted with a band that encircles the tooth. An advantage is one can eat and drink while wearing the braces, but a disadvantage is that one must give up certain foods and eating habits while wearing them, such as gum with sugar and potato chips. Another disadvantage is they have to be periodically tightened by an orthodontist, causing increased amounts of discomfort.		Orthodontic services may be provided by any licensed dentist trained in orthodontics. In North America most orthodontic treatment is done by orthodontists, dentists in diagnosis and treatment of malocclusions—malalignments of the teeth, jaws, or both. A dentist must complete 2–3 years of additional post-doctoral training to earn a specialty certificate in orthodontics. There are many general practitioners who also provide orthodontic services.		The first step is to determine whether braces are suitable for the patient. The doctor consults with the patient and inspects the teeth visually. If braces are appropriate, a records appointment is set up where X-rays, molds, and impressions are made. These records are analyzed to determine the problems and proper course of action. The use of digital models is rapidly increasing in the orthodontic industry. Digital treatment starts with the creation of a three-dimensional digital model of the patient's arches. This model is produced by laser-scanning plaster models created using dental impressions. Computer-automated treatment simulation has the ability to automatically separate the gums and teeth from one another and can handle malocclusions well; this software enables clinicians to ensure, in a virtual setting, that the selected treatment will produce the optimal outcome, with minimal user input.[9]		Typical treatment times vary from six months to two and a half years depending on the complexity and types of problems. Orthognathic surgery may be required in extreme cases. About 2 weeks before the braces are applied, orthodontic spacers may be required to spread apart back teeth in order to create enough space for the bands.		Teeth to be braced will have an adhesive applied to help the cement bond to the surface of the tooth. In most cases the teeth will be banded and then brackets will be added. A bracket will be applied with dental cement, and then cured with light until hardened. This process usually takes a few seconds per tooth. If required, orthodontic spacers may be inserted between the molars to make room for molar bands to be placed at a later date. Molar bands are required to ensure brackets will stick. Bands are also utilized when dental fillings or other dental work make securing a bracket to a tooth infeasible.		An archwire will be threaded between the brackets and affixed with elastic or metal ligatures. Ligatures are available in a wide variety of colors, and the patient can choose which color they like. Archwires are bent, shaped, and tightened frequently to achieve the desired results.		Modern orthodontics makes frequent use of nickel-titanium archwires and temperature-sensitive materials. When cold, the archwire is limp and flexible, easily threaded between brackets of any configuration. Once heated to body temperature, the archwire will stiffen and seek to retain its shape, creating constant light force on the teeth.		Brackets with hooks can be placed, or hooks can be created and affixed to the archwire to affix rubber bands to. The placement and configuration of the rubber bands will depend on the course of treatment and the individual patient. Rubber bands are made in different diameters, colors, sizes, and strengths. They are also typically available in two versions: colored or clear/opaque.		The fitting process can vary between different types of braces, though there are similarities such as the initial steps of molding the teeth before application. For example, with clear braces, impressions of a patient's teeth are evaluated to create a series of trays, which fit to the patient's mouth almost like a protective mouthpiece. With some forms of braces, the brackets are placed in a special form that are customized to the patient's mouth, drastically reducing the application time.		In many cases there is insufficient space in the mouth for all the teeth to fit properly. There are two main procedures to make room in these cases. One is extraction: teeth are removed to create more space. The second is expansion, in which the palate or arch is made larger by using a palatal expander. Expanders can be used with both children and adults. Since the bones of adults are already fused, expanding the palate is not possible without surgery to unfuse them. An expander can be used on an adult without surgery, but would be used to expand the dental arch, and not the palate.		Sometimes children and teenage patients, and occasionally adults, are required to wear a headgear appliance as part of the primary treatment phase to keep certain teeth from moving (for more detail on headgear and facemask appliances see Orthodontic headgear). When braces put pressure on one's teeth, the periodontal membrane stretches on one side and is compressed on the other. This movement needs to be done slowly or otherwise the patient risks losing his or her teeth. This is why braces are worn as long as they are and adjustments are only made every so often.		Braces are typically adjusted every three to six weeks. This helps shift the teeth into the correct position. When they get adjusted, the orthodontist removes the colored or metal ligatures keeping the archwire in place. The archwire is then removed, and may be replaced or modified. When the archwire has been placed back into the mouth, the patient may choose a color for the new elastic ligatures, which are then affixed to the metal brackets. The adjusting process may cause some discomfort to the patient, which is normal.		Patients may need post-orthodontic surgery, such as a fiberotomy or alternatively a gum lift, to prepare their teeth for retainer use and improve the gumline contours after the braces come off.		In order to prevent the teeth from moving back to their original position, retainers are worn once the treatment is complete. Retainers help in maintaining and stabilizing the position of teeth long enough to permit reorganization of the supporting structures after the active phase of orthodontic therapy. If the patient does not wear the retainer appropriately and/or for the right amount of time, the teeth may move towards their previous position. For regular braces, Hawley retainers are used. They are made of metal hooks that surround the teeth and are enclosed by an acrylic plate shaped to fit the patient's palate. For invisalign braces, an Essix retainer is used. This is similar to the regular invisalign braces; it is a clear plastic tray that is firmly fitted to the teeth, which stays in place without a plate fitted to the palate. There is also a bonded retainer where a wire is permanently bonded to the lingual side of the teeth, usually the lower teeth only. Doctors will sometimes refuse to remove this retainer, and a special orthodontic appointment to have it removed may be required.		Headgear needs to be worn between 12–22 hours each day to be effective in correcting the overbite, typically for 12 to 18 months depending on the severity of the overbite, how much it is worn and what growth stage the patient is in. Typically the prescribed daily wear time will be between 14 and 16 hours a day and is frequently used post primary treatment phase to maintain the position of the jaw and arch.		Orthodontic headgear will usually consist of three major components:		The headgear application is one of the most useful appliances available to the orthodontist when looking to correct a Class II malocclusion. See more details in the section Orthodontic headgear.		The pre-finisher is molded to the patient's teeth by use of extreme pressure to the appliance by the person's jaw. The product is then worn a certain amount of time with the user applying force to the appliance in their mouth for 10 to 15 seconds at a time. The goal of the process is to increase the exercise time in applying the force to the appliance. If a person's teeth are not ready for a proper retainer the orthodontist may prescribe the use of a preformed finishing appliance such as the pre-finisher. This appliance fixes gaps between the teeth, small spaces between the upper and lower jaw, and other minor problems.		Experiencing some pain following fitting and activation of fixed orthodontic braces is very common and several methods have been suggested to tackle this.[10][11]		Plaque forms easily when food is retained in and around braces. It is important to maintain proper oral hygiene by brushing thoroughly when wearing braces to prevent tooth decay, decalcification, or unpleasant color changes to the teeth. Interdental brushing may be recommended as an alternative to flossing as the latter may be difficult or unrecommended with braces. Regular use of mouthwash is also recommended.		There is a small chance of allergic reaction to the elastics or to the metal used in braces. In even rarer cases, latex allergy may result in anaphylaxis. Latex-free elastics and alternative metals can be used instead. It is important for those who believe that they are allergic to their braces to notify the orthodontist immediately.		Mouth sores may be triggered by irritation from components of the braces. Many products can increase comfort, including oral rinses, dental wax or dental silicone, and products to help heal sores.		Braces can also be damaged if proper care is not taken. It is important to wear a mouth guard to prevent breakage and/or mouth injury when playing sports. Certain sticky or hard foods such as taffy, raw carrots, hard pretzels, and toffee should be avoided because they can damage braces. Frequent damage to braces can prolong treatment. Some orthodontists recommend sugar-free chewing gum in the belief that it may expedite treatment and relieve soreness; other orthodontists object to gum chewing not because it is sticky, or that it breaks the brackets off, but chewing gum bends the wire.[12]		In the course of treatment orthodontic brackets may pop off due to the forces involved, or due to cement weakening over time. The orthodontist should be contacted immediately for advice if this occurs. In most cases the bracket is replaced.		When teeth move, the end of the arch wire may become displaced, causing it to poke the back of the patient's cheek. Dental wax can be applied to cushion the protruding wire. The orthodontist must be called immediately to have it clipped, or a painful mouth ulcer may form. If the wire is causing severe pain, it may be necessary to carefully bend the edge of the wire in with a spoon or other piece of equipment (e.g. tweezers, clean eraser side of a pencil) until the wire can be clipped by an orthodontist.		Patients with periodontal disease usually must obtain periodontal treatment before getting braces. A deep cleaning is performed, and further treatment may be required before beginning orthodontic treatment. Bone loss due to periodontal disease may lead to tooth loss during treatment.		In some cases, teeth may be loose for a prolonged period of time. One may be able to wiggle one's teeth for a year or two after treatment or longer.		The dental displacement obtained with the orthodontic appliance determines in most cases some degree of root resorption. Only in a few cases is this side effect large enough to be considered real clinical damage to the tooth. In rare cases, the teeth may fall out or have to be extracted due to root resorption.[13][14]		Pain and discomfort are common after adjustment and may cause difficulty eating for a time, often a couple of days. During this period, eating soft foods can help avoid additional pressure on teeth.		Removal of the cemented brackets can also be painful. The cement must be chipped and scraped off which can cause severe pain in patients with sensitive teeth. Often molar bands have been installed for an extended period of time and they may be embedded in the gums at the time of removal.		The metallic look may not be desirable to some people, although transparent or lingual (behind-teeth) varieties are available. According to a survey published in the American Journal of Orthodontics and Dentofacial Orthopedics, dental braces with no visible metal were considered the most attractive. Ceramic braces with thin metal or clear wires were a less desirable option, and braces with metal brackets and metal wires were rated as the least aesthetic combination.[15]		Changes in the shape of the face, jaw, and cheekbones may also occur as a result of wearing braces. Some patients may wish to discuss these potential changes before starting treatment.		Media related to Dental braces at Wikimedia Commons		
The Oxford English Dictionary (OED) is a descriptive dictionary of the English language, published by the Oxford University Press.[1] It traces the historical development of the English language, providing a comprehensive resource to scholars and academic researchers, as well as describing usage in its many variations throughout the world.[2][3] The second edition came to 21,728 pages in 20 volumes, published in 1989.		Work began on the dictionary in 1857, but it was not until 1884 that it began to be published in unbound fascicles as work continued on the project, under the name of A New English Dictionary on Historical Principles; Founded Mainly on the Materials Collected by The Philological Society. In 1895, the title The Oxford English Dictionary (OED) was first used unofficially on the covers of the series, and in 1928 the full dictionary was republished in ten bound volumes. In 1933, the title The Oxford English Dictionary fully replaced the former name in all occurrences in its reprinting as twelve volumes with a one-volume supplement. More supplements came over the years until 1989, when the second edition was published. Since 2000, a third edition of the dictionary has been underway, approximately a third of which is now complete.[citation needed]		The first electronic version of the dictionary was made available in 1988. The online version has been available since 2000, and as of April 2014 was receiving over two million hits per month. The third edition of the dictionary will probably only appear in electronic form; Nigel Portwood, chief executive of Oxford University Press, thinks it unlikely that it will ever be printed.[4][5]						As a historical dictionary, the Oxford English Dictionary explains words by showing their development rather than merely their present-day usages.[6] Therefore, it shows definitions in the order that the sense of the word began being used, including word meanings which are no longer used. Each definition is shown with numerous short usage quotations; in each case, the first quotation shows the first recorded instance of the word that the editors are aware of and, in the case of words and senses no longer in current usage, the last quotation is the last known recorded usage. This allows the reader to get an approximate sense of the time period in which a particular word has been in use, and additional quotations help the reader to ascertain information about how the word is used in context, beyond any explanation that the dictionary editors can provide.		The format of the OED's entries has influenced numerous other historical lexicography projects. The forerunners to the OED, such as the early volumes of the Deutsches Wörterbuch, had initially provided few quotations from a limited number of sources, whereas the OED editors preferred larger groups of quite short quotations from a wide selection of authors and publications. This influenced later volumes of this and other lexicographical works.[7]		According to the publishers, it would take a single person 120 years to "key in" the 59 million words of the OED second edition, 60 years to proofread them, and 540 megabytes to store them electronically.[8] As of 30 November 2005, the Oxford English Dictionary contained approximately 301,100 main entries. Supplementing the entry headwords, there are 157,000 bold-type combinations and derivatives;[9] 169,000 italicized-bold phrases and combinations;[10] 616,500 word-forms in total, including 137,000 pronunciations; 249,300 etymologies; 577,000 cross-references; and 2,412,400 usage quotations. The dictionary's latest, complete print edition (second edition, 1989) was printed in 20 volumes, comprising 291,500 entries in 21,730 pages. The longest entry in the OED2 was for the verb set, which required 60,000 words to describe some 430 senses. As entries began to be revised for the OED3 in sequence starting from M, the longest entry became make in 2000, then put in 2007, then run in 2011.[11][12][13]		Despite its impressive size, the OED is neither the world's largest nor the earliest exhaustive dictionary of a language. Another earlier large dictionary is the Grimm brothers' dictionary of the German language, begun in 1838 and completed in 1961. The first edition of the Vocabolario degli Accademici della Crusca is the first great dictionary devoted to a modern European language (Italian) and was published in 1612; the first edition of Dictionnaire de l'Académie française dates from 1694. The official dictionary of Spanish is the Diccionario de la lengua española (produced, edited, and published by the Real Academia Española), and its first edition was published in 1780. The Kangxi dictionary of Chinese was published in 1716.[14]		The dictionary began as a Philological Society project of a small group of intellectuals in London (and unconnected to Oxford University):[15]:103–4,112 Richard Chenevix Trench, Herbert Coleridge, and Frederick Furnivall, who were dissatisfied with the existing English dictionaries. The Society expressed interest in compiling a new dictionary as early as 1844,[16] but it was not until June 1857 that they began by forming an "Unregistered Words Committee" to search for words that were unlisted or poorly defined in current dictionaries. In November, Trench's report was not a list of unregistered words; instead, it was the study On Some Deficiencies in our English Dictionaries, which identified seven distinct shortcomings in contemporary dictionaries:[17]		The Society ultimately realized that the number of unlisted words would be far more than the number of words in the English dictionaries of the 19th century, and shifted their idea from covering only words that were not already in English dictionaries to a larger project. Trench suggested that a new, truly comprehensive dictionary was needed. On 7 January 1858, the Society formally adopted the idea of a comprehensive new dictionary.[15]:107–8 Volunteer readers would be assigned particular books, copying passages illustrating word usage onto quotation slips. Later the same year, the Society agreed to the project in principle, with the title A New English Dictionary on Historical Principles (NED).[18]:ix–x		Richard Chenevix Trench (1807–1886) played the key role in the project's first months, but his Church of England appointment as Dean of Westminster meant that he could not give the dictionary project the time that it required. He withdrew and Herbert Coleridge became the first editor.[19]:8–9		On 12 May 1860, Coleridge's dictionary plan was published and research was started. His house was the first editorial office. He arrayed 100,000 quotation slips in a 54 pigeon-hole grid.[19]:9 In April 1861, the group published the first sample pages; later that month, Coleridge died of tuberculosis, aged 30.[18]:x		Furnivall then became editor; he was enthusiastic and knowledgeable, yet temperamentally ill-suited for the work.[15]:110 Many volunteer readers eventually lost interest in the project, as Furnivall failed to keep them motivated. Furthermore, many of the slips had been misplaced.		Furnivall believed that, since many printed texts from earlier centuries were not readily available, it would be impossible for volunteers to efficiently locate the quotations that the dictionary needed. As a result, he founded the Early English Text Society in 1864 and the Chaucer Society in 1868 to publish old manuscripts.[18]:xii Furnivall's preparatory efforts lasted 21 years and provided numerous texts for the use and enjoyment of the general public, as well as crucial sources for lexicographers, but they did not actually involve compiling a dictionary. Furnivall recruited more than 800 volunteers to read these texts and record quotations. While enthusiastic, the volunteers were not well trained and often made inconsistent and arbitrary selections. Ultimately, Furnivall handed over nearly two tons of quotation slips and other materials to his successor.[20]		In the 1870s, Furnivall unsuccessfully attempted to recruit both Henry Sweet and Henry Nicol to succeed him. He then approached James Murray, who accepted the post of editor. In the late 1870s, Furnivall and Murray met with several publishers about publishing the dictionary. In 1878, Oxford University Press agreed with Murray to proceed with the massive project; the agreement was formalized the following year.[15]:111–2 The dictionary project finally had a publisher 20 years after the idea was conceived. It was another 50 years before the entire dictionary was complete.		Late in his editorship, Murray learned that a prolific reader named W. C. Minor was a criminal lunatic.[15]:xiii Minor was a Yale University-trained surgeon and military officer in the American Civil War, and was confined to Broadmoor Asylum for the Criminally Insane after killing a man in London. Minor invented his own quotation-tracking system, allowing him to submit slips on specific words in response to editors' requests. The story of Murray and Minor later served as the central focus of The Surgeon of Crowthorne (US title: The Professor and the Madman[15]), a popular book about the creation of the OED.		During the 1870s, the Philological Society was concerned with the process of publishing a dictionary with such an immense scope. They had pages printed by publishers, but no publication agreement was reached; both the Cambridge University Press and the Oxford University Press were approached. The OUP finally agreed in 1879 (after two years of negotiating by Sweet, Furnivall, and Murray) to publish the dictionary and to pay Murray, who was both the editor and the Philological Society president. The dictionary was to be published as interval fascicles, with the final form in four 6,400-page volumes. They hoped to finish the project in ten years.[19]:1		Murray started the project, working in a corrugated iron outbuilding called the "Scriptorium" which was lined with wooden planks, book shelves, and 1,029 pigeon-holes for the quotation slips.[18]:xiii He tracked and regathered Furnivall's collection of quotation slips, which were found to concentrate on rare, interesting words rather than common usages. For instance, there were ten times as many quotations for abusion as for abuse.[21] He appealed, through newspapers distributed to bookshops and libraries, for readers who would report "as many quotations as you can for ordinary words" and for words that were "rare, obsolete, old-fashioned, new, peculiar or used in a peculiar way".[21] Murray had American philologist and liberal arts college professor Francis March manage the collection in North America; 1,000 quotation slips arrived daily to the Scriptorium and, by 1880, there were 2,500,000.[19]:15		The first dictionary fascicle was published on 1 February 1884—twenty-three years after Coleridge's sample pages. The full title was A New English Dictionary on Historical Principles; Founded Mainly on the Materials Collected by The Philological Society; the 352-page volume, words from A to Ant, cost 12s 6d.[19]:251 The total sales were a disappointing 4,000 copies.[22]:169		The OUP saw that it would take too long to complete the work with unrevised editorial arrangements. Accordingly, new assistants were hired and two new demands were made on Murray.[19]:32–33 The first was that he move from Mill Hill to Oxford, which he did in 1885. Murray had his Scriptorium re-erected on his new property.[18]:xvii[19]		Murray resisted the second demand: that if he could not meet schedule, he must hire a second, senior editor to work in parallel to him, outside his supervision, on words from elsewhere in the alphabet. Murray did not want to share the work, feeling that he would accelerate his work pace with experience. That turned out not to be so, and Philip Gell of the OUP forced the promotion of Murray's assistant Henry Bradley (hired by Murray in 1884), who worked independently in the British Museum in London beginning in 1888. In 1896, Bradley moved to Oxford University.[19]		Gell continued harassing Murray and Bradley with his business concerns—containing costs and speeding production—to the point where the project's collapse seemed likely. Newspapers reported the harassment, particularly the Saturday Review, and public opinion backed the editors.[22]:182–83 Gell was fired, and the university reversed his cost policies. If the editors felt that the dictionary would have to grow larger, it would; it was an important work, and worth the time and money to properly finish.		Neither Murray nor Bradley lived to see it. Murray died in 1915, having been responsible for words starting with A–D, H–K, O–P, and T, nearly half the finished dictionary; Bradley died in 1923, having completed E–G, L–M, S–Sh, St, and W–We. By then, two additional editors had been promoted from assistant work to independent work, continuing without much trouble. William Craigie started in 1901 and was responsible for N, Q–R, Si–Sq, U–V, and Wo–Wy.[18]:xix The OUP had previously thought London too far from Oxford but, after 1925, Craigie worked on the dictionary in Chicago, where he was a professor.[18]:xix[19] The fourth editor was Charles Talbut Onions, who compiled the remaining ranges starting in 1914: Su–Sz, Wh–Wo, and X–Z.[23]		In 1919–1920, J. R. R. Tolkien was employed by the OED, researching etymologies of the Waggle to Warlock range;[24] later he parodied the principal editors as "The Four Wise Clerks of Oxenford" in the story Farmer Giles of Ham.[25]		By early 1894, a total of 11 fascicles had been published, or about one per year: four for A–B, five for C, and two for E.[18] Of these, eight were 352 pages long, while the last one in each group was shorter to end at the letter break (which eventually became a volume break). At this point, it was decided to publish the work in smaller and more frequent instalments; once every three months beginning in 1895 there would be a fascicle of 64 pages, priced at 2s 6d. If enough material was ready, 128 or even 192 pages would be published together. This pace was maintained until World War I forced reductions in staff.[18]:xx Each time enough consecutive pages were available, the same material was also published in the original larger fascicles.[18]:xx Also in 1895, the title Oxford English Dictionary (OED) was first used. It then appeared only on the outer covers of the fascicles; the original title was still the official one and was used everywhere else.[18]:xx		The 125th and last fascicle covered words from Wise to the end of W and was published on 19 April 1928, and the full dictionary in bound volumes followed immediately.[18]:xx		William Shakespeare is the most-quoted writer in the completed dictionary, with Hamlet his most-quoted work. George Eliot (Mary Ann Evans) is the most-quoted female writer. Collectively, the Bible is the most-quoted work (but in many different translations); the most-quoted single work is Cursor Mundi.[8]		Between 1928 and 1933, enough additional material had been compiled to make a one-volume supplement, so the dictionary was reissued as the set of 12 volumes and a one-volume supplement in 1933.[18]		In 1933, Oxford had finally put the dictionary to rest; all work ended, and the quotation slips went into storage. However, the English language continued to change and, by the time 20 years had passed, the dictionary was outdated.[26]		There were three possible ways to update it. The cheapest would have been to leave the existing work alone and simply compile a new supplement of perhaps one or two volumes; but then anyone looking for a word or sense and unsure of its age would have to look in three different places. The most convenient choice for the user would have been for the entire dictionary to be re-edited and retypeset, with each change included in its proper alphabetical place; but this would have been the most expensive option, with perhaps 15 volumes required to be produced. The OUP chose a middle approach: combining the new material with the existing supplement to form a larger replacement supplement.		Robert Burchfield was hired in 1957 to edit the second supplement;[27] Onions turned 84 that year but was still able to make some contributions, as well. The work on the supplement was expected to take about seven years.[26] It actually took 29 years, by which time the new supplement (OEDS) had grown to four volumes, starting with A, H, O, and Sea. They were published in 1972, 1976, 1982, and 1986 respectively, bringing the complete dictionary to 16 volumes, or 17 counting the first supplement.		Burchfield emphasized the inclusion of modern-day language and, through the supplement, the dictionary was expanded to include a wealth of new words from the burgeoning fields of science and technology, as well as popular culture and colloquial speech. Burchfield said that he broadened the scope to include developments of the language in English-speaking regions beyond the United Kingdom, including North America, Australia, New Zealand, South Africa, India, Pakistan, and the Caribbean. Burchfield also removed some smaller entries that had been added to the 1933 supplement, for reasons of space;[28] in 2012, an analysis by lexicographer Sarah Ogilvie revealed that many of these entries were in fact foreign loanwords, despite Burchfield's attempt to include more such words. The proportion was estimated from a sample calculation to amount to 17% of the foreign loan words and words from regional forms of English. Many of these had only a single recorded usage, but it ran against what was thought to be the established OED editorial practice and a perception that he had opened up the dictionary to "World English".[29][30][31]		By the time the new supplement was completed, it was clear that the full text of the dictionary would now need to be computerized. Achieving this would require retyping it once, but thereafter it would always be accessible for computer searching – as well as for whatever new editions of the dictionary might be desired, starting with an integration of the supplementary volumes and the main text. Preparation for this process began in 1983, and editorial work started the following year under the administrative direction of Timothy J. Benbow, with John A. Simpson and Edmund S. C. Weiner as co-editors.[32] In 2016, Simpson published his memoir chronicling his years at the OED. See The Word Detective: Searching for the Meaning of It All at the Oxford English Dictionary - A Memoir. Basic Books, New York.		And so the New Oxford English Dictionary (NOED) project began. In the United States, more than 120 typists of the International Computaprint Corporation (now Reed Tech) started keying in over 350,000,000 characters, their work checked by 55 proof-readers in England.[32] Retyping the text alone was not sufficient; all the information represented by the complex typography of the original dictionary had to be retained, which was done by marking up the content in SGML.[32] A specialized search engine and display software were also needed to access it. Under a 1985 agreement, some of this software work was done at the University of Waterloo, Canada, at the Centre for the New Oxford English Dictionary, led by Frank Tompa and Gaston Gonnet; this search technology went on to become the basis for the Open Text Corporation.[33] Computer hardware, database and other software, development managers, and programmers for the project were donated by the British subsidiary of IBM; the colour syntax-directed editor for the project, LEXX, was written by Mike Cowlishaw of IBM.[34] The University of Waterloo, in Canada, volunteered to design the database. A. Walton Litz, an English professor at Princeton University who served on the Oxford University Press advisory council, was quoted in Time as saying "I've never been associated with a project, I've never even heard of a project, that was so incredibly complicated and that met every deadline."[35]		By 1989, the NOED project had achieved its primary goals, and the editors, working online, had successfully combined the original text, Burchfield's supplement, and a small amount of newer material, into a single unified dictionary. The word "new" was again dropped from the name, and the second edition of the OED, or the OED2, was published. The first edition retronymically became the OED1.		The Oxford English Dictionary 2 was printed in 20 volumes. For the first time, there was no attempt to start them on letter boundaries, and they were made roughly equal in size. The 20 volumes started with A, B.B.C., Cham, Creel, Dvandva, Follow, Hat, Interval, Look, Moul, Ow, Poise, Quemadero, Rob, Ser, Soot, Su, Thru, Unemancipated, and Wave.		The content of the OED2 is mostly just a reorganization of the earlier corpus, but the retypesetting provided an opportunity for two long-needed format changes. The headword of each entry was no longer capitalized, allowing the user to readily see those words that actually require a capital letter.[36] Murray had devised his own notation for pronunciation, there being no standard available at the time, whereas the OED2 adopted the modern International Phonetic Alphabet.[36][37] Unlike the earlier edition, all foreign alphabets except Greek were transliterated.[36]		The British quiz show Countdown has awarded the leather-bound complete version to the champions of each series since its inception in 1982.[38]		When the print version of the second edition was published in 1989, the response was enthusiastic. Author Anthony Burgess declared it "the greatest publishing event of the century", as quoted by the Los Angeles Times.[39] Time dubbed the book "a scholarly Everest",[35] and Richard Boston, writing for The Guardian, called it "one of the wonders of the world".[40]		The supplements and their integration into the second edition were a great improvement to the OED as a whole, but it was recognized that most of the entries were still fundamentally unaltered from the first edition. Much of the information in the dictionary published in 1989 was already decades out of date, though the supplements had made good progress towards incorporating new vocabulary. Yet many definitions contained disproven scientific theories, outdated historical information, and moral values that were no longer widely accepted.[41][42] Furthermore, the supplements had failed to recognize many words in the existing volumes as obsolete by the time of the second edition's publication, meaning that thousands of words were marked as current despite no recent evidence of their use.[43]		Accordingly, it was recognized that work on a third edition would have to begin to rectify these problems.[41] The first attempt to produce a new edition came with the Oxford English Dictionary Additions Series, a new set of supplements to complement the OED2 with the intention of producing a third edition from them.[44] The previous supplements appeared in alphabetical installments, whereas the new series had a full A–Z range of entries within each individual volume, with a complete alphabetical index at the end of all words revised so far, each listed with the volume number which contained the revised entry.[44]		However, in the end only three Additions volumes were published this way, two in 1993 and one in 1997,[45][46][47] each containing about 3,000 new definitions.[8] The possibilities of the World Wide Web and new computer technology in general meant that the processes of researching the dictionary and of publishing new and revised entries could be vastly improved. New text search databases offered vastly more material for the editors of the dictionary to work with, and with publication on the Web as a possibility, the editors could publish revised entries much more quickly and easily than ever before.[48] A new approach was called for, and for this reason it was decided to embark on a new, complete revision of the dictionary.		Beginning with the launch of the first OED Online site in 2000, the editors of the dictionary began a major revision project to create a completely revised third edition of the dictionary (OED3), expected to be completed in 2037[49][50] at a projected cost of about £34 million.[51]		Revisions were started at the letter M, with new material appearing every three months on the OED Online website. The editors chose to start the revision project from the middle of the dictionary in order that the overall quality of entries be made more even, since the later entries in the OED1 generally tended to be better than the earlier ones. However, in March 2008, the editors announced that they would alternate each quarter between moving forward in the alphabet as before and updating "key English words from across the alphabet, along with the other words which make up the alphabetical cluster surrounding them".[52] With the relaunch of the OED Online website in December 2010, alphabetical revision was abandoned altogether.[53]		The revision is expected to roughly double the dictionary in size.[5][54] Apart from general updates to include information on new words and other changes in the language, the third edition brings many other improvements, including changes in formatting and stylistic conventions to make entries clearer to read and enable more thorough searches to be made by computer, more thorough etymological information, and a general change of focus away from individual words towards more general coverage of the language as a whole.[48][55] While the original text drew its quotations mainly from literary sources such as novels, plays, and poetry, with additional material from newspapers and academic journals, the new edition will reference more kinds of material that were unavailable to the editors of previous editions, such as wills, inventories, account books, diaries, journals, and letters.[54]		John Simpson was the first chief editor of the OED3. He retired in 2013 and was replaced by Michael Proffitt, who is the eighth chief editor of the dictionary.[56]		The production of the new edition takes full advantage of computer technology, particularly since the June 2005 inauguration of the whimsically named "Perfect All-Singing All-Dancing Editorial and Notation Application", or "Pasadena". With this XML-based system, the attention of lexicographers can be directed more to matters of content than to presentation issues such as the numbering of definitions. The new system has also simplified the use of the quotations database, and enabled staff in New York to work directly on the dictionary in the same way as their Oxford-based counterparts.[57]		Other important computer uses include internet searches for evidence of current usage, and e-mail submissions of quotations by readers and the general public.[58]		Wordhunt was a 2005 appeal to the general public for help in providing citations for 50 selected recent words, and produced antedatings for many. The results were reported in a BBC TV series, Balderdash and Piffle. The OED's small army of devoted readers continue to contribute quotations: the department currently receives about 200,000 a year.[59]		OED currently contains over 600,000 entries.[60]		In 1971, the 13-volume OED1 (1933) was reprinted as a two-volume Compact Edition, by photographically reducing each page to one-half its linear dimensions; each compact edition page held four OED1 pages in a four-up ("4-up") format. The two volume letters were A and P; the first supplement was at the second volume's end.		The Compact Edition included, in a small slip-case drawer, a magnifying glass to help in reading reduced type. Many copies were inexpensively distributed through book clubs. In 1987, the second supplement was published as a third volume to the Compact Edition. In 1991, for the OED2, the compact edition format was re-sized to one-third of original linear dimensions, a nine-up ("9-up") format requiring greater magnification, but allowing publication of a single-volume dictionary. It was accompanied by a magnifying glass as before and A User's Guide to the "Oxford English Dictionary", by Donna Lee Berg.[61] After these volumes were published, though, book club offers commonly continued to sell the two-volume 1971 Compact Edition.[25]		Once the text of the dictionary was digitized and online, it was also available to be published on CD-ROM. The text of the first edition was made available in 1987.[62] Afterward, three versions of the second edition were issued. Version 1 (1992) was identical in content to the printed second edition, and the CD itself was not copy-protected. Version 2 (1999) included the Oxford English Dictionary Additions of 1993 and 1997.		Version 3.0 was released in 2002 with additional words from the OED3 and software improvements. Version 3.1.1 (2007) added support for hard disk installation, so that the user does not have to insert the CD to use the dictionary. It has been reported that this version will work on operating systems other than Microsoft Windows, using emulation programs.[63][64] Version 4.0 of the CD has been available since June 2009 and works with Windows 7 and Mac OS X (10.4 or later).[65] This version uses the CD drive for installation, running only from the hard drive.		On 14 March 2000, the Oxford English Dictionary Online (OED Online) became available to subscribers.[66] The online database contains the entire OED2 and is updated quarterly with revisions that will be included in the OED3 (see above). The online edition is the most up-to-date version of the dictionary available. The OED web site is not optimized for mobile devices, but the developers have stated that there are plans to provide an API that would enable developers to develop different interfaces for querying the OED.[67]		The price for an individual to use this edition is £195 or US$295 every year, even after a reduction in 2004; consequently, most subscribers are large organizations such as universities. Some public libraries and companies have subscribed, as well, including public libraries in the United Kingdom, where access is funded by the Arts Council,[68] and public libraries in New Zealand.[69][70] Individuals who belong to a library which subscribes to the service are able to use the service from their own home without charge.		The OED's utility and renown as a historical dictionary have led to numerous offspring projects and other dictionaries bearing the Oxford name, though not all are directly related to the OED itself.		The Shorter Oxford English Dictionary, originally started in 1902 and completed in 1933,[72] is an abridgement of the full work that retains the historical focus, but does not include any words which were obsolete before 1700 except those used by Shakespeare, Milton, Spenser, and the King James Bible.[73] A completely new edition was produced from the OED2 and published in 1993,[74] with further revisions following in 2002 and 2007.		The Concise Oxford Dictionary is a different work, which aims to cover current English only, without the historical focus. The original edition, mostly based on the OED1, was edited by Francis George Fowler and Henry Watson Fowler and published in 1911, before the main work was completed.[75] Revised editions appeared throughout the twentieth century to keep it up to date with changes in English usage.		In 1998 the New Oxford Dictionary of English (NODE) was published. While also aiming to cover current English, NODE was not based on the OED. Instead, it was an entirely new dictionary produced with the aid of corpus linguistics.[76] Once NODE was published, a similarly brand-new edition of the Concise Oxford Dictionary followed, this time based on an abridgement of NODE rather than the OED; NODE (under the new title of the Oxford Dictionary of English, or ODE) continues to be principal source for Oxford's product line of current-English dictionaries, including the New Oxford American Dictionary, with the OED now only serving as the basis for scholarly historical dictionaries.		The OED lists British headword spellings (e.g., labour, centre) with variants following (labor, center, etc.). For the suffix more commonly spelt -ise in British English, OUP policy dictates a preference for the spelling -ize, e.g., realize vs. realise and globalization vs. globalisation. The rationale is etymological, in that the English suffix is mainly derived from the Greek suffix -ιζειν, (-izein), or the Latin -izāre.[77] However, -ze is also sometimes treated as an Americanism insofar as the -ze suffix has crept into words where it did not originally belong, as with analyse (British English), which is spelt analyze in American English.[78][79]		Despite, and at the same time precisely because of, its claim of authority[80] on the English language, the Oxford English Dictionary has been criticised since at least the 1960s from various angles. It has become a target precisely because of its scope, its claims to authority, its British-centredness and relative neglect of World Englishes,[81] its implied but not acknowledged focus on literary language and, above all, its influence. The OED, as a commercial product, has always had to manoeuvre a thin line between PR, marketing and scholarship and one can argue that its biggest problem is the critical uptake of the work by the interested public. In his review of the 1982 supplement,[82] University of Oxford linguist Roy Harris writes that criticizing the OED is extremely difficult because "one is dealing not just with a dictionary but with a national institution", one that "has become, like the English monarchy, virtually immune from criticism in principle". He further notes that neologisms from respected "literary" authors such as Samuel Beckett and Virginia Woolf are included, whereas usage of words in newspapers or other less "respectable" sources hold less sway, even though they may be commonly used. He writes that the OED's "[b]lack-and-white lexicography is also black-and-white in that it takes upon itself to pronounce authoritatively on the rights and wrongs of usage", faulting the dictionary's prescriptive rather than descriptive usage. To Harris, this prescriptive classification of certain usages as "erroneous" and the complete omission of various forms and usages cumulatively represent the "social bias[es]" of the (presumably well-educated and wealthy) compilers. However, the identification of "erroneous and catachrestic" usages is being removed from third edition entries,[83][84] sometimes in favour of usage notes describing the attitudes to language which have previously led to these classifications.[85]		Harris also faults the editors' "donnish conservatism" and their adherence to prudish Victorian morals, citing as an example the non-inclusion of "various centuries-old 'four-letter words'" until 1972. However, no English dictionary included such words, for fear of possible prosecution under British obscenity laws, until after the conclusion of the Lady Chatterley's Lover obscenity trial in 1960. The first dictionary to include the word fuck was the Penguin English Dictionary of 1965.[86] Joseph Wright's English Dialect Dictionary had included shit in 1905.[87]		The OED's claims of authority have also been questioned by linguists such as Pius ten Hacken, who notes that the dictionary actively strives towards definitiveness and authority but can only achieve those goals in a limited sense, given the difficulties of defining the scope of what it includes.[88]		Founding editor James Murray was also reluctant to include scientific terms, despite their documentation, unless he felt that they were widely enough used. In 1902, he declined to add the word "radium" to the dictionary.[89][90]		In contrast, Tim Bray, co-creator of Extensible Markup Language (XML), credits the OED as the developing inspiration of that markup language.[91] Similarly, author Anu Garg, founder of Wordsmith.org, has called the Oxford English Dictionary a "lex icon".[92]		
Harry Potter is a series of fantasy novels written by British author J. K. Rowling. The novels chronicle the life of a young wizard, Harry Potter, and his friends Hermione Granger and Ron Weasley, all of whom are students at Hogwarts School of Witchcraft and Wizardry. The main story arc concerns Harry's struggle against Lord Voldemort, a dark wizard who intends to become immortal, overthrow the wizard governing body known as the Ministry of Magic, and subjugate all wizards and muggles, a reference term that means non-magical people.		Since the release of the first novel, Harry Potter and the Philosopher's Stone, on 26 June 1997, the books have found immense popularity, critical acclaim and commercial success worldwide. They have attracted a wide adult audience as well as younger readers, and are often considered cornerstones of modern young adult literature.[2] The series has also had its share of criticism, including concern about the increasingly dark tone as the series progressed, as well as the often gruesome and graphic violence it depicts. As of May 2013[update], the books have sold more than 500 million copies worldwide, making them the best-selling book series in history, and have been translated into seventy-three languages.[3][4] The last four books consecutively set records as the fastest-selling books in history, with the final instalment selling roughly eleven million copies in the United States within twenty-four hours of its release.		The series was originally published in English by two major publishers, Bloomsbury in the United Kingdom and Scholastic Press in the United States. A play, Harry Potter and the Cursed Child, based on a story co-written by Rowling, premiered in London on 30 July 2016 at the Palace Theatre, and its script was published by Little, Brown as the eighth book in the series.[5] The original seven books were adapted into an eight-part film series by Warner Bros. Pictures, which has become the second highest-grossing film series of all time as of August 2015[update]. In 2016, the total value of the Harry Potter franchise was estimated at $25 billion,[6] making Harry Potter one of the highest-grossing media franchises of all time.		A series of many genres, including fantasy, drama, coming of age, and the British school story (which includes elements of mystery, thriller, adventure, horror and romance), the world of Harry Potter explores numerous themes and includes many cultural meanings and references.[7] According to Rowling, the main theme is death.[8] Other major themes in the series include prejudice, corruption, and madness.[9]		The success of the books and films has ensured that the Harry Potter franchise continues to expand, with numerous derivative works, a travelling exhibition that premiered in Chicago in 2009, a studio tour in London that opened in 2012, a digital platform on which J.K. Rowling updates the series with new information and insight, and a pentalogy of spin-off films premiering in November 2016, among many other developments. Most recently, themed attractions, collectively known as The Wizarding World of Harry Potter, have been built at several Universal Parks & Resorts amusement parks around the world.		The central character in the series is Harry Potter, an English boy who lives with his aunt, uncle, and cousin - the Dursleys - who discovers, at the age of eleven, that he is a wizard, though he lives in the ordinary world of non-magical people known as Muggles.[10] The wizarding world exists parallel to the Muggle world, albeit hidden and in secrecy. His magical ability is inborn and children with such abilities are invited to attend exclusive magic schools that teach the necessary skills to succeed in the wizarding world.[11] Harry becomes a student at Hogwarts School of Witchcraft and Wizardry, a wizarding academy in Scotland and it is here where most of the events in the series take place. As Harry develops through his adolescence, he learns to overcome the problems that face him: magical, social and emotional, including ordinary teenage challenges such as friendships, infatuation, romantic relationships, schoolwork and exams, anxiety, depression, stress, and the greater test of preparing himself for the confrontation, that lies ahead, in wizarding Britain's increasingly-violent second wizarding war.[12]		Each novel chronicles one year in Harry's life[13] during the period from 1991 to 1998.[14] The books also contain many flashbacks, which are frequently experienced by Harry viewing the memories of other characters in a device called a Pensieve.		The environment Rowling created is intimately connected to reality. The British magical community of the Harry Potter books is inspired by 1990s British culture, European folklore, classical mythology and alchemy, incorporating objects and wildlife such as magic wands, magic plants, potions, spells, flying broomsticks, centaurs, and other magical creatures, the Deathly Hallows, and the Philosopher's Stone, beside others invented by Rowling. While the fantasy land of Narnia is an alternate universe and the Lord of the Rings' Middle-earth a mythic past, the wizarding world of Harry Potter exists in parallel within the real world and contains magical versions of the ordinary elements of everyday life, with the action mostly set in Scotland (Hogwarts), the West Country, Devon, London and Surrey in southeast England.[15] The world only accessible to wizards and magical beings comprises a fragmented collection of overlooked hidden streets, ancient pubs, lonely country manors and secluded castles invisible to the Muggle population.[11]		When the first novel of the series, Harry Potter and the Philosopher's Stone (published in America and other countries as Harry Potter and the Sorcerer's Stone) opens, it is apparent that some significant event has taken place in the Wizarding World – an event so very remarkable, even Muggles (non-magical people) notice signs of it. The full background to this event and Harry Potter's past is revealed gradually through the series. After the introductory chapter, the book leaps forward to a time shortly before Harry Potter's eleventh birthday, and it is at this point that his magical background begins to be revealed.		Despite Harry's aunt and uncle's desperate prevention of Harry gleaning about his powers[16], their efforts are in vain. Harry meets a half-giant, Rubeus Hagrid, who is also his first contact with the Wizarding World. Hagrid reveals himself to be the Keeper of Keys and Grounds at Hogwarts as well as some of Harry's history.[16] Harry learns that, as a baby, he witnessed his parents' murder by the power-obsessed dark wizard Lord Voldemort, who subsequently attempted to kill him as well.[16] Instead, the unexpected happened: Harry survived with only a lightning-shaped scar on his forehead as a memento of the attack and Voldemort disappeared soon afterwards, gravely weakened by his own rebounding curse. As its inadvertent saviour from Voldemort's reign of terror, Harry has become a living legend in the Wizarding World. However, at the orders of the venerable and well-known wizard Albus Dumbledore, the orphaned Harry had been placed in the home of his unpleasant Muggle relatives, the Dursleys, who have kept him safe but treated him poorly, including confining him to a cupboard without meals and torturing him like he is their servant. Hagrid then officially invites Harry to attend Hogwarts School of Witchcraft and Wizardry, a famous magic school in Scotland that educates young teenagers on their magical development for seven years, from age eleven to seventeen.		With Hagrid's help, Harry prepares for and undertakes his first year of study at Hogwarts. As Harry begins to explore the magical world, the reader is introduced to many of the primary locations used throughout the series. Harry meets most of the main characters and gains his two closest friends: Ron Weasley, a fun-loving member of an ancient, large, happy, but poor wizarding family, and Hermione Granger, a gifted, bright, and hardworking witch of non-magical parentage.[16][17] Harry also encounters the school's potions master, Severus Snape, who displays a conspicuously deep and abiding dislike for him, the rich brat Draco Malfoy whom he quickly makes enemies with, and the Defence Against the Dark Arts teacher, Quirinus Quirrell, who later turns out to be allied with Lord Voldemort. He also discovers a talent of flying on broomsticks and is recruited for his house's Quidditch team, a sport in the wizarding world where players fly on broomsticks. The first book concludes with Harry's second confrontation with Lord Voldemort, who, in his quest to regain a body, yearns to gain the power of the Philosopher's Stone, a substance that bestows everlasting life and turns any metal into pure gold.[16]		The series continues with Harry Potter and the Chamber of Secrets, describing Harry's second year at Hogwarts. He and his friends investigate a 50-year-old mystery that appears uncannily related to recent sinister events at the school. Ron's younger sister, Ginny Weasley, enrolls in her first year at Hogwarts, and finds an old notebook in her belongings which turns out to be an alumnus's diary, Tom Marvolo Riddle, later revealed to be Voldemort's younger self, who is bent on ridding the school of "mudbloods", a derogatory term describing wizards and witches of non-magical parentage. The memory of Tom Riddle resides inside of the diary and when Ginny begins to confide in the diary, Voldemort is able to possess her. Through the diary, Ginny acts on Voldemort's orders and unconsciously opens the "Chamber of Secrets", unleashing an ancient monster, later revealed to be a basilisk, which begins attacking students at Hogwarts. It kills those who make direct eye contact with it and petrifies those who look at it indirectly. The book also introduces a new Defence Against the Dark Arts teacher, Gilderoy Lockhart, a highly cheerful, self-conceited wizard with a pretentious facade, later turning out to be a fraud. Harry discovers that prejudice exists in the Wizarding World through delving into the school's history, and learns that Voldemort's reign of terror was often directed at wizards and witches who were descended from Muggles. Harry also learns about the innate ability of his to speak the snake language Parseltongue is rare and often associated with the Dark Arts. When Hermione is attacked and petrified, Harry and Ron finally piece together the puzzles and unlock the Chamber of Secrets, with Harry destroying the diary for good and saving Ginny, and also destroying a part of Voldemort's soul. The end of the book reveals Lucius Malfoy, Draco's father and rival of Ron and Ginny's father, to be the culprit who slipped the book into Ginny's belongings and introduced the diary into Hogwarts.		The third novel, Harry Potter and the Prisoner of Azkaban, follows Harry in his third year of magical education. It is the only book in the series which does not feature Lord Voldemort in any form. Instead, Harry must deal with the knowledge that he has been targeted by Sirius Black, his father's best friend, and, according to the Wizarding World, an escaped mass murderer who assisted in the murder of Harry's parents. As Harry struggles with his reaction to the dementors – dark creatures with the power to devour a human soul and feed on despair – which are ostensibly protecting the school, he reaches out to Remus Lupin, a Defence Against the Dark Arts teacher who is eventually revealed to be a werewolf. Lupin teaches Harry defensive measures which are well above the level of magic generally executed by people his age. Harry comes to know that both Lupin and Black were best friends of his father and that Black was framed by their fourth friend, Peter Pettigrew, who had been hiding Ron's pet rat, Scabbers. [18] In this book, a recurring theme throughout the series is emphasised – in every book there is a new Defence Against the Dark Arts teacher, none of whom lasts more than one school year.		During Harry's fourth year of school (detailed in Harry Potter and the Goblet of Fire), Harry is unwillingly entered as a participant in the Triwizard Tournament, a dangerous yet exciting contest where three "champions", one from each participating school, must compete with each other in three tasks in order to win the Triwizard Cup. This year, Harry must compete against a witch and a wizard "champion" from overseas visiting schools Beauxbatons and Durmstrang, as well as another Hogwarts student, causing Harry's friends to distance themselves from him.[19] Harry is guided through the tournament by their new Defence Against the Dark Arts professor, Alastor "Mad-Eye" Moody, who turns out to be an impostor – one of Voldemort's supporters named Barty Crouch, Jr. in disguise. The point at which the mystery is unravelled marks the series' shift from foreboding and uncertainty into open conflict. Voldemort's plan to have Crouch use the tournament to bring Harry to Voldemort succeeds. Although Harry manages to escape, Cedric Diggory, the other Hogwarts champion in the tournament, is killed by Peter Pettigrew and Voldemort re-enters the Wizarding World with a physical body.		In the fifth book, Harry Potter and the Order of the Phoenix, Harry must confront the newly resurfaced Voldemort. In response to Voldemort's reappearance, Dumbledore re-activates the Order of the Phoenix, a secret society which works from Sirius Black's dark family home to defeat Voldemort's minions and protect Voldemort's targets, especially Harry. Despite Harry's description of Voldemort's recent activities, the Ministry of Magic and many others in the magical world refuse to believe that Voldemort has returned. In an attempt to counter and eventually discredit Dumbledore, who along with Harry is the most prominent voice in the Wizarding World attempting to warn of Voldemort's return, the Ministry appoints Dolores Umbridge as the High Inquisitor of Hogwarts and the new Defence Against the Dark Arts teacher. She transforms the school into a dictatorial regime and refuses to allow the students to learn ways to defend themselves against dark magic.[20]		With Ron and Hermione's suggestion, Harry forms "Dumbledore's Army", a secret study group aimed to teach his classmates the higher-level skills of Defence Against the Dark Arts that he has learned from his previous encounters with Dark wizards. Through those lessons, Harry begins to develop a crush on the popular and attractive Cho Chang. Juggling schoolwork, Umbridge's incessant and persistent efforts to land him in trouble and the defensive lessons, Harry begins to lose sleep as he constantly receives disturbing dreams about a dark corridor in the Ministry of Magic, followed by a burning desire. An important prophecy concerning Harry and Lord Voldemort is then revealed,[21] and Harry discovers that he and Voldemort have a painful connection, allowing Harry to view some of Voldemort's actions telepathically. In the novel's climax, Harry is tricked into seeing Sirius tortured and races to the Ministry of Magic. He and his friends face off against Voldemort's followers nicknamed Death Eaters at the Ministry of Magic. Although the timely arrival of members of the Order of the Phoenix saves the teenagers' lives, Sirius Black is killed in the conflict.		In the sixth book, Harry Potter and the Half-Blood Prince, Voldemort begins waging open warfare. Harry and his friends are relatively protected from that danger at Hogwarts. They are subject to all the difficulties of adolescence – Harry eventually begins dating Ginny, Ron establishes a strong infatuation with fellow Hogwarts student Lavender Brown, and Hermione starts to develop romantic feelings towards Ron. Near the beginning of the novel, lacking his own book, Harry is given an old potions textbook filled with many annotations and recommendations signed by a mysterious writer titled; "the Half-Blood Prince." This book is a source of scholastic success and great recognition from their new potions master, Horace Slughorn, but because of the potency of the spells that are written in it, becomes a source of concern. With war drawing near, Harry takes private lessons with Dumbledore, who shows him various memories concerning the early life of Voldemort in a device called a Pensieve. These reveal that in order to preserve his life, Voldemort has split his soul into pieces, creating a series of Horcruxes – evil enchanted items hidden in various locations, one of which was the diary destroyed in the second book.[22] On their way to collect a Horcrux, Draco, who has joined with the Death Eaters, attempts to attack Dumbledore, and the book culminates in the killing of Dumbledore by Professor Snape, the titular Half-Blood Prince.		Harry Potter and the Deathly Hallows, the last original novel in the series, begins directly after the events of the sixth book. Lord Voldemort has completed his ascension to power and gained control of the Ministry of Magic. Harry, Ron and Hermione drop out of school so that they can find and destroy Voldemort's remaining Horcruxes. To ensure their own safety as well as that of their family and friends, they are forced to isolate themselves. A ghoul pretends to be Ron ill with a contagious disease, Harry and the Dursleys separate, and Hermione wipes her parents' memories. As they search for the Horcruxes, the trio learns details about an ancient prophecy about the Deathly Hallows, three legendary items that when united under one Keeper, would supposedly grant the person the Master of Death. Harry discovers his handy Invisibility Cloak to be one of those items, and Voldemort to be searching for another: the Elder Wand, the most powerful wand in history. At the end of the book, Harry and his friends learn about Dumbledore's past, as well as Snape's true motives – he had worked on Dumbledore's behalf since the murder of Harry's mother. Eventually, Snape is killed by Voldemort out of paranoia.		The book culminates in the Battle of Hogwarts. Harry, Ron and Hermione, in conjunction with members of the Order of the Phoenix and many of the teachers and students, defend Hogwarts from Voldemort, his Death Eaters, and various dangerous magical creatures. Several major characters are killed in the first wave of the battle, including Remus Lupin and Fred Weasley, Ron's older brother. After learning that he himself is a Horcrux, Harry surrenders himself to Voldemort in the Forbidden Forest, who casts a killing curse (Avada Kedavra) at him. The defenders of Hogwarts do not surrender after learning of Harry's presumed death and continue to fight on. Harry awakens and faces Voldemort, whose Horcruxes have all been destroyed. In the final battle, Voldemort's killing curse rebounds off Harry's defensive spell (Expelliarmus) killing Voldemort. Harry Potter marries and has children with Ginny and Hermione marries and has children with Ron.		An epilogue describes the lives of the surviving characters and the effects of Voldemort's death on the Wizarding World. It also introduces the children of all the characters.		Harry Potter and the Cursed Child is a two-part West End stage play.[23] It was written by Jack Thorne and based on a story by author J. K. Rowling, Thorne and director John Tiffany. The play opened on 30 July 2016 at the Palace Theatre, London, England. The script was released on 31 July 2016.[24] The story is set nineteen years after the ending of Harry Potter and the Deathly Hallows and follows Harry Potter, now a Ministry of Magic employee, and his youngest son Albus Severus Potter. This stage play was also released as a two-part play script on 31 July 2016. The play's official synopsis was released on 23 October 2015:[25]		It was always difficult being Harry Potter and it isn’t much easier now that he is an overworked employee of the Ministry of Magic, a husband, and father of three school-age children.		While Harry grapples with a past that refuses to stay where it belongs, his youngest son Albus must struggle with the weight of a family legacy he never wanted. As past and present fuse ominously, both father and son learn the uncomfortable truth: sometimes, darkness comes from unexpected places.		Rowling has expanded the Harry Potter universe with several short books produced for various charities.[26][27] In 2001, she released Fantastic Beasts and Where to Find Them (a purported Hogwarts textbook) and Quidditch Through the Ages (a book Harry reads for fun). Proceeds from the sale of these two books benefited the charity Comic Relief.[28] In 2007, Rowling composed seven handwritten copies of The Tales of Beedle the Bard, a collection of fairy tales that is featured in the final novel, one of which was auctioned to raise money for the Children's High Level Group, a fund for mentally disabled children in poor countries. The book was published internationally on 4 December 2008.[29][30] Rowling also wrote an 800-word prequel in 2008 as part of a fundraiser organised by the bookseller Waterstones.[31] All three of these books contain extra information about the wizarding world not included in the original novels.		In 2016, she released three new e-books: Hogwarts: An Incomplete and Unreliable Guide, Short Stories from Hogwarts of Power, Politics and Pesky Poltergeists and Short Stories from Hogwarts of Heroism, Hardship and Dangerous Hobbies.[32]		In 2011, Rowling launched a new website announcing an upcoming project called Pottermore.[33] Pottermore opened to the general public on 14 April 2012.[34] Pottermore allows users to be sorted, be chosen by their wand and play various minigames. The main purpose of the website was to allow the user to journey though the story with access to content not revealed by JK Rowling previously, with over 18,000 words of additional content.[35]		In September 2015, the website was completely overhauled and most of the features were removed. The site has been redesigned and it mainly focuses on the information already available, rather than exploration.[36]		The Harry Potter novels are mainly directed at a young adult audience as opposed to an audience of middle grade readers, children, or adults. The novels fall within the genre of fantasy literature, and qualify as a type of fantasy called "urban fantasy", "contemporary fantasy", or "low fantasy". They are mainly dramas, and maintain a fairly serious and dark tone throughout, though they do contain some notable instances of tragicomedy and black humour. In many respects, they are also examples of the bildungsroman, or coming of age novel,[37] and contain elements of mystery, adventure, horror, thriller, and romance. The books are also, in the words of Stephen King, "shrewd mystery tales",[38] and each book is constructed in the manner of a Sherlock Holmes-style mystery adventure. The stories are told from a third person limited point of view with very few exceptions (such as the opening chapters of Philosopher's Stone, Goblet of Fire and Deathly Hallows and the first two chapters of Half-Blood Prince).		The series can be considered part of the British children's boarding school genre, which includes Rudyard Kipling's Stalky & Co., Enid Blyton's Malory Towers, St. Clare's and the Naughtiest Girl series, and Frank Richards's Billy Bunter novels: the Harry Potter books are predominantly set in Hogwarts, a fictional British boarding school for wizards, where the curriculum includes the use of magic.[39] In this sense they are "in a direct line of descent from Thomas Hughes's Tom Brown's School Days and other Victorian and Edwardian novels of British public school life", though they are, as many note, more contemporary, grittier, darker, and more mature than the typical boarding school novel, addressing serious themes of death, love, loss, prejudice, coming-of-age, and the loss of innocence in a 1990's British setting.[40][41]		Each of the seven books is set over the course of one school year. Harry struggles with the problems he encounters, and dealing with them often involves the need to violate some school rules. If students are caught breaking rules, they are often disciplined by Hogwarts professors. The stories reach their climax in the summer term, near or just after final exams, when events escalate far beyond in-school squabbles and struggles, and Harry must confront either Voldemort or one of his followers, the Death Eaters, with the stakes a matter of life and death – a point underlined, as the series progresses, by characters being killed in each of the final four books.[42][43] In the aftermath, he learns important lessons through exposition and discussions with head teacher and mentor Albus Dumbledore. The only exception to this school-centred setting is the final novel, Harry Potter and the Deathly Hallows, in which Harry and his friends spend most of their time away from Hogwarts, and only return there to face Voldemort at the dénouement.[42]		According to Rowling, a major theme in the series is death: "My books are largely about death. They open with the death of Harry's parents. There is Voldemort's obsession with conquering death and his quest for immortality at any price, the goal of anyone with magic. I so understand why Voldemort wants to conquer death. We're all frightened of it."[8]		Academics and journalists have developed many other interpretations of themes in the books, some more complex than others, and some including political subtexts. Themes such as normality, oppression, survival, and overcoming imposing odds have all been considered as prevalent throughout the series.[44] Similarly, the theme of making one's way through adolescence and "going over one's most harrowing ordeals – and thus coming to terms with them" has also been considered.[45] Rowling has stated that the books comprise "a prolonged argument for tolerance, a prolonged plea for an end to bigotry" and that they also pass on a message to "question authority and... not assume that the establishment or the press tells you all of the truth".[46]		While the books could be said to comprise many other themes, such as power/abuse of power, violence and hatred, love, loss, prejudice, and free choice, they are, as Rowling states, "deeply entrenched in the whole plot"; the writer prefers to let themes "grow organically", rather than sitting down and consciously attempting to impart such ideas to her readers.[9] Along the same lines is the ever-present theme of adolescence, in whose depiction Rowling has been purposeful in acknowledging her characters' sexualities and not leaving Harry, as she put it, "stuck in a state of permanent pre-pubescence". Rowling has also been praised for her nuanced depiction of the ways in which death and violence affects youth, and humanity as a whole.[47]		Rowling said that, to her, the moral significance of the tales seems "blindingly obvious". The key for her was the choice between what is right and what is easy, "because that … is how tyranny is started, with people being apathetic and taking the easy route and suddenly finding themselves in deep trouble."[48]		In 1990, Rowling was on a crowded train from Manchester to London when the idea for Harry suddenly "fell into her head". Rowling gives an account of the experience on her website saying:[49]		"I had been writing almost continuously since the age of six but I had never been so excited about an idea before. I simply sat and thought, for four (delayed train) hours, and all the details bubbled up in my brain, and this scrawny, black-haired, bespectacled boy who did not know he was a wizard became more and more real to me."		Rowling completed Harry Potter and the Philosopher's Stone in 1995 and the manuscript was sent off to several prospective agents.[50] The second agent she tried, Christopher Little, offered to represent her and sent the manuscript to Bloomsbury.		After eight other publishers had rejected Philosopher's Stone, Bloomsbury offered Rowling a £2,500 advance for its publication.[52][53] Despite Rowling's statement that she did not have any particular age group in mind when beginning to write the Harry Potter books, the publishers initially targeted children aged nine to eleven.[54] On the eve of publishing, Rowling was asked by her publishers to adopt a more gender-neutral pen name in order to appeal to the male members of this age group, fearing that they would not be interested in reading a novel they knew to be written by a woman. She elected to use J. K. Rowling (Joanne Kathleen Rowling), using her grandmother's name as her second name because she has no middle name.[53][55]		Harry Potter and the Philosopher's Stone was published by Bloomsbury, the publisher of all Harry Potter books in the United Kingdom, on 26 June 1997.[56] It was released in the United States on 1 September 1998 by Scholastic – the American publisher of the books – as Harry Potter and the Sorcerer's Stone,[57] after Rowling had received US$105,000 for the American rights – an unprecedented amount for a children's book by a then-unknown author.[58] Fearing that American readers would not associate the word "philosopher" with a magical theme (although the Philosopher's Stone is alchemy-related), Scholastic insisted that the book be given the title Harry Potter and the Sorcerer's Stone for the American market.[59]		The second book, Harry Potter and the Chamber of Secrets was originally published in the UK on 2 July 1998 and in the US on 2 June 1999. Harry Potter and the Prisoner of Azkaban was then published a year later in the UK on 8 July 1999 and in the US on 8 September 1999.[60] Harry Potter and the Goblet of Fire was published on 8 July 2000 at the same time by Bloomsbury and Scholastic.[61] Harry Potter and the Order of the Phoenix is the longest book in the series at 766 pages in the UK version and 870 pages in the US version.[62] It was published worldwide in English on 21 June 2003.[63] Harry Potter and the Half-Blood Prince was published on 16 July 2005, and it sold 9 million copies in the first 24 hours of its worldwide release.[64][65] The seventh and final novel, Harry Potter and the Deathly Hallows, was published on 21 July 2007.[66] The book sold 11 million copies in the first 24 hours of release, breaking down to 2.7 million copies in the UK and 8.3 million in the US.[65]		The series has been translated into 67 languages,[3][67] placing Rowling among the most translated authors in history.[68] The books have seen translations to diverse languages such as Korean, Armenian, Ukrainian, Arabic, Urdu, Hindi, Bengali, Bulgarian, Welsh, Afrikaans, Albanian, Latvian and Vietnamese. The first volume has been translated into Latin and even Ancient Greek,[69] making it the longest published work in Ancient Greek since the novels of Heliodorus of Emesa in the 3rd century AD.[70] The second volume has also been translated into Latin.[71]		Some of the translators hired to work on the books were well-known authors before their work on Harry Potter, such as Viktor Golyshev, who oversaw the Russian translation of the series' fifth book. The Turkish translation of books two to seven was undertaken by Sevin Okyay, a popular literary critic and cultural commentator.[72] For reasons of secrecy, translation on a given book could only start after it had been released in English, leading to a lag of several months before the translations were available. This led to more and more copies of the English editions being sold to impatient fans in non-English speaking countries; for example, such was the clamour to read the fifth book that its English language edition became the first English-language book ever to top the best-seller list in France.[73]		The United States editions were adapted into American English to make them more understandable to a young American audience.[74]		In December 2005, Rowling stated on her web site, "2006 will be the year when I write the final book in the Harry Potter series."[75] Updates then followed in her online diary chronicling the progress of Harry Potter and the Deathly Hallows, with the release date of 21 July 2007. The book itself was finished on 11 January 2007 in the Balmoral Hotel, Edinburgh, where she scrawled a message on the back of a bust of Hermes. It read: "J. K. Rowling finished writing Harry Potter and the Deathly Hallows in this room (552) on 11 January 2007."[76]		Rowling herself has stated that the last chapter of the final book (in fact, the epilogue) was completed "in something like 1990".[77][78] In June 2006, Rowling, on an appearance on the British talk show Richard & Judy, announced that the chapter had been modified as one character "got a reprieve" and two others who previously survived the story had in fact been killed. On 28 March 2007, the cover art for the Bloomsbury Adult and Child versions and the Scholastic version were released.[79][80]		In September 2012, Rowling mentioned in an interview that she might go back to make a "director's cut" of two of the existing Harry Potter books.[81]		For cover art, Bloomsbury chose painted art in a classic style of design, with the first cover a watercolour and pencil drawing by illustrator Thomas Taylor showing Harry boarding the Hogwarts Express, and a title in the font Cochin Bold.[82] The first releases of the successive books in the series followed in the same style but somewhat more realistic, illustrating scenes from the books. These covers were created by first Cliff Wright and then Jason Cockroft.[83]		Due to the appeal of the books among an adult audience, Bloomsbury commissioned a second line of editions in an 'adult' style. These initially used black-and-white photographic art for the covers showing objects from the books (including a very American Hogwarts Express) without depicting people, but later shifted to partial colourisation with a picture of Slytherin's locket on the cover of the final book.		International and later editions have been created by a range of designers, including Mary GrandPré for U.S. audiences and Mika Launis in Finland.[84][85] For a later American release, Kazu Kibuishi created covers in a somewhat anime-influenced style.[86][87]		Fans of the series were so eager for the latest instalment that bookstores around the world began holding events to coincide with the midnight release of the books, beginning with the 2000 publication of Harry Potter and the Goblet of Fire. The events, commonly featuring mock sorting, games, face painting, and other live entertainment have achieved popularity with Potter fans and have been highly successful in attracting fans and selling books with nearly nine million of the 10.8 million initial print copies of Harry Potter and the Half-Blood Prince sold in the first 24 hours.[88][89]		The final book in the series, Harry Potter and the Deathly Hallows became the fastest selling book in history, moving 11 million units in the first twenty-four hours of release.[90] The series has also gathered adult fans, leading to the release of two editions of each Harry Potter book, identical in text but with one edition's cover artwork aimed at children and the other aimed at adults.[91] Besides meeting online through blogs, podcasts, and fansites, Harry Potter super-fans can also meet at Harry Potter symposia.		The word Muggle has spread beyond its Harry Potter origins, becoming one of few pop culture words to land in the Oxford English Dictionary.[92] The Harry Potter fandom has embraced podcasts as a regular, often weekly, insight to the latest discussion in the fandom. Both MuggleCast and PotterCast[93] have reached the top spot of iTunes podcast rankings and have been polled one of the top 50 favourite podcasts.[94]		Some lessons identified in the series include diversity, acceptance, political tolerance, and equality. Surveys of over 1,000 college students in the United States show that those who read the books were significantly different than those who had not. Readers of the series were found to be more tolerant, more opposed to violence and torture, less authoritarian, and less cynical. Although it is not known if this is a cause-and-effect relationship, there is a clear correlation, and it seems that Harry Potter's cultural impact may be stronger than just a fandom bond.[95]		At the University of Michigan in 2009, StarKid Productions performed an original musical parodying the Harry Potter series called A Very Potter Musical. The musical was awarded Entertainment Weekly's 10 Best Viral Videos of 2009.[96]		Characters and elements from the series have inspired scientific names of several organisms, including the dinosaur Dracorex hogwartsia, the spider Eriovixia gryffindori, the wasp Ampulex dementor, and the crab Harryplax severus.[97]		The popularity of the Harry Potter series has translated into substantial financial success for Rowling, her publishers, and other Harry Potter related license holders. This success has made Rowling the first and thus far only billionaire author.[98] The books have sold more than 400 million copies worldwide and have also given rise to the popular film adaptations produced by Warner Bros., all of which have been highly successful in their own right.[99][100] The films have in turn spawned eight video games and have led to the licensing of more than 400 additional Harry Potter products. The Harry Potter brand has been estimated to be worth as much as $25 billion.[6]		The great demand for Harry Potter books motivated the New York Times to create a separate best-seller list for children's literature in 2000, just before the release of Harry Potter and the Goblet of Fire. By 24 June 2000, Rowling's novels had been on the list for 79 straight weeks; the first three novels were each on the hardcover best-seller list.[101] On 12 April 2007, Barnes & Noble declared that Deathly Hallows had broken its pre-order record, with more than 500,000 copies pre-ordered through its site.[102] For the release of Goblet of Fire, 9,000 FedEx trucks were used with no other purpose than to deliver the book.[103] Together, Amazon.com and Barnes & Noble pre-sold more than 700,000 copies of the book.[103] In the United States, the book's initial printing run was 3.8 million copies.[103] This record statistic was broken by Harry Potter and the Order of the Phoenix, with 8.5 million, which was then shattered by Half-Blood Prince with 10.8 million copies.[104] 6.9 million copies of Prince were sold in the U.S. within the first 24 hours of its release; in the United Kingdom more than two million copies were sold on the first day.[105] The initial U.S. print run for Deathly Hallows was 12 million copies, and more than a million were pre-ordered through Amazon and Barnes & Noble.[106]		The Harry Potter series has been recognised by a host of awards since the initial publication of Philosopher's Stone including four Whitaker Platinum Book Awards (all of which were awarded in 2001),[107] three Nestlé Smarties Book Prizes (1997–1999),[108] two Scottish Arts Council Book Awards (1999 and 2001),[109] the inaugural Whitbread children's book of the year award (1999),[110] the WHSmith book of the year (2006),[111] among others. In 2000, Harry Potter and the Prisoner of Azkaban was nominated for a Hugo Award for Best Novel, and in 2001, Harry Potter and the Goblet of Fire won said award.[112] Honours include a commendation for the Carnegie Medal (1997),[113] a short listing for the Guardian Children's Award (1998), and numerous listings on the notable books, editors' Choices, and best books lists of the American Library Association, The New York Times, Chicago Public Library, and Publishers Weekly.[114]		In 2002, British sociologist Andrew Blake named Harry Potter among the icons of British popular culture along with the likes of James Bond and Sherlock Holmes.[115] In 2003, four of the books were named in the top 24 of the BBC's The Big Read survey of the best loved novels in the UK.[116] A 2004 study found that books in the series were commonly read aloud in elementary schools in San Diego County, California.[117] Based on a 2007 online poll, the U.S. National Education Association listed the series in its "Teachers' Top 100 Books for Children".[118] Three of the books placed among the "Top 100 Chapter Books" of all time, or children's novels, in a 2012 survey published by School Library Journal: Sorcerer's Stone ranked number three, Prisoner of Azkaban 12th, and Goblet of Fire 98th.[119]		Early in its history, Harry Potter received positive reviews. On publication, the first book, Harry Potter and the Philosopher's Stone, attracted attention from the Scottish newspapers, such as The Scotsman, which said it had "all the makings of a classic",[120] and The Glasgow Herald, which called it "Magic stuff".[120] Soon the English newspapers joined in, with more than one comparing it to Roald Dahl's work: The Mail on Sunday rated it as "the most imaginative debut since Roald Dahl",[120] a view echoed by The Sunday Times ("comparisons to Dahl are, this time, justified"),[120] while The Guardian called it "a richly textured novel given lift-off by an inventive wit".[120]		By the time of the release of the fifth book, Harry Potter and the Order of the Phoenix, the books began to receive strong criticism from a number of literary scholars. Yale professor, literary scholar, and critic Harold Bloom raised criticisms of the books' literary merits, saying, "Rowling's mind is so governed by clichés and dead metaphors that she has no other style of writing."[121] A. S. Byatt authored a New York Times op-ed article calling Rowling's universe a "secondary secondary world, made up of intelligently patchworked derivative motifs from all sorts of children's literature ... written for people whose imaginative lives are confined to TV cartoons, and the exaggerated (more exciting, not threatening) mirror-worlds of soaps, reality TV and celebrity gossip".[122]		Michael Rosen, a novelist and poet, advocated the books were not suited for children, who would be unable to grasp the complex themes. Rosen also stated that "J. K. Rowling is more of an adult writer."[123] The critic Anthony Holden wrote in The Observer on his experience of judging Harry Potter and the Prisoner of Azkaban for the 1999 Whitbread Awards. His overall view of the series was negative – "the Potter saga was essentially patronising, conservative, highly derivative, dispiritingly nostalgic for a bygone Britain", and he speaks of "a pedestrian, ungrammatical prose style".[124] Ursula K. Le Guin said, "I have no great opinion of it. When so many adult critics were carrying on about the 'incredible originality' of the first Harry Potter book, I read it to find out what the fuss was about, and remained somewhat puzzled; it seemed a lively kid's fantasy crossed with a "school novel", good fare for its age group, but stylistically ordinary, imaginatively derivative, and ethically rather mean-spirited."[125]		By contrast, author Fay Weldon, while admitting that the series is "not what the poets hoped for", nevertheless goes on to say, "but this is not poetry, it is readable, saleable, everyday, useful prose".[126] The literary critic A. N. Wilson praised the Harry Potter series in The Times, stating: "There are not many writers who have JK's Dickensian ability to make us turn the pages, to weep – openly, with tears splashing – and a few pages later to laugh, at invariably good jokes ... We have lived through a decade in which we have followed the publication of the liveliest, funniest, scariest and most moving children's stories ever written".[127] Charles Taylor of Salon.com, who is primarily a movie critic,[128] took issue with Byatt's criticisms in particular. While he conceded that she may have "a valid cultural point – a teeny one – about the impulses that drive us to reassuring pop trash and away from the troubling complexities of art",[129] he rejected her claims that the series is lacking in serious literary merit and that it owes its success merely to the childhood reassurances it offers. Taylor stressed the progressively darker tone of the books, shown by the murder of a classmate and close friend and the psychological wounds and social isolation each causes. Taylor also argued that Philosopher's Stone, said to be the most light-hearted of the seven published books, disrupts the childhood reassurances that Byatt claims spur the series' success: the book opens with news of a double murder, for example.[129]		Stephen King called the series "a feat of which only a superior imagination is capable", and declared "Rowling's punning, one-eyebrow-cocked sense of humor" to be "remarkable". However, he wrote that despite the story being "a good one", he is "a little tired of discovering Harry at home with his horrible aunt and uncle", the formulaic beginning of all seven books.[38] King has also joked that "Rowling's never met an adverb she did not like!" He does however predict that Harry Potter "will indeed stand time's test and wind up on a shelf where only the best are kept; I think Harry will take his place with Alice, Huck, Frodo, and Dorothy and this is one series not just for the decade, but for the ages".[130] Sameer Rahim of The Daily Telegraph disagreed, saying "It depresses me to see 16 and 17 year-olds reading the series when they could be reading the great novels of childhood such as Oliver Twist or A House for Mr Biswas. What that says about the adults who are fanatical fans I’m not sure – but I suspect in years to come people will make a link between our plump, comfortable, infantilising society and the popularity of Potter."[131]		There is ongoing discussion regarding the extent to which the series was inspired by Tolkien's Lord of the Rings books.[132]		Although Time magazine named Rowling as a runner-up for its 2007 Person of the Year award, noting the social, moral, and political inspiration she has given her fandom,[133] cultural comments on the series have been mixed. Washington Post book critic Ron Charles opined in July 2007 that the large numbers of adults reading the Potter series but few other books may represent a "bad case of cultural infantilism", and that the straightforward "good vs. evil" theme of the series is "childish". He also argued "through no fault of Rowling's", the cultural and marketing "hysteria" marked by the publication of the later books "trains children and adults to expect the roar of the coliseum, a mass-media experience that no other novel can possibly provide".[134]		Librarian Nancy Knapp pointed out the books' potential to improve literacy by motivating children to read much more than they otherwise would.[135] The seven-book series has a word count of 1,083,594 (US edition). Agreeing about the motivating effects, Diane Penrod also praised the books' blending of simple entertainment with "the qualities of highbrow literary fiction", but expressed concern about the distracting effect of the prolific merchandising that accompanies the book launches.[136] However, the assumption that Harry Potter books have increased literacy among young people is "largely a folk legend."[137] Research by the National Endowment for the Arts (NEA) has found no increase in reading among children coinciding with the Harry Potter publishing phenomenon, nor has the broader downward trend in reading among Americans been arrested during the rise in the popularity of the Harry Potter books.[137][138] The research also found that children who read Harry Potter books were not more likely to go on to read outside the fantasy and mystery genres.[137] NEA chairman Dana Gioia said the series, "got millions of kids to read a long and reasonably complex series of books. The trouble is that one Harry Potter novel every few years is not enough to reverse the decline in reading."[139]		Jennifer Conn used Snape's and Quidditch coach Madam Hooch's teaching methods as examples of what to avoid and what to emulate in clinical teaching,[140] and Joyce Fields wrote that the books illustrate four of the five main topics in a typical first-year sociology class: "sociological concepts including culture, society, and socialisation; stratification and social inequality; social institutions; and social theory".[141]		Jenny Sawyer wrote in Christian Science Monitor on 25 July 2007 that the books represent a "disturbing trend in commercial storytelling and Western society" in that stories "moral center [sic] have all but vanished from much of today's pop culture ... after 10 years, 4,195 pages, and over 375 million copies, J. K. Rowling's towering achievement lacks the cornerstone of almost all great children's literature: the hero's moral journey". Harry Potter, Sawyer argues, neither faces a "moral struggle" nor undergoes any ethical growth, and is thus "no guide in circumstances in which right and wrong are anything less than black and white".[142] In contrast Emily Griesinger described Harry's first passage through to Platform 9¾ as an application of faith and hope, and his encounter with the Sorting Hat as the first of many in which Harry is shaped by the choices he makes. She also noted the "deeper magic" by which the self-sacrifice of Harry's mother protects the boy throughout the series, and which the power-hungry Voldemort fails to understand.[143]		In an 8 November 2002 Slate article, Chris Suellentrop likened Potter to a "trust-fund kid whose success at school is largely attributable to the gifts his friends and relatives lavish upon him". Noting that in Rowling's fiction, magical ability potential is "something you are born to, not something you can achieve", Suellentrop wrote that Dumbledore's maxim that "It is our choices that show what we truly are, far more than our abilities" is hypocritical, as "the school that Dumbledore runs values native gifts above all else".[144] In a 12 August 2007 New York Times review of Deathly Hallows, however, Christopher Hitchens praised Rowling for "unmooring" her "English school story" from literary precedents "bound up with dreams of wealth and class and snobbery", arguing that she had instead created "a world of youthful democracy and diversity".[145]		In 2010, coinciding with the release of the film Harry Potter and the Deathly Hallows Part 1, a series of articles were written about Private Harry Potter of the British army.[146] This real-life Harry Potter was killed in the Arab Revolt near Hebron in 1939. His grave, located in the British cemetery in Ramla, Israel, began to receive curious visitors leading the Ramla Municipality to list it on their website.[147] The Daily Mail interviewed siblings of Harry Potter who stated, "We couldn't believe people visit his grave, but apparently they come from miles around to have their photo taken next to it."[148]		In 2016, an article written by Diana C. Mutz compares the politics of Harry Potter to the 2016 Donald Trump presidential campaign. She states that 3 themes throughout the books are widely predominant '1) the value of tolerance and respect for difference; 2) opposition to violence and punitiveness; and 3) the dangers of authoritarianism.' She suggests that these themes are also present in the presidential election and it may play a significant role in how Americans have responded to the campaign.[149]		The books have been the subject of a number of legal proceedings, stemming from various conflicts over copyright and trademark infringements. The popularity and high market value of the series has led Rowling, her publishers, and film distributor Warner Bros. to take legal measures to protect their copyright, which have included banning the sale of Harry Potter imitations, targeting the owners of websites over the "Harry Potter" domain name, and suing author Nancy Stouffer to counter her accusations that Rowling had plagiarised her work.[150][151][152] Various religious conservatives have claimed that the books promote witchcraft and religions such as Wicca and are therefore unsuitable for children,[153][154] while a number of critics have criticised the books for promoting various political agendas.[155][156]		The books also aroused controversies in the literary and publishing worlds. From 1997 to 1998, Harry Potter and the Philosopher's Stone won almost all the UK awards judged by children, but none of the children's book awards judged by adults,[157] and Sandra Beckett suggested the reason was intellectual snobbery towards books that were popular among children.[158] In 1999, the winner of the Whitbread Book of the Year award children's division was entered for the first time on the shortlist for the main award, and one judge threatened to resign if Harry Potter and the Prisoner of Azkaban was declared the overall winner; it finished second, very close behind the winner of the poetry prize, Seamus Heaney's translation of the Anglo-Saxon epic Beowulf.[158]		In 2000, shortly before the publication of Harry Potter and the Goblet of Fire, the previous three Harry Potter books topped the New York Times fiction best-seller list and a third of the entries were children's books. The newspaper created a new children's section covering children's books, including both fiction and non-fiction, and initially counting only hardback sales. The move was supported by publishers and booksellers.[101] In 2004, The New York Times further split the children's list, which was still dominated by Harry Potter books into sections for series and individual books, and removed the Harry Potter books from the section for individual books.[159] The split in 2000 attracted condemnation, praise and some comments that presented both benefits and disadvantages of the move.[160] Time suggested that, on the same principle, Billboard should have created a separate "mop-tops" list in 1964 when the Beatles held the top five places in its list, and Nielsen should have created a separate game-show list when Who Wants to Be a Millionaire? dominated the ratings.[161]		In 1998, Rowling sold the film rights of the first four Harry Potter books to Warner Bros. for a reported £1 million ($1,982,900).[162][163] Rowling demanded the principal cast be kept strictly British, nonetheless allowing for the inclusion of Irish actors such as the late Richard Harris as Dumbledore, and for casting of French and Eastern European actors in Harry Potter and the Goblet of Fire where characters from the book are specified as such.[164] After many directors including Steven Spielberg, Terry Gilliam, Jonathan Demme, and Alan Parker were considered, Chris Columbus was appointed on 28 March 2000 as the director for Harry Potter and the Philosopher's Stone (titled "Harry Potter and the Sorcerer's Stone" in the United States), with Warner Bros. citing his work on other family films such as Home Alone and Mrs. Doubtfire and proven experience with directing children as influences for their decision.[165]		After extensive casting, filming began in October 2000 at Leavesden Film Studios and in London itself, with production ending in July 2001.[166][167] Philosopher's Stone was released on 14 November 2001. Just three days after the film's release, production for Harry Potter and the Chamber of Secrets, also directed by Columbus, began. Filming was completed in summer 2002, with the film being released on 15 November 2002.[168] Daniel Radcliffe portrayed Harry Potter, doing so for all succeeding films in the franchise.		Columbus declined to direct Harry Potter and the Prisoner of Azkaban, only acting as producer. Mexican director Alfonso Cuarón took over the job, and after shooting in 2003, the film was released on 4 June 2004. Due to the fourth film beginning its production before the third's release, Mike Newell was chosen as the director for Harry Potter and the Goblet of Fire, released on 18 November 2005.[169] Newell became the first British director of the series, with television director David Yates following suit after he was chosen to helm Harry Potter and the Order of the Phoenix. Production began in January 2006 and the film was released the following year in July 2007.[170] After executives were "really delighted" with his work on the film, Yates was selected to direct Harry Potter and the Half-Blood Prince, which was released on 15 July 2009.[171][172][173][174]		In March 2008, Warner Bros. President and COO Alan F. Horn announced that the final instalment in the series, Harry Potter and the Deathly Hallows, would be released in two cinematic parts: Part 1 on 19 November 2010 and Part 2 on 15 July 2011. Production of both parts started in February 2009, with the final day of principal photography taking place on 12 June 2010.[175][176]		Rowling had creative control on the film series, observing the filmmaking process of Philosopher's Stone and serving as producer on the two-part Deathly Hallows, alongside David Heyman and David Barron.[177] The Harry Potter films have been top-rank box office hits, with all eight releases on the list of highest-grossing films worldwide. Philosopher's Stone was the highest-grossing Harry Potter film up until the release of the final instalment of the series, Deathly Hallows, while Prisoner of Azkaban grossed the least.[178] As well as being a financial success, the film series has also been a success among film critics.[179][180]		Opinions of the films are generally divided among fans, with one group preferring the more faithful approach of the first two films, and another group preferring the more stylised character-driven approach of the later films.[181] Rowling has been constantly supportive of all the films and evaluated Deathly Hallows as her "favourite one" in the series.[182][183][184][185] She wrote on her website of the changes in the book-to-film transition, "It is simply impossible to incorporate every one of my storylines into a film that has to be kept under four hours long. Obviously films have restrictions novels do not have, constraints of time and budget; I can create dazzling effects relying on nothing but the interaction of my own and my readers' imaginations".[186]		At the 64th British Academy Film Awards in February 2011, Rowling was joined by producers David Heyman and David Barron along with directors David Yates, Alfonso Cuarón and Mike Newell in collecting the Michael Balcon Award for Outstanding British Contribution to Cinema on behalf of all the films in the series. Actors Rupert Grint and Emma Watson, who play main characters Ron Weasley and Hermione Granger, were also in attendance.[187][188]		A new series consisting of five films, beginning with Fantastic Beasts and Where to Find Them, will take place before the main series.[189] The first film was released on 8 November 2016 and the next two are due to be released in 2018 and 2020. Rowling wrote the screenplay for the first instalment, marking her first foray into screenwriting.		A number of other non-interactive media games and board games have been released such as Cluedo Harry Potter Edition, Scene It? Harry Potter and Lego Harry Potter models, which are influenced by the themes of both the novels and films.		There are thirteen Harry Potter video games, eight corresponding with the films and books and five spin-offs. The film/book-based games are produced by Electronic Arts, as was Harry Potter: Quidditch World Cup, with the game version of the first entry in the series, Philosopher's Stone, being released in November 2001. Harry Potter and the Philosopher's Stone went on to become one of the best-selling PlayStation games ever.[190] The video games were released to coincide with the films, containing scenery and details from the films as well as the tone and spirit of the books. Objectives usually occur in and around Hogwarts, along with various other magical areas. The story and design of the games follow the selected film's characterisation and plot; EA worked closely with Warner Bros. to include scenes from the films. The last game in the series, Deathly Hallows, was split, with Part 1 released in November 2010 and Part 2 debuting on consoles in July 2011. The two-part game forms the first entry to convey an intense theme of action and violence, with the gameplay revolving around a third-person shooter style format.[191][192]		The spin-off games Lego Harry Potter: Years 1–4 and Lego Harry Potter: Years 5–7 were developed by Traveller's Tales and published by Warner Bros. Interactive Entertainment. The spin-off games Book of Spells and Book of Potions were developed by SCE London Studio and utilise the Wonderbook, an augmented reality book designed to be used in conjunction with the PlayStation Move and PlayStation Eye.[193][194] The Harry Potter universe is also featured in Lego Dimensions, with the settings and side characters featured in the Harry Potter Adventure World, and Harry, Voldemort, and Hermione as playable characters.		All seven Harry Potter books have been released in unabridged audiobook versions, with Stephen Fry reading the UK editions and Jim Dale voicing the series for the American editions.[195][196]		On 20 December 2013, J. K. Rowling announced that she was working on a Harry Potter–based play for which she would be one of the producers. British theatre producers Sonia Friedman and Colin Callender will be the co-producers.[197][198]		On 26 June 2015, on the anniversary of the debut of the first book, Rowling revealed via Twitter that the Harry Potter stage play would be called Harry Potter and The Cursed Child.[199] The Production is expected to open in the summer of 2016 at London's Palace Theatre, London.[200] The first four months of tickets for the June–September performances were sold out within several hours upon release.[201] On 10 February 2016, it was announced via the Pottermore website, that the script would be released in book form, the day after the play's world premiere, making this the 8th book in the series, with events set nineteen years after the closing chapter of Harry Potter and the Deathly Hallows.[202][203]		After the success of the films and books, Universal and Warner Brothers announced they would create The Wizarding World of Harry Potter, a new Harry Potter-themed expansion to the Islands of Adventure theme park at Universal Orlando Resort in Florida. The land officially opened to the public on 18 June 2010.[204] It includes a re-creation of Hogsmeade and several rides. The flagship attraction is Harry Potter and the Forbidden Journey, which exists within a re-creation of Hogwarts School of Witchcraft and Wizardry. Other rides include Dragon Challenge, a pair of inverted roller coasters, and Flight of the Hippogriff, a family roller coaster.		Four years later, on 8 July 2014, Universal opened a Harry Potter-themed area at the Universal Studios Florida theme park. It includes a re-creation of Diagon Alley and connecting alleys and a small section of Muggle London. The flagship attraction is Harry Potter and the Escape from Gringotts roller coaster ride. Universal also added a completely functioning recreation of the Hogwarts Express connecting Kings Cross Station at Universal Studios Florida to the Hogsmeade station at Islands of Adventure. Both Hogsmeade and Diagon Alley contain many shops and restaurants from the book series, including Weasley's Wizard Wheezes and The Leaky Cauldron.		On 15 July 2014, The Wizarding World of Harry Potter opened at the Universal Studios Japan theme park in Osaka, Japan. It includes the village of Hogsmeade, Harry Potter and the Forbidden Journey ride, and Flight of the Hippogriff roller coaster.[205][206]		On 7 April 2016, The Wizarding World of Harry Potter opened at the Universal Studios Hollywood theme park near Los Angeles, California.[207][208]		In March 2011, Warner Bros. announced plans to build a tourist attraction in the United Kingdom to showcase the Harry Potter film series. Warner Bros. Studio Tour London is a behind-the-scenes walking tour featuring authentic sets, costumes and props from the film series. The attraction, Warner Bros. Studio Tour London - The Making of Harry Potter, is located at Warner Bros. Studios, Leavesden, where all eight of the Harry Potter films were made. Warner Bros. constructed two new sound stages to house and showcase the famous sets from each of the British-made productions, following a £100 million investment.[209] It opened to the public in March 2012.[210]				
Glasses, also known as eyeglasses or spectacles, are devices consisting of glass or hard plastic lenses mounted in a frame that holds them in front of a person's eyes, typically using a bridge over the nose and arms which rest over the ears. Glasses are typically used for vision correction, such as with reading glasses and glasses used for nearsightedness. Safety glasses provide eye protection against flying debris for construction workers or lab technicians; these glasses may have protection for the sides of the eyes as well as in the lenses. Some types of safety glasses are used to protect against visible and near-visible light or radiation. Glasses are worn for eye protection in some sports, such as squash. Glasses wearers may use a strap to prevent the glasses from falling off during movement or sports. Wearers of glasses that are used only part of the time may have the glasses attached to a cord that goes around their neck, to prevent the loss of the glasses.		Sunglasses allow better vision in bright daylight, and may protect one's eyes against damage from high levels of ultraviolet light. Typical sunglasses are darkened for protection against bright light or glare; some specialized glasses are clear in dark or indoor conditions, but turn into sunglasses when in bright light. Most sunglasses do not have corrective power in the lenses; however, special prescription sunglasses can be ordered. Specialized glasses may be used for viewing specific visual information (such as stereoscopy) or 3D glasses for viewing three-dimensional movies. Sometimes glasses with no corrective power in the lenses are worn simply for aesthetic or fashion purposes. Even with glasses used for vision correction, a wide range of designs are available for fashion purposes, using plastic, wire, and other materials.		People are more likely to need glasses the older they get with 93% of people between the age of 65-75 wearing corrective lenses.[1][2]		Glasses come in many types. They can be classified by their primary function, but also appear in combinations such as prescription sunglasses or safety glasses with enhanced magnification.		Corrective lenses are used to correct refractive errors by bending the light entering the eye in order to alleviate the effects of conditions such as nearsightedness (myopia), farsightedness (Hypermetropia) or astigmatism. The ability of one's eyes to accommodate their focus to near and distant focus alters over time. Also, few people have eyes that show exactly equal refractive characteristics; one may need a "stronger", (i.e. more refracting), lens than the other. A common condition in people over forty years old is presbyopia, which is caused by the eye's crystalline lens losing elasticity, progressively reducing the ability of the lens to accommodate (i.e. to focus on objects close to the eye). Corrective lenses, to bring the image back into focus on the retina, are made to conform to the prescription of an ophthalmologist or optometrist. A lensmeter can be used to verify the specifications of an existing pair of glasses. Corrective eyeglasses can significantly improve the life quality of the wearer. Not only do they enhance the wearer's visual experience, but can also reduce problems that result from eye strain, such as headaches or squinting.		Pinhole glasses are a type of corrective glasses that do not use a lens. Pinhole glasses do not actually refract the light or change focal length. Instead, they create a diffraction limited system, which has an increased depth of field, similar to using a small aperture in photography. This form of correction has many limitations that prevent it from gaining popularity in everyday use. Pinhole glasses can be made in a DIY fashion by making small holes in a piece of card which is then held in front of the eyes with a strap or cardboard arms.		The most common type of corrective lens is "single vision", which has a uniform refractive index. For people with presbyopia and hyperopia, bifocal and trifocal glasses provide two or three different refractive indices, respectively, and progressive lenses have a continuous gradient. Reading glasses provide a separate set of glasses for focusing on close-by objects. Reading glasses are available without prescription from drugstores, and offer a cheap, practical solution, though these have two simple lenses of equal power, so will not correct refraction problems like astigmatism or refractive or prismatic variations between the left and right eye. For total correction of the individual's sight, glasses complying to a recent ophthalmic prescription are required. Adjustable-focus eyeglasses might be used to replace bifocals or trifocals, or might be used to produce cheaper single-vision glasses (since they don't have to be custom-manufactured for every person).		Safety glasses are worn to protect the eyes during a variety of tasks. They are made with shatter-resistant plastic lenses to protect the eye from flying debris or other matter. Construction workers, factory workers, machinists and lab technicians are often required to wear safety glasses to shield the eyes from flying debris or hazardous splatters such as blood or chemicals. As of 2017, dentists and surgeons in Canada and other countries are required to wear safety glasses to protect against infection from patients' blood or other body fluids. There are also safety glasses for welding, which are styled like wraparound sunglasses, but with much darker lenses, for use in welding where a full sized welding helmet is inconvenient or uncomfortable. These are often called "flash goggles", because they provide protection from welding flash. Nylon frames are usually used for protection eyewear for sports because of their lightweight and flexible properties. Unlike most regular glasses, safety glasses often include protection beside the eyes as well as in front of the eyes.		Sunglasses provide improved comfort and protection against bright light and often against ultraviolet (UV) light. Photochromic lenses, which are photosensitive, darken when struck by UV light. The dark tint of the lenses in a pair of sunglasses blocks the transmission of light through the lens.		Light polarization is an added feature that can be applied to sunglass lenses. Polarization filters are positioned to remove horizontally polarized rays of light, which eliminates glare from horizontal surfaces (allowing wearers to see into water when reflected light would otherwise overwhelm the scene). Polarized sunglasses may present some difficulties for pilots since reflections from water and other structures often used to gauge altitude may be removed. Liquid crystal displays often emit polarized light making them sometimes difficult to view with polarized sunglasses. Sunglasses may be worn just for aesthetic purposes, or simply to hide the eyes. Examples of sunglasses that were popular for these reasons include teashades and mirrorshades. Many blind people wear nearly opaque glasses to hide their eyes for cosmetic reasons.		Sunglasses may also have corrective lenses, which requires a prescription. Clip-on sunglasses or sunglass clips can be attached to another pair of glasses. Some wrap-around sunglasses are large enough to be worn over top of another pair of glasses. Otherwise, many people opt to wear contact lenses to correct their vision so that standard sunglasses can be used.		The illusion of three dimensions on a two dimensional surface can be created by providing each eye with different visual information. 3D glasses create the illusion of three dimensions by filtering a signal containing information for both eyes. The signal, often light reflected off a movie screen or emitted from an electronic display, is filtered so that each eye receives a slightly different image. The filters only work for the type of signal they were designed for.		Anaglyph 3D glasses have a different colored filter for each eye, typically red and blue or red and green. A polarized 3D system on the other hand uses polarized filters. Polarized 3D glasses allow for color 3D, while the red-blue lenses produce an image with distorted coloration. An active shutter 3D system uses electronic shutters. Head-mounted displays can filter the signal electronically and then transmit light directly into the viewers eyes.		Anaglyph and polarized glasses are distributed to audiences at 3D movies. Polarized and active shutter glasses are used with many home theaters. Head-mounted displays are used by a single person, but the input signal can be shared between multiple units.		Glasses can also provide magnification that is useful for people with vision impairments or specific occupational demands. An example would be bioptics or bioptic telescopes which have small telescopes mounted on, in, or behind their regular lenses. Newer designs use smaller lightweight telescopes, which can be embedded into the corrective glass and improve aesthetic appearance (mini telescopic spectacles). They may take the form of self-contained glasses that resemble goggles or binoculars, or may be attached to existing glasses.		Yellow tinted glasses are type of glasses, with a minor yellow tint. Basically performs minor color correction, on top of reducing headaches due to lack of blinking. May also be considered minor, corrective, unprescribed glasses[citation needed]. Depending on the company, these computer or gaming glasses can also filter out high energy blue and ultra-violet light from LCD screens, fluorescent lighting, and other sources of light. This allows for reduced eye-strain[citation needed]. These glasses can be ordered as standard or prescription lenses that fit into standard optical frames.[3] Due to the blue energy blocking nature of these lenses, they also help users sleep at night along with reducing age-related macular degeneration.[4][citation needed]		The ophthalmic frame is the part of a pair of glasses which is designed to hold the lenses in proper position. Ophthalmic frames come in a variety of styles, sizes, materials, shapes, and colors.[5]		Corrective lenses can be produced in many different shapes from a circular lens called a lens blank. Lens blanks are cut to fit the shape of the frame that will hold them. Frame styles vary and fashion trends change over time, resulting in a multitude of lens shapes. For lower power lenses, there are few restrictions which allows for many trendy and fashionable shapes. Higher power lenses can cause distortion of peripheral vision and may become thick and heavy if a large lens shape is used. However, if the lens becomes too small, the field of view can be drastically reduced.		Bifocal, trifocal, and progressive lenses generally require a taller lens shape to leave room for the different segments while preserving an adequate field of view through each segment. Frames with rounded edges are the most efficient for correcting myopic prescriptions, with perfectly round frames being the most efficient. Before the advent of eyeglasses as a fashion item, when frames were constructed with only functionality in mind, virtually all eyeglasses were either round, oval, or curved octagons. It was not until glasses began to be seen as an accessory that different shapes were introduced to be more aesthetically pleasing than functional.		The use of a convex lens to form an enlarged/magnified image is discussed in Alhazen's Book of Optics (1021). Its translation into Latin from Arabic in the 12th century was instrumental to the invention of eyeglasses in 13th century Italy.[8] Englishman Robert Grosseteste's treatise De iride ("On the Rainbow"), written between 1220 and 1235, mentions using optics to "read the smallest letters at incredible distances". A few years later in 1262, Roger Bacon is also known to have written on the magnifying properties of lenses.[9]		Sunglasses, in the form of flat panes of smoky quartz, were used in China during the 12th century.[a] Similarly, the Inuit have used snow goggles for eye protection. While they did not offer any corrective benefits[11] they did improve visual acuity via the pinhole effect.		The first eyeglasses were made in Italy in about 1286, but it is not clear who the inventor was. In a sermon delivered on February 23, 1306, the Dominican friar Giordano da Pisa (ca. 1255–1311) wrote "It is not yet twenty years since there was found the art of making eyeglasses, which make for good vision... And it is so short a time that this new art, never before extant, was discovered. ... I saw the one who first discovered and practiced it, and I talked to him."[12] Giordano's colleague Friar Alessandro della Spina of Pisa (d. 1313) was soon making eyeglasses. The Ancient Chronicle of the Dominican Monastery of St. Catherine in Pisa records: "Eyeglasses, having first been made by someone else, who was unwilling to share them, he [Spina] made them and shared them with everyone with a cheerful and willing heart."[13] By 1301, there were guild regulations in Venice governing the sale of eyeglasses.[14]		The earliest pictorial evidence for the use of eyeglasses is Tommaso da Modena's 1352 portrait of the cardinal Hugh de Provence reading in a scriptorium. Another early example would be a depiction of eyeglasses found north of the Alps in an altarpiece of the church of Bad Wildungen, Germany, in 1403. These early glasses had convex lenses that could correct both hyperopia (farsightedness), and the presbyopia that commonly develops as a symptom of aging. It was not until 1604 that Johannes Kepler published the first correct explanation as to why convex and concave lenses could correct presbyopia and myopia.[b]		Early frames for glasses consisted of two magnifying glasses riveted together by the handles so that they could grip the nose. These are referred to as "rivet spectacles". The earliest surviving examples were found under the floorboards at Kloster Wienhausen, a convent near Celle in Germany; they have been dated to circa 1400.[17]		Claims that Salvino degli Armati of Florence invented eyeglasses have been exposed as hoaxes.[18][19]		Marco Polo is sometimes claimed to have encountered eyeglasses during his travels in China in the 13th century, no such statement appears in his accounts.[20][21] Indeed, the earliest mentions of eyeglasses in China occur in the 15th century and those Chinese sources state that eyeglasses were imported.[22]		It is also sometimes claimed that glasses were first invented in India. In 1907 Professor Berthold Laufer stated in his history of glasses that "the opinion that spectacles originated in India is of the greatest probability and that spectacles must have been known in India earlier than in Europe".[23] However, Joseph Needham showed that the mention of glasses in the manuscript Laufer used to justify the prior invention of them in Asia did not exist in older versions of that manuscript, and the reference to them in later versions was added during the Ming dynasty.[24] In an 1971 article in the British Journal of Ophthalmology it was argued that it: "...is therefore most likely that the use of lenses reached Europe via the Arabs, as did Hindu mathematics and the ophthalmological works of the ancient Hindu surgeon Susruta",[25] but all dates given are well after the existence of eyeglasses in Italy was established, and there had been significant shipments of eye glasses from Italy to the Middle East, with one shipment as large as 24,000 glasses.[26]		The American scientist Benjamin Franklin, who suffered from both myopia and presbyopia, invented bifocals. Serious historians have from time to time produced evidence to suggest that others may have preceded him in the invention; however, a correspondence between George Whatley and John Fenno, editor of The Gazette of the United States, suggested that Franklin had indeed invented bifocals, and perhaps 50 years earlier than had been originally thought.[27] The first lenses for correcting astigmatism were designed by the British astronomer George Airy in 1825.[28]		Over time, the construction of frames for glasses also evolved. Early eyepieces were designed to be either held in place by hand or by exerting pressure on the nose (pince-nez). Girolamo Savonarola suggested that eyepieces could be held in place by a ribbon passed over the wearer's head, this in turn secured by the weight of a hat. The modern style of glasses, held by temples passing over the ears, was developed some time before 1727, possibly by the British optician Edward Scarlett. These designs were not immediately successful, however, and various styles with attached handles such as "scissors-glasses" and lorgnettes were also fashionable from the second half of the 18th century and into the early 19th century.		In the early 20th century, Moritz von Rohr and Zeiss (with the assistance of H. Boegehold and A. Sonnefeld[29]), developed the Zeiss Punktal spherical point-focus lenses that dominated the eyeglass lens field for many years. In 2008, Joshua Silver designed eyewear with adjustable corrective glasses. They work by silicone liquid, a syringe, and a pressure mechanism.[30]		Despite the increasing popularity of contact lenses and laser corrective eye surgery, glasses remain very common, as their technology has improved. For instance, it is now possible to purchase frames made of special memory metal alloys that return to their correct shape after being bent. Other frames have spring-loaded hinges. Either of these designs offers dramatically better ability to withstand the stresses of daily wear and the occasional accident. Modern frames are also often made from strong, light-weight materials such as titanium alloys, which were not available in earlier times.		In the 1930s, "spectacles" were described as "medical appliances."[31] Wearing spectacles was sometimes considered socially humiliating. In the 1970s, fashionable glasses started to become available through manufacturers, and the government also recognized the demand for stylized eyewear.[31]		Graham Pullin describes how devices for disability, like glasses, have traditionally been designed to camouflage against the skin and restore ability without being visible.[31] In the past, design for disability has "been less about projecting a positive image as about trying not to project an image at all."[31] Pullin uses the example of spectacles, traditionally categorized as a medical device for "patients", and outlines how they are now described as eyewear: a fashionable accessory.[31] Much like other fashion designs and accessories, eyewear is created by designers, has reputable labels, and comes in collections, by season and designer.[31] It is becoming more common for consumers purchase eyewear with clear, non-prescription lenses, illustrating that glasses are no longer a social stigma, but a fashionable accessory that "frames your face."[31]		Some organizations like Lions Clubs International,[32] Unite For Sight,[33], ReSpectacle,[34] and New Eyes for the Needy provide a way to donate glasses and sunglasses. Unite For Sight has redistributed more than 200,000 pairs.[35]		Many people require glasses for the reasons listed above. There are many shapes, colors, and materials that can be used when designing frames and lenses that can be utilized in various combinations. Oftentimes, the selection of a frame is made based on how it will affect the appearance of the wearer. Some people with good natural eyesight like to wear eyeglasses as a style accessory.		For most of their history, eyeglasses were seen as unfashionable, and carried several potentially negative connotations: wearing glasses caused individuals to be stigmatized and stereotyped as pious clergymen (as those in religious vocation were the most likely to be literate and therefore the most likely to need reading glasses), elderly, or physically weak and passive.[36][37] The stigma began to fall away in the early 1900s when the popular Theodore Roosevelt was regularly photographed wearing eyeglasses, and in the 1910s when popular comedian Harold Lloyd began wearing a pair of horn-rimmed glasses as the "Glasses" character in his films.[36][37]		Since, eyeglasses have become an acceptable fashion item and often act as a key component in individuals' personal image. Musicians Buddy Holly and John Lennon became synonymous with the styles of eye-glasses they wore to the point that thick, black horn-rimmed glasses are often called "Buddy Holly glasses" and perfectly round metal eyeglass frames called "John Lennon (or Harry Potter) Glasses." British comedic actor Eric Sykes was known in the United Kingdom for wearing thick, square, horn-rimmed glasses, which were in fact a sophisticated hearing aid that alleviated his deafness by allowing him to "hear" vibrations.[38] Some celebrities have become so associated with their eyeglasses that they continued to wear them even after taking alternate measures against vision problems: United States Senator Barry Goldwater and comedian Drew Carey continued to wear non-prescription glasses after being fitted for contacts and getting laser eye surgery, respectively.		Other celebrities have used glasses to differentiate themselves from the characters they play, such as Anne Kirkbride, who wore oversized, 1980s-style round horn-rimmed glasses as Deirdre Barlow in the soap opera Coronation Street, and Masaharu Morimoto, who wears glasses to separate his professional persona as a chef from his stage persona as Iron Chef Japanese. In 2012 some NBA players wear lensless glasses with thick plastic frames like horn-rimmed glasses during post-game interviews, geek chic that draws comparisons to Steve Urkel.[39][40]		In superhero fiction, eyeglasses have become a standard component of various heroes' disguises (as masks), allowing them to adopt a nondescript demeanor when they are not in their superhero persona: Superman is well known for wearing 1950s style horn-rimmed glasses as Clark Kent, while Wonder Woman wears either round, Harold Lloyd style glasses or 1970s style bug-eye glasses as Diana Prince. An example of the halo effect is seen in the stereotype that those who wear glasses are intelligent.		In the 20th century, eyeglasses came to be considered a component of fashion; as such, various different styles have come in and out of popularity. Most are still in regular use, albeit with varying degrees of frequency.				
Orientalism is a term that is used by art historians, literary and cultural studies scholars for the imitation or depiction of aspects in Middle Eastern, South Asian, and East Asian cultures (Eastern cultures). These depictions are usually done by writers, designers and artists from the West. In particular, Orientalist painting, depicting more specifically "the Middle East",[1] was one of the many specialisms of 19th-century academic art, and the literature of Western countries took a similar interest in Oriental themes.		Since the publication of Edward Said's Orientalism in 1978, much academic discourse has begun to use the term "Orientalism" to refer to a general patronizing Western attitude towards Middle Eastern, Asian and North African societies. In Said's analysis, the West essentializes these societies as static and undeveloped—thereby fabricating a view of Oriental culture that can be studied, depicted, and reproduced. Implicit in this fabrication, writes Said, is the idea that Western society is developed, rational, flexible, and superior.[2]						Orientalism refers to the Orient, in reference and opposition to the Occident; the East and the West.[3][4] The word Orient entered the English language as the Middle French orient. The root word oriēns, from the Latin Oriēns, has synonymous denotations: The eastern part of the world; the sky whence comes the sun; the east; the rising sun, etc.; yet the denotation changed as a term of geography. In the "Monk's Tale" (1375), Geoffrey Chaucer wrote: "That they conquered many regnes grete / In the orient, with many a fair citee." The term "orient" refers to countries east of the Mediterranean Sea and Southern Europe. In Place of Fear (1952), Aneurin Bevan used an expanded denotation of the Orient that comprehended East Asia: "the awakening of the Orient under the impact of Western ideas". Edward Said said that Orientalism "enables the political, economic, cultural and social domination of the West, not just during colonial times, but also in the present."[5]		In art history, the term Orientalism refers to the works of the Western artists who specialized in Oriental subjects, produced from their travels in Western Asia, during the 19th century. In that time, artists and scholars were described as Orientalists, especially in France, where the dismissive use of the term Orientalist was made popular by the art critic Jules-Antoine Castagnary.[6] Despite such social disdain for a style of representational art, the French Society of Orientalist Painters was founded in 1893, with Jean-Léon Gérôme as the honorary president;[7] whereas in Britain, the term Orientalist identified "an artist".[8]		In the 18th and 19th centuries, the term Orientalist identified a scholar who specialized in the languages and literatures of the Eastern world. Among such scholars were British officials of the East India Company, who said that the Arab culture, the culture of India, and the Islamic cultures should be studied as equal to the cultures of Europe.[9] Among such scholars is the philologist William Jones, whose studies of Indo-European languages established modern philology. British imperial strategy in India favored Orientalism as a technique for developing good relations with the natives—until the 1820s when the influence of "anglicists" such as Thomas Babington Macaulay and John Stuart Mill led to the promotion of Anglocentric education.[10]		Additionally, Hebraism and Jewish studies gained popularity among British and German scholars in the 18th and 19th century.[11] The academic field of Oriental studies, which comprehended the cultures of the Near East and the Far East, became the fields of Asian studies and Middle Eastern studies.		In the book Orientalism (1978), the cultural critic Edward Said redefined the term Orientalism to describe a pervasive Western tradition — academic and artistic — of prejudiced outsider-interpretations of the Eastern world, which was shaped by the cultural attitudes of European imperialism in the 18th and 19th centuries.[12] The thesis of Orientalism develops Antonio Gramsci's theory of cultural hegemony, and Michel Foucault's theorisation of discourse (the knowledge-and-power relation) to criticise the scholarly tradition of Oriental studies. Said criticised contemporary scholars who perpetuated the tradition of outsider-interpretation of Arabo-Islamic cultures, especially Bernard Lewis and Fouad Ajami.[13][14]		The analyses are of Orientalism in European literature, especially French literature, and do not analyse visual art and Orientalist painting. In that vein, the art historian Linda Nochlin applied Said's methods of critical analysis to art, "with uneven results".[15] In the academy, the book Orientalism (1978) became a foundational text of post-colonial cultural studies. [16] Moreover, in relation to the cultural institution of citizenship, Orientalism has rendered the concept of citizenship as a problem of epistemology, because citizenship originated as a social institution of the Western world; as such, the problem of defining citizenship reconfigures the idea of Europe in time of crises.[17]		Furthermore, Said said that Orientalism, as an "idea of representation is a theoretical one: The Orient is a stage on which the whole East is confined" in order to make the Eastern world "less fearsome to the West";[18] that the developing world, primarily the West, is the cause of colonialism.[19] Moreover, in Empire: A Very Short Introduction (2000), Stephen Howe agreed with Said that Western nations and their empires were created by the exploitation of underdeveloped countries, by the extraction of wealth and labour from one country to another country.[20]		The Moresque style of Renaissance ornament is a European adaptation of the Islamic arabesque that began in the late 15th century and was to be used in some types of work, such as bookbinding, until almost the present day. Early architectural use of motifs lifted from the Indian subcontinent is known as Indo-Saracenic Revival architecture. One of the earliest examples is the façade of Guildhall, London (1788–1789). The style gained momentum in the west with the publication of views of India by William Hodges, and William and Thomas Daniell from about 1795. Examples of "Hindoo" architecture are Sezincote House (c. 1805) in Gloucestershire, built for a nabob returned from Bengal, and the Royal Pavilion in Brighton.		Turquerie, which began as early as the late 15th century, continued until at least the 18th century, and included both the use of "Turkish" styles in the decorative arts, the adoption of Turkish costume at times, and interest in art depicting the Ottoman Empire itself. Venice, the traditional trading partner of the Ottomans, was the earliest centre, with France becoming more prominent in the 18th century.		Chinoiserie is the catch-all term for the fashion for Chinese themes in decoration in Western Europe, beginning in the late 17th century and peaking in waves, especially Rococo Chinoiserie, c. 1740–1770. From the Renaissance to the 18th century, Western designers attempted to imitate the technical sophistication of Chinese ceramics with only partial success. Early hints of Chinoiserie appeared in the 17th century in nations with active East India companies: England (the East India Company), Denmark (the Danish East India Company), the Netherlands (the Dutch East India Company) and France (the French East India Company). Tin-glazed pottery made at Delft and other Dutch towns adopted genuine Ming-era blue and white porcelain from the early 17th century. Early ceramic wares made at Meissen and other centers of true porcelain imitated Chinese shapes for dishes, vases and teawares (see Chinese export porcelain).		Pleasure pavilions in "Chinese taste" appeared in the formal parterres of late Baroque and Rococo German palaces, and in tile panels at Aranjuez near Madrid. Thomas Chippendale's mahogany tea tables and china cabinets, especially, were embellished with fretwork glazing and railings, c. 1753–70. Sober homages to early Xing scholars' furnishings were also naturalized, as the tang evolved into a mid-Georgian side table and squared slat-back armchairs that suited English gentlemen as well as Chinese scholars. Not every adaptation of Chinese design principles falls within mainstream "chinoiserie". Chinoiserie media included imitations of lacquer and painted tin (tôle) ware that imitated japanning, early painted wallpapers in sheets, and ceramic figurines and table ornaments. Small pagodas appeared on chimneypieces and full-sized ones in gardens. Kew has a magnificent garden pagoda designed by William Chambers. The Wilhelma (1846) in Stuttgart is an example of Moorish Revival architecture. Leighton House, built for the artist Frederic Leighton, has a conventional facade but elaborate Arab-style interiors, including original Islamic tiles and other elements as well as Victorian Orientalizing work.		After 1860, Japonism, sparked by the importing of ukiyo-e, became an important influence in the western arts. In particular, many modern French artists such as Claude Monet and Edgar Degas were influenced by the Japanese style. Mary Cassatt, an American artist who worked in France, used elements of combined patterns, flat planes and shifting perspective of Japanese prints in her own images.[21] The paintings of James Abbott McNeill Whistler's "The Peacock Room" demonstrated how he used aspects of Japanese tradition and are some of the finest works of the genre. California architects Greene and Greene were inspired by Japanese elements in their design of the Gamble House and other buildings.		Egyptian Revival architecture was popular mostly in the early and mid-19th century and Indo-Saracenic Revival architecture or Moorish Revival architecture, covering a variety of general Islamic or Indian features, in the later part of the century; "Saracenic" referred to styles from Arabic-speaking areas. Both were sometimes used in the Orient itself by colonial governments.		Depictions of Islamic "Moors" and "Turks" (imprecisely named Muslim groups of southern Europe, North Africa and West Asia) can be found in Medieval, Renaissance, and Baroque art. In Biblical scenes in Early Netherlandish painting, secondary figures, especially Romans, were given exotic costumes that distantly reflected the clothes of the Near East. The Three Magi in Nativity scenes were an especial focus for this. In general art with Biblical settings would not be considered as Orientalist except where contemporary or historicist Middle Eastern detail or settings is a feature of works, as with some paintings by Gentile Bellini and others, and a number of 19th century works. Renaissance Venice had a phase of particular interest in depictions of the Ottoman Empire in painting and prints. Gentile Bellini, who travelled to Constantinople and painted the Sultan, and Vittore Carpaccio were the leading painters. By then the depictions were more accurate, with men typically dressed all in white. The depiction of Oriental carpets in Renaissance painting sometimes draws from Orientalist interest, but more often just reflects the prestige these expensive objects had in the period.[22]		Jean-Étienne Liotard (1702–1789) visited Istanbul and painted numerous pastels of Turkish domestic scenes; he also continued to wear Turkish attire for much of the time when he was back in Europe. The ambitious Scottish 18th-century artist Gavin Hamilton found a solution to the problem of using modern dress, considered unheroic and inelegant, in history painting by using Middle Eastern settings with Europeans wearing local costume, as travelers were advised to do. His huge James Dawkins and Robert Wood Discovering the Ruins of Palmyra (1758, now Edinburgh) elevates tourism to the heroic, with the two travelers wearing what look very like togas. Many travelers had themselves painted in exotic Eastern dress on their return, including Lord Byron, as did many who had never left Europe, including Madame de Pompadour.[23] The growing French interest in exotic Oriental luxury and lack of liberty in the 18th century to some extent reflected a pointed analogy with France's own absolute monarchy.[24] Byron's poetry was highly influential in introducing Europe to the heady cocktail of Romanticism in exotic Oriental settings which was to dominate 19th century Oriental art.		French Orientalist painting was transformed by Napoleon's ultimately unsuccessful invasion of Egypt and Syria in 1798–1801, which stimulated great public interest in Egyptology, and was also recorded in subsequent years by Napoleon's court painters, especially Antoine-Jean Gros, although the Middle Eastern campaign was not one on which he accompanied the army. Two of his most successful paintings, Bonaparte Visiting the Plague Victims of Jaffa (1804) and Battle of Abukir (1806) focus on the Emperor, as he was by then, but include many Egyptian figures, as does the less effective Napoleon at the Battle of the Pyramids (1810). Anne-Louis Girodet de Roussy-Trioson's La Révolte du Caire (1810) was another large and prominent example. A well-illustrated Description de l'Égypte was published by the French Government in twenty volumes between 1809 and 1828, concentrating on antiquities.[25]		Eugène Delacroix's first great success, The Massacre at Chios (1824) was painted before he visited Greece or the East, and followed his friend Théodore Géricault's The Raft of the Medusa in showing a recent incident in distant parts that had aroused public opinion. Greece was still fighting for independence from the Ottomans, and was effectively as exotic as the more Near Eastern parts of the empire. Delacroix followed up with Greece on the Ruins of Missolonghi (1827), commemorating a siege of the previous year, and The Death of Sardanapalus, inspired by Lord Byron, which although set in antiquity has been credited with beginning the mixture of sex, violence, lassitude and exoticism which runs through much French Orientalist painting.[26] In 1832, Delacroix finally visited what is now Algeria, recently conquered by the French, and Morocco, as part of a diplomatic mission to the Sultan of Morocco. He was greatly struck by what he saw, comparing the North African way of life to that of the Ancient Romans, and continued to paint subjects from his trip on his return to France. Like many later Orientalist painters, he was frustrated by the difficulty of sketching women, and many of his scenes featured Jews or warriors on horses. However, he was apparently able to get into the women's' quarters or harem of a house to sketch what became Women of Algiers; few later harem scenes had this claim to authenticity.[27]		When Ingres, the director of the French Académie de peinture, painted a highly colored vision of a Turkish bath, he made his eroticized Orient publicly acceptable by his diffuse generalizing of the female forms (who might all have been the same model). More open sensuality was seen as acceptable in the exotic Orient.[28] This imagery persisted in art into the early 20th century, as evidenced in Henri Matisse's orientalist semi-nudes from his Nice period, and his use of Oriental costumes and patterns. Ingres' pupil Théodore Chassériau (1819–1856) had already achieved success with his nude The Toilette of Esther (1841, Louvre) and equestrian portrait of Ali-Ben-Hamet, Caliph of Constantine and Chief of the Haractas, Followed by his Escort (1846) before he first visited the East, but in later decades the steamship made travel much easier and increasing numbers of artists traveled to the Middle East and beyond, painting a wide range of Oriental scenes.		In many of these works, they portrayed the Orient as exotic, colorful and sensual, not to say stereotyped. Such works typically concentrated on Oriental Islamic, Hebraic, and other Semitic cultures, as those were the ones visited by artists as France became more engaged in North Africa. French artists such as Eugène Delacroix, Jean-Léon Gérôme and Jean-Auguste-Dominique Ingres painted many works depicting Islamic culture, often including lounging odalisques. They stressed both lassitude and visual spectacle. Other scenes, especially in genre painting, have been seen as either closely comparable to their equivalents set in modern-day or historical Europe, or as also reflecting an Orientalist mind-set in the Saidian sense of the term. Gérôme was the precursor, and often the master, of a number of French painters in the later part of the century whose works were often frankly salacious, frequently featuring scenes in harems, public baths and slave auctions (the last two also available with classical decor), and responsible, with others, for "the equation of Orientalism with the nude in pornographic mode";[29] (Gallery, below)		Though British political interest in the territories of the unravelling Ottoman Empire was as intense as in France, it was mostly more discreetly exercised. The origins of British Orientalist 19th-century painting owe more to religion than military conquest or the search for plausible locations for naked women. The leading British genre painter, Sir David Wilkie was 55 when he travelled to Istanbul and Jerusalem in 1840, dying off Gibraltar during the return voyage. Though not noted as a religious painter, Wilkie made the trip with a Protestant agenda to reform religious painting, as he believed that: "a Martin Luther in painting is as much called for as in theology, to sweep away the abuses by which our divine pursuit is encumbered", by which he meant traditional Christian iconography. He hoped to find more authentic settings and decor for Biblical subjects at their original location, though his death prevented more than studies being made. Other artists including the Pre-Raphaelite William Holman Hunt and David Roberts had similar motivations,[30] giving an emphasis on realism in British Orientalist art from the start.[31] The French artist James Tissot also used contemporary Middle Eastern landscape and decor for Biblical subjects, with little regard for historical costumes or other fittings.		William Holman Hunt produced a number of major paintings of Biblical subjects drawing on his Middle Eastern travels, improvising variants of contemporary Arab costume and furnishings to avoid specifically Islamic styles, and also some landscapes and genre subjects. The biblical subjects included The Scapegoat (1856), The Finding of the Saviour in the Temple (1860), and The Shadow of Death (1871). The Miracle of the Holy Fire (1899) was intended as a picturesque satire on the local Eastern Christians, of whom, like most English visitors, Hunt took a very dim view. His A Street Scene in Cairo; The Lantern-Maker's Courtship (1854–61) is a rare contemporary narrative scene, as the young man feels his fiancé's face, which he is not allowed to see, through her veil, as a Westerner in the background beats his way up the street with his stick.[32] This a rare intrusion of a clearly contemporary figure into an Orientalist scene; mostly they claim the picturesqueness of the historical painting so popular at the time, without the trouble of researching authentic costumes and settings.		When Gérôme exhibited For Sale; Slaves at Cairo at the Royal Academy in London in 1871, it was "widely found offensive", perhaps partly because the British liked to think they had successfully suppressed the slave trade in Egypt, also for cruelty and "representing fleshiness for its own sake".[33] But Rana Kabbani believes that "French Orientalist painting, as exemplified by the works of Gérôme, may appear more sensual, gaudy, gory and sexually explicit than its British counterpart, but this is a difference of style not substance ... Similar strains of fascination and repulsion convulsed their artists"[34] Nonetheless, nudity and violence are more evident in British paintings set in the ancient world, and "the iconography of the odalisque ... the Oriental sex slave whose image is offered up to the viewer as freely as she herself supposedly was to her master – is almost entirely French in origin",[28] though taken up with enthusiasm by Italian and other painters.		John Frederick Lewis, who lived for several years in a traditional mansion in Cairo, painted highly detailed works showing both realistic genre scenes of Middle Eastern life and more idealized scenes in upper class Egyptian interiors with no traces of Western cultural influence yet apparent. His careful and seemingly affectionate representation of Islamic architecture, furnishings, screens, and costumes set new standards of realism, which influenced other artists, including Gérôme in his later works. He "never painted a nude", and his wife modelled for several of his harem scenes,[35] which, with the rare examples by the classicist painter Lord Leighton, imagine "the harem as a place of almost English domesticity, ... [where]... women's fully clothed respectability suggests a moral healthiness to go with their natural good looks".[28]		Jean-Jules-Antoine Lecomte du Nouÿ, The White Slave, 1888		Fernand Cormon, The Deposed Favourite, 1872		Jean-Léon Gérôme (1824–1904), Pool in a Harem, c. 1876		Jean Auguste Dominique Ingres, Grande Odalisque, 1814		Jean Auguste Dominique Ingres, with the assistance of his pupil Paul Flandrin, Odalisque with Slave, 1839		Ferdinand Max Bredt (1860–1921) Turkish ladies 1893		Giulio Rosati, Inspection of New Arrivals, 1858–1917, Circassian beauties being inspected		John Frederick Lewis, The Reception, 1873		Other artists concentrated on landscape painting, often of desert scenes, including Richard Dadd and Edward Lear. David Roberts (1796–1864) produced architectural and landscape views, many of antiquities, and published very successful books of lithographs from them.[36]		Russian Orientalist art was largely concerned with the areas of Central Asia that Russia was conquering during the century, and also in historical painting with the Mongols who had dominated Russia for much of the Middle Ages, who were rarely shown in a good light. Nationalist historical painting in Central Europe and the Balkans dwelt on Turkish oppression, with battle scenes and maidens about to be raped.		The Saidian analysis has not prevented a strong revival of interest in, and collecting of, 19th century Orientalist works since the 1970s, the latter was in large part led by Middle Eastern buyers.[37]		Authors and composers are not commonly referred to as "Orientalist" in the way that artists are, and relatively few specialized in Oriental topics or styles, or are even best known for their works including them. But many major figures, from Mozart to Flaubert, have produced significant works with Oriental subjects or treatments. Lord Byron with his four long "Turkish tales" in poetry, is one of the most important writers to make exotic fantasy Oriental settings a significant theme in the literature of Romanticism. Verdi's opera Aida (1871) is set in Egypt as portrayed through the content and the visual spectacle. "Aida" depicts a militaristic Egypt's tyranny over Ethiopia.[38]		Irish Orientalism had a particular character, drawing on various beliefs about early historical links between Ireland and the East, few of which are now regarded as historically correct. The mythical Milesians are one example of this. The Irish were also conscious of the views of other nations seeing them as comparably backward to the East, and Europe's "backyard Orient".[39]		In music, Orientalism may be applied to styles occurring in different periods, such as the alla Turca, used by multiple composers including Mozart and Beethoven.[40] The American musicologist Richard Taruskin has identified in 19th-century Russian music a strain of Orientalism: "the East as a sign or metaphor, as imaginary geography, as historical fiction, as the reduced and totalized other against which we construct our (not less reduced and totalized) sense of ourselves".[41] Taruskin concedes Russian composers, unlike those in France and Germany, felt an "ambivalence" to the theme since "Russia was a contiguous empire in which Europeans, living side by side with 'orientals', identified (and intermarried) with them far more than in the case of other colonial powers".[42]		Nonetheless, Taruskin characterizes Orientalism in Romantic Russian music has having melodies "full of close little ornaments and melismas",[43] chromatic accompanying lines, drone bass[44]—characteristics which were used by Glinka, Balakirev, Borodin, Rimsky-Korsakov, Lyapunov, and Rachmaninov. These musical characteristics evoke "not just the East, but the seductive East that emasculates, enslaves, renders passive. In a word, it signifies the promise of the experience of nega, a prime attribute of the orient as imagined by the Russians. ... In opera and song, nega often simply denotes S-E-X a la russe, desired or achieved."[44]		Orientalism is also traceable in music that is considered to have effects of exoticism, including the japonisme in Claude Debussy's piano music all the way to the sitar being used in recordings by The Beatles.[40]		The Romantic movement in literature began in 1785 and ended around 1830. The term "Romantic" references the ideas and culture that writers of the time reflected in their work. During this time, the culture and objects of the East began to have a profound effect on Europe. Extensive traveling by artists and members of the European elite brought travelogues and sensational tales back to the West creating a great interest in all things "foreign". Romantic Orientalism incorporates African and Asian geographic locations, well-known colonial and "native" personalities, folklore, and philosophies to create a literary environment of colonial exploration from a distinctly European worldview. The current trend in analysis of this movement references a belief in this literature as a mode to justify European colonial endeavors with the expansion of territory.[45]		In his novel Salammbô, Gustave Flaubert used ancient Carthage in North Africa as a foil to ancient Rome. He portrayed its culture as morally corrupting and suffused with dangerously alluring eroticism. This novel proved hugely influential on later portrayals of ancient Semitic cultures.		Said argues that the continuity of Orientalism into the present can be found in influential images, particularly through Hollywood, as the West has now grown to include the United States of America.[46] Many blockbuster movies, such as the Indiana Jones series, The Mummy films, and even the Disney Aladdin movies demonstrate the imagined geographies of the East.[46] The movies all portray the lead heroic characters as being from the Western world, while the villains come from the East.[46] The representation of the Orient has continued in film, although this representation does not necessarily have any truth to it.		The overly sexualized character of Jasmine in Aladdin is simply a continuation of the paintings from the 19th century where women were represented as erotic, sexualized male fantasies.[47]		In The Tea House of the August Moon, as argued by Pedro Iacobelli, there are tropes of orientalism. He notes, that the film "tells us more about the Americans and the American's image of Okinawa rather than about the Okinawan people".[48]The film characterizes the Okinawans as "merry but backward" and "de-politicized" which ignored Okinawan political protests over forceful land acquisition by the American military at the time.		Kimiko Akita, in "Orientalism and the Binary of Fact and Fiction in Memoirs of a Geisha", argues that Memoirs of a Geisha contains orientalist tropes and deep "cultural misrepresentations".[49] She states that Memoirs of a Geisha "reinforces the idea of Japanese culture and geisha as exotic, backward, irrational, dirty, profane, promiscuous, bizarre, and enigmatic".		During the Romantic Period of the 19th century, ballet developed a preoccupation with the exotic. This exoticism ranged from ballets set in Scotland to those based on ethereal creatures. By the later part of the century, ballets were capturing the presumed essence of the mysterious East. These ballets often included sexual themes and tended to be based on assumptions of people rather than on concrete facts. Orientalism is apparent in numerous ballets.		The Orient motivated several major ballets, which have survived since the late nineteenth and early twentieth centuries. Le Corsaire premiered in 1856 at the Paris Opera, with choreography by Joseph Mazilier.[50] Marius Petipa re-choreographed the ballet for the Maryinsky Ballet in St. Petersburg, Russia in 1899.[50] Its complex storyline, loosely based on Lord Byron's poem,[51] takes place in Turkey and focuses on a love story between a pirate and a beautiful slave girl. Scenes include a bazaar where women are sold to men as slaves, and the Pasha's Palace, which features his harem of wives.[50] In 1877, Marius Petipa choreographed La Bayadere, the love story of an Indian temple dancer and Indian warrior. This ballet was based on Kalidasa's play Sakuntala.[52] La Bayadere used vaguely Indian costuming, and incorporated Indian inspired hand gestures into classical ballet. In addition, it included a 'Hindu Dance,' motivated by Kathak, an Indian dance form.[52] Another ballet, Sheherazade, choreographed by Michel Fokine in 1910 to music by Nicholas Rimsky-Korsakov, is a story involving a shah's wife and her illicit relations with a Golden Slave, originally played by Vaslav Nijinsky.[52] The ballet's controversial fixation on sex includes an orgy in an oriental harem. When the shah discovers the actions of his numerous wives and their lovers, he orders the deaths of those involved.[52] Sheherazade was loosely based on folktales of questionable authenticity.		Several lesser-known ballets of the late nineteenth and early twentieth century also reveal Orientalism. For instance, in Petipa's Pharaoh's Daughter (1862), an Englishman imagines himself, in an opium-induced dream, as an Egyptian boy who wins the love of the Pharaoh's daughter, Aspicia.[52] Aspicia's costume consisted of 'Egyptian' décor on a tutu.[52] Another ballet, Hippolyte Monplaisir's Brahma, which premiered in 1868 in La Scala, Italy,[53] is a story that involves romantic relations between a slave girl and Brahma, the Hindu god, when he visits earth.[52] In addition, in 1909, Serge Diagilev included Cleopatra in Ballet's Russe's repertory. With its theme of sex, this revision of Fokine's Une Nuit d' Egypte combined the "exoticism and grandeur" that audiences of this time craved.[52]		As one of the pioneers of modern dance in America, Ruth St Denis also explored Orientalism in her dancing. Her dances were not authentic; she drew inspiration from photographs, books, and later from museums in Europe.[54] Yet, the exoticism of her dances catered to the interests of society women in America.[52] She included Radha and The Cobras in her 'Indian' program in 1906. In addition, she found success in Europe with another Indian-themed ballet, The Nautch in 1908. In 1909, upon her return to America, St Denis created her first 'Egyptian' work, Egypta.[52] Her preference for Orientalism continued, culminating with Ishtar of the Seven Gates in 1923, about a Babylonian goddess.[52]		While Orientalism in dance climaxed in the late nineteenth and early twentieth centuries, it is still present in modern times. For instance, major ballet companies regularly perform Le Corsaire, La Bayadere, and Sheherazade. Furthermore, Orientalism is also found within newer versions of ballets. In versions of The Nutcracker, such as the 2010 American Ballet Theatre production, the Chinese dance uses an arm position with the arms bent at a ninety-degree angle and the index fingers pointed upwards, while the Arabian dance uses two dimensional bent arm movements. Inspired by ballets of the past, stereotypical 'Oriental' movements and arm positions have developed and remain.		An exchange of Western and Eastern ideas about spirituality developed as the West traded with and established colonies in Asia.[55] The first Western translation of a Sanskrit text appeared in 1785,[56] marking the growing interest in Indian culture and languages.[57] Translations of the Upanishads, which Arthur Schopenhauer called "the consolation of my life", first appeared in 1801 and 1802.[58][note 1] Early translations also appeared in other European languages.[60] 19th-century transcendentalism was influenced by Asian spirituality, prompting Ralph Waldo Emerson (1803–1882) to pioneer the idea of spirituality as a distinct field.[61]		A major force in the mutual influence of Eastern and Western spirituality and religiosity was the Theosophical Society,[62][63] a group searching for ancient wisdom from the East and spreading Eastern religious ideas in the West.[64][55] One of its salient features was the belief in "Masters of Wisdom",[65][note 2] "beings, human or once human, who have transcended the normal frontiers of knowledge, and who make their wisdom available to others".[65] The Theosophical Society also spread Western ideas in the East, contributing to its modernisation and a growing nationalism in the Asian colonies.[55]		The Theosophical Society had a major influence on Buddhist modernism[55] and Hindu reform movements.[63][55] Between 1878 and 1882, the Society and the Arya Samaj were united as the Theosophical Society of the Arya Samaj.[66] Helena Blavatsky, along with H. S. Olcott and Anagarika Dharmapala, was instrumental in the Western transmission and revival of Theravada Buddhism.[67][68][69]		Another major influence was Vivekananda,[70][71] who popularised his modernised interpretation[72] of Advaita Vedanta during the later 19th and early 20th century in both India and the West,[71] emphasising anubhava ("personal experience") over scriptural authority.[73]		The term "Occidentalism" is often used to refer to negative views of the Western world found in Eastern societies and is founded on the sense of nationalism that spread in reaction to colonialism.[74]		The action of "othering" cultures occurs when groups are labeled as different due to characteristics that distinguish them from the perceived norm.[75] Edward Said, the author of the book Orientalism, argued that western powers and influential individuals such as social scientists and artists othered "the Orient".[75] The evolution of ideologies is often initially embedded in the language, and continues to ripple through the fabric of society by taking over the culture, economy and political sphere.[76]		Much of Said's criticism of Western Orientalism is based on what he describes as articularizing trends. These ideologies are present in Asian works by Indian, Chinese, and Japanese writers and artists, in their views of Western culture and tradition.		A particularly significant development is the manner in which Orientalism has taken shape in non-Western cinema, as for instance in Hindi cinema.		Said has been accused of Occidentalizing the west in his critique Orientalism, i.e. of being guilty of falsely characterizing the West in the same way that he accuses Western scholars of falsely characterizing the East.[77] Said essentialized the west by creating a homogeneous image of the area. Currently, the west consists not only of Europe, but also the United States, which has become more influential and dominant over the years.[77]		
The Predator (also known as Yautja or Hish-Qu-Ten) is a fictional extraterrestrial species featured in the Predator science-fiction franchise, characterized by its trophy hunting of other species for sport. First introduced in 1987 as the main antagonist of the film Predator, the Predator creatures returned in the sequels Predator 2 (1990) and Predators (2010), the upcoming Shane Black installment The Predator (2018), and the crossover franchise Alien vs. Predator (2004) and Aliens vs. Predator: Requiem (2007).		The Predator has been the subject of numerous novels, video games, and comic books, both on their own and as part of the Alien vs. Predator crossover imprint. Although a definitive name for the species is not given in the films, the names Predators, Yautja[1] (E-wat-ya), Hish-qu-Ten[2] have been alternatively used in the expanded universe, suggesting that two unique "Predator-species" exist. Created by brothers Jim and John Thomas, the Predators are depicted as large, sapient and sentient humanoid creatures who possess advanced technology, such as active camouflage, directed-energy weapons, and interstellar travel.		The Predator design is credited to special effects artist Stan Winston. While flying to Japan with Aliens director James Cameron, Winston, who had been hired to design the Predator, was doing concept art on the flight. Cameron saw what he was drawing and said, "I always wanted to see something with mandibles." Winston then included them in his designs.[3] Stan Winston's studio created all of the physical effects for Predator and Predator 2, creating the body suit for actor Kevin Peter Hall and the mechanical facial effects. The studio was hired after attempts to create a convincing monster (including Jean-Claude Van Damme wearing a much different body suit) had failed. Arnold Schwarzenegger recommended Winston after his experience working on The Terminator.[3]		The Predator was originally designed with a long neck, a dog-like head and a single eye. This design was abandoned when it became apparent that the jungle locations would make shooting the complex design too difficult.[3] Originally, the studio contracted the makeup effects for the alien from Richard Edlund's Boss Film Creature Shop. However, problems filming the alien in Mexico led the makeup effects responsibilities to be given to Stan Winston. According to former Boss Films make-up supervisor Steve Johnson, the makeup failed because of an impractical design by McTiernan that included 12-inch length extensions that gave the Predator a backward bent satyr-leg. The design did not work in the jungle locations. After six weeks of shooting in the jungles of Palenque, Mexico, the production had to shut down so that Winston could make a new Predator. This took eight months and then filming resumed for five weeks, ending in February 1987.[4]		Jean-Claude Van Damme was originally cast as the Predator; the idea was that star's abilities in martial arts would make the Predator an agile, ninja-esque hunter. When compared to Schwarzenegger, Carl Weathers, and Jesse Ventura, actors known for their bodybuilding regimens, it became apparent a more physically imposing man was needed to make the creature appear threatening.[3] Eventually, Van Damme was removed from the film and replaced by actor and mime artist Kevin Peter Hall.[3] Hall, standing at an imposing height of 7 feet 2 inches (2.18 m), had just finished work as a sasquatch in Harry and the Hendersons.[3]		Hall played the Predator in the first and second movies. He was trained in the art of mime and used many tribal dance moves in his performance, such as during the fight between Arnold Schwarzenegger and the Predator at the end of the first movie. In Predator 2, according to a "making of" featurette, Danny Glover suggested the Los Angeles Lakers to be the other Predators because Glover himself was a big fan. Hall persuaded some of the Lakers to play background Predators because they couldn't find anyone on short notice.[5] Hall died not long after Predator 2 was released in theaters.		In Alien vs. Predator, Welsh actor Ian Whyte, standing at 7 feet 1 inch and a fan of the Predator comics and movies, took over as the man in the Predator suits, such as portraying the "Celtic" Predator during its fight with an Alien warrior.[6] Whyte returned to portray the "Wolf" Predator in Aliens vs. Predator: Requiem.[7]		In Predators, actors Brian Steele and Carey Jones both portrayed a new breed of Predator known as the "Black Super Predators",[8] who have been dropping humans on their planet for many years to play a survival game against them.[8] In a nod to the first film, Derek Mears played the Predator as the creature appeared in the original, dubbed the "Classic Predator".[9]		The Predator's blood was made from a combination of the liquid from glow sticks mixed with K-Y Jelly. The mixture loses its glow quickly, so new batches had to be quickly made between takes. The technique was used in all five films featuring the Predator.		The camouflage effect was designed by R/Greenberg Associates, under the direction of Joel Hynek. The idea for the effect came in a dream one of the Thomas brothers (who wrote the film) had, in which there was a chrome man who was inside a reflective sphere. The man blended in, perfectly camouflaged, reflecting from all directions and only visible when in motion. It took quite a while before they figured out how to do it, which was basically an image repeated in a pattern of ripples in the shape of the Predator's body. It proved very effective and was a new way of presenting an "invisible man." Before there was digital rendering technology all of the camouflage was done optically using photo-chemical means, so that one would never get the same result twice from combining the same pieces of film.		After the original movies, Amalgamated Dynamics took over from Stan Winston Studio in creating the props for the Predators in the Alien vs. Predator film and a number of effects houses worked on the various other effects.		First appearing in the 1987 film, Predator, the eponymous character lands in Val Verde via starship. He has come to this location due to the wars between rebel and government forces, already having made several kills among the locals before beginning to hunt down a United States Army Special Forces group sent there to rescue presidential cabinet ministers kidnapped by guerrilla forces. The Predator dispatches the soldiers one by one with a vast array of weaponry until Major Dutch Schaeffer (Arnold Schwarzenegger) is the last one alive. Dutch eventually confronts the creature, covering himself in mud to hide his heat signature from the Predator's thermal imaging, and setting up numerous booby traps. Though he manages to disable the Predator's cloaking ability, the Predator manages to capture him, and then, in a display of Honour, discards his mask and electronic weaponry before challenging Dutch to a final duel. Physically outmatched, Dutch eventually sets off one of his traps, which crushes and mortally wounds the creature. After being asked what he is by Dutch, the Predator simply mimics his question and sets off his self-destruct device before laughing maniacally, though Dutch manages to escape the explosion.[10]		Set in 1997, ten years after the events of the first film, the 1990 sequel follows a new Predator who sets his sights on Los Angeles due to its summer heat and drug wars between Jamaican and Colombian cartels, as well as the L.A.P.D. attempting to fight both gangs (Promotional material for the film said this Predator was younger, and chose a densely populated urban area for a more ambitious hunt). After eliminating leaders from both gangs, the Predator begins actively targeting the L.A.P.D. officers attempting to investigate his handiwork, specifically Lieutenant Michael Harrigan (Danny Glover) and his three partners (Rubén Blades, María Conchita Alonso and Bill Paxton). Special agent Peter Keyes (Gary Busey), purportedly sent by the DEA to investigate cartels, but actually leading the secretive Outworld Life Forms task force, attempts to capture the Predator alive for government study but he and most of his OWLF team are outsmarted and slaughtered by their quarry. Towards the end of the movie, the Predator is ultimately confronted by Harrigan in his own ship and killed when Harrigan uses one of his own weapons against him. The Predator's clan-mates de-cloak and carry away the dead Predator's body and give Harrigan a flintlock dating from 1715 as a sign of respect. The film also makes a reference to the Alien films, as shown in the Predators' trophy room, which has a skull closely resembling that of an Alien.[5]		In 2004, a Predator ship arrives in Earth's orbit to draw humans to an ancient Predator training ground on Bouvetøya, an island about a thousand miles north of Antarctica. A buried pyramid which gives off a "heat bloom" attracts humans led by Charles Bishop Weyland (Lance Henriksen), who unknowingly activates an Alien egg production line. Three Predator novitiates enter the structure, killing all humans in their way with the intention of hunting the newly formed alien warriors. Two Predators die in the ensuing battle, and the third (credited as Scar in the credits) allies himself with the lone surviving human, Alexa Woods (Sanaa Lathan) to battle the escaped Queen Alien. The Queen is defeated, but not before she fatally wounds the last Predator. The Predator ship hovering above the battleground uncloaks and the crew retrieves the fallen Predator. A Predator elder gives Alexa a spear as a sign of respect, and then departs. Once the Predator ship is in orbit, it is revealed that a chestburster was in the Scar Predator's corpse, though this specimen has Predator mandibles.[6]		Set immediately after the previous film, the Predalien hybrid on board the Predator scout ship, which just separated from the mothership from the previous film, has grown to full adult size and sets about killing the Predators on board the ship, causing it to crash in Gunnison, Colorado. The last survivor activates a distress signal with a video of the Predalien, which is received by a veteran Predator, who sets off towards Earth to "clean up" the infestation. When he arrives, the Predator tracks the Aliens into a section of the sewer below town. He removes evidence of their presence as he goes by using a corrosive blue liquid. He uses a laser net to try to contain the creatures, but the Aliens still manage to escape into the town above. The Predator fashions a plasma pistol from his remaining plasma caster, replacing one that was damaged while he hunted Aliens all across town (accidentally cutting the power to the town in the process) during a confrontation with human survivors. Later on, the Predator encounters the same human survivors again in the Alien Hive and loses his plasma pistol, with the last four humans taking it with them as they flee town in the hospital helicopter. The Predator then fights the Predalien singlehandedly, and the two mortally wound one another just as the United States military drops a tactical nuclear bomb on the town, incinerating both combatants as well as the few remaining humans in the city. The salvaged plasma pistol is then taken by United States Military officers to Ms. Yutani.[7]		In Predators (which deliberately distances itself from the prior Alien vs. Predator films),[11] it is revealed that there are two warring Predator tribes: one group uses quadrupedal hunting beasts and elaborate traps to hunt, and the other hunts traditionally. An international group of soldiers and dangerous criminals from different locations from Earth are dropped onto a forested planet used as a Predator game reserve. After numerous skirmishes resulting in the deaths of two Predators and all but two of the captured humans, the last Predator manages to kill another member of his kind from a rival tribe, but is defeated in combat by the human survivors. The survivors then head off to seek a way back to their home, just in time to witness more people be dropped.		In June 2014, Fox announced a sequel which Shane Black will direct and co-write with Fred Dekker, and John Davis will produce.[12]		In the Aliens vs. Predator novel series (based on the Dark Horse Comics) by David Bischoff, Steve and Stephani Perry, the Predators, known in the series as "yautja", are depicted as living in a matriarchal clan-based society bearing similarities to a pack mentality whose strongest and most skilled of the group lead. The Predators are portrayed as sexually dimorphic mammals. The females are larger and stronger than males,[13] and sport more prominent mammary glands (like human females). Both genders give off a strong musk to signify aggression, and females can also emit it when in estrus. This musk can be detected by other Predators and canids, though it is imperceptible to humans. Predators in the Perry novels are not monogamous, and it is common for veteran warriors to sire hundreds of offspring (known as sucklings) with multiple mates. It is also revealed that their blood has the capacity of partially neutralizing the acidity of Alien blood. Their religion is partially explored in the series, showing that they are polytheistic, and that their equivalent of the Grim Reaper is the so-called "Black Warrior", who is seen as an eternal adversary who eventually wins all battles.[1]		Though female Predators are occasionally referred to in Steve and Stephani Perry's novel series, one does not make an appearance until the comic book limited series Aliens vs Predator: Deadliest of Species. The female's design contradicts the descriptions given in the Perry novel series, as it superficially shows little distinction from males.[14]		The Darkhorse/TopCow crossover MindHunter, which pits the Witchblade, Darkness, Aliens, and Predator franchises against each other, depicts a female Predator in a manner closer to the Perry description. It is very tall, has feminine hips, mammary glands, and a very muscular build, with different armor than the males.		In Randy Stradley's miniseries Aliens vs. Predator: War, it is revealed through the narration of the character Machiko Noguchi that Predators were responsible for the spread of Aliens throughout the galaxy, though the Predators themselves deny this, stating that their large interplanetary distribution is due to simultaneous convergent evolution.[15]		The comic series Predator and Aliens vs Predator: Three World War introduce a clan of Predators referred to as "Killers", who are enemies of mainstream Predators (here referred to as "Hunters") because of their tradition of training Aliens as attack animals rather than hunting them, as well as their desire for killing as opposed to honorable hunting. The character Machiko Noguchi notes in issue #1 of Three World War that "You have to understand the mindset of the Hunters, and the honor they place on facing a worthy opponent on an equal footing... a kill is the end result, but it's not the point of a hunt.... For the 'Killers,' that wasn't the case. They were all about the killing." They are first seen in the 2009 Predator series, where a number interfere in an East African civil war, coming into conflict with both humans and their Hunter counterparts. By the time of Three World War the Killers are assumed to have been wiped out by the Hunters, but some survive and begin attacking human colonies, forcing Noguchi to forge an alliance between humans and the Hunters in order to deal with them.[16][17]		In John Shirley's stand alone novel Predator: Forever Midnight, Predators, now referred to as "Hish", are shown to possess a gland located between their neck and collarbone which secretes powerful hormones into their bloodstream and which drives them to hyper-aggression. When this gland is over-stimulated, it sends the creatures into a frenzied rage, causing them to attempt killing any living thing in sight, including members of their own species. This "kill rage" can be contagious and spread from one Predator to another, driving them all to attack each other. The Predators as a species barely survived the wars provoked by their kill glands, and they have learned to control the gland's secretions with artificial hormone regulators.[2]		In Ian Edginton and Alex Maleev's graphic novel Aliens vs. Predator: Eternal and the videogame Predator: Concrete Jungle, Predator flesh and blood, if consumed, is shown to have the capacity of greatly lengthening a human's lifespan.		In the first-person shooting video game Call of Duty: Ghosts, Predator appears as a hidden killstreak on the multiplayer map "Ruins" from the Devastation map pack. The player can play as Predator for a brief period by completing a Field Order and obtaining a care package. Predator is also a playable guest character via downloadable content in the fighting game Mortal Kombat X, opposite an Alien.[18]		Predators are physically distinguished from humans by their greater height, arthropod-like mandibles and long, hair-like appendages on their heads that are set into their skulls (popularly perceived as "dreadlocks"). Their bodies are resilient to damage, capable of recovering from multiple gunshot wounds[5][10] and radiation doses which would be fatal to humans.[5] Their wounds do however require medical attention and they incorporate a portable surgical kit in their armor for this purpose. They are also capable of enduring excruciating pain. Predators are much stronger than humans, having been portrayed as being easily capable of outmatching a conditioned adult human male[10] and shattering solid concrete with their bare hands. They are also skilled climbers, and will readily move through trees[10] or across rooftops[5] in pursuit of prey. Though capable of surviving exposure in Antarctic temperatures for an extended period of time,[6] it is implied that Predators have a preference for hot equatorial climates.[5][10] Their blood is luminescent phosphor green in color. Their vision operates mainly in the infrared portion of the electromagnetic spectrum; they can easily detect heat differentials in their surroundings but are unable to easily distinguish among objects of the same relative temperature.[10] A Predator's bio-mask increases its ability to see in a variety of spectra, ranging from the low infrared to the high ultraviolet, and also filters the ambient heat from the area, allowing them to see things with greater clarity and detail.[5] While they are capable of breathing Earth's atmosphere,[10] the creature in Predator 2 is seen using a breathing mask after losing his helmet (Although it should be noted that this Predator had just been shot multiple times and may have therefore not been operating at his full potential). Their dietary habits are also mentioned in Predator 2, where it is revealed that the creature regularly visits a slaughterhouse every two days to feed on the stored meat there.[5]		Throughout their film appearances, Predators have undergone numerous design variations. In Predator 2, the main Predator was designed to look more urban and hip than its predecessor. Design changes included tribal ornamentation on the forehead, which was made steeper and shallower, brighter skin coloration and a greater number of fangs.[20] This Predator was made less reliant on his plasma caster, and more cunning with the use of nets, spears and bladed weaponry. In Alien vs. Predator, the appearance of the Predators was redesigned to make them seem more heroic. Redesigns included a reduction in head and waist size, broader shoulders, a more muscular physique, piranha-like teeth on the upper jaw, and dryer and less clammy skin to further differentiate them from the Aliens.[21] In Aliens vs. Predator: Requiem, the Predator was returned to the sleeker design concept prior to Alien vs. Predator.[22] For the so-called "Black Super Predators" in Predators, the designers used the differences between a cassette tape and an iPod as an analogy in differentiating the new Predators from the classic. The Super Predators were designed as leaner and taller than the "classic" Predator design, and they have longer faces, tighter armor, and more swept back dreadlocks.[23]		Predator culture revolves around the hunting and stalking of dangerous lifeforms. After making a kill, Predators typically skin or decapitate the carcass, converting it into a trophy. If immobilized or at the brink of death, a hunter will activate the mass-explosive self-destruct-mechanism in his wristband, honorably erasing any trace of its presence to its prey.[10] It is often alluded to that the reason Predators hunt is not for sustenance or elimination of threats, but as sportsmanship or rite of passage, as they will normally attack only life forms that have the ability to provide them with a challenge. In Predators, it is revealed that there are at least two different Predator tribes, which are engaged in a long lasting blood feud. The film also introduced a pack of spined, quadrupedal beasts used as flushing dogs by the "Super Predators". Creature designer Gregory Nicotero used hyenas as a basis for the creature's physique, and the spines were added later by Chris Olivia.[23]		Predators made contact with early human civilizations such as the Ancient Egyptians, the Khmer Empire, and Aztecs, as well as a fictitious culture inhabiting what is now Bouvetøya.[6] Upon arriving on Earth, the Predators were worshipped as gods by humans, and they taught many of the civilizations how to build pyramids (an explanation as to why many of these different ancient societies had distinctly similar cultures and architecture), but in return expected sacrifices of humans for use as hosts for huntable Xenomorphs (Aliens)- the ultimate prey for initiates. The Predators returned to Bouvetøya every century to consummate the bargain, until at one point in the ritual, the Xenomorphs spread out of control, resulting in the Predators detonating a bomb that obliterated the entire civilization.[6] Relations between humans and Predators deteriorated from that time on; the Predators then viewed humans as little more than another quarry to hunt.		Predators feature prominently in the folklore of certain cultures; some Latin American people refer to the species as "El Diablo que hace trofeos de los hombres" (Spanish for "The Demon who makes trophies of men"),[10] and Jamaican superstition identifies Predators as demons from the spirit world.[5] When hunting humans, Predators normally avoid certain individuals such as children and some adults if they are unarmed, though they will spare armed ones if they happen to be pregnant[5] or sickly unless they are attacked by them.[6] A human who has managed to kill a Predator or a Xenomorph in single combat[5] or has fought alongside a Predator is usually spared by the deceased hunter's comrades and given a gift (often a rare or exotic weapon) as a sign of respect.[6]		A learner's first successful Alien hunt is completed with the marking of his helmet and forehead with the acidic blood of his kill.[6] The hunter generally operates alone. Even when hunters appear in groups, they rarely perform anything that resembles teamwork. Predators use Aliens as prey, creating artificial gaming reserves by keeping Queens and even Facehuggers in captivity.[6] It is shown in a brief scene in Aliens vs. Predator: Requiem that Predators have had prior contact with a race of creatures resembling the "Space Jockey" in the film Alien. This is confirmed in the film's DVD commentary.[24]		The script of the Predators is expressed in the films and other media through written patterns of dashes. These written symbols appear on the creatures' gauntlet displays, their helmets, architecture, and many other surfaces. The most common vocalizations of the Predators consists of a series of clicks, roars, snarls, and growls which are consisted of recorded vocalizations of animals such as lions, tigers, leopards, jaguars, cougars, black bears, grizzly bears, alligators, and elephants. Predators will mimic human language on occasion, and have been shown to use their helmets to understand and speak human languages.[5][10] Author Steve Perry designed a constructed language set for the Aliens vs. Predator novel series.[1]		
Otaku (おたく/オタク) is a Japanese term for people with obsessive interests, commonly the anime and manga fandom. Its contemporary usage originated with Akio Nakamori's 1983 essay in Manga Burikko.[1][2] Otaku may be used as a pejorative; its negativity stems from the stereotypical view of otaku and the media's reporting on Tsutomu Miyazaki, "The Otaku Murderer", in 1989. According to studies published in 2013, the term has become less negative, and an increasing number of people now self-identify as otaku.[3]		Otaku subculture is a central theme of various anime and manga works, documentaries and academic research. The subculture began in the 1980s as changing social mentalities and the nurturing of otaku traits by Japanese schools combined with the resignation of such individuals to become social outcasts. The subculture's birth coincided with the anime boom, after the release of works such as Mobile Suit Gundam before it branched into Comic Market. The definition of otaku subsequently became more complex, and numerous classifications of otaku emerged. In 2005, the Nomura Research Institute divided otaku into twelve groups and estimated the size and market impact of each of these groups. Other institutions have split it further or focus on a single otaku interest. These publications classify distinct groups including anime, manga, camera, automobile, idol and electronics otaku. The economic impact of otaku has been estimated to be as high as ¥2 trillion ($18 billion).[4]						Otaku is derived from a Japanese term for another person's house or family (お宅, otaku). This word is often used metaphorically, as an honorific second-person pronoun. In this usage, its literal translation is "you". For example, in the anime Macross, first aired in 1982, the character Lynn Minmay uses the term this way.[5] The modern slang form, which is distinguished from the older usage by being written only in hiragana (おたく), katakana (オタク or, less frequently, ヲタク) or rarely in rōmaji, first appeared in public discourse in the 1980s, through the work of humorist and essayist Akio Nakamori. His 1983 series An Investigation of "Otaku" (『おたく』の研究, "Otaku" no Kenkyū), printed in the lolicon magazine Manga Burikko, applied the term to unpleasant fans in caricature. Animators Haruhiko Mikimoto and Shōji Kawamori had used the term between themselves as an honorific second-person pronoun since the late 1970s.[5] Supposedly, some fans used it past the point in their relationships where others would have moved on to a less formal style. Because this misuse indicated social awkwardness, Nakamori chose the word itself to label the fans.[5] Morikawa Kaichirō, an author and lecturer at Meiji University, identified this as the origin of its contemporary usage.[6][7]		Another claim for the origin of the term comes from the works of science fiction author Motoko Arai, who used the word in her novels as a second-person pronoun and the readers adopted the term for themselves. However, a different claim points to a 1981 Variety magazine essay.[8][Note 1][9]		In 1989, the case of Tsutomu Miyazaki, "The Otaku Murderer", brought the fandom, very negatively, to national attention.[6] Miyazaki, who randomly chose and murdered four girls, had a collection of 5,763 video tapes, some containing anime and slasher films that were found interspersed with videos and pictures of his victims. Later that year, the contemporary knowledge magazine Bessatsu Takarajima dedicated its 104th issue to the topic of otaku. It was called Otaku no Hon (おたくの本, lit. The Book of Otaku) and delved into the subculture of otaku with 19 articles by otaku insiders, among them Akio Nakamori. This publication has been claimed by scholar Rudyard Pesimo to have popularized the term.[10]		In modern Japanese slang, the term otaku is mostly equivalent to "geek" or "nerd", but in a more derogatory manner than used in the West.[6] However, it can relate to any fan of any particular theme, topic, hobby or form of entertainment.[6] "When these people are referred to as otaku, they are judged for their behaviors - and people suddenly see an “otaku” as a person unable to relate to reality".[11][12] The word entered English as a loanword from the Japanese language. It is typically used to refer to a fan of anime/manga but can also refer to Japanese video games or Japanese culture in general. The American magazine Otaku USA popularizes and covers these aspects.[13][14] The usage of the word is a source of contention among some fans, owing to its negative connotations and stereotyping of the fandom. Widespread English exposure to the term came in 1988 with the release of Gunbuster, which referred to anime fans as otaku. Gunbuster was released officially in English in March 1990. The term's usage spread throughout rec.arts.anime with discussions about Otaku no Video's portrayal of otaku before its 1994 English release. Positive and negative aspects, including the pejorative usage, were intermixed.[14] The term was also popularized by William Gibson's 1996 novel Idoru, which references otaku.[15]		Morikawa Kaichirō identifies the subculture as distinctly Japanese, a product of the school system and society. Japanese schools have a class structure which functions as a caste system, but clubs are an exception to the social hierarchy. In these clubs, a student's interests will be recognized and nurtured, catering to the interests of otaku. Secondly, the vertical structure of Japanese society identifies the value of individuals by their success. Until the late 1980s, unathletic and unattractive males focused on academics, hoping to secure a good job and marry to raise their social standing. Those unable to succeed socially focused instead on their interests, often into adulthood, with their lifestyle centering on those interests, furthering the creation of the otaku subculture.[6]		Even prior to the coinage of the term, the stereotypical traits of the subculture were identified in a 1981 issue of Fan Rōdo (Fan road) about "culture clubs".[6] These individuals were drawn to anime, a counter-culture, with the release of hard science fiction works such as Mobile Suit Gundam. These works allowed a congregation and development of obsessive interests that turned anime into a medium for unpopular students, catering to obsessed fans. After these fans discovered Comic Market, the term was used as a self-confirming and self-mocking collective identity.[6]		The 1989 "Otaku Murderer" case gave a negative connotation to the fandom from which it has not fully recovered. The usage of "(interest) otaku", however, is used for teasing or self-deprecation, but the unqualified term remains negative.[6] The identification of otaku turned negative in late 2004 when Kaoru Kobayashi kidnapped, sexually assaulted, and murdered a seven-year-old first-grade student. Japanese journalist Akihiro Ōtani suspected that Kobayashi's crime was committed by a member of the figure moe zoku even before his arrest.[16] Although Kobayashi was not an otaku, the degree of social hostility against otaku increased. Otaku were seen by law enforcement as possible suspects for sex crimes, and local governments called for stricter laws controlling the depiction of eroticism in otaku materials.[17]		Not all attention has been negative. In his book, Otaku, Hiroki Azuma observed: "Between 2001 and 2007, the otaku forms and markets quite rapidly won social recognition in Japan", citing the fact that "[i]n 2003, Hayao Miyazaki won the Academy Award for his Spirited Away; around the same time Takashi Murakami achieved recognition for otaku-like designs; in 2004, the Japanese pavilion in the 2004 International Architecture exhibition of the Venice Biennale (Biennale Architecture) featured “otaku”. In 2005, the word moe - one of the keywords of the present volume - was chosen as one of the top ten “buzzwords of the year."[18] The former Prime Minister of Japan Taro Aso has also claimed to be an otaku, using this subculture to promote Japan in foreign affairs.[19] In 2013, a Japanese study of 137,734 people found that 42.2% self-identify as a type of otaku. This study suggests that the stigma of the word has vanished, and the term has been embraced by many.[3]		The district of Akihabara in Tokyo, where there are maid cafes featuring waitresses who dress up and act like maids or anime characters, is a notable attraction center for otaku. Akihabara also has dozens of stores specializing in anime, manga, retro video games, figurines, card games and other collectibles.[20] Another popular location is Otome Road in Ikebukuro, Tokyo. In Nagoya, students from Nagoya City University started a project on ways to help promote hidden tourist attractions related to the otaku culture to attract more otaku to the city.[21]		There are specific terms for different types of otaku, including Fujoshi (腐女子, lit. "rotten girl"), a self-mockingly pejorative Japanese term for female fans of yaoi, which focuses on homosexual male relationships.[22] Reki-jo are female otaku who are interested in Japanese history. Some terms refer to a location, such as Akiba-kei, a slang term meaning "Akihabara-style" which applies to those familiar with Akihabara's culture. Another is Wotagei or otagei (ヲタ芸 or オタ芸), a type of cheering that is part of Akiba-kei. Other terms, such as Itasha (痛車), literally "painful car", describe vehicles who are decorated with fictional characters, especially bishōjo game or eroge characters.[23][24]		Otaku often participate in self-mocking through the production or interest in humor directed at their subculture. Anime and manga otaku are the subject of numerous self-critical works, such as Otaku no Video, which contains a live-interview mockumentary that pokes fun at the otaku subculture and includes Gainax's own staff as the interviewees.[25] Other works depict otaku subculture less critically, such as Genshiken and Comic Party. A well-known novel-cum-manga-cum-anime is Welcome to the N.H.K., which focuses on the subcultures popular with otaku and highlights other social outcasts such as the hikikomori and NEETs. Works that focus on an otaku character include WataMote, the story of an unattractive and unsociable otome game otaku who exhibits delusions about her social status.[26] Watamote is a self-mocking insight that follows the heroine's delusion and attempts to reform herself only by facing reality with comedic results on the path to popularity. An American documentary, Otaku Unite!, focuses on the American side of the otaku culture.[27]		The Nomura Research Institute (NRI) has made two major studies into otaku, the first in 2004 and a revised study with a more specific definition in 2005.[28][29] The 2005 study defines twelve major fields of otaku interests. Of these groups, manga (Japanese comics) was the largest, with 350,000 individuals and ¥83 billion market scale. Idol otaku were the next largest group, with 280,000 individuals and ¥61 billion. Travel otaku with 250,000 individuals and ¥81 billion. PC otaku with 190,000 individuals and ¥36 billion. Video game otaku with 160,000 individuals and ¥21 billion. Automobile otaku with 140,000 individuals and ¥54 billion. Animation (anime) otaku with 110,000 individuals and ¥20 billion. The remaining five categories include Mobile IT equipment otaku, with 70,000 individuals and ¥8 billion; Audio-visual equipment otaku, with 60,000 individuals and ¥12 billion; camera otaku, with 50,000 individuals and ¥18 billion; fashion otaku, with 40,000 individuals and ¥13 billion; and railway otaku, with 20,000 individuals and ¥4 billion.[29] These values were partially released with a much higher estimation in 2004, but this definition focused on the consumerism and not the "unique psychological characteristics" of otaku used in the 2005 study.[28][29]		NRI's 2005 study also put forth five archetypes of otaku. The first is the family-oriented otaku, who has broad interests and is more mature than other otaku; their object of interest is secretive and they are "closet otaku". The second is the serious "leaving my own mark on the world" otaku, with interests in mechanical or business personality fields. The third type is the "media-sensitive multiple interest" otaku, whose diverse interests are shared with others. The fourth type is the "outgoing and assertive otaku", who gain recognition by promoting their hobby. The last is the "fan magazine-obsessed otaku", which is predominately female with a small group of males being the "moe type"; the secret hobby is focused on the production or interest in fan works.[29] The Hamagin Research Institute found that moe-related content was worth ¥88.8 billion ($807 million) in 2005, and one analyst estimated the market could be as much as ¥2 trillion ($18 billion).[4] Japan based Tokyo Otaku Mode a place for news relating to Otaku has been liked on Facebook almost 10 million times.[30]		Other classifications of otaku interests include vocaloid, cosplay, figures and professional wrestling as categorized by the Yano Research Institute. Yano Research reports and tracks market growth and trends in sectors heavily influenced by otaku consumerism. In 2012, it noted around 30% growth in dating sim and online gaming otaku, while vocaloid, cosplay, idols and maid services grew by 10%, confirming its 2011 predictions.[31][32]		
A pedant is a person who is excessively concerned with formalism, accuracy, and precision, or one who makes an ostentatious and arrogant show of learning.						The English language word "pedant" comes from the French pédant (used in 1566 in Darme & Hatzfeldster's Dictionnaire général de la langue française) or its older mid-15th century Italian source pedante, "teacher, schoolmaster". (Compare the Spanish pedante.) The origin of the Italian pedante is uncertain, but several dictionaries suggest that it was contracted from the medieval Latin pædagogans, present participle of pædagogare, "to act as pedagogue, to teach" (Du Cange).[1] The Latin word is derived from Greek παιδαγωγός, paidagōgós, παιδ- "child" + ἀγειν "to lead", which originally referred to a slave who escorted children to and from school but later meant "a source of instruction or guidance".[2][3]		The term in English is typically used with a negative connotation to refer to someone who is over-concerned with minutiae and whose tone is condescending.[4] Thomas Nashe wrote in Have with you to Saffron-walden (1596), page 43: "O, tis a precious apothegmaticall [terse] Pedant, who will finde matter inough to dilate a whole daye of the first inuention [invention] of Fy, fa, fum". However, when the word was first used by Shakespeare in Love's Labour's Lost (1598), it simply meant "teacher".		Obsessive–compulsive personality disorder is in part characterized by a form of pedantry that is excessively concerned with the correct following of rules, procedures, and practices.[5] Sometimes the rules that OCPD sufferers obsessively follow are of their own devising, or are corruptions or reinterpretations of the letter of actual rules.		Pedantry can also be an indication of specific developmental disorders. In particular, people with Asperger syndrome often have behaviour characterized by pedantic speech.[6]		
Primitivism is a Western art movement that borrows visual forms from non-Western or prehistoric peoples, such as Paul Gauguin's inclusion of Tahitian motifs in paintings and ceramics. Borrowings from primitive art has been important to the development of modern art.[1]		The term "primitivism" is often applied to other professional painters working in the style of naïve or folk art like Henri Rousseau, Mikhail Larionov, Paul Klee and others.						Whether and to what extent we should simplify our lives and get "back to basics" is a debate that has been going on since the invention of writing.[2] In antiquity the superiority of the simple life was expressed in the Myth of the Golden Age, depicted in the genre of European poetry and visual art known as the Pastoral. The debate about the merits and demerits of a simple, versus a complex life, gained new urgency with the European encounter with hitherto unknown peoples after the exploration of the Americas and Pacific Islands by Columbus and others.		During the Enlightenment, arguments about the supposed superiority of indigenous peoples were chiefly used as a rhetorical device to criticize aspects of European society. In the realm of aesthetics, however, the eccentric Italian philosopher, historian and jurist Giambattista Vico (1688–1744) was the first to argue that primitive man was closer to the sources of poetry and artistic inspiration than "civilized" or modern man. Vico was writing in the context of the celebrated contemporary debate, known as the great Quarrel of the Ancients and the Moderns, over which was better, the classic poetry of Homer and the Bible or modern vernacular literature.		In the 18th century, the German scholar Friedrich August Wolf identified the distinctive character of oral literature and located Homer and the Bible as examples of folk or oral tradition (Prolegomena to Homer, 1795). Vico and Wolf's ideas were developed further in the beginning of the 19th century by Herder.[3] Nevertheless, although influential in literature, such arguments were known to a relatively small number of educated people and their impact was limited or non-existent in the sphere of visual arts.[4]		The 19th century saw for the first time the emergence of historicism, or the ability to judge different eras by their own context and criteria. A result of this new historicism, new schools of visual art arose that aspired to hitherto unprecedented levels of historical fidelity in setting and costumes. Neoclassicism in visual art and architecture was one result. Another such "historicist" movement in art was the Nazarene movement in Germany, which took inspiration from the so-called Italian "primitive" school of devotional paintings (i.e., before the age of Raphael and the discovery of oil painting).		Where conventional academic painting (after Raphael) used dark glazes, highly selective, idealized forms, and rigorous suppression of details, the Nazarenes used clear outlines, bright colors, and paid meticulous attention to detail. This German school had its English counterpart in the Pre-Raphaelites, who were primarily inspired by the critical writings of John Ruskin, who admired the painters before Raphael (such as Botticelli) and who also recommended painting outdoors, hitherto unheard of.		Two phenomena shook the world of visual art in the mid-19th century. The first was the invention of the photographic camera, which arguably spurred the development of Realism in art. The second was a discovery in the world of mathematics of non-Euclidean geometry, which overthrew the 2000-year-old seeming absolutes of Euclidean geometry and threw into question conventional Renaissance perspective by suggesting the possible existence of multiple dimensional worlds and perspectives in which things might look very different.[5]		The discovery of possible new dimensions had the opposite effect of photography and worked to counteract realism. Artists, mathematicians, and intellectuals now realized that there were other ways of seeing things beyond what they had been taught in Beaux Arts Schools of Academic painting, which prescribed a rigid curriculum based on the copying of idealized classical forms and held up Renaissance perspective painting as the culmination of civilization and knowledge.[6] Beaux Arts academies held than non-Western and tribal peoples had had no art or only inferior art.		In rebellion against this dogmatic approach, artists began to try to depict realities that might exist in a world beyond the limitations of the three dimensional world of conventional representation mediated by classical sculpture. They looked to Japanese and Chinese art, which was learned and sophisticated and did not employ Renaissance one-point perspective. Non-euclidean perspective (Cubism) and tribal art fascinated Western European artists who saw them as portraying the reality of the spirit world. They also looked to the art of untrained painters and to children's art, which they believed depicted interior emotional realities that had been ignored in conventional, cook-book-style academic painting.		Tribal and other non-European art also appealed to those who were unhappy with the repressive aspects of European culture, as pastoral art had done for millennia.[7] Imitations of tribal or archaic art also fall into the category of nineteenth-century "historicism", as these imitations strive to reproduce this art in an authentic manner. Actual examples of tribal, archaic, and folk art were prized by both creative artists and collectors.		Paul Gauguin's paintings, Pablo Picasso's paintings and Igor Stravinsky's music are sometimes cited as examples of primitivism in art. Stravinsky's The Rite of Spring, is "primitivist" in that its programmatic subject is a pagan rite: a human sacrifice in pre-Christian Russia. It uses dissonance and loud, repetitive rhythms to depict "Dionysian" modernism, i.e., abandonment of inhibition (restraint standing for civilization). Nevertheless, Stravinsky was a master of learned classical tradition and worked within its bounds. In his later work he adopted a more "Apollonian" neoclassicism, to use Nietzsche's terminology, although in his use of serialism he still rejects 19th-century convention. In modern visual art, Picasso's work is also understood as rejecting Beaux Arts artistic expectations and expressing primal impulses, whether he worked in a cubist, neo-classical, or tribal-art-influenced vein.		Primitivism gained a new impetus from anxieties about technological innovation but above all from the "Age of Discovery", which introduced the West to previously unknown peoples and simultaneously opened doors for colonialism and the direct scrutiny of radically different peoples.[8] As the European Enlightenment and the collapse of feudalism ensued, philosophers started questioning many fixed medieval assumptions about the nature of man, the position of man in society, and the dogmatic Catholic cosmology. They began questioning the nature of humanity and its origins through a discussion of the natural man, which had intrigued theologians since the European encounter with the New World.		From the 18th century onwards, Western thinkers and artists continued to engage in the retrospective tradition, that is "the conscious search in history for a more deeply expressive, permanent human nature and cultural structure in contrast to the nascent modern realities".[9] Their search led them to parts of the world that they constituted as representing alternatives to modern civilization.		Up until the 19th century only very few explorers were able to travel and bring back objects. But the 19th-century invention of the steamboat made indigenous cultures of European colonies and their artifacts more accessible to the direct observation and analysis of art lovers. European-trained artists and connoisseurs prized in these objects the stylistic traits they defined as attributes of primitive expression: absence of linear perspective, simple outlines, presence of symbolic signs such as the hieroglyph, emotive distortions of the figure, and the energetic rhythms resulting from the use of repetitive ornamental pattern.[10] These energizing stylistic attributes, present in the visual arts of Africa, Oceana, and the Indians of the Americas, could also be found in the archaic and peasant art of Europe and Asia, as well.		Painter Paul Gauguin sought to escape European civilization and technology when he took up residence in the French colony of Tahiti and adopted a simple lifestyle which he felt to be more natural than the one he had left behind.		Gauguin's search for the primitive was manifestly a desire for more sexual freedom than was available in 19th-century Europe, and this is reflected in such paintings as The Spirit of the Dead Keeps Watch (1892), Parau na te Varua ino (1892), Anna the Javanerin (1893), Te Tamari No Atua (1896), and Cruel Tales (1902), among others. Gauguin's view of Tahiti as an earthly Arcadia of free love, gentle climate, and naked nymphs is quite similar, if not identical, to that of the classical pastoral of academic art, which has shaped Western perceptions of rural life for millennia. One of his Tahitian paintings is even called "Tahitian Pastoral" and another "Where Do We Come From".[11] In this way Gauguin extended the academic pastoral tradition of Beaux Arts schools which had hitherto been based solely on idealized European figures copied from Ancient Greek sculpture to include non-European models.		Gauguin also believed he was celebrating Tahitian society and defending the Tahitians against European colonialism. Feminist postcolonial critics, however, decry the fact that Gauguin took adolescent mistresses, one of them as young as thirteen.[12] They remind us that like many men of his time and later, Gauguin saw freedom, especially sexual freedom, strictly from the male point of view. Using Gauguin as an example of what is "wrong" with primitivism, these critics conclude that, in their view, elements of primitivism include the "dense interweave of racial and sexual fantasies and power both colonial and patriarchal".[13] To these critics, primitivism such as Gauguin's demonstrates fantasies about racial and sexual difference in "an effort to essentialize notions of primitiveness" with "Otherness". Thus, they contend, primitivism becomes a process analogous to Exoticism and Orientalism, as conceived by Edward Said, in which European imperialism and monolithic and degrading views of the "East" by the "West" defined colonized peoples and their cultures.[14] In other words, although Gauguin believed he was celebrating and defending the Tahitians, to the extent that he allegedly saw them as "other", he participated in the outlook of his time and nationality to a greater extent than he realized and in the guise of celebrating them victimized the Tahitians all over again.		During the early 20th century, the European cultural elite were discovering African, Micronesian and Native American art. Artists such as Henri Matisse and Pablo Picasso were intrigued and inspired by the stark power and simplicity of styles of those cultures. Around 1906, Picasso, Matisse, André Derain and other artists in Paris had acquired an interest in primitivism, Iberian sculpture,[15] African art and tribal masks, in part because of the compelling works of Paul Gauguin that had suddenly achieved center stage in the avant-garde circles of Paris. Gauguin's powerful posthumous retrospective exhibitions at the Salon d'Automne in Paris in 1903 and an even larger one in 1906 had a stunning and powerful influence on Picasso's paintings.		From 1906 to 1909 Pablo Picasso's paintings explored the impact of Primitivism through Iberian sculpture, African sculpture, African traditional masks, and other historical works including the Mannerist paintings of El Greco, resulting in his masterpiece Les Demoiselles D'Avignon and the invention of Cubism.[16]		
An International Standard Serial Number (ISSN) is an eight-digit serial number used to uniquely identify a serial publication.[1] The ISSN is especially helpful in distinguishing between serials with the same title. ISSN are used in ordering, cataloging, interlibrary loans, and other practices in connection with serial literature.[2]		The ISSN system was first drafted as an International Organization for Standardization (ISO) international standard in 1971 and published as ISO 3297 in 1975.[3] ISO subcommittee TC 46/SC 9 is responsible for maintaining the standard.		When a serial with the same content is published in more than one media type, a different ISSN is assigned to each media type. For example, many serials are published both in print and electronic media. The ISSN system refers to these types as print ISSN (p-ISSN) and electronic ISSN (e-ISSN), respectively.[citation needed] Conversely, as defined in ISO 3297:2007, every serial in the ISSN system is also assigned a linking ISSN (ISSN-L), typically the same as the ISSN assigned to the serial in its first published medium, which links together all ISSNs assigned to the serial in every medium.[4]						The format of the ISSN is an eight digit code, divided by a hyphen into two four-digit numbers.[1] As an integer number, it can be represented by the first seven digits.[5] The last code digit, which may be 0-9 or an X, is a check digit. Formally, the general form of the ISSN code (also named "ISSN structure" or "ISSN syntax") can be expressed as follows:[6]		or by a PCRE regular expression:[7]		The ISSN of the journal Hearing Research, for example, is 0378-5955, where the final 5 is the check digit, that is C=5. To calculate the check digit, the following algorithm may be used:		To confirm the check digit, calculate the sum of all eight digits of the ISSN multiplied by its position in the number, counting from the right (if the check digit is X, then add 10 to the sum). The modulus 11 of the sum must be 0.		There is an online ISSN checker that can validate an ISSN, based on the above algorithm.[8][9]		ISSN codes are assigned by a network of ISSN National Centres, usually located at national libraries and coordinated by the ISSN International Centre based in Paris. The International Centre is an intergovernmental organization created in 1974 through an agreement between UNESCO and the French government. The International Centre maintains a database of all ISSNs assigned worldwide, the ISDS Register (International Serials Data System) otherwise known as the ISSN Register. At the end of 2016[update], the ISSN Register contained records for 1,943,572 items.[10]		ISSN and ISBN codes are similar in concept, where ISBNs are assigned to individual books. An ISBN might be assigned for particular issues of a serial, in addition to the ISSN code for the serial as a whole. An ISSN, unlike the ISBN code, is an anonymous identifier associated with a serial title, containing no information as to the publisher or its location. For this reason a new ISSN is assigned to a serial each time it undergoes a major title change.		Since the ISSN applies to an entire serial a new identifier, the Serial Item and Contribution Identifier (SICI), was built on top of it to allow references to specific volumes, articles, or other identifiable components (like the table of contents).		Separate ISSNs are needed for serials in different media (except reproduction microforms). Thus, the print and electronic media versions of a serial need separate ISSNs.[11] Also, a CD-ROM version and a web version of a serial require different ISSNs since two different media are involved. However, the same ISSN can be used for different file formats (e.g. PDF and HTML) of the same online serial.		This "media-oriented identification" of serials made sense in the 1970s. In the 1990s and onward, with personal computers, better screens, and the Web, it makes sense to consider only content, independent of media. This "content-oriented identification" of serials was a repressed demand during a decade, but no ISSN update or initiative occurred. A natural extension for ISSN, the unique-identification of the articles in the serials, was the main demand application. An alternative serials' contents model arrived with the indecs Content Model and its application, the digital object identifier (DOI), as ISSN-independent initiative, consolidated in the 2000s.		Only later, in 2007, ISSN-L was defined in the new ISSN standard (ISO 3297:2007) as an "ISSN designated by the ISSN Network to enable collocation or versions of a continuing resource linking among the different media".[12]		The ISSN Register is not freely available for interrogation on the web, but is available by subscription. There are several routes to the identification and verification of ISSN codes for the public:		An ISSN can be encoded as a uniform resource name (URN) by prefixing it with "urn:ISSN:".[13] For example, Rail could be referred to as "urn:ISSN:0953-4563". URN namespaces are case-sensitive, and the ISSN namespace is all caps.[14] If the checksum digit is "X" then it is always encoded in uppercase in a URN.		The util URNs are content-oriented, but ISSN is media-oriented:		A unique URN for serials simplifies the search, recovery and delivery of data for various services including, in particular, search systems and knowledge databases.[12] ISSN-L (see Linking ISSN below) was created to fill this gap.		There are two most popular media types that adopted special labels (indicating below in italics), and one in fact ISSN-variant, with also an optional label. All are used in standard metadata context like JATS, and the labels also, frequently, as abbreviations.		p-ISSN is a standard label for "Print ISSN", the ISSN for the print media (paper) version of a serial. Usually it is the "default media", so the "default ISSN".		e-ISSN (or eISSN) is a standard label for "Electronic ISSN", the ISSN for the electronic media (online) version of a serial.		ISSN-L is a unique identifier for all versions of the serial containing the same content across different media. As defined by ISO 3297:2007, the "linking ISSN (ISSN-L)" provides a mechanism for collocation or linking among the different media versions of the same continuing resource.		The ISSN-L is one ISSN number among the existing ISSNs, so, does not change the use or assignment of "ordinary" ISSNs;[16] it is based on the ISSN of the first published medium version of the publication. If the print and online versions of the publication are published at the same time, the ISSN of the print version is chosen as the basis of the ISSN-L.		With ISSN-L is possible to designate one single ISSN for all those media versions of the title. The use of ISSN-L facilitates search, retrieval and delivery across all media versions for services like OpenURL, library catalogues, search engines or knowledge bases.[17]		
Paul Graham (born 13 November 1964[1]) is an English born computer scientist, entrepreneur, venture capitalist, author, and blogger. He is best known for his work on Lisp, his former startup Viaweb (later renamed "Yahoo! Store"), co-founding the influential startup accelerator and seed capital firm Y Combinator, his blog, and Hacker News. He is the author of several programming books, such as: On Lisp[3] (1993), ANSI Common Lisp[4] (1995), and Hackers & Painters[5] (2004). Technology journalist Steven Levy has described Graham as a 'hacker philosopher'.[6]				In 1996, Graham and Robert Morris founded Viaweb, the first application service provider (ASP). Viaweb's software, originally written mostly in Common Lisp, allowed users to make their own Internet stores. In the summer of 1998 Viaweb was sold to Yahoo! for 455,000 shares of Yahoo! stock, valued at $49.6 million.[7] At Yahoo! the product became Yahoo! Store.		He later gained fame for his essays on his popular website paulgraham.com. Essay subjects range from "Beating the Averages",[8] which compares Lisp to other programming languages and introduced the hypothetical programming language Blub, to "Why Nerds are Unpopular",[9] a discussion of nerd life in high school. A collection of his essays has been published as Hackers & Painters [5] by O'Reilly, which includes a discussion of the growth of Viaweb and what Graham perceives to be the advantages of Lisp to program it.		In 2001, Graham announced that he was working on a new dialect of Lisp named Arc. Over the years since, he has written several essays describing features or goals of the language, and some internal projects at Y Combinator have been written in Arc, most notably the Hacker News web forum and news aggregator program.		In 2005, after giving a talk at the Harvard Computer Society later published as "How to Start a Startup", Graham along with Trevor Blackwell, Jessica Livingston and Robert Morris started Y Combinator to provide seed funding to a large number of startups, particularly those started by younger, more technically oriented founders. Y Combinator has now invested in more than 1300 startups, including Justin.tv, Xobni, Dropbox, Airbnb and Stripe.[10]		In response to the proposed Stop Online Piracy Act (SOPA), Graham announced in late 2011 that no representatives of any company supporting it would be invited to Y Combinator's Demo Day events.[11]		BusinessWeek included Paul Graham in 2008 edition of its annual feature, The 25 Most Influential People on the Web.[12]		In 2008, Paul Graham married Jessica Livingston.[13][14][15]		Graham has a Bachelor of Arts in philosophy from Cornell University[16][17] (1986).[18] He then attended Harvard University, earning Master of Science (1988) and Doctor of Philosophy (1990) degrees in Computer Science.[16][19] He has also studied painting at the Rhode Island School of Design and at the Accademia di Belle Arti in Florence.[16][19]		Graham proposed a "disagreement hierarchy" in a 2008 essay "How to Disagree",[20] putting types of argument into a seven-point hierarchy and observing that "If moving up the disagreement hierarchy makes people less mean, that will make most of them happier." Graham also suggested that the hierarchy can be thought as a pyramid, as the highest forms of disagreement are rarer.		Following this hierarchy, Graham notes that articulate forms of name-calling (e.g. "The author is a self-important dilettante") are no different from crude insults.		The hierarchy resembles Friedrich Glasl's model of conflict escalation[further explanation needed].		Graham considers the hierarchy of programming languages with the example of "Blub", a hypothetically average language "right in the middle of the abstractness continuum. It is not the most powerful language, but it is more powerful than Cobol or machine language."[21] It was used by Graham to illustrate a comparison, beyond Turing completeness, of programming language power, and more specifically to illustrate the difficulty of comparing a programming language one knows to one that one does not.[22]		Graham considers a hypothetical Blub programmer. When the programmer looks down the "power continuum", he considers the lower languages to be less powerful because they miss some feature that a Blub programmer is used to. But when he looks up, he fails to realise that he is looking up: he merely sees "weird languages" with unnecessary features and assumes they are equivalent in power, but with "other hairy stuff thrown in as well". When Graham considers the point of view of a programmer using a language higher than Blub, he describes that programmer as looking down on Blub and noting its "missing" features from the point of view of the higher language.[22]		Graham describes this as the "Blub paradox" and concludes that "By induction, the only programmers in a position to see all the differences in power between the various languages are those who understand the most powerful one."[22]		The concept has been cited[why?] by writers such as Joel Spolsky.[23]		
The International Standard Book Number (ISBN) is a unique[a][b] numeric commercial book identifier.		An ISBN is assigned to each edition and variation (except reprintings) of a book. For example, an e-book, a paperback and a hardcover edition of the same book would each have a different ISBN. The ISBN is 13 digits long if assigned on or after 1 January 2007, and 10 digits long if assigned before 2007. The method of assigning an ISBN is nation-based and varies from country to country, often depending on how large the publishing industry is within a country.		The initial ISBN configuration of recognition was generated in 1967 based upon the 9-digit Standard Book Numbering (SBN) created in 1966. The 10-digit ISBN format was developed by the International Organization for Standardization (ISO) and was published in 1970 as international standard ISO 2108 (the SBN code can be converted to a ten digit ISBN by prefixing it with a zero).		Occasionally, a book may appear without a printed ISBN if it is printed privately or the author does not follow the usual ISBN procedure; however, this can be rectified later.[1]		Another identifier, the International Standard Serial Number (ISSN), identifies periodical publications such as magazines; and the International Standard Music Number (ISMN) covers for musical scores.						The Standard Book Numbering (SBN) code is a 9-digit commercial book identifier system created by Gordon Foster, Emeritus Professor of Statistics at Trinity College, Dublin,[2] for the booksellers and stationers WHSmith and others in 1965.[3] The ISBN configuration of recognition was generated in 1967 in the United Kingdom by David Whitaker[4] (regarded as the "Father of the ISBN"[5]) and in 1968 in the US by Emery Koltay[4] (who later became director of the U.S. ISBN agency R.R. Bowker).[5][6][7]		The 10-digit ISBN format was developed by the International Organization for Standardization (ISO) and was published in 1970 as international standard ISO 2108.[3][4] The United Kingdom continued to use the 9-digit SBN code until 1974. ISO has appointed the International ISBN Agency as the registration authority for ISBN worldwide and the ISBN Standard is developed under the control of ISO Technical Committee 46/Subcommittee 9 TC 46/SC 9. The ISO on-line facility only refers back to 1978.[8]		An SBN may be converted to an ISBN by prefixing the digit "0". For example, the second edition of Mr. J. G. Reeder Returns, published by Hodder in 1965, has "SBN 340 01381 8" – 340 indicating the publisher, 01381 their serial number, and 8 being the check digit. This can be converted to ISBN 0-340-01381-8; the check digit does not need to be re-calculated.		Since 1 January 2007, ISBNs have contained 13 digits, a format that is compatible with "Bookland" European Article Number EAN-13s.[9]		An ISBN is assigned to each edition and variation (except reprintings) of a book. For example, an ebook, a paperback, and a hardcover edition of the same book would each have a different ISBN.[10] The ISBN is 13 digits long if assigned on or after 1 January 2007, and 10 digits long if assigned before 2007. An International Standard Book Number consists of 4 parts (if it is a 10 digit ISBN) or 5 parts (for a 13 digit ISBN):		A 13-digit ISBN can be separated into its parts (prefix element, registration group, registrant, publication and check digit), and when this is done it is customary to separate the parts with hyphens or spaces. Separating the parts (registration group, registrant, publication and check digit) of a 10-digit ISBN is also done with either hyphens or spaces. Figuring out how to correctly separate a given ISBN number is complicated, because most of the parts do not use a fixed number of digits.[13]		ISBN issuance is country-specific, in that ISBNs are issued by the ISBN registration agency that is responsible for that country or territory regardless of the publication language. The ranges of ISBNs assigned to any particular country are based on the publishing profile of the country concerned, and so the ranges will vary depending on the number of books and the number, type, and size of publishers that are active. Some ISBN registration agencies are based in national libraries or within ministries of culture and thus may receive direct funding from government to support their services. In other cases, the ISBN registration service is provided by organisations such as bibliographic data providers that are not government funded. In Canada, ISBNs are issued at no cost with the stated purpose of encouraging Canadian culture.[14] In the United Kingdom, United States, and some other countries, where the service is provided by non-government-funded organisations, the issuing of ISBNs requires payment of a fee.		Australia: ISBNs are issued by the commercial library services agency Thorpe-Bowker,[15] and prices range from $42 for a single ISBN (plus a $55 registration fee for new publishers) to $2,890 for a block of 1,000 ISBNs. Access is immediate when requested via their website.[16]		Brazil: National Library of Brazil, a government agency, is responsible for issuing ISBNs, and there is a cost of R$16 [17]		Canada: Library and Archives Canada, a government agency, is responsible for issuing ISBNs, and there is no cost. Works in French are issued an ISBN by the Bibliothèque et Archives nationales du Québec.		Colombia: Cámara Colombiana del Libro, a NGO, is responsible for issuing ISBNs. Cost of issuing an ISBN is about USD 20.		Hong Kong: The Books Registration Office (BRO), under the Hong Kong Public Libraries, issues ISBNs in Hong Kong. There is no fee.[18]		India: The Raja Rammohun Roy National Agency for ISBN (Book Promotion and Copyright Division), under Department of Higher Education, a constituent of the Ministry of Human Resource Development, is responsible for registration of Indian publishers, authors, universities, institutions, and government departments that are responsible for publishing books.[19] There is no fee associated in getting ISBN in India.[20]		Italy: The privately held company EDISER srl, owned by Associazione Italiana Editori (Italian Publishers Association) is responsible for issuing ISBNs.[21] The original national prefix 978-88 is reserved for publishing companies, starting at €49 for a ten-codes block[22] while a new prefix 979-12 is dedicated to self-publishing authors, at a fixed price of €25 for a single code.		Maldives: The National Bureau of Classification (NBC) is responsible for ISBN registrations for publishers who are publishing in the Maldives.[citation needed]		Malta: The National Book Council (Maltese: Il-Kunsill Nazzjonali tal-Ktieb) issues ISBN registrations in Malta.[23][24][25]		Morocco: The National Library of Morocco is responsible for ISBN registrations for publishing in Morocco and Moroccan-occupied portion of Western Sahara.		New Zealand: The National Library of New Zealand is responsible for ISBN registrations for publishers who are publishing in New Zealand.[26]		Pakistan: The National Library of Pakistan is responsible for ISBN registrations for Pakistani publishers, authors, universities, institutions, and government departments that are responsible for publishing books.		South Africa: The National Library of South Africa is responsible for ISBN issuance for South African publishing institutions and authors.		United Kingdom and Republic of Ireland: The privately held company Nielsen Book Services Ltd, part of Nielsen Holdings N.V., is responsible for issuing ISBNs in blocks of 10, 100 or 1000. Prices start from £120 (plus VAT) for the smallest block on a standard turnaround of ten days.[27]		United States: In the United States, the privately held company R.R. Bowker issues ISBNs.[4] There is a charge that varies depending upon the number of ISBNs purchased, with prices starting at $125.00 for a single number. Access is immediate when requested via their website.[28]		Publishers and authors in other countries obtain ISBNs from their respective national ISBN registration agency. A directory of ISBN agencies is available on the International ISBN Agency website.		The registration group identifier is a 1- to 5-digit number that is valid within a single prefix element (i.e. one of 978 or 979).[11] Registration group identifiers have primarily been allocated within the 978 prefix element.[29] The single-digit group identifiers within the 978 prefix element are: 0 or 1 for English-speaking countries; 2 for French-speaking countries; 3 for German-speaking countries; 4 for Japan; 5 for Russian-speaking countries; and 7 for People's Republic of China. An example 5-digit group identifier is 99936, for Bhutan. The allocated group IDs are: 0–5, 600–621, 7, 80–94, 950–989, 9926–9989, and 99901–99976.[30] Books published in rare languages typically have longer group identifiers.[31]		Within the 979 prefix element, the registration group identifier 0 is reserved for compatibility with International Standard Music Numbers (ISMNs), but such material is not actually assigned an ISBN.[11] The registration group identifiers within prefix element 979 that have been assigned are 10 for France, 11 for the Republic of Korea, and 12 for Italy.[32]		The original 9-digit standard book number (SBN) had no registration group identifier, but prefixing a zero (0) to a 9-digit SBN creates a valid 10-digit ISBN.		The national ISBN agency assigns the registrant element (cf. Category:ISBN agencies) and an accompanying series of ISBNs within that registrant element to the publisher; the publisher then allocates one of the ISBNs to each of its books. In most countries, a book publisher is not required by law to assign an ISBN; however, most bookstores only handle ISBN bearing publications.[citation needed]		A listing of more than 900,000 assigned publisher codes is published, and can be ordered in book form (€1399, US$1959). The web site of the ISBN agency does not offer any free method of looking up publisher codes.[33] Partial lists have been compiled (from library catalogs) for the English-language groups: identifier 0 and identifier 1.		Publishers receive blocks of ISBNs, with larger blocks allotted to publishers expecting to need them; a small publisher may receive ISBNs of one or more digits for the registration group identifier, several digits for the registrant, and a single digit for the publication element. Once that block of ISBNs is used, the publisher may receive another block of ISBNs, with a different registrant element. Consequently, a publisher may have different allotted registrant elements. There also may be more than one registration group identifier used in a country. This might occur once all the registrant elements from a particular registration group have been allocated to publishers.		By using variable block lengths, registration agencies are able to customise the allocations of ISBNs that they make to publishers. For example, a large publisher may be given a block of ISBNs where fewer digits are allocated for the registrant element and many digits are allocated for the publication element; likewise, countries publishing many titles have few allocated digits for the registration group identifier and many for the registrant and publication elements.[34] Here are some sample ISBN-10 codes, illustrating block length variations.		English-language registration group elements are 0 and 1 (2 of more than 220 registration group elements). These two registration group elements are divided into registrant elements in a systematic pattern, which allows their length to be determined, as follows:[35]		A check digit is a form of redundancy check used for error detection, the decimal equivalent of a binary check bit. It consists of a single digit computed from the other digits in the number. The method for the ten digit code is an extension of that for SBNs, the two systems are compatible, and SBN prefixed with "0" will give the same check-digit as without – the digit is base eleven, and can be 0-9 or X. The system for thirteen digit codes is not compatible and will, in general, give a different check digit from the corresponding 10 digit ISBN, and does not provide the same protection against transposition. This is because the thirteen digit code was required to be compatible with the EAN format, and hence could not contain an "X".		The 2001 edition of the official manual of the International ISBN Agency says that the ISBN-10 check digit[36] – which is the last digit of the ten-digit ISBN – must range from 0 to 10 (the symbol X is used for 10), and must be such that the sum of all the ten digits, each multiplied by its (integer) weight, descending from 10 to 1, is a multiple of 11.		For example, for an ISBN-10 of 0-306-40615-2:		Formally, using modular arithmetic, we can say:		It is also true for ISBN-10's that the sum of all the ten digits, each multiplied by its weight in ascending order from 1 to 10, is a multiple of 11. For this example:		Formally, we can say:		The two most common errors in handling an ISBN (e.g., typing or writing it) are a single altered digit or the transposition of adjacent digits. It can be proved that all possible valid ISBN-10's have at least two digits different from each other. It can also be proved that there are no pairs of valid ISBN-10's with eight identical digits and two transposed digits. (These are true only because the ISBN is less than 11 digits long, and because 11 is a prime number.) The ISBN check digit method therefore ensures that it will always be possible to detect these two most common types of error, i.e. if either of these types of error has occurred, the result will never be a valid ISBN – the sum of the digits multiplied by their weights will never be a multiple of 11. However, if the error occurs in the publishing house and goes undetected, the book will be issued with an invalid ISBN.[37]		In contrast, it is possible for other types of error, such as two altered non-transposed digits, or three altered digits, to result in a valid ISBN number (although it is still unlikely).		Modular arithmetic is convenient for calculating the check digit using modulus 11. Each of the first nine digits of the ten-digit ISBN—excluding the check digit itself—is multiplied by a number in a sequence from 10 to 2, and the remainder of the sum, with respect to 11, is computed. The resulting remainder, plus the check digit, must equal a multiple of 11 (either 0 or 11). Therefore, the check digit is (11 minus the remainder of the sum of the products modulo 11) modulo 11. Taking the remainder modulo 11 a second time accounts for the possibility that the first remainder is 0. Without the second modulo operation the calculation could end up with 11 – 0 = 11 which is invalid. (Strictly speaking the first "modulo 11" is unneeded, but it may be considered to simplify the calculation.)		For example, the check digit for an ISBN-10 of 0-306-40615-? is calculated as follows:		Thus the check digit is 2, and the complete sequence is ISBN 0-306-40615-2. The value x 10 {\displaystyle x_{10}} required to satisfy this condition might be 10; if so, an 'X' should be used.		It is possible to avoid the multiplications in a software implementation by using two accumulators. Repeatedly adding t into s computes the necessary multiples:		The modular reduction can be done once at the end, as shown above (in which case s could hold a value as large as 496, for the invalid ISBN 99999-999-9-X), or s and t could be reduced by a conditional subtract after each addition.		The 2005 edition of the International ISBN Agency's official manual[38] describes how the 13-digit ISBN check digit is calculated. The ISBN-13 check digit, which is the last digit of the ISBN, must range from 0 to 9 and must be such that the sum of all the thirteen digits, each multiplied by its (integer) weight, alternating between 1 and 3, is a multiple of 10.		Formally, using modular arithmetic, we can say:		The calculation of an ISBN-13 check digit begins with the first 12 digits of the thirteen-digit ISBN (thus excluding the check digit itself). Each digit, from left to right, is alternately multiplied by 1 or 3, then those products are summed modulo 10 to give a value ranging from 0 to 9. Subtracted from 10, that leaves a result from 1 to 10. A zero (0) replaces a ten (10), so, in all cases, a single check digit results.		For example, the ISBN-13 check digit of 978-0-306-40615-? is calculated as follows:		Thus, the check digit is 7, and the complete sequence is ISBN 978-0-306-40615-7.		In general, the ISBN-13 check digit is calculated as follows.		Let		Then		This check system – similar to the UPC check digit formula – does not catch all errors of adjacent digit transposition. Specifically, if the difference between two adjacent digits is 5, the check digit will not catch their transposition. For instance, the above example allows this situation with the 6 followed by a 1. The correct order contributes 3×6+1×1 = 19 to the sum; while, if the digits are transposed (1 followed by a 6), the contribution of those two digits will be 3×1+1×6 = 9. However, 19 and 9 are congruent modulo 10, and so produce the same, final result: both ISBNs will have a check digit of 7. The ISBN-10 formula uses the prime modulus 11 which avoids this blind spot, but requires more than the digits 0-9 to express the check digit.		Additionally, if the sum of the 2nd, 4th, 6th, 8th, 10th, and 12th digits is tripled then added to the remaining digits (1st, 3rd, 5th, 7th, 9th, 11th, and 13th), the total will always be divisible by 10 (i.e., end in 0).		The conversion is quite simple as one only needs to prefix "978" to the existing number and calculate the new checksum using the ISBN-13 algorithm.		Publishers and libraries have varied policies about the use of the ISBN check digit. Publishers sometimes fail to check the correspondence of a book title and its ISBN before publishing it; that failure causes book identification problems for libraries, booksellers, and readers.[39] For example, ISBN 0-590-76484-5 is shared by two books – Ninja gaiden® : a novel based on the best-selling game by Tecmo (1990) and Wacky Laws (1997), both published by Scholastic.		Most libraries and booksellers display the book record for an invalid ISBN issued by the publisher. The Library of Congress catalogue contains books published with invalid ISBNs, which it usually tags with the phrase "Cancelled ISBN".[40] However, book-ordering systems such as Amazon.com will not search for a book if an invalid ISBN is entered to its search engine.[citation needed] OCLC often indexes by invalid ISBNs, if the book is indexed in that way by a member library.		Only the term "ISBN" should be used; the terms "eISBN" and "e-ISBN" have historically been sources of confusion and should be avoided. If a book exists in one or more digital (e-book) formats, each of those formats must have its own ISBN. In other words, each of the three separate EPUB, Amazon Kindle, and PDF formats of a particular book will have its own specific ISBN. They should not share the ISBN of the paper version, and there is no generic "eISBN" which encompasses all the e-book formats for a title.[41]		Currently the barcodes on a book's back cover (or inside a mass-market paperback book's front cover) are EAN-13; they may have a separate barcode encoding five digits for the currency and the recommended retail price.[42] For 10 digit ISBNs, the number "978", the Bookland "country code", is prefixed to the ISBN in the barcode data, and the check digit is recalculated according to the EAN13 formula (modulo 10, 1x and 3x weighting on alternate digits).		Partly because of an expected shortage in certain ISBN categories, the International Organization for Standardization (ISO) decided to migrate to a thirteen-digit ISBN (ISBN-13). The process began 1 January 2005 and was planned to conclude 1 January 2007.[43] As of 2011, all the 13-digit ISBNs began with 978. As the 978 ISBN supply is exhausted, the 979 prefix was introduced. Part of the 979 prefix is reserved for use with the Musicland code for musical scores with an ISMN. 10 digit ISMN codes differed visually as they began with an "M" letter; the bar code represents the "M" as a zero (0), and for checksum purposes it counted as a 3. All ISMNs are now 13 digits commencing 979-0; 979-1 to 979-9 will be used by ISBN.		Publisher identification code numbers are unlikely to be the same in the 978 and 979 ISBNs, likewise, there is no guarantee that language area code numbers will be the same. Moreover, the ten-digit ISBN check digit generally is not the same as the thirteen-digit ISBN check digit. Because the GTIN-13 is part of the Global Trade Item Number (GTIN) system (that includes the GTIN-14, the GTIN-12, and the GTIN-8), the 13-digit ISBN falls within the 14-digit data field range.[44]		Barcode format compatibility is maintained, because (aside from the group breaks) the ISBN-13 barcode format is identical to the EAN barcode format of existing 10-digit ISBNs. So, migration to an EAN-based system allows booksellers the use of a single numbering system for both books and non-book products that is compatible with existing ISBN based data, with only minimal changes to information technology systems. Hence, many booksellers (e.g., Barnes & Noble) migrated to EAN barcodes as early as March 2005. Although many American and Canadian booksellers were able to read EAN-13 barcodes before 2005, most general retailers could not read them. The upgrading of the UPC barcode system to full EAN-13, in 2005, eased migration to the ISBN-13 in North America.		
American Splendor is a series of autobiographical comic books written by Harvey Pekar and drawn by a variety of artists. The first issue was published in 1976 and the most recent in September 2008, with publication occurring at irregular intervals. Publishers have been, at various times, Harvey Pekar himself, Dark Horse Comics, and DC Comics.[1]		The comics have been adapted into a film of the same name and a number of theatrical productions.						Despite comic books in the United States being traditionally the province of fantasy-adventure and other genre stories, Pekar felt that the medium could be put to wider use:		When I was a little kid, and I was reading these comics in the '40s, I kind of got sick of them because after a while, they were just formulaic. I figured there was some kind of a flaw that keeps them from getting better than they are, and then when I saw Robert Crumb's work in the early '60s, when he moved from Philadelphia to Cleveland, and he moved around the corner from me, I thought 'Man, comics are where it's at'.[2]		Pekar's philosophy of the potential of comics is also expressed in his often repeated statement that 'comics are words and pictures. You can do anything with words and pictures'. In an interview with Walrus Comix, Pekar described how the idea of producing his own comic book developed. In 1972 when Crumb was visiting him in Cleveland, Pekar showed him his story ideas. Not only did Crumb agree to draw some of them but also offered to show them to other artists to draw. By 1975, Pekar decided to produce and publish his own comic book.[3]		The stories in American Splendor concern the everyday life of Pekar in Cleveland, Ohio. Situations covered include Pekar's job as a file clerk at a Veteran's Administration hospital and his relations with colleagues and patients there. There are also stories about Pekar and his relations with friends and family, including his third wife Joyce Brabner and their adopted daughter Danielle. Other stories concern everyday situations such as Pekar's troubles with his car, money, his health, and his concerns and anxieties in general.[1] Several issues (#14, #13, #18) give accounts of Pekar's becoming a recurring guest on the NBC television show Late Night with David Letterman, including a 1987 interview segment in which Pekar criticized Letterman for ducking criticism of General Electric, the parent company of NBC. American Splendor sometimes departs from Pekar's own life, with stories about jazz musicians (#23), the artists for his comics (#25), and a three-issue miniseries American Splendor: Unsung Hero (#29-31), which chronicles the Vietnam experience of Pekar's African-American co-worker Robert McNeill.		As Pekar was not an artist himself, and was incapable of "drawing a straight line", according to a line in the film version of his story, he recruited his friend, underground comics artist Robert Crumb, to help create a comics series. Besides Crumb, other notable American Splendor illustrators include Alison Bechdel, Brian Bram, Chester Brown, Alan Moore, David Collier, Gary Dumm, Frank Stack, Drew Friedman, Dean Haspiel, Val Mayerik, Josh Neufeld, Spain Rodriguez, Joe Sacco, Gerry Shamray, Jim Woodring, Joe Zabel, Ed Piskor, and Greg Budgett. Later issues employed a new crop of artists, including Ty Templeton, Richard Corben, Hunt Emerson, Eddie Campbell, Gilbert Hernandez, Ho Che Anderson, and Rick Geary.		Pekar produced seventeen issues of American Splendor from 1976 to 1993, which, except for the last few issues, he also self-published and self-distributed. By keeping back issues in print and available (contrary to the industry practice of the time), Pekar continued to receive income on previously-completed work, although at the time some of them were published, according to his Comics Journal interview, he was losing thousands of dollars per year on the books.[4] Starting in 1994, additional American Splendor were published by Dark Horse Comics, although these issues are not numbered. They include the two-issue American Splendor: Windfall and several themed issues such as American Splendor: Transatlantic Comics and American Splendor: On the Job. In September 2006, a four-issue American Splendor mini-series was published by the DC Comics imprint Vertigo. A second four-issue miniseries was published by DC in 2008.		Many stories from American Splendor have been collected into trade paperbacks from various publishers, their material not (for the most part) overlapping.		Pekar wrote two larger works which carry the American Splendor label, Our Movie Year (Ballantine Books, 2004), a collection of comics written about or at the time of the American Splendor film, and Ego & Hubris: The Michael Malice Story (Ballantine, 2006), a biography of the early life of the author Michael Malice.		Pekar also wrote two graphic novels which are not officially labeled American Splendor but which should arguably be considered part of it: Our Cancer Year (Four Walls Eight Windows, 1994), co-written with Pekar's wife Joyce Brabner and illustrated by Frank Stack, covering the year when Pekar was diagnosed with cancer; and The Quitter (DC Comics, 2005), illustrated by Dean Haspiel, which deals with Pekar's youth.		In 2003 a movie adaptation featuring Paul Giamatti playing Pekar (as well as appearances by Pekar himself) and Hope Davis as his wife was released to critical acclaim and first honors at the Sundance Film Festival in addition to the Writers Guild of America Award for best adapted screenplay. It was written and directed by documentarists Shari Springer Berman and Robert Pulcini. It was filmed entirely on location in Cleveland and Lakewood in Ohio. It was nominated for Best Adapted Screenplay at the 2003 Academy Awards (it lost to The Lord of the Rings: The Return of the King).		Theatrical productions based on American Splendor have been mounted over the years. The first of these was produced by The Independent Eye in Lancaster, Pennsylvania in 1985, adapted and directed by Conrad Bishop. The second, produced in 1987 at Washington, DC's Arena Stage, was adapted by Lloyd Rose and directed by James C. Nicola. The third, which is represented in fictionalized form in the American Splendor movie, ran from September 1990 through September 1991 at Hollywood's Theater in Los Angeles, California; it was adapted and directed by Vince Waldron, and starred Dan Castellaneta as Harvey.		
A computer is a device that can be instructed to carry out arbitrary sequences of arithmetic or logical operations automatically. The ability of computers to follow generalized sets of operations, called programs, enable them to perform an extremely wide range of tasks.		Such computers are used as control systems for a very wide variety of industrial and consumer devices. This includes simple special purpose devices like microwave ovens and remote controls, factory devices such as industrial robots and computer assisted design, but also in general purpose devices like personal computers and mobile devices such as smartphones. The Internet is run on computers and it connects millions of other computers.		Since ancient times, simple manual devices like the abacus aided people in doing calculations. Early in the Industrial Revolution, some mechanical devices were built to automate long tedious tasks, such as guiding patterns for looms. More sophisticated electrical machines did specialized analog calculations in the early 20th century. The first digital electronic calculating machines were developed during World War II. The speed, power, and versatility of computers has increased continuously and dramatically since then.		Conventionally, a modern computer consists of at least one processing element, typically a central processing unit (CPU), and some form of memory. The processing element carries out arithmetic and logical operations, and a sequencing and control unit can change the order of operations in response to stored information. Peripheral devices include input devices (keyboards, mice, joystick, etc.), output devices (monitor screens, printers, etc.), and input/output devices that perform both functions (e.g., the 2000s-era touchscreen). Peripheral devices allow information to be retrieved from an external source and they enable the result of operations to be saved and retrieved.		According to the Oxford English Dictionary, the first known use of the word "computer" was in 1613 in a book called The Yong Mans Gleanings by English writer Richard Braithwait: "I haue [sic] read the truest computer of Times, and the best Arithmetician that euer [sic] breathed, and he reduceth thy dayes into a short number." This usage of the term referred to a person who carried out calculations or computations. The word continued with the same meaning until the middle of the 20th century. From the end of the 19th century the word began to take on its more familiar meaning, a machine that carries out computations.[1]		The Online Etymology Dictionary gives the first attested use of "computer" in the "1640s, [meaning] "one who calculates,"; this is an "... agent noun from compute (v.)". The Online Etymology Dictionary states that the use of the term to mean "calculating machine" (of any type) is from 1897." The Online Etymology Dictionary indicates that the "modern use" of the term, to mean "programmable digital electronic computer" dates from "... 1945 under this name; [in a] theoretical [sense] from 1937, as Turing machine".[2]		Devices have been used to aid computation for thousands of years, mostly using one-to-one correspondence with fingers. The earliest counting device was probably a form of tally stick. Later record keeping aids throughout the Fertile Crescent included calculi (clay spheres, cones, etc.) which represented counts of items, probably livestock or grains, sealed in hollow unbaked clay containers.[3][4] The use of counting rods is one example.		The abacus was initially used for arithmetic tasks. The Roman abacus was developed from devices used in Babylonia as early as 2400 BC. Since then, many other forms of reckoning boards or tables have been invented. In a medieval European counting house, a checkered cloth would be placed on a table, and markers moved around on it according to certain rules, as an aid to calculating sums of money.		The Antikythera mechanism is believed to be the earliest mechanical analog "computer", according to Derek J. de Solla Price.[5] It was designed to calculate astronomical positions. It was discovered in 1901 in the Antikythera wreck off the Greek island of Antikythera, between Kythera and Crete, and has been dated to circa 100 BC. Devices of a level of complexity comparable to that of the Antikythera mechanism would not reappear until a thousand years later.		Many mechanical aids to calculation and measurement were constructed for astronomical and navigation use. The planisphere was a star chart invented by Abū Rayhān al-Bīrūnī in the early 11th century.[6] The astrolabe was invented in the Hellenistic world in either the 1st or 2nd centuries BC and is often attributed to Hipparchus. A combination of the planisphere and dioptra, the astrolabe was effectively an analog computer capable of working out several different kinds of problems in spherical astronomy. An astrolabe incorporating a mechanical calendar computer[7][8] and gear-wheels was invented by Abi Bakr of Isfahan, Persia in 1235.[9] Abū Rayhān al-Bīrūnī invented the first mechanical geared lunisolar calendar astrolabe,[10] an early fixed-wired knowledge processing machine[11] with a gear train and gear-wheels,[12] circa 1000 AD.		The sector, a calculating instrument used for solving problems in proportion, trigonometry, multiplication and division, and for various functions, such as squares and cube roots, was developed in the late 16th century and found application in gunnery, surveying and navigation.		The planimeter was a manual instrument to calculate the area of a closed figure by tracing over it with a mechanical linkage.		The slide rule was invented around 1620–1630, shortly after the publication of the concept of the logarithm. It is a hand-operated analog computer for doing multiplication and division. As slide rule development progressed, added scales provided reciprocals, squares and square roots, cubes and cube roots, as well as transcendental functions such as logarithms and exponentials, circular and hyperbolic trigonometry and other functions. Aviation is one of the few fields where slide rules are still in widespread use, particularly for solving time–distance problems in light aircraft. To save space and for ease of reading, these are typically circular devices rather than the classic linear slide rule shape. A popular example is the E6B.		In the 1770s Pierre Jaquet-Droz, a Swiss watchmaker, built a mechanical doll (automata) that could write holding a quill pen. By switching the number and order of its internal wheels different letters, and hence different messages, could be produced. In effect, it could be mechanically "programmed" to read instructions. Along with two other complex machines, the doll is at the Musée d'Art et d'Histoire of Neuchâtel, Switzerland, and still operates.[13]		The tide-predicting machine invented by Sir William Thomson in 1872 was of great utility to navigation in shallow waters. It used a system of pulleys and wires to automatically calculate predicted tide levels for a set period at a particular location.		The differential analyser, a mechanical analog computer designed to solve differential equations by integration, used wheel-and-disc mechanisms to perform the integration. In 1876 Lord Kelvin had already discussed the possible construction of such calculators, but he had been stymied by the limited output torque of the ball-and-disk integrators.[14] In a differential analyzer, the output of one integrator drove the input of the next integrator, or a graphing output. The torque amplifier was the advance that allowed these machines to work. Starting in the 1920s, Vannevar Bush and others developed mechanical differential analyzers.		Charles Babbage, an English mechanical engineer and polymath, originated the concept of a programmable computer. Considered the "father of the computer",[15] he conceptualized and invented the first mechanical computer in the early 19th century. After working on his revolutionary difference engine, designed to aid in navigational calculations, in 1833 he realized that a much more general design, an Analytical Engine, was possible. The input of programs and data was to be provided to the machine via punched cards, a method being used at the time to direct mechanical looms such as the Jacquard loom. For output, the machine would have a printer, a curve plotter and a bell. The machine would also be able to punch numbers onto cards to be read in later. The Engine incorporated an arithmetic logic unit, control flow in the form of conditional branching and loops, and integrated memory, making it the first design for a general-purpose computer that could be described in modern terms as Turing-complete.[16][17]		The machine was about a century ahead of its time. All the parts for his machine had to be made by hand — this was a major problem for a device with thousands of parts. Eventually, the project was dissolved with the decision of the British Government to cease funding. Babbage's failure to complete the analytical engine can be chiefly attributed to difficulties not only of politics and financing, but also to his desire to develop an increasingly sophisticated computer and to move ahead faster than anyone else could follow. Nevertheless, his son, Henry Babbage, completed a simplified version of the analytical engine's computing unit (the mill) in 1888. He gave a successful demonstration of its use in computing tables in 1906.		During the first half of the 20th century, many scientific computing needs were met by increasingly sophisticated analog computers, which used a direct mechanical or electrical model of the problem as a basis for computation. However, these were not programmable and generally lacked the versatility and accuracy of modern digital computers.[18] The first modern analog computer was a tide-predicting machine, invented by Sir William Thomson in 1872. The differential analyser, a mechanical analog computer designed to solve differential equations by integration using wheel-and-disc mechanisms, was conceptualized in 1876 by James Thomson, the brother of the more famous Lord Kelvin.[14]		The art of mechanical analog computing reached its zenith with the differential analyzer, built by H. L. Hazen and Vannevar Bush at MIT starting in 1927. This built on the mechanical integrators of James Thomson and the torque amplifiers invented by H. W. Nieman. A dozen of these devices were built before their obsolescence became obvious. By the 1950s the success of digital electronic computers had spelled the end for most analog computing machines, but analog computers remained in use during the 1950s in some specialized applications such as education (control systems) and aircraft (slide rule).		By 1938 the United States Navy had developed an electromechanical analog computer small enough to use aboard a submarine. This was the Torpedo Data Computer, which used trigonometry to solve the problem of firing a torpedo at a moving target. During World War II similar devices were developed in other countries as well.		Early digital computers were electromechanical; electric switches drove mechanical relays to perform the calculation. These devices had a low operating speed and were eventually superseded by much faster all-electric computers, originally using vacuum tubes. The Z2, created by German engineer Konrad Zuse in 1939, was one of the earliest examples of an electromechanical relay computer.[19]		In 1941, Zuse followed his earlier machine up with the Z3, the world's first working electromechanical programmable, fully automatic digital computer.[20][21] The Z3 was built with 2000 relays, implementing a 22 bit word length that operated at a clock frequency of about 5–10 Hz.[22] Program code was supplied on punched film while data could be stored in 64 words of memory or supplied from the keyboard. It was quite similar to modern machines in some respects, pioneering numerous advances such as floating point numbers. Rather than the harder-to-implement decimal system (used in Charles Babbage's earlier design), using a binary system meant that Zuse's machines were easier to build and potentially more reliable, given the technologies available at that time.[23] The Z3 was Turing complete.[24][25]		Purely electronic circuit elements soon replaced their mechanical and electromechanical equivalents, at the same time that digital calculation replaced analog. The engineer Tommy Flowers, working at the Post Office Research Station in London in the 1930s, began to explore the possible use of electronics for the telephone exchange. Experimental equipment that he built in 1934 went into operation five years later, converting a portion of the telephone exchange network into an electronic data processing system, using thousands of vacuum tubes.[18] In the US, John Vincent Atanasoff and Clifford E. Berry of Iowa State University developed and tested the Atanasoff–Berry Computer (ABC) in 1942,[26] the first "automatic electronic digital computer".[27] This design was also all-electronic and used about 300 vacuum tubes, with capacitors fixed in a mechanically rotating drum for memory.[28]		During World War II, the British at Bletchley Park achieved a number of successes at breaking encrypted German military communications. The German encryption machine, Enigma, was first attacked with the help of the electro-mechanical bombes. To crack the more sophisticated German Lorenz SZ 40/42 machine, used for high-level Army communications, Max Newman and his colleagues commissioned Flowers to build the Colossus.[28] He spent eleven months from early February 1943 designing and building the first Colossus.[29] After a functional test in December 1943, Colossus was shipped to Bletchley Park, where it was delivered on 18 January 1944[30] and attacked its first message on 5 February.[28]		Colossus was the world's first electronic digital programmable computer.[18] It used a large number of valves (vacuum tubes). It had paper-tape input and was capable of being configured to perform a variety of boolean logical operations on its data, but it was not Turing-complete. Nine Mk II Colossi were built (The Mk I was converted to a Mk II making ten machines in total). Colossus Mark I contained 1,500 thermionic valves (tubes), but Mark II with 2,400 valves, was both 5 times faster and simpler to operate than Mark I, greatly speeding the decoding process.[31][32]		The U.S.-built ENIAC[33] (Electronic Numerical Integrator and Computer) was the first electronic programmable computer built in the US. Although the ENIAC was similar to the Colossus, it was much faster, more flexible, and it was Turing-complete. Like the Colossus, a "program" on the ENIAC was defined by the states of its patch cables and switches, a far cry from the stored program electronic machines that came later. Once a program was written, it had to be mechanically set into the machine with manual resetting of plugs and switches.		It combined the high speed of electronics with the ability to be programmed for many complex problems. It could add or subtract 5000 times a second, a thousand times faster than any other machine. It also had modules to multiply, divide, and square root. High speed memory was limited to 20 words (about 80 bytes). Built under the direction of John Mauchly and J. Presper Eckert at the University of Pennsylvania, ENIAC's development and construction lasted from 1943 to full operation at the end of 1945. The machine was huge, weighing 30 tons, using 200 kilowatts of electric power and contained over 18,000 vacuum tubes, 1,500 relays, and hundreds of thousands of resistors, capacitors, and inductors.[34]		The principle of the modern computer was proposed by Alan Turing in his seminal 1936 paper,[35] On Computable Numbers. Turing proposed a simple device that he called "Universal Computing machine" and that is now known as a universal Turing machine. He proved that such a machine is capable of computing anything that is computable by executing instructions (program) stored on tape, allowing the machine to be programmable. The fundamental concept of Turing's design is the stored program, where all the instructions for computing are stored in memory. Von Neumann acknowledged that the central concept of the modern computer was due to this paper.[36] Turing machines are to this day a central object of study in theory of computation. Except for the limitations imposed by their finite memory stores, modern computers are said to be Turing-complete, which is to say, they have algorithm execution capability equivalent to a universal Turing machine.		Early computing machines had fixed programs. Changing its function required the re-wiring and re-structuring of the machine.[28] With the proposal of the stored-program computer this changed. A stored-program computer includes by design an instruction set and can store in memory a set of instructions (a program) that details the computation. The theoretical basis for the stored-program computer was laid by Alan Turing in his 1936 paper. In 1945 Turing joined the National Physical Laboratory and began work on developing an electronic stored-program digital computer. His 1945 report "Proposed Electronic Calculator" was the first specification for such a device. John von Neumann at the University of Pennsylvania also circulated his First Draft of a Report on the EDVAC in 1945.[18]		The Manchester Small-Scale Experimental Machine, nicknamed Baby, was the world's first stored-program computer. It was built at the Victoria University of Manchester by Frederic C. Williams, Tom Kilburn and Geoff Tootill, and ran its first program on 21 June 1948.[37] It was designed as a testbed for the Williams tube, the first random-access digital storage device.[38] Although the computer was considered "small and primitive" by the standards of its time, it was the first working machine to contain all of the elements essential to a modern electronic computer.[39] As soon as the SSEM had demonstrated the feasibility of its design, a project was initiated at the university to develop it into a more usable computer, the Manchester Mark 1.		The Mark 1 in turn quickly became the prototype for the Ferranti Mark 1, the world's first commercially available general-purpose computer.[40] Built by Ferranti, it was delivered to the University of Manchester in February 1951. At least seven of these later machines were delivered between 1953 and 1957, one of them to Shell labs in Amsterdam.[41] In October 1947, the directors of British catering company J. Lyons & Company decided to take an active role in promoting the commercial development of computers. The LEO I computer became operational in April 1951[42] and ran the world's first regular routine office computer job.		The bipolar transistor was invented in 1947. From 1955 onwards transistors replaced vacuum tubes in computer designs, giving rise to the "second generation" of computers. Compared to vacuum tubes, transistors have many advantages: they are smaller, and require less power than vacuum tubes, so give off less heat. Silicon junction transistors were much more reliable than vacuum tubes and had longer, indefinite, service life. Transistorized computers could contain tens of thousands of binary logic circuits in a relatively compact space.		At the University of Manchester, a team under the leadership of Tom Kilburn designed and built a machine using the newly developed transistors instead of valves.[43] Their first transistorised computer and the first in the world, was operational by 1953, and a second version was completed there in April 1955. However, the machine did make use of valves to generate its 125 kHz clock waveforms and in the circuitry to read and write on its magnetic drum memory, so it was not the first completely transistorized computer. That distinction goes to the Harwell CADET of 1955,[44] built by the electronics division of the Atomic Energy Research Establishment at Harwell.[44][45]		The next great advance in computing power came with the advent of the integrated circuit. The idea of the integrated circuit was first conceived by a radar scientist working for the Royal Radar Establishment of the Ministry of Defence, Geoffrey W.A. Dummer. Dummer presented the first public description of an integrated circuit at the Symposium on Progress in Quality Electronic Components in Washington, D.C. on 7 May 1952.[46]		The first practical ICs were invented by Jack Kilby at Texas Instruments and Robert Noyce at Fairchild Semiconductor.[47] Kilby recorded his initial ideas concerning the integrated circuit in July 1958, successfully demonstrating the first working integrated example on 12 September 1958.[48] In his patent application of 6 February 1959, Kilby described his new device as "a body of semiconductor material ... wherein all the components of the electronic circuit are completely integrated".[49][50] Noyce also came up with his own idea of an integrated circuit half a year later than Kilby.[51] His chip solved many practical problems that Kilby's had not. Produced at Fairchild Semiconductor, it was made of silicon, whereas Kilby's chip was made of germanium.		This new development heralded an explosion in the commercial and personal use of computers and led to the invention of the microprocessor. While the subject of exactly which device was the first microprocessor is contentious, partly due to lack of agreement on the exact definition of the term "microprocessor", it is largely undisputed that the first single-chip microprocessor was the Intel 4004,[52] designed and realized by Ted Hoff, Federico Faggin, and Stanley Mazor at Intel.[53]		With the continued miniaturization of computing resources, and advancements in portable battery life, portable computers grew in popularity in the 2000s.[54] The same developments that spurred the growth of laptop computers and other portable computers allowed manufacturers to integrate computing resources into cellular phones. These so-called smartphones and tablets run on a variety of operating systems and have become the dominant computing device on the market, with manufacturers reporting having shipped an estimated 237 million devices in 2Q 2013.[55]		Computers are typically classified based on their uses:		The term hardware covers all of those parts of a computer that are tangible physical objects. Circuits, computer chips, graphic cards, sound cards, memory (RAM), motherboard, displays, power supplies, cables, keyboards, printers and "mice" input devices are all hardware.		A general purpose computer has four main components: the arithmetic logic unit (ALU), the control unit, the memory, and the input and output devices (collectively termed I/O). These parts are interconnected by buses, often made of groups of wires. Inside each of these parts are thousands to trillions of small electrical circuits which can be turned off or on by means of an electronic switch. Each circuit represents a bit (binary digit) of information so that when the circuit is on it represents a "1", and when off it represents a "0" (in positive logic representation). The circuits are arranged in logic gates so that one or more of the circuits may control the state of one or more of the other circuits.		When unprocessed data is sent to the computer with the help of input devices, the data is processed and sent to output devices. The input devices may be hand-operated or automated. The act of processing is mainly regulated by the CPU. Some examples of hand-operated input devices are:		The means through which computer gives output are known as output devices. Some examples of output devices are:		The control unit (often called a control system or central controller) manages the computer's various components; it reads and interprets (decodes) the program instructions, transforming them into control signals that activate other parts of the computer.[57] Control systems in advanced computers may change the order of execution of some instructions to improve performance.		A key component common to all CPUs is the program counter, a special memory cell (a register) that keeps track of which location in memory the next instruction is to be read from.[58]		The control system's function is as follows—note that this is a simplified description, and some of these steps may be performed concurrently or in a different order depending on the type of CPU:		Since the program counter is (conceptually) just another set of memory cells, it can be changed by calculations done in the ALU. Adding 100 to the program counter would cause the next instruction to be read from a place 100 locations further down the program. Instructions that modify the program counter are often known as "jumps" and allow for loops (instructions that are repeated by the computer) and often conditional instruction execution (both examples of control flow).		The sequence of operations that the control unit goes through to process an instruction is in itself like a short computer program, and indeed, in some more complex CPU designs, there is another yet smaller computer called a microsequencer, which runs a microcode program that causes all of these events to happen.		The control unit, ALU, and registers are collectively known as a central processing unit (CPU). Early CPUs were composed of many separate components but since the mid-1970s CPUs have typically been constructed on a single integrated circuit called a microprocessor.		The ALU is capable of performing two classes of operations: arithmetic and logic.[59] The set of arithmetic operations that a particular ALU supports may be limited to addition and subtraction, or might include multiplication, division, trigonometry functions such as sine, cosine, etc., and square roots. Some can only operate on whole numbers (integers) whilst others use floating point to represent real numbers, albeit with limited precision. However, any computer that is capable of performing just the simplest operations can be programmed to break down the more complex operations into simple steps that it can perform. Therefore, any computer can be programmed to perform any arithmetic operation—although it will take more time to do so if its ALU does not directly support the operation. An ALU may also compare numbers and return boolean truth values (true or false) depending on whether one is equal to, greater than or less than the other ("is 64 greater than 65?"). Logic operations involve Boolean logic: AND, OR, XOR, and NOT. These can be useful for creating complicated conditional statements and processing boolean logic.		Superscalar computers may contain multiple ALUs, allowing them to process several instructions simultaneously.[60] Graphics processors and computers with SIMD and MIMD features often contain ALUs that can perform arithmetic on vectors and matrices.		A computer's memory can be viewed as a list of cells into which numbers can be placed or read. Each cell has a numbered "address" and can store a single number. The computer can be instructed to "put the number 123 into the cell numbered 1357" or to "add the number that is in cell 1357 to the number that is in cell 2468 and put the answer into cell 1595." The information stored in memory may represent practically anything. Letters, numbers, even computer instructions can be placed into memory with equal ease. Since the CPU does not differentiate between different types of information, it is the software's responsibility to give significance to what the memory sees as nothing but a series of numbers.		In almost all modern computers, each memory cell is set up to store binary numbers in groups of eight bits (called a byte). Each byte is able to represent 256 different numbers (28 = 256); either from 0 to 255 or −128 to +127. To store larger numbers, several consecutive bytes may be used (typically, two, four or eight). When negative numbers are required, they are usually stored in two's complement notation. Other arrangements are possible, but are usually not seen outside of specialized applications or historical contexts. A computer can store any kind of information in memory if it can be represented numerically. Modern computers have billions or even trillions of bytes of memory.		The CPU contains a special set of memory cells called registers that can be read and written to much more rapidly than the main memory area. There are typically between two and one hundred registers depending on the type of CPU. Registers are used for the most frequently needed data items to avoid having to access main memory every time data is needed. As data is constantly being worked on, reducing the need to access main memory (which is often slow compared to the ALU and control units) greatly increases the computer's speed.		Computer main memory comes in two principal varieties:		RAM can be read and written to anytime the CPU commands it, but ROM is preloaded with data and software that never changes, therefore the CPU can only read from it. ROM is typically used to store the computer's initial start-up instructions. In general, the contents of RAM are erased when the power to the computer is turned off, but ROM retains its data indefinitely. In a PC, the ROM contains a specialized program called the BIOS that orchestrates loading the computer's operating system from the hard disk drive into RAM whenever the computer is turned on or reset. In embedded computers, which frequently do not have disk drives, all of the required software may be stored in ROM. Software stored in ROM is often called firmware, because it is notionally more like hardware than software. Flash memory blurs the distinction between ROM and RAM, as it retains its data when turned off but is also rewritable. It is typically much slower than conventional ROM and RAM however, so its use is restricted to applications where high speed is unnecessary.[61]		In more sophisticated computers there may be one or more RAM cache memories, which are slower than registers but faster than main memory. Generally computers with this sort of cache are designed to move frequently needed data into the cache automatically, often without the need for any intervention on the programmer's part.		I/O is the means by which a computer exchanges information with the outside world.[62] Devices that provide input or output to the computer are called peripherals.[63] On a typical personal computer, peripherals include input devices like the keyboard and mouse, and output devices such as the display and printer. Hard disk drives, floppy disk drives and optical disc drives serve as both input and output devices. Computer networking is another form of I/O. I/O devices are often complex computers in their own right, with their own CPU and memory. A graphics processing unit might contain fifty or more tiny computers that perform the calculations necessary to display 3D graphics.[citation needed] Modern desktop computers contain many smaller computers that assist the main CPU in performing I/O. A 2016-era flat screen display contains its own computer circuitry.		While a computer may be viewed as running one gigantic program stored in its main memory, in some systems it is necessary to give the appearance of running several programs simultaneously. This is achieved by multitasking i.e. having the computer switch rapidly between running each program in turn.[64] One means by which this is done is with a special signal called an interrupt, which can periodically cause the computer to stop executing instructions where it was and do something else instead. By remembering where it was executing prior to the interrupt, the computer can return to that task later. If several programs are running "at the same time". then the interrupt generator might be causing several hundred interrupts per second, causing a program switch each time. Since modern computers typically execute instructions several orders of magnitude faster than human perception, it may appear that many programs are running at the same time even though only one is ever executing in any given instant. This method of multitasking is sometimes termed "time-sharing" since each program is allocated a "slice" of time in turn.[65]		Before the era of inexpensive computers, the principal use for multitasking was to allow many people to share the same computer. Seemingly, multitasking would cause a computer that is switching between several programs to run more slowly, in direct proportion to the number of programs it is running, but most programs spend much of their time waiting for slow input/output devices to complete their tasks. If a program is waiting for the user to click on the mouse or press a key on the keyboard, then it will not take a "time slice" until the event it is waiting for has occurred. This frees up time for other programs to execute so that many programs may be run simultaneously without unacceptable speed loss.		Some computers are designed to distribute their work across several CPUs in a multiprocessing configuration, a technique once employed only in large and powerful machines such as supercomputers, mainframe computers and servers. Multiprocessor and multi-core (multiple CPUs on a single integrated circuit) personal and laptop computers are now widely available, and are being increasingly used in lower-end markets as a result.		Supercomputers in particular often have highly unique architectures that differ significantly from the basic stored-program architecture and from general purpose computers.[66] They often feature thousands of CPUs, customized high-speed interconnects, and specialized computing hardware. Such designs tend to be useful only for specialized tasks due to the large scale of program organization required to successfully utilize most of the available resources at once. Supercomputers usually see usage in large-scale simulation, graphics rendering, and cryptography applications, as well as with other so-called "embarrassingly parallel" tasks.		Software refers to parts of the computer which do not have a material form, such as programs, data, protocols, etc. Software is that part of a computer system that consists of encoded information or computer instructions, in contrast to the physical hardware from which the system is built. Computer software includes computer programs, libraries and related non-executable data, such as online documentation or digital media. Computer hardware and software require each other and neither can be realistically used on its own. When software is stored in hardware that cannot easily be modified, such as with BIOS ROM in an IBM PC compatible computer, it is sometimes called "firmware".		There are thousands of different programming languages—some intended to be general purpose, others useful only for highly specialized applications.		The defining feature of modern computers which distinguishes them from all other machines is that they can be programmed. That is to say that some type of instructions (the program) can be given to the computer, and it will process them. Modern computers based on the von Neumann architecture often have machine code in the form of an imperative programming language. In practical terms, a computer program may be just a few instructions or extend to many millions of instructions, as do the programs for word processors and web browsers for example. A typical modern computer can execute billions of instructions per second (gigaflops) and rarely makes a mistake over many years of operation. Large computer programs consisting of several million instructions may take teams of programmers years to write, and due to the complexity of the task almost certainly contain errors.		This section applies to most common RAM machine-based computers.		In most cases, computer instructions are simple: add one number to another, move some data from one location to another, send a message to some external device, etc. These instructions are read from the computer's memory and are generally carried out (executed) in the order they were given. However, there are usually specialized instructions to tell the computer to jump ahead or backwards to some other place in the program and to carry on executing from there. These are called "jump" instructions (or branches). Furthermore, jump instructions may be made to happen conditionally so that different sequences of instructions may be used depending on the result of some previous calculation or some external event. Many computers directly support subroutines by providing a type of jump that "remembers" the location it jumped from and another instruction to return to the instruction following that jump instruction.		Program execution might be likened to reading a book. While a person will normally read each word and line in sequence, they may at times jump back to an earlier place in the text or skip sections that are not of interest. Similarly, a computer may sometimes go back and repeat the instructions in some section of the program over and over again until some internal condition is met. This is called the flow of control within the program and it is what allows the computer to perform tasks repeatedly without human intervention.		Comparatively, a person using a pocket calculator can perform a basic arithmetic operation such as adding two numbers with just a few button presses. But to add together all of the numbers from 1 to 1,000 would take thousands of button presses and a lot of time, with a near certainty of making a mistake. On the other hand, a computer may be programmed to do this with just a few simple instructions. The following example is written in the MIPS assembly language:		Once told to run this program, the computer will perform the repetitive addition task without further human intervention. It will almost never make a mistake and a modern PC can complete the task in a fraction of a second.		In most computers, individual instructions are stored as machine code with each instruction being given a unique number (its operation code or opcode for short). The command to add two numbers together would have one opcode; the command to multiply them would have a different opcode, and so on. The simplest computers are able to perform any of a handful of different instructions; the more complex computers have several hundred to choose from, each with a unique numerical code. Since the computer's memory is able to store numbers, it can also store the instruction codes. This leads to the important fact that entire programs (which are just lists of these instructions) can be represented as lists of numbers and can themselves be manipulated inside the computer in the same way as numeric data. The fundamental concept of storing programs in the computer's memory alongside the data they operate on is the crux of the von Neumann, or stored program[citation needed], architecture. In some cases, a computer might store some or all of its program in memory that is kept separate from the data it operates on. This is called the Harvard architecture after the Harvard Mark I computer. Modern von Neumann computers display some traits of the Harvard architecture in their designs, such as in CPU caches.		While it is possible to write computer programs as long lists of numbers (machine language) and while this technique was used with many early computers,[67] it is extremely tedious and potentially error-prone to do so in practice, especially for complicated programs. Instead, each basic instruction can be given a short name that is indicative of its function and easy to remember – a mnemonic such as ADD, SUB, MULT or JUMP. These mnemonics are collectively known as a computer's assembly language. Converting programs written in assembly language into something the computer can actually understand (machine language) is usually done by a computer program called an assembler.		Programming languages provide various ways of specifying programs for computers to run. Unlike natural languages, programming languages are designed to permit no ambiguity and to be concise. They are purely written languages and are often difficult to read aloud. They are generally either translated into machine code by a compiler or an assembler before being run, or translated directly at run time by an interpreter. Sometimes programs are executed by a hybrid method of the two techniques.		Machine languages and the assembly languages that represent them (collectively termed low-level programming languages) tend to be unique to a particular type of computer. For instance, an ARM architecture computer (such as may be found in a smartphone or a hand-held videogame) cannot understand the machine language of an x86 CPU that might be in a PC.[68]		Though considerably easier than in machine language, writing long programs in assembly language is often difficult and is also error prone. Therefore, most practical programs are written in more abstract high-level programming languages that are able to express the needs of the programmer more conveniently (and thereby help reduce programmer error). High level languages are usually "compiled" into machine language (or sometimes into assembly language and then into machine language) using another computer program called a compiler.[69] High level languages are less related to the workings of the target computer than assembly language, and more related to the language and structure of the problem(s) to be solved by the final program. It is therefore often possible to use different compilers to translate the same high level language program into the machine language of many different types of computer. This is part of the means by which software like video games may be made available for different computer architectures such as personal computers and various video game consoles.		These 4G languages are less procedural than 3G languages. The benefit of 4GL is that they provide ways to obtain information without requiring the direct help of a programmer. An example of a 4GL is SQL.		Program design of small programs is relatively simple and involves the analysis of the problem, collection of inputs, using the programming constructs within languages, devising or using established procedures and algorithms, providing data for output devices and solutions to the problem as applicable. As problems become larger and more complex, features such as subprograms, modules, formal documentation, and new paradigms such as object-oriented programming are encountered. Large programs involving thousands of line of code and more require formal software methodologies. The task of developing large software systems presents a significant intellectual challenge. Producing software with an acceptably high reliability within a predictable schedule and budget has historically been difficult; the academic and professional discipline of software engineering concentrates specifically on this challenge.		Errors in computer programs are called "bugs". They may be benign and not affect the usefulness of the program, or have only subtle effects. But in some cases, they may cause the program or the entire system to "hang", becoming unresponsive to input such as mouse clicks or keystrokes, to completely fail, or to crash. Otherwise benign bugs may sometimes be harnessed for malicious intent by an unscrupulous user writing an exploit, code designed to take advantage of a bug and disrupt a computer's proper execution. Bugs are usually not the fault of the computer. Since computers merely execute the instructions they are given, bugs are nearly always the result of programmer error or an oversight made in the program's design.[70] Admiral Grace Hopper, an American computer scientist and developer of the first compiler, is credited for having first used the term "bugs" in computing after a dead moth was found shorting a relay in the Harvard Mark II computer in September 1947.[71]		Firmware is the technology which has the combination of both hardware and software such as BIOS chip inside a computer. This chip (hardware) is located on the motherboard and has the BIOS set up (software) stored in it.		Computers have been used to coordinate information between multiple locations since the 1950s. The U.S. military's SAGE system was the first large-scale example of such a system, which led to a number of special-purpose commercial systems such as Sabre.[72] In the 1970s, computer engineers at research institutions throughout the United States began to link their computers together using telecommunications technology. The effort was funded by ARPA (now DARPA), and the computer network that resulted was called the ARPANET.[73] The technologies that made the Arpanet possible spread and evolved.		In time, the network spread beyond academic and military institutions and became known as the Internet. The emergence of networking involved a redefinition of the nature and boundaries of the computer. Computer operating systems and applications were modified to include the ability to define and access the resources of other computers on the network, such as peripheral devices, stored information, and the like, as extensions of the resources of an individual computer. Initially these facilities were available primarily to people working in high-tech environments, but in the 1990s the spread of applications like e-mail and the World Wide Web, combined with the development of cheap, fast networking technologies like Ethernet and ADSL saw computer networking become almost ubiquitous. In fact, the number of computers that are networked is growing phenomenally. A very large proportion of personal computers regularly connect to the Internet to communicate and receive information. "Wireless" networking, often utilizing mobile phone networks, has meant networking is becoming increasingly ubiquitous even in mobile computing environments.		A computer does not need to be electronic, nor even have a processor, nor RAM, nor even a hard disk. While popular usage of the word "computer" is synonymous with a personal electronic computer, the modern[74] definition of a computer is literally: "A device that computes, especially a programmable [usually] electronic machine that performs high-speed mathematical or logical operations or that assembles, stores, correlates, or otherwise processes information."[75] Any device which processes information qualifies as a computer, especially if the processing is purposeful.[citation needed]		Historically, computers evolved from mechanical computers and eventually from vacuum tubes to transistors. However, conceptually computational systems as flexible as a personal computer can be built out of almost anything. For example, a computer can be made out of billiard balls (billiard ball computer); an often quoted example.[citation needed] More realistically, modern computers are made out of transistors made of photolithographed semiconductors.		There is active research to make computers out of many promising new types of technology, such as optical computers, DNA computers, neural computers, and quantum computers. Most computers are universal, and are able to calculate any computable function, and are limited only by their memory capacity and operating speed. However different designs of computers can give very different performance for particular problems; for example quantum computers can potentially break some modern encryption algorithms (by quantum factoring) very quickly.		There are many types of computer architectures:		Of all these abstract machines, a quantum computer holds the most promise for revolutionizing computing.[76] Logic gates are a common abstraction which can apply to most of the above digital or analog paradigms. The ability to store and execute lists of instructions called programs makes computers extremely versatile, distinguishing them from calculators. The Church–Turing thesis is a mathematical statement of this versatility: any computer with a minimum capability (being Turing-complete) is, in principle, capable of performing the same tasks that any other computer can perform. Therefore, any type of computer (netbook, supercomputer, cellular automaton, etc.) is able to perform the same computational tasks, given enough time and storage capacity.		A computer will solve problems in exactly the way it is programmed to, without regard to efficiency, alternative solutions, possible shortcuts, or possible errors in the code. Computer programs that learn and adapt are part of the emerging field of artificial intelligence and machine learning.		As the use of computers has spread throughout society, there are an increasing number of careers involving computers.		The need for computers to work well together and to be able to exchange information has spawned the need for many standards organizations, clubs and societies of both a formal and informal nature.		
American Scientist (informally abbreviated AmSci) is an American bimonthly science and technology magazine published since 1913 by Sigma Xi, The Scientific Research Society. Each issue includes four to five feature articles written by prominent scientists and engineers who review research in fields from molecular biology to computer engineering.		Each issue also includes the work of cartoonists, including those of Sidney Harris, Benita Epstein, and Mark Heath. Also included is the Scientists' Nightstand that reviews a vast range of science-related books and novels.		American Scientist Online (ISSN 1545-2786) was launched in May 2003.[2]				
FC Zulu is a Danish television program on TV 2 Zulu. It tells the story about 16 nerds, who had never touched a football before, trained in 2004 for three months, with the goal of playing against FCK in PARKEN. Against all odds, they were able to score a goal. This year, the incapable nerds return, again led by the coach duo, Mark Strudal and TNT. This time is the opponent is none other than Swedish FC Zulu-copy FC Z who FC Zulu beat 6-0.		In 2005 they were nominated for an Emmy for best 'Non-Scripted-Entertainment', but didn't win. The show has later been launched in several other countries, with FC Nerds as the name instead.						One ball, 16 cones. - 16 nerds, who had never touched a football, top-train for three months, after the completion of which they will play against FCK, one of the top soccer teams in Denmark, in Parken, the national stadium. In the lead as coach is the former national team player and goal-king Mark Strudal. FC Zulu consists of a pack of logical thinking computer nerds, role players and a single politician. When the players meet their coach Mark Strudal for the first time, they realized that what they had signed up for was no picnic. First challenge: a training match against a teenage girl team from Skovlunde. Result: a shocked Mark Strudal.		Strudal is tired of the nerds' intellectual debating society on the pitch. They must learn to act as a team. To that end, he sends them on a teambuilding trip to Lejre. Strudal also has a hidden agenda - he wants to find the team's captain on the trip. Dressed in Viking clothes and small leather moccasins, FC ZULU would, during the course of two days, become a close-knit unit. The solidarity is on trial for real, when one of the players is kidnapped by five hostile riders and hung out over "The Victin Bog".		The boys are gradually well started with the training, but they should also learn to sell themselves as a real football team. They should therefore definitely have a battle song à la "Re-Sepp-Ten". With Signe Svendsen as FC Zulu's own "Dodo" go the team in the studio, but there is a surprise for the boys - they should not only use their a bit rusty song voices, but their condition will also be tested, when it come to realize for them, that a real music video (of course) also has both fancy dancing scenes and hot girls.		One ball, 16 cones. - 16 nerds, who never had touched a football, should top-train in three months, whereupon they will play against FCK i PARKEN - a scenario, which all the players fears. To learn them to watch the fear in the eyes, sends Mark Strudal his team into Vridsløselille State Prison to play against the prisoners. The prisoners' team consists of murder- and narco- prisoners, so the panic in FC Zulu spreads, when the players see their opponents for the first time. Can referee Kim Milton control the prisoners?		The players don't have much wildness, e.g. excuse they still graciously, if they are running into each other on the pitch by accident. A desperate coach Strudal sends the nerds on re-education at the male therapist Carl-Mar Møller. A decision, which makes the intellectual Zulu-nerds so angry, that they in secret plans a surprise, which makes even Carl-Mar gape.		Nerds are often superintelligent - but are more interested in computers than the latest trends. It must be different, if the players should charm the female fans in Parken. Strudal sends the players to make-over at Zulu's "Modepatrulje" (Danish: Fashion Patrol), who gives the players a "lovely" treatment. There is a big surprise for the nerds' families, when they later appear with the right Beckham-look on the red runner in Imperial Cinema.		So are we there soon - the nerds from FC Zulu should very soon play the big match in Parken against the FCK-lions. Even though Strudal has used all strengths into the training, the players are still a bit doubtful on the pitch. Strudal has therefore asked two earlier, known Brøndby-players to help, to give FC Zulu tips about how to beat FCK. The boys should moreover play faith match against the girl junior team, they met on their very first day as FC Zulu-players. The players are keen to beat the girls and they give all, they got inside.		So is it the big day, where the form for real should be tested, and Mark Strudal should find out if all the hard training have borne fruit: Today should FC Zulu meet the FCK-lions in Parken in front of thousands of spectators. The players are being accommodated on a hotel, just like real footballers, and we follow their preparations and anger attacks from early morning to when they are on the pitch in front of the lions. Will their exertions bear fruit? Can FC Zulu beat FCK on their home ground in Parken?		FC Zulu 1-6 FCK All-stars		In FC Zulu's first season as a football team, they have been permitted to go through things - they never had dreamed about. Now meets Mark Strudal again his FC Zulu team after the winter break. And he is shocked. The nerds are again totally out of shape - and they have additionally got a lot of star crotches, because they now are famous. But Strudal has a cunning plan, as the nerds not had expected.		The nerds on the country. How will the old players receive the four new heavyrock-interested players? The team's coach Mark Strudal means, that his team should get together before they for real start a new, hard season. FC Zulu taker therefore so far out on the country, as you almost can come, namely to Gåser in Northern Jutland, where the farmer Mogens Mortensen knows, how you makes the right team spirit: All it needs is just a little bit of horses, pigs, cows and sheep.		It is totally wrong with the condition, the dripples and the ball feeling at the FC Zulu-players. What will happen when the nerds, there badly can dribble, should play in front of filled stands in the decisive match against the Swedish nerd-team? When it is the nation's honour, there are risked, plans Mark Strudal a daring plan: Ball jugglers and circus artist should teach FC Zulu body- and ball control. And where is the press more merciless than in Circus Benneweis' ring?		The nerds' discipline to the football training is bad. Very bad. And Mark Strudal is so frustrated over their lax pose, that he sends them on "re-education" in the Life Guards. But none of the nerds likes that-is-just-how-it-is-education - and in short time succeed it the 16 football cones, to get the sergeants on Høvelte barracks go crazy.		The rumours say that the Swedish nerds, as FC Zulu should play national game against, is some big guys, there are ready to smash FC Zulu. Mark Strudal let the earlier American football player and coach Claus Elming have his team, who promises, that he will give FC Zulu a hardness, brutality and cynicism, as they never will forget. Later should FC Zulu play match against some terrifying rocks.		The important national game against the Swedes is just around the corner. Therefore, Mark Strudal decides that the boys should be toptuned on a training stay in foreign countries. But the nerds is more minded on relaxing and holiday than training. For the first time i the progress loses Mark Strudal courage - and with that is FC Zulu on way to the precipice. Strudal has namely planned, that FC Zulu should play their first international match.		Not yet available.		Not yet available.		FC Zulu (Denmark) 6-0 FC Z (Sweden)		FC Zulu (Denmark) 4-0 Tufte IL (Norway)		FC Zulu (Denmark) 2-0 Paketes FC (Spain)		
Criminal Minds is an American police procedural crime drama television series created by Jeff Davis, and is the original show in the Criminal Minds franchise. It premiered on September 22, 2005, on the broadcast network CBS and October 5, 2005, on CTV. The series is produced by The Mark Gordon Company, in association with CBS Television Studios and ABC Studios (a subsidiary of The Walt Disney Company). Criminal Minds is set primarily at the FBI's Behavioral Analysis Unit (BAU) based in Quantico, Virginia. In accordance with the show's plot, Criminal Minds differs from many procedural dramas by focusing on profiling the criminal, called the unsub or "unknown subject", rather than the crime itself.		The show has an ensemble cast that has had many cast member changes since its inception. Thomas Gibson, Shemar Moore,[1] Matthew Gray Gubler, A. J. Cook, and Kirsten Vangsness are the only actors to have appeared in every season. The series follows a group of FBI profilers who set about catching various criminals through behavioral profiling. The plot focuses on the team working cases and on the personal lives of the characters, depicting the hardened life and statutory requirements of a profiler. The show spawned two spin-offs: Criminal Minds: Suspect Behavior (2011) and Criminal Minds: Beyond Borders (2016–2017).		On April 7, 2017, CBS renewed the series for a thirteenth season.[2]						When the series premiered in September 2005, it featured FBI Agents Jason Gideon, Aaron Hotchner, Elle Greenaway, Derek Morgan, Spencer Reid, Jennifer "J.J." Jareau, and Penelope Garcia. For season one, Garcia was not a main cast member, but rather had a recurring role, although she appeared in most episodes. In 2006, at the start of season two, Lola Glaudini announced her departure from the show, as she wanted to return home to New York City.[3] Paget Brewster replaced her in the role of Emily Prentiss.		At the start of season three, Mandy Patinkin announced his departure from the show because he was deeply disturbed by the content of the series.[4] He left letters of apology for his fellow cast members, explaining his reasons and wishing them luck. Joe Mantegna replaced him as David Rossi, a best-selling author and FBI agent who comes out of retirement. During season three, A.J. Cook became pregnant with her first child. Her pregnancy was written into the show. Cook's son, Mekhai Andersen, has been written into a recurring role as Jennifer's son Henry. Cook's void was filled by Meta Golding, who played Jordan Todd, an FBI agent who works with the agency's Counter Terrorism Unit. In season six, Jennifer is forced to accept a promotion at The Pentagon, causing her to leave the BAU.		Later that season, Emily is seemingly killed off. Although she survives, she does not appear for the rest of the season. Cook and Brewster were both replaced by Rachel Nichols as Ashley Seaver, an FBI cadet. CBS's decision to release Cook and Brewster from their contracts resulted in numerous fans writing angry letters to the studio and signing protest petitions.[5] CBS rehired Cook and Brewster as Jennifer Jareau and Emily Prentiss, respectively; Nichols was released.[6][7] In February 2012, Brewster announced her departure from the show after the seventh season.[8] She was replaced in the eighth season by Jeanne Tripplehorn, who played Alex Blake, a linguistics expert.[9] Later in season nine, Paget Brewster made a special guest appearance, reprising her role as Emily Prentiss in the 200th episode.		After two seasons, Tripplehorn was released from the show.[10][11] Former Ghost Whisperer star Jennifer Love Hewitt joined the cast as Kate Callahan, a former undercover FBI agent who joins the BAU.[12] During season ten, Jason Gideon was killed off-screen. Executive producer and showrunner Erica Messer said CBS and ABC Studios were fine with the decision because it was clear that Patinkin would not come back again, but the show would feature him in a flashback if he were ever to return in the future.[13] Following the conclusion of season ten, Hewitt and Cook announced that they'll both be on hiatus from the show due to their pregnancies. Hewitt did not return for season eleven,[14] while Cook returned after the first seven episodes of season eleven.[15] Aisha Tyler, who plays Dr. Tara Lewis, joined the show at the start of season eleven.		Later that season, Shemar Moore, who plays Derek Morgan, left the show after eleven seasons. He had thought to leave in the previous season when his contract ended but was convinced to stay to give his character a proper sendoff.[16][17] Messer said the initial thought was for Moore to do six episodes, but when that didn't feel like enough, they settled on Moore doing the first eighteen episodes of that season and he departed in March 2016.[18] He is replaced in the twelfth season by former CSI: Miami star Adam Rodriguez, who plays Luke Alvez, a Fugitive Task Force Agent.[19] A week after Moore left, Paget Brewster, who plays Emily Prentiss, made her second special guest appearance since leaving in season seven, her first being in season nine. In season twelve, Brewster once again became a series regular on the show.[20][21][22][23]		On August 10, 2016, it was announced that former recurring star Aisha Tyler would be promoted to series regular for the twelfth season.[24] The next day, it was reported that Thomas Gibson, who portrays Aaron Hotchner, had been suspended from and written off the show for at least one, most likely two episodes in the twelfth season due to an on-set altercation with one of the producers.[25] On August 12, 2016, Gibson was fired from the program due to this incident.[26] On September 30, 2016, it was announced that Gibson's character would be replaced by Damon Gupton, who will play Special Agent Stephen Walker, a seasoned profiler from the Behavioral Analysis Program (the counterintelligence division of the FBI) who will bring his spy-hunting skills to the BAU.[27] On June 11, 2017, it was announced that Damon Gupton had been fired from the show after one season. CBS said his departure was "part of a creative change on the show".[28]		On June 20, 2017, CBS announced that Daniel Henney, who was a series regular on Criminal Minds: Beyond Borders as Matt Simmons, would join the main show as a series regular for the thirteenth season.[29]		Twelve complete seasons of Criminal Minds have been aired, and the thirteenth season is currently in commission. As of May 10, 2017[update], 277 episodes of Criminal Minds have aired, concluding the twelfth season.		The first season of Criminal Minds received mixed reviews from critics.[37] It has a Metacritic score of 42 based on 21 reviews, indicating "mixed or average reviews".[37]		Dorothy Rabinowitz said, in her review for The Wall Street Journal, "From the evidence of the first few episodes, Criminal Minds may be a hit, and deservedly" and gave particular praise to Gubler and Patinkin's performance.[38] Ned Martel in The New York Times was less positive, saying, "The problem with "Criminal Minds" is its many confusing maladies, applied to too many characters" and felt that "as a result, the cast seems like a spilled trunk of broken toys, with which the audience—and perhaps the creators—may quickly become bored".[39] The Chicago Tribune reviewer, Sid Smith, felt that the show "may well be worth a look", though he too criticized the "confusing plots and characters".[40] Writing in PopMatters, Marco Lanzagorta criticized the show after its premiere, saying it "confuses critical thinking with supernatural abilities" and its characters conform to stereotypes.[41] In the Los Angeles Times, Mary McNamara gave a similar review, and praised Patinkin and Gubler's performances.[42]		In 2016, a New York Times study of the 50 TV shows with the most Facebook Likes found that "like several of the other police procedurals", Criminal Minds "is more popular in rural areas, particularly in the southeastern half of the country. It hits peak popularity in Alabama and rural Tennessee and is least popular in Santa Barbara, Calif."[43]		Seasonal rankings (based on average total viewers per episode) of Criminal Minds.		* The season two episode "The Big Game" achieved a series-high rating by attracting an audience of 26.31 million viewers and an 18–49 rating of 9.3.[82]		The show ranked number nine in DVR playback (2.35 million viewers), according to Nielsen prime DVR lift data from September 22& to November 23, 2008.[83]		For the week of October 10, 2010, Criminal Minds ranked sixth in DVR playback (2.40 million viewers), and seventh in the demo playback (1.0 demo) according to Nielsen prime DVR lift data.[84]		The series is in syndication on A&E Network, and Ion Television.[85]		Early seasons of Criminal Minds have begun airing on Rewind Networks's HITS TV channel in South East Asia, Hong Kong and Taiwan.[86]		Criminal Minds has produced two spin-offs: Criminal Minds: Suspect Behavior and Criminal Minds: Beyond Borders, as well as a video game.		The season five episode "The Fight" introduced a second BAU team and launched a new series called Criminal Minds: Suspect Behavior. The spin-off series debuted February 16, 2011, on CBS[87] but was canceled after a short 13-episode season owing to low ratings.[88] On September 6, 2011, CBS DVD released The Complete Series on a four-disc set. It was packaged as "The DVD Edition".		The cast features Forest Whitaker as the lead role of Sam Cooper; including Janeane Garofalo, Michael Kelly, Beau Garrett, Matt Ryan, Richard Schiff, and Kirsten Vangsness, who reprises her role as Penelope Garcia from the original series.		A proposed new series in the Criminal Minds franchise to be named Criminal Minds: Beyond Borders was announced in January 2015. Former CSI: NY star Gary Sinise (who is also a producer on the show) and Anna Gunn were cast in the lead roles of Jack Garrett and Lily Lambert, though the latter departed after the backdoor pilot.[89] Tyler James Williams has been cast as Russ "Monty" Montgomery and Daniel Henney as Matt Simmons, with Alana de la Garza as Clara Seger and Annie Funke as Mae Jarvis further being cast as series regulars.[90]		The series follows the FBI agents of the International Response Team (IRT) helping American citizens who find themselves in trouble abroad.[35][91] CBS aired a backdoor pilot on April 8, 2015 in the Criminal Minds slot, with a crossover episode titled "Beyond Borders".[35][36] The second spin-off series debuted March 16, 2016, on CBS.[92] On May 16, 2016, CBS renewed the series for a second season.[93] On May 14, 2017, CBS canceled the series after two seasons due to low ratings.[94]		CBS announced in October 2009 that Legacy Interactive would develop a video game based on the show. The game would require players to examine crime scenes for clues to help solve murder mysteries. The interactive puzzle game was released in 2012, but the show's cast was not involved with the project so it did not feature any of their voices.[95][96][97]				
Square used as slang may mean many things when referring to a person or in common language. It is often used to speak of a person who is regarded as dull, rigidly conventional, and out of touch with current trends.		In referring to a person, the word originally meant someone who was honest, traditional and loyal. An agreement that is equitable on all sides is a "square deal". During the rise of jazz music, the term transformed from a compliment to an insult.						In the parlance of jazz, a square was a person who failed to appreciate the medium, or, more broadly, someone who was out of date or out of touch, hence the saying "be there or be square". In the counterculture movements that started in the 1940s and took momentum in the 1960s a "square" referred to someone who clung to repressive, traditional, stereotypical, one-sided, or "in the box" ways of thinking. The term was used by hipsters in the 1940s, beatniks in the 1950s, hippies in the 1960s, yippies in the 1970s, and other individuals who took part in the movements which emerged to contest the more conservative national, political, religious, philosophical, musical, and social trends. It comes from the square representing a four-beat rhythm as shown by a conductor's hands.[1] It was in this context that Sly and the Family Stone's trumpet player Cynthia Robinson yelled out in the hit "Dance to the Music": "All the squares go home!" If the counterculture was a shift from conservatism to liberalism, then square was what liberal people called conservative people and things. While the term waned in popularity during the 1980s, it remained in the public consciousness, particularly of the American baby boom generation, enough that its broad meaning (of a person who respects traditional principles) is exemplified in Huey Lewis's 1986 hit "Hip To Be Square".		The term found its way into various parts of popular culture. Perhaps the most obvious recurring reference today would be this line from "Jailhouse Rock", a song most famously sung by Elvis Presley:		An early example of the usage of the term can be found in the 1946 recording by Harry Gibson, who performed as Harry the Hipster, "What's his Story?" which includes the stanza:		In 1945 the song "Tampico" by Stan Kenton contained the lyrics:		"You ask a Mexican band to play a "rumba-down-dare" there, He turns and Says to the boys, "Hey fellas, dig that square!"		Or an earlier song by Harry Gibson, from 1944, called "Stop That Dancing Up There," which includes:		Square can mean good and honest, first attested in the 1560s,[1] a sense preserved in the phrases "fair and square", "three square meals a day", "a square deal"; or upstanding, as in "squaring up" (to an antagonist). As a symbol of rectitude, the square, or set-square, is one of the principal allegorical symbols in Freemasonry.		The term was used in the American Cub Scout Promise until 1971.[2]		The chorus of the George M. Cohan song "Mary's a Grand Old Name" concludes with these lines:		'Square' is also an obscure Canadian slang term for a case of 24 beer bottles.[3]		'Square' is also a slang term for a cigarette among the African-American community.		
Johannes Grenzfurthner (German: [joˈhanəs ˈgrɛntsfʊrtnə]; born 1975 in Vienna) is an award-winning Austrian artist, filmmaker, writer, actor, curator, theatre director and lecturer. He is known as the founder, conceptualist and artistic director of monochrom, an international art and theory group. Most of his artworks are labelled monochrom.		He is one of the most outspoken researchers in the field of sexuality and technology,[1] and one of the founders of 'techno-hedonism' (see also: barbots).		Recurring topics in Grenzfurthner's art and writing are film, technology, political activism, contemporary art, performance art, humour, philosophy, sexuality, critical theory, robotics, postmodernism, media theory, cultural studies, popular culture studies, science fiction, and the debate about intellectual property.		Boing Boing magazine refers to him as leitnerd,[2] a wordplay with the German term Leitkultur that ironically hints at Grenzfurthner's role in nerd/hacker/art culture.						In the early 1990s, Johannes Grenzfurthner was an active member of several BBS message boards.[3] He used his online connections to create a zine or alternative magazine that dealt with art, technology and subversive cultures, and was influenced by US magazines like Mondo 2000. Grenzfurthner's motivation was to react to the emerging conservativism in cyber-cultures of the early 1990s,[4] and to combine his political background in the Austrian punk and antifa movement with discussion of new technologies and the cultures they create.[5] The publication featured many interviews and essays, for example by Bruce Sterling, HR Giger, Richard Kadrey, Arthur Kroker, Negativland, Kathy Acker, Michael Marrak, DJ Spooky, Geert Lovink, Lars Gustafsson, Tony Serra, Friedrich Kittler, Jörg Buttgereit, Eric Drexler, Terry Pratchett, Jack Sargeant and Bob Black,[6] in its specific experimental layout style.[7] In 1995 the group decided to cover new artistic practices[8][9][10] and started experimenting with different media: computer games, robots, puppet theater, musical, short films, pranks, conferences, online activism, which Grenzfurthner calls 'Urban Hacking'[11] or more specific: 'Context hacking', a term that Grenzfurthner coined.[12]		The group is known for working with different media, art and entertainment formats. Johannes Grenzfurthner calls this "looking for the best weapon of mass distribution of an idea".[14]		Grenzfurthner is the group's artistic director.		Grenzfurthner is head of Arse Elektronika[15][16] festival in San Francisco (2007 – ), an annual academic[17] and artistic[18] conference and anthology series that focusses on sexuality and technology. The first conference was curated by Grenzfurthner in 2007 to answer questions about the impact of sexuality on technological innovation and adoption.		Grenzfurthner is hosting Roboexotica,[19] the international Festival for Cocktail-Robotics (2002–) which invites researchers and artists to build machines that serve or mix cocktails. V. Vale calls Roboexotica "an ironic attempt to criticize techno-triumphalism and to dissect technological hypes."[20]		Grenzfurthner is head of Hedonistika,[21] a "smorgastic Festival for Gastrobots, Culinatronics, Advanced Snackhacks and Nutritional Mayhem", an event dedicated to approaches in gastronomical robots, cooking machines, molecular cuisine and experimental food performances. The first installment was presented in Montréal at the 2014 'Biennale internationale d'art numérique'.[22] The second installment was presented in Holon, near Tel Aviv, at 'Print Screen Festival'.[23]		Grenzfurthner wrote and directed a range of theatre plays,[24][25][26] street theatre performances[27][28] and short films,[29] but also works as a movie producer.[30][31]		His first feature film as a director was the independent fantasy comedy Die Gstettensaga: The Rise of Echsenfriedl[32] (2014). His first feature documentary is Traceroute (2016). He is working on several feature films (e.g. Sierra Zulu,[33][34], but his current main projects are the documentary Glossary of Broken Dreams[35] and Tycho,[36] a farcical film musical about the life and times of eccentric astronomer Tycho Brahe.		Grenzfurthner lectures at different universities in Austria (e.g. University of Applied Sciences, Graz, Austria; University of Arts and Industrial Design, Linz, Austria[37][38]), Germany (Institute of Culture and Aesthetics of Digital Media at the Leuphana University of Lüneburg) and in the United States.[39][40] He works as a thesis advisor, for example for Georg Mir's ethical tabletop roleplaying game Michtim: Fluffy Adventures.		He has published numerous books, essays and articles on contemporary art, communication processes and philosophy including Mind and Matter: Comparative Approaches Towards Complexity, Do androids sleep with electric sheep?, Of Intercourse and Intracourse: Sexuality, Biomodification and the Techno-Social Sphere and Pr0nnovation?: Pornography and Technological Innovation.[41][42][43][44][45]		Grenzfurthner published the much debated[46] pamphlet "Hacking the Spaces", that dealt with exclusionist tendencies in the hackerspaces movement.[47] He extended his critique through lectures at the 2012 and 2014 Hackers on Planet Earth conferences in New York City.		He worked for various online/print magazines and radio stations (e.g. ORF,[48] Telepolis, Boing Boing[49]).		Grenzfurthner is a comedian and performs at various venues, e.g. Vienna's Rabenhof Theater.[50] Parts of his comedy show "Schicksalsjahre eines Nerds" form the basis of his documentary film Traceroute (2016).		He is a presenter and emcee for various events.[51][52]		Grenzfurthner is acting in theater plays[53][54] and films, for example Andi Haller's feature film Zero Crash.[55]		Grenzfurthner sees monochrom as a community and social incubator of critical and subversive thinkers.[56] An example is Bre Pettis of MakerBot Industries, who got inspired to create 3d printers during an art residency with Grenzfurthner at monochrom in 2007. Pettis wanted to create a robot that could print shot glasses for the event Roboexotica and did research about the RepRap project at the Vienna hackerspace Metalab.[57] Shot glasses remained a theme throughout the history of MakerBot.[58]		Grenzfurthner was one of the core team members in the development process of netznetz, a new kind of community-based funding system for net culture and net art together with the culture department of the city government of Vienna.		He started the "Hackbus" community,[59][60][61] a platform[vague] and movement for mobile hackerspaces.		Together with Florian Hufsky, Leo Findeisen and Juxi Leitner, Grenzfurthner co-organized the first international conference of the pirate parties.[62][63]		He is frequently invited as a jury member for festivals and cons.[64]		Grenzfurthner is a professional creative consultant and offers coaching sessions for individuals and companies.[65]		He was behind an acclaimed series of viral marketing videos for Boing Boing Video, dealing with a mysterious packages full of Cheetos. The series is set in the alternative universe of Soviet Unterzoegersdorf.[66][67][68][69][70]		He conceptualized and co-built a robot installation to promote the products of sex toy company Bad Dragon.[71]		Johannes Grenzfurthner lives and works in Vienna and Durango, Colorado.		He grew up in Stockerau in rural Lower Austria[72] and talks about it in his stand-up comedy "Schicksalsjahre eines Nerds" (2014) and his semi-autobiographical documentary film Traceroute (2016).		Grenzfurthner uses his personal life as an inspiration for his art and activism. On his personal website he states that he deals with his claustrophobic tendencies by "occasionally burying people alive."[74]		As a kid and teenager, Grenzfurthner spent a lot of time at his grandparents' farm in the small village of Unterzögersdorf (a cadastral municipality of Stockerau). His grandparents' stories about Naziism, World War II and the Soviet occupation in allied-occupied Austria (1945-1955) influenced monochrom's long-term project Soviet Unterzoegersdorf.[75][76]		He identifies as a leftist and atheist.		He is married to filmmaker Anna Grenzfurthner.[77]		
Association football, more commonly known as football or soccer,[a] is a team sport played between two teams of eleven players with a spherical ball. It is played by 250 million players in over 200 countries and dependencies, making it the world's most popular sport.[3][4][5][6] The game is played on a rectangular field with a goal at each end. The object of the game is to score by getting the ball into the opposing goal.		Players are not allowed to touch the ball with their hands or arms while it is in play, unless they are goalkeepers (and then only when within their penalty area). Other players mainly use their feet to strike or pass the ball, but may also use their head or torso. The team that scores the most goals by the end of the match wins. If the score is level at the end of the game, either a draw is declared or the game goes into extra time or a penalty shootout depending on the format of the competition. The Laws of the Game were originally codified in England by The Football Association in 1863. Association football is governed internationally by the International Federation of Association Football (FIFA; French: Fédération Internationale de Football Association), which organises World Cups for both men and women every four years.[7]								The rules of association football were codified in England by the Football Association in 1863 and the name association football was coined to distinguish the game from the other forms of football played at the time, specifically rugby football. The first written "reference to the inflated ball used in the game" was in the mid-14th century: "Þe heued fro þe body went, Als it were a foteballe".[8] The Online Etymology Dictionary states that the word "soccer" was "split off in 1863".[8] According to Partha Mazumdar, the term soccer originated in England, first appearing in the 1880s as an Oxford "-er" abbreviation of the word "association".[9]		Within the English-speaking world, association football is now usually called football in the United Kingdom and mainly soccer in Canada and the United States. People in Australia, Ireland, South Africa and New Zealand use either or both terms, although national associations in Australia and New Zealand now primarily use "football" for the formal name.[10]		According to FIFA, the Chinese competitive game cuju (蹴鞠, literally "kick ball") is the earliest form of football for which there is evidence.[12] Cuju players could use any part of the body apart from hands and the intent was kicking a ball through an opening into a net. It was remarkably similar to modern football, though similarities to rugby occurred.[13][14] During the Han Dynasty (206 BC – 220 AD), cuju games were standardised and rules were established.[13]		Phaininda and episkyros were Greek ball games.[15][16] An image of an episkyros player depicted in low relief on a vase at the National Archaeological Museum of Athens[11] appears on the UEFA European Championship Cup.[17] Athenaeus, writing in 228 AD, referenced the Roman ball game harpastum. Phaininda, episkyros and harpastum were played involving hands and violence. They all appear to have resembled rugby football, wrestling and volleyball more than what is recognizable as modern football.[13][18][19][20][21][22] As with pre-codified "mob football", the antecedent of all modern football codes, these three games involved more handling the ball than kicking.[23][24] Non-competitive games included kemari in Japan, chuk-guk in Korea and woggabaliri in Australia.		Association football in itself does not have a classical history.[25] Notwithstanding any similarities to other ball games played around the world FIFA has recognised that no historical connection exists with any game played in antiquity outside Europe.[26] The modern rules of association football are based on the mid-19th century efforts to standardise the widely varying forms of football played in the public schools of England. The history of football in England dates back to at least the eighth century AD.[27]		The Cambridge Rules, first drawn up at Cambridge University in 1848, were particularly influential in the development of subsequent codes, including association football. The Cambridge Rules were written at Trinity College, Cambridge, at a meeting attended by representatives from Eton, Harrow, Rugby, Winchester and Shrewsbury schools. They were not universally adopted. During the 1850s, many clubs unconnected to schools or universities were formed throughout the English-speaking world, to play various forms of football. Some came up with their own distinct codes of rules, most notably the Sheffield Football Club, formed by former public school pupils in 1857,[28] which led to formation of a Sheffield FA in 1867. In 1862, John Charles Thring of Uppingham School also devised an influential set of rules.[29]		These ongoing efforts contributed to the formation of The Football Association (The FA) in 1863, which first met on the morning of 26 October 1863 at the Freemasons' Tavern in Great Queen Street, London.[30] The only school to be represented on this occasion was Charterhouse. The Freemason's Tavern was the setting for five more meetings between October and December, which eventually produced the first comprehensive set of rules. At the final meeting, the first FA treasurer, the representative from Blackheath, withdrew his club from the FA over the removal of two draft rules at the previous meeting: the first allowed for running with the ball in hand; the second for obstructing such a run by hacking (kicking an opponent in the shins), tripping and holding. Other English rugby clubs followed this lead and did not join the FA and instead in 1871 formed the Rugby Football Union. The eleven remaining clubs, under the charge of Ebenezer Cobb Morley, went on to ratify the original thirteen laws of the game.[30] These rules included handling of the ball by "marks" and the lack of a crossbar, rules which made it remarkably similar to Victorian rules football being developed at that time in Australia. The Sheffield FA played by its own rules until the 1870s with the FA absorbing some of its rules until there was little difference between the games.[31]		The world's oldest football competition is the FA Cup, which was founded by C. W. Alcock and has been contested by English teams since 1872. The first official international football match also took place in 1872, between Scotland and England in Glasgow, again at the instigation of C. W. Alcock. England is also home to the world's first football league, which was founded in Birmingham in 1888 by Aston Villa director William McGregor.[32] The original format contained 12 clubs from the Midlands and Northern England.		The laws of the game are determined by the International Football Association Board (IFAB).[33] The board was formed in 1886[34] after a meeting in Manchester of The Football Association, the Scottish Football Association, the Football Association of Wales, and the Irish Football Association. FIFA, the international football body, was formed in Paris in 1904 and declared that they would adhere to Laws of the Game of the Football Association.[35] The growing popularity of the international game led to the admittance of FIFA representatives to the International Football Association Board in 1913. The board consists of four representatives from FIFA and one representative from each of the four British associations.[36]		Today, football is played at a professional level all over the world. Millions of people regularly go to football stadiums to follow their favourite teams,[37] while billions more watch the game on television or on the internet.[38][39] A very large number of people also play football at an amateur level. According to a survey conducted by FIFA published in 2001, over 240 million people from more than 200 countries regularly play football.[40] Football has the highest global television audience in sport.[41]		In many parts of the world football evokes great passions and plays an important role in the life of individual fans, local communities, and even nations. R. Kapuscinski says that Europeans who are polite, modest, or humble fall easily into rage when playing or watching football games.[42] The Côte d'Ivoire national football team helped secure a truce to the nation's civil war in 2006[43] and it helped further reduce tensions between government and rebel forces in 2007 by playing a match in the rebel capital of Bouaké, an occasion that brought both armies together peacefully for the first time.[44] By contrast, football is widely considered to have been the final proximate cause for the Football War in June 1969 between El Salvador and Honduras.[45] The sport also exacerbated tensions at the beginning of the Yugoslav Wars of the 1990s, when a match between Dinamo Zagreb and Red Star Belgrade degenerated into rioting in May 1990.[46]		Women may have been playing "football" for as long as the game has existed. Evidence shows that an ancient version of the game (Tsu Chu) was played by women during the Han Dynasty (25–220 CE). Two female figures are depicted in Han Dynasty (25–220 CE) frescoes, playing Tsu Chu.[47] There are, however, a number of opinions about the accuracy of dates, the earliest estimates at 5000 BCE.[48]		Association football, the modern game, also has documented early involvement of women. An annual competition in Mid-Lothian, Scotland during the 1790s is reported, too.[49][50] In 1863, football governing bodies introduced standardised rules to prohibit violence on the pitch, making it more socially acceptable for women to play.[51] The first match recorded by the Scottish Football Association took place in 1892 in Glasgow. In England, the first recorded game of football between women took place in 1895.[51][52]		The most well-documented early European team was founded by activist Nettie Honeyball in England in 1894. It was named the British Ladies' Football Club. Nettie Honeyball is quoted, "I founded the association late last year [1894], with the fixed resolve of proving to the world that women are not the 'ornamental and useless' creatures men have pictured. I must confess, my convictions on all matters where the sexes are so widely divided are all on the side of emancipation, and I look forward to the time when ladies may sit in Parliament and have a voice in the direction of affairs, especially those which concern them most." [53] Honeyball and those like her paved the way for women's football. However the women's game was frowned upon by the British football associations, and continued without their support. It has been suggested that this was motivated by a perceived threat to the 'masculinity' of the game.[54]		Women's football became popular on a large scale at the time of the First World War, when employment in heavy industry spurred the growth of the game, much as it had done for men fifty years earlier. The most successful team of the era was Dick, Kerr's Ladies of Preston, England. The team played in the first women's international matches in 1920, against a team from Paris, France, in April, and also made up most of the England team against a Scottish Ladies XI in 1920, and winning 22-0.[49]		Despite being more popular than some men's football events (one match saw a 53,000 strong crowd),[55] women's football in England suffered a blow in 1921 when The Football Association outlawed the playing of the game on Association members' pitches, on the grounds that the game (as played by women) was distasteful.[56] Some speculated that this may have also been due to envy of the large crowds that women's matches attracted.[57] This led to the formation of the English Ladies Football Association and play moved to rugby grounds.[58]		Association football has been played by women since at least the time of the first recorded women's games in the late 19th century.[59][60] It has traditionally been associated with charity games and physical exercise, particularly in the United Kingdom.[60] In the late 1960s and early 1970s women's association football was organised in the United Kingdom, eventually becoming the most prominent team sport for British women.[60]		The growth in women's football has seen major competitions being launched at both national and international level mirroring the male competitions. Women's football has faced many struggles. It had a "golden age" in the United Kingdom in the early 1920s when crowds reached 50,000 at some matches;[61] this was stopped on 5 December 1921 when England's Football Association voted to ban the game from grounds used by its member clubs. The FA's ban was rescinded in December 1969 with UEFA voting to officially recognise women's football in 1971.[60] The FIFA Women's World Cup was inaugurated in 1991 and has been held every four years since,[62] while women's football has been an Olympic event since 1996.		Association football is played in accordance with a set of rules known as the Laws of the Game. The game is played using a spherical ball of 68–70 cm (27–28 in) circumference,[63] known as the football (or soccer ball). Two teams of eleven players each compete to get the ball into the other team's goal (between the posts and under the bar), thereby scoring a goal. The team that has scored more goals at the end of the game is the winner; if both teams have scored an equal number of goals then the game is a draw. Each team is led by a captain who has only one official responsibility as mandated by the Laws of the Game: to represent his or her team in the coin toss prior to kick-off or penalty kicks.[64]		The primary law is that players other than goalkeepers may not deliberately handle the ball with their hands or arms during play, though they must use both their hands during a throw-in restart. Although players usually use their feet to move the ball around they may use any part of their body (notably, "heading" with the forehead)[65] other than their hands or arms.[66] Within normal play, all players are free to play the ball in any direction and move throughout the pitch, though the ball cannot be received in an offside position.[67]		During gameplay, players attempt to create goal-scoring opportunities through individual control of the ball, such as by dribbling, passing the ball to a teammate, and by taking shots at the goal, which is guarded by the opposing goalkeeper. Opposing players may try to regain control of the ball by intercepting a pass or through tackling the opponent in possession of the ball; however, physical contact between opponents is restricted. Football is generally a free-flowing game, with play stopping only when the ball has left the field of play or when play is stopped by the referee for an infringement of the rules. After a stoppage, play recommences with a specified restart.[68]		At a professional level, most matches produce only a few goals. For example, the 2005–06 season of the English Premier League produced an average of 2.48 goals per match.[69] The Laws of the Game do not specify any player positions other than goalkeeper,[70] but a number of specialised roles have evolved. Broadly, these include three main categories: strikers, or forwards, whose main task is to score goals; defenders, who specialise in preventing their opponents from scoring; and midfielders, who dispossess the opposition and keep possession of the ball to pass it to the forwards on their team. Players in these positions are referred to as outfield players, to distinguish them from the goalkeeper. These positions are further subdivided according to the area of the field in which the player spends most time. For example, there are central defenders, and left and right midfielders. The ten outfield players may be arranged in any combination. The number of players in each position determines the style of the team's play; more forwards and fewer defenders creates a more aggressive and offensive-minded game, while the reverse creates a slower, more defensive style of play. While players typically spend most of the game in a specific position, there are few restrictions on player movement, and players can switch positions at any time.[71] The layout of a team's players is known as a formation. Defining the team's formation and tactics is usually the prerogative of the team's manager.[72]		There are 17 laws in the official Laws of the Game, each containing a collection of stipulation and guidelines. The same laws are designed to apply to all levels of football, although certain modifications for groups such as juniors, seniors, women and people with physical disabilities are permitted. The laws are often framed in broad terms, which allow flexibility in their application depending on the nature of the game. The Laws of the Game are published by FIFA, but are maintained by the International Football Association Board (IFAB).[73] In addition to the seventeen laws, numerous IFAB decisions and other directives contribute to the regulation of football.		Each team consists of a maximum of eleven players (excluding substitutes), one of whom must be the goalkeeper. Competition rules may state a minimum number of players required to constitute a team, which is usually seven. Goalkeepers are the only players allowed to play the ball with their hands or arms, provided they do so within the penalty area in front of their own goal. Though there are a variety of positions in which the outfield (non-goalkeeper) players are strategically placed by a coach, these positions are not defined or required by the Laws.[70]		The basic equipment or kit players are required to wear includes a shirt, shorts, socks, footwear and adequate shin guards. An athletic supporter and protective cup is highly recommended for male players by medical experts and professionals.[74][75] Headgear is not a required piece of basic equipment, but players today may choose to wear it to protect themselves from head injury. Players are forbidden to wear or use anything that is dangerous to themselves or another player, such as jewellery or watches. The goalkeeper must wear clothing that is easily distinguishable from that worn by the other players and the match officials.[76]		A number of players may be replaced by substitutes during the course of the game. The maximum number of substitutions permitted in most competitive international and domestic league games is three, though the permitted number may vary in other competitions or in friendly matches. Common reasons for a substitution include injury, tiredness, ineffectiveness, a tactical switch, or timewasting at the end of a finely poised game. In standard adult matches, a player who has been substituted may not take further part in a match.[77] IFAB recommends "that a match should not continue if there are fewer than seven players in either team." Any decision regarding points awarded for abandoned games is left to the individual football associations.[78]		A game is officiated by a referee, who has "full authority to enforce the Laws of the Game in connection with the match to which he has been appointed" (Law 5), and whose decisions are final. The referee is assisted by two assistant referees. In many high-level games there is also a fourth official who assists the referee and may replace another official should the need arise.[79]		The ball is spherical with a circumference of between 68 and 70 centimetres (27 and 28 in), a weight in the range of 410 to 450 grams (14 to 16 oz), and a pressure between 0.6 and 1.1 bars (8.5 and 15.6 pounds per square inch) at sea level. In the past the ball was made up of leather panels sewn together, with a latex bladder for pressurisation but modern balls at all levels of the game are now synthetic.[80][81]		As the Laws were formulated in England, and were initially administered solely by the four British football associations within IFAB, the standard dimensions of a football pitch were originally expressed in imperial units. The Laws now express dimensions with approximate metric equivalents (followed by traditional units in brackets), though use of imperial units remains popular in English-speaking countries with a relatively recent history of metrication (or only partial metrication), such as Britain.[82]		The length of the pitch, or field, for international adult matches is in the range of 100–110 m (110–120 yd) and the width is in the range of 64–75 m (70–80 yd). Fields for non-international matches may be 90–120 m (100–130 yd) length and 45–90 m (50–100 yd) in width, provided that the pitch does not become square. In 2008, the IFAB initially approved a fixed size of 105 m (344 ft) long and 68 m (223 ft) wide as a standard pitch dimension for international matches;[83] however, this decision was later put on hold and was never actually implemented.[84]		The longer boundary lines are touchlines, while the shorter boundaries (on which the goals are placed) are goal lines. A rectangular goal is positioned at the middle of each goal line.[85] The inner edges of the vertical goal posts must be 24 feet (7.3 m) apart, and the lower edge of the horizontal crossbar supported by the goal posts must be 8 feet (2.4 m) above the ground. Nets are usually placed behind the goal, but are not required by the Laws.[86]		In front of the goal is the penalty area. This area is marked by the goal line, two lines starting on the goal line 16.5 m (18 yd) from the goalposts and extending 16.5 m (18 yd) into the pitch perpendicular to the goal line, and a line joining them. This area has a number of functions, the most prominent being to mark where the goalkeeper may handle the ball and where a penalty foul by a member of the defending team becomes punishable by a penalty kick. Other markings define the position of the ball or players at kick-offs, goal kicks, penalty kicks and corner kicks.[87]		A standard adult football match consists of two periods of 45 minutes each, known as halves. Each half runs continuously, meaning that the clock is not stopped when the ball is out of play. There is usually a 15-minute half-time break between halves. The end of the match is known as full-time.[88] The referee is the official timekeeper for the match, and may make an allowance for time lost through substitutions, injured players requiring attention, or other stoppages. This added time is called additional time in FIFA documents,[89][90] but is most commonly referred to as stoppage time or injury time, while loss time can also be used as a synonym. The duration of stoppage time is at the sole discretion of the referee. The referee alone signals the end of the match. In matches where a fourth official is appointed, towards the end of the half the referee signals how many minutes of stoppage time he intends to add. The fourth official then informs the players and spectators by holding up a board showing this number. The signalled stoppage time may be further extended by the referee.[88] Added time was introduced because of an incident which happened in 1891 during a match between Stoke and Aston Villa. Trailing 1–0 and with just two minutes remaining, Stoke were awarded a penalty. Villa's goalkeeper kicked the ball out of the ground, and by the time the ball had been recovered, the 90 minutes had elapsed and the game was over.[91] The same law also states that the duration of either half is extended until the penalty kick to be taken or retaken is completed, thus no game shall end with a penalty to be taken.[92]		In league competitions, games may end in a draw. In knockout competitions where a winner is required various methods may be employed to break such a deadlock, some competitions may invoke replays.[93] A game tied at the end of regulation time may go into extra time, which consists of two further 15-minute periods. If the score is still tied after extra time, some competitions allow the use of penalty shootouts (known officially in the Laws of the Game as "kicks from the penalty mark") to determine which team will progress to the next stage of the tournament. Goals scored during extra time periods count towards the final score of the game, but kicks from the penalty mark are only used to decide the team that progresses to the next part of the tournament (with goals scored in a penalty shootout not making up part of the final score).[64]		In competitions using two-legged matches, each team competes at home once, with an aggregate score from the two matches deciding which team progresses. Where aggregates are equal, the away goals rule may be used to determine the winners, in which case the winner is the team that scored the most goals in the leg they played away from home. If the result is still equal, extra time and potentially a penalty shootout are required.[64]		In the late 1990s and early 2000s, the IFAB experimented with ways of creating a winner without requiring a penalty shootout, which was often seen as an undesirable way to end a match. These involved rules ending a game in extra time early, either when the first goal in extra time was scored (golden goal), or if one team held a lead at the end of the first period of extra time (silver goal). Golden goal was used at the World Cup in 1998 and 2002. The first World Cup game decided by a golden goal was France's victory over Paraguay in 1998. Germany was the first nation to score a golden goal in a major competition, beating Czech Republic in the final of Euro 1996. Silver goal was used in Euro 2004. Both these experiments have been discontinued by IFAB.[94]		Under the Laws, the two basic states of play during a game are ball in play and ball out of play. From the beginning of each playing period with a kick-off until the end of the playing period, the ball is in play at all times, except when either the ball leaves the field of play, or play is stopped by the referee. When the ball becomes out of play, play is restarted by one of eight restart methods depending on how it went out of play:		A foul occurs when a player commits an offence listed in the Laws of the Game while the ball is in play. The offences that constitute a foul are listed in Law 12. Handling the ball deliberately, tripping an opponent, or pushing an opponent, are examples of "penal fouls", punishable by a direct free kick or penalty kick depending on where the offence occurred. Other fouls are punishable by an indirect free kick.[66]		The referee may punish a player's or substitute's misconduct by a caution (yellow card) or dismissal (red card). A second yellow card in the same game leads to a red card, and which results in a dismissal. A player given a yellow card is said to have been "booked", the referee writing the player's name in his official notebook. If a player has been dismissed, no substitute can be brought on in their place and the player must leave the field. Misconduct may occur at any time, and while the offences that constitute misconduct are listed, the definitions are broad. In particular, the offence of "unsporting behaviour" may be used to deal with most events that violate the spirit of the game, even if they are not listed as specific offences. A referee can show a yellow or red card to a player, substitute or substituted player. Non-players such as managers and support staff cannot be shown the yellow or red card, but may be expelled from the technical area if they fail to conduct themselves in a responsible manner.[66]		Rather than stopping play, the referee may allow play to continue if doing so will benefit the team against which an offence has been committed. This is known as "playing an advantage".[100] The referee may "call back" play and penalise the original offence if the anticipated advantage does not ensue within "a few seconds". Even if an offence is not penalised due to advantage being played, the offender may still be sanctioned for misconduct at the next stoppage of play.[101]		The referee's decision in all on-pitch matters is considered final.[102] The score of a match cannot be altered after the game, even if later evidence shows that decisions (including awards/non-awards of goals) were incorrect.		Along with the general administration of the sport, football associations and competition organisers also enforce good conduct in wider aspects of the game, dealing with issues such as comments to the press, clubs' financial management, doping, age fraud and match fixing. Most competitions enforce mandatory suspensions for players who are sent off in a game.[103] Some on-field incidents, if considered very serious (such as allegations of racial abuse), may result in competitions deciding to impose heavier sanctions than those normally associated with a red card.[b] Some associations allow for appeals against player suspensions incurred on-field if clubs feel a referee was incorrect or unduly harsh.[103]		Sanctions for such infractions may be levied on individuals or on to clubs as a whole. Penalties may include fines, points deductions (in league competitions) or even expulsion from competitions. For example, the English and Scottish leagues will often deduct 10 points from a team that enters financial administration. Among other administrative sanctions are penalties against game forfeiture. Teams that had forfeited a game or had been forfeited against would be awarded a technical loss or win.		The recognised international governing body of football (and associated games, such as futsal and beach soccer) is FIFA. The FIFA headquarters are located in Zürich, Switzerland. Six regional confederations are associated with FIFA; these are:[104]		National associations oversee football within individual countries. These are generally synonymous with sovereign states, (for example: the Fédération Camerounaise de Football in Cameroon) but also include a smaller number of associations responsible for sub-national entities or autonomous regions (for example the Scottish Football Association in Scotland). 209 national associations are affiliated both with FIFA and with their respective continental confederations.[104]		While FIFA is responsible for arranging competitions and most rules related to international competition, the actual Laws of the Game are set by the International Football Association Board, where each of the UK Associations has one vote, while FIFA collectively has four votes.[36]		The major international competition in football is the World Cup, organised by FIFA. This competition takes place every four years since 1930 with the exception of 1942 and 1946 tournaments, which were cancelled due to World War II. Approximately 190–200 national teams compete in qualifying tournaments within the scope of continental confederations for a place in the finals. The finals tournament, which is held every four years, involves 32 national teams competing over a four-week period.[c] The World Cup is the most prestigious association football tournament in the world as well as the most widely viewed and followed sporting event in the world, exceeding even the Olympic Games; the cumulative audience of all matches of the 2006 FIFA World Cup was estimated to be 26.29 billion with an estimated 715.1 million people watching the final match, a ninth of the entire population of the planet.[105][106][107][108] The current champion is Germany, which won its fourth title at the 2014 tournament in Brazil. FIFA Women's World Cup has been held every four years since 1991. Under the tournament's current format, national teams vie for 23 slots in a three-year qualification phase. (The host nation's team is automatically entered as the 24th slot.) The current champion is the United States, after winning their third title in the 2015 FIFA Women's World Cup.		There has been a football tournament at every Summer Olympic Games since 1900, except at the 1932 games in Los Angeles.[109] Before the inception of the World Cup, the Olympics (especially during the 1920s) had the same status as the World Cup. Originally, the event was for amateurs only.[35] As professionalism spread around the world, the gap in quality between the World Cup and the Olympics widened. The countries that benefited most were the Soviet Bloc countries of Eastern Europe, where top athletes were state-sponsored while retaining their status as amateurs. Between 1948 and 1980, 23 out of 27 Olympic medals were won by Eastern Europe, with only Sweden (gold in 1948 and bronze in 1952), Denmark (bronze in 1948 and silver in 1960) and Japan (bronze in 1968) breaking their dominance. For the 1984 Los Angeles Games, the IOC decided to admit professional players. FIFA still did not want the Olympics to rival the World Cup, so a compromise was struck that allowed teams from Africa, Asia, Oceania and CONCACAF to field their strongest professional sides, while restricting UEFA and CONMEBOL teams to players who had not played in a World Cup. Since 1992 male competitors must be under 23 years old, and since 1996, players under 23 years old, with three over-23 year old players, are allowed per squad. A women's tournament was added in 1996; in contrast to the men's event, full international sides without age restrictions play the women's Olympic tournament.[110]		After the World Cup, the most important international football competitions are the continental championships, which are organised by each continental confederation and contested between national teams. These are the European Championship (UEFA), the Copa América (CONMEBOL), African Cup of Nations (CAF), the Asian Cup (AFC), the CONCACAF Gold Cup (CONCACAF) and the OFC Nations Cup (OFC). The FIFA Confederations Cup is contested by the winners of all six continental championships, the current FIFA World Cup champions and the country which is hosting the Confederations Cup. This is generally regarded as a warm-up tournament for the upcoming FIFA World Cup and does not carry the same prestige as the World Cup itself. The most prestigious competitions in club football are the respective continental championships, which are generally contested between national champions, for example the UEFA Champions League in Europe and the Copa Libertadores in South America. The winners of each continental competition contest the FIFA Club World Cup.[111]		The governing bodies in each country operate league systems in a domestic season, normally comprising several divisions, in which the teams gain points throughout the season depending on results. Teams are placed into tables, placing them in order according to points accrued. Most commonly, each team plays every other team in its league at home and away in each season, in a round-robin tournament. At the end of a season, the top team is declared the champion. The top few teams may be promoted to a higher division, and one or more of the teams finishing at the bottom are relegated to a lower division.[112]		The teams finishing at the top of a country's league may be eligible also to play in international club competitions in the following season. The main exceptions to this system occur in some Latin American leagues, which divide football championships into two sections named Apertura and Clausura (Spanish for Opening and Closing), awarding a champion for each.[113] The majority of countries supplement the league system with one or more "cup" competitions organised on a knock-out basis.		Some countries' top divisions feature highly paid star players; in smaller countries and lower divisions, players may be part-timers with a second job, or amateurs. The five top European leagues – the Bundesliga (Germany), Premier League (England),[114] La Liga (Spain), Serie A (Italy), and Ligue 1 (France) – attract most of the world's best players and each of the leagues has a total wage cost in excess of £600 million/€763 million/US$1.185 billion.[115]		Variants of football have been codified for reduced-sized teams (i.e. five-a-side football) play in non-field environments (i.e. beach soccer, indoor soccer, and futsal) and for teams with disabilities (i.e. paralympic association football).		Casual games can be played with only minimal equipment – a basic game can be played on almost any open area of reasonable size with just a ball and items to mark the positions of two sets of goalposts. Such games can have team sizes that vary from eleven-a-side, can use a limited or modified subset of the official rules, and can be self-officiated by the players.		
JSTOR (/ˈdʒeɪstɔːr/ JAY-stor;[3] short for Journal Storage) is a digital library founded in 1995. Originally containing digitized back issues of academic journals, it now also includes books and primary sources, and current issues of journals.[4] It provides full-text searches of almost 2,000 journals.[5] As of 2013, more than 8,000 institutions in more than 160 countries had access to JSTOR;[5] most access is by subscription, but some older public domain content is freely available to anyone.[6] JSTOR's revenue was $69 million in 2014.[7]						William G. Bowen, president of Princeton University from 1972 to 1988, founded JSTOR.[8] JSTOR originally was conceived as a solution to one of the problems faced by libraries, especially research and university libraries, due to the increasing number of academic journals in existence. Most libraries found it prohibitively expensive in terms of cost and space to maintain a comprehensive collection of journals. By digitizing many journal titles, JSTOR allowed libraries to outsource the storage of journals with the confidence that they would remain available long-term. Online access and full-text search ability improved access dramatically.		Bowen initially considered using CD-ROMs for distribution.[9] However, Ira Fuchs, Princeton University's vice-president for Computing and Information Technology, convinced Bowen that CD-ROM was an increasingly outdated technology and that network distribution could eliminate redundancy and increase accessibility. (For example, all Princeton's administrative and academic buildings were networked by 1989; the student dormitory network was completed in 1994; and campus networks like the one at Princeton were, in turn, linked to larger networks such as BITNET and the Internet.) JSTOR was initiated in 1995 at seven different library sites, and originally encompassed ten economics and history journals. JSTOR access improved based on feedback from its initial sites, and it became a fully searchable index accessible from any ordinary web browser. Special software was put in place[where?] to make pictures and graphs clear and readable.[10]		With the success of this limited project, Bowen and Kevin Guthrie, then-president of JSTOR, wanted to expand the number of participating journals. They met with representatives of the Royal Society of London and an agreement was made[by whom?] to digitize the Philosophical Transactions of the Royal Society dating from its beginning in 1665. The work of adding these volumes to JSTOR was completed by December 2000.[10]		The Andrew W. Mellon Foundation funded JSTOR initially. Until January 2009 JSTOR operated as an independent, self-sustaining nonprofit organization with offices in New York City and in Ann Arbor, Michigan. Then JSTOR merged with the nonprofit Ithaka Harbors, Inc.[11] - a nonprofit organization founded in 2003 and "dedicated to helping the academic community take full advantage of rapidly advancing information and networking technologies."[1]		JSTOR content is provided by more than 900 publishers.[5] The database contains more than 1,900 journal titles,[5] in more than 50 disciplines. Each object is uniquely identified by an integer value, starting at 1.		In addition to the main site, the JSTOR labs group operates an open service that allows access to the contents of the archives for the purposes of corpus analysis at its Data for Research service.[12] This site offers a search facility with graphical indication of the article coverage and loose integration into the main JSTOR site. Users may create focused sets of articles and then request a dataset containing word and n-gram frequencies and basic metadata. They are notified when the dataset is ready and may download it in either XML or CSV formats. The service does not offer full-text, although academics may request that from JSTOR, subject to a non-disclosure agreement.		JSTOR Plant Science[13] is available in addition to the main site. JSTOR Plant Science provides access to content such as plant type specimens, taxonomic structures, scientific literature, and related materials and aimed at those researching, teaching, or studying botany, biology, ecology, environmental, and conservation studies. The materials on JSTOR Plant Science are contributed through the Global Plants Initiative (GPI)[14] and are accessible only to JSTOR and GPI members. Two partner networks are contributing to this: the African Plants Initiative, which focuses on plants from Africa, and the Latin American Plants Initiative, which contributes plants from Latin America.		JSTOR launched its Books at JSTOR program in November 2012, adding 15,000 current and backlist books to its site. The books are linked with reviews and from citations in journal articles.[15]		JSTOR is licensed mainly to academic institutions, public libraries, research institutions, museums, and schools. More than 7,000 institutions in more than 150 countries have access.[4] JSTOR has been running a pilot program of allowing subscribing institutions to provide access to their alumni, in addition to current students and staff. The Alumni Access Program officially launched in January 2013.[16] Individual subscriptions also are available to certain journal titles through the journal publisher.[17] Every year, JSTOR blocks 150 million attempts by non-subscribers to read articles.[18]		Inquiries have been made about the possibility of making JSTOR open access. According to Harvard Law professor Lawrence Lessig, JSTOR had been asked "how much would it cost to make this available to the whole world, how much would we need to pay you? The answer was $250 million".[19]		In late 2010 and early 2011, Internet activist Aaron Swartz used MIT's data network to bulk-download a substantial portion of JSTOR's collection of academic journal articles.[20][21] When the bulk-download was discovered, a video camera was placed in the room to film the mysterious visitor and the relevant computer was left untouched. Once video was captured of the visitor, the download was stopped and Swartz identified. Rather than pursue a civil lawsuit against him, in June 2011 they reached a settlement wherein he surrendered the downloaded data.[20][21]		The following month, federal authorities charged Swartz with several "data theft"-related crimes, including wire fraud, computer fraud, unlawfully obtaining information from a protected computer, and recklessly damaging a protected computer.[22][23] Prosecutors in the case claimed that Swartz acted with the intention of making the papers available on P2P file-sharing sites.[21][24]		Swartz surrendered to authorities, pleaded not guilty to all counts, and was released on $100,000 bail. In September 2012, U.S. attorneys increased the number of charges against Swartz from four to thirteen, with a possible penalty of 35 years in prison and $1 million in fines.[25][26] The case still was pending when Swartz committed suicide in January 2013.[27] Prosecutors dropped the charges after his death.[28]		The availability of most journals on JSTOR is controlled by a "moving wall," which is an agreed-upon delay between the current volume of the journal and the latest volume available on JSTOR. This time period is specified by agreement between JSTOR and the publisher of the journal, which usually is three to five years. Publishers may request that the period of a "moving wall" be changed or request discontinuation of coverage. Formerly, publishers also could request that the "moving wall" be changed to a "fixed wall"—a specified date after which JSTOR would not add new volumes to its database. As of November 2010[update], "fixed wall" agreements were still in effect with three publishers of 29 journals made available online through sites controlled by the publishers.[29]		In 2010, JSTOR started adding current issues of certain journals through its Current Scholarship Program.[30]		Beginning September 6, 2011, JSTOR made public domain content freely available to the public.[31][32] This "Early Journal Content" program constitutes about 6% of JSTOR's total content, and includes over 500,000 documents from more than 200 journals that were published before 1923 in the United States, and before 1870 in other countries.[31][32][33] JSTOR stated that it had been working on making this material free for some time. The Swartz controversy and Greg Maxwell's protest torrent of the same content led JSTOR to "press ahead" with the initiative.[31][32] As of 2017, JSTOR does not have plans to extend it to other public domain content, stating that "We do not believe that just because something is in the public domain, it can always be provided for free".[34]		In January 2012, JSTOR started a pilot program, "Register & Read," offering limited no-cost access (not open access) to archived articles for individuals who register for the service. At the conclusion of the pilot, in January 2013, JSTOR expanded Register & Read from an initial 76 publishers to include about 1,200 journals from over 700 publishers.[35] Registered readers may read up to three articles online every two weeks, but may not print or download PDFs.[36]		This is done by placing up to 3 items on a "shelf". The "Shelf" is under "My JSTOR" below "My Profile". The 3 works can then be read online at any time. An item cannot be removed from the shelf until it has been there for 14 days. Removing an old work from the shelf creates space for a new one, but doing so means the old work can no longer be accessed until it is shelved again.		JSTOR is conducting a pilot program with Wikipedia, whereby established editors are given reading privileges through the Wikipedia Library, as with a university library.[37][38]		In 2012, JSTOR users performed nearly 152 million searches, with more than 113 million article views and 73.5 million article downloads.[5] JSTOR has been used as a resource for linguistics research to investigate trends in language use over time and also to analyze gender differences in scholarly publishing.[39][40]		
The Journal of Men's Studies (abbreviated JMS) is a peer-reviewed journal established in 1992 as the first published by Men's Studies Press. As of 2015 the journal is published by Sage Publications.				
The Leningrad première of Shostakovich's Symphony No. 7 took place on 9 August 1942 during the Second World War, while the city (now Saint Petersburg) was under siege by Nazi German forces. Dmitri Shostakovich (pictured) had intended for the piece to be premièred by the Leningrad Philharmonic Orchestra, but they had been evacuated because of the siege, along with the composer, and the world première was instead held in Kuybyshev. The Leningrad première was performed by the surviving musicians of the Leningrad Radio Orchestra, supplemented with military performers. Most of the musicians were starving, and three died during rehearsals. Supported by a Soviet military offensive intended to silence German forces, the performance was a success, prompting an hour-long ovation. The symphony was broadcast to the German lines by loudspeaker as a form of psychological warfare. The Leningrad première was considered by music critics to be one of the most important artistic performances of the war because of its psychological and political effects. Reunion concerts featuring surviving musicians were convened in 1964 and 1992 to commemorate the event. (Full article...)		August 9: International Day of the World's Indigenous Peoples; National Women's Day in South Africa		Hieronymus Bosch (d. 1516) · Elizabeth Schuyler Hamilton (b. 1757) · Gillian Anderson (b. 1968)		Marina City is a mixed-use residential/commercial building complex in Chicago, Illinois. It occupies almost an entire city block on State Street and sits on the north bank of the Chicago River in downtown Chicago, directly across from the Loop. The complex consists of two corncob-shaped, 587-foot (179 m), 65-story towers, as well as a saddle-shaped auditorium building and a mid-rise hotel building. Designed by Bertrand Goldberg, Marina City was the first building in the United States to be constructed with tower cranes.		Photograph: Diego Delso		Wikipedia is hosted by the Wikimedia Foundation, a non-profit organization that also hosts a range of other projects:		This Wikipedia is written in English. Started in 2001 (2001), it currently contains 5,456,578 articles. Many other Wikipedias are available; some of the largest are listed below.		
The Jews (/dʒuːz/;[13] Hebrew: יְהוּדִים‎ ISO 259-3 Yhudim, Israeli pronunciation [jehuˈdim]), also known as the Jewish people, are an ethnoreligious group[14] and nation[15][16][17] originating from the Israelites, or Hebrews, of the Ancient Near East.[18][19] Jewish ethnicity, nationhood and religion are strongly interrelated, as Judaism is the traditional faith of the Jewish nation,[20][21][22] while its observance varies from strict observance to complete nonobservance.		Jews originated as a national and religious group in the Middle East during the second millennium BCE,[9] in the part of the Levant known as the Land of Israel.[23] The Merneptah Stele appears to confirm the existence of a people of Israel somewhere in Canaan as far back as the 13th century BCE (Late Bronze Age).[24][25] The Israelites, as an outgrowth of the Canaanite population,[26] consolidated their hold with the emergence of the Kingdom of Israel, and the Kingdom of Judah. Some consider that these Canaanite sedentary Israelites melded with incoming nomadic groups known as 'Hebrews'.[27] Though few sources in the Bible mention the exilic periods in detail,[28] the experience of diaspora life, from the Ancient Egyptian rule over the Levant, to Assyrian Captivity and Exile, to Babylonian Captivity and Exile, to Seleucid Imperial rule, to the Roman occupation and Exile, and the historical relations between Jews and their homeland thereafter, became a major feature of Jewish history, identity and memory.[29]		The worldwide Jewish population reached a peak of 16.7 million prior to World War II,[30] but approximately 6 million Jews were systematically murdered[31][32] during the Holocaust. Since then the population has slowly risen again, and as of 2016[update] was estimated at 14.4 million by the Berman Jewish DataBank,[1] or less than 0.2% of the total world population (roughly one in every 514 people).[33] According to the report, about 44% of all Jews reside in Israel (6.3 million), and 40% in the United States (5.7 million), with most of the remainder living in Europe (1.4 million) and Canada (0.4 million).[1] These numbers include all those who self-identified as Jews in a socio-demographic study or were identified as such by a respondent in the same household.[34] The exact world Jewish population, however, is difficult to measure. In addition to issues with census methodology, disputes among proponents of halakhic, secular, political, and ancestral identification factors regarding who is a Jew may affect the figure considerably depending on the source.[35] Israel is the only country where Jews form a majority of the population. The modern State of Israel was established as a Jewish state and defines itself as such in its Declaration of Independence and Basic Laws. Its Law of Return grants the right of citizenship to any Jew who requests it.[36]		Despite their small percentage of the world's population, Jews have significantly influenced and contributed to human progress in many fields, including philosophy,[37] ethics,[38] literature, business, fine arts and architecture, religion, music, theatre[39] and cinema, medicine,[40][41] as well as science and technology, both historically and in modern times.						The English word Jew continues Middle English Gyw, Iewe. These terms derive from Old French giu, earlier juieu, which had elided (dropped) the letter "d" from the Medieval Latin Iudaeus, which, like the New Testament Greek term Ioudaios, meant both Jews and Judeans / "of Judea".[42]		The Greek term was originally a loan from Aramaic Y'hūdāi, corresponding to Hebrew: יְהוּדִי‎, Yehudi (sg.); יְהוּדִים‎, Yehudim (pl.), in origin the term for a member of the tribe of Judah or the people of the kingdom of Judah. According to the Hebrew Bible, the name of both the tribe and kingdom derive from Judah, the fourth son of Jacob.[43]		The Hebrew word for Jew, יְהוּדִי‎ ISO 259-3 Yhudi, is pronounced [jehuˈdi], with the stress on the final syllable, in Israeli Hebrew, in its basic form.[44] The Ladino name is ג׳ודיו‎, Djudio (sg.); ג׳ודיוס‎, Djudios (pl.); Yiddish: ייִד‎ Yid (sg.); ייִדן‎, Yidn (pl.).		The etymological equivalent is in use in other languages, e.g., يَهُودِيّ yahūdī (sg.), al-yahūd (pl.), and بَنُو اِسرَائِيل banū isrāʼīl in Arabic, "Jude" in German, "judeu" in Portuguese, "juif" in French, "jøde" in Danish and Norwegian, "judío" in Spanish, "jood" in Dutch, "żyd" in Polish etc., but derivations of the word "Hebrew" are also in use to describe a Jew, e.g., in Italian (Ebreo), in Persian ("Ebri/Ebrani" (Persian: عبری/عبرانی‎‎)) and Russian (Еврей, Yevrey).[45] The German word "Jude" is pronounced [ˈjuːdə], the corresponding adjective "jüdisch" [ˈjyːdɪʃ] (Jewish) is the origin of the word "Yiddish".[46] (See Jewish ethnonyms for a full overview.)		According to The American Heritage Dictionary of the English Language, fourth edition (2000):		It is widely recognized that the attributive use of the noun Jew, in phrases such as Jew lawyer or Jew ethics, is both vulgar and highly offensive. In such contexts Jewish is the only acceptable possibility. Some people, however, have become so wary of this construction that they have extended the stigma to any use of Jew as a noun, a practice that carries risks of its own. In a sentence such as There are now several Jews on the council, which is unobjectionable, the substitution of a circumlocution like Jewish people or persons of Jewish background may in itself cause offense for seeming to imply that Jew has a negative connotation when used as a noun.[47]		A factual reconstruction for the origin of the Jews is a difficult and complex endeavor. It requires examining at least 3,000 years of ancient human history using documents in vast quantities and variety written in at least ten near Eastern languages. As archaeological discovery relies upon researchers and scholars from diverse disciplines, the goal is to interpret all of the factual data, focusing on the most consistent theory. In this case, it is complicated by long standing politics and religious and cultural prejudices.[48]		According to the Hebrew Bible narrative, Jewish ancestry is traced back to the Biblical patriarchs such as Abraham, his son Isaac, Isaac's son Jacob, and the Biblical matriarchs Sarah, Rebecca, Leah, and Rachel, who lived in Canaan. The Twelve Tribes are described as descending from the twelve sons of Jacob. Jacob and his family migrated to Ancient Egypt after being invited to live with Jacob's son Joseph by the Pharaoh himself. The patriarchs' descendants were later enslaved until the Exodus led by Moses, after which the Israelites conquered Canaan under Moses' successor Joshua, went through the period of the Biblical judges after the death of Joshua, then through the mediation of Samuel became subject to a king, Saul, who was succeeded by David and then Solomon, after whom the United Monarchy ended and was split into a separate Kingdom of Israel and a Kingdom of Judah. The Kingdom of Judah is described as comprising the Tribe of Judah, the Tribe of Benjamin, and partially the tribe of Tribe of Levi, and later adding other tribes who migrated there from the Kingdom of Israel.[49][50]		Modern archaeology has largely discarded the historicity of this narrative,[51] with it being reframed as constituting the Israelites' inspiring national myth narrative. The Israelites and their culture, according to the modern archaeological account, did not overtake the region by force, but instead branched out of the Canaanite peoples and culture through the development of a distinct monolatristic—and later monotheistic—religion centered on Yahweh, one of the Ancient Canaanite deities. The growth of Yahweh-centric belief, along with a number of cultic practices, gradually gave rise to a distinct Israelite ethnic group, setting them apart from other Canaanites.[52][53][54]		The Israelites become visible in the historical record as a people between 1200 and 1000 BCE.[55] It is not certain if a period like that of the Biblical judges occurred[56][57][58][59][60] nor if there was ever a United Monarchy.[61][62][63][64] There is well accepted archeological evidence referring to "Israel" in the Merneptah Stele which dates to about 1200 BCE;[24][25] and the Canaanites are archeologically attested in the Middle Bronze Age,[65][66] There is debate about the earliest existence of the Kingdoms of Israel and Judah and their extent and power, but historians agree that a Kingdom of Israel existed by ca. 900 BCE[62]:169–195[63][64] and that a Kingdom of Judah existed by ca. 700 BCE.[67] It is widely accepted that the Kingdom of Israel was destroyed around 720 BCE, when it was conquered by the Neo-Assyrian Empire.[50]		The term Jew originated from the Roman "Judean" and denoted someone from the southern kingdom of Judah.[68] The shift of ethnonym from "Israelites" to "Jews" (inhabitant of Judah), although not contained in the Torah, is made explicit in the Book of Esther (4th century BCE),[69] a book in the Ketuvim, the third section of the Jewish Tanakh. In 587 BCE Nebuchadnezzar II, King of the Neo-Babylonian Empire, besieged Jerusalem, destroyed the First Temple, and deported the most prominent citizens of Judah.[70]		According to the book of Ezra, the Persian Cyrus the Great ended th Babylonian exile in 538 BCE,[71] the year after he captured Babylon.[72] The exile ended with the return under Zerubbabel the Prince (so-called because he was a descendant of the royal line of David) and Joshua the Priest (a descendant of the line of the former High Priests of the Temple) and their construction of the Second Temple in the period 521–516 BCE.[71] The Cyrus Cylinder, an ancient tablet on which is written a declaration in the name of Cyrus referring to restoration of temples and repatriation of exiled peoples, has often been taken as corroboration of the authenticity of the biblical decrees attributed to Cyrus,[73] but other scholars point out that the cylinder's text is specific to Babylon and Mesopotamia and makes no mention of Judah or Jerusalem.[73] Professor Lester L. Grabbe asserted that the "alleged decree of Cyrus" regarding Judah, "cannot be considered authentic", but that there was a "general policy of allowing deportees to return and to re-establish cult sites". He also stated that archaeology suggests that the return was a "trickle" taking place over decades, rather than a single event.[74]		As part of the Persian Empire, the former Kingdom of Judah became the province of Judah (Yehud Medinata)[75] with different borders, covering a smaller territory.[74] The population of the province was greatly reduced from that of the kingdom, archaeological surveys showing a population of around 30,000 people in the 5th to 4th centuries BCE.[62]:308 The region was under control of the Achaemenids until the fall of their empire in c. 333 BCE to Alexander the Great. Jews were also politically independent during the Hasmonean dynasty spanning from 140 to 37 BCE and to some degree under the Herodian dynasty from 37 BCE to 6 CE. Since the destruction of the Second Temple in 70 CE, most Jews have lived in diaspora.[76]		Genetic studies on Jews show that most Jews worldwide bear a common genetic heritage which originates in the Middle East, and that they bear their strongest resemblance to the peoples of the Fertile Crescent.[77][78][79] The genetic composition of different Jewish groups shows that Jews share a common genetic pool dating back 4,000 years, as a marker of their common ancestral origin. Despite their long-term separation, Jewish communities maintained commonalities in culture, tradition, and language.[80]		The Jewish people and the religion of Judaism are strongly interrelated. Converts to Judaism typically have a status within the Jewish ethnos equal to those born into it.[81] However, several converts to Judaism, as well as ex-Jews, have claimed that converts are treated as second-class Jews by many of the born-Jews.[82] Conversion is not encouraged by mainstream Judaism, and is considered a difficult task. A significant portion of conversions are undertaken by children of mixed marriages, or by would-be or current spouses of Jews.[83]		The Hebrew Bible, a religious interpretation of the traditions and early national history of the Jews, established the first of the Abrahamic religions, which are now practiced by 54% of the world. Judaism guides its adherents in both practice and belief, and has been called not only a religion, but also a "way of life,"[84] which has made drawing a clear distinction between Judaism, Jewish culture, and Jewish identity rather difficult. Throughout history, in eras and places as diverse as the ancient Hellenic world,[85] in Europe before and after The Age of Enlightenment (see Haskalah),[86] in Islamic Spain and Portugal,[87] in North Africa and the Middle East,[87] India,[88] China,[89] or the contemporary United States[90] and Israel,[91] cultural phenomena have developed that are in some sense characteristically Jewish without being at all specifically religious. Some factors in this come from within Judaism, others from the interaction of Jews or specific communities of Jews with their surroundings, others from the inner social and cultural dynamics of the community, as opposed to from the religion itself. This phenomenon has led to considerably different Jewish cultures unique to their own communities.[92]		After the destruction of the Second Temple Judaism lost much of its sectarian nature.[93]:69 Nevertheless, a significant Hellenized Diaspora remained, centered in Alexandria, at the time the largest urban Jewish community in the world. Hellenism was a force not just in the Diaspora but also in the Land of Israel over a long period of time. Generally, scholars view Rabbinic Judaism as having been meaningfully influenced by Hellenism.[citation needed]		Without a Temple, Greek speaking Jews no longer looked to Jerusalem in the way they had before. Judaism separated into a linguistically Greek and a Hebrew / Aramaic sphere.[94]: 8–11 The theology and religious texts of each community were distinctively different.[94]: 11–13 Hellenized Judaism never developed yeshivas to study the Oral Law. Rabbinic Judaism (centered in the Land of Israel and Babylon) almost entirely ignores the Hellenized Diaspora in its writings.[94]: 13–14 Hellenized Judaism eventually disappeared as its practitioners assimilated into Greco-Roman culture, leaving a strong Rabbinic eastern Diaspora with large centers of learning in Babylon.[94]: 14–16		By the first century, the Jewish community in Babylonia, to which Jews were exiled after the Babylonian conquest as well as after the Bar Kokhba revolt in 135 CE, already held a speedily growing[95] population of an estimated one million Jews, which increased to an estimated two million[96] between the years 200 CE and 500 CE, both by natural growth and by immigration of more Jews from the Land of Israel, making up about one-sixth of the world Jewish population at that era.[96] The 13th-century author Bar Hebraeus gave a figure of 6,944,000 Jews in the Roman world Salo Wittmayer Baron considered the figure convincing.[97] The figure of seven million within and one million outside the Roman world in the mid-first century became widely accepted, including by Louis Feldman. However, contemporary scholars now accept that Bar Hebraeus based his figure on a census of total Roman citizens. The figure of 6,944,000 being recorded in Eusebius' Chronicon.[98][99] Louis Feldman, previously an active supporter of the figure, now states that he and Baron were mistaken.[100]: 185 Feldman's views on active Jewish missionizing have also changed. While viewing classical Judaism as being receptive to converts, especially from the second century BCE through the first century CE, he points to a lack of either missionizing tracts or records of the names of rabbis who sought converts, as evidence for the lack of active Jewish missionizing.[100]: 205–06 Feldman maintains that conversion to Judaism was common and the Jewish population was large both within the Land of Israel and in the Diaspora.[100]: 183–203, 206 Other historians believe that conversion during the Roman era was limited in number and did not account for much of the Jewish population growth, due to various factors such as the illegality of male conversion to Judaism in the Roman world from the mid-second century. Another factor that made conversion difficult in the Roman world was the halakhic requirement of circumcision, a requirement that proselytizing Christianity quickly dropped. The Fiscus Judaicus, a tax imposed on Jews in 70 CE and relaxed to exclude Christians in 96 CE, also limited Judaism's appeal.[101]		Judaism shares some of the characteristics of a nation,[15][16][17] an ethnicity,[14] a religion, and a culture,[102][103][104] making the definition of who is a Jew vary slightly depending on whether a religious or national approach to identity is used.[105][106] Generally, in modern secular usage Jews include three groups: people who were born to a Jewish family regardless of whether or not they follow the religion, those who have some Jewish ancestral background or lineage (sometimes including those who do not have strictly matrilineal descent), and people without any Jewish ancestral background or lineage who have formally converted to Judaism and therefore are followers of the religion.[107]		Historical definitions of Jewish identity have traditionally been based on halakhic definitions of matrilineal descent, and halakhic conversions. Historical definitions of who is a Jew date back to the codification of the Oral Torah into the Babylonian Talmud, around 200 CE. Interpretations of sections of the Tanakh, such as Deuteronomy 7:1–5, by Jewish sages, are used as a warning against intermarriage between Jews and Canaanites because "[the non-Jewish husband] will cause your child to turn away from Me and they will worship the gods (i.e., idols) of others." Leviticus 24:10 says that the son in a marriage between a Hebrew woman and an Egyptian man is "of the community of Israel." This is complemented by Ezra 10:2–3, where Israelites returning from Babylon vow to put aside their gentile wives and their children.[108][109] Since the anti-religious Haskalah movement of the late 18th and 19th centuries, halakhic interpretations of Jewish identity have been challenged.[110]		According to historian Shaye J. D. Cohen, the status of the offspring of mixed marriages was determined patrilineally in the Bible. He brings two likely explanations for the change in Mishnaic times: first, the Mishnah may have been applying the same logic to mixed marriages as it had applied to other mixtures (Kil'ayim). Thus, a mixed marriage is forbidden as is the union of a horse and a donkey, and in both unions the offspring are judged matrilineally.[111] Second, the Tannaim may have been influenced by Roman law, which dictated that when a parent could not contract a legal marriage, offspring would follow the mother.[111]		Within the world's Jewish population there are distinct ethnic divisions, most of which are primarily the result of geographic branching from an originating Israelite population, and subsequent independent evolutions. An array of Jewish communities was established by Jewish settlers in various places around the Old World, often at great distances from one another, resulting in effective and often long-term isolation. During the millennia of the Jewish diaspora the communities would develop under the influence of their local environments: political, cultural, natural, and populational. Today, manifestations of these differences among the Jews can be observed in Jewish cultural expressions of each community, including Jewish linguistic diversity, culinary preferences, liturgical practices, religious interpretations, as well as degrees and sources of genetic admixture.[112]		Jews are often identified as belonging to one of two major groups: the Ashkenazim and the Sephardim. Ashkenazim, or "Germanics" (Ashkenaz meaning "Germany" in Hebrew), are so named denoting their German Jewish cultural and geographical origins, while Sephardim, or "Hispanics" (Sefarad meaning "Spain/Hispania" or "Iberia" in Hebrew), are so named denoting their Spanish/Portuguese Jewish cultural and geographic origins. The more common term in Israel for many of those broadly called Sephardim, is Mizrahim (lit. "Easterners", Mizrach being "East" in Hebrew), that is, in reference to the diverse collection of Middle Eastern and North African Jews who are often, as a group, referred to collectively as Sephardim (together with Sephardim proper) for liturgical reasons, although Mizrahi Jewish groups and Sephardi Jews proper are ethnically distinct.[113]		Smaller groups include, but are not restricted to, Indian Jews such as the Bene Israel, Bnei Menashe, Cochin Jews, and Bene Ephraim; the Romaniotes of Greece; the Italian Jews ("Italkim" or "Bené Roma"); the Teimanim from Yemen; various African Jews, including most numerously the Beta Israel of Ethiopia; and Chinese Jews, most notably the Kaifeng Jews, as well as various other distinct but now almost extinct communities.[114]		The divisions between all these groups are approximate and their boundaries are not always clear. The Mizrahim for example, are a heterogeneous collection of North African, Central Asian, Caucasian, and Middle Eastern Jewish communities that are no closer related to each other than they are to any of the earlier mentioned Jewish groups. In modern usage, however, the Mizrahim are sometimes termed Sephardi due to similar styles of liturgy, despite independent development from Sephardim proper. Thus, among Mizrahim there are Egyptian Jews, Iraqi Jews, Lebanese Jews, Kurdish Jews, Libyan Jews, Syrian Jews, Bukharian Jews, Mountain Jews, Georgian Jews, Iranian Jews and various others. The Teimanim from Yemen are sometimes included, although their style of liturgy is unique and they differ in respect to the admixture found among them to that found in Mizrahim. In addition, there is a differentiation made between Sephardi migrants who established themselves in the Middle East and North Africa after the expulsion of the Jews from Spain and Portugal in the 1490s and the pre-existing Jewish communities in those regions.[114]		Ashkenazi Jews represent the bulk of modern Jewry, with at least 70% of Jews worldwide (and up to 90% prior to World War II and the Holocaust). As a result of their emigration from Europe, Ashkenazim also represent the overwhelming majority of Jews in the New World continents, in countries such as the United States, Canada, Argentina, Australia, and Brazil. In France, the immigration of Jews from Algeria (Sephardim) has led them to outnumber the Ashkenazim.[115] Only in Israel is the Jewish population representative of all groups, a melting pot independent of each group's proportion within the overall world Jewish population.[116]		Hebrew is the liturgical language of Judaism (termed lashon ha-kodesh, "the holy tongue"), the language in which most of the Hebrew scriptures (Tanakh) were composed, and the daily speech of the Jewish people for centuries. By the 5th century BCE, Aramaic, a closely related tongue, joined Hebrew as the spoken language in Judea.[117] By the 3rd century BCE, some Jews of the diaspora were speaking Greek.[118] Others, such as in the Jewish communities of Babylonia, were speaking Hebrew and Aramaic, the languages of the Babylonian Talmud. These languages were also used by the Jews of Israel at that time.[citation needed]		For centuries, Jews worldwide have spoken the local or dominant languages of the regions they migrated to, often developing distinctive dialectal forms or branches that became independent languages. Yiddish is the Judæo-German language developed by Ashkenazi Jews who migrated to Central Europe. Ladino is the Judæo-Spanish language developed by Sephardic Jews who migrated to the Iberian peninsula. Due to many factors, including the impact of the Holocaust on European Jewry, the Jewish exodus from Arab and Muslim countries, and widespread emigration from other Jewish communities around the world, ancient and distinct Jewish languages of several communities, including Judæo-Georgian, Judæo-Arabic, Judæo-Berber, Krymchak, Judæo-Malayalam and many others, have largely fallen out of use.[4]		For over sixteen centuries Hebrew was used almost exclusively as a liturgical language, and as the language in which most books had been written on Judaism, with a few speaking only Hebrew on the Sabbath.[119] Hebrew was revived as a spoken language by Eliezer ben Yehuda, who arrived in Palestine in 1881. It had not been used as a mother tongue since Tannaic times.[117] Modern Hebrew is now one of the two official languages of the State of Israel along with Modern Standard Arabic.[120]		Despite efforts to revive Hebrew as the national language of the Jewish people, knowledge of the language is not commonly possessed by Jews worldwide and English has emerged as the lingua franca of the Jewish diaspora.[121][122][123][124][125] Although many Jews once had sufficient knowledge of Hebrew to study the classic literature, and Jewish languages like Yiddish and Ladino were commonly used as recently as the early 20th century, most Jews lack such knowledge today and English has by and large superseded most Jewish vernaculars. The three most commonly spoken languages among Jews today are Hebrew, English, and Russian. Some Romance languages, particularly French and Spanish, are also widely used.[4] Yiddish has been spoken by more Jews in history than any other language,[126] but it is far less used today following the Holocaust and the adoption of Modern Hebrew by the Zionist movement and the State of Israel. In some places, the mother language of the Jewish community differs from that of the general population or the dominant group. For example, in Quebec, the Ashkenazic majority has adopted English, while the Sephardic minority uses French as its primary language.[127][128][129] Similarly, South African Jews adopted English rather than Afrikaans.[130] Due to both Czarist and Soviet policies,[131][132] Russian has superseded Yiddish as the language of Russian Jews, but these policies have also affected neighboring communities.[133] Today, Russian is the first language for many Jewish communities in a number of Post-Soviet states, such as Ukraine[134][135][136][137] and Uzbekistan,[138] as well as for Ashkenazic Jews in Azerbaijan,[139] Georgia,[140] and Tajikistan.[141][142] Although communities in North Africa today are small and dwindling, Jews there had shifted from a multilingual group to a monolingual one (or nearly so), speaking French in Algeria,[143] Morocco,[139] and the city of Tunis,[144][145] while most North Africans continue to use Arabic or Berber as their mother tongue.[citation needed]		Y DNA studies tend to imply a small number of founders in an old population whose members parted and followed different migration paths.[146] In most Jewish populations, these male line ancestors appear to have been mainly Middle Eastern. For example, Ashkenazi Jews share more common paternal lineages with other Jewish and Middle Eastern groups than with non-Jewish populations in areas where Jews lived in Eastern Europe, Germany and the French Rhine Valley. This is consistent with Jewish traditions in placing most Jewish paternal origins in the region of the Middle East.[147][148] Conversely, the maternal lineages of Jewish populations, studied by looking at mitochondrial DNA, are generally more heterogeneous.[149] Scholars such as Harry Ostrer and Raphael Falk believe this indicates that many Jewish males found new mates from European and other communities in the places where they migrated in the diaspora after fleeing ancient Israel.[150] In contrast, Behar has found evidence that about 40% of Ashkenazi Jews originate maternally from just four female founders, who were of Middle Eastern origin. The populations of Sephardi and Mizrahi Jewish communities "showed no evidence for a narrow founder effect."[149] Subsequent studies carried out by Feder et al. confirmed the large portion of non-local maternal origin among Ashkenazi Jews. Reflecting on their findings related to the maternal origin of Ashkenazi Jews, the authors conclude "Clearly, the differences between Jews and non-Jews are far larger than those observed among the Jewish communities. Hence, differences between the Jewish communities can be overlooked when non-Jews are included in the comparisons."[151][152][153]		Studies of autosomal DNA, which look at the entire DNA mixture, have become increasingly important as the technology develops. They show that Jewish populations have tended to form relatively closely related groups in independent communities, with most in a community sharing significant ancestry in common.[154] For Jewish populations of the diaspora, the genetic composition of Ashkenazi, Sephardi, and Mizrahi Jewish populations show a predominant amount of shared Middle Eastern ancestry. According to Behar, the most parsimonious explanation for this shared Middle Eastern ancestry is that it is "consistent with the historical formulation of the Jewish people as descending from ancient Hebrew and Israelite residents of the Levant" and "the dispersion of the people of ancient Israel throughout the Old World".[155] North African, Italian and others of Iberian origin show variable frequencies of admixture with non-Jewish historical host populations among the maternal lines. In the case of Ashkenazi and Sephardi Jews (in particular Moroccan Jews), who are closely related, the source of non-Jewish admixture is mainly southern European, while Mizrahi Jews show evidence of admixture with other Middle Eastern populations and Sub-Saharan Africans. Behar et al. have remarked on an especially close relationship of Ashkenazi Jews and modern Italians.[155][156][157] Jews were found to be more closely related to groups in the north of the Fertile Crescent (Kurds, Turks, and Armenians) than to Arabs.[158]		The studies also show that persons of Sephardic Bnei Anusim origin (those who are descendants of the "anusim" who were forced to convert to Catholicism) throughout today's Iberia (Spain and Portugal) and Ibero-America (Hispanic America and Brazil), estimated at up to 19.8% of the modern population of Iberia and at least 10% of the modern population of Ibero-America, have Sephardic Jewish ancestry within the last few centuries. The Bene Israel and Cochin Jews of India, Beta Israel of Ethiopia, and a portion of the Lemba people of Southern Africa, meanwhile, despite more closely resembling the local populations of their native countries, also have some more remote ancient Jewish descent.[159][160][161][153]		According to the Israel Central Bureau of Statistics there were 13,421,000 Jews worldwide in 2009, roughly 0.19% of the world's population at the time.[162]		According to the 2007 estimates of The Jewish People Policy Planning Institute, the world's Jewish population is 13.2 million.[163] Adherents.com cites figures ranging from 12 to 18 million.[164] These statistics incorporate both practicing Jews affiliated with synagogues and the Jewish community, and approximately 4.5 million unaffiliated and secular Jews.[citation needed]		According to Sergio DellaPergola, a demographer of the Jewish population, in 2015 there were about 6.3 million Jews in Israel, 5.7 million in the United States, and 2.3 million in the rest of the world.[165]		Israel, the Jewish nation-state, is the only country in which Jews make up a majority of the citizens.[166] Israel was established as an independent democratic and Jewish state on 14 May 1948.[167] Of the 120 members in its parliament, the Knesset,[168] as of 2016, 14 members of the Knesset are Arab citizens of Israel (not including the Druze), most representing Arab political parties. One of Israel's Supreme Court judges is also an Arab citizen of Israel.[169]		Between 1948 and 1958, the Jewish population rose from 800,000 to two million.[170] Currently, Jews account for 75.4% of the Israeli population, or 6 million people.[171][172] The early years of the State of Israel were marked by the mass immigration of Holocaust survivors in the aftermath of the Holocaust and Jews fleeing Arab lands.[173] Israel also has a large population of Ethiopian Jews, many of whom were airlifted to Israel in the late 1980s and early 1990s.[174] Between 1974 and 1979 nearly 227,258 immigrants arrived in Israel, about half being from the Soviet Union.[175] This period also saw an increase in immigration to Israel from Western Europe, Latin America, and North America.[176]		A trickle of immigrants from other communities has also arrived, including Indian Jews and others, as well as some descendants of Ashkenazi Holocaust survivors who had settled in countries such as the United States, Argentina, Australia, Chile, and South Africa. Some Jews have emigrated from Israel elsewhere, because of economic problems or disillusionment with political conditions and the continuing Arab–Israeli conflict. Jewish Israeli emigrants are known as yordim.[177]		The waves of immigration to the United States and elsewhere at the turn of the 19th century, the founding of Zionism and later events, including pogroms in Russia, the massacre of European Jewry during the Holocaust, and the founding of the state of Israel, with the subsequent Jewish exodus from Arab lands, all resulted in substantial shifts in the population centers of world Jewry by the end of the 20th century.[178]		More than half of the Jews live in the Diaspora (see Population table). Currently, the largest Jewish community outside Israel, and either the largest or second-largest Jewish community in the world, is located in the United States, with 5.2 million to 6.4 million Jews by various estimates. Elsewhere in the Americas, there are also large Jewish populations in Canada (315,000), Argentina (180,000–300,000), and Brazil (196,000–600,000), and smaller populations in Mexico, Uruguay, Venezuela, Chile, Colombia and several other countries (see History of the Jews in Latin America).[180] Demographers disagree on whether the United States has a larger Jewish population than Israel, with many maintaining that Israel surpassed the United States in Jewish population during the 2000s, while others maintain that the United States still has the largest Jewish population in the world. Currently, a major national Jewish population survey is planned to ascertain whether or not Israel has overtaken the United States in Jewish population.[181]		Western Europe's largest Jewish community, and the third-largest Jewish community in the world, can be found in France, home to between 483,000 and 500,000 Jews, the majority of whom are immigrants or refugees from North African countries such as Algeria, Morocco, and Tunisia (or their descendants).[182] The United Kingdom has a Jewish community of 292,000. In Eastern Europe, there are anywhere from 350,000 to one million Jews living in the former Soviet Union, but exact figures are difficult to establish. In Germany, the 102,000 Jews registered with the Jewish community are a slowly declining population,[183] despite the immigration of tens of thousands of Jews from the former Soviet Union since the fall of the Berlin Wall.[184] Thousands of Israelis also live in Germany, either permanently or temporarily, for economic reasons.[185]		Prior to 1948, approximately 800,000 Jews were living in lands which now make up the Arab world (excluding Israel). Of these, just under two-thirds lived in the French-controlled Maghreb region, 15–20% in the Kingdom of Iraq, approximately 10% in the Kingdom of Egypt and approximately 7% in the Kingdom of Yemen. A further 200,000 lived in Pahlavi Iran and the Republic of Turkey. Today, around 26,000 Jews live in Arab countries[186] and around 30,000 in Iran and Turkey. A small-scale exodus had begun in many countries in the early decades of the 20th century, although the only substantial aliyah came from Yemen and Syria.[187] The exodus from Arab and Muslim countries took place primarily from 1948. The first large-scale exoduses took place in the late 1940s and early 1950s, primarily in Iraq, Yemen and Libya, with up to 90% of these communities leaving within a few years. The peak of the exodus from Egypt occurred in 1956. The exodus in the Maghreb countries peaked in the 1960s. Lebanon was the only Arab country to see a temporary increase in its Jewish population during this period, due to an influx of refugees from other Arab countries, although by the mid-1970s the Jewish community of Lebanon had also dwindled. In the aftermath of the exodus wave from Arab states, an additional migration of Iranian Jews peaked in the 1980s when around 80% of Iranian Jews left the country.[citation needed]		Outside Europe, the Americas, the Middle East, and the rest of Asia, there are significant Jewish populations in Australia (112,500) and South Africa (70,000).[30] There is also a 7,500-strong community in New Zealand.[citation needed]		Since at least the time of the Ancient Greeks, a proportion of Jews have assimilated into the wider non-Jewish society around them, by either choice or force, ceasing to practice Judaism and losing their Jewish identity.[188] Assimilation took place in all areas, and during all time periods,[188] with some Jewish communities, for example the Kaifeng Jews of China, disappearing entirely.[189] The advent of the Jewish Enlightenment of the 18th century (see Haskalah) and the subsequent emancipation of the Jewish populations of Europe and America in the 19th century, accelerated the situation, encouraging Jews to increasingly participate in, and become part of, secular society. The result has been a growing trend of assimilation, as Jews marry non-Jewish spouses and stop participating in the Jewish community.[190]		Rates of interreligious marriage vary widely: In the United States, it is just under 50%,[191] in the United Kingdom, around 53%; in France; around 30%,[192] and in Australia and Mexico, as low as 10%.[193][194] In the United States, only about a third of children from intermarriages affiliate with Jewish religious practice.[195] The result is that most countries in the Diaspora have steady or slightly declining religiously Jewish populations as Jews continue to assimilate into the countries in which they live.[citation needed]		The Jewish people and Judaism have experienced various persecutions throughout Jewish history. During Late Antiquity and the Early Middle Ages the Roman Empire (in its later phases known as the Byzantine Empire) repeatedly repressed the Jewish population, first by ejecting them from their homelands during the pagan Roman era and later by officially establishing them as second-class citizens during the Christian Roman era.[196][197]		According to James Carroll, "Jews accounted for 10% of the total population of the Roman Empire. By that ratio, if other factors had not intervened, there would be 200 million Jews in the world today, instead of something like 13 million."[198]		Later in medieval Western Europe, further persecutions of Jews by Christians occurred, notably during the Crusades—when Jews all over Germany were massacred—and a series of expulsions from the Kingdom of England, Germany, France, and, in the largest expulsion of all, Spain and Portugal after the Reconquista (the Catholic Reconquest of the Iberian Peninsula), where both unbaptized Sephardic Jews and the ruling Muslim Moors were expelled.[199][200]		In the Papal States, which existed until 1870, Jews were required to live only in specified neighborhoods called ghettos.[201]		Islam and Judaism have a complex relationship. Traditionally Jews and Christians living in Muslim lands, known as dhimmis, were allowed to practice their religions and administer their internal affairs, but they were subject to certain conditions.[202] They had to pay the jizya (a per capita tax imposed on free adult non-Muslim males) to the Islamic state.[202] Dhimmis had an inferior status under Islamic rule. They had several social and legal disabilities such as prohibitions against bearing arms or giving testimony in courts in cases involving Muslims.[203] Many of the disabilities were highly symbolic. The one described by Bernard Lewis as "most degrading"[204] was the requirement of distinctive clothing, not found in the Quran or hadith but invented in early medieval Baghdad; its enforcement was highly erratic.[204] On the other hand, Jews rarely faced martyrdom or exile, or forced compulsion to change their religion, and they were mostly free in their choice of residence and profession.[205]		Notable exceptions include the massacre of Jews and forcible conversion of some Jews by the rulers of the Almohad dynasty in Al-Andalus in the 12th century,[206] as well as in Islamic Persia,[207] and the forced confinement of Moroccan Jews to walled quarters known as mellahs beginning from the 15th century and especially in the early 19th century.[208] In modern times, it has become commonplace for standard antisemitic themes to be conflated with anti-Zionist publications and pronouncements of Islamic movements such as Hezbollah and Hamas, in the pronouncements of various agencies of the Islamic Republic of Iran, and even in the newspapers and other publications of Turkish Refah Partisi."[209]		Throughout history, many rulers, empires and nations have oppressed their Jewish populations or sought to eliminate them entirely. Methods employed ranged from expulsion to outright genocide; within nations, often the threat of these extreme methods was sufficient to silence dissent. The history of antisemitism includes the First Crusade which resulted in the massacre of Jews;[199] the Spanish Inquisition (led by Tomás de Torquemada) and the Portuguese Inquisition, with their persecution and autos-da-fé against the New Christians and Marrano Jews;[210] the Bohdan Chmielnicki Cossack massacres in Ukraine;[211] the Pogroms backed by the Russian Tsars;[212] as well as expulsions from Spain, Portugal, England, France, Germany, and other countries in which the Jews had settled.[200] According to a 2008 study published in the American Journal of Human Genetics, 19.8% of the modern Iberian population has Sephardic Jewish ancestry,[213] indicating that the number of conversos may have been much higher than originally thought.[214][215]		The persecution reached a peak in Nazi Germany's Final Solution, which led to the Holocaust and the slaughter of approximately 6 million Jews.[216] Of the world's 15 million Jews in 1939, more than a third were killed in the Holocaust.[217][218] The Holocaust—the state-led systematic persecution and genocide of European Jews (and certain communities of North African Jews in European controlled North Africa) and other minority groups of Europe during World War II by Germany and its collaborators remains the most notable modern-day persecution of Jews.[219] The persecution and genocide were accomplished in stages. Legislation to remove the Jews from civil society was enacted years before the outbreak of World War II.[220] Concentration camps were established in which inmates were used as slave labour until they died of exhaustion or disease.[221] Where the Third Reich conquered new territory in Eastern Europe, specialized units called Einsatzgruppen murdered Jews and political opponents in mass shootings.[222] Jews and Roma were crammed into ghettos before being transported hundreds of miles by freight train to extermination camps where, if they survived the journey, the majority of them were killed in gas chambers.[223] Virtually every arm of Germany's bureaucracy was involved in the logistics of the mass murder, turning the country into what one Holocaust scholar has called "a genocidal nation."[224]		Throughout Jewish history, Jews have repeatedly been directly or indirectly expelled from both their original homeland, the Land of Israel, and many of the areas in which they have settled. This experience as refugees has shaped Jewish identity and religious practice in many ways, and is thus a major element of Jewish history.[225] The incomplete list of major and other noteworthy migrations that follows includes numerous instances of expulsion or departure under duress:		Israel is the only country with a Jewish population that is consistently growing through natural population growth, although the Jewish populations of other countries, in Europe and North America, have recently increased through immigration. In the Diaspora, in almost every country the Jewish population in general is either declining or steady, but Orthodox and Haredi Jewish communities, whose members often shun birth control for religious reasons, have experienced rapid population growth.[249]		Orthodox and Conservative Judaism discourage proselytism to non-Jews, but many Jewish groups have tried to reach out to the assimilated Jewish communities of the Diaspora in order for them to reconnect to their Jewish roots. Additionally, while in principle Reform Judaism favors seeking new members for the faith, this position has not translated into active proselytism, instead taking the form of an effort to reach out to non-Jewish spouses of intermarried couples.[250]		There is also a trend of Orthodox movements pursuing secular Jews in order to give them a stronger Jewish identity so there is less chance of intermarriage. As a result of the efforts by these and other Jewish groups over the past 25 years, there has been a trend (known as the Baal Teshuva movement) for secular Jews to become more religiously observant, though the demographic implications of the trend are unknown.[251] Additionally, there is also a growing rate of conversion to Jews by Choice of gentiles who make the decision to head in the direction of becoming Jews.[252]		There is no single governing body for the Jewish community, nor a single authority with responsibility for religious doctrine.[253] Instead, a variety of secular and religious institutions at the local, national, and international levels lead various parts of the Jewish community on a variety of issues.[254]		Jews have made a myriad of contributions to humanity in a broad and diverse range of fields, including the sciences, arts, politics, and business.[255] Although Jews comprise only 0.2% of the world's population, over 20%[256][257][258][259][260][261] of Nobel Prize laureates have been Jewish or of Jewish descent, with multiple winners in each category.		
Gerald Jay Sussman (born February 8, 1947) is the Panasonic Professor of Electrical Engineering at the Massachusetts Institute of Technology (MIT). He received his S.B. and Ph.D. degrees in mathematics from MIT in 1968 and 1973 respectively. He has been involved in artificial intelligence research at MIT since 1964. His research has centered on understanding the problem-solving strategies used by scientists and engineers, with the goals of automating parts of the process and formalizing it to provide more effective methods of science and engineering education. Sussman has also worked in computer languages, in computer architecture and in VLSI design.						Sussman is a coauthor (with Hal Abelson and his wife Julie Sussman) of the introductory computer science textbook Structure and Interpretation of Computer Programs. It was used at MIT for several decades, and has been translated into several languages.		Sussman's contributions to artificial intelligence include problem solving by debugging almost-right plans, propagation of constraints applied to electrical circuit analysis and synthesis, dependency-based explanation and dependency-based backtracking, and various language structures for expressing problem-solving strategies. Sussman and his former student, Guy L. Steele Jr., invented the Scheme programming language in 1975.		Sussman saw that artificial intelligence ideas can be applied to computer-aided design. Sussman developed, with his graduate students, sophisticated computer-aided design tools for VLSI. Steele made the first Scheme chips in 1978. These ideas and the AI-based CAD technology to support them were further developed in the Scheme chips of 1979 and 1981. The technique and experience developed were then used to design other special-purpose computers. Sussman was the principal designer of the Digital Orrery, a machine designed to do high-precision integrations for orbital mechanics experiments. The Orrery was designed and built by a few people in a few months, using AI-based simulation and compilation tools.		Using the Digital Orrery, Sussman has worked with Jack Wisdom to discover numerical evidence for chaotic motions in the outer planets. The Digital Orrery is now retired at the Smithsonian Institution in Washington, DC. Sussman was also the lead designer of the Supercomputer Toolkit, another multiprocessor computer optimized for evolving systems of ordinary differential equations. The Supercomputer Toolkit was used by Sussman and Wisdom to confirm and extend the discoveries made with the Digital Orrery to include the entire planetary system.		Sussman has pioneered the use of computational descriptions to communicate methodological ideas in teaching subjects in Electrical Circuits and in Signals and Systems. Over the past decade Sussman and Wisdom have developed a subject that uses computational techniques to communicate a deeper understanding of advanced classical mechanics. In Computer Science: Reflections on the Field, Reflections from the Field, he writes "...computational algorithms are used to express the methods used in the analysis of dynamical phenomena. Expressing the methods in a computer language forces them to be unambiguous and computationally effective. Students are expected to read the programs and to extend them and to write new ones. The task of formulating a method as a computer-executable program and debugging that program is a powerful exercise in the learning process. Also, once formalized procedurally, a mathematical idea becomes a tool that can be used directly to compute results." Sussman and Wisdom, with Meinhard Mayer, have produced a textbook, Structure and Interpretation of Classical Mechanics, to capture these new ideas.		Sussman and Abelson have also been a part of the Free Software Movement, including releasing MIT/GNU Scheme as free software[1] and serving on the Board of Directors of the Free Software Foundation.[2]		For his contributions to computer-science education, Sussman received the ACM's Karl Karlstrom Outstanding Educator Award in 1990, and the Amar G. Bose award for teaching in 1991.		Sussman, Hal Abelson, and Richard Stallman are the only founding directors still active on the board of directors of the Free Software Foundation (FSF).		Sussman is a fellow of the Institute of Electrical and Electronics Engineers (IEEE), a member of the National Academy of Engineering (NAE), a fellow of the Association for the Advancement of Artificial Intelligence (AAAI), a fellow of the Association for Computing Machinery (ACM), a fellow of the American Association for the Advancement of Science (AAAS), a fellow of the New York Academy of Sciences (NYAS), and a fellow of the American Academy of Arts and Sciences. He is also a bonded locksmith, a life member of the American Watchmakers-Clockmakers Institute (AWI), a member of the Massachusetts Watchmakers-Clockmakers Association (MWCA), a member of the Amateur Telescope Makers of Boston (ATMOB), and a member of the American Radio Relay League (ARRL).		
Merriam-Webster, Incorporated, is an American company that publishes reference books, especially known for its dictionaries.		In 1831, George and Charles Merriam founded the company as G & C Merriam Co. in Springfield, Massachusetts. In 1843, after Noah Webster died, the company bought the rights to An American Dictionary of the English Language from Webster's estate. All Merriam-Webster dictionaries trace their lineage to this source.		In 1964, Encyclopædia Britannica, Inc. acquired Merriam-Webster, Inc. as a subsidiary. The company adopted its current name in 1982.[1][2]						In 1806, Webster published his first dictionary, A Compendious Dictionary of the English Language. In 1807 Webster started two decades of intensive work to expand his publication into a fully comprehensive dictionary, An American Dictionary of the English Language. To help him trace the etymology of words, Webster learned 26 languages. Webster hoped to standardize American speech, since Americans in different parts of the country used somewhat different vocabularies and spelled, pronounced, and used words differently.		Webster completed his dictionary during his year abroad in 1825 in Paris, and at the University of Cambridge. His 1820s book contained 70,000 words, of which about 12,000 had never appeared in a dictionary before. As a spelling reformer, Webster believed that English spelling rules were unnecessarily complex, so his dictionary introduced American English spellings, replacing colour with color, waggon with wagon, and centre with center. He also added American words, including skunk and squash, that did not appear in British dictionaries. At the age of 70 in 1828, Webster published his dictionary; it sold poorly, with only 2,500 copies putting him in debt. However, in 1840, he published the second edition in two volumes with much greater success.		Author and poet Nathan W. Austin explores the intersection of lexicographical and poetic practices in American literature, and attempts to map out a "lexical poetics" using Webster's dictionaries as a base. He shows ways that American poetry inherited Webster's ideas and draws on his lexicography to develop the language. Austin explicates key definitions from the Compendious (1806) and American (1828) dictionaries, and expresses various concerns, including the politics of American English, the question of national identity and culture in the early moments of American independence, and the poetics of citation and definition.[3]		In 1843, after Webster's death, George Merriam and Charles Merriam secured publishing and revision rights to the 1840 edition of the dictionary. They published a revision in 1847, which did not change any of the main text but merely added new sections, and a second update with illustrations in 1859. In 1864, Merriam published a greatly expanded edition, which was the first version to change Webster's text, largely overhauling his work yet retaining many of his definitions and the title "An American Dictionary". This began a series of revisions that were described as being "unabridged" in content. In 1884 it contained 118,000 words, "3000 more than any other English dictionary".[4]		With the edition of 1890, the dictionary was retitled Webster's International. The vocabulary was vastly expanded in Webster's New International editions of 1909 and 1934, totaling over half a million words, with the 1934 edition retrospectively called Webster's Second International or simply "The Second Edition" of the New International.		The Collegiate Dictionary was introduced in 1898 and the series is now in its eleventh edition. Following the publication of Webster's International in 1890, two Collegiate editions were issued as abridgments of each of their Unabridged editions. With the ninth edition (Webster's Ninth New Collegiate Dictionary (WNNCD), published in 1983), the Collegiate adopted changes which distinguish it as a separate entity rather than merely an abridgment of the Third New International (the main text of which has remained virtually unrevised since 1961). Some proper names were returned to the word list, including names of Knights of the Round Table. The most notable change was the inclusion of the date of the first known citation of each word, to document its entry into the English language. The eleventh edition (published in 2003) includes more than 225,000 definitions, and more than 165,000 entries. A CD-ROM of the text is sometimes included. This dictionary is preferred as a source "for general matters of spelling" by the influential The Chicago Manual of Style, which is followed by many book publishers and magazines in the United States. The Chicago Manual states that it "normally opts for" the first spelling listed.[5]		Merriam overhauled the dictionary again with the 1961 Webster's Third New International under the direction of Philip B. Gove, making changes that sparked public controversy. Many of these changes were in formatting, omitting needless punctuation, or avoiding complete sentences when a phrase was sufficient. Others, more controversial, signaled a shift from linguistic prescriptivism and towards describing American English as it was used at that time.[6]		Since the 1940s, the company has added many specialized dictionaries, language aides, and other references to its repertoire.		The G. & C. Merriam Company lost its right to exclusive use of the name "Webster" after a series of lawsuits placed that name in public domain. Its name was changed to "Merriam-Webster, Incorporated" with the publication of Webster's Ninth New Collegiate Dictionary in 1983. Previous publications had used "A Merriam-Webster Dictionary" as a subtitle for many years and will be found on older editions.		The company has been a subsidiary of Encyclopædia Britannica, Inc. since 1964.		In 1996, Merriam-Webster launched its first website, which provided free access to an online dictionary and thesaurus.[7]		Merriam-Webster has also published dictionaries of synonyms, English usage, geography (Merriam-Webster's Geographical Dictionary), biography, proper names, medical terms, sports terms, slang, Spanish/English, and numerous others. Non-dictionary publications include Collegiate Thesaurus, Secretarial Handbook, Manual for Writers and Editors, Collegiate Encyclopedia, Encyclopedia of Literature, and Encyclopedia of World Religions.		On February 16, 2007, Merriam-Webster announced the launch of a mobile dictionary and thesaurus service developed with mobile search-and-information provider AskMeNow. Consumers use the service to access definitions, spelling and synonyms via text message. Services also include Merriam-Webster's Word of the Day—and Open Dictionary, a wiki service that provides subscribers the opportunity to create and submit their own new words and definitions.[8]		The Merriam-Webster company once used a unique set of phonetic symbols in their dictionaries—intended to help people from different parts of the United States learn how to pronounce words the same way as others who spoke with the same accent or dialect did. Unicode accommodated IPA symbols, but did not specify room for Merriam-Webster phonetics. Hence, to enable computerized access to the pronunciation without having to rework all dictionaries to IPA notation, the online services of Merriam-Webster specify phonetics using a less-specific set of ASCII characters.		Merriam creates entries by finding uses of a particular word in print and recording them in a database of citations.[6] Editors at Merriam spend about an hour a day looking at print sources, from books and newspapers to less formal publications, like advertisements and product packaging, to study the uses of individual words and choose things that should be preserved in the citation file. Merriam–Webster's citation file contains more than 16 million entries documenting individual uses of words. Millions of these citations are recorded on 3-by-5 cards in their paper citation files. The earliest entries in the paper citation files date back to the late 19th century. Since 2009, all new entries are recorded in an electronic database.[6]		
Acne, also known as acne vulgaris, is a long-term skin disease that occurs when hair follicles are clogged with dead skin cells and oil from the skin.[10] It is characterized by blackheads or whiteheads, pimples, oily skin, and possible scarring.[1][2][11] It primarily affects areas of the skin with a relatively high number of oil glands, including the face, upper part of the chest, and back.[12] The resulting appearance can lead to anxiety, reduced self-esteem and, in extreme cases, depression or thoughts of suicide.[3][4]		Genetics is thought to be the primary cause of acne in 80% of cases.[2] The role of diet and cigarette smoking is unclear, and neither cleanliness nor exposure to sunlight appear to play a part.[2][13][14] During puberty, in both sexes, acne is often brought on by an increase in hormones such as testosterone.[5] A frequent factor is excessive growth of the bacterium Propionibacterium acnes, which is normally present on the skin.[5]		Many treatment options for acne are available, including lifestyle changes, medications, and medical procedures. Eating fewer simple carbohydrates such as sugar may help.[7] Treatments applied directly to the affected skin, such as azelaic acid, benzoyl peroxide, and salicylic acid, are commonly used.[8] Antibiotics and retinoids are available in formulations that are applied to the skin and taken by mouth for the treatment of acne.[8] However, resistance to antibiotics may develop as a result of antibiotic therapy.[15] Several types of birth control pills help against acne in women.[8] Isotretinoin pills are usually reserved for severe acne due to greater potential side effects.[8] Early and aggressive treatment of acne is advocated by some in the medical community to decrease the overall long-term impact to individuals.[4]		In 2015, acne was estimated to affect 633 million people globally, making it the 8th most common disease worldwide.[9][16] Acne commonly occurs in adolescence and affects an estimated 80–90% of teenagers in the Western world.[17][18][19] Lower rates are reported in some rural societies.[19][20] Children and adults may also be affected before and after puberty.[21] Although acne becomes less common in adulthood, it persists in nearly half of affected people into their twenties and thirties and a smaller group continue to have difficulties into their forties.[2]		The severity of acne vulgaris (Gr. ἀκµή, "point" + L. vulgaris, "common")[22] can be classified as mild, moderate, or severe as this helps to determine an appropriate treatment regimen.[18] Mild acne is classically defined by the presence of clogged skin follicles (known as comedones) limited to the face with occasional inflammatory lesions.[18] Moderate severity acne is said to occur when a higher number of inflammatory papules and pustules occur on the face compared to mild cases of acne and are found on the trunk of the body.[18] Severe acne is said to occur when nodules (the painful 'bumps' lying under the skin) are the characteristic facial lesions and involvement of the trunk is extensive.[18][23]		Large nodules were previously referred to as cysts, and the term nodulocystic has been used in the medical literature to describe severe cases of inflammatory acne.[23] True cysts are in fact rare in those with acne and the term severe nodular acne is now the preferred terminology.[23]		Acne inversa (L. invertō, "upside down") and acne rosacea (rosa, "rose-colored" + -āceus, "forming") are not true forms of acne and respectively refer to the skin conditions hidradenitis suppurativa (HS) and rosacea.[24][25][26] Although HS shares certain common features with acne vulgaris, such as a tendency to clog skin follicles with skin cell debris, the condition otherwise lacks the defining features of acne and is therefore considered a distinct skin disorder.[24]		Typical features of acne include increased secretion of oily sebum by the skin, microcomedones, comedones, papules, nodules (large papules), pustules, and often results in scarring.[27][28] The appearance of acne varies with skin color. It may result in psychological and social problems.[18]		Acne scars are caused by inflammation within the dermal layer of skin and are estimated to affect 95% of people with acne vulgaris.[29] The scar is created by abnormal healing following this dermal inflammation.[30] Scarring is most likely to take place with severe acne, but may occur with any form of acne vulgaris.[29] Acne scars are classified based on whether the abnormal healing response following dermal inflammation leads to excess collagen deposition or loss at the site of the acne lesion.[31]		Atrophic acne scars have lost collagen from the healing response and are the most common type of acne scar (account for approximately 75% of all acne scars).[30][31] They may be further classified as ice-pick scars, boxcar scars, and rolling scars.[29] Ice-pick scars are narrow (less than 2 mm across), deep scars that extend into the dermis.[30] Boxcar scars are round or ovoid indented scars with sharp borders and vary in size from 1.5–4 mm across.[30] Rolling scars are wider than icepick and boxcar scars (4–5 mm across) and have a wave-like pattern of depth in the skin.[30]		Hypertrophic scars are uncommon, and are characterized by increased collagen content after the abnormal healing response.[30] They are described as firm and raised from the skin.[30][32] Hypertrophic scars remain within the original margins of the wound, whereas keloid scars can form scar tissue outside of these borders.[30] Keloid scars from acne occur more often in men and people with darker skin, and usually occur on the trunk of the body.[30]		Postinflammatory hyperpigmentation (PIH) is usually the result of nodular acne lesions. These lesions often leave behind an inflamed darkened mark after the original acne lesion has resolved. This inflammation stimulates specialized pigment-producing skin cells (known as melanocytes) to produce more melanin pigment which leads to the skin's darkened appearance.[33] People with darker skin color are more frequently affected by this condition.[34] Pigmented scar is a common term used for PIH, but is misleading as it suggests the color change is permanent. Often, PIH can be prevented by avoiding any aggravation of the nodule, and can fade with time. However, untreated PIH can last for months, years, or even be permanent if deeper layers of skin are affected.[35] Even minimal skin exposure to the sun's ultraviolet rays can sustain hyperpigmentation.[33] Daily use of SPF 15 or higher sunscreen can minimize such a risk.[35]		Risk factors for the development of acne, other than genetics, have not been conclusively identified. Possible secondary contributors include hormones, infections, diet and stress. Studies investigating the impact of smoking on the incidence and severity of acne have been inconclusive.[2][36][37] Sunlight and cleanliness are not associated with acne.[14]		The predisposition to acne for specific individuals is likely explained by a genetic component, a theory which is supported by studies examining the rates of acne among twins and first-degree relatives.[2] Severe acne may be associated with XYY syndrome.[38] Acne susceptibility is likely due to the influence of multiple genes, as the disease does not follow a classic (Mendelian) inheritance pattern. Multiple gene candidates have been proposed including certain variations in tumor necrosis factor-alpha (TNF-alpha), IL-1 alpha, and CYP1A1 genes, among others.[17] Increased risk is associated with the 308 G/A single nucleotide polymorphism variation in the gene for TNF.[39]		Hormonal activity, such as occurs during menstrual cycles and puberty, may contribute to the formation of acne. During puberty, an increase in sex hormones called androgens causes the skin follicle glands to grow larger and make more oily sebum.[12] Several hormones have been linked to acne, including the androgens testosterone, dihydrotestosterone (DHT), and dehydroepiandrosterone (DHEA); high levels of growth hormone (GH) and insulin-like growth factor 1 (IGF-1) have also been associated with worsened acne.[40] Both androgens and IGF-1 seem to be essential for acne to occur, as acne does not develop in individuals with complete androgen insensitivity syndrome (CAIS) or Laron syndrome (insensitivity to GH, resulting in very low IGF-1 levels).[41][42]		Medical conditions that commonly cause a high-androgen state, such as polycystic ovary syndrome, congenital adrenal hyperplasia, and androgen-secreting tumors, can cause acne in affected individuals.[43][44] Conversely, people who lack androgenic hormones or are insensitive to the effects of androgens rarely have acne.[43] An increase in androgen and oily sebum synthesis can be seen during pregnancy.[44][45] Acne can be a side effect of testosterone replacement therapy or of anabolic steroid use.[1][46] Over-the-counter bodybuilding and dietary supplements are commonly found to contain illegally added anabolic steroids.[1][47]		It is widely suspected that the anaerobic bacterial species Propionibacterium acnes (P. acnes) contributes to the development of acne, but its exact role is not clear.[2] There are specific sub-strains of P. acnes associated with normal skin, and moderate or severe inflammatory acne.[48] It is unclear whether these undesirable strains evolve on-site or are acquired, or possibly both depending on the person. These strains have the capability of changing, perpetuating, or adapting to the abnormal cycle of inflammation, oil production, and inadequate sloughing of dead skin cells from acne pores. Infection with the parasitic mite Demodex is associated with the development of acne.[28][49] It is unclear whether eradication of the mite improves acne.[49]		The relationship between diet and acne is unclear, as there is no high-quality evidence which establishes any definitive link.[50] High-glycemic-load diets have been found to have different degrees of effect on acne severity.[7][51][52] Multiple randomized controlled trials and nonrandomized studies have found a lower-glycemic-load diet to be effective in reducing acne.[51] There is weak observational evidence suggesting that dairy milk consumption is positively associated with a higher frequency and severity of acne.[49][50][51][53][54] Milk contains whey protein and hormones such as bovine IGF-1 and precursors of dihydrotestosterone.[51] These components are hypothesized to promote the effects of insulin and IGF-1 and thereby increase the production of androgen hormones, sebum, and promote the formation of comedones.[51] Effects from other potentially contributing dietary factors, such as consumption of chocolate or salt, are not supported by the evidence.[50][53] Chocolate does contain varying amounts of sugar, which can lead to a high glycemic load, and it can be made with or without milk. Few studies have examined the relationship between obesity and acne.[2] Vitamin B12 may trigger skin outbreaks similar to acne (acneiform eruptions), or worsen existing acne, when taken in doses exceeding the recommended daily intake.[55]		Few high-quality studies have been performed which demonstrate that stress causes or worsens acne.[56] While the connection between acne and stress has been debated, some research indicates that increased acne severity is associated with high stress levels in certain contexts (e.g., in association with hormonal changes seen in premenstrual syndrome).[57][58]		Mechanical obstruction of skin follicles with helmets or chinstraps can worsen pre-existing acne.[59]		Several medications can worsen pre-existing acne, with examples being lithium, hydantoin, isoniazid, glucocorticoids, iodides, bromides, and testosterone.[38]		Acne vulgaris is a chronic skin disease of the pilosebaceous unit and develops due to blockages in the skin's hair follicles. These blockages are thought to occur as a result of the following four abnormal processes: a higher than normal amount of oily sebum production (influenced by androgens), excessive deposition of the protein keratin leading to comedo formation, colonization of the follicle by Propionibacterium acnes (P. acnes) bacteria, and the local release of pro-inflammatory chemicals in the skin.[48]		The earliest pathologic change is the formation of a plug (a microcomedone), which is driven primarily by excessive growth, reproduction, and accumulation of skin cells in the hair follicle.[1] In normal skin, the skin cells that have died come up to the surface and exit the pore of the hair follicle.[10] However, increased production of oily sebum in those with acne causes the dead skin cells to stick together.[10] The accumulation of dead skin cell debris and oily sebum blocks the pore of the hair follicle, thus forming the microcomedone.[10] This is further exacerbated by the biofilm created by P. acnes within the hair follicle.[43] If the microcomedone is superficial within the hair follicle, the skin pigment melanin is exposed to air, resulting in its oxidation and dark appearance (known as a blackhead or open comedo).[1][10][18] In contrast, if the microcomedone occurs deep within the hair follicle, this causes the formation of a whitehead (known as a closed comedo).[1][10]		The main hormonal driver of oily sebum production in the skin is dihydrotestosterone.[1] Another androgenic hormone responsible for increased sebaceous gland activity is DHEA-S. Higher amounts of DHEA-S are secreted during adrenarche (a stage of puberty), and this leads to an increase in sebum production. In a sebum-rich skin environment, the naturally occurring and largely commensal skin bacterium P. acnes readily grows and can cause inflammation within and around the follicle due to activation of the innate immune system.[10] P. acnes triggers skin inflammation in acne by increasing the production of several pro-inflammatory chemical signals (such as IL-1α, IL-8, TNF-α, and LTB4); IL-1α is known to be essential to comedo formation.[43]		A major mechanism of acne-related skin inflammation is mediated by P. acnes's ability to bind and activate a class of immune system receptors known as toll-like receptors (TLRs), especially TLR2 and TLR4.[43][60][61] Activation of TLR2 and TLR4 by P. acnes leads to increased secretion of IL-1α, IL-8, and TNF-α.[43] Release of these inflammatory signals attracts various immune cells to the hair follicle including neutrophils, macrophages, and Th1 cells.[43] IL-1α stimulates increased skin cell activity and reproduction, which in turn fuels comedo development.[43] Furthermore, sebaceous gland cells produce more antimicrobial peptides, such as HBD1 and HBD2, in response to binding of TLR2 and TLR4.[43]		P. acnes also provokes skin inflammation by altering the fatty composition of oily sebum.[43] Oxidation of the lipid squalene by P. acnes is of particular importance. Squalene oxidation activates NF-κB (a protein complex) and consequently increases IL-1α levels.[43] Additionally, squalene oxidation leads to increased activity of the 5-lipoxygenase enzyme responsible for conversion of arachidonic acid to leukotriene B4 (LTB4).[43] LTB4 promotes skin inflammation by acting on the peroxisome proliferator-activated receptor alpha (PPARα) protein.[43] PPARα increases activity of activator protein 1 (AP-1) and NF-κB, thereby leading to the recruitment of inflammatory T cells.[43] The inflammatory properties of P. acnes can be further explained by the bacterium's ability to convert sebum triglycerides to pro-inflammatory free fatty acids via secretion of the enzyme lipase.[43] These free fatty acids spur production of cathelicidin, HBD1, and HBD2, thus leading to further inflammation.[43]		This inflammatory cascade typically leads to the formation of inflammatory acne lesions, including papules, infected pustules, or nodules.[1] If the inflammatory reaction is severe, the follicle can break into the deeper layers of the dermis and subcutaneous tissue and cause the formation of deep nodules.[1][62][63] Involvement of AP-1 in the aforementioned inflammatory cascade leads to activation of matrix metalloproteinases, which contribute to local tissue destruction and scar formation.[43]		Comedones (blackheads and whiteheads) must be present to diagnose acne. In their absence, an appearance similar to that of acne would suggest a different skin disorder.[26] Microcomedones (the precursor to blackheads and whiteheads) are not visible to the naked eye when inspecting the skin and can only be seen with a microscope.[26] There are many features that may indicate a person's acne vulgaris is sensitive to hormonal influences. Historical and physical clues that may suggest hormone-sensitive acne include onset between ages 20 and 30; worsening the week before a woman's menstrual cycle; acne lesions predominantly over the jawline and chin; and inflammatory/nodular acne lesions.[1]		Several scales exist to grade the severity of acne vulgaris, but no single technique has been universally accepted as the diagnostic standard.[64][65] Cook's acne grading scale uses photographs to grade severity from 0 to 8 (0 being the least severe and 8 being the most severe). This scale was the first to use a standardized photographic protocol to assess acne severity; since its creation in 1979, the scale has undergone several revisions.[65] The Leeds acne grading technique counts acne lesions on the face, back, and chest and categorizes them as inflammatory or non-inflammatory. Leeds scores range from 0 (least severe) to 10 (most severe) though modified scales have a maximum score of 12.[65][66] The Pillsbury acne grading scale simply classifies the severity of the acne from 1 (least severe) to 4 (most severe).[64][67]		Many skin conditions can mimic acne vulgaris and are collectively known as acneiform eruptions.[26] Such conditions include angiofibromas, epidermal cysts, flat warts, folliculitis, keratosis pilaris, milia, perioral dermatitis, and rosacea, among others.[18][68] Age is one factor which may help distinguish between these disorders. Skin disorders such as perioral dermatitis and keratosis pilaris can appear similar to acne but tend to occur more frequently in childhood, whereas rosacea tends to occur more frequently in older adults.[18] Facial redness triggered by heat or the consumption of alcohol or spicy food is suggestive of rosacea.[69] The presence of comedones helps health professionals differentiate acne from skin disorders that are similar in appearance.[8] Chloracne, due to exposure to certain chemicals, may look very similar to acne vulgaris.[70]		Many different treatments exist for acne. These include alpha hydroxy acid, anti-androgen medications, antibiotics, antiseborrheic medications, azelaic acid, benzoyl peroxide, hormonal treatments, keratolytic soaps, nicotinamide, retinoids, and salicylic acid.[71] They are believed to work in at least four different ways, including the following: reducing inflammation, hormonal manipulation, killing P. acnes, and normalizing skin cell shedding and sebum production in the pore to prevent blockage.[72] Common treatments include topical therapies such as antibiotics, benzoyl peroxide, and retinoids, and systemic therapies including antibiotics, hormonal agents, and oral retinoids.[18][73]		Recommended therapies for first-line use in acne vulgaris treatment include topical retinoids, benzoyl peroxide, and topical or oral antibiotics.[74] Procedures such as light therapy and laser therapy are not considered to be first-line treatments and typically have an adjunctive role due to their high cost and limited evidence of efficacy.[73] Medications for acne work by targeting the early stages of comedo formation and are generally ineffective for visible skin lesions; improvement in the appearance of acne is typically expected between six and eight weeks after starting therapy.[1]		A diet low in simple sugars is recommended as a method of improving acne.[51] As of 2014, evidence is insufficient to recommend milk restriction for this purpose.[51]		Benzoyl peroxide (BPO) is a first-line treatment for mild and moderate acne due to its effectiveness and mild side-effects (mainly skin irritation). In the skin follicle, benzoyl peroxide kills P. acnes by oxidizing its proteins through the formation of oxygen free radicals and benzoic acid. These free radicals are thought to interfere with the bacterium's metabolism and ability to make proteins.[75][76] Additionally, benzoyl peroxide is mildly effective at breaking down comedones and inhibiting inflammation.[74][76] Benzoyl peroxide may be paired with a topical antibiotic or retinoid such as benzoyl peroxide/clindamycin and benzoyl peroxide/adapalene, respectively.[34]		Side effects include increased skin photosensitivity, dryness, redness and occasional peeling.[77] Sunscreen use is often advised during treatment, to prevent sunburn. Lower concentrations of benzoyl peroxide are just as effective as higher concentrations in treating acne but are associated with fewer side effects.[76][78] Unlike antibiotics, benzoyl peroxide does not appear to generate bacterial antibiotic resistance.[77]		Retinoids are medications which reduce inflammation, normalize the follicle cell life cycle, and reduce sebum production.[43][79] They are structurally related to vitamin A.[79] The retinoids appear to influence the cell life cycle in the follicle lining. This helps prevent the accumulation of skin cells within the hair follicle that can create a blockage. They are a first-line acne treatment,[1] especially for people with dark-colored skin, and are known to lead to faster improvement of postinflammatory hyperpigmentation.[34]		Frequently used topical retinoids include adapalene, isotretinoin, retinol, tazarotene, and tretinoin.[45] They often cause an initial flare-up of acne and facial flushing, and can cause significant skin irritation. Generally speaking, retinoids increase the skin's sensitivity to sunlight and are therefore recommended for use at night.[1] Tretinoin is the least expensive of the topical retinoids and is the most irritating to the skin, whereas adapalene is the least irritating to the skin but costs significantly more.[1][80] Tazarotene is the most effective and expensive topical retinoid, but is not as well-tolerated.[1][80] Retinol is a form of vitamin A that has similar but milder effects, and is used in many over-the-counter moisturizers and other topical products.		Isotretinoin is an oral retinoid that is very effective for severe nodular acne, and moderate acne that is stubborn to other treatments.[1][18] One to two months use is typically adequate to see improvement. Acne often resolves completely or is much milder after a 4–6 month course of oral isotretinoin.[1] After a single course, about 80% of people report an improvement, with more than 50% reporting complete remission.[18] About 20% of patients require a second course.[18] Although concerns have emerged that isotretinoin use is linked with an increased risk of psychiatric side effects, such as depression and suicidality, there is no clear evidence to support these claims.[1][18] Isotretinoin use in women of childbearing age is regulated due to its known harmful effects in pregnancy.[18] For such a woman to be considered a candidate for isotretinoin, she must have a confirmed negative pregnancy test and use an effective form of birth control.[18] In 2008, the United States started the iPLEDGE program to prevent isotretinoin use during pregnancy.[81] iPledge requires the woman under consideration for isotretinoin therapy to have two negative pregnancy tests and mandates the use of two types of birth control for at least one month before therapy begins and one month after therapy is complete.[81] The effectiveness of the iPledge program has been questioned due to continued instances of contraception nonadherence.[81][82]		Antibiotics are frequently applied to the skin or taken orally to treat acne and are thought to work due to their antimicrobial activity against P. acnes and their ability to reduce inflammation.[18][77][83] With the widespread use of antibiotics for acne and an increased frequency of antibiotic-resistant P. acnes worldwide, antibiotics are becoming less effective,[77] especially macrolide antibiotics such as topical erythromycin.[15][83] Commonly used antibiotics, either applied to the skin or taken orally, include clindamycin, erythromycin, metronidazole, sulfacetamide, and tetracyclines such as doxycycline and minocycline.[45] When antibiotics are applied to the skin, they are typically used for mild to moderately severe acne.[18] Antibiotics taken orally are generally considered to be more effective than topical antibiotics, and produce faster resolution of inflammatory acne lesions than topical applications.[1] Topical and oral antibiotics are not recommended for use together.[83]		Oral antibiotics are recommended for no longer than three months as antibiotic courses exceeding this duration are associated with the development of antibiotic resistance and show no clear benefit over shorter courses.[83] Furthermore, if long-term oral antibiotics beyond three months are thought to be necessary, it is recommended that benzoyl peroxide and/or a retinoid be used at the same time to limit the risk of P. acnes developing antibiotic resistance.[83] Dapsone is not a first-line topical antibiotic due to higher cost and lack of clear superiority over other antibiotics.[1] Topical dapsone is not recommended for use with benzoyl peroxide due to yellow-orange skin discoloration with this combination.[10]		In women, acne can be improved with the use of any combined birth control pill.[84] These decrease the production of androgen hormones by the ovaries, resulting in lower skin production of sebum, and consequently reduce acne severity.[10] Combinations containing third- or fourth-generation progestins such as desogestrel, drospirenone, or norgestimate may be more beneficial.[84] A 2014 review found that oral antibiotics appear to be somewhat more effective than birth control pills at decreasing the number of inflammatory acne lesions at three months.[85] However, the two therapies are approximately equal in efficacy at six months for decreasing the number of inflammatory, non-inflammatory, and total acne lesions.[85] The authors of the analysis suggested that birth control pills may be a preferred first-line acne treatment, over oral antibiotics, in certain women due to similar efficacy at six months and a lack of associated antibiotic resistance.[85]		Antiandrogens such as cyproterone acetate and spironolactone have been used successfully to treat acne, especially in women with signs of excessive androgen production such as increased hairiness or skin production of sebum, or baldness.[10][45] Spironolactone is an effective treatment for acne in adult women, but unlike combination oral contraceptives, is not approved by the United States Food and Drug Administration for this purpose.[1][34] The drug is primarily used as an aldosterone antagonist and is thought to be a useful acne treatment due to its ability to block the androgen receptor at higher doses.[34] It may be used with or without an oral contraceptive.[34] Hormonal therapies should not be used to treat acne during pregnancy or lactation as they have been associated with birth disorders such as hypospadias, and feminization of the male fetus or infant.[45] Finasteride is likely an effective treatment for acne.[1]		Azelaic acid has been shown to be effective for mild to moderate acne when applied topically at a 20% concentration.[62][86] Treatment twice daily for six months is necessary, and is as effective as topical benzoyl peroxide 5%, isotretinoin 0.05%, and erythromycin 2%.[87] Azelaic acid is thought to be an effective acne treatment due to its ability to reduce skin cell accumulation in the follicle, and its antibacterial and anti-inflammatory properties.[62] It has a slight skin-lightening effect due to its ability to inhibit melanin synthesis, and is therefore useful in treating of individuals with acne who are also affected by postinflammatory hyperpigmentation.[1] Azelaic acid may cause skin irritation but is otherwise very safe.[88] It is less effective and more expensive than retinoids.[1]		Salicylic acid is a topically applied beta-hydroxy acid that stops bacteria from reproducing and has keratolytic properties.[89][90] It opens obstructed skin pores and promotes shedding of epithelial skin cells.[89] Salicylic acid is known to be less effective than retinoid therapy.[18] Dry skin is the most commonly seen side effect with topical application, though darkening of the skin has been observed in individuals with darker skin types.[1]		Topical and oral preparations of nicotinamide (the amide form of vitamin B3) have been suggested as alternative medical treatments.[91] It is thought to improve acne due to its anti-inflammatory properties, and its ability to suppress sebum production, and promote wound healing.[91] Topical and oral preparations of zinc have similarly been proposed as effective treatments for acne; evidence to support their use for this purpose is limited.[92] The purported efficacy of zinc is attributed to its capacity to reduce inflammation and sebum production, and inhibit P. acnes.[92] Antihistamines may improve symptoms among those already taking isotretinoin due to their anti-inflammatory properties and their ability to suppress sebum production.[93]		Hydroquinone lightens the skin when applied topically by inhibiting tyrosinase, the enzyme responsible for converting the amino acid tyrosine to the skin pigment melanin, and is used to treat acne-associated postinflammatory hyperpigmentation.[33] By interfering with new production of melanin in the epidermis, hydroquinone leads to less hyperpigmentation as darkened skin cells are naturally shed over time.[33] Improvement in skin hyperpigmentation is typically seen within six months when used twice daily. Hydroquinone is ineffective for hyperpigmentation affecting deeper layers of skin such as the dermis.[33] The use of a sunscreen with SPF 15 or higher in the morning with reapplication every two hours is recommended when using hydroquinone.[33] Its application only to affected areas lowers the risk of lightening the color of normal skin but can lead to a temporary ring of lightened skin around the hyperpigmented area.[33] Hydroquinone is generally well-tolerated; side effects are typically mild (e.g., skin irritation) and occur with use of a higher than the recommended 4% concentration.[33] Most preparations contain the preservative sodium metabisulfite, which has been linked to rare cases of allergic reactions including anaphylaxis and severe asthma exacerbations in susceptible people.[33] In extremely rare cases, repeated improper topical application of high-dose hydroquinone has been associated with an accumulation of homogentisic acid in connective tissues, a condition known as exogenous ochronosis.[33]		Combination therapy—using medications of different classes together, each with a different mechanism of action—has been demonstrated to be a more efficacious approach to acne treatment than monotherapy.[10][45] The use of topical benzoyl peroxide and antibiotics together has been shown to be more effective than antibiotics alone.[10] Similarly, using a topical retinoid with an antibiotic clears acne lesions faster than the use of antibiotics alone.[10] Frequently used combinations include the following: antibiotic and benzoyl peroxide, antibiotic and topical retinoid, or topical retinoid and benzoyl peroxide.[45] The pairing of benzoyl peroxide with a retinoid is preferred over the combination of a topical antibiotic with a retinoid since both regimens are effective but benzoyl peroxide does not lead to antibiotic resistance.[10]		Although the late stages of pregnancy are associated with an increase in sebaceous gland activity in the skin, pregnancy has not been reliably associated with worsened acne severity.[94] In general, topically applied medications are considered the first-line approach to acne treatment during pregnancy, as they have little systemic absorption and are therefore unlikely to harm a developing fetus.[94] Highly recommended therapies include topically applied benzoyl peroxide (category C) and azelaic acid (category B).[94] Salicylic acid carries a category C safety rating due to higher systemic absorption (9–25%), and an association between the use of anti-inflammatory medications in the third trimester and adverse effects to the developing fetus including too little amniotic fluid in the uterus and early closure of the babies' ductus arteriosus blood vessel.[45][94] Prolonged use of salicylic acid over significant areas of the skin or under occlusive dressings is not recommended as these methods increase systemic absorption and the potential for fetal harm.[94] Tretinoin (category C) and adapalene (category C) are very poorly absorbed, but certain studies have suggested teratogenic effects in the first trimester.[94] Due to persistent safety concerns, topical retinoids are not recommended for use during pregnancy.[95] In studies examining the effects of topical retinoids during pregnancy, fetal harm has not been seen in the second and third trimesters.[94] Retinoids contraindicated for use during pregnancy include the topical retinoid tazarotene, and oral retinoids isotretinoin and acitretin (all category X).[94] Spironolactone is relatively contraindicated for use during pregnancy due to its antiandrogen effects.[1] Finasteride is not recommended as it is highly teratogenic.[1]		Topical antibiotics deemed safe during pregnancy include clindamycin, erythromycin, and metronidazole (all category B), due to negligible systemic absorption.[45][94] Nadifloxacin and dapsone (category C) are other topical antibiotics that may be used to treat acne in pregnant women, but have received less study.[45][94] No adverse fetal events have been reported from the topical use of dapsone.[94] If retinoids are used there is a high risk of abnormalities occurring in the developing fetus; women of childbearing age are therefore required to use effective birth control if retinoids are used to treat acne.[18] Oral antibiotics deemed safe for pregnancy (all category B) include azithromycin, cephalosporins, and penicillins.[94] Tetracyclines (category D) are contraindicated during pregnancy as they are known to deposit in developing fetal teeth, resulting in yellow discoloration and thinned tooth enamel.[1][94] Their use during pregnancy has been associated with development of acute fatty liver of pregnancy and is further avoided for this reason.[94]		Comedo extraction is supported by limited evidence but is recommended for comedones that do not improve with standard treatment.[8][74] Another procedure for immediate relief is injection of a corticosteroid into an inflamed acne comedo.[74] Electrocautery and electrofulguration have also been reported as effective treatments for comedones.[96][page needed]		Light therapy is a treatment method that involves delivering certain specific wavelengths of light to an area of skin affected by acne. Both regular and laser light have been used. When regular light is used immediately following the application of a sensitizing substance to the skin such as aminolevulinic acid or methyl aminolevulinate, the treatment is referred to as photodynamic therapy (PDT).[10][86] PDT has the most supporting evidence of all light therapies.[74] Many different types of nonablative lasers (i.e., lasers that do not vaporize the top layer of the skin but rather induce a physiologic response in the skin from the light) have been used to treat acne, including those that use infrared wavelengths of light. Ablative lasers (such as CO2 and fractional types) have also been used to treat active acne and its scars. When ablative lasers are used, the treatment is often referred to as laser resurfacing because, as mentioned previously, the entire upper layers of the skin are vaporized.[97] Ablative lasers are associated with higher rates of adverse effects compared with nonablative lasers, with examples being postinflammatory hyperpigmentation, persistent facial redness, and persistent pain.[8][98][99] Physiologically, certain wavelengths of light, used with or without accompanying topical chemicals, are thought to kill bacteria and decrease the size and activity of the glands that produce sebum.[86] As of 2012, evidence for various light therapies was insufficient to recommend them for routine use.[8] Disadvantages of light therapy can include its cost, the need for multiple visits, time required to complete the procedure(s), and pain associated with some of the treatment modalities.[10] Various light therapies appear to provide a short-term benefit, but data for long-term outcomes, and for outcomes in those with severe acne, are sparse;[72][100] it may have a role for individuals whose acne has been resistant to topical medications.[10] Typical side effects include skin peeling, temporary reddening of the skin, swelling, and postinflammatory hyperpigmentation.[10]		Dermabrasion is an effective therapeutic procedure for reducing the appearance of superficial atrophic scars of the boxcar and rolling varieties.[30] Ice-pick scars do not respond well to treatment with dermabrasion due to their depth.[30] The procedure is painful and has many potential side effects such as skin sensitivity to sunlight, redness, and decreased pigmentation of the skin.[30] Dermabrasion has fallen out of favor with the introduction of laser resurfacing.[30] Unlike dermabrasion, there is no evidence that microdermabrasion is an effective treatment for acne.[8]		Microneedling is a procedure in which an instrument with multiple rows of tiny needles is rolled over the skin to elicit a wound healing response and stimulate collagen production to reduce the appearance of atrophic acne scars in people with darker skin color.[97] Notable adverse effects of microneedling include postinflammatory hyperpigmentation and tram track scarring (described as discrete slightly raised scars in a linear distribution similar to a tram track). The latter is thought to be primarily attributable to improper technique by the practitioner, including the use of excessive pressure or inappropriately large needles.[97][101]		Subcision is useful for treatment of superficial atrophic acne scars and involves the use of a small needle to loosen the fibrotic adhesions that result in the depressed appearance of the scar.[102][103][104]		Chemical peels can be used to reduce the appearance of acne scars.[30] Mild peels include those using glycolic acid, lactic acid, salicylic acid, Jessner's solution, or a lower concentrations (20%) of trichloroacetic acid. These peels only affect the epidermal layer of the skin and can be useful in the treatment of superficial acne scars as well as skin pigmentation changes from inflammatory acne.[30] Higher concentrations of trichloroacetic acid (30–40%) are considered to be medium-strength peels and affect skin as deep as the papillary dermis.[30] Formulations of trichloroacetic acid concentrated to 50% or more are considered to be deep chemical peels.[30] Medium-strength and deep-strength chemical peels are more effective for deeper atrophic scars, but are more likely to cause side effects such as skin pigmentation changes, infection, and small white superficial cysts known as milia.[30]		Complementary therapies have been investigated for treating people with acne.[105] Low-quality evidence suggests topical application of tea tree oil or bee venom may reduce the total number of skin lesions in those with acne.[105] Tea tree oil is thought to be approximately as effective as benzoyl peroxide or salicylic acid, but has been associated with allergic contact dermatitis.[1] Proposed mechanisms for tea tree oil's anti-acne effects include antibacterial action against P. acnes, and anti-inflammatory properties.[61] Numerous other plant-derived therapies have been observed to have positive effects against acne (e.g., basil oil and oligosaccharides from seaweed); however, few studies have been performed, and most have been of lower methodological quality.[106] There is a lack of high-quality evidence for the use of acupuncture, herbal medicine, or cupping therapy for acne.[105]		Many over-the-counter treatments in many forms are available, which are often referred to as cosmeceuticals.[107] Certain types of makeup may be useful to mask acne.[108] In those with oily skin, a water-based product is often preferred.[108][109]		Acne usually improves around the age of 20, but may persist into adulthood.[71] Permanent physical scarring may occur.[18] There is good evidence to support the idea that acne and associated scarring negatively impact a person's psychological state, worsen mood, lower self-esteem, and are associated with a higher risk of anxiety disorders, depression, and suicidal thoughts.[3][29][49] Another psychological complication of acne vulgaris is acne excoriée, which occurs when a person persistently picks and scratches pimples, irrespective of the severity of their acne.[57][110] This can lead to significant scarring, changes in the affected person's skin pigmentation, and a cyclic worsening of the affected person's anxiety about their appearance.[57] Rare complications from acne or its treatment include the formation of pyogenic granulomas, osteoma cutis, and solid facial edema.[111] Early and aggressive treatment of acne is advocated by some in the medical community to reduce the chances of these poor outcomes.[4]		Globally, acne affects approximately 650 million people, or about 9.4% of the population, as of 2010.[112] It affects nearly 90% of people in Western societies during their teenage years, but can occur before adolescence and may persist into adulthood.[17][18][21] While acne that first develops between the ages of 21 and 25 is uncommon, it affects 54% of women and 40% of men older than 25 years of age,[45][113] and has a lifetime prevalence of 85%.[45] About 20% of those affected have moderate or severe cases.[2] It is slightly more common in females than males (9.8% versus 9.0%).[112] In those over 40 years old, 1% of males and 5% of females still have problems.[18]		Rates appear to be lower in rural societies.[20] While some research has found it affects people of all ethnic groups,[114] acne may not occur in the non-Westernized peoples of Papua New Guinea and Paraguay.[115]		Acne affects 40–50 million people in the United States (16%) and approximately 3–5 million in Australia (23%).[85][116] Severe acne tends to be more common in people of Caucasian or Hispanic descent than in people of African descent.[19]		Pharaohs are recorded as having had acne, which may be the earliest known reference to the disease. Since at least the reign of Cleopatra (69–30 BC), the application of sulfur to the skin has been recognized as a useful treatment for acne.[117] The sixth-century Greek physician Aëtius of Amida is credited with coining the term "ionthos" (ίονθωξ,) or "acnae", which is believed to have been a reference to facial skin lesions that occur during "the 'acme' of life" (puberty).[118]		In the 16th century, the French physician and botanist François Boissier de Sauvages de Lacroix provided one of the earlier descriptions of acne. He used the term "psydracia achne" to describe small, red and hard tubercles that altered a person's facial appearance during adolescence, and were neither itchy nor painful.[118]		The recognition and characterization of acne progressed in 1776 when Josef Plenck (an Austrian physician) published a book that proposed the novel concept of classifying skin diseases by their elementary (initial) lesions.[118] In 1808 the English dermatologist Robert Willan refined Plenck's work by providing the first detailed descriptions of several skin disorders using a morphologic terminology that remains in use today.[118] Thomas Bateman continued and expanded on Robert Willan's work as his student and provided the first descriptions and illustrations of acne accepted as accurate by modern dermatologists.[118] Erasmus Wilson, in 1842, was the first to make the distinction between acne vulgaris and rosacea.[96][page needed] The first professional medical monograph dedicated entirely to acne was published in New York in 1885.[119][120]		Scientists initially hypothesized that acne represented a disease of the skin's hair follicle, and occurred due to blockage of the pore by sebum. During the 1880s, bacteria were observed by microscopy in skin samples affected by acne and were regarded as the causal agents of comedones, sebum production, and ultimately acne.[118] During the mid-twentieth century, dermatologists realized that no single hypothesized factor (sebum, bacteria, or excess keratin) could completely explain the disease.[118] This led to the current understanding that acne could be explained by a sequence of related events, beginning with blockage of the skin follicle by excessive dead skin cells, followed by bacterial invasion of the hair follicle pore, changes in sebum production, and inflammation.[118]		The approach to acne treatment underwent significant changes during the twentieth century. Retinoids were introduced as a medical treatment for acne in 1943.[79] Benzoyl peroxide was first proposed as a treatment in 1958 and has been routinely used for this purpose since the 1960s.[121] Acne treatment was modified in the 1950s with the introduction of oral tetracycline antibiotics (such as minocycline). These reinforced the idea amongst dermatologists that bacterial growth on the skin plays an important role in causing acne.[118] Subsequently, in the 1970s tretinoin (original trade name Retin A) was found to be an effective treatment.[122] The development of oral isotretinoin (sold as Accutane and Roaccutane) followed in 1980.[123] After its introduction in the United States it was recognized as a medication highly likely to cause birth defects if taken during pregnancy. In the United States, more than 2,000 women became pregnant while taking isotretinoin between 1982 and 2003, with most pregnancies ending in abortion or miscarriage. About 160 babies were born with birth defects.[124][125]		Treatment of acne with topical crushed dry ice (termed "cryoslush") was first described in 1907, but is no longer performed commonly.[126] Prior to 1960, the use of X-rays was also a common treatment.[127][128]		The costs and social impact of acne are substantial. In the United States, acne vulgaris is responsible for more than 5 million doctor visits and costs over US$2.5 billion each year in direct costs.[13] Similarly, acne vulgaris is responsible for 3.5 million doctor visits each year in the United Kingdom.[18] Sales for the top ten leading acne treatment brands in the US in 2015, have been reported as amounting to $352 million.[129]		Misperceptions about acne's causative and aggravating factors are common, and those affected by it are often blamed for their condition.[130] Such blame can worsen the affected person's sense of self-esteem.[130] Until the 20th century, even among dermatologists, the list of causes was believed to include excessive sexual thoughts and masturbation.[119] Dermatology's association with sexually transmitted infections, especially syphilis, contributed to the stigma.[119]		Acne vulgaris and its resultant scars have been associated with significant social and academic difficulties that can last into adulthood, including difficulties obtaining employment.[29][131] Until the 1930s, it was largely seen as a trivial problem among middle-class girls – a trivial problem, because, unlike smallpox and tuberculosis, no one died from it, and a feminine problem, because boys were much less likely to seek medical assistance for it.[119] During the Great Depression, dermatologists discovered that young men with acne had difficulty obtaining jobs, and during World War II, some soldiers in tropical climates developed such severe and widespread tropical acne on their bodies that they were declared medically unfit for duty.[119]		Many celebrities are known to have had acne, with examples being Kelly Clarkson, P. Diddy, Avril Lavigne, Lindsay Lohan, Alyssa Milano, Katy Perry, Jessica Simpson, Britney Spears and Vanessa Williams, all of whom have admitted to suffering from the condition.[132]		Efforts to better understand the mechanisms of sebum production are underway. The aim of this research is to develop medications that target and interfere with the hormones that are known to increase sebum production (e.g., IGF-1 and alpha-melanocyte-stimulating hormone).[10] Additional sebum-lowering medications being researched include topical antiandrogens and peroxisome proliferator-activated receptor modulators.[10] Another avenue of early-stage research has focused on how to best use laser and light therapy to selectively destroy sebum-producing glands in the skin's hair follicles in order to reduce sebum production and improve acne appearance.[10]		The use of antimicrobial peptides against P. acnes is under investigation as a treatment for acne to overcoming antibiotic resistance.[10] In 2007, the first genome sequencing of a P. acnes bacteriophage (PA6) was reported. The authors proposed applying this research toward development of bacteriophage therapy as an acne treatment in order to overcome the problems associated with long-term antibiotic therapy such as bacterial resistance.[133] Oral and topical probiotics are also being evaluated as treatments for acne.[134] Probiotics have been hypothesized to have therapeutic effects for those affected by acne due to their ability to decrease skin inflammation and improve skin moisture by increasing the skin's ceramide content.[134] As of 2014, studies examining the effects of probiotics on acne in humans were limited.[134]		Decreased levels of retinoic acid in the skin may contribute to comedo formation. To address this deficiency, methods to increase the skin's production of retinoid acid are being explored.[10] A vaccine against inflammatory acne has shown promising results in mice and humans.[48][135] Some have voiced concerns about creating a vaccine designed to neutralize a stable community of normal skin bacteria that is known to protect the skin from colonization by more harmful microorganisms.[136]		Acne can occur on cats,[137] dogs,[138] and horses.[139][140]								
English Albanian  · Arabic  · American Sign Language  · Neo-Aramaic  · Armenian  · Azerbaijani  · Belarusian  · Czech  · Danish  · Dutch  · Finnish  · French  · German  · Greek  · Hebrew  · Hungarian  · Italian  · Kurdish  · Ladino  · Lithuanian  · Norwegian  · Pashto  · Persian  · Polish  · Portuguese  · Romanian  · Russian  · Slovak  · South Slavic  · Spanish  · Swedish  · Tamazight  · Turkish  · Ukrainian  · Yiddish		White Americans are Americans who are considered or reported as White. The United States Census Bureau defines White people as those "having origins in any of the original peoples of Europe, the Middle East-North Africa."[2] Like all official U.S. racial categories, "White" has a "not Hispanic or Latino" and a "Hispanic or Latino" component,[3] the latter consisting mostly of White Mexican Americans and White Cuban Americans. The term "Caucasian" is often used interchangeably with "White", although the terms are not synonymous.[4][5][6][7][8][9]		The largest ancestries of American Whites are: German Americans (16.5%), Irish Americans (11.9%), English Americans (9.2%), Italian Americans (5.8%), French Americans (4%), Polish Americans (3%), Scottish Americans (1.9%), Scotch-Irish Americans (1.7%), Dutch Americans (1.6%), Norwegian Americans (1.5%), and Swedish Americans (1.4%).[10][11][12] However, the English-Americans and British-Americans demography is considered a serious under-count as the stock tend to self-report and identify as simply "Americans" (6.9%), due to the length of time they have inhabited America.[6][7][8][9]		Whites (including Hispanics who identify as White) constitute the majority, with a total of about 246,660,710, or 77.35% of the population as of 2014. Non-Hispanic Whites totaled about 197,870,516, or 62.06% of the U.S. population.						Definitions of who is "White" have changed throughout the history of the United States.		The term "White American" can encompass many different ethnic groups. Although the United States Census purports to reflect a social definition of race, the social dimensions of race are more complex than Census criteria. The 2000 U.S. census states that racial categories "generally reflect a social definition of race recognized in this country. They do not conform to any biological, anthropological or genetic criteria."[13]		The Census question on race lists the categories White or European American, Black or African American, American Indian and Alaska Native, Native Hawaiian or Other Pacific Islander, Asian, plus "Some other race", with the respondent having the ability to mark more than one racial and\or ethnic category. The Census Bureau defines White people as follows:		"White" refers to a person having origins in any of the original peoples of Europe, the Middle East or North Africa. It includes people who indicated their race(s) as "White" or reported entries such as Irish, German, Italian, Lebanese, Arab, Moroccan or Caucasian.[2]		In U.S. census documents, the designation White overlaps, as do all other official racial categories, with the term Hispanic or Latino, which was introduced in the 1980 census as a category of ethnicity, separate and independent of race.[14][15] Hispanic and Latino Americans as a whole make up a racially diverse group and as a whole are the largest minority in the country.[16][17]		In cases where individuals do not self-identify, the U.S. census parameters for race give each national origin a racial value.		Additionally, people who reported Muslim (or a sect of Islam such as Shi'ite or Sunni), Jewish, Zoroastrian, or Caucasian as their "race" in the "Some other race" section, without noting a country of origin, are automatically tallied as White.[19] The US Census considers the write-in response of "Caucasian" or "Aryan" to be a synonym for White in their ancestry code listing.[20]		In the contemporary United States, essentially anyone of European descent is considered White. However, many of the non-European ethnic groups classified as White by the U.S. Census, such as Jewish-Americans, Arab-Americans, and Hispanics or Latinos may not identify as, and may not be perceived to be, White.[21][22][23][24][25][26]		The definition of White has changed significantly over the course of American history. Among Europeans, those not considered White at some point in American history include Italians, Greeks, Spaniards, Irish, Swedes, Germans, Finns, Russians, and French.[26][27][28]		Early on in the United States, white generally referred to those of British ancestry or northern (Nordic) and northwestern (British and French) European descent.[29]		David R. Roediger argues that the construction of the white race in the United States was an effort to mentally distance slave owners from slaves.[30] The process of officially being defined as white by law often came about in court disputes over pursuit of citizenship.[31]		Critical race theory developed in the 1970s and 1980s, influenced by the language of critical legal studies, which challenged concepts such as objective truth, rationality and judicial neutrality, and by critical theory. Academics and activists disillusioned with the outcomes of the civil African-American Civil Rights Movement pointed out that though African Americans supposedly enjoyed legal equality, white Americans continued to hold disproportionate power and still had superior living standards. Liberal ideas such as meritocracy and equal opportunity, they argued, hid and reinforced deep structural inequalities and thus serves the interests of a white elite. Critical race theorists see racism as embedded in public attitudes and institutions, and highlight institutional racism and unconscious biases. Legal scholar Derrick Bell advanced the interest convergence principle, which suggests that whites support minority rights only when doing so is also in their self-interest.[32]		As Whites, especially White Anglo-Saxon Protestants, or WASPs, are the dominant racial and cultural group, according to sociologist Steven Seidman, writing from a critical theory perspective, "White culture constitutes the general cultural mainstream, causing non-White culture to be seen as deviant, in either a positive or negative manner. Moreover, Whites tend to be disproportionately represented in powerful positions, controlling almost all political, economic, and cultural institutions."		Yet, according to Seidman, Whites are most commonly unaware of their privilege and the manner in which their culture has always been dominant in the US, as they do not identify as members of a specific racial group but rather incorrectly perceive their views and culture as "raceless", when in fact it is ethno-national (ethnic/cultural) specific, with a racial base component.[33]		Whites (non-Hispanic and Hispanic) made up 79.8% or 75% of the American population in 2008.[16][17][38][39] This latter number is sometimes recorded as 77.1% when it includes about 2% of the population who are identified as white in combination with one or more other races. The largest ethnic groups (by ancestry) among White Americans were Germans, followed by Irish and English.[40] In the 1980 census 49,598,035 Americans cited that they were of English ancestry, making them 26% of the country and the largest group at the time, and in fact larger than the population of England itself.[41] Slightly more than half of these people would cite that they were of "American" ancestry on subsequent censuses and virtually everywhere that "American" ancestry predominates on the 2000 census corresponds to places where "English" predominated on the 1980 census.[42][43]		White Americans are projected to remain the majority, though with their percentage decreasing to 72% of the total population by 2050. However, projections state that non-Hispanic Whites of that group will become less than 50% of the population by 2042 because Non-Hispanic Whites have the lowest fertility rate of any major racial group in the United States,[44] mass-immigration of other ethnic groups with higher birth rates, and because of intermarriage with Hispanic Whites.		While over ten million White people can trace part of their ancestry back to the Pilgrims who arrived on the Mayflower in 1620 (this common statistic overlooks the Jamestown, Virginia foundations of America and roots of even earlier colonist-descended Americans, such as Spanish Americans in St. Augustine, Florida), over 35 million whites have at least one ancestor who passed through the Ellis Island immigration station, which processed arriving immigrants from 1892 until 1954. See also: European Americans.		According to the Census definition, White Americans are the majority racial group in almost all of the United States. They are not the majority in Hawaii, many American Indian reservations, parts of the South known as the Black Belt, the District of Columbia, all US territories, and in many urban areas throughout the country. Non-Hispanic whites are also not the majority in several southwestern states.		Overall the highest concentration of those referred to as "White alone" by the Census Bureau was found in the Midwest, New England, the Rocky Mountain states, Kentucky, and West Virginia. The lowest concentration of whites was found in southern and mid-Atlantic states.[3][45][46]		Although all large geographical areas are dominated by White Americans, much larger differences can be seen between specific parts of large cities.		States with the highest percentages of White Americans, as of 2007:[47]		States with the highest percentages of non-Hispanic Whites, as of 2007:[48]				White Americans have the second highest median household income and personal income levels in the nation, by cultural background. The median income per household member was also the highest, since White Americans had the smallest households of any racial demographic in the nation. In 2006, the median individual income of a White American age 25 or older was $33,030, with those who were full-time employed, and of age 25 to 64, earning $34,432. Since 42% of all households had two income earners, the median household income was considerably higher than the median personal income, which was $48,554 in 2005. Jewish Americans rank first in household income, personal income, and educational attainment among White Americans.[49] In 2005, White households had a median household income of $48,977, which is 10.3% above the national median of $44,389. Among Cuban Americans, with 86% classifying as White, those persons born in the US have a higher median income and educational attainment level than most other Whites.[50]		The poverty rates for White Americans are the second-lowest of any racial group, with 10.8% of white individuals living below the poverty line, 3% lower than the national average.[51] However, due to Whites' majority status, 48% of Americans living in poverty are white.[52]		White Americans' educational attainment is the second-highest in the country, after Asian Americans'. Overall, nearly one-third of White Americans had a Bachelor's degree, with the educational attainment for Whites being higher for those born outside the United States: 37.6% of foreign born, and 29.7% of native born Whites had a college degree. Both figures are above the national average of 27.2%.[53]		Gender income inequality was the greatest among Whites, with White men outearning White women by 48%. Census Bureau data for 2005 reveals that the median income of White females was lower than that of males of all races. In 2005, the median income for White American females was only slightly higher than that of African American females.[54]		White Americans are more likely to live in suburbs and small cities than their black counterparts.[55]		[56]		From their earliest presence in North America, White Americans have contributed literature, art, cinema, religion, agricultural skills, foods, science and technology, fashion and clothing styles, music, language, legal system, political system, and social and technological innovation to American culture. White American culture derived its earliest influences from English, Scottish, Welsh, and Irish settlers and is quantitatively the largest proportion of American culture.[58] The overall American culture reflects White American culture. The culture has been developing since long before the United States formed a separate country. Much of American culture shows influences from English culture. Colonial ties to Great Britain spread the English language, legal system and other cultural attributes.[59]		In his 1989 book Albion's Seed: Four British Folkways in America, David Hackett Fischer explores the details of the folkways of four groups of settlers from the British Isles that came to the American colonies during the 17th and 18th centuries from distinct regions of Britain and Ireland. His thesis is that the culture of each group persisted (albeit in modified form), providing the basis for the modern United States.[60]		According to Fischer, the foundation of America's four regional cultures was formed from four mass migrations from four regions of the British Isles by four distinct ethno-cultural groups. New England's formative period occurred between 1629 and 1640 when Puritans, mostly from East Anglia, settled there, thus forming the basis for the New England regional culture.[61] The next mass migration was of southern English Cavaliers and their working class English servants to the Chesapeake Bay region between 1640 and 1675. This spawned the creation of the American Southern culture.[62]		Then, between 1675 and 1725, thousands of Irish, Cornish, English and Welsh Quakers plus many Germans sympathetic to Quaker ideas, led by William Penn, settled the Delaware Valley. This resulted in the formation of the General American culture, although, according to Fischer, this is really a "regional culture", even if it does today encompass most of the U.S. from the mid-Atlantic states to the Pacific Coast.[63] Finally, a huge number of settlers from the borderlands between England and Scotland, and from northern Ireland, migrated to Appalachia between 1717 and 1775. This resulted in the formation of the Upland South regional culture, which has since expanded to the west to West Texas and parts of the American Southwest.[64]		In his book, Fischer brings up several points. He states that the U.S. is not a country with one "general" culture and several "regional" culture, as is commonly thought. Rather, there are only four regional cultures as described above, and understanding this helps one to more clearly understand American history as well as contemporary American life. Fischer asserts that it is not only important to understand where different groups came from, but when. All population groups have, at different times, their own unique set of beliefs, fears, hopes and prejudices. When different groups came to America and brought certain beliefs and values with them, these ideas became, according to Fischer, more or less frozen in time, even if they eventually changed in their original place of origin.[65]		Some White Americans have varying amounts of American Indian and Sub-Saharan African ancestry. In a recent study, Gonçalves et al. 2007 reported Sub-Saharan and Amerindian mtDna lineages at a frequency of 3.1% (respectively 0.9% and 2.2%) in American Caucasians (Please note that in the USA, "Caucasian" includes people from North Africa and Western Asia as well as Europeans).[66] Recent research on Y-chromosomes and mtDNA detected no African admixture in European-Americans. The sample included 628 European-American Y-chromosomes and mtDNA from 922 European-Americans[67]		DNA analysis on White Americans by geneticist Mark D. Shriver showed an average of 0.7% Sub-Saharan African admixture and 3.2% Native American admixture.[68] The same author, in another study, claimed that about 30% of all White Americans, approximately 66 million people, have a median of 2.3% of Black African admixture.[69] Shriver discovered his ancestry is 10 percent African, and Shriver's partner in DNA Print Genomics, J.T. Frudacas, contradicted him two years later stating "Five percent of European Americans exhibit some detectable level of African ancestry."[70]		From the 23andMe database, about 5 to at least 13 percent of self-identified White American Southerners have greater than 1 percent African ancestry.[71] Southern states with the highest African American populations, tended to have the highest percentages of self-identified White Americans unknowingly carrying hidden African ancestry.[72] White Americans (European Americans) on average are: “98.6 percent European, 0.19 percent African and 0.18 percent Native American.” Inferred British/Irish ancestry is found in European Americans from all states at mean proportions of above 20%, and represents a majority of ancestry, above 50% mean proportion, in states such as Mississippi, Arkansas, and Tennessee. Scandinavian ancestry in European Americans is highly localized; most states show only trace mean proportions of Scandinavian ancestry, while it comprises a significant proportion, upwards of 10%, of ancestry in European Americans from Minnesota and the Dakotas.[71][72]		Although most Hispanic Americans self-identify in the white racial category of the US Census and/or other official government data collecting, an overwhelming majority of them would in their personal lives consider themselves as ethnically mestizo (of mixed European and Amerindian background) or mulatto (of mixed European and sub-Saharan African background).		Thus, only a minority of those Hispanic Americans who self-identified in their personal lives as mestizo or mulatto actually selected "multiracial" as their race on the U.S. census, with 9 out of every 10 of them preferring to pick white, one of the five single race categories available on the U.S. census.[73]		In contrast to non-Hispanic European Americans, whose average European ancestry ranges about 98.6%,[71][74] genetic research has found that the average European admixture among self-identified Hispanic White Americans is 73% European, while the average European admixture for Hispanic Americans overall (regardless of their self-identified race) is 65.1% European admixture.		2 Russia is a transcontinental country in Eastern Europe and Northern Asia. The vast majority of its population (80%) lives in European Russia, therefore Russia as a whole is included as a European country here.		3 Yugoslav Americans are the American people from the former Yugoslavia.		4 Turkey is a transcontinental country in the Middle East and Southeast Europe. Has a small part of its territory (3%) in Southeast Europe called Turkish Thrace.		5 Armenia, Azerbaijan, and Georgia are transcontinental countries. They have a small part of their territories in the European part of the Caucasus.		6 Kazakhstan is technically a bicontinental country, having a small portion in European hands.		
Freakazoid! is an American animated television series created by Bruce Timm and Paul Dini and developed by Tom Ruegger for the Kids' WB programming block of The WB. The series chronicles the adventures of the title character, Freakazoid, a manic, insane superhero who battles with an array of super villains. The show also features mini-episodes of adventures of other bizarre superheroes. The show was produced by Amblin Television and Warner Bros. Animation. The cartoon was the third animated series produced by the collaboration of Steven Spielberg and Warner Bros. Animation during the animation renaissance of the 1990s.		Bruce Timm, best known as a major principal of the DC animated universe, originally intended it to be a straightforward superhero action-adventure cartoon with comic overtones, but executive producer Steven Spielberg asked series producer and writer Tom Ruegger and the Animaniacs team to turn Freakazoid! into a flat-out comedy.[1] The show is similar to fellow Ruegger-led programs such as Animaniacs, and the humor is unique in its inclusions of slapstick, fourth wall firings, parody, surreal humor and pop cultural references.		The series was one of the first to debut on the new Kids' WB Saturday morning block of The WB, on September 9, 1995. The series lasted for two seasons, finishing with 24 episodes, the final one broadcast on June 1, 1997. Although the series originally struggled in the ratings, reruns on Cartoon Network and a fan following have elevated the series to become a cult hit.[2] The show also ranked #53 on IGN's Top 100 Animated Series list.[3]						The show's title character is the superhero alter ego of geeky 16-year-old (later changed to 17-year-old) Dexter Douglas who attends Harry Connick High School. His name is a parody of various superheroes' alliterative names (e.g. Bruce Banner, Peter Parker). Dexter gained his abilities from a computer bug activated by a "secret key sequence" that must be typed (a reference to the Pentium FDIV bug). The sequence of keys is "@[=g3,8d]\&fbb=-q]/hk%fg" (the quotes are included), as seen when Roddy MacStew types the combination in "The Chip (Act 2)". The bug manifests when the user presses Delete after entering the string, and was first activated when Dexter's cat crawled onto the keyboard. Becoming absorbed into his computer and instantly gaining all the information on the Internet, Freakazoid has enhanced strength and endurance, extraordinary speed and agility, and negligible amounts of sanity. These changes make him a powerful and fearsome force for upholding freedom and righteousness, unless he gets distracted by something like a bear riding a motorcycle. He has a base called the Freakalair, a parody of the Batcave, built by his mute butler, Ingmar. The Freakalair contains a "Hall of Nifty Things to Know" and even a mad scientist lab. His greatest weakness, as he once explained to a villain, is that he can be imprisoned in a cage with graphite bars charged with negative ions. He also expresses a great aversion to "poo gas".		Peripheral powers come and go: Freakazoid once developed telekinesis triggered by anger that was never mentioned again after the episode, and once crossed the globe to yell at a Tibetan monk for raking too loudly, but apologizes to him later in the same plot. He also has the ability to assume the form of electricity and cover long distances instantaneously, although he just as often simply sticks his arms forward and runs while making swooshing sounds with his mouth, pretending to fly.		Dexter can change into and out of Freakazoid at will with the words "Freak out!" and "Freak in!". When not in Freakazoid mode, Dexter looks and acts completely normal, and his family is unaware that anything has happened to him. Freakazoid spends this time in an area of Dexter's brain called the Freakazone, where he reflects, has profound thoughts, and watches reruns of The Rat Patrol.		While the show's setting is set around Washington, D.C., the locale often varies with the show's humor, taking Freakazoid to locations around the world as needed.		Freakazoid! features a number of campy villains and enemies, including:		A few characters fall somewhere in the space between "enemies" and "allies" to land squarely in the category of "nuisances."		Freakazoid! also features several mini-segments, primarily during the first season. Each of these have their own theme songs, title cards and cast, and only rarely "cross over" into the continuity of the main show. These segments include:		The voice actors of the show Freakazoid! included various actors from other television series and films. Tress MacNeille, Maurice LaMarche, Jeff Bennett, and Frank Welker, who all provided voices in the series Animaniacs, were on Freakazoid!. Actors Edward Asner, Ricardo Montalbán, Larry Cedar, Jonathan Harris, and Stephen Furst also provided voices for the series. Also, writers John P. McCann and Paul Rugg (who played Freakazoid) added voices themselves.		Casting for the show had been difficult for the Freakazoid! staff, as no lead character had been found even after extensive auditions.[4] Eventually, when writer Paul Rugg was brought to demonstrate the voice in a recording session, he ended up filling the role, as he said: "I went in there and did it. Then they played it for Steven Spielberg and he said 'Yep! Fine, sure, great,' and then I panicked ... and I had to do it."[4] Rugg played the role of Freakazoid through the entire series run.		The music for Freakazoid! was written by Richard Stone, Steve Bernstein, Julie Bernstein, Gordon Goodwin and Tim Kelly. Stone won a Daytime Emmy with lyricist (and senior producer) Tom Ruegger for the main title song in 1996. Julie Bernstein was nominated for a Daytime Emmy for Outstanding Original Song in 1998 for the song "Invisibo" from the episode Freak-a-Panel.[citation needed]		Cartoonist Mike Allred has criticized the show and its lead character as plagiarism of his comic book Madman,[5] asserting that the title characters share several personality traits, and wear similar costumes featuring a chest emblem including an exclamation mark. During the short run of the show, Allred remained relatively silent on the subject, but in 2003, he responded to a question about the show on the message board of his official website:		[Show creator] Bruce Timm was kind enough to tell me that Madman was a direct inspiration for the show, with comics open and referred to when developing the show.		Stupidly, I was flattered; happy to inspire anything. But when the show came out, with no acknowledgement or credit or any kind of compensation, I slowly became annoyed as everyone and their uncle confronted me with "there's this cartoon that's ripping off Madman" and "you oughta sue".		I simply wrote a friendly letter to [show producer] Steven Spielberg telling him his production was a direct lift of my creation, I had no intention of creating ripples, I just wanted him to know that I knew. No one replied, which is fine. And to be honest, Madman is an amalgam of a half a dozen other influences. So who am I to complain (the exclamation mark on the chest still kinda urks [sic] me a little though. A little too close for comfort).[5]		The humor in Freakazoid! relied heavily on slapstick, parody and pop cultural references. Due to the series being metafiction, much of the series was self-aware humor (i.e. breaking the fourth wall); for instance, after the first appearance of the Freakmobile, the show goes immediately into an impromptu commercial for a toy version, and later in the episode, Freakazoid addresses an audience, congratulating the staff on how hard they have worked to make the show toyetic. A typically strange running gag involves a repeated credit for "Weena Mercator as the Hopping Woman", though no such character appears in any episode. Her credit is usually preceded by a number of other fictional names and followed by a fictional director. The show also incorporated humor aimed at the then-newly founded WB Network, such as questioning the meaning of the initials "WB", e.g., "Weird Butt" or "Wet Bananas" instead of Warner Bros.		Freakazoid! made frequent use of stock footage, including the peaceful scene of a field of flowers ("Relax-O-Vision"), numerous people screaming ("Scream-O-Vision"), traditionally dressed Bavarians dancing and slapping each other, a man being shot in the belly with a cannonball and a man wrestling a bear.		Cameo appearances were also a major element of the show's humor. At various times, Freakazoid! hosted appearances by characters from other Warner Bros. Cartoons such as Pinky and the Brain, Animaniacs and even an insinuation appearance of the Batman from Bruce Timm's animated version, which has a similar drawing style (its concept of old movie-style title cards at the beginning of each episode was also replicated in Freakazoid!). Portrayals of many celebrities (including producer Steven Spielberg) and guest appearances by such figures as Jack Valenti, Leonard Maltin and Mark Hamill as themselves were also commonplace. Norm Abram had an entire episode, "Normadeus", built around him. One original character, a bizarre-looking man named Emmitt Nervend, plays no role whatsoever other than enabling a Where's Waldo-esque hunt for his constant cameos (complete with the number of his appearances announced in the closing credits).		One of the show's longest cameo appearances was when Wakko (from Animaniacs) and Brain (from Pinky and the Brain) appeared in a scene in which they argue with Freakazoid over which of their shows is Steven Spielberg's favorite, with Freakazoid arguing that his show was the favorite because "we got a memo". (Tiny Toon Adventures was not represented in the discussion as it was on Nickelodeon at the time, while the others were on Kids' WB.) However, when the trio confronts Steven over the issue, he simply replies: "Who are you people?".		Freakazoid was created by animator Bruce Timm, who had previously produced Batman: The Animated Series, and Paul Dini, who was a story editor for Tiny Toon Adventures.[2] Timm was called upon by Steven Spielberg, who Timm said "liked" Timm's Batman series, to help create a new superhero show.[6] After a meeting with Spielberg, Timm said that Spielberg had "really liked" the idea for the series,[6] after which Timm and Dini created the character Freakazoid, an edgy superhero with a manic personality. Timm came up with the name for the character naturally, as he recalled, "The name 'Freakazoid' just kind of jumped out of me, I don't even know where from. I said 'Oh, yeah, 'Freakazoid', that might be an interesting name.'"[2]		Timm originally created Freakazoid to be a serious "adventure show" with some comedic undertones.[2] However, his initial idea for the series did not come to be, as he stated:		I don't mind that it's not on my résumé. [Laughs] I bailed on it really early. It started out as an adventure show, but it ended up turning into more and more of a comedy show; every time we'd have a meeting with Steven, the concept would kinda [sic] change, and it kept leaning more and more towards zany comedy. It really started out almost like Spider-Man, on that level of, like, a teenage superhero. And it reached a point where it became a comedy with the Tiny Toon Adventures/Animaniacs kind of humor. (...) I don't have anything against that; I just don't have a flair for it, so I bailed—I just hung out here while my staff had to do the show. [Laughs][6]		After Timm left the series, Tom Ruegger, who developed other Spielberg series Tiny Toon Adventures and Animaniacs, was brought in to redevelop the series Timm had created "from the ground up".[2] Ruegger's version of the series used some of Timm's designs and concepts, but Timm said that the series was "radically altered" to become the comedy series that was more to Spielberg's liking.[2]		Ruegger then began writing stories for the series, and came up with a pile of very short segments. Spielberg liked what Ruegger had written, but wanted longer stories for the series as well. Ruegger then asked writers John McCann and Paul Rugg to come onto the series to write longer, more elaborate stories for the series and, according to Rugg, "(...) figure out what this [Freakazoid!] was going to be, and the answer was like, 'We didn't know', and still don't".[2]		Freakazoid premiered on Kids' WB Saturday lineup on September 9, 1995.[1] During its run, Freakazoid came across problems of appealing to its target demographic, young children. Tom Ruegger said that Freakazoid had done poorly in ratings because the audience that the series gathered was older than the target audience.[2] Also, Freakazoid ran into timeslot problems. Writer John McCann said that the timeslot of the series changed frequently: "They put it at eight o' clock in the morning, 3:30 in the afternoon, they shifted it all around; we couldn't even find it, and we wrote the thing".[2] The series ran on Kids' WB until February 14, 1997, when it was canceled due to poor ratings, airing only one complete season and part of a second season.[1] The series won a Daytime Emmy Award for Outstanding Special Class Animated Program.[2][7] Rugg said the series' demise was the result of a combination of people not understanding the series, timeslot changes, appealing to the wrong demographics, and that "(...) there aren't a lot of Nielsen boxes in federal prisons. Had there been, I'm telling you, we'd still be on the air today".[2] Bruce Timm said that the series still has a cult following of fans who ask him questions about the series whenever they meet him.		However, the show was later picked up by Cartoon Network and was rebroadcast from April 5, 1997 until March 29, 2003.[1] The series had a total number of 24 episodes. In 2006, Freakazoid! was one of the shows scheduled to be broadcast on the AOL broadband channel, In2TV.[8]		Freakazoid never had his own comic book, but he did make a special guest crossover in issue #35 of the Animaniacs comic published by DC Comics.[9]		Warner Home Video has released the entire series on DVD in Region 1.		
African-American culture, also known as Black-American culture, in the United States refers to the cultural contributions of African Americans to the culture of the United States, either as part of or distinct from mainstream American culture. The distinct identity of African-American culture is rooted in the historical experience of the African-American people, including the Middle Passage. The culture is both distinct and enormously influential on American culture as a whole.		African-American culture is primarily rooted in West and Central Africa. Understanding its identity within the culture of the United States it is, in the anthropological sense, conscious of its origins as largely a blend of West and Central African cultures. Although slavery greatly restricted the ability of African-Americans to practice their original cultural traditions, many practices, values and beliefs survived, and over time have modified and/or blended with European cultures and other cultures such as that of Native Americans. African-American identity was established during the slavery period, producing a dynamic culture that has had and continues to have a profound impact on American culture as a whole, as well as that of the broader world.[1]		Elaborate rituals and ceremonies were a significant part of African Americans' ancestral culture. Many West African societies traditionally believed that spirits dwelled in their surrounding nature. From this disposition, they treated their environment with mindful care. They also generally believed that a spiritual life source existed after death, and that ancestors in this spiritual realm could then mediate between the supreme creator and the living. Honor and prayer was displayed to these "ancient ones," the spirit of those past. West Africans also believed in spiritual possession.[2]		In the beginning of the eighteenth century, Christianity began to spread across North Africa; this shift in religion began displacing traditional African spiritual practices. The enslaved Africans brought this complex religious dynamic within their culture to America. This fusion of traditional African beliefs with Christianity provided a common place for those practicing religion in Africa and America.[2]		After emancipation, unique African-American traditions continued to flourish, as distinctive traditions or radical innovations in music, art, literature, religion, cuisine, and other fields. 20th-century sociologists, such as Gunnar Myrdal, believed that African Americans had lost most of their cultural ties with Africa.[3] But, anthropological field research by Melville Herskovits and others demonstrated that there has been a continuum of African traditions among Africans of the diaspora.[4] The greatest influence of African cultural practices on European culture is found below the Mason-Dixon line in the American South.[5][6]		For many years African-American culture developed separately from European-American culture, both because of slavery and the persistence of racial discrimination in America, as well as African-American slave descendants' desire to create and maintain their own traditions. Today, African-American culture has become a significant part of American culture and yet, at the same time, remains a distinct cultural body.[7]						From the earliest days of American slavery in the 17th century, slave owners sought to exercise control over their slaves by attempting to strip them of their African culture. The physical isolation and societal marginalization of African slaves and, later, of their free progeny, however, facilitated the retention of significant elements of traditional culture among Africans in the New World generally, and in the U.S. in particular. Slave owners deliberately tried to repress independent political or cultural organization in order to deal with the many slave rebellions or acts of resistance that took place in the United States, Brazil, Haiti, and the Dutch Guyanas.[8]		African cultures, slavery, slave rebellions, and the civil rights movement have shaped African-American religious, familial, political, and economic behaviors. The imprint of Africa is evident in a myriad of ways: in politics, economics, language, music, hairstyles, fashion, dance, religion, cuisine, and worldview.		In turn, African-American culture has had a pervasive, transformative impact on many elements of mainstream American culture. This process of mutual creative exchange is called creolization.[7] Over time, the culture of African slaves and their descendants has been ubiquitous in its impact on not only the dominant American culture, but on world culture as well.[9]		Slaveholders limited or prohibited education of enslaved African Americans because they feared it might empower their chattel and inspire or enable emancipatory ambitions. In the United States, the legislation that denied slaves formal education likely contributed to their maintaining a strong oral tradition, a common feature of indigenous African cultures.[10] African-based oral traditions became the primary means of preserving history, mores, and other cultural information among the people. This was consistent with the griot practices of oral history in many African and other cultures that did not rely on the written word. Many of these cultural elements have been passed from generation to generation through storytelling. The folktales provided African Americans the opportunity to inspire and educate one another.[10]		Examples of African-American folktales include trickster tales of Br'er Rabbit[11] and heroic tales such as that of John Henry.[12] The Uncle Remus stories by Joel Chandler Harris helped to bring African-American folk tales into mainstream adoption.[13] Harris did not appreciate the complexity of the stories nor their potential for a lasting impact on society.[14] Other narratives that appear as important, recurring motifs in African-American culture are the "Signifying Monkey", "The Ballad of Shine", and the legend of Stagger Lee.		The legacy of the African-American oral tradition manifests in diverse forms. African-American preachers tend to perform rather than simply speak. The emotion of the subject is carried through the speaker's tone, volume, and cadence, which tend to mirror the rising action, climax, and descending action of the sermon. Often song, dance, verse, and structured pauses are placed throughout the sermon. Call and response is another pervasive element of the African-American oral tradition. It manifests in worship in what is commonly referred to as the "amen corner." In direct contrast to recent tradition in other American and Western cultures, it is an acceptable and common audience reaction to interrupt and affirm the speaker.[15] This pattern of interaction is also in evidence in music, particularly in blues and jazz forms. Hyperbolic and provocative, even incendiary, rhetoric is another aspect of African-American oral tradition often evident in the pulpit in a tradition sometimes referred to as "prophetic speech."[16]		Modernity and migration of black communities to the North has had a history of placing strain on the retention of black cultural practices and traditions. The urban and radically different spaces in which black culture was being produced raised fears in anthropologists and sociologists that the southern black folk aspect of black popular culture were at risk of being lost in history. The study over the fear of losing black popular cultural roots from the South have a topic of interest to many anthropologists, who among them include Zora Neale Hurston. Through her extensive studies of Southern folklore and cultural practices,Hustron has claimed that the popular Southern folklore traditions and practices are not dying off. Instead they are evolving, developing, and re-creating themselves in different regions.[17]		Other aspects of African-American oral tradition include the dozens, signifying, trash talk, rhyming, semantic inversion and word play, many of which have found their way into mainstream American popular culture and become international phenomena.[18]		Spoken word artistry is another example of how the African-American oral tradition has influenced modern popular culture. Spoken word artists employ the same techniques as African-American preachers including movement, rhythm, and audience participation.[19] Rap music from the 1980s and beyond has been seen as an extension of oral culture.[10]		The first major public recognition of African-American culture occurred during the Harlem Renaissance pioneered by Alain Locke. In the 1920s and 1930s, African-American music, literature, and art gained wide notice. Authors such as Zora Neale Hurston and Nella Larsen and poets such as Langston Hughes, Claude McKay, and Countee Cullen wrote works describing the African-American experience. Jazz, swing, blues and other musical forms entered American popular music. African-American artists such as William H. Johnson and Palmer Hayden created unique works of art featuring African Americans.[18]		The Harlem Renaissance was also a time of increased political involvement for African Americans. Among the notable African-American political movements founded in the early 20th century are the United Negro Improvement Association and the National Association for the Advancement of Colored People. The Nation of Islam, a notable quasi-Islamic religious movement, also began in the early 1930s.[20]		The Black Power movement of the 1960s and 1970s followed in the wake of the non-violent Civil Rights Movement. The movement promoted racial pride and ethnic cohesion in contrast to the focus on integration of the Civil Rights Movement, and adopted a more militant posture in the face of racism.[21] It also inspired a new renaissance in African-American literary and artistic expression generally referred to as the African-American or "Black Arts Movement".		The works of popular recording artists such as Nina Simone ("Young, Gifted and Black") and The Impressions ("Keep On Pushing"), as well as the poetry, fine arts, and literature of the time, shaped and reflected the growing racial and political consciousness.[22] Among the most prominent writers of the African-American Arts Movement were poet Nikki Giovanni;[23] poet and publisher Don L. Lee, who later became known as Haki Madhubuti; poet and playwright Leroi Jones, later known as Amiri Baraka; and Sonia Sanchez. Other influential writers were Ed Bullins, Dudley Randall, Mari Evans, June Jordan, Larry Neal, and Ahmos Zu-Bolton.		Another major aspect of the African-American Arts Movement was the infusion of the African aesthetic, a return to a collective cultural sensibility and ethnic pride that was much in evidence during the Harlem Renaissance and in the celebration of Négritude among the artistic and literary circles in the U.S., Caribbean, and the African continent nearly four decades earlier: the idea that "black is beautiful". During this time, there was a resurgence of interest in, and an embrace of, elements of African culture within African-American culture that had been suppressed or devalued to conform to Eurocentric America. Natural hairstyles, such as the afro, and African clothing, such as the dashiki, gained popularity. More importantly, the African-American aesthetic encouraged personal pride and political awareness among African Americans.[24]		African-American music is rooted in the typically polyrhythmic music of the ethnic groups of Africa, specifically those in the Western, Sahelean, and Sub-Saharan regions. African oral traditions, nurtured in slavery, encouraged the use of music to pass on history, teach lessons, ease suffering, and relay messages. The African pedigree of African-American music is evident in some common elements: call and response, syncopation, percussion, improvisation, swung notes, blue notes, the use of falsetto, melisma, and complex multi-part harmony.[10] During slavery, Africans in America blended traditional European hymns with African elements to create spirituals.[25]		Many African Americans sing "Lift Every Voice and Sing" in addition to the American national anthem, "The Star-Spangled Banner", or in lieu of it. Written by James Weldon Johnson and John Rosamond Johnson in 1900 to be performed for the birthday of Abraham Lincoln, the song was, and continues to be, a popular way for African Americans to recall past struggles and express ethnic solidarity, faith, and hope for the future.[26] The song was adopted as the "Negro National Anthem" by the NAACP in 1919.[27] Many African-American children are taught the song at school, church or by their families. "Lift Ev'ry Voice and Sing" traditionally is sung immediately following, or instead of, "The Star-Spangled Banner" at events hosted by African-American churches, schools, and other organizations.[28]		In the 19th century, as the result of the blackface minstrel show, African-American music entered mainstream American society. By the early 20th century, several musical forms with origins in the African-American community had transformed American popular music. Aided by the technological innovations of radio and phonograph records, ragtime, jazz, blues, and swing also became popular overseas, and the 1920s became known as the Jazz Age. The early 20th century also saw the creation of the first African-American Broadway shows, films such as King Vidor's Hallelujah!, and operas such as George Gershwin's Porgy and Bess.		Rock and roll, doo wop, soul, and R&B developed in the mid-20th century. These genres became very popular in white audiences and were influences for other genres such as surf. During the 1970s, the dozens, an urban African-American tradition of using rhyming slang to put down one's enemies (or friends), and the West Indian tradition of toasting developed into a new form of music. In the South Bronx the half speaking, half singing rhythmic street talk of "rapping" grew into the hugely successful cultural force known as hip hop.[29]		Hip Hop would become a multicultural movement, however, it still remained important to many African Americans. The African-American Cultural Movement of the 1960s and 1970s also fueled the growth of funk and later hip-hop forms such as rap, hip house, new jack swing, and go-go. House music was created in black communities in Chicago in the 1980s. African-American music has experienced far more widespread acceptance in American popular music in the 21st century than ever before. In addition to continuing to develop newer musical forms, modern artists have also started a rebirth of older genres in the form of genres such as neo soul and modern funk-inspired groups.[30]		In contemporary art, black subject matter has been used as raw material to portray the Black experience and aesthetics. The way Blacks' facial features were once conveyed as stereotypical in media and entertainment continues to be an influence within art. Dichotomies arise from artworks such as Open Casket by Dana Schutz based on the murder of Emmett Till to remove the painting and destroy it from the way Black pain is conveyed.[31] Meanwhile, Black artists such as Kerry James Marshall portrays the Black body as empowerment and Black invisibility.		African-American dance, like other aspects of African-American culture, finds its earliest roots in the dances of the hundreds of African ethnic groups that made up African slaves in the Americas as well as influences from European sources in the United States. Dance in the African tradition, and thus in the tradition of slaves, was a part of both every day life and special occasions. Many of these traditions such as get down, ring shouts, and other elements of African body language survive as elements of modern dance.[32]		In the 19th century, African-American dance began to appear in minstrel shows. These shows often presented African Americans as caricatures for ridicule to large audiences. The first African-American dance to become popular with white dancers was the cakewalk in 1891.[33] Later dances to follow in this tradition include the Charleston, the Lindy Hop, the Jitterbug and the swing.[34]		During the Harlem Renaissance, African-American Broadway shows such as Shuffle Along helped to establish and legitimize African-American dancers. African-American dance forms such as tap, a combination of African and European influences, gained widespread popularity thanks to dancers such as Bill Robinson and were used by leading white choreographers, who often hired African-American dancers.[34]		Contemporary African-American dance is descended from these earlier forms and also draws influence from African and Caribbean dance forms. Groups such as the Alvin Ailey American Dance Theater have continued to contribute to the growth of this form. Modern popular dance in America is also greatly influenced by African-American dance. American popular dance has also drawn many influences from African-American dance most notably in the hip-hop genre.[35]		From its early origins in slave communities, through the end of the 20th century, African-American art has made a vital contribution to the art of the United States.[36] During the period between the 17th century and the early 19th century, art took the form of small drums, quilts, wrought-iron figures, and ceramic vessels in the southern United States. These artifacts have similarities with comparable crafts in West and Central Africa. In contrast, African-American artisans like the New England–based engraver Scipio Moorhead and the Baltimore portrait painter Joshua Johnson created art that was conceived in a thoroughly western European fashion.[37]		During the 19th century, Harriet Powers made quilts in rural Georgia, United States that are now considered among the finest examples of 19th-century Southern quilting.[38] Later in the 20th century, the women of Gee's Bend developed a distinctive, bold, and sophisticated quilting style based on traditional African-American quilts with a geometric simplicity that developed separately but was like that of Amish quilts and modern art.[39]		After the American Civil War, museums and galleries began more frequently to display the work of African-American artists. Cultural expression in mainstream venues was still limited by the dominant European aesthetic and by racial prejudice. To increase the visibility of their work, many African-American artists traveled to Europe where they had greater freedom. It was not until the Harlem Renaissance that more European Americans began to pay attention to African-American art in America.[40]		During the 1920s, artists such as Raymond Barthé, Aaron Douglas,[41] Augusta Savage,[42] and photographer James Van Der Zee[43] became well known for their work. During the Great Depression, new opportunities arose for these and other African-American artists under the WPA. In later years, other programs and institutions, such as the New York City-based Harmon Foundation, helped to foster African-American artistic talent. Augusta Savage, Elizabeth Catlett, Lois Mailou Jones, Romare Bearden, Jacob Lawrence, and others exhibited in museums and juried art shows, and built reputations and followings for themselves.		In the 1950s and 1960s, there were very few widely accepted African-American artists. Despite this, The Highwaymen, a loose association of 27 African-American artists from Ft. Pierce, Florida, created idyllic, quickly realized images of the Florida landscape and peddled some 50,000 of them from the trunks of their cars. They sold their art directly to the public rather than through galleries and art agents, thus receiving the name "The Highwaymen". Rediscovered in the mid-1990s, today they are recognized as an important part of American folk history.[44][45] Their artwork is widely collected by enthusiasts and original pieces can easily fetch thousands of dollars in auctions and sales.[46]		The Black Arts Movement of the 1960s and 1970s was another period of resurgent interest in African-American art. During this period, several African-American artists gained national prominence, among them Lou Stovall, Ed Love, Charles White, and Jeff Donaldson. Donaldson and a group of African-American artists formed the Afrocentric collective AfriCOBRA, which remains in existence today. The sculptor Martin Puryear, whose work has been acclaimed for years, was being honored with a 30-year retrospective of his work at the Museum of Modern Art in New York in November 2007.[47] Notable contemporary African-American artists include Willie Cole, David Hammons, Eugene J. Martin, Mose Tolliver, Reynold Ruffins, the late William Tolliver, and Kara Walker.[48]		African-American literature has its roots in the oral traditions of African slaves in America. The slaves used stories and fables in much the same way as they used music.[10] These stories influenced the earliest African-American writers and poets in the 18th century such as Phillis Wheatley and Olaudah Equiano. These authors reached early high points by telling slave narratives.		During the early 20th century Harlem Renaissance, numerous authors and poets, such as Langston Hughes, W. E. B. Du Bois, and Booker T. Washington, grappled with how to respond to discrimination in America. Authors during the Civil Rights Movement, such as Richard Wright, James Baldwin, and Gwendolyn Brooks wrote about issues of racial segregation, oppression, and other aspects of African-American life. This tradition continues today with authors who have been accepted as an integral part of American literature, with works such as Roots: The Saga of an American Family by Alex Haley, The Color Purple by Alice Walker, Beloved by Nobel Prize-winning Toni Morrison, and fiction works by Octavia Butler and Walter Mosley. Such works have achieved both best-selling and/or award-winning status.[49]		The African-American Museum Movement emerged during the 1950s and 1960s to preserve the heritage of the African-American experience and to ensure its proper interpretation in American history.[50] Museums devoted to African-American history are found in many African-American neighborhoods. Institutions such as the African American Museum and Library at Oakland, The African American Museum in Cleveland and the Natchez Museum of African American History and Culture[51] were created by African Americans to teach and investigate cultural history that, until recent decades was primarily preserved through oral traditions.[52] Other prominent museums include Chicago's DuSable Museum of African American History and the National Museum of African American History and Culture, part of the Smithsonian Institution in Washington, D.C.		Generations of hardships imposed on the African-American community created distinctive language patterns. Slave owners often intentionally mixed people who spoke different African languages to discourage communication in any language other than English. This, combined with prohibitions against education, led to the development of pidgins, simplified mixtures of two or more languages that speakers of different languages could use to communicate.[53] Examples of pidgins that became fully developed languages include Creole, common to Louisiana,[54] and Gullah, common to the Sea Islands off the coast of South Carolina and Georgia.[55]		African American Vernacular English (AAVE) is a variety (dialect, ethnolect, and sociolect) of the American English language closely associated with the speech of, but not exclusive to, African Americans.[56] While AAVE is academically considered a legitimate dialect because of its logical structure, some of both whites and African Americans consider it slang or the result of a poor command of Standard American English. Many African Americans who were born outside the American South still speak with hints of AAVE or southern dialect. Inner-city African-American children who are isolated by speaking only AAVE sometimes have more difficulty with standardized testing and, after school, moving to the mainstream world for work.[57][58] It is common for many speakers of AAVE to code switch between AAVE and Standard American English depending on the setting.[59]		The Black Arts Movement, a cultural explosion of the 1960s, saw the incorporation of surviving cultural dress with elements from modern fashion and West African traditional clothing to create a uniquely African-American traditional style. Kente cloth is the best known African textile.[60] These festive woven patterns, which exist in numerous varieties, were originally made by the Ashanti and Ewe peoples of Ghana and Togo. Kente fabric also appears in a number of Western style fashions ranging from casual T-shirts to formal bow ties and cummerbunds. Kente strips are often sewn into liturgical and academic robes or worn as stoles. Since the Black Arts Movement, traditional African clothing has been popular amongst African Americans for both formal and informal occasions.[61] Other manifestations of traditional African dress in common evidence in African-American culture are vibrant colors, mud cloth, trade beads and the use of Adinkra motifs in jewelry and in couture and decorator fabrics.		Another common aspect of fashion in African-American culture involves the appropriate dress for worship in the Black church. It is expected in most churches that an individual present their best appearance for worship. African-American women in particular are known for wearing vibrant dresses and suits. An interpretation of a passage from the Christian Bible, "...every woman who prays or prophesies with her head uncovered dishonors her head...",[62] has led to the tradition of wearing elaborate Sunday hats, sometimes known as "crowns".[63][64]		Hair styling in African-American culture is greatly varied. African-American hair is typically composed of coiled curls, which range from tight to wavy. Many women choose to wear their hair in its natural state. Natural hair can be styled in a variety of ways, including the afro, twist outs, braid outs, and wash and go styles. It is a myth that natural hair presents styling problems or is hard to manage; this myth seems prevalent because mainstream culture has, for decades, attempted to get African American women to conform to its standard of beauty (i.e., straight hair). To that end, some women prefer straightening of the hair through the application of heat or chemical processes.[65] Although this can be a matter of personal preference, the choice is often affected by straight hair being a beauty standard in the West and the fact that hair type can affect employment. However, more and more women are wearing their hair in its natural state and receiving positive feedback. Alternatively, the predominant and most socially acceptable practice for men is to leave one's hair natural.[66][67]		Often, as men age and begin to lose their hair, the hair is either closely cropped, or the head is shaved completely free of hair. However, since the 1960s, natural hairstyles, such as the afro, braids, and dreadlocks, have been growing in popularity. Despite their association with radical political movements and their vast difference from mainstream Western hairstyles, the styles have attained considerable, but certainly limited, social acceptance.[68]		Maintaining facial hair is more prevalent among African-American men than in other male populations in the U.S.[69] In fact, the soul patch is so named because African-American men, particularly jazz musicians, popularized the style.[70] The preference for facial hair among African-American men is due partly to personal taste, but also because they are more prone than other ethnic groups to develop a condition known as pseudofolliculitis barbae, commonly referred to as razor bumps, many prefer not to shave.[71]		European-Americans have sometimes adopted different hairbraiding techniques and other forms of African-American hair. There are also individuals and groups who are working towards raising the standing of the African aesthetic among African Americans and internationally as well. This includes efforts toward promoting as models those with clearly defined African features; the mainstreaming of natural hairstyles; and, in women, fuller, more voluptuous body types.[68][72]		While African Americans practice a number of religions, Protestant Christianity is by far the most prevalent.[73] Additionally, 14% of Muslims in the United States and Canada are black.		The religious institutions of African-American Christians commonly are referred to collectively as the black church. During slavery, many slaves were stripped of their African belief systems and typically denied free religious practice, forced to become Christian. Slaves managed, however, to hang on to some practices by integrating them into Christian worship in secret meetings. These practices, including dance, shouts, African rhythms, and enthusiastic singing, remain a large part of worship in the African-American church.[74]		African-American churches taught that all people were equal in God's eyes and viewed the doctrine of obedience to one's master taught in white churches as hypocritical – yet accepted and propagated internal hierarchies and support for corporal punishment of children among other things .[74] Instead the African-American church focused on the message of equality and hopes for a better future.[75] Before and after emancipation, racial segregation in America prompted the development of organized African-American denominations. The first of these was the AME Church founded by Richard Allen in 1787.[74]		After the Civil War the merger of three smaller Baptist groups formed the National Baptist Convention This organization is the largest African-American Christian Denomination and the second largest Baptist denomination in the United States. An African-American church is not necessarily a separate denomination. Several predominantly African-American churches exist as members of predominantly white denominations.[76] African-American churches have served to provide African-American people with leadership positions and opportunities to organize that were denied in mainstream American society. Because of this, African-American pastors became the bridge between the African-American and European American communities and thus played a crucial role in the Civil Rights Movement.[77]		Like many Christians, African-American Christians sometimes participate in or attend a Christmas play. Black Nativity by Langston Hughes is a re-telling of the classic Nativity story with gospel music.[78] Productions can be found in African-American theaters and churches all over the country.[79]		Generations before the advent of the Atlantic slave trade, Islam was a thriving religion in West Africa due to its peaceful introduction via the lucrative Trans-Saharan trade between prominent tribes in the southern Sahara and the Arabs and Berbers in North Africa. In his attesting to this fact the West African scholar Cheikh Anta Diop explained: "The primary reason for the success of Islam in Black Africa [...] consequently stems from the fact that it was propagated peacefully at first by solitary Arabo-Berber travelers to certain Black kings and notables, who then spread it about them to those under their jurisdiction".[80] Many first-generation slaves were often able to retain their Muslim identity, their descendants were not. Slaves were either forcibly converted to Christianity as was the case in the Catholic lands or were besieged with gross inconveniences to their religious practice such as in the case of the Protestant American mainland.[81]		In the decades after slavery and particularly during the depression era, Islam reemerged in the form of highly visible and sometimes controversial movements in the African-American community. The first of these of note was the Moorish Science Temple of America, founded by Noble Drew Ali. Ali had a profound influence on Wallace Fard, who later founded the Black nationalist Nation of Islam in 1930. Elijah Muhammad became head of the organization in 1934. Much like Malcolm X, who left the Nation of Islam in 1964, many African-American Muslims now follow traditional Islam.[82]		Many former members of the Nation of Islam converted to Sunni Islam when Warith Deen Mohammed took control of the organization after his father's death in 1975 and taught its members the traditional form of Islam based on the Qur'an.[83] A survey by the Council on American-Islamic Relations shows that 30% of Sunni Mosque attendees are African Americans. In fact, most African-American Muslims are orthodox Muslims, as only 2% are of the Nation of Islam.[84]		There are 150,000 African Americans in the United States who practice Judaism.[85] Some of these are members of mainstream Jewish groups like the Reform, Conservative, or Orthodox branches of Judaism; others belong to non-mainstream Jewish groups like the Black Hebrew Israelites. The Black Hebrew Israelites are a collection of African-American religious organizations whose practices and beliefs are derived to some extent from Judaism. Their varied teachings often include, that African Americans are descended from the Biblical Israelites.[86]		Studies have shown in the last 10 to 15 years there has been major increase in African-Americans identifying as Jewish.[85] As such this misconception may become less common in the future. Rabbi Capers Funnye, the first cousin of Michelle Obama, says in response to skepticism by some on people being African-American and Jewish at the same time, "I am a Jew, and that breaks through all color and ethnic barriers."[87]		Aside from Christianity, Islam, and Judaism, there are also African Americans who follow Buddhism and a number of other religions. There is a small but growing number of African Americans who participate in African traditional religions, such as West African Vodun, Santería, Ifá and diasporic traditions like the Rastafari movement. Many of them are immigrants or descendants of immigrants from the Caribbean and South America, where these are practiced. Because of religious practices, such as animal sacrifice, which are no longer common among the larger American religions, these groups may be viewed negatively and are sometimes the victims of harassment. It must be stated, however, that since the Supreme Court judgement that was given to the Lukumi Babaluaye church of Florida in 1993, there has been no major legal challenge to their right to function as they see fit.[88]		In a 2008 Pew Forum survey, 12% of African Americans described themselves as nothing in particular (11%), agnostic (1%), or atheist (<0.5%).[89]		For most African Americans, the observance of life events follows the pattern of mainstream American culture. While African Americans and whites often lived to themselves for much of American history, both groups generally had the same perspective on American culture. There are some traditions that are unique to African Americans.		Some African Americans have created new rites of passage that are linked to African traditions. Some pre-teen and teenage boys and girls take classes to prepare them for adulthood. These classes tend to focus on spirituality, responsibility, and leadership. Many of these programs are modeled after traditional African ceremonies, with the focus largely on embracing African cultures.[90]		To this day, some African-American couples choose to "jump the broom" as a part of their wedding ceremony. Although the practice, which can be traced back to Ghana,[91] fell out of favor in the African-American community after the end of slavery, it has experienced a slight resurgence in recent years as some couples seek to reaffirm their African heritage.[92]		Funeral traditions tend to vary based on a number of factors, including religion and location, but there are a number of commonalities. Probably the most important part of death and dying in the African-American culture is the gathering of family and friends. Either in the last days before death or shortly after death, typically any friends and family members that can be reached are notified. This gathering helps to provide spiritual and emotional support, as well as assistance in making decisions and accomplishing everyday tasks.		The spirituality of death is very important in African-American culture. A member of the clergy or members of the religious community, or both, are typically present with the family through the entire process. Death is often viewed as transitory rather than final. Many services are called homegoings or homecomings, instead of funerals, based on the belief that the person is going home to the afterlife; "Returning to god" or the Earth (also see Euphemism as well as Connotation).[93] The entire end of life process is generally treated as a celebration of the person's life, deeds and accomplishments - the "good things" rather than a mourning of loss. This is most notably demonstrated in the New Orleans jazz funeral tradition where upbeat music, dancing, and food encourage those gathered to be happy and celebrate the homegoing of a beloved friend.[94]		In studying of the African American culture, food cannot be left out as one of the medians to understand their traditions, religion, interaction, and social and cultural structures of their community. Observing the ways they prepare their food and eat their food ever since the enslaved era, reveals about the nature and identity of African American culture in the United States.[95] Derek Hicks examines the origins of "gumbo", which is considered a soul food to African Americans, in his reference to the intertwinement of food and culture in African American community. No written evidence are found historically about the gumbo or its recipes, so through the African American's nature of orally passing their stories and recipes down, gumbo came to represent their truly communal dish. Gumbo is said to be "an invention of enslaved Africans and African Americans."[96] By mixing and cooking leftover ingredients from their White owners (often less desirable cuts of meats and vegetables) all together into a dish that has consistency between stew and soup, African Americans took the detestable and created it into a desirable dish. Through sharing of this food in churches with a gathering of their people, they not only shared the food, but also experience, feelings, attachment, and sense of unity that brings the community together.				The cultivation and use of many agricultural products in the United States, such as yams, peanuts, rice, okra, sorghum, grits, indigo dyes, and cotton, can be traced to African influences. African-American foods reflect creative responses to racial and economic oppression and poverty. Under slavery, African Americans were not allowed to eat better cuts of meat, and after emancipation many were often too poor to afford them.[97]		Soul food, a hearty cuisine commonly associated with African Americans in the South (but also common to African Americans nationwide), makes creative use of inexpensive products procured through farming and subsistence hunting and fishing. Pig intestines are boiled and sometimes battered and fried to make chitterlings, also known as "chitlins." Ham hocks and neck bones provide seasoning to soups, beans and boiled greens (turnip greens, collard greens, and mustard greens).[98]		Other common foods, such as fried chicken and fish, macaroni and cheese, cornbread, and hoppin' john (black-eyed peas and rice) are prepared simply. When the African-American population was considerably more rural than it generally is today, rabbit, opossum, squirrel, and waterfowl were important additions to the diet. Many of these food traditions are especially predominant in many parts of the rural South.[98]		Traditionally prepared soul food is often high in fat, sodium, and starch. Highly suited to the physically demanding lives of laborers, farmhands and rural lifestyles generally, it is now a contributing factor to obesity, heart disease, and diabetes in a population that has become increasingly more urban and sedentary. As a result, more health-conscious African Americans are using alternative methods of preparation, eschewing trans fats in favor of natural vegetable oils and substituting smoked turkey for fatback and other, cured pork products; limiting the amount of refined sugar in desserts; and emphasizing the consumption of more fruits and vegetables than animal protein. There is some resistance to such changes, however, as they involve deviating from long culinary tradition.[99]		As with other American racial and ethnic groups, African Americans observe ethnic holidays alongside traditional American holidays. Holidays observed in African-American culture are not only observed by African Americans but are widely considered American holidays. The birthday of noted American civil rights leader Martin Luther King, Jr has been observed nationally since 1983.[100] It is one of three federal holidays named for an individual.[101]		Black History Month is another example of another African-American observance that has been adopted nationally and its teaching is even required by law in some states. Black History Month is an attempt to focus attention on previously neglected aspects of the American history, chiefly the lives and stories of African Americans. It is observed during the month of February to coincide with the founding of the NAACP and the birthdays of Frederick Douglass, a prominent African-American abolitionist, and Abraham Lincoln, the United States president who signed the Emancipation Proclamation.[100]		On June 7, 1979 President Jimmy Carter decreed that June would be the month of black music. For the past 28 years, presidents have announced to Americans that Black Music Month (also called African-American Music Month) should be recognized as a critical part of American heritage. Black Music Month is highlighted with various events urging citizens to revel in the many forms of music from gospel to hip-hop. African-American musicians, singers, and composers are also highlighted for their contributions to the nation's history and culture.[102]		Less-widely observed outside of the African-American community is Emancipation Day popularly known as Juneteenth or Freedom Day, in recognition of the official reading of the Emancipation Proclamation on June 19, 1865 in Texas.[103] Juneteenth is a day when African Americans reflect on their unique history and heritage. It is one of the fastest growing African-American holidays with observances in the United States. Another holiday not widely observed outside of the African-American community is the birthday of Malcolm X. The day is observed on May 19 in American cities with a significant African-American population, including Washington, D.C.[104]		Another noted African-American holiday is Kwanzaa. Like Emancipation Day, it is not widely observed outside of the African-American community, although it is growing in popularity with both African-American and African communities. African-American scholar and activist "Maulana" Ron Karenga invented the festival of Kwanzaa in 1966, as an alternative to the increasing commercialization of Christmas. Derived from the harvest rituals of Africans, Kwanzaa is observed each year from December 26 through January 1. Participants in Kwanzaa celebrations affirm their African heritage and the importance of family and community by drinking from a unity cup; lighting red, black, and green candles; exchanging heritage symbols, such as African art; and recounting the lives of people who struggled for African and African-American freedom.[105]		Negro Election Day is also another festival derived from rituals of African culture specifically West Africa and revolves around the voting of a black official in New England colonies during the 18th century.		Although many African-American names are common among the larger population of the United States, distinct naming trends have emerged within the African American culture. Prior to the 1950s and 1960s, most African-American names closely resembled those used within European American culture.[106] A dramatic shift in naming traditions began to take shape in the 1960s and 1970s in America. With the rise of the mid-century Civil Rights Movement, there was a dramatic rise in names of various origins.[107] The practice of adopting neo-African or Islamic names gained popularity during that era. Efforts to recover African heritage inspired selection of names with deeper cultural significance. Before this, using African names was uncommon because African Americans were several generations removed from the last ancestor to have an African name, as slaves were often given European names and most surnames are of Anglo origin.[108]		African-American names have origins in many languages including French, Latin, English, Arabic, and African languages. One very notable influence on African-American names is the Muslim religion. Islamic names entered the popular culture with the rise of The Nation of Islam among Black Americans with its focus on civil rights. The popular name "Aisha" has origins in the Qur'an. Despite the origins of these names in the Muslim religion and the place of the Nation of Islam in the civil rights movement, many Muslim names such as Jamal and Malik entered popular usage among Black Americans simply because they were fashionable, and many Islamic names are now commonly used by African Americans regardless of their religion. Names of African origin began to crop up as well. Names like Ashanti, Tanisha, Aaliyah, Malaika have origins in the continent of Africa.[106][109]		By the 1970s and 1980s, it had become common within the culture to invent new names, although many of the invented names took elements from popular existing names. Prefixes such as La/Le-, Da/De-, Ra/Re-, or Ja/Je- and suffixes such as -ique/iqua, -isha, and -aun/-awn are common, as well as inventive spellings for common names.[110]		Even with the rise of creative names, it is also still common for African Americans to use biblical, historic, or European names.[106][111][112]		When slavery was practiced in the United States, it was common for families to be separated through sale. Even during slavery, however, many African-American families managed to maintain strong familial bonds. Free African men and women, who managed to buy their own freedom by being hired out, who were emancipated, or who had escaped their masters, often worked long and hard to buy the members of their families who remained in bondage and send for them.		Others, separated from blood kin, formed close bonds based on fictive kin; play relations, play aunts, cousins, and the like. This practice, a holdover from African oral traditions such as sanankouya, survived Emancipation, with non-blood family friends commonly accorded the status and titles of blood relations. This broader, more African concept of what constitutes family and community, and the deeply rooted respect for elders that is part of African traditional societies, may be the genesis of the common use of the terms like "cousin" (or "cuz"), "aunt", "uncle", "brother", "sister", "Mother", and "Mama" when addressing other African-American people, some of whom may be complete strangers.		Immediately after slavery, African-American families struggled to reunite and rebuild what had been taken. As late as 1960, when most African Americans lived under some form of segregation, 78% of African-American families were headed by married couples. This number steadily declined during the latter half of the 20th century.[113] For the first time since slavery, a majority of African-American children live in a household with only one parent, typically the mother.[114]		This apparent weakness is balanced by mutual-aid systems established by extended family members to provide emotional and economic support. Older family members pass on social and cultural traditions such as religion and manners to younger family members. In turn, the older family members are cared for by younger family members when they cannot care for themselves. These relationships exist at all economic levels in the African-American community, providing strength and support both to the African-American family and the community.[115]		Since the passing of the Voting Rights Act, African Americans are voting and being elected to public office in increasing numbers. As of 2008,[update] there were approximately 10,000 African-American elected officials in America.[116] African Americans are overwhelmingly Democratic. Only 11% of African Americans voted for George W. Bush in the 2004 Presidential Election.[117]		Social issues such as racial profiling,[118] the racial disparity in sentencing,[119] higher rates of poverty,[120] lower access to health care[121] and institutional racism[122] in general are important to the African-American community. While the divide on racial and fiscal issues has remained consistently wide for decades, seemingly indicating a wide social divide, African Americans tend to hold the same optimism and concern for America as whites.		An area where African Americans in general outstrip whites is in their condemnation of homosexuality. Prominent leaders in the Black church have demonstrated against gay rights issues such as gay marriage. This stands in stark contrast to the down-low phenomenon of covert male–male sexual acts. There are those within the community who take a different position, notably the late Coretta Scott King[123] and the Reverend Al Sharpton, the latter of whom, when asked in 2003 whether he supported gay marriage, replied that he might as well have been asked if he supported black marriage or white marriage.[124]		African-American neighborhoods are types of ethnic enclaves found in many cities in the United States. The formation of African-American neighborhoods is closely linked to the history of segregation in the United States, either through formal laws, or as a product of social norms. Despite this, African-American neighborhoods have played an important role in the development of nearly all aspects of both African-American culture and broader American culture.		Many affluent African-American communities exist today, including the following: Woodmore, Maryland; Hillcrest, Rockland County, New York; Redan and Cascade Heights, Georgia; Mitchellville, Maryland; Quinby, South Carolina; Forest Park, Oklahoma; Mount Airy, Philadelphia, Pennsylvania.		Due to segregated conditions and widespread poverty some African-American neighborhoods in the United States have been called "ghettos". The use of this term is controversial and, depending on the context, potentially offensive. Despite mainstream America's use of the term "ghetto" to signify a poor urban area populated by ethnic minorities, those living in the area often used it to signify something positive. The African-American ghettos did not always contain dilapidated houses and deteriorating projects, nor were all of its residents poverty-stricken. For many African Americans, the ghetto was "home", a place representing authentic "blackness" and a feeling, passion, or emotion derived from the rising above the struggle and suffering of being of African descent in America.[125]		Langston Hughes relays in the "Negro Ghetto" (1931) and "The Heart of Harlem" (1945): "The buildings in Harlem are brick and stone/And the streets are long and wide,/But Harlem's much more than these alone,/Harlem is what's inside." Playwright August Wilson used the term "ghetto" in Ma Rainey's Black Bottom (1984) and Fences (1987), both of which draw upon the author's experience growing up in the Hill District of Pittsburgh, an African-American ghetto.[126]		Although African-American neighborhoods may suffer from civic disinvestment,[127] with lower-quality schools,[128] less-effective policing[129] and fire protection,[130][131] there are institutions such as churches and museums and political organizations that help to improve the physical and social capital of African-American neighborhoods. In African-American neighborhoods the churches may be important sources of social cohesion.[132] For some African Americans, the kind spirituality learned through these churches works as a protective factor against the corrosive forces of racism.[133] Museums devoted to African-American history are also found in many African-American neighborhoods.		Many African-American neighborhoods are located in inner cities, and these are the mostly residential neighborhoods located closest to the central business district. The built environment is often row houses or brownstones, mixed with older single-family homes that may be converted to multi-family homes. In some areas there are larger apartment buildings. Shotgun houses are an important part of the built environment of some southern African-American neighborhoods. The houses consist of three to five rooms in a row with no hallways. This African-American house design is found in both rural and urban southern areas, mainly in African-American communities and neighborhoods.[134]		
Harvey Lawrence Pekar (/ˈpiːkɑːr/; October 8, 1939 – July 12, 2010) was an American underground comic book writer, music critic, and media personality, best known for his autobiographical American Splendor comic series. In 2003, the series inspired a well-received film adaptation of the same name.		Frequently described as the "poet laureate of Cleveland,"[2][3] Pekar "helped change the appreciation for, and perceptions of, the graphic novel, the drawn memoir, the autobiographical comic narrative."[4] Pekar described his work as "autobiography written as it's happening. The theme is about staying alive, getting a job, finding a mate, having a place to live, finding a creative outlet. Life is a war of attrition. You have to stay active on all fronts. It's one thing after another. I've tried to control a chaotic universe. And it's a losing battle. But I can't let go. I've tried, but I can't."[5]						Harvey Pekar and his younger brother Allen were born in Cleveland, Ohio to Saul and Dora Pekar, immigrants from Białystok, Poland. Saul Pekar was a Talmudic scholar who owned a grocery store on Kinsman Avenue, with the family living above the store.[6] While Pekar said he wasn't close to his parents due to their dissimilar backgrounds and because they worked all the time, he still "marveled at how devoted they were to each other. They had so much love and admiration for one another."[7]		As a child, Pekar's first language was Yiddish, and he learned to read and appreciate novels in the language.[8]		Pekar said that for the first few years of his life, he didn't have friends.[9] The neighborhood he lived in had once been all white but became mostly black by the 1940s; as one of the only white kids still living there Pekar was often beaten up. He later believed this instilled in him "a profound sense of inferiority."[10] However, this experience also taught him to eventually become a "respected street scrapper."[10]		Harvey Pekar graduated from Shaker Heights High School in 1957, then attended Case Western Reserve University, where he dropped out after a year.[6] He then served in the United States Navy, and after discharge returned to Cleveland where he worked odd jobs before being hired as file clerk at Cleveland's Veteran's Administration Hospital. He held this job even after becoming famous, refusing all promotions until he finally retired in 2001.[6][10]		Pekar was married from 1960 to 1972 to his first wife, Karen Delaney.[citation needed] His second wife was Helen Lark Hall.[citation needed] Pekar's third wife was writer Joyce Brabner,[citation needed] with whom he collaborated on Our Cancer Year, a graphic novel autobiography of his harrowing yet successful treatment for lymphoma. He lived in Cleveland Heights, Ohio with Brabner and their foster daughter Danielle.[11]		Pekar's friendship with Robert Crumb eventually led to the creation of the self-published, autobiographical comic book series American Splendor. Crumb and Pekar became friends through their mutual love of jazz records[12] when Crumb was living in Cleveland in the mid-1960s. Crumb's work in underground comics led Pekar to see the form's possibilities, saying, "Comics could do anything that film could do. And I wanted in on it."[13] It took Pekar a decade to do so: "I theorized for maybe ten years about doing comics."[14]		Around 1972, Pekar laid out some stories with crude stick figures and showed them to Crumb and another artist, Robert Armstrong. Impressed, they both offered to illustrate. Pekar & Crumb's one-pager "Crazy Ed" was published as the back cover of Crumb's The People's Comics (Golden Gate Publishing Company, 1972), becoming Pekar's first published work of comics. Including "Crazy Ed" and before the publication of American Splendor #1, Pekar wrote a number of other comic stories that were published in a variety of outlets:		The first issue of Pekar's self-published American Splendor series appeared in May 1976, with stories illustrated by the likes of Crumb, Dumm, Budgett, and Brian Bram. American Splendor documented Pekar's daily life in the aging neighborhoods of his native Cleveland. Pekar's best-known and longest-running collaborators include Crumb, Dumm, Budgett, Spain Rodriguez, Joe Zabel, Gerry Shamray, Frank Stack, Mark Zingarelli, and Joe Sacco. In the 2000s, he teamed regularly with artists Dean Haspiel and Josh Neufeld. Other cartoonists who worked with him include Jim Woodring, Chester Brown, Alison Bechdel, Gilbert Hernandez, Eddie Campbell, David Collier, Drew Friedman, Ho Che Anderson, Rick Geary, Ed Piskor, Hunt Emerson, Bob Fingerman, Brian Bram, and Alex Wald; as well as such non-traditional illustrators as Pekar's wife, Joyce Brabner, and comics writer Alan Moore.		Stories from the American Splendor comics have been collected in many books and anthologies.		A film adaptation of American Splendor was released in 2003, directed by Robert Pulcini and Shari Springer Berman.[15] It featured Paul Giamatti as Pekar, as well as appearances by Pekar himself. Pekar wrote about the effects of the film in American Splendor: Our Movie Year.		In 2006, Pekar released a four-issue American Splendor miniseries through the DC Comics imprint Vertigo.[16] This was collected in the American Splendor: Another Day paperback. In 2008 Vertigo released a second "season" of American Splendor that was collected in the American Splendor: Another Dollar paperback.		In addition to his autobiographical work on American Splendor, Pekar wrote a number of biographies. The first of these, American Splendor: Unsung Hero (2003), documented the Vietnam War experience of Robert McNeill, one of Pekar's African-American coworkers at Cleveland's VA hospital.		On October 5, 2005, the DC Comics imprint Vertigo released Pekar's autobiographical hardcover The Quitter, with artwork by Dean Haspiel. The book detailed Pekar's early years.		In 2006, Pekar released another biography for Ballantine/Random House, Ego & Hubris: The Michael Malice Story, about the life of Michael Malice, who was the founding editor of OverheardinNewYork.com.[17]		Pekar was the first guest editor for the collection The Best American Comics 2006 published by Houghton Mifflin, the first comics collection in the "Best American series" series.		In June 2007, Pekar collaborated with student Heather Roberson and artist Ed Piskor on the book Macedonia, which centers around Roberson's studies in the country.[18][19]		January 2008 saw another biographical work from Pekar, Students for a Democratic Society: A Graphic History, released through Hill & Wang.		In March 2009, Pekar released The Beats, a history of the Beat Generation, including Jack Kerouac and Allen Ginsberg, illustrated by Ed Piskor.[20] In May 2009 he released Studs Terkel's Working: A Graphic Adaptation.		In 2010, Pekar launched a webcomic with the online magazine Smith, titled The Pekar Project.[21]		In 2011, Abrams Comicarts published "Yiddishkeit," co-edited by Pekar with Paul Buhle and with Hershl Hartman, depicting many aspects of Yiddish language and culture. Artists in this anthology include many of Pekar's previous collaborators.		Pekar's comic book success led to a guest appearance on Late Night with David Letterman on October 15, 1986. Pekar was invited back repeatedly and made five more appearances in quick succession. These appearances were notable for verbal altercations between Pekar and Letterman, particularly on the subject of General Electric's ownership of NBC. The most heated of these was in the August 31, 1988 episode of Late Night, in which Pekar accused Letterman of appearing to be a shill for General Electric and Letterman promised never to invite Pekar back on the show. However, Pekar did appear on Late Night again on April 20, 1993, and appeared on the Late Show With David Letterman in 1994.[22]		Pekar was a prolific record collector as well as a freelance book and jazz critic, focusing on significant figures from jazz's golden age but also championing out-of-mainstream artists such as Birth, Scott Fields, Fred Frith and Joe Maneri. He reviewed literary fiction in the early 1990s in such periodicals as the Los Angeles Reader,[citation needed] the Review of Contemporary Fiction,[citation needed] and Woodward Review.[citation needed] Pekar won awards for his essays broadcast on public radio. He appeared in Alan Zweig's 2000 documentary film about record collecting, Vinyl.[23] In August 2007, Pekar was featured on the Cleveland episode of Anthony Bourdain: No Reservations with host Anthony Bourdain.[24]		While American Splendor theater adaptations had previously occurred,[25] in 2009, Pekar made his theatrical debut with Leave Me Alone!, a jazz opera for which Pekar wrote the libretto. Leave Me Alone! featured music by Dan Plonsey and was co-produced by Real Time Opera and Oberlin College premiering at Finney Chapel on January 31, 2009.[26]		In 2009, Pekar was featured in The Cartoonist, a documentary film on the life and work of Jeff Smith, creator of Bone.[27]		Shortly before 1 a.m. on July 12, 2010, Pekar's wife found Pekar dead in their Cleveland Heights, Ohio, home.[6] No immediate cause was determined,[28] but in October the Cuyahoga County coroner's office ruled it was an accidental overdose of antidepressants fluoxetine and bupropion.[29] Pekar had been diagnosed with cancer for the third time and was about to undergo treatment.[6] His headstone features one of his quotations as an epitaph: "Life is about women, gigs, an' bein' creative."[30]		Some Pekar works were to be released posthumously,[31] including two collaborations with Joyce Brabner, The Big Book Of Marriage and Harvey and Joyce Plumb the Depths of Depression, as well as a collection of the webcomics that ran as a part of The Pekar Project.[32] Working with illustrator Summer McClinton, Pekar also finished a book on American Marxist Louis Proyect, tentatively called The Unrepentant Marxist, after Proyect's blog. In the works since 2008, the book was to be published by Random House. After a conflict between Proyect and Joyce Brabner, Brabner announced that she would hold the book back indefinitely.[33] As of April 2014, those four titles have not been released.		In December 2010, the last story Pekar wrote — "Harvey Pekar Meets the Thing", in which Pekar has a conversation with Ben Grimm — was published in the Marvel Comics anthology Strange Tales II; the story was illustrated by Ty Templeton.[34]		"I think probably the most important thing about American Splendor, in all its incarnations, is that there were very few people in the earlier days of comics prepared to put their work where their mouth was. Harvey believed there was no limit to how good comics could be. To chronicle his life from these tiny wonderful moments of magic and of heartbreak — and the most important thing was that he did it."		Frequently described as the "poet laureate of Cleveland,"[2][3] Pekar "helped change the appreciation for, and perceptions of, the graphic novel, the drawn memoir, the autobiographical comic narrative."[4]		His American Splendor "remains one of the most compelling and transformative series in the history of comics."[35] In addition, Pekar was the first author to publicly distribute "memoir comic books."[36] While it is common today for people to publicly write about their lives on blogs, social media platforms, and in graphic novels, "In the mid-seventies, Harvey Pekar was doing all this before it was ubiquitous and commercialized."[36]		In October 2012 a statue of Pekar was installed at the Cleveland Heights-University Heights Library, a place he visited almost daily.[37][38]		
Coordinates: 32°42′22.60″N 117°09′42.63″W﻿ / ﻿32.7062778°N 117.1618417°W﻿ / 32.7062778; -117.1618417		San Diego Comic-Con International is a multi-genre entertainment and comic convention held annually in San Diego, California, United States. It was founded as the Golden State Comic Book Convention in 1970 by a group of San Diegans that included Shel Dorf, Richard Alf, Ken Krueger, and Mike Towry[2][3][4][5]; later, it was called the "San Diego Comic Book Convention". The name, as given on its website, is Comic-Con International: San Diego; but it is commonly known simply as Comic-Con or the San Diego Comic-Con or "SDCC".[6][7] It is a four-day event (Thursday–Sunday) held during the summer at the San Diego Convention Center in San Diego. On the Wednesday evening prior to the official opening of the event, there is a preview for professionals, exhibitors, and select guests pre-registered for all four days. This pre-event is called Preview Night and is only open for three hours, from 6pm to 9pm, to give attendees a preview of what is to come in the upcoming days of the event.		Comic-Con International also produces two other conventions, WonderCon, held in Anaheim, and the Alternative Press Expo (APE), held in San Francisco. Since 1974, Comic-Con has bestowed its annual Inkpot Award on guests and persons of interest in the popular arts industries, as well as on members of Comic-Con's board of directors and the Convention committee. It is also the home of the Will Eisner Awards.		Originally showcasing primarily comic books and science fiction/fantasy related film, television, and similar popular arts, the convention has since included a larger range of pop culture and entertainment elements across virtually all genres, including horror, animation, anime, manga, toys, collectible card games, video games, webcomics, and fantasy novels. According to Forbes, the convention is the "largest convention of its kind in the world;"[8] Publishers Weekly wrote "Comic-Con International: San Diego is the largest show in North America;"[9] it is also the largest convention held in San Diego.[10] In 2010, it filled the San Diego Convention Center to capacity with more than 130,000 attendees.[11] In addition to drawing huge crowds, the event holds several Guinness World Records including the largest annual comic and pop culture festival in the world.[12]						The convention was founded in 1970 by Shel Dorf, Richard Alf, Ken Krueger, Mike Towry, Barry Alfonso, Bob Sourk, and Greg Bear.[2][3] Detroit, Michigan-born, comics fan Shel Dorf, had, in the mid-1960s, mounted the Detroit Triple-Fan Fairs, one of the first commercial comics-fan conventions. When he moved to San Diego, California, in 1970,[13] he organized a one-day convention (Golden State Comic-Minicon) on March 21, 1970, "as a kind of 'dry run' for the larger convention he hoped to stage." Dorf went on to be associated with the convention as president or manager, variously, for years until becoming estranged from the organization.[14] Alf co-chaired the first convention with Krueger and became chairman in 1971.[3]		Following the initial gathering, Dorf's first three-day San Diego comics convention, the Golden State Comic-Con,[13] drew 300 people[15] and was held at the U.S. Grant Hotel[13] from August 1–3, 1970.[16] Other locations in the convention's early years included the El Cortez Hotel, the University of California, San Diego, and Golden Hall, before being moved to the San Diego Convention Center in 1991.[17] Richard Alf, chairman in 1971, has noted an early factor in the Con's growth was an effort "to expand the Comic-Con [organizing] committee base by networking with other fandoms such as the Society for Creative Anachronism and the Mythopoeic Society, among others. (We found a lot of talent and strength through diversity)."[18] By the late 1970s, the show had grown to such an extent that Bob Schreck recalled visiting with his then-boss Gary Berman of Creation Conventions and reflecting, "While [Berman] kept repeating (attempting to convince himself) 'This show's not any bigger than ours!' I was quietly walking the floor stunned and in awe of just how much bigger it really was. I was blown away."[19]		The convention is organized by a panel of 13 board members, 16 to 20 full-time and part-time workers, and 80 volunteers who assist via committees. Comic Con International is a non-profit organization, and proceeds of the event go to funding it, as well as the Alternative Press Expo (APE) and WonderCon.[17] The convention logo was designed by Richard Bruning and Josh Beatman in 1995. In September 2010, the convention announced that it would stay in San Diego through 2015.[20][21] In 2015, working with Lionsgate, a video channel was created to host Comic-Con related content.[22][23]		According to the San Diego Convention and Visitor's Bureau, the convention has an annual regional economic impact of $162.8 million,[24][25] with a $180 million economic impact in 2011.[26]		Along with panels, seminars, and workshops with comic book professionals, there are previews of upcoming feature films, and portfolio review sessions with top comic book and video game companies. The evenings include events such as awards ceremonies, the annual Masquerade costume contest, and the Comic-Con International Independent Film Festival, which showcases shorts and feature-length movies that do not have distribution or distribution deals.		Traditional events include an eclectic film program, screening rooms devoted to Japanese animation, gaming, programs such as cartoonist Scott Shaw!'s "Oddball Comics" slide show and animation expert Jerry Beck's program featuring TV's "worst cartoons ever", as well as over 350 hours of other programming on all aspects of comic books and pop culture.		Like most comic-book conventions, Comic-Con features a large floorspace for exhibitors. These include media companies such as movie studios and TV networks, as well as comic-book dealers and collectibles merchants. And like most comics conventions, Comic-Con includes an autograph area, as well as the Artists' Alley where comics artists can sign autographs and sell or do free sketches. Despite the name, artists' alleys can include writers and even models.		Academics and comic industry professionals annually hold the Comics Arts Conference at Comic-Con, presenting scholarly studies on comics as a medium.		In recent years, the number of television shows that are promoted far outnumber films. During the 2011 convention, at least 80 TV shows were represented, compared to about 35 films.[27] The shows not only promote in the exhibit halls, but also use screenings and panels of various actors, writers, producers, and others from their shows.		Examples of the wide variety of TV shows recently promoted include Arrowverse shows, Being Human, Bones, Burn Notice, Castle, Chuck, Grimm, MythBusters, Nikita, Once Upon a Time, Psych, Rick and Morty, Supernatural, The Simpsons, The Big Bang Theory, The Originals, The Vampire Diaries, Fringe, Lost Girl, Sanctuary, Torchwood/ Doctor Who, and Warehouse 13.		HBO and Showtime has big attractions with shows like Game of Thrones, Dexter, Shameless, and True Blood.[27][28][29]		In 2013, there were 1075 total panels held during the convention, the plurality of which were anime-focused (29%), followed by comic-focused panels (26%). 1036 vendors participated in the convention in 2013.[30]		There are at least 17 separate rooms in the convention center used for panels and screenings, ranging in size from 280 seats to 6,100 seats. The two biggest are Ballroom 20, which seats approximately 4,900;[31] and Hall H, which seats just over 6,100.[32]		The neighboring Hilton Bayfront is also used, with its main ballroom (Indigo) seating up to 2,600.[33][34] The other neighboring hotel, the Marriott Marquis & Marina, also hosts a lot of Comic-Con activity. Among other things, the hotel serves as the anime headquarters and is where the nighttime films are shown.[33]		In the 21st century, the convention has drawn toy and collectibles designers who sell "Comic-Con Exclusive" products. Such companies have included Lego, Hasbro, Funko, Gentle Giant LTD,[35] Mattel, NECA, ThinkGeek, Sideshow Collectibles, Entertainment Earth, Bif Bang Pow!, Mezco, Toynami, and Kotobukiya.[36][37] Most such exclusives are licensed properties of movie, comic book, and animation characters.		Comic-Con International has served as the setting for Mark Hamill's Comic Book: The Movie, and for an episode of the HBO television series Entourage, the latter of which, while set at the event, was not filmed there. Comic-Con also served as an excuse for the fictional characters Seth Cohen and Ryan Atwood's trip to Tijuana, Mexico in episode 7 ("The Escape") of the first season of TV series The O.C. The convention also featured prominently as a setting for the Numb3rs episode "Graphic". In season 4 of Beauty and the Geek, an episode was featured where the contestants traveled to Comic-Con 07 and were given a challenge to create their own superheroes. In an episode of Punk'd, Hilary Swank gets Punk'd after an "attack from talking robot". In season 5, episode six, of the Showtime show Weeds, attendees from Comic-Con 2009 are seen in Silas and Doug's medicinal marijuana club.		Comic-Con featured at some length in the 2011 movie Paul which stars Simon Pegg and Nick Frost.[38] Issue No. 72 of The Invincible Iron Man (1974) was set at the July–August 1974 Comic-Con at the El Cortez Hotel, and featured cameos by a few of the special guests.		Comic-Con is mentioned in the CBS television show The Big Bang Theory in several episodes, and in NBC's Chuck in the episode "Chuck Versus the Sandworm", as an event the characters enjoy attending.[39][40] On the Futurama episode "Lrrreconcilable Ndndifferences", the main characters attend the 3010 convention (with it being referred to as "Comic-Con Intergalactic" and the iconic eye logo now sporting multiple eyes), where Fry looks for approval for his own comic while Bender attends a panel from Matt Groening (creator of Futurama as well as The Simpsons) on his new show "Futurella" (a twist on the title of the show and a parody of its cancellation by Fox).		In "It's My Party and I'll Bang If I Want To", an episode of the 2011 season of The Real World: San Diego, the cast attends Comic-Con made up as zombies in order to pass out promotional flyers for the House of Blues, where they worked as part of their season work assignment.[41][42] Filmmaker Morgan Spurlock released a 2011 documentary feature film set at the convention, Comic-Con Episode IV: A Fan's Hope. Writer Robert Salkowitz also used the 2011 Comic-Con as a backdrop for his book Comic-Con and the Business of Pop Culture, an analysis of the comics industry's 21st-century dilemmas and what the future may hold.[43]		Every year Conan O'Brien has a week of live shows from Comic-Con.[44]		Comic-Con Magazine, formerly known as Update, is the official magazine of San Diego Comic-Con International, WonderCon, and Alternative Press Expo, published free by San Diego Comic-Con International in the United States. The seed of the Comic-Con Magazine was a short one-shot issue of The Spirit, based on Comic-Con and sold exclusively in 1976 at the San Diego Comic-Con International. The Comic-Con Magazine debuted as Update in July 2005 and mainly focused on the winners of the Eisner Awards.[74] The last Update issue appeared in July 2008;[75] then it went on hiatus. When it came back, it was as Comic-Con Magazine, which not only covered San Diego Comic-Con International, but also WonderCon and the Alternative Press Expo, more commonly known as APE. The new Comic-Con Magazine features interviews with Comic-Con attendees and complete coverage of the Comic-Con events.[76][77] The fourth issue of Comic-Con Magazine was a hybrid with Comic-Con's Souvenir Book with cover art by Alex Ross, in full color and exclusive to Comic-Con attendees.[76][78]		A large number of exhibitors from art, comics, games, film, TV and publishing are at Comic-con, including Scientology funded Galaxy Press, which only publishes works by L. Ron Hubbard, Scientology's founder |url=https://en.wikipedia.org/wiki/L._Ron_Hubbard |title=L. Ron Hubbard		Capacity attendance at Comic-Con in 2006 and 2007 has caused crowding issues. Concerns have been raised that the event is possibly too massive for the San Diego Convention Center, Comic-Con's home through at least 2021.[79] In 2006, Comic-Con, for the first time, had to close registration for a few hours on Saturday to accommodate crowds. In response, for 2007, Comic-Con introduced a new three-day membership that did not include Saturday. Nevertheless, the 2007 show went on to sell out Saturday, as well as Friday and Sunday for the first time. Additionally, both the four-day and three-day memberships sold out for the first time. For 2008, the three-day memberships were abandoned and the convention decided to sell memberships only in advance, with no on-site registration.[80] In 2008, all memberships were sold out before the convention for the first time ever. This sellout has given rise to the new phenomenon of Comic-Con memberships being scalped for exorbitant prices on websites such as eBay and Craigslist.[81]		In April 2008, David Glanzer, Comic-Con's director of marketing and public relations, commented on the organization's desire to remain in San Diego:		We've been approached by other cities, [but] I don't think anybody wants to leave San Diego. I certainly don't. It's a perfect fit for us. It's expensive, whether it be paying for the street signs that tell you what streets are closed, or for any police or the hall or any of the myriad things, it's expensive. But it's a great city. There's been some talk of expansion of the center, which we would certainly welcome. Hopefully if everything lines up, we will be here for many more years.[17]		Heidi McDonald reported on her blog The Beat as of October 7, 2009, Preview Night for the 2010 show had already sold out. Glazner explained the early sell-out:		For 2010 the decision was made to offer an option (of whether they wanted to attend Preview Night) to those who pre-registered for four-day badges. We limited the number of badges for Preview Night to the number of those who attended in 2008.[82]		Mark Evanier on his blog News from ME noted as of November 9, 2009, that all 4-day passes for the 2010 show had already been sold out.[83] On February 23, 2010, The Orange County Register reported that the larger Anaheim Convention Center in Anaheim would be making a bid to become the new home of Comic-Con starting in 2013.[84] On September 30, 2010, Comic Con announced that they had extended their stay up to 2015. The North County Times reported on July 26, 2010, that 4-day passes with access to Preview night for the 2011 Convention had sold out two hours before the 2010 convention closed.[85] Comic-Con International announced that 4-day passes for the 2014 convention (July 24–27) would no longer be available and only single days would be sold.[86] Due to overcrowding, organizers of the event capped attendance;[87] this cap has been in place since 2007.[88]		As of October 2013, a $520 million proposed expansion to the San Diego Convention Center received approval from the California Coastal Commission.[89] The proposed expansion would increase the available space within the convention center and had a target completion date of early 2016.[90] The expansion would add approximately 225,000 square feet of exhibit space, an additional 35%; and a brand-new 80,000 square foot ballroom, 20% larger than Hall H. The plan would also add a second tower to the Hilton Bayfront hotel, adding 500 rooms adjacent to the Convention Center.[91] Due to the proposed expansion of the convention center, Comic Con extended its contract for San Diego to 2016.[92] In 2014, convention center expansion was halted due to a lawsuit.[93] As of July 2015, convention center expansion is effectively frozen, partly because the city no longer has financing lined up for it (any financing plan would involve taxpayer money and would have to be approved by a public vote), and partly because the city lost the rights to the only contiguous parcel of land where expansion could occur.[94] Other cities, including Los Angeles, began to seek to have Comic-Con move out of San Diego;[95] In 2015, Comic-Con entered into negotiations with San Diego.[96] As a result of these negotiations, Comic-Con entered into a contract to stay in San Diego through 2018.[97] In 2017 the commitment to San Diego was extended to 2021.[98]		In 2012, a 53-year-old woman crossing at a red light was hit by a car and killed in the days leading up to the convention.[99] In 2013, a young woman attempted to jump off the balcony of a local high-rise, but nearby stuntmen prevented it.[100]		In 2014, multiple pedestrians marching in Comic-Con's ZombieWalk were struck by a car forcing its way through an intersection.[101] A 64-year-old woman sustained serious injuries to her arm; two others had minor injuries.[102][103]		The same year, a teenage cosplayer was initially thought to have been sexually assaulted early Sunday morning, and a suspect was arrested on Sunday at the San Diego Marriott Hotel and Marina.[104] Police later stated that the teenage girl was injured in a fall; the arrested individual was released without any charges.[105]		
The Big Bang Theory is an American television sitcom created by Chuck Lorre and Bill Prady, both of whom serve as executive producers on the series, along with Steven Molaro. All three also serve as head writers. The show premiered on CBS on September 24, 2007.[3] The series' tenth season premiered on September 19, 2016.[4] In March 2017, the series was renewed for two additional seasons, bringing its total to twelve, and running through the 2018–19 television season. The eleventh season is set to premiere on September 25, 2017.[5]		The show is primarily centered on five characters living in Pasadena, California: Leonard Hofstadter and Sheldon Cooper, both physicists at Caltech, who share an apartment; Penny, a waitress and aspiring actress who later becomes a pharmaceutical representative and who lives across the hall; and Leonard and Sheldon's similarly geeky and socially awkward friends and co-workers, aerospace engineer Howard Wolowitz and astrophysicist Raj Koothrappali. The geekiness and intellect of the four men are contrasted for comic effect with Penny's social skills and common sense.[6][7]		Over time, supporting characters have been promoted to starring roles: Leslie Winkle, a physicist who dated Leonard and Howard; neuroscientist Amy Farrah Fowler, who joins the group after being matched to Sheldon on a dating website (and later becomes Sheldon's girlfriend); Bernadette Rostenkowski, Howard's wife (previously his girlfriend), a microbiologist and former part-time waitress alongside Penny; Stuart Bloom, the cash-strapped owner of the comic book store the characters often visit; and Emily Sweeney, a dermatologist who dated Raj.		The structure of the original, unaired pilot, developed for the 2006–07 television season, was substantially different from the series' current form. The only characters retained in both pilots were Leonard (Johnny Galecki) and Sheldon (Jim Parsons), who are named after Sheldon Leonard, a longtime figure in episodic television as producer, director and actor. Althea (Vernee Watson) was a character featured in both pilots and the first series episode.[8] Two female leads were Canadian actress Amanda Walsh as Katie, "a street-hardened, tough-as-nails woman with a vulnerable interior,"[9][10] and Iris Bahr as Gilda, a scientist colleague and friend of the male characters. Sheldon and Leonard meet Katie after she breaks up with a boyfriend and they invite her to share their apartment. Gilda is threatened by Katie's presence. Test audiences reacted negatively to Katie, but they liked Sheldon and Leonard. The original pilot used Thomas Dolby's hit "She Blinded Me with Science" as its theme song.		Although the show was not picked up, its creators were given an opportunity to retool it and produce a second pilot. They brought in the remaining cast and retooled the show to its final format. Katie was replaced by Penny (Kaley Cuoco). The original unaired pilot never has officially been released, but it has circulated on the Internet. On the evolution of the show, Chuck Lorre said, "We did the 'Big Bang Pilot' about two and a half years ago, and it sucked ... but there were two remarkable things that worked perfectly, and that was Johnny and Jim. We rewrote the thing entirely and then we were blessed with Kaley and Simon and Kunal." As to whether the world will ever see the original pilot on a future DVD release, Lorre said, "Wow, that would be something. We will see. Show your failures..."[11]		The first and second pilots of The Big Bang Theory were directed by James Burrows, who did not continue with the show. The reworked second pilot led to a 13-episode order by CBS on May 14, 2007.[12] Prior to its airing on CBS, the pilot episode was distributed on iTunes free of charge. The show premiered on September 24, 2007, and was picked up for a full 22-episode season on October 19, 2007.[13] The show is filmed in front of a live audience,[14] and is produced by Warner Bros. Television and Chuck Lorre Productions.[15] Production was halted on November 6, 2007, due to the Writers Guild of America strike. Nearly three months later, on February 4, 2008, the series was temporarily replaced by a short-lived sitcom, Welcome to the Captain. The series returned on March 17, 2008, in an earlier time slot[16] and ultimately only 17 episodes were produced for the first season.[17][18]		After the strike ended, the show was picked up for a second season, airing in the 2008–2009 season, premiering in the same time slot on September 22, 2008.[19] With increasing ratings, the show received a two-year renewal through the 2010–11 season in 2009.[20][21] In 2011, the show was picked up for three more seasons.[22] In March 2014, the show was renewed again for three more years through the 2016–17 season. Therefore, the series will at least reach 10 seasons. This marks the second time the series has gained a three-year renewal.[23] In March 2017, the series was renewed for two additional seasons, bringing its total to 12, and running through the 2018–19 television season.[24]		David Saltzberg, a professor of physics and astronomy at the University of California, Los Angeles, checks scripts and provides dialogue, mathematics equations, and diagrams used as props.[6] According to executive producer/co-creator Bill Prady, "We're working on giving Sheldon an actual problem that he's going to be working on throughout the [first] season so there's actual progress to the boards ... . We worked hard to get all the science right."[7]		Several of the actors in The Big Bang Theory previously worked together on the sitcom Roseanne, including Johnny Galecki, Sara Gilbert, Laurie Metcalf (who plays Sheldon's mother, Mary Cooper), and Meagen Fay (who plays Bernadette's mother). Additionally, Lorre was a writer on the series for several seasons.		The Canadian alternative rock band Barenaked Ladies wrote and recorded the show's theme song, which describes the history and formation of the universe and the Earth. Co-lead singer Ed Robertson was asked by Lorre and Prady to write a theme song for the show after the producers attended one of the band's concerts in Los Angeles. By coincidence, Robertson had recently read Simon Singh's book Big Bang,[25][26] and at the concert improvised a freestyle rap about the origins of the universe.[citation needed] Lorre and Prady phoned him shortly thereafter and asked him to write the theme song. Having been asked to write songs for other films and shows, but ending up being rejected because of producer's favor of other artist's song, Robertson agreed to write the theme only after learning that Lorre and Prady had not asked anyone else.[citation needed]		On October 9, 2007, a full-length (1 minute and 45 seconds) version of the song was released commercially.[27] Although some sources identify the song title as "History of Everything,"[28] the cover art for the single identifies the title as "Big Bang Theory Theme." A music video also was released via special features on The Complete Fourth Season DVD and Blu-ray set.[29][30] The theme was included on the band's greatest hits album, Hits from Yesterday & the Day Before, released on September 27, 2011.[31] In September 2015, TMZ uncovered court documents showing that Steven Page sued former bandmate Robertson over the song, alleging that he was promised 20% of the proceeds, but that Robertson has kept that money entirely for himself.[32]		For the first three seasons, Galecki, Parsons, and Cuoco, the three main stars of the show, received at most $60,000 per episode. The salary for the three went up to $200,000 per episode for the fourth season. Their per-episode pay went up an additional $50,000 in each of the following three seasons, culminating in $350,000 per episode in the seventh season.[33][34] In September 2013, Bialik and Rauch renegotiated the contracts they held since they were introduced to the series in 2010. On their old contracts, each was making $20,000–$30,000 per episode, while the new contracts doubled that, beginning at $60,000 per episode, increasing steadily to $100,000 per episode by the end of the contract, as well as adding another year for both.[35]		By season seven, Galecki, Parsons, and Cuoco were also receiving 0.25% of the series' back-end money. Before production began on the eighth season, the three plus Helberg and Nayyar, looked to renegotiate new contracts, with Galecki, Parsons, and Cuoco seeking around $1 million per episode, as well as more back-end money.[36] Contracts were signed in the beginning of August 2014, giving the three principal actors an estimated $1 million per episode for three years, with the possibility to extend for a fourth year. The deals also include larger pieces of the show, signing bonuses, production deals, and advances towards the back-end.[37] Helberg and Nayyar were also able to renegotiate their contracts, giving them a per-episode pay in the "mid-six-figure range", up from around $100,000 per episode they each received in years prior. The duo, who were looking to have salary parity with Parsons, Galecki, and Cuoco, signed their contracts after the studio and producers threatened to write the characters out of the series if a deal could not be reached before the start of production on season eight.[38] By season 10, Helberg and Nayyar reached the $1 million per episode parity with Parsons, Galecki, and Cuoco, due to a clause in their deals signed in 2014.[39]		In March 2017, the main cast members (Galecki, Parsons, Cuoco, Helberg, and Nayyar) took a 10% pay cut so Bialik and Rauch could increase their earnings from $100,000 per episode to $450,000.[40] This put Galecki, Parsons, Cuoco, Helberg and Nayyar at $900,000 per episode, with Parsons, Galecki, and Helberg also receiving overall deals with Warner Bros. Television.[24] By the end of April, Bialik and Rauch had signed deals to earn $500,000 per episode, each, with the deals also including a separate development component for both actors. The deal was an increase from the $175,000 - $200,000 the duo had been making per episode.[41]		These actors are credited in all episodes of the series:		These actors were first credited as guest stars and later promoted to main cast:		As the theme of the show revolves around science, many distinguished and high-profile scientists have appeared as guest stars on the show. Famous astrophysicist and Nobel laureate George Smoot had a cameo appearance in the second season.[56] Theoretical physicist Brian Greene also appeared on the show in the fourth season. Astrophysicist, science populizer, and physics outreach specialist Neil deGrasse Tyson also appeared in the fourth season.		Cosmologist Stephen Hawking made a short guest appearance in the fifth season episode;[57] in the eighth season, Hawking video conferences with Sheldon and Leonard, and makes another appearance in the 200th episode. In the fifth and sixth seasons, NASA astronaut Michael J. Massimino played himself multiple times in the role of Howard's fellow astronaut. Bill Nye appeared in the seventh season, and Elon Musk made an appearance in the ninth season.		Much of the series focuses on science, particularly physics. The four main male characters are employed at Caltech and have science-related occupations, as do Bernadette and Amy. The characters frequently banter about scientific theories or news (notably around the start of the show), and make science-related jokes.		Science has also interfered with the characters' romantic lives. Leslie breaks up with Leonard when he sides with Sheldon in his support for string theory rather than loop quantum gravity.[69] When Leonard joins Sheldon, Raj, and Howard on a three-month Arctic research trip, it separates Leonard and Penny at a time their relationship is budding. When Bernadette takes an interest in Leonard's work, it makes both Penny and Howard envious and results in Howard confronting Leonard, and Penny asking Sheldon to teach her physics.[70] Sheldon and Amy also briefly end their relationship after an argument over which of their fields is superior.[71]		David Saltzberg, who has a PhD in physics, has served as the science consultant for the show for six seasons and attends every taping.[72] While Saltzberg knows physics, he sometimes needs assistance from Mayim Bialik, who has a PhD in neuroscience.[73] Saltzberg sees early versions of scripts which need scientific information added to them, and he also points out where the writers, despite their knowledge of science, have made a mistake. He is usually not needed during a taping unless a lot of science, and especially the whiteboard, is involved.[73]		The four main male characters are all avid science fiction, fantasy, and comic book fans and memorabilia collectors.		Star Trek in particular is frequently referenced and Sheldon identifies strongly with the character of Spock, so much so that when he is given a used napkin signed by Leonard Nimoy as a Christmas gift from Penny he is overwhelmed with excitement and gratitude ("I possess the DNA of Leonard Nimoy?!").[74] Star Trek: The Original Series cast member George Takei has made a cameo, and Leonard Nimoy made a cameo as the voice of Sheldon's vintage Mr. Spock action figure (both cameos were in dream sequences). Star Trek: The Next Generation cast members Brent Spiner and LeVar Burton have had cameos as themselves,[75][76] while Wil Wheaton has a recurring role as a fictionalized version of himself.		They are also fans of Star Wars, Battlestar Galactica, and Doctor Who. In the episode "The Ornithophobia Diffusion", when there is a delay in watching Star Wars on Blu-ray, Howard complains, "If we don't start soon, George Lucas is going to change it again" (referring to Lucas' controversial alterations to the films) and in "The Hot Troll Deviation", Katee Sackhoff of Battlestar Galactica appeared as Howard's fantasy dream girl. The characters have different tastes in franchises with Sheldon praising Firefly but disapproving of Leonard's enjoyment of Babylon 5.[77][n 1] With regard to fantasy, the four make frequent references to The Lord of the Rings and Harry Potter novels and movies. Additionally, Howard can speak Sindarin, one of the two Elvish languages from The Lord of the Rings.		Wednesday night is the group's designated "comic book night"[78] because that is the day of the week when new comic books are released. The comic book store is run by fellow geek and recurring character Stuart. On a number of occasions, the group members have dressed up as pop culture characters, including The Flash, Aquaman, Frodo Baggins, Superman, Batman, Spock, The Doctor, Green Lantern, and Thor.[79] As a consequence of losing a bet to Stuart and Wil Wheaton, the group members are forced to visit the comic book store dressed as Catwoman, Wonder Woman, Batgirl, and Supergirl.[80] DC Comics announced that, to promote its comics, the company will sponsor Sheldon wearing Green Lantern T-shirts.[81]		Various games have been featured, as well as referenced, on the series (e.g. World of Warcraft, Halo, Mario, etc.), including fictional games like Mystic Warlords of Ka'a (which became a reality in 2011)[82] and Rock-paper-scissors-lizard-Spock.		One of the recurring plot lines is the relationship between Leonard and Penny. Leonard becomes attracted to Penny in the pilot episode and his need to do favors for her is a frequent point of humor in the first season. Their first long term relationship begins when Leonard returns from a three-month expedition to the North Pole in the season 3 premiere. However, when Leonard tells Penny that he loves her, she realizes she cannot say it back. Both Leonard and Penny go on to date other people; most notably with Leonard dating Raj's sister Priya for much of season 4. This relationship is jeopardized when Leonard comes to falsely believe that Raj has slept with Penny, and ultimately ends when Priya sleeps with a former boyfriend in "The Good Guy Fluctuation".		Penny, who admits to missing Leonard in "The Roommate Transmogrification", accepts his request to renew their relationship in "The Beta Test Initiation". After Penny suggests having sex in "The Launch Acceleration", Leonard breaks the mood by proposing to her. Penny says "no" but does not break up with him. She stops a proposal a second time in "The Tangible Affection Proof". In the sixth-season episode, "The 43 Peculiarity", Penny finally tells Leonard that she loves him. Although they both feel jealousy when the other receives significant attention from the opposite sex, Penny is secure enough in their relationship to send him off on an exciting four-month expedition without worrying in "The Bon Voyage Reaction". After Leonard returns, their relationship blossoms over the seventh season. In the penultimate episode "The Gorilla Dissolution", Penny admits that they should marry and when Leonard realizes that she is serious, he proposes with a ring that he had been saving for years. Leonard and Penny decide to elope to Las Vegas in the season 8 finale, but beforehand, wanting no secrets, Leonard admits to kissing another woman, Mandy Chow (Melissa Tang) while on an expedition on the North Sea. Despite this, Leonard and Penny finally elope in the season 9 premiere.		In the third-season finale, Raj and Howard search for a woman compatible with Sheldon and discover neurobiologist Amy Farrah Fowler. Like him, she has a history of social ineptitude and participates in online dating only to fulfill an agreement with her mother. This spawns a storyline in which Sheldon and Amy communicate daily while insisting to Leonard and Penny that they are not romantically involved. In "The Agreement Dissection", Sheldon and Amy talk in her apartment after a night of dancing and she kisses him on the lips. Instead of getting annoyed, Sheldon says "fascinating" and later asks Amy to be his girlfriend in "The Flaming Spittoon Acquisition". The same night he draws up "The Relationship Agreement" to verify the ground rules of him as her boyfriend and vice versa (similar to his "Roommate Agreement" with Leonard). Amy agrees but later regrets not having had a lawyer read through it.		In the episode "The Launch Acceleration", Amy tries to use her "neurobiology bag of tricks" to increase the attraction between herself and Sheldon. In the final fifth-season episode "The Countdown Reflection", Sheldon takes Amy's hand as Howard is launched into space. In the sixth season first episode "The Date Night Variable", after a dinner in which Sheldon fails to live up to this expectation, Amy gives Sheldon an ultimatum that their relationship is over unless he tells her something from his heart. Amy accepts Sheldon's romantic speech even after learning that it is a line from the first Spider-Man movie. In "The Cooper/Kripke Inversion" Sheldon states that he has been working on his discomfort about physical contact and admits that "it's a possibility" that he could one day have sex with Amy. Amy is revealed to have similar feelings in "The Love Spell Potential". Sheldon explains that he never thought about intimacy with anyone before Amy.[83]		"The Locomotive Manipulation" is the first episode in which Sheldon initiates a kiss with Amy. Although initially done in a fit of sarcasm, he discovers that he enjoys the feeling. Consequently, Sheldon slowly starts to open up over the rest of the season, and starts a more intimate relationship with Amy. However, in the season finale, Sheldon leaves temporarily to cope with several changes and Amy becomes distraught. However, in "The Prom Equivalency" he hides in his room to avoid going to a mock prom reenactment with her. In the resulting stand-off, Amy is about to confess that she loves Sheldon, but he surprises her by saying that he loves her too. This prompts Amy to have a panic attack.		In the season eight finale, Sheldon and Amy get into a fight about commitment on their 5-year anniversary. Amy tells Sheldon that she needs to think about the future of their relationship, unaware that Sheldon was about to propose to her. Season nine sees Sheldon harassing Amy about making up her mind until she breaks up with him. Both struggle with singlehood and trying to be friends for the next few weeks until they reunite in episode ten and have sex for the first time on Amy's birthday.		In scenes set at Howard's home, he interacts with his rarely-seen mother (voiced by Carol Ann Susi until her death) by shouting from room to room in the house. She similarly interacts with other characters in this manner.[84] She reflects the Jewish mother stereotype in some ways, such as being overly controlling of Howard's adult life and sometimes trying to make him feel guilty about causing her trouble. She is dependent on Howard, as she requires him to help her with her wig and makeup in the morning. Howard in turn is attached to his mother to the point where she still cuts his meat for him, takes him to the dentist, does his laundry and "grounds" him when he returns home after briefly moving out.[85] Until Howard's marriage to Bernadette in the fifth-season finale, Howard's former living situation led Leonard's psychiatrist mother to speculate that he may suffer from some type of pathology,[86] and Sheldon to refer to their relationship as Oedipal.[87] In season 8, Howard's mother dies in her sleep while in Florida, which devastates Howard and Stuart, who briefly lived with Mrs. Wolowitz.		Like most shows created by Chuck Lorre, The Big Bang Theory ends by showing a vanity card written by Lorre after the credits, followed by the Warner Bros. Television closing logo. These cards are archived on Lorre's website.[88]		Through the use of his vanity cards at the end of episodes, Lorre alleged that the program had been plagiarized by a show produced and aired in Belarus. Officially titled Теоретики (The Theorists), the show features "clones" of the main characters, a similar opening sequence, and what appears to be a very close Russian translation of the scripts.[89] Lorre expressed annoyance and described his inquiry with the Warner Bros. legal department about options. The television production company and station's close relationship with the Belarus government was cited as the reason that any attempt to claim copyright infringement would be in vain because the company copying the episodes is operated by the government.[90]		However, no legal action was required to end production of the other show: as soon as it became known that the show was unlicensed, the actors quit and the producers cancelled it.[91] Dmitriy Tankovich (who plays Leonard's counterpart, "Seva") said in an interview, "I'm upset. At first, the actors were told all legal issues were resolved. We didn't know it wasn't the case, so when the creators of The Big Bang Theory started talking about the show, I was embarrassed. I can't understand why our people first do, and then think. I consider this to be the rock bottom of my career. And I don't want to take part in a stolen show".[92]		Initial reception for the series was mixed. The review aggregation website Rotten Tomatoes reported a 55% approval rating for the first season based on reviews from 22 critics, with an average rating of 5.18/10. The website's critical consensus reads, "The Big Bang Theory brings a new class of character to mainstream television, but much of the comedy feels formulaic and stiff."[93] On Metacritic, the season holds a score of 57 out of 100, based on reviews from 23 critics, indicating "mixed or average reviews".[94] Later seasons received more acclaim and in 2013, TV Guide ranked the series #52 on its list of the 60 Best Series of All Time.[95]		The Big Bang Theory started off slowly in the ratings, failing to make the top 50 in its first season (ranking 68th), and ranking 40th in its second season. When the third season premiered on September 21, 2009, however, The Big Bang Theory ranked as CBS's highest-rated show of that evening in the adults 18–49 demographic (4.6/10) along with a then-series-high 12.83 million viewers.[96] After the first three seasons aired at different times on Monday nights, CBS moved the show to Thursdays at 8:00 ET for the 2010–2011 schedule, to be in direct competition with NBC's Comedy Block and Fox's American Idol (then the longest reigning leading primetime show on U.S. television from 2004 to 2011).[97] During its fourth season, it became television's highest rated comedy, just barely beating out eight-year champ Two and a Half Men. However, in the age 18–49 demographic (the show's target age range), it was the second highest rated comedy, behind ABC's Modern Family. The fifth season opened with viewing figures of over 14 million.[98]		The sixth season boasts some of the highest-rated episodes for the show so far, with a then-new series high set with "The Bakersfield Expedition", with 20 million viewers,[99] a first for the series, which along with NCIS, made CBS the first network to have two scripted series reach that large an audience in the same week since 2007. In the sixth season, the show became the highest rated and viewed scripted show in the 18–49 demographic, trailing only the live regular NBC Sunday Night Football coverage,[100][101] and was third in total viewers, trailing NCIS and Sunday Night Football.[102] Season seven of the series opened strong, continuing the success gained in season six, with the second episode of the premiere, "The Deception Verification", setting the new series high in viewers with 20.44 million.[103][104]		Showrunner Steve Molaro, who took over from Bill Prady with the sixth season, credits some of the show's success to the sitcom's exposure in off-network syndication, particularly on TBS, while Michael Schneider of TV Guide attributes it to the timeslot move two seasons earlier. Chuck Lorre and CBS Entertainment president Nina Tassler also credit the success to the influence of Molaro, in particular the deepening exploration of the firmly established regular characters and their interpersonal relationships, such as the on-again, off-again relationship between Leonard and Penny.[105] Throughout much of the 2012–13 season, The Big Bang Theory placed first in all of syndication ratings, receiving formidable competition from only Judge Judy and Wheel of Fortune (first-run syndication programs). By the end of the 2012–13 television season, The Big Bang Theory had dethroned Judge Judy as the ratings leader in all of syndicated programming with 7.1, Judy descending to second place for that season with a 7.0.[106] The Big Bang Theory did not place first in syndication ratings for the 2013–14 television season, beaten out by Judge Judy.[107]		The show made its United Kingdom debut on Channel 4 on February 14, 2008. The show was also shown as a 'first-look' on Channel 4's digital offshoot E4 prior to the main channel's airing. While the show's ratings were not deemed strong enough for the main channel, they were considered the opposite for E4. For each following season, all episodes were shown first-run on E4, with episodes only aired on the main channel in a repeat capacity, usually on a weekend morning. From the third season, the show aired in two parts, being split so that it could air new episodes for longer throughout the year. This was due to rising ratings. The first part began airing on December 17, 2009 at 9:00 p.m. while the second part, containing the remaining eleven episodes, began airing in the same time period from May 6, 2010. The first half of the fourth season began airing on November 4, 2010, at 9:00 p.m., drawing 877,000 viewers, with a further 256,000 watching on the E4+1 hour service. This gave the show an overall total of 1.13 million viewers, making it E4's most watched programme for that week. The increased ratings continued over subsequent weeks.[137]		The fourth season's second half began on June 30, 2011. Season 5 began airing on November 3, 2011 at 8:00 p.m. as part of E4's Comedy Thursdays, acting as a lead-in to the channel's newest comedy, Perfect Couples. Episode 19, the highest-viewed episode of the season, attracted 1.4 million viewers.[138] Season 6 premiered on November 15, 2012, with 1.89 million viewers and a further 469,000 on the time shift channel, bringing the total to 2.31 million, E4's highest viewing ratings of 2012, and the highest the channel had received since June 2011. The sixth season returned in mid 2013 to finish airing the remaining episodes.[139] Season 7 premiered on E4 on October 31, 2013 at 8:30pm and hit multiple ratings records this season. The second half of season seven aired in mid 2014.[140] The eighth season premiered on E4 on October 23, 2014 at 8:30 p.m.[141] During its eighth season, The Big Bang Theory shared its 8:30 p.m. time period with fellow CBS comedy, 2 Broke Girls. Following the airing of the first eight episodes of that show's fourth season, The Big Bang Theory returned to finish airing its eighth season on March 19, 2015.[142]		Netflix UK & Ireland announced on February 13, 2016 that seasons 1–8 would be available to stream from February 15, 2016.[143]		The Big Bang Theory started off quietly in Canada, but managed to garner major success in later seasons. The Big Bang Theory is telecast throughout Canada via the CTV Television Network in simultaneous substitution with cross-border CBS affiliates. Now immensely popular in Canada, The Big Bang Theory is also rerun daily on the Canadian cable channel The Comedy Network.		The season 4 premiere garnered an estimated 3.1 million viewers across Canada. This is the largest audience for a sitcom since the series finale of Friends (12.4 million viewers). The Big Bang Theory has pulled ahead and has now become the most-watched entertainment television show in Canada.[144]		The Big Bang Theory premiered in the United States on September 24, 2007 on CBS. The series debuted in Canada on CTV in September 2007.[145] On February 14, 2008, the series debuted in the United Kingdom on channels E4 and Channel 4.[146] In Australia the first seven seasons of the series began airing on the Seven Network and 7mate from October 2015 and also gained the rights to season 8 in 2016, though the Nine Network has rights to air seasons nine & ten.[147][148]		In May 2010, it was reported that the show had been picked up for syndication, mainly among Fox's owned and operated stations and other local stations, with Warner Bros. Television's sister cable network TBS holding the show's cable syndication rights. Broadcast of old shows began airing in September 2011. TBS now airs the series in primetime on Tuesdays, Wednesdays, and Thursdays, with evening broadcasts on Saturdays (TBS's local sister station in Atlanta also holds local weeknight rights to the series).[149] Although details of the syndication deal have not been revealed, it was reported the deal "set a record price for a cable off-network sitcom purchase".[150] CTV holds national broadcast syndication rights in Canada, while sister cable network The Comedy Network holds cable rights.		Warner Bros. Television controls the online rights for the show.[151][152] Full episodes are available at tv.com, while short clips and recently aired full episodes are available on cbs.com.[153] In Canada, recent episode(s) and pictures are available on CTV.ca.[154] After the show has aired in New Zealand the shows are available in full online at TVNZ's on demand web service.		The first and second seasons were only available on DVD upon their time of release in 2008[182] and 2009.[183] Starting with the release of the third season in 2010[184] and continuing every year with every new season, a Blu-ray disc set has also been released in conjunction with the DVD. In 2012, Warner Bros. released the first two seasons on Blu-ray,[185] marking the first time that all episodes were available on the Blu-ray disc format.		In August 2009, the sitcom won the best comedy series TCA award and Jim Parsons (Sheldon) won the award for individual achievement in comedy.[186] In 2010, the show won the People's Choice Award for Favorite Comedy, while Parsons won a Primetime Emmy Award for Outstanding Lead Actor in a Comedy Series.[187] On January 16, 2011, Parsons was awarded a Golden Globe for Best Performance by an Actor in a Television Series – Comedy or Musical, an award that was presented by co-star Kaley Cuoco. On September 18, 2011, Parsons was again awarded an Emmy for Best Actor in a Comedy Series. On January 9, 2013, the show won People's Choice Award for Favorite Comedy for a second time. August 25, 2014, Jim Parsons was awarded an Emmy for Best Actor in a Comedy Series.[188] The Big Bang Theory also won the 2016 People's Choice Awards for under Favorite TV Show and Favorite Network TV Comedy with Jim Parsons winning Favorite Comedic TV Actor.[189] On January 20, 2016, The Big Bang Theory also won the International category at the UK's National Television Awards.[190]		On March 16, 2014, a Lego Ideas project[191] portraying the living room scene in Lego style with the main cast as minifigures reached 10,000 supporters on the platform, which qualified it to be considered as an official set by the Lego Ideas review board. On November 7, 2014, Lego Ideas approved the design and began refining it. The set was released in August 2015, with an exclusive pre-sale taking place at the San Diego Comic-Con International.[citation needed]		In November 2016, it was reported that CBS was in negotiations to create a spin-off of The Big Bang Theory centered on Sheldon as a young boy. The prequel series, described as "a Malcolm in the Middle-esque single-camera family comedy" would be executive produced by Lorre and Molaro, with Prady expected to be involved in some capacity, and intended to air in the 2017–18 season alongside The Big Bang Theory.[192][193] The initial idea for the series came from Parsons, who passed it along to The Big Bang Theory producers.[194] In early March 2017, Iain Armitage was cast as the younger Sheldon, as well as Zoe Perry as his mother, Mary Cooper. Perry is the real-life daughter of Laurie Metcalf, who portrays Mary Cooper on The Big Bang Theory.[194]		On March 13, 2017, CBS ordered the spin-off Young Sheldon series. Jon Favreau will direct and executive produce the pilot. Created by Lorre and Molaro, the series follows 9-year-old Sheldon Cooper as he attends high school in East Texas. Alongside Armitage as 9-year-old Sheldon Cooper and Perry as Mary Cooper, Lance Barber stars as George Cooper, Sheldon's father; Raegan Revord stars as Missy Cooper, Sheldon's twin sister; and Montana Jordan as George Cooper, Sheldon's older brother. Jim Parsons reprises his role as adult Sheldon Cooper, as narrator for the series. Parsons, Lorre, Molaro and Todd Spiewak will also serve as executive producers on the series, for Chuck Lorre Productions, Inc. in association with Warner Bros. Television.[195] It is scheduled to begin airing on November 2, 2017, after The Big Bang Theory. It will have a special preview on September 25, 2017.[196]		
African Americans (also referred to as Black Americans or Afro-Americans[3]) are an ethnic group of Americans with total or partial ancestry from any of the black racial groups of Africa.[4][5] The term may also be used to include only those individuals who are descended from enslaved Africans.[6][7] As a compound adjective the term is usually hyphenated as African-American.[8][9]		Black and African Americans constitute the third largest racial and ethnic group in the United States (after White Americans and Hispanic and Latino Americans).[10] Most African Americans are descendants of enslaved peoples within the boundaries of the present United States.[11][12] On average, African Americans are of West/Central African and European descent, and some also have Native American ancestry.[13] According to US Census Bureau data, African immigrants generally do not self-identify as African American. The overwhelming majority of African immigrants identify instead with their own respective ethnicities (~95%).[14] Immigrants from some Caribbean, Central American and South American nations and their descendants may or may not also self-identify with the term.[9]		African-American history starts in the 16th century, with peoples from West Africa forcibly taken as slaves to Spanish America, and in the 17th century with West African slaves taken to English colonies in North America. After the founding of the United States, black people continued to be enslaved, with four million denied freedom from bondage prior to the Civil War.[15] Due largely to notions of white supremacy, they were treated as second-class citizens. The Naturalization Act of 1790 limited U.S. citizenship to whites only, and only white men of property could vote.[16][17] These circumstances were changed by Reconstruction, development of the black community, participation in the great military conflicts of the United States, the elimination of racial segregation, and the Civil Rights Movement which sought political and social freedom. In 2008, Barack Obama became the first African American to be elected President of the United States.[18]						The first African slaves arrived via Santo Domingo to the San Miguel de Gualdape colony (most likely located in the Winyah Bay area of present-day South Carolina), founded by Spanish explorer Lucas Vázquez de Ayllón in 1526.[19]		The ill-fated colony was almost immediately disrupted by a fight over leadership, during which the slaves revolted and fled the colony to seek refuge among local Native Americans. De Ayllón and many of the colonists died shortly afterwards of an epidemic and the colony was abandoned. The settlers and the slaves who had not escaped returned to Haiti, whence they had come.[19]		The first recorded Africans in British North America (including most of the future United States) were "20 and odd negroes" who came to Jamestown, Virginia via Cape Comfort in August 1619 as indentured servants.[20] As English settlers died from harsh conditions, more and more Africans were brought to work as laborers.[21]		Typically, young men or women would sign a contract of indenture in exchange for transportation to the New World. The landowner received 50 acres of land from the state (headrights) for each servant purchased (around £6 per person, equivalent to 9 months income in the 17th century) from a ship's captain.[citation needed]		An indentured servant (who could be white or black) would work for several years (usually four to seven) without wages. The status of indentured servants in early Virginia and Maryland was similar to slavery. Servants could be bought, sold, or leased and they could be physically beaten for disobedience or running away. Unlike slaves, they were freed after their term of service expired or was bought out, their children did not inherit their status, and on their release from contract they received "a year's provision of corn, double apparel, tools necessary", and a small cash payment called "freedom dues".[22]		Africans could legally raise crops and cattle to purchase their freedom.[23] They raised families, married other Africans and sometimes intermarried with Native Americans or English settlers.[24]		By the 1640s and 1650s, several African families owned farms around Jamestown and some became wealthy by colonial standards and purchased indentured servants of their own. In 1640, the Virginia General Court recorded the earliest documentation of lifetime slavery when they sentenced John Punch, a Negro, to lifetime servitude under his master Hugh Gwyn for running away.[25][26]		One of Dutch African arrivals, Anthony Johnson, would later own one of the first black "slaves", John Casor, resulting from the court ruling of a civil case.[27][28]		The popular conception of a race-based slave system did not fully develop until the 18th century. The Dutch West India Company introduced slavery in 1625 with the importation of eleven black slaves into New Amsterdam (present-day New York City). All the colony's slaves, however, were freed upon its surrender to the British.[29]		Massachusetts was the first British colony to legally recognize slavery in 1641. In 1662 Virginia passed a law that children of enslaved women (who were of African descent and thus foreigners) took the status of the mother, rather than that of the father, as under English common law. This principle was called partus sequitur ventrum.[30][31]		By an act of 1699, the colony ordered all free blacks deported, virtually defining as slaves all persons of African descent who remained in the colony.[32] In 1670 the colonial assembly passed a law prohibiting free and baptized negroes (and Indians) from purchasing Christians (in this act meaning English or European whites) but allowing them to buy persons "of their owne nation".[33]		The earliest African-American congregations and churches were organized before 1800 in both northern and southern cities following the Great Awakening. By 1775, Africans made up 20% of the population in the American colonies, which made them the second largest ethnic group after the English.[34]		During the 1770s, Africans, both enslaved and free, helped rebellious English colonists secure American Independence by defeating the British in the American Revolution.[35] Africans and Englishmen fought side by side and were fully integrated.[36]		Blacks played a role in both sides in the American Revolution. Activists in the Patriot cause included James Armistead, Prince Whipple and Oliver Cromwell.[37]		Slavery had been tacitly enshrined in the U.S. Constitution through provisions such as Article I, Section 2, Clause 3, commonly known as the 3/5 compromise. Slavery, which by then meant almost exclusively African Americans, was the most important political issue in the antebellum United States, leading to one crisis after another. Among these were the Missouri Compromise, the Compromise of 1850, the Fugitive Slave Act, and the Dred Scott decision.		By 1860, there were 3.5 to 4.4 million enslaved blacks in the United States due to the Atlantic slave trade, and another 488,000–500,000 African Americans lived free (with legislated limits)[38] across the country.[39][40] With legislated limits imposed upon them in addition to "unconquerable prejudice" from whites according to Henry Clay,[41] some blacks who weren't enslaved left the U.S. for Liberia in Africa.[38] Liberia began as a settlement of the American Colonization Society (ACS) in 1821, with the abolitionist members of the ACS believing blacks would face better chances for freedom and equality in Africa.[38]		The slaves not only constituted a large investment, they produced America's most valuable product and export: cotton. They not only helped build the U.S. Capitol, they built the White House and other District of Columbia buildings. (Washington was a slave trading center.[42]) Similar building projects existed in slaveholding states.		In 1863, during the American Civil War, President Abraham Lincoln signed the Emancipation Proclamation. The proclamation declared that all slaves in Confederate-held territory were free.[43] Advancing Union troops enforced the proclamation with Texas being the last state to be emancipated, in 1865.[44]		Slavery in Union-held Confederate territory continued, at least on paper, until the passage of the Thirteenth Amendment in 1865.[45] Prior to the Civil War, only white men of property could vote, and the Naturalization Act of 1790 limited U.S. citizenship to whites only.[16][17] The 14th Amendment (1868) gave African-Americans citizenship, and the 15th Amendment (1870) gave African-American males the right to vote (only males could vote in the U.S. at the time).		African Americans quickly set up congregations for themselves, as well as schools and community/civic associations, to have space away from white control or oversight. While the post-war Reconstruction era was initially a time of progress for African Americans, that period ended in 1876. By the late 1890s, Southern states enacted Jim Crow laws to enforce racial segregation and disenfranchisement.[46] Most African Americans obeyed the Jim Crow laws, in order to avoid racially motivated violence. To maintain self-esteem and dignity, African Americans such as Anthony Overton and Mary McLeod Bethune continued to build their own schools, churches, banks, social clubs, and other businesses.[47]		In the last decade of the 19th century, racially discriminatory laws and racial violence aimed at African Americans began to mushroom in the United States, a period often referred to as the "nadir of American race relations". These discriminatory acts included racial segregation—upheld by the United States Supreme Court decision in Plessy v. Ferguson in 1896—which was legally mandated by southern states and nationwide at the local level of government, voter suppression or disenfranchisement in the southern states, denial of economic opportunity or resources nationwide, and private acts of violence and mass racial violence aimed at African Americans unhindered or encouraged by government authorities.[48]		The desperate conditions of African Americans in the South sparked the Great Migration of the early 20th century which led to a growing African-American community in the Northern United States.[50] The rapid influx of blacks disturbed the racial balance within Northern cities, exacerbating hostility between both black and white Northerners. Urban riots—whites attacking blacks—became a northern problem.[51] The Red Summer of 1919 was marked by hundreds of deaths and higher casualties across the U.S. as a result of race riots that occurred in more than three dozen cities, such as the Chicago race riot of 1919 and the Omaha race riot of 1919. Overall, blacks in Northern cities experienced systemic discrimination in a plethora of aspects of life. Within employment, economic opportunities for blacks were routed to the lowest-status and restrictive in potential mobility. Within the housing market, stronger discriminatory measures were used in correlation to the influx, resulting in a mix of "targeted violence, restrictive covenants, redlining and racial steering".[52] While many whites defended their space with violence, intimidation, or legal tactics toward African Americans, many other whites migrated to more racially homogeneous suburban or exurban regions, a process known as white flight.[53]		By the 1950s, the Civil Rights Movement was gaining momentum. A 1955 lynching that sparked public outrage about injustice was that of Emmett Till, a 14-year-old boy from Chicago. Spending the summer with relatives in Money, Mississippi, Till was killed for allegedly having wolf-whistled at a white woman. Till had been badly beaten, one of his eyes was gouged out, and he was shot in the head. The visceral response to his mother's decision to have an open-casket funeral mobilized the black community throughout the U.S.[54] Vann R. Newkirk| wrote "the trial of his killers became a pageant illuminating the tyranny of white supremacy".[54] The state of Mississippi tried two defendants, but they were speedily acquitted by an all-white jury.[55] One hundred days after Emmett Till's murder, Rosa Parks refused to give up her seat on the bus in Alabama—indeed, Parks told Emmett's mother Mamie Till that "the photograph of Emmett’s disfigured face in the casket was set in her mind when she refused to give up her seat on the Montgomery bus."[56]		The March on Washington for Jobs and Freedom and the conditions which brought it into being are credited with putting pressure on Presidents John F. Kennedy and Lyndon B. Johnson. Johnson put his support behind passage of the Civil Rights Act of 1964 that banned discrimination in public accommodations, employment, and labor unions, and the Voting Rights Act of 1965, which expanded federal authority over states to ensure black political participation through protection of voter registration and elections. By 1966, the emergence of the Black Power movement, which lasted from 1966 to 1975, expanded upon the aims of the Civil Rights Movement to include economic and political self-sufficiency, and freedom from white authority.[57]		During the postwar period, many African Americans continued to be economically disadvantaged relative to other Americans. Average black income stood at 54 percent of that of white workers in 1947, and 55 percent in 1962. In 1959, median family income for whites was $5,600, compared with $2,900 for nonwhite families. In 1965, 43 percent of all black families fell into the poverty bracket, earning under $3,000 a year. The Sixties saw improvements in the social and economic conditions of many black Americans.[58]		From 1965 to 1969, black family income rose from 54 to 60 percent of white family income. In 1968, 23 percent of black families earned under $3,000 a year, compared with 41 percent in 1960. In 1965, 19 percent of black Americans had incomes equal to the national median, a proportion that rose to 27 percent by 1967. In 1960, the median level of education for blacks had been 10.8 years, and by the late Sixties the figure rose to 12.2 years, half a year behind the median for whites.[58]		Politically and economically, African Americans have made substantial strides during the post-civil rights era. In 1989, Douglas Wilder became the first African American elected governor in U.S. history. Clarence Thomas became the second African-American Supreme Court Justice. In 1992 Carol Moseley-Braun of Illinois became the first African-American woman elected to the U.S. Senate. There were 8,936 black officeholders in the United States in 2000, showing a net increase of 7,467 since 1970. In 2001 there were 484 black mayors.[59]		In 2005, the number of Africans immigrating to the United States, in a single year, surpassed the peak number who were involuntarily brought to the United States during the Atlantic Slave Trade.[60] On November 4, 2008, Democratic Senator Barack Obama defeated Republican Senator John McCain to become the first African American to be elected President. At least 95 percent of African-American voters voted for Obama.[61][62] He also received overwhelming support from young and educated whites, a majority of Asians,[63] Hispanics,[63] and Native Americans[64][not in citation given] picking up a number of new states in the Democratic electoral column.[61][62] Obama lost the overall white vote, although he won a larger proportion of white votes than any previous nonincumbent Democratic presidential candidate since Jimmy Carter.[65] Four years later, Obama was reelected president by a similar margin on November 6, 2012.[citation needed]		In 1790, when the first U.S. Census was taken, Africans (including slaves and free people) numbered about 760,000—about 19.3% of the population. In 1860, at the start of the Civil War, the African-American population had increased to 4.4 million, but the percentage rate dropped to 14% of the overall population of the country. The vast majority were slaves, with only 488,000 counted as "freemen". By 1900, the black population had doubled and reached 8.8 million.[citation needed]		In 1910, about 90% of African Americans lived in the South. Large numbers began migrating north looking for better job opportunities and living conditions, and to escape Jim Crow laws and racial violence. The Great Migration, as it was called, spanned the 1890s to the 1970s. From 1916 through the 1960s, more than 6 million black people moved north. But in the 1970s and 1980s, that trend reversed, with more African Americans moving south to the Sun Belt than leaving it.[66]		The following table of the African-American population in the United States over time shows that the African-American population, as a percentage of the total population, declined until 1930 and has been rising since then.		By 1990, the African-American population reached about 30 million and represented 12% of the U.S. population, roughly the same proportion as in 1900.[68]		At the time of the 2000 Census, 54.8% of African Americans lived in the South. In that year, 17.6% of African Americans lived in the Northeast and 18.7% in the Midwest, while only 8.9% lived in the western states. The west does have a sizable black population in certain areas, however. California, the nation's most populous state, has the fifth largest African-American population, only behind New York, Texas, Georgia, and Florida. According to the 2000 Census, approximately 2.05% of African Americans identified as Hispanic or Latino in origin,[10] many of whom may be of Brazilian, Puerto Rican, Dominican, Cuban, Haitian, or other Latin American descent. The only self-reported ancestral groups larger than African Americans are the Irish and Germans.[69] Because many African Americans trace their ancestry to colonial American origins, some simply self-identify as "American".[citation needed]		According to the 2010 US Census, nearly 3% of people who self-identified as black had recent ancestors who immigrated from another country. Self-reported non-Hispanic black immigrants from the Caribbean, mostly from Jamaica and Haiti, represented 0.9% of the US population, at 2.6 million.[70] Self-reported black immigrants from Sub-Saharan Africa also represented 0.9%, at about 2.8 million.[70] Additionally, self-identified Black Hispanics represented 0.4% of the United States population, at about 1.2 million people, largely found within the Puerto Rican and Dominican communities.[71] Self-reported black immigrants hailing from other countries in the Americas, such as Brazil and Canada, as well as several European countries, represented less than 0.1% of the population. Mixed-Race Hispanic and non-Hispanic Americans who identified as being part black, represented 0.9% of the population. Of the 12.6% of United States residents who identified as black, around 10.3% were "native black American" or ethnic African Americans, who are direct descendants of West/Central Africans brought to the U.S. as slaves. These individuals make up well over 80% of all blacks in the country. When including people of mixed-race origin, about 13.5% of the US population self-identified as black or "mixed with black".[72] However, according to the U.S. census bureau, evidence from the 2000 Census indicates that many African and Caribbean immigrant ethnic groups do not identify as "Black, African Am., or Negro". Instead, they wrote in their own respective ethnic groups in the "Some Other Race" write-in entry. As a result, the census bureau devised a new, separate "African American" ethnic group category in 2010 for ethnic African Americans.[73] Following lobbying led by the Arab American Institute, a national organization representing Arab Americans, the census bureau also announced in 2014 that it may establish an additional new ethnic category for populations from the Middle East, North Africa and the Arab world.[74]		Almost 58% of African Americans lived in metropolitan areas in 2000. With over 2 million black residents, New York City had the largest black urban population in the United States in 2000, overall the city has a 28% black population. Chicago has the second largest black population, with almost 1.6 million African Americans in its metropolitan area, representing about 18 percent of the total metropolitan population.[citation needed]		After 100 years of African-Americans leaving the south in large numbers seeking better opportunities in the west and north, a movement known as the Great Migration, there is now a reverse trend, called the New Great Migration. A growing percentage of African-Americans from the west and north are migrating to the southern region of the U.S. for economic and cultural reasons. New York City, Chicago, and Los Angeles have the highest decline in African Americans, while Atlanta, Dallas, and Houston have the highest increase respectively.[75]		Among cities of 100,000 or more, Detroit, Michigan had the highest percentage of black residents of any U.S. city in 2010, with 82%. Other large cities with African-American majorities include Jackson, Mississippi (79.4%), Miami Gardens, Florida (76.3%), Baltimore, Maryland (63%), Birmingham, Alabama (62.5%), Memphis, Tennessee (61%), New Orleans, Louisiana (60%), Montgomery, Alabama (56.6%), Flint, Michigan (56.6%), Savannah, Georgia (55.0%), Augusta, Georgia (54.7%), Atlanta, Georgia (54%, see African Americans in Atlanta), Cleveland, Ohio (53.3%), Newark, New Jersey (52.35%), Washington, D.C. (50.7%), Richmond, Virginia (50.6%), Mobile, Alabama (50.6%), Baton Rouge, Louisiana (50.4%), and Shreveport, Louisiana (50.4%).		The nation's most affluent community with an African-American majority resides in View Park–Windsor Hills, California with an annual median income of $159,618.[76] Other largely affluent predominately African-American communities include Prince George's County in Maryland (namely Mitchellville, Woodmore, and Upper Marlboro), Dekalb County in Georgia, Charles City County in Virginia, Baldwin Hills in California, Hillcrest and Uniondale in New York, and Cedar Hill, DeSoto, and Missouri City in Texas. Queens County, New York is the only county with a population of 65,000 or more where African Americans have a higher median household income than White Americans.[77]		Seatack, Virginia is currently the oldest African-American community in the United States.[78] It survives today with a vibrant and active civic community.[79]		By 2012, African Americans had advanced greatly in education attainment. They still lagged overall compared to white or Asian Americans but surpassed other ethnic minorities, with 19 percent earning bachelor's degrees and 6 percent earning advanced degrees.[80] Between 1995 and 2009, freshmen college enrollment for African Americans increased by 73 percent and only 15 percent for whites.[81] Black women are enrolled in college more than any other race and gender group, leading all with 9.7% enrolled according to the 2011 U.S. Census Bureau.[82][83] Predominantly black schools for kindergarten through twelfth grade students were common throughout the U.S. before the 1970s. By 1972, however, desegregation efforts meant that only 25% of Black students were in schools with more than 90% non-white students. However, since then, a trend towards re-segregation affected communities across the country: by 2011, 2.9 million African-American students were in such overwhelmingly minority schools, including 53% of Black students in school districts that were formerly under desegregation orders.[84][85]		Historically black colleges and universities (HBCUs), which were originally set up when segregated colleges did not admit African Americans, continue to thrive and educate students of all races today. The majority of HBCUs were established in the southeastern United States, Alabama has the most HBCUs of any state.[86][87]		As late as 1947, about one third of African Americans over 65 were considered to lack the literacy to read and write their own names. By 1969, illiteracy as it had been traditionally defined, had been largely eradicated among younger African Americans.[88]		US Census surveys showed that by 1998, 89 percent of African Americans aged 25 to 29 had completed a high-school education, less than whites or Asians, but more than Hispanics. On many college entrance, standardized tests and grades, African Americans have historically lagged behind whites, but some studies suggest that the achievement gap has been closing. Many policy makers have proposed that this gap can and will be eliminated through policies such as affirmative action, desegregation, and multiculturalism.[89]		The average high school graduation rate of blacks in the United States has steadily increased to 71% in 2013.[90] Separating this statistic into component parts shows it varies greatly depending upon the state and the school district examined. 38% of black males graduated in the state of New York but in Maine 97% graduated and exceeded the white male graduation rate by 11 percentage points.[91] In much of the southeastern United States and some parts of the southwestern United States the graduation rate of white males was in fact below 70% such as in Florida where a 62% of white males graduated high school. Examining specific school districts paints an even more complex picture. In the Detroit school district the graduation rate of black males was 20% but 7% for white males. In the New York City school district 28% of black males graduate high school compared to 57% of white males. In Newark County[where?] 76% of black males graduated compared to 67% for white males. Further academic improvement has occurred in 2015. Roughly 23% of all blacks have bachelor's degrees. In 1988, 21% of whites had obtained a bachelor's degree versus 11% of blacks. In 2015, 23% of blacks had obtained a bachelor's degree versus 36% of whites.[92] Foreign born blacks, 9% of the black population, made even greater strides. They exceed native born blacks by 10 percentage points.[92]		In Chicago, Marva Collins, an African-American educator, created a low cost private school specifically for the purpose of teaching low-income African-American children whom the public school system had labeled as being "learning disabled".[93] One article about Marva Collins' school stated,		Working with students having the worst of backgrounds, those who were working far below grade level, and even those who had been labeled as 'unteachable,' Marva was able to overcome the obstacles. News of third grade students reading at ninth grade level, four-year-olds learning to read in only a few months, outstanding test scores, disappearance of behavioral problems, second-graders studying Shakespeare, and other incredible reports, astounded the public.[94]		During the 2006–2007 school year, Collins' school charged $5,500 for tuition, and parents said that the school did a much better job than the Chicago public school system.[95] Meanwhile, during the 2007–2008 year, Chicago public school officials claimed that their budget of $11,300 per student was not enough.[96]		Economically, African Americans have benefited from the advances made during the Civil Rights era, particularly among the educated, but not without the lingering effects of historical marginalization when considered as a whole. The racial disparity in poverty rates has narrowed. The black middle class has grown substantially. In 2010, 45% of African Americans owned their homes, compared to 67% of all Americans.[98] The poverty rate among African Americans has decreased from 26.5% in 1998 to 24.7% in 2004, compared to 12.7% for all Americans.[99]		African Americans have a combined buying power of over $892 billion currently and likely over $1.1 trillion by 2012.[101][102] In 2002, African American-owned businesses accounted for 1.2 million of the US's 23 million businesses.[103] As of 2011[update] African American-owned businesses account for approximately 2 million US businesses.[104] Black-owned businesses experienced the largest growth in number of businesses among minorities from 2002 to 2011.[104]		In 2004, African-American men had the third-highest earnings of American minority groups after Asian Americans and non-Hispanic whites.[105]		Twenty-five percent of blacks had white-collar occupations (management, professional, and related fields) in 2000, compared with 33.6% of Americans overall.[106][107] In 2001, over half of African-American households of married couples earned $50,000 or more.[107] Although in the same year African Americans were over-represented among the nation's poor, this was directly related to the disproportionate percentage of African-American families headed by single women; such families are collectively poorer, regardless of ethnicity.[107]		In 2006, the median earnings of African-American men was more than black and non-black American women overall, and in all educational levels.[108][109][110][111][112] At the same time, among American men, income disparities were significant; the median income of African-American men was approximately 76 cents for every dollar of their European American counterparts, although the gap narrowed somewhat with a rise in educational level.[108][113]		Overall, the median earnings of African-American men were 72 cents for every dollar earned of their Asian American counterparts, and $1.17 for every dollar earned by Hispanic men.[108][111][114] On the other hand, by 2006, among American women with post-secondary education, African-American women have made significant advances; the median income of African-American women was more than those of their Asian-, European- and Hispanic American counterparts with at least some college education.[109][110][115]		The US public sector is the single most important source of employment for African Americans.[116] During 2008–2010, 21.2% of all Black workers were public employees, compared with 16.3% of non-Black workers.[116] Both before and after the onset of the Great Recession, African Americans were 30% more likely than other workers to be employed in the public sector.[116]		The public sector is also a critical source of decent-paying jobs for Black Americans. For both men and women, the median wage earned by Black employees is significantly higher in the public sector than in other industries.[116]		In 1999, the median income of African-American families was $33,255 compared to $53,356 of European Americans. In times of economic hardship for the nation, African Americans suffer disproportionately from job loss and underemployment, with the black underclass being hardest hit. The phrase "last hired and first fired" is reflected in the Bureau of Labor Statistics unemployment figures. Nationwide, the October 2008 unemployment rate for African Americans was 11.1%,[117] while the nationwide rate was 6.5%.[118]		The income gap between black and white families is also significant. In 2005, employed blacks earned 65% of the wages of whites, down from 82% in 1975.[99] The New York Times reported in 2006 that in Queens, New York, the median income among African-American families exceeded that of white families, which the newspaper attributed to the growth in the number of two-parent black families. It noted that Queens was the only county with more than 65,000 residents where that was true.[77]		In 2011, it was reported that 72% of black babies were born to unwed mothers.[119] The poverty rate among single-parent black families was 39.5% in 2005, according to Williams, while it was 9.9% among married-couple black families. Among white families, the respective rates were 26.4% and 6% in poverty.[120]		The life expectancy for Black men in 2008 was 70.8 years.[121] Life expectancy for Black women was 77.5 years in 2008.[121] In 1900, when information on Black life expectancy started being collated, a Black man could expect to live to 32.5 years and a Black woman 33.5 years.[121] In 1900, White men lived an average of 46.3 years and White women lived an average of 48.3 years.[121] African-American life expectancy at birth is persistently five to seven years lower than European Americans.[122]		Black people have higher rates of obesity, diabetes and hypertension than the US average.[121] For adult Black men, the rate of obesity was 31.6% in 2010.[123] For adult Black women, the rate of obesity was 41.2% in 2010.[123] African Americans have higher rates of mortality than does any other racial or ethnic group for 8 of the top 10 causes of death.[124] In 2013, among men, black men had the highest rate of getting cancer, followed by white, Hispanic, Asian/Pacific Islander (A/PI), and American Indian/Alaska Native (AI/AN) men. Among women, white women had the highest rate of getting cancer, followed by black, Hispanic, Asian/Pacific Islander, and American Indian/Alaska Native women.[125] Violence has an impact upon African-American life expectancy. A report from the U.S. Department of Justice states "In 2005, homicide victimization rates for blacks were 6 times higher than the rates for whites".[126] The report also found that "94% of black victims were killed by blacks."[126]		AIDS is one of the top three causes of death for African-American men aged 25–54 and for African-American women aged 35–44 years. In the United States, African Americans make up about 48% of the total HIV-positive population and make up more than half of new HIV cases. The main route of transmission for women is through unprotected heterosexual sex. African-American women are 19 times more likely to contract HIV than other women.[127]		Washington, D.C. has the nation's highest rate of HIV/AIDS infection, at 3%. This rate is comparable to what is seen in West Africa, and is considered a severe epidemic.[128] Dr. Ray Martins, Chief Medical Officer at the Whitman-Walker Clinic, the largest provider of HIV care in Washington D.C., estimated that the actual underlying percent with HIV/AIDS in the city is "closer to five percent".[128]		According to a Gallup survey conducted from June to September 2012, 4.6 percent of Black or African Americans self identify as LGBT; this is greater than the estimated 3.4 percent of American adults that self identify as LGBT in the total population.[129]		The majority of African Americans are Protestant, many of whom follow the historically black churches.[130] The term Black church refers to churches which minister to predominantly African-American congregations. Black congregations were first established by freed slaves at the end of the 17th century, and later when slavery was abolished more African Americans were allowed to create a unique form of Christianity that was culturally influenced by African spiritual traditions.[131]		According to a 2007 survey, more than half of the African-American population are part of the historically black churches.[132] The largest Protestant denomination among African Americans are the Baptists,[133] distributed mainly in four denominations, the largest being the National Baptist Convention, USA and the National Baptist Convention of America.[134] The second largest are the Methodists,[135] the largest denominations are the African Methodist Episcopal Church and the African Methodist Episcopal Zion Church.[134][136]		Pentecostals are distributed among several different religious bodies, with the Church of God in Christ as the largest among them by far.[134] About 16% of African-American Christians are members of white Protestant communions,[135] these denominations (which include the United Church of Christ) mostly have a 2 to 3% African-American membership.[137] There are also large numbers of Roman Catholics, constituting 5% of the African-American population.[132] Of the total number of Jehovah's Witnesses, 22% are black.[130]		Some African Americans follow Islam. Historically, between 15 and 30% of enslaved Africans brought to the Americas were Muslims, but most of these Africans were converted to Christianity during the era of American slavery.[138] During the twentieth century, some African Americans converted to Islam, mainly through the influence of black nationalist groups that preached with distinctive Islamic practices; including the Moorish Science Temple of America, and the largest organization, the Nation of Islam, founded in the 1930s, which attracted at least 20,000 people by 1963,[139][140] prominent members included activist Malcolm X and boxer Muhammad Ali.[141]		Malcolm X is considered the first person to start the movement among African Americans towards mainstream Islam, after he left the Nation and made the pilgrimage to Mecca.[142] In 1975, Warith Deen Mohammed, the son of Elijah Muhammad took control of the Nation after his father's death and guided the majority of its members to orthodox Islam.[143] However, a few members rejected these changes, in particular Louis Farrakhan, who revived the Nation of Islam in 1978 based on its original teachings.[citation needed]		African-American Muslims constitute 20% of the total U.S. Muslim population,[144] the majority are Sunni or orthodox Muslims, some of these identify under the community of W. Deen Mohammed.[145][146] The Nation of Islam led by Louis Farrakhan has a membership ranging from 20,000–50,000 members.[147]		There are relatively few African-American Jews; estimates of their number range from 20,000[148] to 200,000.[149] Most of these Jews are part of mainstream groups such as the Reform, Conservative, or Orthodox branches of Judaism; although there are significant numbers of people who are part of non-mainstream Jewish groups, largely the Black Hebrew Israelites, whose beliefs include the claim that African Americans are descended from the Biblical Israelites.[150]		Confirmed atheists are less than one half of one-percent, similar to numbers for Hispanics.[151][152][153]		African Americans have a long and diverse history of business ownership. Although the first African-American business is unknown, slaves captured from West Africa are believed to have established commercial enterprises as peddlers and skilled craftspeople as far back as the 17th century. Around 1900, Booker T. Washington became the most famous proponent of African American businesses. His critic and rival W.E.B. DuBois also commended business as a vehicle for African American advancement.[154]		African American Vernacular English (AAVE) is a variety (dialect, ethnolect, and sociolect) of American English, commonly spoken by urban working-class and largely bi-dialectal middle-class African Americans.[155] Non-linguists sometimes call it Ebonics (a term that also has other meanings and connotations).[citation needed]		African American Vernacular English evolved during the antebellum period through interaction between speakers of 16th and 17th century English of Great Britain and Ireland and various West African languages. As a result, the variety shares parts of its grammar and phonology with the Southern American English dialect. Where African American Vernacular English differs from Standard American English (SAE) is in certain pronunciation characteristics, tense usage and grammatical structures that were derived from West African languages, particularly those belonging to the Niger-Congo family.[156]		Virtually all habitual speakers of African American Vernacular English can understand and communicate in Standard American English. As with all linguistic forms, AAVE's usage is influenced by various factors, including geographical, educational and socioeconomic background, as well as formality of setting.[156] Additionally, there are many literary uses of this variety of English, particularly in African-American literature.[citation needed]		Some of the new words used by the people include "fleek" which means on point and "throwing shade" which means offending someone.[157]		Recent surveys of African Americans using a genetic testing service have found varied ancestries which show different tendencies by region and sex of ancestors. These studies found that on average, African Americans have 73.2-82.1% West African, 16.7%-24% European, and 0.8–1.2% Native American genetic heritage, with large variation between individuals.[159][160][161] Genetics websites themselves have reported similar ranges, with some finding 1 or 2 percent Native American ancestry and Ancestry.com reporting an outlying percentage of European ancestry among African Americans, 29%.[162]		According to a genome-wide study by Bryc et al. (2009), the overall ancestry of African Americans was formed through historic admixture between West/Central Africans (more frequently females) and Europeans (more frequently males). Consequently, the 365 African Americans in their sample have a genome-wide average of 78.1% West African ancestry and 18.5% European ancestry, with large variation among individuals (ranging from 99% to 1% West African ancestry). The West African ancestral component in African Americans is most similar to that in present-day speakers from the non-Bantu branches of the Niger-Congo (Niger-Kordofanian) family.[159][nb 1]		Correspondingly, Montinaro et al. (2014) observed that around 50% of the overall ancestry of African Americans traces back to the Niger-Congo-speaking Yoruba of southwestern Nigeria and southern Benin, reflecting the centrality of this West Africa region in the Atlantic Slave Trade. The next most frequent ancestral component found among African Americans was derived from Great Britain, in keeping with historical records. It constitutes a little over 10% of their overall ancestry, and is most similar to the Northwest European ancestral component also carried by Barbadians.[164] Zakharaia et al. (2009) found a similar proportion of Yoruba associated ancestry in their African-American samples, with a minority also drawn from Mandenka and Bantu populations. Additionally, the researchers observed an average European ancestry of 21.9%, again with significant variation between individuals.[158] Bryc et al. (2009) note that populations from other parts of the continent may also constitute adequate proxies for the ancestors of some African-American individuals; namely, ancestral populations from Guinea Bissau, Senegal and Sierra Leone in West Africa and Angola in Southern Africa.[159]		Altogether, genetic studies suggest that African Americans are a multiracial people. According to DNA analysis led in 2006 by Penn State geneticist Mark D. Shriver, around 58 percent of African Americans have at least 12.5% European ancestry (equivalent to one European great-grandparent and his/her forebears), 19.6 percent of African Americans have at least 25% European ancestry (equivalent to one European grandparent and his/her forebears), and 1 percent of African Americans have at least 50% European ancestry (equivalent to one European parent and his/her forebears).[13][165] According to Shriver, around 5 percent of African Americans also have at least 12.5% Native American ancestry (equivalent to one Native American great-grandparent and his/her forebears).[166][167]		According to a Y-DNA study by Sims et al. (2007), the majority (~60%) of African Americans belong to various subclades of the E3a (E1b1a) paternal haplogroup. This is the most common genetic paternal lineage found today among West/Central African males, and is also a signature of the historical Bantu migrations. The next most frequent Y-DNA haplogroup observed among African Americans is the R1b clade, which around 15% of African Americans carry. This lineage is most common today among Northwestern European males. The remaining African Americans mainly belong to the paternal haplogroup I (~7%), which is also frequent in Northwestern Europe.[168]		According to an mtDNA study by Salas et al. (2005), the maternal lineages of African Americans are most similar to haplogroups that are today especially common in West Africa (>55%), followed closely by West-Central Africa and Southwestern Africa (<41%). The characteristic West African haplogroups L1b, L2b,c,d, and L3b,d and West-Central African haplogroups L1c and L3e in particular occur at high frequencies among African Americans. As with the paternal DNA of African Americans, contributions from other parts of the continent to their maternal gene pool are insignificant.[169]		African-American names are part of the cultural traditions of African Americans. Prior to the 1950s and 1960s, most African-American names closely resembled those used within European American culture.[170] Babies of that era were generally given a few common names, with children using nicknames to distinguish the various people with the same name. With the rise of 1960s civil rights movement, there was a dramatic increase in names of various origins.[171]		By the 1970s and 1980s, it had become common among African Americans to invent new names for themselves, although many of these invented names took elements from popular existing names. Prefixes such as La/Le, Da/De, Ra/Re and Ja/Je, and suffixes like -ique/iqua, -isha and -aun/-awn are common, as are inventive spellings for common names. The book Baby Names Now: From Classic to Cool--The Very Last Word on First Names places the origins of "La" names in African-American culture in New Orleans.[172]		Even with the rise of inventive names, it is still common for African Americans to use biblical, historical, or traditional European names. Daniel, Christopher, Michael, David, James, Joseph, and Matthew were thus among the most frequent names for African-American boys in 2013.[170][173][174]		The name LaKeisha is typically considered American in origin, but has elements of it that were drawn from both French and West/Central African roots. Other names like LaTanisha, JaMarcus, DeAndre, and Shaniqua were created in the same way. Punctuation marks are seen more often within African-American names than other American names, such as the names Mo'nique and D'Andre.[170]		African Americans have improved their social and economic standing significantly since the Civil Rights Movement and recent decades have witnessed the expansion of a robust, African -middle class across the United States. Unprecedented access to higher education and employment in addition to representation in the highest levels of American government has been gained by African Americans in the post-civil rights era.[citation needed]		One of the most serious and long-standing issues within African-American communities is poverty. Poverty is associated with higher rates of marital stress and dissolution, physical and mental health problems, disability, cognitive deficits, low educational attainment, and crime.[175] In 2004, almost 25% of African-American families lived below the poverty level.[99] In 2007, the average income for African Americans was approximately $34,000, compared to $55,000 for whites.[176] Forty percent of prison inmates are African American.[177] African Americans experience a higher rate of unemployment than the general population.[178] African American males are more likely to be killed by police.[179] This is one of the factors that led to the creation of the Black Lives Matter movement.[180]		Collectively, African Americans are more involved in the American political process than other minority groups in the United States, indicated by the highest level of voter registration and participation in elections among these groups in 2004.[181] African Americans collectively attain higher levels of education than immigrants to the United States.[181] African Americans also have the highest level of Congressional representation of any minority group in the U.S.[182]		A large majority of African Americans support the Democratic Party. In the 2004 Presidential Election, Democrat John Kerry received 88% of the African-American vote compared to 11% for Republican George W. Bush.[183] Although there is an African-American lobby in foreign policy, it has not had the impact that African-American organizations have had in domestic policy.[184]		Many African Americans were excluded from electoral politics in the decades following the end of Reconstruction. For those that could participate, until the New Deal, African Americans were supporters of the Republican Party because it was Republican President Abraham Lincoln who helped in granting freedom to American slaves; at the time, the Republicans and Democrats represented the sectional interests of the North and South, respectively, rather than any specific ideology, and both right and left were represented equally in both parties.		The African-American trend of voting for Democrats can be traced back to the 1930s during the Great Depression, when Franklin D. Roosevelt's New Deal program provided economic relief to African Americans; Roosevelt's New Deal coalition turned the Democratic Party into an organization of the working class and their liberal allies, regardless of region. The African-American vote became even more solidly Democratic when Democratic presidents John F. Kennedy and Lyndon B. Johnson pushed for civil rights legislation during the 1960s. In 1960, nearly a third of African Americans voted for Republican Richard Nixon.[185]		After over 50 years, marriage rates for all Americans began to decline while divorce rates and out-of-wedlock births have climbed.[186] These changes have been greatest among African Americans. After more than 70 years of racial parity black marriage rates began to fall behind whites.[186] Single-parent households have become common, and according to US census figures released in January 2010, only 38 percent of black children live with both their parents.[187]		In 2008, Democrats overwhelmingly voted 70% against California Proposition 8, African Americans voted 58% in favor of it while 42% voted against Proposition 8.[188] On May 9, 2012, Barack Obama, the first black president, became the first US president to support same-sex marriage. After Obama's endorsement there is a rapid growth in support for same-sex marriage among African Americans. Now 59% of African Americans support same-sex marriage, which is higher than support among the national average (53%) and white Americans (50%).[189]		Polls in North Carolina,[190] Pennsylvania,[191] Missouri,[192] Maryland,[193] Ohio,[194] Florida,[195] and Nevada[196] have also shown an increase in support for same sex marriage among African Americans. On November 6, 2012, Maryland, Maine, and Washington all voted for approve of same-sex marriage, along with Minnesota rejecting a constitutional amendment banning same-sex marriage. Exit polls in Maryland show about 50% of African Americans voted for same-sex marriage, showing a vast evolution among African Americans on the issue and was crucial in helping pass same-sex marriage in Maryland.[197]		Blacks hold far more conservative opinions on abortion, extramarital sex, and raising children out of wedlock than Democrats as a whole.[198] On financial issues, however, African Americans are in line with Democrats, generally supporting a more progressive tax structure to provide more government spending on social services.[199]		African Americans have fought in every war in the history of the United States.[200]		The gains made by African Americans in the Civil Rights Movement and in the Black Power movement not only obtained certain rights for African Americans, but changed American society in far-reaching and fundamentally important ways. Prior to the 1950s, Black Americans in the South were subject to de jure discrimination, or Jim Crow laws. They were often the victims of extreme cruelty and violence, sometimes resulting in deaths: by the post World War II era, African Americans became increasingly discontented with their long-standing inequality. In the words of Martin Luther King, Jr., African Americans and their supporters challenged the nation to "rise up and live out the true meaning of its creed that all men are created equal ..."[201]		The Civil Rights Movement marked an enormous change in American social, political, economic and civic life. It brought with it boycotts, sit-ins, nonviolent demonstrations and marches, court battles, bombings and other violence; prompted worldwide media coverage and intense public debate; forged enduring civic, economic and religious alliances; and disrupted and realigned the nation's two major political parties.		Over time, it has changed in fundamental ways the manner in which blacks and whites interact with and relate to one another. The movement resulted in the removal of codified, de jure racial segregation and discrimination from American life and law, and heavily influenced other groups and movements in struggles for civil rights and social equality within American society, including the Free Speech Movement, the disabled, the women's movement, Native Americans, and migrant workers.		Some activists and academics contend that news media coverage of African-American news concerns or dilemmas is inadequate[202][203][204] or the news media present distorted images of African Americans.[205] To combat this, Robert L. Johnson founded Black Entertainment Television, a network that targets young African Americans and urban audiences in the United States. Most programming on the network consists of rap and R&B music videos and urban-oriented movies and series. The channel also shows syndicated television series, original programs, and some public affairs programs. On Sunday mornings, BET broadcasts a lineup of network-produced Christian programming; other, non-affiliated Christian programs are also shown during the early morning hours daily. BET is now a global network that reaches 90 million households in the United States, Caribbean, Canada, and the United Kingdom.[206]		In addition to BET there is Centric, which is a spin-off cable television channel of BET, created originally as BET on Jazz to showcase jazz music-related programming, especially that of black jazz musicians. Programming has been expanded to include a block of urban programs as well as some R&B, soul, and world music.[207]		TV One is another African-American-oriented network and a direct competitor to BET, targeting African-American adults with a broad range of programming. The network airs original lifestyle and entertainment-oriented shows, movies, fashion and music programming, as well as classic series such as 227, Good Times, Martin, Boston Public and It's Showtime at the Apollo. The network primarily owned by Radio One. Founded and controlled by Catherine Hughes, it is one of the nation's largest radio broadcasting companies and the largest African-American-owned radio broadcasting company in the United States.[208]		Other African-American networks scheduled to launch in 2009 are the Black Television News Channel founded by former Congressman J. C. Watts and Better Black Television founded by Percy Miller.[209][210] In June 2009, NBC News launched a new website named The Grio[211] in partnership with the production team that created the black documentary film Meeting David Wilson. It is the first African-American video news site that focuses on underrepresented stories in existing national news. The Grio consists of a broad spectrum of original video packages, news articles, and contributor blogs on topics including breaking news, politics, health, business, entertainment and Black History.[212]		From their earliest presence in North America, African Americans have significantly contributed literature, art, agricultural skills, cuisine, clothing styles, music, language, and social and technological innovation to American culture. The cultivation and use of many agricultural products in the United States, such as yams, peanuts, rice, okra, sorghum, grits, watermelon, indigo dyes, and cotton, can be traced to West African and African-American influences. Notable examples include George Washington Carver, who created 300 products from peanuts, 118 products from sweet potatoes, and 75 products from pecans; and George Crum, a local legend associates him with the creation of the potato chip in 1853.[213][214] Soul food is a variety of cuisine popular among African Americans. It is closely related to the cuisine of the Southern United States. The descriptive terminology may have originated in the mid-1960s, when soul was a common definer used to describe African-American culture (for example, soul music). African Americans were the first peoples in the United States to make fried chicken, along with Scottish immigrants to the South. Although the Scottish had been frying chicken before they emigrated, they lacked the spices and flavor that African Americans had used when preparing the meal. The Scottish American settlers therefore adopted the African-American method of seasoning chicken.[215] However, fried chicken was generally a rare meal in the African-American community, and was usually reserved for special events or celebrations.[216]		African-American music is one of the most pervasive African-American cultural influences in the United States today and is among the most dominant in mainstream popular music. Hip hop, R&B, funk, rock and roll, soul, blues, and other contemporary American musical forms originated in black communities and evolved from other black forms of music, including blues, doo-wop, barbershop, ragtime, bluegrass, jazz, and gospel music.		African-American-derived musical forms have also influenced and been incorporated into virtually every other popular music genre in the world, including country and techno. African-American genres are the most important ethnic vernacular tradition in America, as they have developed independent of African traditions from which they arise more so than any other immigrant groups, including Europeans; make up the broadest and longest lasting range of styles in America; and have, historically, been more influential, interculturally, geographically, and economically, than other American vernacular traditions.[217]		African Americans have also had an important role in American dance. Bill T. Jones, a prominent modern choreographer and dancer, has included historical African-American themes in his work, particularly in the piece "Last Supper at Uncle Tom's Cabin/The Promised Land". Likewise, Alvin Ailey's artistic work, including his "Revelations" based on his experience growing up as an African American in the South during the 1930s, has had a significant influence on modern dance. Another form of dance, Stepping, is an African-American tradition whose performance and competition has been formalized through the traditionally black fraternities and sororities at universities.[218]		Many African-American authors have written stories, poems, and essays influenced by their experiences as African Americans. African-American literature is a major genre in American literature. Famous examples include Langston Hughes, James Baldwin, Richard Wright, Zora Neale Hurston, Ralph Ellison, Nobel Prize winner Toni Morrison, and Maya Angelou.		African-American inventors have created many widely used devices in the world and have contributed to international innovation. Norbert Rillieux created the technique for converting sugar cane juice into white sugar crystals. Moreover, Rillieux left Louisiana in 1854 and went to France, where he spent ten years working with the Champollions deciphering Egyptian hieroglyphics from the Rosetta Stone.[219] Most slave inventors were nameless, such as the slave owned by the Confederate President Jefferson Davis who designed the ship propeller used by the Confederate navy.[220]		By 1913 over 1,000 inventions were patented by black Americans. Among the most notable inventors were Jan Matzeliger, who developed the first machine to mass-produce shoes,[221] and Elijah McCoy, who invented automatic lubrication devices for steam engines.[222] Granville Woods had 35 patents to improve electric railway systems, including the first system to allow moving trains to communicate.[223] Garrett A. Morgan developed the first automatic traffic signal and gas mask.[224]		Lewis Howard Latimer invented an improvement for the incandescent light bulb.[225] More recent inventors include Frederick McKinley Jones, who invented the movable refrigeration unit for food transport in trucks and trains.[226] Lloyd Quarterman worked with six other black scientists on the creation of the atomic bomb (code named the Manhattan Project.)[227] Quarterman also helped develop the first nuclear reactor, which was used in the atomically powered submarine called the Nautilus.[228]		A few other notable examples include the first successful open heart surgery, performed by Dr. Daniel Hale Williams,[229] and the air conditioner, patented by Frederick McKinley Jones.[226] Dr. Mark Dean holds three of the original nine patents on the computer on which all PCs are based.[230][231][232] More current contributors include Otis Boykin, whose inventions included several novel methods for manufacturing electrical components that found use in applications such as guided missile systems and computers,[233] and Colonel Frederick Gregory, who was not only the first black astronaut pilot but the person who redesigned the cockpits for the last three space shuttles. Gregory was also on the team that pioneered the microwave instrumentation landing system.[234]		The term African American carries important political overtones. Earlier terms used to describe Americans of African ancestry referred more to skin color than to ancestry, and were conferred upon the group by colonists and Americans of European ancestry; people with dark skins were considered inferior in fact and in law. The terms (such as colored, person of color, or negro) were included in the wording of various laws and legal decisions which some thought were being used as tools of white supremacy and oppression.[235] There developed among blacks in America a growing desire for a term of self-identification of their own choosing.[citation needed]		In the 1980s, the term African American was advanced on the model of, for example, German-American or Irish-American to give descendants of American slaves and other American blacks who lived through the slavery era a heritage and a cultural base.[235] The term was popularized in black communities around the country via word of mouth and ultimately received mainstream use after Jesse Jackson publicly used the term in front of a national audience in 1988. Subsequently, major media outlets adopted its use.[235]		Surveys show that the majority of Black Americans have no preference for African American versus Black American,[236] although they have a slight preference for Black American in personal settings and African American in more formal settings.[237]		Many African Americans have expressed a preference for the term African American because it was formed in the same way as the terms for the many other ethnic groups currently living in the nation. Some argued further that, because of the historical circumstances surrounding the capture, enslavement and systematic attempts to de-Africanize blacks in the United States under chattel slavery, most African Americans are unable to trace their ancestry to a specific African nation; hence, the entire continent serves as a geographic marker.		The term African American embraces pan-Africanism as earlier enunciated by prominent African thinkers such as Marcus Garvey, W. E. B. Du Bois and George Padmore. The term Afro-Usonian, and variations of such, are more rarely used.[238][239]		Since 1977, in an attempt to keep up with changing social opinion, the United States government has officially classified black people (revised to black or African American in 1997) as "having origins in any of the black racial groups of Africa."[240] Other federal offices, such as the United States Census Bureau, adhere to the Office of Management and Budget standards on race in its data collection and tabulations efforts.[241] In preparation for the United States 2010 Census, a marketing and outreach plan, called 2010 Census Integrated Communications Campaign Plan (ICC) recognized and defined African Americans as black people born in the United States. From the ICC perspective, African Americans are one of three groups of black people in the United States.[242]		The ICC plan was to reach the three groups by acknowledging that each group has its own sense of community that is based on geography and ethnicity.[243] The best way to market the census process toward any of the three groups is to reach them through their own unique communication channels and not treat the entire black population of the U.S. as though they are all African Americans with a single ethnic and geographical background. The U.S. Department of Justice Federal Bureau of Investigation categorizes black or African-American people as "A person having origins in any of the black racial groups of Africa" through racial categories used in the UCR Program adopted from the Statistical Policy Handbook (1978) and published by the Office of Federal Statistical Policy and Standards, U.S. Department of Commerce, derived from the 1977 Office of Management and Budget classification.[244]		Historically, "race mixing" between black and white people was taboo in the United States. So-called anti-miscegenation laws, barring blacks and whites from marrying or having sex, were established in colonial America as early as 1691,[245] and endured in many Southern states until the Supreme Court ruled them unconstitutional in Loving v. Virginia (1967). The taboo among American whites surrounding white-black relations is a historical consequence of the oppression and racial segregation of African Americans.[246] Historian David Brion Davis notes the racial mixing that occurred during slavery was frequently attributed by the planter class to the "lower-class white males" but Davis concludes that "there is abundant evidence that many slaveowners, sons of slaveowners, and overseers took black mistresses or in effect raped the wives and daughters of slave families."[247] A famous example was Thomas Jefferson's mistress, Sally Hemings.[248]		Harvard University historian Henry Louis Gates Jr. wrote in 2009 that "African Americans [...] are a racially mixed or mulatto people—deeply and overwhelmingly so" (see genetics). After the Emancipation Proclamation, Chinese American men married African-American women in high proportions to their total marriage numbers due to few Chinese American women being in the United States.[249] African slaves and their descendants have also had a history of cultural exchange and intermarriage with Native Americans,[250] although they did not necessarily retain social, cultural or linguistic ties to Native peoples.[251] There are also increasing intermarriages and offspring between non-Hispanic blacks and Hispanics of any race, especially between Puerto Ricans and African Americans (American-born blacks).[252] According to author M.M. Drymon, many African Americans identify as having Scots-Irish ancestry.[253]		Racially mixed marriages have become increasingly accepted in the United States since the Civil Rights Movement and up to the present day.[254] Approval in national opinion polls have risen from 36% in 1978, to 48% in 1991, 65% in 2002, 77% in 2007.[255] A Gallup poll conducted in 2013 found that 84% of whites and 96% of blacks approved of interracial marriage, and 87% overall.[256]		In her book The End of Blackness, as well as in an essay on the liberal website Salon,[257] author Debra Dickerson has argued that the term black should refer strictly to the descendants of Africans who were brought to America as slaves, and not to the sons and daughters of black immigrants who lack that ancestry. In her opinion, President Barack Obama, who is the son of a Kenyan immigrant, although technically black, is not African-American.[257][258] She makes the argument that grouping all people of African descent together regardless of their unique ancestral circumstances would inevitably deny the lingering effects of slavery within the American community of slave descendants, in addition to denying black immigrants recognition of their own unique ancestral backgrounds. "Lumping us all together", Dickerson wrote, "erases the significance of slavery and continuing racism while giving the appearance of progress".[257]		Similar viewpoints have been expressed by Stanley Crouch in a New York Daily News piece, Charles Steele, Jr. of the Southern Christian Leadership Conference[259] and African-American columnist David Ehrenstein of the Los Angeles Times, who accused white liberals of flocking to blacks who were Magic Negros, a term that refers to a black person with no past who simply appears to assist the mainstream white (as cultural protagonists/drivers) agenda.[260] Ehrenstein went on to say "He's there to assuage white 'guilt' they feel over the role of slavery and racial segregation in American history."[260]		Former Secretary of State Condoleezza Rice (who was famously mistaken for a "recent American immigrant" by French President Nicolas Sarkozy),[261] said "descendants of slaves did not get much of a head start, and I think you continue to see some of the effects of that." She has also rejected an immigrant designation for African Americans and instead prefers the term black or white to denote the African and European U.S. founding populations.[262]		Before the independence of the Thirteen Colonies until the abolition of slavery in 1865, an African-American slave was commonly known as a negro. Free negro was the legal status in the territory of an African-American person who was not a slave.[263] The term colored later also began to be used until the second quarter of the 20th century, when it was considered outmoded and generally gave way again to the exclusive use of negro. By the 1940s, the term was commonly capitalized (Negro); but by the mid-1960s, it was considered disparaging. By the end of the 20th century, negro had come to be considered inappropriate and was rarely used and perceived as a pejorative.[264][265] The term is rarely used by younger black people, but remained in use by many older African Americans who had grown up with the term, particularly in the southern U.S.[266] The term remains in use in some contexts, such as the United Negro College Fund, an American philanthropic organization that funds scholarships for black students and general scholarship funds for 39 private historically black colleges and universities, as well as in Latin America where Spanish and Portuguese are spoken. Pronounced slightly differently, it is the word for the color black, and is rarely perceived as a pejorative.[citation needed]		There are many other deliberately insulting terms. Many were in common use (e.g., nigger), but had become unacceptable in normal discourse before the end of the 20th century. One exception is the use, among the black community, of the slur nigger rendered as nigga, representing the pronunciation of the word in African American Vernacular English. This usage has been popularized by the rap and hip-hop music cultures and is used as part of an in-group lexicon and speech. It is not necessarily derogatory and, when used among black people, the word is often used to mean "homie" or "friend".[citation needed]		Acceptance of intra-group usage of the word nigga is still debated, although it has established a foothold among younger generations. The NAACP denounces the use of both nigga and nigger. Mixed-race usage of nigga is still considered taboo, particularly if the speaker is white. However, trends indicate that usage of the term in intragroup settings is increasing even among white youth due to the popularity of rap and hip hop culture.[267]		Diaspora:		Lists:		
A nerd is a person who is intellectually knowledgeable or bright, but socially inept.		Nerd or nerds may also refer to:						
William Henry Gates III (born October 28, 1955) is a co-founder of Microsoft and is an American business magnate, investor, author and philanthropist.[2][3]		In 1975, Gates and Paul Allen launched Microsoft, which became the world's largest PC software company. During his career at Microsoft, Gates held the positions of chairman, CEO and chief software architect, while also being the largest individual shareholder until May 2014.[4][a] Gates stepped down as chief executive officer of Microsoft in January 2000, but he remained as chairman and created the position of chief software architect for himself.[7] In June 2006, Gates announced that he would be transitioning from full-time work at Microsoft to part-time work and full-time work at the Bill & Melinda Gates Foundation.[8] He gradually transferred his duties to Ray Ozzie and Craig Mundie.[9] He stepped down as chairman of Microsoft in February 2014 and assumed a new post as technology adviser to support the newly appointed CEO Satya Nadella.[10]		Gates is one of the best-known entrepreneurs of the personal computer revolution. He has been criticized for his business tactics, which have been considered anti-competitive. This opinion has been upheld by numerous court rulings.[11] Later in his career, Gates pursued a number of philanthropic endeavors. He donated large amounts of money to various charitable organizations and scientific research programs through the Bill & Melinda Gates Foundation, which was established in 2000.		Since 1987, Gates has been included in the Forbes list of the world's wealthiest people.[12] As of July 2017[update], he is the richest person in the world, with an estimated net worth of US$89.9 billion.[1] In 2009, Gates and Warren Buffett founded The Giving Pledge, whereby they and other billionaires pledge to give at least half of their wealth to philanthropy.[13]						Gates was born in Seattle, Washington on October 28, 1955. He is the son of William H. Gates Sr.[b] (born 1925) and Mary Maxwell Gates (1929–1994). His ancestry includes English, German, Irish, and Scots-Irish.[14][15] His father was a prominent lawyer, and his mother served on the board of directors for First Interstate BancSystem and the United Way. Gates' maternal grandfather was JW Maxwell, a national bank president. Gates has one elder sister, Kristi (Kristianne), and one younger sister, Libby. He is the fourth of his name in his family, but is known as William Gates III or "Trey" because his father had the "II" suffix.[16] Early on in his life, Gates' parents had a law career in mind for him.[17] When Gates was young, his family regularly attended a church of the Congregational Christian Churches, a Protestant Reformed denomination.[18][19][20] The family encouraged competition; one visitor reported that "it didn't matter whether it was hearts or pickleball or swimming to the dock ... there was always a reward for winning and there was always a penalty for losing".[21]		At 13, he enrolled in the Lakeside School, a private preparatory school.[22] When he was in the eighth grade, the Mothers' Club at the school used proceeds from Lakeside School's rummage sale to buy a Teletype Model 33 ASR terminal and a block of computer time on a General Electric (GE) computer for the school's students.[23] Gates took an interest in programming the GE system in BASIC, and was excused from math classes to pursue his interest. He wrote his first computer program on this machine: an implementation of tic-tac-toe that allowed users to play games against the computer. Gates was fascinated by the machine and how it would always execute software code perfectly. When he reflected back on that moment, he said, "There was just something neat about the machine."[24] After the Mothers Club donation was exhausted, he and other students sought time on systems including DEC PDP minicomputers. One of these systems was a PDP-10 belonging to Computer Center Corporation (CCC), which banned four Lakeside students – Gates, Paul Allen, Ric Weiland, and Kent Evans – for the summer after it caught them exploiting bugs in the operating system to obtain free computer time.[25][26]		At the end of the ban, the four students offered to find bugs in CCC's software in exchange for computer time. Rather than use the system via Teletype, Gates went to CCC's offices and studied source code for various programs that ran on the system, including programs in Fortran, Lisp, and machine language. The arrangement with CCC continued until 1970, when the company went out of business. The following year, Information Sciences, Inc. hired the four Lakeside students to write a payroll program in COBOL, providing them computer time and royalties. After his administrators became aware of his programming abilities, Gates wrote the school's computer program to schedule students in classes. He modified the code so that he was placed in classes with "a disproportionate number of interesting girls."[27] He later stated that "it was hard to tear myself away from a machine at which I could so unambiguously demonstrate success."[24] At age 17, Gates formed a venture with Allen, called Traf-O-Data, to make traffic counters based on the Intel 8008 processor.[28] In early 1973, Bill Gates served as a congressional page in the U.S. House of Representatives.[29]		Gates graduated from Lakeside School in 1973 and was a National Merit Scholar.[30] He scored 1590 out of 1600 on the Scholastic Aptitude Tests (SAT) and enrolled at Harvard College in the autumn of 1973.[31][32] He chose a pre-law major but took mathematics and graduate level computer science courses.[33] While at Harvard, he met fellow student Steve Ballmer. Gates left Harvard after two years while Ballmer would stay and graduate magna cum laude. Years later, Ballmer succeeded Gates as Microsoft's CEO; maintaining that office from 2000, until he resigned from the company in 2014.[34]		In his second year, Gates devised an algorithm for pancake sorting as a solution to one of a series of unsolved problems[35] presented in a combinatorics class by Harry Lewis, one of his professors. Gates' solution held the record as the fastest version for over thirty years;[35][36] its successor is faster by only one percent.[35] His solution was later formalized in a published paper in collaboration with Harvard computer scientist Christos Papadimitriou.[37]		Gates did not have a definite study plan while he was a student at Harvard[38] and spent a lot of time using the school's computers. Gates remained in contact with Paul Allen, and he joined him at Honeywell during the summer of 1974.[39] The following year saw the release of the MITS Altair 8800 based on the Intel 8080 CPU, and Gates and Allen saw this as the opportunity to start their own computer software company.[40] Gates dropped out of Harvard at this time. He had talked this decision over with his parents, who were supportive of him after seeing how much Gates wanted to start his own company.[38] Gates explained his official status with Harvard that, "...if things [Microsoft] hadn't worked out, I could always go back to school. I was officially on [a] leave [of absence]."[41]		After Gates read the January 1975 issue of Popular Electronics that demonstrated the Altair 8800, he contacted Micro Instrumentation and Telemetry Systems (MITS), the creators of the new microcomputer, to inform them that he and others were working on a BASIC interpreter for the platform.[42] In reality, Gates and Allen did not have an Altair and had not written code for it; they merely wanted to gauge MITS's interest. MITS president Ed Roberts agreed to meet them for a demo, and over the course of a few weeks they developed an Altair emulator that ran on a minicomputer, and then the BASIC interpreter. The demonstration, held at MITS's offices in Albuquerque, was a success and resulted in a deal with MITS to distribute the interpreter as Altair BASIC. Paul Allen was hired into MITS,[43] and Gates took a leave of absence from Harvard to work with Allen at MITS in Albuquerque in November 1975. They named their partnership "Micro-Soft" and had their first office located in Albuquerque.[43] Within a year, the hyphen was dropped, and on November 26, 1976, the trade name "Microsoft" was registered with the Office of the Secretary of the State of New Mexico.[43] Gates never returned to Harvard to complete his studies.		Microsoft's Altair BASIC was popular with computer hobbyists, but Gates discovered that a pre-market copy had leaked into the community and was being widely copied and distributed. In February 1976, Gates wrote an Open Letter to Hobbyists in the MITS newsletter in which he asserted that more than 90% of the users of Microsoft Altair BASIC had not paid Microsoft for it and by doing so the Altair "hobby market" was in danger of eliminating the incentive for any professional developers to produce, distribute, and maintain high-quality software.[44] This letter was unpopular with many computer hobbyists, but Gates persisted in his belief that software developers should be able to demand payment. Microsoft became independent of MITS in late 1976, and it continued to develop programming language software for various systems.[43] The company moved from Albuquerque to its new home in Bellevue, Washington, on January 1, 1979.[42]		During Microsoft's early years, all employees had broad responsibility for the company's business. Gates oversaw the business details, but continued to write code as well. In the first five years, Gates personally reviewed every line of code the company shipped, and often rewrote parts of it as he saw fit.[45][unreliable source?]		IBM approached Microsoft in July 1980 in reference to an operating systen for its upcoming personal computer, the IBM PC.[46] Big Blue first proposed that Microsoft write the BASIC interpreter. When IBM's representatives mentioned that they needed an operating system, Gates referred them to Digital Research (DRI), makers of the widely used CP/M operating system.[47] IBM's discussions with Digital Research went poorly, and they did not reach a licensing agreement. IBM representative Jack Sams mentioned the licensing difficulties during a subsequent meeting with Gates and told him to get an acceptable operating system. A few weeks later, Gates proposed using 86-DOS (QDOS), an operating system similar to CP/M that Tim Paterson of Seattle Computer Products (SCP) had made for hardware similar to the PC. Microsoft made a deal with SCP to become the exclusive licensing agent, and later the full owner, of 86-DOS. After adapting the operating system for the PC, Microsoft delivered it to IBM as PC DOS in exchange for a one-time fee of $50,000.[48]		Gates did not offer to transfer the copyright on the operating system, because he believed that other hardware vendors would clone IBM's system.[48] They did, and the sales of MS-DOS made Microsoft a major player in the industry.[49] Despite IBM's name on the operating system, the press quickly identified Microsoft as being very influential on the new computer. PC Magazine asked if Gates were "the man behind the machine?",[46] and InfoWorld quoted an expert as stating "it's Gates' computer".[50] Gates oversaw Microsoft's company restructuring on June 25, 1981, which re-incorporated the company in Washington state and made Gates the president of Microsoft and its board chairman.[42]		Microsoft launched its first retail version of Microsoft Windows on November 20, 1985, and in August, the company struck a deal with IBM to develop a separate operating system called OS/2. Although the two companies successfully developed the first version of the new system, mounting creative differences caused the partnership to deteriorate.[51]		From Microsoft's founding in 1975 until 2006, Gates had primary responsibility for the company's product strategy. He gained a reputation for being distant from others; as early as 1981 an industry executive complained in public that "Gates is notorious for not being reachable by phone and for not returning phone calls."[52] Another executive recalled that after he showed Gates a game and defeated him 35 of 37 times, when they met again a month later Gates "won or tied every game. He had studied the game until he solved it. That is a competitor."[53]		Gates was an executve who met regularly with Microsoft's senior managers and program managers. In firsthand accounts of these meetings, he was described as being verbally combative. He also berated managers for perceived holes in their business strategies or proposals that placed the company's long-term interests at risk.[54][55] He interrupted presentations with such comments "That's the stupidest thing I've ever heard!"[56] and "Why don't you just give up your options and join the Peace Corps?"[57] The target of his outburst then had to defend the proposal in detail until, hopefully, Gates was fully convinced.[56] When subordinates appeared to be procrastinating, he was known to remark sarcastically, "I'll do it over the weekend."[58][59][60]		Gates was an active software developer in Microsoft's early history, particularly on the company's programming language products, but his basic role in most of the company's history was primarily as a manager and executive. Gates has not officially been on a development team since working on the TRS-80 Model 100,[61] but as late as 1989 he wrote code that shipped with the company's products.[59] He remained interested in technical details; in 1985, Jerry Pournelle wrote that when he watched Gates announce Microsoft Excel, "Something else impressed me. Bill Gates likes the program, not because it's going to make him a lot of money (although I'm sure it will do that), but because it's a neat hack."[62] On June 15, 2006, Gates announced that he would transition out of his day-to-day role over the next two years to dedicate more time to philanthropy. He divided his responsibilities between two successors when he placed Ray Ozzie in charge of day-to-day management and Craig Mundie in charge of long-term product strategy.[63]		Many decisions that led to antitrust litigation over Microsoft's business practices have had Gates' approval. In the 1998 United States v. Microsoft case, Gates gave deposition testimony that several journalists characterized as evasive. He argued with examiner David Boies over the contextual meaning of words such as, "compete", "concerned", and "we". The judge and other observers in the court room were seen laughing at various points during the deposition.[64] BusinessWeek reported:		Early rounds of his deposition show him offering obfuscatory answers and saying 'I don't recall,' so many times that even the presiding judge had to chuckle. Worse, many of the technology chief's denials and pleas of ignorance were directly refuted by prosecutors with snippets of e-mail that Gates both sent and received.[65]		Gates later said he had simply resisted attempts by Boies to mischaracterize his words and actions. As to his demeanor during the deposition, he said, "Did I fence with Boies? ... I plead guilty. Whatever that penalty is should be levied against me: rudeness to Boies in the first degree."[66] Despite Gates' denials, the judge ruled that Microsoft had committed monopolization and tying, and blocking competition, both in violation of the Sherman Antitrust Act.[66]		In 2008, Gates appeared in a series of ads to promote Microsoft. The first commercial, co-starring Jerry Seinfeld, is a 90-second talk between strangers as Seinfeld walks up on a discount shoe store (Shoe Circus) in a mall and notices Gates buying shoes inside. The salesman is trying to sell Mr. Gates shoes that are a size too big. As Gates is buying the shoes, he holds up his discount card, which uses a slightly altered version of his own mugshot of his arrest in New Mexico in 1977, for a traffic violation.[67] As they are walking out of the mall, Seinfeld asks Gates if he has melded his mind to other developers, after getting a "Yes", he then asks if they are working on a way to make computers edible, again getting a "Yes". Some say that this is an homage to Seinfeld's own show about "nothing" (Seinfeld).[68] In a second commercial in the series, Gates and Seinfeld are at the home of an average family trying to fit in with normal people.[69]		Since leaving day-to-day operations at Microsoft, Gates has continued his philanthropy and works on other projects.		According to the Bloomberg Billionaires Index, Gates was the world's highest-earning billionaire in 2013, as his net worth increased by US$15.8 billion to US$78.5 billion. As of January 2014, most of Gates' assets are held in Cascade Investment LLC, an entity through which he owns stakes in numerous businesses, including Four Seasons Hotels and Resorts, and Corbis Corp.[70] On February 4, 2014, Gates stepped down as chairman of Microsoft to become Technology Advisor alongside new CEO Satya Nadella.[10][71]		Gates provided his perspective on a range of issues in a substantial interview that was published in the March 27, 2014 issue of Rolling Stone magazine. In the interview, Gates provided his perspective on climate change, his charitable activities, various tech companies and people involved in them, and the state of America. In response to a question about his greatest fear when he looks 50 years into the future, Gates stated: "... there'll be some really bad things that'll happen in the next 50 or 100 years, but hopefully none of them on the scale of, say, a million people that you didn't expect to die from a pandemic, or nuclear or bioterrorism." Gates also identified innovation as the "real driver of progress" and pronounced that "America's way better today than it's ever been."[72] Gates' days are planned for him, similar to the US President's schedule, on a minute-by-minute basis.[73]		Gates married Melinda French in Hawaii on January 1, 1994; he was 38 and she was 29. They have three children: Jennifer Katharine (b. 1996), Rory John (b. 1999), and Phoebe Adele (b. 2002). The family resides in a modern design mansion, which is an earth-sheltered house in the side of a hill overlooking Lake Washington in Medina near Seattle in the state of Washington, United States. According to 2007 King County public records, the total assessed value of the property (land and house) is $125 million, and the annual property taxes are $991,000. The 66,000 sq ft (6,100 m2) estate has a 60-foot (18 m) swimming pool with an underwater music system, as well as a 2,500 sq ft (230 m2) gym and a 1,000 sq ft (93 m2) dining room.[74]		In an interview with Rolling Stone, Gates stated in regard to his faith:		The moral systems of religion, I think, are super important. We've raised our kids in a religious way; they've gone to the Catholic church that Melinda goes to and I participate in. I've been very lucky, and therefore I owe it to try and reduce the inequity in the world. And that's kind of a religious belief. I mean, it's at least a moral belief.[75]		In the same interview, Gates said: "I agree with people like Richard Dawkins that mankind felt the need for creation myths. Before we really began to understand disease and the weather and things like that, we sought false explanations for them. Now science has filled in some of the realm – not all – that religion used to fill. But the mystery and the beauty of the world is overwhelmingly amazing, and there's no scientific explanation of how it came about. To say that it was generated by random numbers, that does seem, you know, sort of an uncharitable view [laughs]. I think it makes sense to believe in God, but exactly what decision in your life you make differently because of it, I don't know."[75]		One of Gates' private acquisitions is the Codex Leicester, which is a collection of writings by Leonardo da Vinci that Gates bought for $30.8 million at an auction in 1994.[76] Gates is also known as an avid reader, and the ceiling of his large home library is engraved with a quotation from The Great Gatsby.[77] He also enjoys playing bridge, tennis, and golf.[78][79]		In 1999, his wealth briefly surpassed $101 billion, which caused the media to call Gates a "centibillionaire".[80] Despite his wealth and extensive business travel, Gates usually flew coach until 1997, when he bought a private jet.[81] Since 2000, the nominal value of his Microsoft holdings has declined due to a fall in Microsoft's stock price after the dot-com bubble burst and the multibillion-dollar donations he has made to his charitable foundations. In a May 2006 interview, Gates commented that he wished that he were not the richest man in the world because he disliked the attention it brought.[82] In March 2010, Gates was the second wealthiest person behind Carlos Slim, but regained the top position in 2013, according to the Bloomberg Billionaires List.[83][84] Carlos Slim retook the position again in June 2014[85][86] (but then lost the top position back to Gates). Between 2009 and 2014, his wealth doubled from US$40 billion to more than US$82 billion.[87]		Gates has several investments outside Microsoft, which in 2006 paid him a salary of $616,667 and $350,000 bonus totalling $966,667.[88] In 1989, he founded Corbis, a digital imaging company. In 2004, he became a director of Berkshire Hathaway, the investment company headed by long-time friend Warren Buffett.[89] In 2016, he revealed that he was color-blind when discussing his gaming habits.[90]		In 2009, Gates and Warren Buffett founded The Giving Pledge, whereby they and other billionaires pledge to give at least half of their wealth to philanthropy.[13]		Gates studied the work of Andrew Carnegie and John D. Rockefeller, and in 1994, sold some of his Microsoft stock to create the "William H. Gates Foundation." In 2000, Gates and his wife combined three family foundations to create the charitable Bill & Melinda Gates Foundation, which was identified by the Funds for NGOs company in 2013, as the world's wealthiest charitable foundation, with assets reportedly valued at more than $34.6 billion.[91][92] The Foundation allows benefactors to access information that shows how its money is being spent, unlike other major charitable organizations such as the Wellcome Trust.[93][94]		The foundation is organized into four program areas: Global Development Division, Global Health Division, United States Division, and Global Policy & Advocacy Division.[95]		Gates has credited the generosity and extensive philanthropy of David Rockefeller as a major influence. Gates and his father met with Rockefeller several times, and their charity work is partly modeled on the Rockefeller family's philanthropic focus, whereby they are interested in tackling the global problems that are ignored by governments and other organizations.[96] As of 2007, Bill and Melinda Gates were the second-most generous philanthropists in America, having given over $28 billion to charity;[97] the couple plan to eventually donate 95 percent of their wealth to charity.[98]		The Bill and Melinda Gates Foundation supports the use of genetically modified organisms in agricultural development. Specifically, the foundation is supporting the International Rice Research Institute in developing Golden Rice, a genetically modified rice variant used to combat Vitamin A deficiency.[99]		Melinda Gates suggested that people should emulate the philanthropic efforts of the Salwen family, which had sold its home and given away half of its value, as detailed in The Power of Half.[100] Gates and his wife invited Joan Salwen to Seattle to speak about what the family had done, and on December 9, 2010, Gates, investor Warren Buffett, and Facebook founder and CEO Mark Zuckerberg signed a commitment they called the "Gates-Buffet Giving Pledge." The pledge is a commitment by all three to donate at least half of their wealth over the course of time to charity.[101][102][103]		Gates has recently expressed concern about the existential threats of superintelligence; in a Reddit "ask me anything", he stated that		First the machines will do a lot of jobs for us and not be super intelligent. That should be positive if we manage it well. A few decades after that though the intelligence is strong enough to be a concern. I agree with Elon Musk and some others on this and don't understand why some people are not concerned.[104][105][106][107]		In a March 2015 interview, with Baidu's CEO, Robin Li, Gates claimed he would "highly recommend" Nick Bostrom's recent work, Superintelligence: Paths, Dangers, Strategies.[108]		Gates has also provided personal donations to educational institutions. In 1999, Gates donated $20 million to the Massachusetts Institute of Technology (MIT) for the construction of a computer laboratory named the "William H. Gates Building" that was designed by architect Frank Gehry. While Microsoft had previously given financial support to the institution, this was the first personal donation received from Gates.[109]		The Maxwell Dworkin Laboratory of the Harvard John A. Paulson School of Engineering and Applied Sciences is named after the mothers of both Gates and Microsoft President Steven A. Ballmer, both of whom were students (Ballmer was a member of the School's graduating class of 1977, while Gates left his studies for Microsoft), and donated funds for the laboratory's construction.[110] Gates also donated $6 million to the construction of the Gates Computer Science Building, completed in January 1996, on the campus of Stanford University. The building contains the Computer Science Department (CSD) and the Computer Systems Laboratory (CSL) of Stanford's Engineering department.[111]		On August 15, 2014, Bill Gates posted a video of himself on Facebook in which he is seen dumping a bucket of ice water on his head. Gates posted the video after Facebook founder Mark Zuckerberg challenged him to do so in order to raise awareness for the disease ALS (amyotrophic lateral sclerosis).[112]		Since about 2005, Bill Gates and his foundation have taken an interest in solving global sanitation problems. For example, they announced the "Reinvent the Toilet Challenge", which has received considerable media interest.[113] To raise awareness for the topic of sanitation and possible solutions, Gates drank water that was "produced from human feces" in 2014 – in fact it was produced from a sewage sludge treatment process called the Omni-processor.[114][115] In early 2015, he also appeared with Jimmy Fallon on The Tonight Show and challenged him to see if he could taste the difference between this reclaimed water or bottled water.[116]		Bill and Melinda Gates have said that they intend to leave their three children $10 million each as their inheritance. With only $30 million kept in the family, they appear to be on a course to give away about 99.96 percent of their wealth.[117]		In 2007, the Los Angeles Times criticized the foundation for investing its assets in companies that have been accused of worsening poverty, polluting heavily, and pharmaceutical companies that do not sell to the developing world.[118] In response to press criticism, the foundation announced a review of its investments to assess social responsibility.[119] It subsequently canceled the review and stood by its policy of investing for maximum return, while using voting rights to influence company practices.[120] The Gates Millennium Scholars program has been criticized by Ernest W. Lefever for its exclusion of Caucasian students.[121] The scholarship program is administered by the United Negro College Fund.[122] In 2014, Bill Gates sparked a protest in Vancouver when he decided to donate 50 million dollars to UNAIDS through the Bill & Melinda Gates Foundation for the purpose of mass circumcision in Zambia and Swaziland.[123][124]		On April 29, 2017, Bill Gates partnered with Swiss Tennis legend Roger Federer in playing a non competitive tennis match to a packed house at Key Arena in Seattle. The event was in support of Roger Federer Foundation's charity efforts in Africa.[125] Federer and Gates played against John Isner and Pearl Jam lead guitarist Mike McReady. Gates and Federer won the game 6-4.		In 1987, Gates was listed as a billionaire in Forbes magazine's 400 Richest People in America issue, just days before his 32nd birthday. As the world's youngest self-made billionaire, he was worth $1.25 billion, over $900 million more than he'd been worth the year before, when he'd debuted on the list.[12] Since 1987, Gates has been included in the Forbes The World's Billionaires list and was the wealthiest from 1995 to 1996,[126] 1998 to 2007, 2009, and has been since 2014.[1] Gates was number one on Forbes' 400 Richest Americans list from 1993 through to 2007.[127][needs update]		Time magazine named Gates one of the 100 people who most influenced the 20th century, as well as one of the 100 most influential people of 2004, 2005, and 2006. Time also collectively named Gates, his wife Melinda and U2's lead singer Bono as the 2005 Persons of the Year for their humanitarian efforts.[128] In 2006, he was voted eighth in the list of "Heroes of our time".[129] Gates was listed in the Sunday Times power list in 1999, named CEO of the year by Chief Executive Officers magazine in 1994, ranked number one in the "Top 50 Cyber Elite" by Time in 1998, ranked number two in the Upside Elite 100 in 1999, and was included in The Guardian as one of the "Top 100 influential people in media" in 2001.[130]		According to Forbes, Gates was ranked as the fourth most powerful person in the world in 2012,[131] up from fifth in 2011.[132]		In 1994, he was honored as the twentieth Distinguished Fellow of the British Computer Society. In 1999, Gates received New York Institute of Technology's President's Medal.[133] Gates has received honorary doctorates from Nyenrode Business Universiteit, Breukelen, The Netherlands, in 2000;[134] the Royal Institute of Technology, Stockholm, Sweden, in 2002;[135] Waseda University, Tokyo, Japan, in 2005; Tsinghua University, Beijing, China, in April 2007;[136] Harvard University in June 2007;[137] the Karolinska Institute, Stockholm, in 2007,[138] and Cambridge University in June 2009.[139] He was also made an honorary trustee of Peking University in 2007.[140]		Gates was made an honorary Knight Commander of the Order of the British Empire (KBE) by Queen Elizabeth II in 2005.[141] In November 2006, he was awarded the Placard of the Order of the Aztec Eagle, together with his wife Melinda who was awarded the Insignia of the same order, both for their philanthropic work around the world in the areas of health and education, particularly in Mexico, and specifically in the program "Un país de lectores".[142] Gates received the 2010 Bower Award for Business Leadership from The Franklin Institute for his achievements at Microsoft and his philanthropic work.[143] Also in 2010, he was honored with the Silver Buffalo Award by the Boy Scouts of America, its highest award for adults, for his service to youth.[144]		Entomologists named Bill Gates' flower fly, Eristalis gatesi, in his honor in 1997.[145]		In 2002, Bill and Melinda Gates received the Jefferson Award for Greatest Public Service Benefiting the Disadvantaged.[146]		In 2006, Gates received the James C. Morgan Global Humanitarian Award from The Tech Awards.[147]		In 2015, Gates, along with his wife Melinda, received the Padma Bhushan, India's third-highest civilian award for their social work in the country.[148][149]		In 2016, President Barack Obama honored Gates and his wife Melinda with the Presidential Medal of Freedom for their philanthropic efforts.[150]		In 2017, President François Hollande awarded Bill and Melinda with the France's highest national award - Legion of Honour in Paris for their charity efforts.[151]		To date, Bill Gates has authored two books:		In 2013, Gates became a LinkedIn Influencer.[157]		Gates was the guest on BBC Radio 4's Desert Island Discs on January 31, 2016, in which he talks about his relationships with his father and Steve Jobs, meeting his then future wife Melinda Ann French, the start of Microsoft and some of his habits (for example reading The Economist "from cover to cover every week"). His choice of things to take on a desert island were, for music: "Blue Skies" by Willie Nelson; book: The Better Angels of Our Nature by Steven Pinker; and luxury item: a DVD Collection of Lectures from The Teaching Company.[158]		
She's Out of Control is an independent American 1989 coming of age comedy film starring Tony Danza, Ami Dolenz and Catherine Hicks.[1] The original music score was composed by Alan Silvestri. The film was marketed with the tagline ". . . girls go wild, guys go crazy and Dads go nuts." The film was shot with the working title Daddy's Little Girl.						Widower Doug Simpson (Danza) is a radio manager from California who lives with his two daughters, Katie (Dolenz) and Bonnie (Laura Mooney). When Katie turns 15, she feels it's time to start looking more grown-up. She's been dating Richard, the boy next door, whom her father adores, since middle school. In addition, her unflattering wardrobe has been complemented by her thick glasses and full set of braces. When Doug leaves on a business trip, Katie transforms herself into a knockout beauty with help from her father's girlfriend Janet Pearson (Hicks).		When Doug returns, he is shocked to find boys from every walk of life interested in dating Katie. When his obsession with Katie and her boyfriends reaches extreme limits, Janet suggests that Doug needs psychiatric help and he seeks out an expert who gives him advice that goes wrong whenever it is applied. Throughout the latter half of the film, Katie has three boyfriends, two of whom she eventually stops dating. At the end of the film, Katie takes a class trip to Europe and reunites with Richard again – at which point Bonnie, her younger tomboy sister, begins her own dating spree. Doug also finds out the "expert" was anything but, as he never had a daughter himself.		Based on 19 reviews, Rotten Tomatoes gave the film a rating of 11%.[2]		Chicago film critic Roger Ebert gave the film the rare zero stars rating on his written review of the film,[3] saying:		What planet did the makers of this film come from? What assumptions do they have about the purpose and quality of life? I ask because She's Out of Control is simultaneously so bizarre and so banal that it's a first: the first movie fabricated entirely from sitcom cliches and plastic lifestyles, without reference to any known plane of reality.		Leonard Maltin also panned the film, stating that it was a "superficial expanded sitcom with Danza offering a one-note performance," concluding with "this one seems as if it was spit out of a computer."[4]		The soundtrack, distributed by MCA Records in April 1989, was released on vinyl, cassette, and compact disc. The track listing includes:		Other songs featured in the film that did not appear on the soundtrack:		
The Breakfast Club is a 1985 American coming-of-age comedy-drama film written, produced, and directed by John Hughes, starring Emilio Estevez, Paul Gleason, Anthony Michael Hall, Judd Nelson, Molly Ringwald and Ally Sheedy. The storyline follows five teenagers, each members of different high school cliques, who spend a Saturday in detention together and come to realize that they are all more than their respective stereotypes, while facing a strict disciplinarian.		The film premiered in Los Angeles on February 7, 1985. Universal Pictures released the film in cinemas in the United States on February 15, 1985. It received critical acclaim and earned $51.5 million on a $1 million budget. Critics consider it one of the greatest high school films of all time, as well as one of Hughes' most memorable and recognizable works. The media referred to the film's five main actors as members of a group called the "Brat Pack".		The title comes from the nickname invented by students and staff for morning detention at New Trier High School, the school attended by the son of one of John Hughes' friends. Thus, those who were sent to detention before school starting time were designated members of "The Breakfast Club". In 2016, the film was selected for preservation in the United States National Film Registry by the Library of Congress as being "culturally, historically, or aesthetically significant".		The film was digitally remastered and was re-screened throughout 430 theaters in celebration of its 30th anniversary in 2015.[4]						On Saturday, March 24, 1984, five students report at 7:00 a.m. for all-day detention at Shermer High School in Shermer, Illinois. While not complete strangers, each of them comes from a different clique, and they seem to have nothing in common: the beautiful and pampered Claire Standish, the state champion wrestler Andrew Clark, the geekish intellect Brian Johnson, the introverted outcast Allison Reynolds, and the rebellious delinquent John Bender.		They gather in the high school library, where assistant principal Richard Vernon instructs them not to speak, move from their seats, or sleep until they are released at 3:00 p.m. He assigns them a thousand-word essay, in which each must describe "who you think you are." He then leaves, returning only occasionally to check on them. Bender, who has a particularly antagonistic relationship with Vernon, ignores the rules and frequently riles up the other students, teasing Brian and Andrew and harassing Claire. Allison is initially quiet, except for an occasional random outburst. Over the course of the day, Vernon gives Bender several weekends' worth of additional detention and even locks him in a storage closet, but he escapes and returns to the library.		The students pass the hours by talking, arguing, and, at one point, smoking marijuana that Bender retrieves from his locker. Gradually, they open up to each other and reveal their deepest personal secrets: Allison is a compulsive liar; Andrew cannot easily think for himself; Bender comes from an abusive household; Brian was planning suicide with a flare gun due to the inability to cope with a bad grade; and Claire is a virgin who feels constant pressure from her friends to be a certain way. They also discover that they all have strained relationships with their parents, which are a key cause for their personal issues as well: Allison's parents ignore her due to their own problems; Andrew's father constantly criticizes his efforts at wrestling and pushes him as hard as possible; Bender's father verbally and physically abuses him; Brian's overbearing parents put immense pressure on him to earn high grades; and Claire's parents use her to get back at each other during frequent arguments. The students realize that, even with their differences, they face similar pressures and complications in their lives.		Despite their differences in social status, the group begins to form friendships as the day progresses. Claire gives Allison a makeover, to reveal just how pretty she really is, which sparks romantic interest in Andrew. Claire decides to break her "pristine" virgin appearance by kissing Bender in the closet and giving him a hickey. Although they suspect that the relationships will end with the end of their detention, their mutual experiences will change the way they look at their peers afterwards.		As the detention nears its end, the group requests that Brian complete the essay for everyone and John returns to the storage closet to fool Vernon into thinking he has not left. Brian writes the essay and leaves it in the library for Vernon to read after they leave. As the students part ways outside the school, Allison and Andrew kiss, as do Claire and Bender. Allison rips Andrew's state champion patch from his letterman jacket to keep, and Claire gives Bender one of her diamond earrings, which he attaches to his earlobe. Vernon reads the essay (read by Brian in voice-over), in which Brian states that Vernon has already judged who they are, using simple definitions and stereotypes. One by one, the five students' voices add, "But what we found out is that each one of us is a brain, and an athlete, and a basket case, a princess, and a criminal." Brian signs the letter as "The Breakfast Club." Bender raises his fist in triumph as he walks across the school football field toward home.		In the first draft Claire was known as "Cathy Douglas".[5]		Molly Ringwald and Anthony Michael Hall both starred in Hughes' 1984 film Sixteen Candles. Towards the end of filming, Hughes asked them both to be in The Breakfast Club. Hall became the first to be cast, agreeing to the role of Brian Johnson. Ringwald was originally approached to play the character of Allison Reynolds, but she was "really upset" because she wanted to play Claire Standish. She eventually convinced the director and the studio to give her the part.[6] The role of Allison ultimately went to Ally Sheedy.		Emilio Estevez originally auditioned for the role of John Bender. However, when Hughes was unable to find someone to play Andrew Clarke, Estevez was recast. Nicolas Cage was considered for the role of John Bender, which was the last role to be cast, though the role was narrowed down to John Cusack and Judd Nelson. Hughes eventually cast Cusack, but decided to replace him with Nelson before shooting began, because Cusack did not look threatening enough for the role. At one point, Hughes was disappointed at Nelson because he stayed in character by harassing Ringwald off-camera, and the other actors had to convince Hughes not to fire him.[6][7]		Rick Moranis was originally cast as the janitor but left due to creative differences and was replaced by John Kapelos.[8]		In 1999, Hughes said that his request to direct the film met with resistance and skepticism because he lacked filmmaking experience.[9] Hughes ultimately convinced the film's investors that due to the modest $1 million budget and its single location shoot, he could greatly minimize their risk. Hughes originally thought that The Breakfast Club would be his directorial debut. Hughes opted for an insular, largely one room set and wrote it about high school students, who would be played by younger actors.[10]		Principal photography began on March 28, 1984, and ended in May. Filming took place at Maine North High School in Des Plaines, Illinois, shuttered in 1981. The same setting was used for interior scenes of Hughes' 1986 film Ferris Bueller's Day Off, which featured exterior shots from nearby Glenbrook North High School. The library at Maine North High School, considered too small for the film, prompted the crew to build the set in the school's gymnasium.[11] The actors rehearsed with each other for three weeks and then shot the film in sequence.[12] On the Ferris Bueller's Day Off DVD commentary (featured on the 2004 DVD version), Hughes revealed that he shot the two films concurrently to save time and money, and some outtakes of both films feature elements of the film crews working on the other film. The first print was 150 minutes in length.[13]		During a cast reunion in honor of the film's 25th anniversary, Ally Sheedy revealed that a Director's Cut existed but Hughes' widow did not disclose any details concerning its whereabouts.[7]		In 2015 the first draft of the film's script was discovered in a Maine South High School cabinet as district employees were moving offices to a new building.[5]		The film's poster, featuring the five characters huddled together, was photographed by Annie Leibovitz toward the end of shooting. The shot of five actors gazing at the camera influenced the way teen films were marketed from that point on.[14] The poster refers to the five "types" of the story using slightly different terms than those used in the film, and in a different sequence, stating "They were five total strangers with nothing in common, meeting for the first time. A brain, a beauty, a jock, a rebel and a recluse." The poster itself was so influential, it was parodied in The Texas Chainsaw Massacre 2 a year later.		The main theme of the film is the constant struggle of the American teenager to be understood, by adults and by themselves. It explores the pressure put on teenagers to fit into their own realms of high school social constructs, as well as the lofty expectations of their parents, teachers, and other authority figures. On the surface, the students have little in common with each other. However, as the day rolls on, they eventually bond over a common disdain for the aforementioned issues of peer pressure and parental expectations.[15][16] Stereotyping is another theme. Once the obvious stereotypes are broken down, the characters "empathize with each other's struggles, dismiss some of the inaccuracies of their first impressions, and discover that they are more similar than different."[17]		The main adult character, Mr. Vernon, is not portrayed in a positive light. He consistently talks down to the students and flaunts his authority throughout the film. Bender is the only one who stands up to Vernon.[15]		The film premiered in Los Angeles on February 7, 1985. Universal Pictures released the film in cinemas on February 15, 1985 in the United States.		In 2003, the film was released on DVD as part of the "High School Reunion Collection".[18] In 2008, a "Flashback Edition" DVD was released with several special features, including an audio commentary with Anthony Michael Hall and Judd Nelson.[19] A 25th Anniversary Edition Blu-ray was released in 2010,[20] and the same disc was re-released with a DVD and digital copy in 2012 as part of Universal's 100th Anniversary series.[21][22] On March 10, 2015, the 30th Anniversary Edition was released. This release was digitally remastered and restored from the original 35mm film negatives for better picture quality on DVD, Digital HD and Blu-ray.[23]		The film received high critical acclaim. The review aggregator website Rotten Tomatoes gives the film a 89% approval rating based on 56 reviews with a critical consensus states "The Breakfast Club is a warm, insightful, and very funny look into the inner lives of teenagers".[24] Review aggregator Metacritic assigned the film a weighted average score of 62% based on 11 reviews from mainstream critics, considered to be "generally favorable reviews".[25]		In February 1985, the film debuted at #3 at the box office (behind blockbuster film Beverly Hills Cop and Witness starring Harrison Ford).[26] Grossing $45,875,171 domestically and $51,525,171 worldwide, the film is a box office success, given its alleged $1 million budget.[27]		Anthony Michael Hall, Judd Nelson, Molly Ringwald, Paul Gleason and Ally Sheedy won a Silver Bucket of Excellence Award at the 2005 MTV Movie Awards in 2005.		The Breakfast Club is known as the "quintessential 1980s film" and is considered as one of the best films of the decade.[31] In 2008, Empire magazine ranked it #369 on their The 500 Greatest Movies of All Time list.[32] It then rose 331 places to rank at #38 on their 2014 list.[33] Similarly, The New York Times placed the film on its Best 1000 Movies Ever list[34] and Entertainment Weekly ranked the film number 1 on its list of the 50 Best High School Movies.[35]		In the 2001 parody film Not Another Teen Movie, Gleason reprised his role as Assistant Principal Vernon in a short scene that parodies The Breakfast Club.[36] Degrassi: The Next Generation paid homage to the film in its third season episode "Take on Me" (2003). As in the film, a diverse cross-section of the student body from different cliques are detained on a Saturday. Principal Raditch wears an approximation of Principal Vernon's "Barry Manilow" suit.		In 2005, the film received the Silver Bucket of Excellence Award in honor of its 20th anniversary at the MTV Movie Awards. For the event, MTV attempted to reunite the original cast. Sheedy, Ringwald, and Hall appeared together on stage, with Kapelos in the audience; Gleason gave the award to his former castmates. Estevez could not attend because of other commitments, and Nelson appeared earlier in the show but left before the on-stage reunion, prompting Hall to joke that the two were "in Africa with Dave Chappelle". Yellowcard performed Simple Minds' anthem for the film, "Don't You (Forget About Me)", at the awards. At the 82nd Academy Awards (March 7, 2010), Sheedy, Hall, Ringwald, and Nelson all appeared in a tribute to John Hughes — who had died a few months prior — along with other actors who had worked with him, including Jon Cryer from Pretty in Pink, Matthew Broderick from Ferris Bueller's Day Off, and Macaulay Culkin from Home Alone.		The Breakfast Club soundtrack album was released on February 19, 1985 by A&M Records. The album peaked at No. 17 on the Billboard 200 album chart. The song "Don't You (Forget About Me)" performed by Scottish rock band Simple Minds was released on February 20, 1985 in the United States and on April 8, 1985 in the United Kingdom as a single and reached No. 1 on the Billboard Hot 100.		In a June 25, 1985 review for The Village Voice, music critic Robert Christgau gave the album a "D-" and said that it has "utterly negligible" songs, and he commended Simple Minds for trying to distance themselves from their song, "Don't You (Forget About Me)," best known for being played during the film's opening and closing credits.[37] In a retrospective review for AllMusic, Stephen Thomas Erlewine gave the soundtrack three out of five stars and wrote that, apart from Simple Minds' "undisputed masterpiece," the album is largely "disposable" and marred by "'80s artifacts" and "forgettable instrumentals".[38]		
Boing Boing is a website, first established as a zine in 1988, later becoming a group blog. Common topics and themes include technology, futurism, science fiction, gadgets, intellectual property, Disney and left-wing politics. It twice won the Bloggies for Weblog of the Year, in 2004 and 2005. The editors are Mark Frauenfelder, Cory Doctorow, David Pescovitz, Xeni Jardin, and Rob Beschizza,[2] and the publisher is Jason Weisberger.						Boing Boing (originally bOING bOING) started as a zine in 1988 by married duo Mark Frauenfelder and Carla Sinclair.[3] Issues were subtitled "The World's Greatest Neurozine". Associate editors included Gareth Branwyn, Jon Lebkowsky, and Paco Nathan. Along with Mondo 2000, Boing Boing was an influence in the development of the cyberpunk subculture. It reached a maximum circulation of 17,500 copies.[3] The last issue of the zine was #15.		Boing Boing was established as a Web site in 1995[4] and one year later was a web-only publication.[3] While researching for an article about blogs in 1999, Frauenfelder became acquainted with the Blogger software. He relaunched Boing Boing as a weblog on 21 January 2000, describing it as a "directory of wonderful things."[3] Over time, Frauenfelder was joined by four co-editors: Doctorow, Pescovitz, Jardin and Beschizza, who previously contributed to Wired magazine. Maggie Koerth-Baker, after a run as a guest blogger in 2009,[5] joined the site as its Science Editor.		In September 2003, Boing Boing removed their Quicktopics user-comment feature without warning or explanation. Bloggers commenting on the change at the time speculated that it stemmed from "identity impersonators and idiot flamers" pretending to be co-editors.[6] Xeni Jardin was a guest on the NewsHour with Jim Lehrer to discuss the Washington Post's decision to remove its Comments section on its website, and she spoke from her experience at Boing Boing.[7] In August 2007, Boing Boing staff launched a redesigned site, which included a restored comment facility, moderated by Teresa Nielsen Hayden.		In 2004, the project incorporated as Happy Mutants LLC, and John Battelle became the blog's business manager.[8][9] Boing Boing, by the mid-2000s, "had become one of the most-read and linked-to blogs in the world" according to Fast Company.[3]		The site added advertising over the course of late 2004, placed above and to the left and right of material, and, in 2005, in the site's RSS feed as well. Editor Cory Doctorow noted that "John [Battelle] said it's going to be harder to make a little money to pay your bandwidth bills than it will be to make a lot of money and have a real source of income from this."[10] The advertising income during the first quarter was already $27,000, and as of 2010[update], Boing Boing still "makes a nice living for its founders and a handful of contract employees," but it is no longer a member of Battelle's blog network Federated Media Publishing, Inc.[3]		Boing Boing featured a "guest blogger" sidebar, then stopped the series in summer of 2004. In 2008, the "guest blogger" series was resumed, with guests posting in the main blog for two-week periods. Guests have included Charles Platt, John Shirley, Mark Dery, Tiffany Lee Brown, Karen Marcelo of Survival Research Laboratories, Johannes Grenzfurthner of monochrom, Rudy Rucker, Gareth Branwyn, Wiley Wiggins, Jason Scott of textfiles.com, Jessamyn West of librarian.net, journalists Danny O'Brien and Quinn Norton and comedian John Hodgman.		In September 2006, Boing Boing introduced a weekly podcast, "Boing Boing Boing", intended to cover the week's posts and upcoming projects. The show's cast consists of the Boing Boing editors, accompanied by a weekly guest. In the same month, Boing Boing introduced a second podcast called "Get Illuminated", which features interviews with writers, artists, and other creatives.		The site's own original content is licensed under a Creative Commons Attribution Non-Commercial license, as of August 2008.[11]		In September 2009, Boing Boing refused to comply with a demand from Polo Ralph Lauren's lawyers to remove a post concerning a heavily manipulated image of model Filippa Hamilton, originally published by the Photoshop Disasters blog. The latter was forced to comply with the company's demand by its hosting provider.[12] Ralph Lauren issued DMCA takedown notices to BoingBoing's ISP and Blogspot, which hosts Photoshop Disasters, claiming their use of the image infringed copyright. Blogspot complied, but Boing Boing's ISP consulted with Boing Boing and agreed that the image was fair use. As a result, Boing Boing issued a mocking rebuttal,[13] using the same image again and posting the takedown notice.		The rebuttal was widely reported, including on frequently viewed websites such as The Huffington Post[14] and ABC News.[15]				On the evening of 27 October 2010,[16] a hacker attacked the Boing Boing website. About one hour later, it was down, but then returned.		On 3 May 2011, the first podcast of "Gweek" was released. Gweek is a podcast in which the editors and friends of Boing Boing talk about comic books, science fiction and fantasy, video games, TV shows, music, movies, tools, gadgets, apps, and other "neat" stuff. In the first episode of Gweek, Rob Beschizza and Mark Frauenfelder discussed subjects such as the video game "Portal 2", graphic novels, upcoming science fiction books, and recommendations of some of their favorite adventure games for mobile platforms. Boing Boing has since added several other podcasts.[17]		In 2013, Boing Boing switched from the proprietary Disqus comment system to Discourse, an open-source internet forum developed by Jeff Atwood, Robin Ward and Sam Saffron.[18]		A "unicorn chaser" is a concept created by Boing Boing editors as an antidote to blog postings linking to sites containing disgusting or shocking images. The shocking post would be immediately followed by another post containing a picture of a unicorn. Xeni Jardin posted the first one (titled "And now, we pause for a Unicorn Moment.") in August 2003 as a reply to a picture of a rash posted by editor Mark Frauenfelder in an attempt to get readers to diagnose it for him.[19] It has also been used as an antidote for posts containing photos of a brain tumor, a man who pumped up the skin of his face with saline solution, many different ways to clean one's earwax, and a lengthy discussion of the Internet video "2 Girls 1 Cup".		On 18 May 2007, Boing Boing announced that Virgin America, as part of its "Name Our Planes!" campaign, would be naming one of its new aircraft "Unicorn Chaser," after having asked Boing Boing to suggest a name.[20]		In August 2007, Boing Boing introduced a gadgets-focused companion site headed by former Gizmodo editor Joel Johnson. Johnson left in July 2009, to be replaced by Rob Beschizza,[21] formerly of Wired News. Other writers include Steven Leckart and Lisa Katayama.[22] Offworld, a blog covering video games edited by Brandon Boyer, was added in November 2008.[23] These sites were incorporated into Boing Boing itself around 2010[citation needed]. Plans to revive the Offworld site were announced in 2015, with Leigh Alexander as Editor-in-Chief and Laura Hudson as Editor.[24] Leigh Alexander left Offworld in early 2016 and Offworld was once again inactive.		In October 2007, Boing Boing started a new component, Boing Boing TV, that consists of video segments including SPAMasterpiece Theater (2008) with John Hodgman, produced by its co-editors in conjunction with DECA, the Digital Entertainment Corporation of America. The episodes appear online, as well as on Virgin America flights.[25]		Boing Boing has been described as an "outspoken critic of censorship elsewhere",[26] and operates a high speed, high quality Tor exit node,[27] yet it has been accused of practicing forms of censorship itself.[improper synthesis?] For example, the act of "disemvoweling" was popularized by the site—literally stripping out the vowels of any comment a moderator had taken exception to.[28][29] More recently it has opted to drop comments completely, and ban commentators without warning, especially those expressing conservative views;[30][31][32][better source needed] As an example of increasingly stringent censorship, Boing Boing deleted the first 8 or 9 comments on a software piracy related article with no explanation.[33]		Sex blogger Violet Blue has been mentioned, interviewed and once contributed at Boing Boing. On 23 June 2008, Blue posted on her blog, Tiny Nibbles,[34] that all posts related to her had been deleted from Boing Boing, without explanation.[35] The Los Angeles Times featured an interview that cast the silence on the part of Boing Boing on the matter as "inexplicable", causing a controversy as Boing Boing "has often presented itself as a stalwart of cultural openness".[36][37] A heated debate ensued after a brief statement on the Boing Boing site regarding this action stated: "Violet behaved in a way that made us reconsider whether we wanted to lend her any credibility or associate with her. It's our blog and so we made an editorial decision, like we do every single day".[38] In commentary attached to that blog entry, "many commenters surmised that they had something to do with Blue's suing to stop a porn star from also using the name Violet Blue," and many commenters found the removal troubling, but Xeni Jardin said that she hoped she would not have to make the reasons public.[39][40]		
Attractiveness or attraction is a quality that causes an interest, desire in, or gravitation to something or someone. [1] Attraction may also refer to the object of the attraction itself, as in tourist attraction.						Visual attractiveness or visual appeal is attraction produced primarily by visual stimuli.		Physical attractiveness is the perception of the physical traits of an individual human person as pleasing or beautiful. It can include various implications, such as sexual attractiveness, cuteness, similarity and physique.[citation needed]		Judgment of attractiveness of physical traits is partly universal to all human cultures, partly dependent on culture or society or time period, partly biological, and partly subjective and individual.[2]		
In sociology and cultural studies, reappropriation or reclamation is the cultural process by which a group reclaims terms or artifacts that were previously used in a way disparaging of that group.[1]						The term reappropriation is an extension of the term appropriation or cultural appropriation used in anthropology, sociology and cultural studies to describe the reabsorbing of subcultural styles and forms, or those from other cultures, into mass culture through a process of commodification: the mass-marketing of alternate lifestyles, practices, and artifacts.[citation needed]				A reclaimed or reappropriated word is a word that was at one time pejorative but has been brought back into acceptable usage, usually starting within the communities that experienced oppression under that word but sometimes also among the general populace as well.[1] (The term "reclaimed word" more often implies usage by a member of the group that is referred.)		That can have wider implications in the fields of discourse and has been described in terms of personal or sociopolitical empowerment.[2]		Reclaiming or reappropriating a word involves re-evaluating a term that is or was, in the dominant culture, used to by a majority to oppress various minorities of the same culture,[citation needed] such as "queer", once seen as pejorative but now reclaimed and used as a self-reference by some.[3][4]		Reclaimed words differ from general reclamation outside of language because of their deliberately provocative nature. In addition to neutral or acceptable connotations, reclaimed words often acquire positive meaning within the circles of the informed.[1] Outside the community, such transitions are rare. As such, the use of these terms by outside parties is usually viewed as strongly derogatory. For some terms, even "reclaimed" usage by members of the community concerned is a subject of controversy; for example, there is considerable debate within the transgender community over attempts to reclaim the term "tranny", usually applied offensively to trans women.[5][6][7]		Michel Foucault discusses the idea of reclaimed words as a "reverse discourse" in his History of Sexuality: Volume I.				There are many recent English-language examples of some linguistic reappropriation in the areas of human sexuality, gender roles, sexual orientation, etc. Among these are:		However, the phenomenon is much older, especially in politics and religion.		In England, for example, Cavalier was a derogatory nickname reappropriated as self-identification,[12] while Roundhead, a Royalists derisory term for the supporters of the Parliamentary cause, is not (it was a punishable offence in the New Model Army to call a fellow soldier a roundhead).[13] Tory (orig. from Middle Irish word for "pursued man" Tóraidhe ), Whig (from "whiggamore" (See the Whiggamore Raid)) and "Suffragette" are other British examples.		The term Prime Minister was originally used disparagingly by Members of the British Parliament towards Sir Robert Walpole, whose official title was First Lord of the Treasury. Over time, the title became official and honorific.		In the American colonies, British officers used Yankee, from a term used by the British to refer to Dutch pirates, as a derogatory term against colonists. British soldiers created the early versions of the song "Yankee Doodle," as a criticism of the uncultured colonists, but during the Revolution, as the colonists began to reappropriate the label "yankee" as a point of pride, they likewise reappropriated the song, altering verses, and turning it into a patriotic anthem.[14]		The Dutch and German languages actually have a separate word for such a term, "geuzennaam" (Dutch, commonly used) and "Geusenwort" (German, used among linguists). These words derive from the geuzen, i.e., Dutch opponents to Spanish rule in the 16th century, who eventually created the Netherlands under William of Orange. Being derisively called "beggars" ("gueux" in French of the era) by their opponents, they appropriated a Dutchified form of the word as their own "battle name". In French during the French Revolution the word "Sans-culottes" (literally "without knee-breeches", which were worn by the upper classes) gained a similar meaning.		The women campaigning for the vote in the early 20th century were known as "suffragists" until their opponents dubbed the more militant faction "suffragettes", intending it as a disparaging diminutive, but the Women's Social and Political Union embraced it.		More recent political examples include:		One of the older examples of successful reclaiming is the term "Jesuit" to refer to members of the Society of Jesus. This was originally a derogatory term referring to people who too readily invoked the name of Jesus in their politics,[citation needed] but which members of the Society adopted over time for themselves, so that the word came to refer exclusively to them, and generally in a positive or neutral sense,[15] even though the term "Jesuitical" is derived from the Society of Jesus and is used to mean things like: manipulative, conspiring, treacherous, capable of intellectually justifying anything by convoluted reasoning.[16][17][18][19]		Another example can be found in the origins of Methodism; early members were originally mocked for their "methodical" and rule-driven religious devotion, founder John Wesley embraced the term for his movement.[20]		To a lesser extent, and more controversially among the groups referred to, many racial, ethnic, and class terms have been reappropriated:		More generally, any kind of community can reappropriate words referring to them:		
A superhero (sometimes rendered super-hero or super hero) is a type of heroic stock character who possesses supernatural or superhuman powers and who is dedicated to fighting crime, protecting the public, and usually battling supervillains. A female superhero is sometimes called a superheroine (also rendered super-heroine or super heroine), although the word superhero is commonly used for females also. Superhero fiction is the genre of fiction that is centered on such characters, especially in American comic books since the 1930s.		By most definitions, characters do not require actual superhuman powers or phenomena to be deemed superheroes.[1][2][3] While the Dictionary.com definition of "superhero" is "a figure, especially in a comic strip or cartoon, endowed with superhuman powers and usually portrayed as fighting evil or crime",[4] the longstanding Merriam-Webster dictionary gives the definition as "a fictional hero having extraordinary or superhuman powers; also: an exceptionally skillful or successful person".[5] Terms such as masked crime fighters, costumed adventurers or masked vigilantes are sometimes used to refer to characters such as the Spirit, who may not be explicitly referred to as superheroes but nevertheless share similar traits.		Some superheroes use their powers to counter daily crime while also combating threats against humanity from supervillains, who are their criminal counterparts. Often at least one of these supervillains will be the superhero's archenemy. Some long-running superheroes such as Batman, Spider-Man, Superman, Captain America, Wonder Woman, Iron Man, the Flash, Wolverine, Green Lantern, and Hulk have a rogues gallery of many villains.						The word 'superhero' dates to at least 1917.[6] Antecedents of the archetype include such folkloric heroes as Robin Hood, who adventured in distinctive clothing.[7] The 1903 play The Scarlet Pimpernel and its spinoffs popularized the idea of a masked avenger and the superhero trope of a secret identity.[7] Shortly afterward, masked and costumed pulp fiction characters such as Zorro (1919), The Shadow (1930) and comic strip heroes, such as the Phantom (1936) began appearing, as did non-costumed characters with super strength, including Patoruzú (1928), the comic-strip character Popeye (1929) and novelist Philip Wylie's protagonist Hugo Danner (1930).[8]		In the 1930s, both trends came together in some of the earliest superpowered costumed heroes such as Japan's Ōgon Bat[9][10] (visualized in painted panels used by kamishibai oral storytellers in Japan since 1931), Mandrake the Magician[11][12][13] (1934), Superman in 1938 and Captain Marvel (1939) at the beginning of the Golden Age of Comic Books.		During the 1940s there were many superheroes: The Flash, Green Lantern and Blue Beetle debuted in this era. This era saw the debut of first known female superhero, writer-artist Fletcher Hanks's character Fantomah, an ageless ancient Egyptian woman in the modern day who could transform into a skull-faced creature with superpowers to fight evil; she debuted in Fiction House's Jungle Comics #2 (Feb. 1940), credited to the pseudonymous "Barclay Flagg".[14][15] The Invisible Scarlet O'Neil, a non-costumed character who fought crime and wartime saboteurs using the superpower of invisibility created by Russell Stamm, would debut in the eponymous syndicated newspaper comic strip a few months later on June 3, 1940.[16]		One superpowered character was portrayed as an antiheroine, a rarity for its time: the Black Widow, a costumed emissary of Satan who killed evildoers in order to send them to Hell — debuted in Mystic Comics #4 (Aug. 1940), from Timely Comics, the 1940s predecessor of Marvel Comics. Most of the other female costumed crime-fighters during this era lacked superpowers. Notable characters include The Woman in Red,[17][18] introduced in Standard Comics' Thrilling Comics #2 (March 1940); Lady Luck, debuting in the Sunday-newspaper comic-book insert The Spirit Section June 2, 1940; the comedic character Red Tornado, debuting in All-American Comics #20 (Nov 1940); Miss Fury,[19] debuting in the eponymous comic strip by female cartoonist Tarpé Mills on April 6, 1941; the Phantom Lady, introduced in Quality Comics Police Comics #1 (Aug. 1941); the Black Cat,[20][21] introduced in Harvey Comics' Pocket Comics #1 (also Aug. 1941); and the Black Canary, introduced in Flash Comics #86 (Aug. 1947) as a supporting character.[22] The most iconic comic book superheroine, who debuted during the Golden Age, is Wonder Woman.[23] Modeled from the myth of the Amazons of Greek mythology, she was created by psychologist William Moulton Marston, with help and inspiration from his wife Elizabeth and their mutual lover Olive Byrne.[24][25] Wonder Woman's first appearance was in All Star Comics #8 (Dec. 1941), published by All-American Publications, one of two companies that would merge to form DC Comics in 1944.		In 1952, Osamu Tezuka's manga Tetsuwan Atom (more popularly known in the west as Astro Boy) was published. The series focused upon a robot boy built by a scientist to replace his deceased son. Being built from an incomplete robot originally intended for military purposes Astro Boy possessed amazing powers such as flight through thrusters in his feet and the incredible mechanical strength of his limbs.		The 1950s saw the Silver Age of Comics. During this era DC introduced the likes of Batwoman in 1956, Supergirl, Miss Arrowette, and Bat-Girl; all female derivatives of established male superheroes. 1958 saw the debut of superhero Moonlight Mask on Japanese television.		The Marvel Comics teams of the early 1960s typically included at least one (and often the only) female member, much like DC's flagship superhero team the Justice League of America (whose initial roster included Wonder Woman as the token female); examples include the Fantastic Four's Invisible Girl, the X-Men's Jean Grey (originally known as Marvel Girl), the Avengers' Wasp, and the Brotherhood of Mutants' Scarlet Witch (who later joined the Avengers). In 1963, Astro Boy was adapted into a highly influential anime television series. Phantom Agents in 1964 focused on ninjas working for the Japanese government and would be the foundation for Sentai-type series. 1966 saw the debut of sci-fi/horror series Ultra Q created by Eiji Tsuburaya this would eventually lead on to the sequel Ultraman, spawning a successful franchise focused upon the Giant Hero subgenre where the Superheroes would be as big as giant monsters (Kaiju) that they fought.		In 1972, the Science Ninja Team Gatchaman anime debuted, which built upon the superhero team idea of the live-action Phantom Agents as well as introducing different colors for team members and special vehicles to support them, said vehicles could also combine into a larger one. Another important event was the debut of Mazinger Z by Go Nagai, creating the Super Robot genre. Go Nagai also wrote the manga Cutey Honey in 1973, although the Magical Girl genre already existed, Nagai's manga introduced Transformation sequences that would become a staple of Magical Girl media.		The 1970s would see more anti-heroes introduced into Superhero fiction such examples included the debut of Shotaro Ishinomori's Skull Man in 1970, Go Nagai's Devilman in 1972 and Gerry Conway and John Romita's Punisher in 1974.		The dark Skull Man manga would later get a television adaptation and underwent drastic changes. The protagonist was redesigned resemble a grasshopper, becoming the renowned first masked hero of the Kamen Rider series. Kamen Rider is a motorcycle riding hero in an insect-like costume, who shouts Henshin (Transform) to don his costume and gain superhuman powers.		The ideas of second-wave feminism, which spread through the 1960s into the 1970s, greatly influenced the way comic book companies would depict as well as market their female characters: Wonder Woman was for a time revamped as a mod-dressing martial artist directly inspired by the Emma Peel character from the British television series The Avengers (no relation to the superhero team of the same name),[26] but later reverted to Marston's original concept after the editors of Ms. magazine publicly disapproved of the character being depowered and without her traditional costume;[27] Supergirl was moved from being a secondary feature on Action Comics to headline Adventure Comics in 1969; the Lady Liberators appeared in an issue of The Avengers as a group of mind-controlled superheroines led by Valkyrie (actually a disguised supervillainess) and were meant to be a caricatured parody of feminist activists;[28] and Jean Grey became the embodiment of a cosmic being known as the Phoenix Force with seemingly unlimited power in the late 1970s, a stark contrast from her depiction as the weakest member of her team a decade ago.		Both major publishers began introducing new superheroines with a more distinct feminist theme as part of their origin stories or character development. Examples include Big Barda, Power Girl, and the Huntress by DC comics; and from Marvel, the second Black Widow, Shanna the She-Devil, and The Cat.[29] Female supporting characters who were successful professionals or hold positions of authority in their own right also debuted in the pages of several popular superhero titles from the late 1950s onward: Hal Jordan's love interest Carol Ferris was introduced as the Vice-President of Ferris Aircraft and later took over the company from her father; Medusa, who was first introduced in the Fantastic Four series, is a member of the Inhuman Royal Family and a prominent statesperson within her people's quasi-feudal society; and Carol Danvers, a decorated officer in the United States Air Force who would become a costumed superhero herself years later.		In 1975 Shotaro Ishinomori's Himitsu Sentai Gorenger debuted on what is now TV Asahi, it brought the concepts of multi-colored teams and supporting vehicles that debuted in Gatchaman into live-action. In 1978, Toei adapted Spider-Man into a live-action series. In this continuity, Spider-Man had a vehicle called Marveller that could transform into a giant and powerful robot called Leopardon, this idea would be carried over to Toei's Battle Fever J and now multi-colored teams not only had support vehicles but giant robots to fight giant monsters with.		In subsequent decades, popular characters like Dazzler, She-Hulk, Elektra, Catwoman, Witchblade, Spider-Girl, Batgirl and the Birds of Prey became stars of long-running eponymous titles. Female characters began assuming leadership roles in many ensemble superhero teams; the Uncanny X-Men series and its related spin-off titles in particular have included many female characters in pivotal roles since the 1970s.[30] Volume 4 of the X-Men comic book series featured an all-female team as part of the Marvel NOW! branding initiative in 2013.[31] Superpowered female characters like Buffy the Vampire Slayer[32] and Darna[33][34] have a tremendous influence on popular culture in their respective countries of origin.		With more and more anime, manga and Tokusatsu being translated or adapted, western audiences were beginning to experience the Japanese styles of superhero fiction more than they were able to before. Saban's Mighty Morphin Power Rangers, an adaptation of Zyuranger created a multimedia franchise that used footage from Super Sentai.[35] Internationally, the Japanese comic book character, Sailor Moon, is recognized as one of the most important and popular female superheroes ever created.[36][37][38][39][40]		Many superhero characters display the following traits:				Many superheroes work independently. However, there are also many superhero teams. Some, such as the Fantastic Four, DNAgents, and the X-Men, have common origins and usually operate as a group. Some are families in which the parents and kids have superpowers, like The Incredibles. Others, such as DC Comics’s Justice League and Marvel’s Avengers, are "all-star" groups consisting of heroes with separate origins who also operate individually, yet will team up to confront larger threats. The shared setting or "universes" of Marvel, DC and other publishers also allow for regular superhero crossover team-ups. Some superheroes, especially those introduced in the 1940s, work with a young sidekick (e.g. Batman and Robin, Captain America and Bucky). This has become less common since more sophisticated writing and older audiences have lessened the need for characters who specifically appeal to child readers. Sidekicks are seen as a separate classification of superheroes.		Although superhero fiction is considered a form of fantasy/adventure, it crosses into many genres. Some superhero franchises resemble crime fiction (e.g. Batman, Spider-Man, Daredevil), horror fiction (e.g. Hellboy, The Spectre), paranoid fiction (e.g. Watchmen,[41] Marvelman/Miracleman), and conventional science fiction (e.g. Green Lantern, Guardians of the Galaxy). Many of the earliest superheroes, such as the Sandman and the Clock, were rooted in the pulp fiction of their predecessors.		Within their own fictional universes, public perception of superheroes varies greatly. Some, like Superman and the Fantastic Four, are adored and seen as important civic leaders or even celebrities. Others, like Hulk, Spider-Man and the characters of Watchmen, are met with public skepticism or outright hostility. A few, such as the X-Men and Doom Patrol, defend a populace that almost unanimously misunderstands and despises them.		A superhero's costume helps make him or her recognizable to the general public. Costumes are often colorful to enhance the character's visual appeal and frequently incorporate the superhero's name and theme. For example, Daredevil resembles a red devil, Captain America's costume echoes the American flag, Batman's costume resembles a large bat, and Spider-Man's costume features a spiderweb pattern. The convention of superheroes wearing masks (frequently without visible pupils) and skintight unitards originated with Lee Falk's comic strip The Phantom.		Many features of superhero costumes recur frequently, including the following:		Many superheroes (and supervillains) operate from a base or headquarters. These bases are often equipped with state-of-the-art, highly advanced, or alien technologies. They are typically set in disguised or in secret locations to avoid being detected by enemies or the general public (for example, Superman's Fortress of Solitude or the Batcave). Some bases, such as the Baxter Building or the Hall of Justice, are common public knowledge (even if their precise location may remain secret). Since the genre's creation, these bases of operation have become increasingly grandiose. [42] Many heroes and villains who do not have a permanent headquarters may have a mobile base of operations instead.		To the heroes and villains who have secret bases, these bases can serve a variety of functions, including (but not limited to) the following:		Most dictionary definitions[6][43] and common usages of the term are generic and not limited to the characters of any particular company or companies.		Nevertheless, variations on the term "Super Hero" are jointly claimed by DC Comics and Marvel Comics as trademarks in the United States. Registrations of "Super Hero" marks have been maintained by DC and Marvel since the 1960s,[44] including U.S. Trademark Serial Nos. 72243225 and 73222079. In 2009, the term "Super Heroes" was registered as a typography-independent "descriptive" US trademark co-owned by DC and Marvel.[45] Both DC Comics and Marvel Comics have been assiduous in protecting their rights in the "Super Hero" trademarks in jurisdictions where the registrations are in force, including the United States, the United Kingdom, and Australia, and including in respect of various goods and services falling outside comic book publications. [46]		Critics in the legal community dispute whether the "Super Hero" marks meet the legal standard for trademark protection in the United States-distinctive designation of a single source of a product or service. Controversy exists over each element of that standard: whether "Super Hero" is distinctive rather than generic, whether "Super Hero" designates a source of products or services, and whether DC and Marvel jointly represent a single source.[47] Some critics further characterize the marks as a misuse of trademark law to chill competition.[48] To date, aside from a failed trademark removal action brought in 2016 against DC Comics' and Marvel Comics' United Kingdom registration, no dispute involving the trademark "Super Hero" has ever been to trial or hearing.[49]		In keeping with their origins as representing the archetypical hero stock character in 1930s American comics, superheroes are predominantly depicted as white Anglo-Saxon American middle- or upper-class heterosexual young adult males who are typically tall, athletic, educated, physically attractive and in perfect health. Beginning in the 1960s with the civil rights movement in the United States, and increasingly with the rising concern over political correctness in the 1980s, superhero fiction centered on cultural, ethnic, national and racial minority groups (from the perspective of US demographics) began to be produced. This began with depiction of black superheroes in the 1960s, followed in the 1970s with a number of other ethnic superheroes.[50] In keeping with the political mood of the time, cultural diversity and inclusivism would be an important part of superhero groups starting from the 1980s. In the 1990s, this was further augmented by the first depictions of superheroes as homosexual.		In 1966, Marvel Comics introduced the Black Panther, an African monarch who became the first non-caricatured black superhero.[51] The first African-American superhero, the Falcon, followed in 1969, and three years later, Luke Cage, a self-styled "hero-for-hire", became the first black superhero to star in his own series. In 1989, the Monica Rambeau incarnation of Captain Marvel was the first female black superhero from a major publisher to get her own title in a special one-shot issue. In 1971, Red Wolf became the first Native American in the superheroic tradition to headline a series.[52] In 1973, Shang-Chi became the first prominent Asian superhero to star in an American comic book (Kato had been the deuteragonist of the Green Hornet media franchise series since its inception in the 1930s.[53]). Kitty Pryde, a member of the X-Men, was an openly Jewish superhero in mainstream American comic books as early as 1978.[54]		Comic-book companies were in the early stages of cultural expansion and many of these characters played to specific stereotypes; Cage and many of his contemporaries often employed lingo similar to that of blaxploitation films, Native Americans were often associated with shamanism and wild animals, and Asian Americans were often portrayed as kung fu martial artists. Subsequent minority heroes, such as the X-Men's Storm and the Teen Titans' Cyborg avoided such conventions; they were both part of ensemble teams, which became increasingly diverse in subsequent years. The X-Men, in particular, were revived in 1975 with a line-up of characters culled from several nations, including the Kenyan Storm, German Nightcrawler, Russian Colossus, Irish Banshee, and Japanese Sunfire. In 1993, Milestone Comics, an African-American-owned media/publishing company entered into a publishing agreement with DC Comics that allowed them to introduce a line of comics that included characters of many ethnic minorities. Milestone's initial run lasted four years, during which it introduced Static, a character adapted into the WB Network animated series Static Shock.		In addition to the creation of new minority heroes, publishers have filled the identities and roles of once-Caucasian heroes with new characters from minority backgrounds. The African-American John Stewart appeared in the 1970s as an alternate for Earth's Green Lantern Hal Jordan, and would become a regular member of the Green Lantern Corps from the 1980s onward. The creators of the 2000s-era Justice League animated series selected Stewart as the show's Green Lantern. In the Ultimate Marvel universe, Miles Morales, a multiracial American youth who was also bitten by a genetically-altered spider, debuted as the new Spider-Man after the apparent death of the original. Kamala Khan, a Pakistani-American teenager who is revealed to have Inhuman lineage after her shapeshifting powers manifested, takes on the identity of Ms. Marvel in 2014. Her self-titled comic book series became a cultural phenomenon, with extensive media coverage by CNN, the New York Times and The Colbert Report, and embraced by anti-Islamophobia campaigners in San Francisco who plastered over anti-Muslim bus adverts with Kamala stickers.[55] Other such successor-heroes of color include James "Rhodey" Rhodes as Iron Man, Ryan Choi as the Atom, and Jaime Reyes as Blue Beetle.		Certain established characters have had their ethnicity changed when adapted to another continuity or media. A notable example is Nick Fury, who is reinterpreted as African-American both in the Ultimate Marvel as well as the Marvel Cinematic Universe continuities.		In 1992, Marvel revealed that Northstar, a member of the Canadian mutant superhero team Alpha Flight, was homosexual, after years of implication.[56] This ended a long-standing editorial mandate that there would be no homosexual characters in Marvel comics.[57] Although some minor secondary characters in DC Comics' mature-audience 1980s miniseries Watchmen were gay, and the reformed supervillain Pied Piper came out to Wally West in an issue of The Flash in 1991, Northstar is considered to be the first openly gay superhero appearing in mainstream comic books. From the mid-2000s onward, several established Marvel and DC comics characters (or a variant version of the pre-existing character) were outed or reintroduced as LGBT individuals by both publishers. Examples include the Mikaal Tomas incarnation of Starman in 1998; Colossus in the Ultimate X-Men series; Renee Montoya in DC's Gotham Central series in 2003; the Kate Kane incarnation of Batwoman in 2006; Rictor and Shatterstar in an issue of X-Factor in 2009; the Golden Age Green Lantern Alan Scott is reimagined as openly gay following The New 52 reboot;[58][59] and in 2015, a younger time displaced version of Iceman in an issue of All-New X-Men.[60]		Many new openly gay, lesbian and bisexual characters have since emerged in superhero fiction, such as Gen¹³'s Rainmaker, Apollo and Midnighter of The Authority, and Wiccan and Hulkling of the Young Avengers. Notable transgender or gender bending characters are fewer in number by comparison: the alter ego of superheroine Zsazsa Zaturnnah, a seminal character in Philippine popular culture,[61] is an effeminate gay man who transforms into a female superhuman after ingesting a magical stone. Desire from Neil Gaiman's The Sandman series and Xavin from the Runaways are both characters who could (and often) change their gender at will. Alysia Yeoh, a supporting character created by writer Gail Simone for the Batgirl ongoing series published by DC Comics, received substantial media attention in 2011 for being the first major transgender character written in a contemporary context in a mainstream American comic book.[62]		The Sailor Moon series is known for featuring a substantial number of openly LGBT characters since its inception, as Japan have traditionally been more open about portraying homosexuality in its children's media compared to many countries in the West.[63][64] Certain characters who are presented as homosexual or transgender in one continuity may not be presented as such in others, particularly with dubbed versions made for international release.[65]		
Being overweight is having more body fat than is optimally healthy. Being overweight is especially common where food supplies are plentiful and lifestyles are sedentary.		Excess weight has reached epidemic proportions globally, with more than 1 billion adults being either overweight or obese in 2003.[1] In 2013 this increased to more than 2 billion.[2] Increases have been observed across all age groups.		A healthy body requires a minimum amount of fat for proper functioning of the hormonal, reproductive, and immune systems, as thermal insulation, as shock absorption for sensitive areas, and as energy for future use. But the accumulation of too much storage fat can impair movement, flexibility, and alter the appearance of the body.						The degree to which a person is overweight is generally described by the body mass index (BMI). Overweight is defined as a BMI of 25 or more, thus it includes pre-obesity defined as a BMI between 25 and 30 and obesity as defined by a BMI of 30 or more.[3][4] Pre-obese and overweight however are often used interchangeably, thus giving overweight a common definition of a BMI of between 25–30. There are, however, several other common ways to measure the amount of adiposity or fat present in an individual's body.		The most common method for discussing this subject and the one used primarily by researchers and advisory institutions is BMI. Definitions of what is considered overweight vary by ethnicity. The current definition proposed by the US National Institutes of Health (NIH) and the World Health Organization (WHO) designates whites, Hispanics and blacks with a BMI of 25 or more as overweight. For Asians, overweight is a BMI between 23 and 29.9 and obesity for all groups is a BMI of 30 or more.		BMI, however, does not account extremes of muscle mass, some rare genetic factors, the very young, and a few other individual variations. Thus it is possible for an individuals with a BMI of less than 25 to have excess body fat, while others may have a BMI that is significantly higher without falling into this category.[7] Some of the above methods for determining body fat are more accurate than BMI but come with added complexity.		If an individual is overweight and has excess body fat it can create or lead to health risks. Reports are surfacing, however, that being mildly overweight to slightly obese – BMI being between 24 and 31.9 – may be actually beneficial and that people with a BMI between 24 and 31.9 could actually live longer than normal weight or underweight persons.[8][9]		While the negative health outcomes associated with obesity are accepted within the medical community, the health implications of the overweight category are more controversial. The generally accepted view is that being overweight causes similar health problems to obesity, but to a lesser degree. A 2016 review estimated that the risk of death increases by seven percent among overweight people with a BMI of 25 to 27.5 and 20 percent among overweight people with a BMI of 27.5 to 30.[10] The Framingham heart study found that being overweight at age 40 reduced life expectancy by three years.[11] Being overweight also increases the risk of oligospermia and azoospermia in men.[12]		Katherine Flegal et al., however, found that the mortality rate for individuals who are classified as overweight (BMI 25 to 30) may actually be lower than for those with an "ideal" weight (BMI 18.5 to 25).[13][14]		Being overweight has been identified as a cause of cancer, and is projected to overtake smoking as the primary cause of cancer in developed countries as cases of cancer linked to smoking dwindle.[15][not in citation given]		Psychological well-being is also at risk in the overweight individual due to social discrimination. However, children under the age of eight are normally not affected.[16]		Being overweight has been shown not to increase mortality in older people: in a study of 70 to 75-year old Australians, mortality was lowest for "overweight" individuals (BMI 25 to 30),[17] while a study of Koreans found that, among those initially aged 65 or more, an increase in BMI to above 25 was not associated with increased risk of death.[18]		Being overweight is generally caused by the intake of more calories (by eating) than are expended by the body (by exercise and everyday activity). Factors that may contribute to this imbalance include:		People who have insulin dependant diabetes and chronically overdose insulin may gain weight, while people who already are overweight may develop insulin tolerance, and in the long run develop type II diabetes.		The usual treatments for overweight individuals is diet and physical exercise.		Dietitians generally recommend eating several balanced meals dispersed through the day, with a combination of progressive, primarily aerobic, physical exercise.		Because these general treatments help most case of obesity, they are common in all levels of overweight individuals.		As much as 64% of the United States' adult population is considered either overweight or obese, and this percentage has increased over the last four decades.[19]		
Newsweek is an American weekly news magazine founded in 1933. It was published in four English-language editions and 12 global editions written in the language of the circulation region.		Between 2008 and 2012, Newsweek underwent internal and external contractions designed to shift the magazine's focus and audience while improving its finances. Instead, losses accelerated: revenue dropped 38 percent from 2007 to 2009. The revenue declines prompted an August 2010 sale by owner The Washington Post Company to audio pioneer Sidney Harman—for a purchase price of one dollar and an assumption of the magazine's liabilities.[2][3]		In November 2010, Newsweek merged with the news and opinion website The Daily Beast, forming The Newsweek Daily Beast Company, after negotiations between the owners of the two publications. Tina Brown, The Daily Beast's editor-in-chief, served as the editor of both publications. Newsweek was jointly owned by the estate of the late Harman and the diversified American media and Internet company IAC.[4][5]		Newsweek ceased print publication with the December 31, 2012, issue and transitioned to an all-digital format, called Newsweek Global.[6][7][8]		On August 3, 2013, IBT Media announced it had acquired Newsweek from IAC on terms that were not disclosed; the acquisition included the Newsweek brand and its online publication, but did not include The Daily Beast.[9] IBT Media relaunched a print edition of Newsweek on March 7, 2014.[10][11]						In 2003, worldwide circulation was more than 4 million, including 2.7 million in the U.S; by 2010 it reduced to 1.5 million (with newsstand sales declining to just over 40,000 copies per week). Newsweek publishes editions in Japanese, Korean, Polish, Spanish, Rioplatense Spanish, Arabic, and Turkish, as well as an English language Newsweek International. Russian Newsweek, published since 2004, was shut in October 2010.[12] The Bulletin (an Australian weekly until 2008) incorporated an international news section from Newsweek.		Based in New York City, the magazine claimed 22 bureaus in 2011: nine in the U.S.: New York City, Los Angeles, Chicago/Detroit, Dallas, Miami, Washington, D.C., Boston and San Francisco, and others overseas in London, Paris, Berlin, Moscow, Jerusalem, Baghdad, Tokyo, Hong Kong, Beijing, South Asia, Cape Town, Mexico City and Buenos Aires.[citation needed]		According to a 2015 column in the NY Post ("Media Ink": March 6, 2015), Newsweek's circulation had fallen to "just over 100,000" with staff at that time numbering "about 60 editorial staffers," up from a low of "less than 30 editorial staffers" in 2013, but with announced plans then to grow the number to "close to 100 in the next year." [13]				News-Week was launched in 1933 by Thomas J.C. Martyn, a former foreign-news editor for Time. He obtained financial backing from a group of U.S. stockholders "which included Ward Cheney, of the Cheney silk family, John Hay Whitney, and Paul Mellon, son of Andrew W. Mellon". Paul Mellon's ownership in Newsweek apparently represented "the first attempt of the Mellon family to function journalistically on a national scale."[14] The group of original owners invested around $2.5 million. Other large stockholders prior to 1946 were public utilities investment banker Stanley Childs and Wall Street corporate lawyer Wilton Lloyd-Smith.		Journalist Samuel T. Williamson served as the first editor-in-chief of Newsweek. The first issue of the magazine was dated 17 February 1933. Seven photographs from the week's news were printed on the first issue's cover.[15]		In 1937 News-Week merged with the weekly journal Today, which had been founded in 1932 by future New York Governor and diplomat W. Averell Harriman, and Vincent Astor of the prominent Astor family. As a result of the deal, Harriman and Astor provided $600,000 in venture capital funds and Vincent Astor became both the chairman of the board and its principal stockholder between 1937 and his death in 1959.[citation needed]		In 1937 Malcolm Muir took over as president and editor-in-chief. He changed the name to Newsweek, emphasized interpretive stories, introduced signed columns, and launched international editions. Over time the magazine developed a broad spectrum of material, from breaking stories and analysis to reviews and commentary.[citation needed]				The magazine was purchased by The Washington Post Company in 1961.[16]		Osborn Elliott was named editor of Newsweek in 1961 and became the editor in chief in 1969.		In 1970, Eleanor Holmes Norton represented sixty female employees of Newsweek who had filed a claim with the Equal Employment Opportunity Commission that Newsweek had a policy of only allowing men to be reporters.[17] The women won, and Newsweek agreed to allow women to be reporters.[17] The day the claim was filed, Newsweek's cover article was "Women in Revolt", covering the feminist movement; the article was written by a woman who had been hired on a freelance basis since there were no female reporters at the magazine.[18]		Edward Kosner became editor from 1975 to 1979 after directing the magazine’s extensive coverage of the Watergate scandal that led to the resignation of President Richard Nixon in 1974.		Richard M. Smith became Chairman in 1998, the year that the magazine inaugurated its "Best High Schools in America" list,[19] a ranking of public secondary schools based on the Challenge Index, which measures the ratio of Advanced Placement or International Baccalaureate exams taken by students to the number of graduating students that year, regardless of the scores earned by students or the difficulty in graduating. Schools with average SAT scores above 1300 or average ACT scores above 27 are excluded from the list; these are categorized instead as "Public Elite" High Schools. In 2008, there were 17 Public Elites.[20]		Smith resigned as board chairman in December 2007.[21]				During 2008–2009, Newsweek undertook a dramatic business restructuring.[22][23] Citing difficulties in competing with online news sources to provide unique news in a weekly publication, the magazine refocused its content on opinion and commentary beginning with its May 24, 2009, issue. It shrank its subscriber rate base, from 3.1 million to 2.6 million in early 2008, to 1.9 million in July 2009 and then to 1.5 million in January 2010—a decline of 50% in one year. Meacham described his strategy as "counterintuitive" as it involved discouraging renewals and nearly doubling subscription prices as it sought a more affluent subscriber base for its advertisers.[24] During this period, the magazine also laid off staff. While advertising revenues were down almost 50% compared to the prior year, expenses were also diminished, whereby the publishers hoped Newsweek would return to profitability.[25]		The financial results for 2009 as reported by The Washington Post Company showed that advertising revenue for Newsweek was down 37% in 2009 and the magazine division reported an operating loss for 2009 of $29.3 million compared to a loss of $16 million in 2008.[26] During the first quarter of 2010, the magazine lost nearly $11 million.[27]		By May 2010, Newsweek had been losing money for the past two years and was put up for sale.[28] The sale attracted international bidders. One bidder was Syrian entrepreneur Abdulsalam Haykal, CEO of Syrian publishing company Haykal Media, who brought together a coalition of Middle Eastern investors with his company. Haykal later claimed his bid was ignored by Newsweek's bankers, Allen & Co.[29]		The magazine was sold to audio pioneer Sidney Harman on August 2, 2010, for $1 in exchange for assuming the magazine's financial liabilities.[3][30] Harman's bid was accepted over three competitors.[31] Meacham left the magazine upon completion of the sale. Sidney Harman was the husband of Jane Harman, at that time a member of Congress from California.		At the end of 2010, Newsweek merged with the online publication The Daily Beast, following extensive negotiations between the respective proprietors. Tina Brown, The Daily Beast's editor-in-chief, became editor of both publications. The new entity, The Newsweek Daily Beast Company, was 50% owned by IAC/InterActiveCorp and 50% by Harman.[4][5][32]		The goal of The Newsweek Daily Beast Company was to have The Daily Beast be a source of instant analysis of the news, while Newsweek would serve to take a look at the bigger picture, provide deeper analysis, and "connect the dots," in the words of Harman, and for both publications to ultimately be profitable.[citation needed]		During her tenure as editor-in-chief of Newsweek, Brown has taken the news weekly in a different direction from her predecessor. Whereas Jon Meacham looked to make the focus solely on politics and world affairs, Brown brought the focus back onto all of current events, not just politics, business, and world affairs (although these issues are still the focus of the magazine). This was evidenced by an increased attention to fashion and pop culture as seen in many of her covers since taking the job.[citation needed]				Newsweek was redesigned in March 2011.[33] The new Newsweek moved the "Perspectives" section to the front of the magazine, where it served essentially as a highlight reel of the past week on The Daily Beast. More room was made available in the front of the magazine for columnists, editors, and special guests. A new "News Gallery" section featured two-page spreads of photographs from the week with a brief article accompanying each one. The "NewsBeast" section featured short articles, a brief interview with a newsmaker, and several graphs and charts for quick reading in the style of The Daily Beast. This is where the Newsweek staple "Conventional Wisdom" was located. Brown retained Newsweek's focus on in-depth, analytical features and original reporting on politics and world affairs, as well as a new focus on longer fashion and pop culture features. A larger culture section named "Omnivore" featured art, music, books, film, theater, food, travel, and television, including a weekly "Books" and "Want" section. The back page was reserved for a "My Favorite Mistake" column written by celebrity guest columnists about a mistake they made that defines who they are.[33]		On July 25, 2012, the company operating Newsweek indicated the publication was likely to go digital to cover its losses and could undergo other changes by the next year. Barry Diller, Chairman of the conglomerate IAC/InterActiveCorp, said his firm was looking at options since its partner in the Newsweek/Daily Beast operation had pulled out.[34]		On October 18, 2012, the company announced that the American print edition would be discontinued at the end of 2012 after 80 years of publication, citing the increasing difficulty of maintaining a paper weekly magazine in the face of declining advertising and subscription revenues and increasing costs for print production and distribution.[6] The online edition is named "Newsweek Global".[8] The success of the relaunched print edition (see below) suggests that IAC's strategy, which was then consistent with the industry-wide rush to digital, was short sighted. Ironically the last print edition sought to capitalise on 'print fetish' by sensationalising the end of the format.		In April 2013, IAC Chairman and Founder Barry Diller stated at the Milken Global Conference that he "wished he hadn't bought" Newsweek because his company had lost money on the magazine and called the purchase a "mistake" and a "fool's errand."[35]		On August 3, 2013, IBT Media acquired Newsweek from IAC on terms that were not disclosed; the acquisition included the Newsweek brand and its online publication, but did not include The Daily Beast.[9]		On March 7, 2014, IBT Media relaunched a print edition of Newsweek[10] with a cover story on the alleged creator of Bitcoin, which was widely criticized for its lack of substantive evidence.[11]		IBT Media returned the publication to profitability on October 8, 2014.[36]		In January 2015, the Serbian edition of the magazine, Newsweek Serbia, was to be relaunched under license to Adria Media Group.[37]		In February 2017, IBT Media appointed Matt McAllester, then Editor of Newsweek International, as Global Editor-in-chief of Newsweek.[38]		In 1970, Eleanor Holmes Norton represented sixty female employees of Newsweek who had filed a claim with the Equal Employment Opportunity Commission that Newsweek had a policy of only allowing men to be reporters.[17] The women won, and Newsweek agreed to allow women to be reporters.[17] The day the claim was filed, Newsweek's cover article was "Women in Revolt", covering the feminist movement; the article was written by Helen Dudar, a freelancer, on the belief that there were no female writers at the magazine capable of handling the assignment. Those passed over included Elizabeth Peer, who had spent five years in Paris as a foreign correspondent.[39]		The 1986 cover of Newsweek featured an article that said "women who weren't married by 40 had a better chance of being killed by a terrorist than of finding a husband".[40][41] Newsweek eventually apologized for the story and in 2010 launched a study that discovered 2 in 3 women who were 40 and single in 1986 had married since.[40][42] The story caused a "wave of anxiety" and some "skepticism" amongst professional and highly educated women in the United States.[40][42] The article was cited several times in the 1993 Hollywood film Sleepless in Seattle starring Tom Hanks and Meg Ryan.[40][43] Comparisons have been made with this article and the current rising issues surrounding the social stigma of unwed women in Asia called sheng nu.[40]		Former Alaska Governor and 2008 Republican Vice Presidential nominee Sarah Palin was featured on the cover of the November 23, 2009, issue of Newsweek, with the caption "How do you Solve a Problem Like Sarah?" featuring an image of Palin in athletic attire and posing. Palin herself, the Los Angeles Times and other commentators accused Newsweek of sexism for their choice of cover in the November 23, 2009 issue discussing Palin's book, Going Rogue: An American Life. "It's sexist as hell," wrote Lisa Richardson for the Los Angeles Times.[44] Taylor Marsh of The Huffington Post called it "the worst case of pictorial sexism aimed at political character assassination ever done by a traditional media outlet."[45] David Brody of CBN News stated: "This cover should be insulting to women politicians."[46] The cover includes a photo of Palin used in the August 2009 issue of Runner's World.[47][48][49] The photographer may have breached his contract with Runner's World when he permitted its use in Newsweek, as Runner's World maintained certain rights to the photo until August 2010. It is uncertain, however, whether this particular use of the photo was prohibited.[50]		Minnesota Republican Congresswoman and presidential candidate Michele Bachmann was featured on the cover of Newsweek magazine in August 2011, dubbed "the Queen of Rage".[51] The photo of her was perceived as unflattering, as it portrayed her with a wide eyed expression some said made her look "crazy".[52] Sources called the depiction "sexist",[53] and Sarah Palin denounced the publication. Newsweek defended the cover's depiction of her, saying its other photos of Bachmann showed similar intensity.[54]		In the May 9, 2005, issue of Newsweek, an article by reporter Michael Isikoff stated that interrogators at Guantanamo Bay "in an attempt to rattle suspects, flushed a Qur'an down a toilet." Detainees had earlier made similar complaints but this was the first time a government source had appeared to confirm the story. The news was reported to be a cause of widespread rioting and massive anti-American protests throughout some parts of the Islamic world (causing at least 15 deaths in Afghanistan).[55]		A 2004 study by Tim Groseclose and Jeff Milyo asserted that Newsweek, along with all other mainstream news outlets except for Fox News and the Wall Street Journal, exhibited a "liberal bias" .[56][57] However, media watchdog Media Matters for America described Groseclose's and Milyo's study as "riddled with flaws".[58] Eric Alterman, writing for the Center for American Progress, criticized the study for its "shockingly desultory intellectual underpinnings and almost comically obvious ideological imperatives".[59] Berkeley linguist Geoffrey Nunberg stated that Groseclose's and Milyo's work was "based on unsupported, ideology-driven premises" and suffered from "severe issues of data quality".[59][60]		Newsweek's Washington Bureau Chief and later Assistant Managing Editor Evan Thomas stated: "I think Newsweek is a little liberal," and, in 1996, "there is a liberal bias at Newsweek, the magazine I work for."[61]		Fareed Zakaria, a Newsweek columnist and editor of Newsweek International, attended a secret meeting on November 29, 2001, with a dozen policy makers, Middle East experts and members of influential policy research organizations that produced a report for President George W. Bush and his cabinet outlining a strategy for dealing with Afghanistan and the Middle East in the aftermath of September 11, 2001. The meeting was held at the request of Paul D. Wolfowitz, then the Deputy Secretary of Defense. The unusual presence of journalists, who also included Robert D. Kaplan of The Atlantic Monthly, at such a strategy meeting was revealed in Bob Woodward's 2006 book State of Denial: Bush at War, Part III. Woodward reported in his book that, according to Mr. Kaplan, everyone at the meeting signed confidentiality agreements not to discuss what happened. Mr. Zakaria told The New York Times that he attended the meeting for several hours but did not recall being told that a report for the President would be produced.[62] On October 21, 2006, after verification, the Times published a correction that stated:		An article in Business Day on Oct. 9 about journalists who attended a secret meeting in November 2001 called by Paul D. Wolfowitz, then the deputy secretary of defense, referred incorrectly to the participation of Fareed Zakaria, the editor of Newsweek International and a Newsweek columnist. Mr. Zakaria was not told that the meeting would produce a report for the Bush administration, nor did his name appear on the report.[62]		The cover story of the January 15, 2015, issue, titled What Silicon Valley Thinks of Women proved controversial, due to both its illustration, described as "the cartoon of a faceless female in spiky red heels, having her dress lifted up by a cursor arrow," and its content, described as "a 5,000-word article on the creepy, sexist culture of the tech industry."[63][64] Among those offended by the cover were Today Show co-host Tamron Hall, who commented "I think it’s obscene and just despicable, honestly." Newsweek editor in chief James Impoco explained "We came up with an image that we felt represented what that story said about Silicon Valley ... If people get angry, they should be angry."[64] The article's author, Nina Burleigh, asked, "Where were all these offended people when women like Heidi Roizen published accounts of having a venture capitalist stick her hand in his pants under a table while a deal was being discussed?"[65]		In January, 1998, Newsweek reporter Michael Isikoff was the first reporter to investigate allegations of a sexual relationship between U.S. President Bill Clinton and Monica Lewinsky, but the editors spiked the story.[66] The story soon surfaced online in the Drudge Report.		In the 2008 U.S. presidential election, the John McCain campaign wrote a lengthy letter to the editor criticizing a cover story in May 2008.[67]		Notable regular contributors to Newsweek have included:		1 2 3 4 5 6 7		NBC Wall Street Journal Agence France-Presse MSNBC Bloomberg BNA Washington Examiner Talk Media News/Univision		Fox News CBS Radio AP Radio Foreign Pool Time Yahoo! News Dallas Morning News		CBS News Bloomberg McClatchy Washington Times SiriusXM Salem Radio Globe/Roll Call		AP NPR AURN The Hill Regionals Newsmax CBN		ABC News Washington Post Politico Fox News Radio CSM/NY Post Daily Mail BBC/OAN		Reuters NY Times Chicago Tribune VOA RealClearPolitics HuffPost/NY Daily News BuzzFeed/Daily Beast		CNN USA Today ABC Radio National Journal Al Jazeera/PBS Westwood One Financial Times/Guardian		
Detroit (/dᵻˈtrɔɪt/[6]) is the most populous city in the U.S. state of Michigan, the largest city on the United States–Canada border, and the seat of Wayne County. The municipality of Detroit had a 2015 estimated population of 677,116, making it the 21st-most populous city in the United States. The metropolitan area, known as Metro Detroit, is home to 4.3 million people, making it the second-largest in the Midwest after Chicago.		Detroit is a major port on the Detroit River, one of the four major straits that connect the Great Lakes system to the Saint Lawrence Seaway. The Detroit Metropolitan Airport is among the most important hubs in the United States. The City of Detroit anchors the third-largest economic region in the Midwest, behind Chicago and Minneapolis, and the 14th-largest in the United States.[7] Detroit and its neighboring Canadian city Windsor are connected through a tunnel and the Ambassador Bridge, the busiest international crossing in North America.[8] Detroit is best known as the center of the American automobile industry, and the "Big Three" auto manufacturers General Motors, Ford, and Chrysler are all headquartered in Metro Detroit.		Detroit was founded on July 24, 1701 by the French explorer and adventurer Antoine de la Mothe Cadillac and a party of settlers. During the 19th century, it became an important industrial hub at the center of the Great Lakes region. With expansion of the auto industry in the early 20th century, the city and its suburbs experienced rapid growth, and by the 1940s, the city had become the fourth-largest in the country. However, due to industrial restructuring, the loss of jobs in the auto industry, and rapid suburbanization, Detroit lost considerable population from the late 20th century to the present. Since reaching a peak of 1.85 million at the 1950 census, Detroit's population has declined by more than 60 per cent.[3] In 2013, Detroit became the largest U.S. city to file for bankruptcy, which it successfully exited in December 2014, when the city government regained control of Detroit's finances.[9]		Detroit's diverse culture has had both local and international influence, particularly in music, with the city giving rise to the genres of Motown and techno, and playing an important role in the development of jazz, hip-hop, rock, and punk music. The erstwhile rapid growth of Detroit left a globally unique stock of architectural monuments and historic places of the first half of the 20th century, and since the 2000s conservation efforts managed to save many architectural pieces and allowed several large-scale revitalisations, including the restoration of several historic theatres and entertainment venues, highrise renovations, new sports stadiums, and a riverfront revitalization project. More recently, the population of Downtown Detroit, Midtown Detroit, and various other neighborhoods has increased. In 2015, Detroit was named a "City of Design" by UNESCO, the first U.S. city to receive that designation.[10]						Paleo-Indian people inhabited areas near Detroit as early as 11,000 years ago including the culture referred to as the Mound-builders.[11] In the 17th century, the region was inhabited by Huron, Odawa, Potawatomi and Iroquois peoples.[12]		The first Europeans did not penetrate into the region and reach the straits of Detroit until French missionaries and traders worked their way around the League of the Iroquois, with whom they were at war, and other Iroquoian tribes in the 1630s.[13] The north side of Lake Erie was held by the Huron and Neutral peoples until the 1650s, when the Iroquois pushed both and the Erie people away from the lake and its beaver-rich feeder streams in the Beaver Wars of 1649–1655.[13] By the 1670s, the war-weakened Iroquois laid claim to as far south as the Ohio River valley in northern Kentucky as hunting grounds,[13] and had absorbed many other Iroquoian peoples after defeating them in war.[13] For the next hundred years, virtually no British, colonist, or French action was contemplated without consultation with, or consideration of the Iroquois' likely response.[13] When the French and Indian War evicted the Kingdom of France from Canada, it removed one barrier to British colonists migrating west. (See main article).		British negotiations with the Iroquois would both prove critical and lead to a Crown policy limiting the west of the Alleghenies settlements below the Great Lakes, which gave many American would-be migrants a casus belli for supporting the American Revolution. The 1778 raids and resultant 1779 decisive Sullivan Expedition reopened the Ohio Country to westward emigration, which began almost immediately, and by 1800 white settlers were pouring westwards.		The city was named by French colonists, referring to the Detroit River (French: le détroit du lac Érié, meaning the strait of Lake Erie), linking Lake Huron and Lake Erie; in the historical context, the strait included the St. Clair River, Lake St. Clair and the Detroit River.[15][16]		On the shores of the strait, in 1701, the French officer Antoine de la Mothe Cadillac, along with fifty-one French people and French Canadians, founded a settlement called Fort Pontchartrain du Détroit, naming it after Louis Phélypeaux, comte de Pontchartrain, Minister of Marine under Louis XIV.[17] France offered free land to colonists to attract families to Detroit; when it reached a total population of 800 in 1765, it was the largest European settlement between Montreal and New Orleans, both also French settlements.[18] By 1773, the population of Detroit was 1,400. By 1778, its population was up to 2,144 and it was the third-largest city in the Province of Quebec.[19]		The region's economy was based on the lucrative fur trade, in which numerous Native American people had important roles. The flag of Detroit reflects its French colonial heritage. Descendants of the earliest French and French Canadian settlers formed a cohesive community, who gradually were replaced as the dominant population after more Anglo-American settlers came to the area in the early 19th century. Living along the shores of Lakes St. Clair, and south to Monroe and downriver suburbs, the French Canadians of Detroit, also known as Muskrat French, remain a subculture in the region today.[20][21]		During the French and Indian War (1754–63), the North American front of the Seven Years' War between Britain and France, British troops gained control of the settlement in 1760. They shortened the name to Detroit. Several Native American tribes launched Pontiac's Rebellion (1763), and conducted a siege of Fort Detroit, but failed to capture it. In defeat, France ceded its territory in North America east of the Mississippi to Britain following the war.		Following the American Revolutionary War and United States independence, Britain ceded Detroit along with other territory in the area under the Jay Treaty (1796), which established the northern border with Canada.[22] In 1805, fire destroyed most of the Detroit settlement, which consisted mostly of wooden buildings. A river warehouse and brick chimneys of the former wooden homes were the sole structures to survive.[23]		From 1805 to 1847, Detroit was the capital of Michigan (first the territory, then the state). Detroit surrendered without a fight to British troops during the War of 1812 in the Siege of Detroit. The Battle of Frenchtown (January 18–23, 1813) was part of a United States effort to retake the city, and American troops suffered their highest fatalities of any battle in the war. This battle is commemorated at River Raisin National Battlefield Park south of Detroit in Monroe County. Detroit was finally recaptured by the United States later that year.		It was incorporated as a city in 1815.[14] As the city expanded, a geometric street plan developed by Augustus B. Woodward was followed, featuring grand boulevards as in Paris.		Prior to the American Civil War, the city's access to the Canada–US border made it a key stop for refugee slaves gaining freedom in the North along the Underground Railroad. Many went across the Detroit River to Canada to escape pursuit by slave catchers.[14] There were estimated to be 20,000 to 30,000 African-American refugees who settled in Canada.[24] George DeBaptiste was considered to be the "president" of the Detroit Underground Railroad, William Lambert the "vice president" or "secretary" and Laura Haviland the "superintendent".[25]		Numerous men from Detroit volunteered to fight for the Union during the American Civil War, including the 24th Michigan Infantry Regiment (part of the legendary Iron Brigade), which fought with distinction and suffered 82% casualties at the Battle of Gettysburg in 1863. When the First Volunteer Infantry Regiment arrived to fortify Washington, DC, President Abraham Lincoln is quoted as saying "Thank God for Michigan!" George Armstrong Custer led the Michigan Brigade during the Civil War and called them the "Wolverines".[26]		During the late 19th century, several Gilded Age mansions reflecting the wealth of industry and shipping magnates were built east and west of the current downtown, along the major avenues of the Woodward plan. Most notable among them was the David Whitney House located at 4421 Woodward Avenue, which became a prime location for mansions. During this period some referred to Detroit as the Paris of the West for its architecture, grand avenues in the Paris style, and for Washington Boulevard, recently electrified by Thomas Edison.[14] The city had grown steadily from the 1830s with the rise of shipping, shipbuilding, and manufacturing industries. Strategically located along the Great Lakes waterway, Detroit emerged as a major port and transportation hub.		In 1896, a thriving carriage trade prompted Henry Ford to build his first automobile in a rented workshop on Mack Avenue. During this growth period, Detroit expanded its borders by annexing all or part of several surrounding villages and townships.		In 1903, Henry Ford founded the Ford Motor Company. Ford's manufacturing—and those of automotive pioneers William C. Durant, the Dodge Brothers, Packard, and Walter Chrysler—established Detroit's status in the early 20th century as the world's automotive capital.[14] The growth of the auto industry was reflected by changes in businesses throughout the Midwest and nation, with the development of garages to service vehicles and gas stations, as well as factories for parts and tires.		With the rapid growth of industrial workers in the auto factories, labor unions such as the American Federation of Labor and the United Auto Workers fought to organize workers to gain them better working conditions and wages. They initiated strikes and other tactics in support of improvements such as the 8-hour day/40-hour work week, increased wages, greater benefits and improved working conditions. The labor activism during those years increased influence of union leaders in the city such as Jimmy Hoffa of the Teamsters and Walter Reuther of the Autoworkers.		The city became the 4th-largest in the nation in 1920, after only New York City, Chicago and Philadelphia, with the influence of the booming auto industry.		The prohibition of alcohol from 1920 to 1933 resulted in the Detroit River becoming a major conduit for smuggling of illegal Canadian spirits.[27]		Detroit, like many places in the United States, developed racial conflict and discrimination in the 20th century following rapid demographic changes as hundreds of thousands of new workers were attracted to the industrial city; in a short period it became the 4th-largest city in the nation. The Great Migration brought rural blacks from the South; they were outnumbered by southern whites who also migrated to the city. Immigration brought southern and eastern Europeans of Catholic and Jewish faith; these new groups competed with native-born whites for jobs and housing in the booming city. Detroit was one of the major Midwest cities that was a site for the dramatic urban revival of the Ku Klux Klan beginning in 1915. "By the 1920s the city had become a stronghold of the KKK," whose members opposed Catholic and Jewish immigrants, as well as black Americans.[28] The Black Legion, a secret vigilante group, was active in the Detroit area in the 1930s, when one-third of its estimated 20,000 to 30,000 members in Michigan were based in the city. It was defeated after numerous prosecutions following the kidnapping and murder in 1936 of Charles Poole, a Catholic Works Progress Administration organizer. A total of 49 men of the Black Legion were convicted of numerous crimes, with many sentenced to life in prison for murder.		In the 1940s the world's "first urban depressed freeway" ever built, the Davison,[29] was constructed in Detroit. During World War II, the government encouraged retooling of the American automobile industry in support of the Allied powers, leading to Detroit's key role in the American Arsenal of Democracy.[30]		Jobs expanded so rapidly that 400,000 people were attracted to the city from 1941 to 1943, including 50,000 blacks in the second wave of the Great Migration, and 350,000 whites, many of them from the South. Some European immigrants and their descendants feared black competition for jobs and housing. The federal government prohibited discrimination in defense work but when in June 1943, Packard promoted three blacks to work next to whites on its assembly lines, 25,000 whites walked off the job.[31] The Detroit race riot of 1943 took place three weeks after the Packard plant protest. Over the course of three days, 34 people were killed, of whom 25 were African American, and approximately 600 were injured, 75% black people.[28][32]		Industrial mergers in the 1950s, especially in the automobile sector, increased oligopoly in the American auto industry. Detroit manufacturers such as Packard and Hudson merged into other companies and eventually disappeared. At its peak population of 1,849,568, in the 1950 Census, the city was the 5th-largest in the United States, after New York City, Chicago, Philadelphia and Los Angeles.		As in other major American cities in the postwar era, construction of an extensive highway and freeway system around Detroit and pent-up demand for new housing stimulated suburbanization; highways made commuting by car easier. In 1956, Detroit's last heavily used electric streetcar line along the length of Woodward Avenue was removed and replaced with gas-powered buses. It was the last line of what had once been a 534-mile network of electric streetcars. In 1941 at peak times, a streetcar ran on Woodward Avenue every 60 seconds.[33][34]		All of these changes in the area's transportation system favored low-density, auto-oriented development rather than high-density urban development, and industry also moved to the suburbs. The metro Detroit area developed as one of the most sprawling job markets in the United States by the 21st century, and combined with poor public transport, resulted in many jobs beyond the reach of urban low-income workers.[35]		In 1950, the city held about one-third of the state's population, anchored by its industries and workers. Over the next sixty years, the city's population declined to less than 10 percent of the state's population. During the same time period, the sprawling Detroit metropolitan area, which surrounds and includes the city, grew to contain more than half of Michigan's population.[14] The shift of population and jobs eroded Detroit's tax base.		In June 1963, Rev. Martin Luther King, Jr. gave a major speech in Detroit that foreshadowed his "I Have a Dream" speech in Washington, D.C. two months later. While the Civil Rights Movement gained significant federal civil rights laws in 1964 and 1965, longstanding inequities resulted in confrontations between the police and inner city black youth wanting change. Longstanding tensions in Detroit culminated in the Twelfth Street riot in July 1967. Governor George W. Romney ordered the Michigan National Guard into Detroit, and President Johnson sent in U.S. Army troops. The result was 43 dead, 467 injured, over 7,200 arrests, and more than 2,000 buildings destroyed, mostly in black residential and business areas. Thousands of small businesses closed permanently or relocated to safer neighborhoods. The affected district lay in ruins for decades.[36] It was the most costly riot in the United States.		On August 18, 1970, the NAACP filed suit against Michigan state officials, including Governor William Milliken, charging de facto public school segregation. The NAACP argued that although schools were not legally segregated, the city of Detroit and its surrounding counties had enacted policies to maintain racial segregation in public schools. The NAACP also suggested a direct relationship between unfair housing practices and educational segregation, which followed segregated neighborhoods.[37] The District Court held all levels of government accountable for the segregation in its ruling. The Sixth Circuit Court affirmed some of the decision, holding that it was the state's responsibility to integrate across the segregated metropolitan area.[38] The U.S. Supreme Court took up the case February 27, 1974.[37] The subsequent Milliken v. Bradley decision had wide national influence. In a narrow decision, the Court found that schools were a subject of local control and that suburbs could not be forced to solve problems in the city's school district.		"Milliken was perhaps the greatest missed opportunity of that period," said Myron Orfield, professor of law at the University of Minnesota. "Had that gone the other way, it would have opened the door to fixing nearly all of Detroit's current problems."[39] John Mogk, a professor of law and an expert in urban planning at Wayne State University in Detroit, says, "Everybody thinks that it was the riots [in 1967] that caused the white families to leave. Some people were leaving at that time but, really, it was after Milliken that you saw mass flight to the suburbs. If the case had gone the other way, it is likely that Detroit would not have experienced the steep decline in its tax base that has occurred since then."[39]		In November 1973, the city elected Coleman Young as its first black mayor. After taking office, Young emphasized increasing racial diversity in the police department.[40] Young also worked to improve Detroit's transportation system, but tension between Young and his suburban counterparts over regional matters was problematic throughout his mayoral term. In 1976, the federal government offered $600 million for building a regional rapid transit system, under a single regional authority.[41] But the inability of Detroit and its suburban neighbors to solve conflicts over transit planning resulted in the region losing the majority of funding for rapid transit. Following the failure to reach an agreement over the larger system, the City moved forward with construction of the elevated downtown circulator portion of the system, which became known as the Detroit People Mover.[42]		The gasoline crises of 1973 and 1979 also affected Detroit and the U.S. auto industry. Buyers chose smaller, more fuel-efficient cars made by foreign makers as the price of gas rose. Efforts to revive the city were stymied by the struggles of the auto industry, as their sales and market share declined. Automakers laid off thousands of employees and closed plants in the city, further eroding the tax base. To counteract this, the city used eminent domain to build two large new auto assembly plants in the city.[43]		As mayor, Young sought to revive the city by seeking to increase investment in the city's declining downtown. The Renaissance Center, a mixed-use office and retail complex, opened in 1977. This group of skyscrapers was an attempt to keep businesses in downtown.[14][44][45] Young also gave city support to other large developments to attract middle and upper-class residents back to the city. Despite the Renaissance Center and other projects, the downtown area continued to lose businesses to the automobile dependent suburbs. Major stores and hotels closed and many large office buildings went vacant. Young was criticized for being too focused on downtown development and not doing enough to lower the city's high crime rate and improve city services.		Long a major population center and site of worldwide automobile manufacturing, Detroit has suffered a long economic decline produced by numerous factors.[46][47][48] Like many industrial American cities, Detroit reached its population peak in the 1950 census. The peak population was 1.8 million people. Following suburbanization, industrial restructuring, and loss of jobs (as described above), by the 2010 census, the city had less than 40 percent of that number, with just over 700,000 residents. The city has declined in population in each census since 1950.[49][50]		High unemployment was compounded by middle-class flight to the suburbs, and some residents leaving the state to find work. The city was left with a higher proportion of poor in its population, reduced tax base, depressed property values, abandoned buildings, abandoned neighborhoods, high crime rates and a pronounced demographic imbalance.		In 1993 Young retired as Detroit's longest serving mayor, deciding not to seek a sixth term. That year the city elected Dennis Archer, a former Michigan Supreme Court justice. Archer prioritized downtown development and easing tensions with Detroit's suburban neighbors. A referendum to allow casino gambling in the city passed in 1996; several temporary casino facilities opened in 1999, and permanent downtown casinos with hotels opened in 2007–08.[51]		Campus Martius, a reconfiguration of downtown's main intersection as a new park was opened in 2004. The park has been cited as one of the best public spaces in the United States.[52][53][54] The city's riverfront has been the focus of redevelopment, following successful examples of other older industrial cities. In 2001, the first portion of the International Riverfront was completed as a part of the city's 300th anniversary celebration, with miles of parks and associated landscaping completed in succeeding years. In 2011, the Port Authority Passenger Terminal opened with the riverwalk connecting Hart Plaza to the Renaissance Center.[45]		Since 2006, $9 billion has been invested in downtown and surrounding neighborhoods; $5.2 billion of that has come in 2013 and 2014.[55] Construction activity, particularly rehabilitation of historic downtown buildings, has increased markedly. The number of vacant downtown buildings has dropped from nearly 50 to around 13.[when?][56] Among the most notable redevelopment projects are the Book Cadillac Hotel and the Fort Shelby Hotel; the David Broderick Tower; and the David Whitney Building.[44]		Little Caesars Arena, a new home for the Detroit Red Wings and the Detroit Pistons with attached residential, hotel, and retail use is under construction and set to open in fall 2017.[57] The plans for the project call for mixed-use residential on the blocks surrounding the arena and the renovation of the vacant 14-story Eddystone Hotel. It will be a part of The District in Detroit, a group of places owned by Olympia Entertainment Inc., including Comerica Park and the Detroit Opera House, among others.		Detroit's protracted decline has resulted in severe urban decay and thousands of empty buildings around the city. Some parts of Detroit are so sparsely populated that the city has difficulty providing municipal services. The city has considered various solutions, such as demolishing abandoned homes and buildings; removing street lighting from large portions of the city; and encouraging the small population in certain areas to move to more populated locations.[58][59][60][61][62] Roughly half of the owners of Detroit's 305,000 properties failed to pay their 2011 tax bills, resulting in about $246 million in taxes and fees going uncollected, nearly half of which was due to Detroit; the rest of the money would have been earmarked for Wayne County, Detroit Public Schools, and the library system.[63]		In September 2008, Mayor Kwame Kilpatrick (who had served for six years) resigned following felony convictions. In 2013, Kilpatrick was convicted on 24 federal felony counts, including mail fraud, wire fraud, and racketeering,[64] and was sentenced to 28 years in federal prison.[65] The former mayor's activities cost the city an estimated $20 million.[66] In 2013, felony bribery charges were brought against seven building inspectors.[67] In 2016, further corruption charges were brought against 12 principals, a former school superintendent and supply vendor[68] for a $12 million kickback scheme.[69][70] Law professor Peter Henning argues that Detroit's corruption is not unusual for a city its size, especially when compared with Chicago.[71]		The city's financial crisis resulted in the state of Michigan taking over administrative control of its government.[72] The state governor declared a financial emergency in March 2013, appointing Kevyn Orr as emergency manager. On July 18, 2013, Detroit became the largest U.S. city to file for bankruptcy.[73] It was declared bankrupt by U.S. District Court on December 3, 2013, in light of the city's $18.5 billion debt and its inability to fully repay its thousands of creditors.[74] On November 7, 2014 the city's plan for exiting bankruptcy was approved. The following month on December 11 the city officially exited bankruptcy. The plan allowed the city to eliminate $7 billion in debt and invest $1.7 billion into improved city services.[75]		One of the largest post bankruptcy efforts to improve city services has been work to fix the city's broken street lighting system. At one time it was estimated that 40% of lights were not working. The plan calls for replacing outdated high pressure sodium lights with 65,000 LED lights. Construction began in late 2014 and finished in December 2016 making Detroit the largest U.S city with all LED street lighting.[76]		In the 2010s, several initiatives were taken by Detroit's citizens and new inhabitants to improve the cityscape by renovating and revitalizing neighborhoods. Such include the Motor City Blight Busters[77] and various urban gardening movements.[78] The well-known symbol of the city's decades-long demise, the Michigan Central Station, is renovated with new windows, elevators and facilities since 2015.[79] Several other landmark buildings were fully renovated and transformed into condominiums, hotels, offices or for cultural uses. Detroit is mentioned as a city of renaissance.[80]		Detroit is the center of a three-county urban area (population 3,734,090, area of 1,337 square miles (3,460 km2), a 2010 United States Census) six-county metropolitan statistical area (2010 Census population of 4,296,250, area of 3,913 square miles [10,130 km2]), and a nine-county Combined Statistical Area (2010 Census population of 5,218,852, area of 5,814 square miles [15,060 km2]).[4][81][82]		According to the U.S. Census Bureau, the city has a total area of 142.87 square miles (370.03 km2), of which 138.75 square miles (359.36 km2) is land and 4.12 square miles (10.67 km2) is water.[2] Detroit is the principal city in Metro Detroit and Southeast Michigan situated in the Midwestern United States and the Great Lakes region.		The Detroit River International Wildlife Refuge is the only international wildlife preserve in North America, uniquely located in the heart of a major metropolitan area. The Refuge includes islands, coastal wetlands, marshes, shoals, and waterfront lands along 48 miles (77 km) of the Detroit River and Western Lake Erie shoreline.		The city slopes gently from the northwest to southeast on a till plain composed largely of glacial and lake clay. The most notable topographical feature in the city is the Detroit Moraine, a broad clay ridge on which the older portions of Detroit and Windsor sit, rising approximately 62 feet (19 m) above the river at its highest point.[83] The highest elevation in the city is located directly north of Gorham Playground on the northwest side approximately three blocks south of 8 Mile Road, at a height of 675 to 680 feet (206 to 207 m).[84] Detroit's lowest elevation is along the Detroit River, at a surface height of 572 feet (174 m).[85]		Belle Isle Park is a 982-acre (1.534 sq mi; 397 ha) island park in the Detroit River, between Detroit and Windsor, Ontario. It is connected to the mainland by the MacArthur Bridge in Detroit. Belle Isle Park contains such attractions as the James Scott Memorial Fountain, the Belle Isle Conservatory, the Detroit Yacht Club on an adjacent island, a half-mile (800 m) beach, a golf course, a nature center, monuments, and gardens. The city skyline may be viewed from the island.		Three road systems cross the city: the original French template, with avenues radiating from the waterfront; and true north–south roads based on the Northwest Ordinance township system. The city is north of Windsor, Ontario. Detroit is the only major city along the Canada–US border in which one travels south in order to cross into Canada.		Detroit has four border crossings: the Ambassador Bridge and the Detroit–Windsor Tunnel provide motor vehicle thoroughfares, with the Michigan Central Railway Tunnel providing railroad access to and from Canada. The fourth border crossing is the Detroit–Windsor Truck Ferry, located near the Windsor Salt Mine and Zug Island. Near Zug Island, the southwest part of the city was developed over a 1,500-acre (610 ha) salt mine that is 1,100 feet (340 m) below the surface. The Detroit salt mine run by the Detroit Salt Company has over 100 miles (160 km) of roads within.[86][87]		Detroit and the rest of southeastern Michigan have a humid continental climate (Köppen Dfa) which is influenced by the Great Lakes; the city and close-in suburbs are part of USDA Hardiness zone 6b, with farther-out northern and western suburbs generally falling in zone 6a.[89] Winters are cold, with moderate snowfall and temperatures not rising above freezing on an average 44 days annually, while dropping to or below 0 °F (−18 °C) on an average 4.4 days a year; summers are warm to hot with temperatures exceeding 90 °F (32 °C) on 12 days.[88] The warm season runs from May to September. The monthly daily mean temperature ranges from 25.6 °F (−3.6 °C) in January to 73.6 °F (23.1 °C) in July. Official temperature extremes range from 105 °F (41 °C) on July 24, 1934 down to −21 °F (−29 °C) on January 21, 1984; the record low maximum is −5 °F (−21 °C) on January 19, 1994, while, conversely the record high minimum is 80 °F (27 °C) on August 1, 2006, the most recent of five occurrences.[88] A decade or two may pass between readings of 100 °F (38 °C) or higher, which last occurred July 17, 2012. The average window for freezing temperatures is October 20 thru April 22, allowing a growing season of 180 days.[88]		Precipitation is moderate and somewhat evenly distributed throughout the year, although the warmer months such as May and June average more, averaging 33.5 inches (850 mm) annually, but historically ranging from 20.49 in (520 mm) in 1963 to 47.70 in (1,212 mm) in 2011.[88] Snowfall, which typically falls in measurable amounts between November 15 through April 4 (occasionally in October and very rarely in May),[88] averages 42.5 inches (108 cm) per season, although historically ranging from 11.5 in (29 cm) in 1881–82 to 94.9 in (241 cm) in 2013–14.[88] A thick snowpack is not often seen, with an average of only 27.5 days with 3 in (7.6 cm) or more of snow cover.[88] Thunderstorms are frequent in the Detroit area. These usually occur during spring and summer.[90]		Seen in panorama, Detroit's waterfront shows a variety of architectural styles. The post modern Neo-Gothic spires of the One Detroit Center (1993) were designed to blend with the city's Art Deco skyscrapers. Together with the Renaissance Center, they form a distinctive and recognizable skyline. Examples of the Art Deco style include the Guardian Building and Penobscot Building downtown, as well as the Fisher Building and Cadillac Place in the New Center area near Wayne State University. Among the city's prominent structures are United States' largest Fox Theatre, the Detroit Opera House, and the Detroit Institute of Arts.[93][94]		While the Downtown and New Center areas contain high-rise buildings, the majority of the surrounding city consists of low-rise structures and single-family homes. Outside of the city's core, residential high-rises are found in upper-class neighborhoods such as the East Riverfront extending toward Grosse Pointe and the Palmer Park neighborhood just west of Woodward. The University Commons-Palmer Park district in northwest Detroit, near the University of Detroit Mercy and Marygrove College, anchors historic neighborhoods including Palmer Woods, Sherwood Forest, and the University District.		The National Register of Historic Places lists several area neighborhoods and districts. Neighborhoods constructed prior to World War II feature the architecture of the times, with wood-frame and brick houses in the working-class neighborhoods, larger brick homes in middle-class neighborhoods, and ornate mansions in upper-class neighborhoods such as Brush Park, Woodbridge, Indian Village, Palmer Woods, Boston-Edison, and others.		Some of the oldest neighborhoods are along the Woodward and East Jefferson corridors. Some newer residential construction may also be found along the Woodward corridor, the far west, and northeast. Some of the oldest extant neighborhoods include West Canfield and Brush Park, which have both seen multimillion-dollar restorations and construction of new homes and condominiums.[44][95]		Many of the city's architecturally significant buildings have been listed on the National Register of Historic Places; the city has one of United States' largest surviving collections of late 19th- and early 20th-century buildings.[94] Architecturally significant churches and cathedrals in the city include St. Joseph's, Old St. Mary's, the Sweetest Heart of Mary, and the Cathedral of the Most Blessed Sacrament.[93]		The city has substantial activity in urban design, historic preservation, and architecture.[96] A number of downtown redevelopment projects—of which Campus Martius Park is one of the most notable—have revitalized parts of the city. Grand Circus Park stands near the city's theater district, Ford Field, home of the Detroit Lions, and Comerica Park, home of the Detroit Tigers.[93] Other projects include the demolition of the Ford Auditorium off of Jefferson St.		The Detroit International Riverfront includes a partially completed three-and-one-half mile riverfront promenade with a combination of parks, residential buildings, and commercial areas. It extends from Hart Plaza to the MacArthur Bridge accessing Belle Isle Park (the largest island park in a U.S. city). The riverfront includes Tri-Centennial State Park and Harbor, Michigan's first urban state park. The second phase is a two-mile (3.2-kilometre) extension from Hart Plaza to the Ambassador Bridge for a total of five miles (8.0 kilometres) of parkway from bridge to bridge. Civic planners envision that the pedestrian parks will stimulate residential redevelopment of riverfront properties condemned under eminent domain.		Other major parks include River Rouge (in the southwest side), the largest park in Detroit; Palmer (north of Highland Park) and Chene Park (on the east river downtown).[97]		Detroit has a variety of neighborhood types. The revitalized Downtown, Midtown, and New Center areas feature many historic buildings and are high density, while further out, particularly in the northeast and on the fringes,[98] high vacancy levels are problematic, for which a number of solutions have been proposed. In 2007, Downtown Detroit was recognized as a best city neighborhood in which to retire among the United States' largest metro areas by CNN Money Magazine editors.[99]		Lafayette Park is a revitalized neighborhood on the city's east side, part of the Ludwig Mies van der Rohe residential district.[100] The 78-acre (32 ha) development was originally called the Gratiot Park. Planned by Mies van der Rohe, Ludwig Hilberseimer and Alfred Caldwell it includes a landscaped, 19-acre (7.7 ha) park with no through traffic, in which these and other low-rise apartment buildings are situated.[100] Immigrants have contributed to the city's neighborhood revitalization, especially in southwest Detroit.[101] Southwest Detroit has experienced a thriving economy in recent years, as evidenced by new housing, increased business openings and the recently opened Mexicantown International Welcome Center.[102]		The city has numerous neighborhoods consisting of vacant properties resulting in low inhabited density in those areas, stretching city services and infrastructure. These neighborhoods are concentrated in the northeast and on the city's fringes.[98] A 2009 parcel survey found about a quarter of residential lots in the city to be undeveloped or vacant, and about 10% of the city's housing to be unoccupied.[98][104][105] The survey also reported that most (86%) of the city's homes are in good condition with a minority (9%) in fair condition needing only minor repairs.[104][105][106][107]		To deal with vacancy issues, the city has begun demolishing the derelict houses, razing 3,000 of the total 10,000 in 2010,[108] but the resulting low density creates a strain on the city's infrastructure. To remedy this, a number of solutions have been proposed including resident relocation from more sparsely populated neighborhoods and converting unused space to urban agricultural use, including Hantz Woodlands, though the city expects to be in the planning stages for up to another two years.[109][110]		Public funding and private investment have also been made with promises to rehabilitate neighborhoods. In April 2008, the city announced a $300-million stimulus plan to create jobs and revitalize neighborhoods, financed by city bonds and paid for by earmarking about 15% of the wagering tax.[109] The city's working plans for neighborhood revitalizations include 7-Mile/Livernois, Brightmoor, East English Village, Grand River/Greenfield, North End, and Osborn.[109] Private organizations have pledged substantial funding to the efforts.[111][112] Additionally, the city has cleared a 1,200-acre (490 ha) section of land for large-scale neighborhood construction, which the city is calling the Far Eastside Plan.[113] In 2011, Mayor Dave Bing announced a plan to categorize neighborhoods by their needs and prioritize the most needed services for those neighborhoods.[114]		In the 2010 United States Census, the city had 713,777 residents, ranking it the 18th most populous city in the United States.[3][49]		Of the large shrinking cities of the United States, Detroit has had the most dramatic decline in population of the past 60 years (down 1,135,791) and the second largest percentage decline (down 61.4%, second only to St. Louis, Missouri's 62.7%). While the drop in Detroit's population has been ongoing since 1950, the most dramatic period was the significant 25% decline between the 2000 and 2010 Census.[49]		The population collapse has resulted in large numbers of abandoned homes and commercial buildings, and areas of the city hit hard by urban decay.[58][59][60][61][62]		Detroit's 713,777 residents represent 269,445 households, and 162,924 families residing in the city. The population density was 5,144.3 people per square mile (1,895/km²). There were 349,170 housing units at an average density of 2,516.5 units per square mile (971.6/km²). Housing density has declined. The city has demolished thousands of Detroit's abandoned houses, planting some areas and in others allowing the growth of urban prairie.		Of the 269,445 households, 34.4% had children under the age of 18 living with them, 21.5% were married couples living together, 31.4% had a female householder with no husband present, 39.5% were non-families, 34.0% were made up of individuals, and 3.9% had someone living alone who is 65 years of age or older. Average household size was 2.59, and average family size was 3.36.		There is a wide distribution of age in the city, with 31.1% under the age of 18, 9.7% from 18 to 24, 29.5% from 25 to 44, 19.3% from 45 to 64, and 10.4% 65 years of age or older. The median age was 31 years. For every 100 females there were 89.1 males. For every 100 females age 18 and over, there were 83.5 males.		According to a 2014 study, 67% of the population of the city identified themselves as Christians, with 49% professing attendance Protestant churches, and 16% professing Roman Catholic beliefs,[116][117] while 24% claim no religious affiliation. Other religions collectively make up about 8% of the population.		The loss of industrial and working-class jobs in the city has resulted in high rates of poverty and associated problems.[118] From 2000 to 2009, the city's estimated median household income fell from $29,526 to $26,098.[119] As of 2010[update] the mean income of Detroit is below the overall U.S. average by several thousand dollars. Of every three Detroit residents, one lives in poverty. Luke Bergmann, author of Getting Ghost: Two Young Lives and the Struggle for the Soul of an American City, said in 2010, "Detroit is now one of the poorest big cities in the country."[120]		In the 2010 American Community Survey, median household income in the city was $25,787, and the median income for a family was $31,011. The per capita income for the city was $14,118. 32.3% of families had income at or below the federally defined poverty level. Out of the total population, 53.6% of those under the age of 18 and 19.8% of those 65 and older had income at or below the federally defined poverty line.		Oakland County in Metro Detroit, once rated amongst the wealthiest US counties per household, is no longer shown in the top 25 listing of Forbes magazine. But internal county statistical methods—based on measuring per capita income for counties with more than one million residents—show that Oakland is still within the top 12, slipping from the 4th-most affluent such county in the U.S. in 2004 to 11th-most affluent in 2009.[121][122][123] Detroit dominates Wayne County, which has an average household income of about $38,000, compared to Oakland County's $62,000.[124][125]		The city's population increased more than sixfold during the first half of the 20th century, fed largely by an influx of European, Middle Eastern (Lebanese, Assyrian/Chaldean), and Southern migrants to work in the burgeoning automobile industry.[129] In 1940, Whites were 90.4% of the city's population.[127] Since 1950 the city has seen a major shift in its population to the suburbs. In 1910, fewer than 6,000 blacks called the city home;[130] in 1930 more than 120,000 blacks lived in Detroit.[131] The thousands of African Americans who came to Detroit were part of the Great Migration of the 20th century.[132]		Detroit remains one of the most racially segregated cities in the United States.[133][134] From the 1940s to the 1970s a second wave of Blacks moved to Detroit to escape Jim Crow laws in the south and find jobs.[135] However, they soon found themselves excluded from white areas of the city—through violence, laws, and economic discrimination (e.g., redlining).[136] White residents attacked black homes: breaking windows, starting fires, and exploding bombs.[133][136] The pattern of segregation was later magnified by white migration to the suburbs.[134] One of the implications of racial segregation, which correlates with class segregation, may be overall worse health for some populations.[134][137]		While Blacks/African-Americans comprised only 13 percent of Michigan's population in 2010, they made up nearly 82 percent of Detroit's population. The next largest population groups were Whites, at 10 percent, and Hispanics, at 6 percent.[138] According to the 2010 Census, segregation in Detroit has decreased in absolute and in relative terms. In the first decade of the 21st century, about two-thirds of the total black population in metropolitan area resided within the city limits of Detroit.[139][140] The number of integrated neighborhoods has increased from 100 in 2000 to 204 in 2010. The city has also moved down the ranking, from number one most segregated to number four.[141] A 2011 op-ed in The New York Times attributed the decreased segregation rating to the overall exodus from the city, cautioning that these areas may soon become more segregated. This pattern already happened in the 1970s, when apparent integration was actually a precursor to white flight and resegregation.[133] Over a 60-year period, white flight occurred in the city. According to an estimate of the Michigan Metropolitan Information Center, from 2008 to 2009 the percentage of non-Hispanic White residents increased from 8.4% to 13.3%. Some empty nesters and many younger White people moved into the city while many African Americans moved to the suburbs.[142]		Detroit has a Mexican-American population. In the early 20th century thousands of Mexicans came to Detroit to work in agricultural, automotive, and steel jobs. During the Mexican Repatriation of the 1930s many Mexicans in Detroit were willingly repatriated or forced to repatriate. By the 1940s the Mexican community began to settle what is now Mexicantown. The population significantly increased in the 1990s due to immigration from Jalisco. In 2010 Detroit had 48,679 Hispanics, including 36,452 Mexicans. The number of Hispanics was a 70% increase from the number in 1990.[143]		After World War II, many people from Appalachia settled in Detroit. Appalachians formed communities and their children acquired southern accents.[144] Many Lithuanians settled in Detroit during the World War II era, especially on the city's Southwest side in the West Vernor area,[citation needed] where the renovated Lithuanian Hall reopened in 2006.[145][146]		In 2001, 103,000 Jews, or about 1.9% of the population, were living in the Detroit area, in both Detroit and Ann Arbor.[147]		As of 2002, of all of the municipalities in the Wayne County-Oakland County-Macomb County area, Detroit had the second largest Asian population. As of that year Detroit's percentage of Asians was 1%, far lower than the 13.3% of Troy.[148] By 2000 Troy had the largest Asian American population in the tricounty area, surpassing Detroit.[149]		As of 2002 there are four areas in Detroit with significant Asian and Asian American populations. Northeast Detroit has population of Hmong with a smaller group of Lao people. A portion of Detroit next to eastern Hamtramck includes Bangladeshi Americans, Indian Americans, and Pakistani Americans; nearly all of the Bangladeshi population in Detroit lives in that area. Many of those residents own small businesses or work in blue collar jobs, and the population in that area is mostly Muslim. The area north of Downtown Detroit; including the region around the Henry Ford Hospital, the Detroit Medical Center, and Wayne State University; has transient Asian national origin residents who are university students or hospital workers. Few of them have permanent residency after schooling ends. They are mostly Chinese and Indian but the population also includes Filipinos, Koreans, and Pakistanis. In Southwest Detroit and western Detroit there are smaller, scattered Asian communities including an area in the westside adjacent to Dearborn and Redford Township that has a mostly Indian Asian population, and a community of Vietnamese and Laotians in Southwest Detroit.[148]		As of 2006[update] the city has one of the U.S.'s largest concentrations of Hmong Americans.[150] In 2006, the city had about 4,000 Hmong and other Asian immigrant families. Most Hmong live east of Coleman Young Airport near Osborn High School. Hmong immigrant families generally have lower incomes than those of suburban Asian families.[151]				Several major corporations are based in the city, including three Fortune 500 companies. The most heavily represented sectors are manufacturing (particularly automotive), finance, technology, and health care. The most significant companies based in Detroit include: General Motors, Quicken Loans, Ally Financial, Compuware, Shinola, American Axle, Little Caesars, DTE Energy, Lowe Campbell Ewald, Blue Cross Blue Shield of Michigan, and Rossetti Architects.		About 80,500 people work in downtown Detroit, comprising one-fifth of the city's employment base.[153][154] Aside from the numerous Detroit-based companies listed above, downtown contains large offices for Comerica, Chrysler, HP Enterprise, Deloitte, PricewaterhouseCoopers, KPMG, and Ernst & Young. Ford Motor Company is located in the adjacent city of Dearborn.		Thousands more employees work in Midtown, north of the central business district. Midtown's anchors are the city's largest single employer Detroit Medical Center, Wayne State University, and the Henry Ford Health System in New Center. Midtown is also home to watchmaker Shinola and an array of small and startup companies. New Center bases TechTown, a research and business incubator hub that is part of the WSU system.[155] Like downtown and Corktown, Midtown also has a fast-growing retailing and restaurant scene.		A number of the city's downtown employers are relatively new, as there has been a marked trend of companies moving from satellite suburbs around Metropolitan Detroit into the downtown core.[citation needed] Compuware completed its world headquarters in downtown in 2003. OnStar, Blue Cross Blue Shield, and HP Enterprise Services are located at the Renaissance Center. PricewaterhouseCoopers Plaza offices are adjacent to Ford Field, and Ernst & Young completed its office building at One Kennedy Square in 2006. Perhaps most prominently, in 2010, Quicken Loans, one of the largest mortgage lenders, relocated its world headquarters and 4,000 employees to downtown Detroit, consolidating its suburban offices.[156] In July 2012, the U.S. Patent and Trademark Office opened its Elijah J. McCoy Satellite Office in the Rivertown/Warehouse District as its first location outside Washington, D.C.'s metropolitan area.[157]		In April 2014, the Department of Labor reported the city's unemployment rate at 14.5%.[158]		The city of Detroit and other private-public partnerships have attempted to catalyze the region's growth by facilitating the building and historical rehabilitation of residential high-rises in the downtown, creating a zone that offers many business tax incentives, creating recreational spaces such as the Detroit RiverWalk, Campus Martius Park, Dequindre Cut Greenway, and Green Alleys in Midtown. The city itself has cleared sections of land while retaining a number of historically significant vacant buildings in order to spur redevelopment;[159] though it has struggled with finances, the city issued bonds in 2008 to provide funding for ongoing work to demolish blighted properties.[109] Two years earlier, downtown reported $1.3 billion in restorations and new developments which increased the number of construction jobs in the city.[44] In the decade prior to 2006, downtown gained more than $15 billion in new investment from private and public sectors.[160]		Despite the city's recent financial issues, many developers remain unfazed by Detroit's problems.[161] Midtown is one of the most successful areas within Detroit to have a residential occupancy rate of 96%.[162] Numerous developments have been recently completely or are in various stages of construction. These include the $82 million reconstruction of downtown's David Whitney Building (now an Aloft Hotel and luxury residences), the Woodward Garden Block Development in Midtown, the residential conversion of the David Broderick Tower in downtown, the rehabilitation of the Book Cadillac Hotel (now a Westin and luxury condos) and Fort Shelby Hotel (now Doubletree) also in downtown, and various smaller projects.[163]		Downtown's population of young professionals is growing and retail is expanding.[164][165][166] A study in 2007 found out that Downtown's new residents are predominantly young professionals (57% are ages 25 to 34, 45% have bachelor's degrees, and 34% have a master's or professional degree),[153][164][167] a trend which has hastened over the last decade. John Varvatos is set to open a downtown store in 2015, and Restoration Hardware is rumored to be opening a store nearby.[168]		On July 25, 2013, Meijer, a midwestern retail chain, opened its first supercenter store in Detroit,;[169] this was a 20 million dollar, 190,000-square-foot store in the northern portion of the city and it also is the centerpiece of a new 72 million dollar shopping center named Gateway Marketplace.[170] On June 11, 2015, Meijer opened its second supercenter store in the city.[171]		On May 21, 2014, JPMorgan Chase announced that it was injecting $100 million over five years into Detroit's economy, providing development funding for a variety of projects that would increase employment. It is the largest commitment made to any one city by the nation's biggest bank.[citation needed] Of the $100 million, $50 million will go toward development projects, $25 million will go toward city blight removal, $12.5 million will go for job training, $7 million will go for small businesses in the city, and $5.5 million will go toward the M-1 light rail project (Qline).[172] On May 19, 2015, JPMorgan Chase announced that it has invested $32 million for two redevelopment projects in the city's Capitol Park district, the Capitol Park Lofts (the former Capitol Park Building) and the Detroit Savings Bank building at 1212 Griswold. Those investments are separate from Chase's five-year, $100-million commitment.[173] On May 10, 2017, J.P. Morgan Chase & Co. announced a $50 million increase in the $100 million investment the firm committed to economic development and neighborhood stabilization in Detroit by 2019. Half of the $150 million will be grants and the other half is going to toward a variety of loan funds for small business growth, mixed-use real estate development and residential housing projects. [174]		In the central portions of Detroit, the population of young professionals, artists, and other transplants is growing and retail is expanding.[164][165] This dynamic is luring additional new residents, and former residents returning from other cities, to the city's Downtown along with the revitalized Midtown and New Center areas.[153][164][165][167]		A desire to be closer to the urban scene has also attracted some young professionals to reside in inner ring suburbs such as Ferndale and Royal Oak, Michigan.[175] Detroit's proximity to Windsor, Ontario, provides for views and nightlife, along with Ontario's minimum drinking age of 19.[176] A 2011 study by Walk Score recognized Detroit for its above average walkability among large U.S. cities.[177] About two-thirds of suburban residents occasionally dine and attend cultural events or take in professional games in the city of Detroit.[178]		Known as the world's automotive center,[179] "Detroit" is a metonym for that industry.[180] Detroit's auto industry, some of which was converted to wartime defense production, was an important element of the American "Arsenal of Democracy" supporting the Allied powers during World War II.[181] It is an important source of popular music legacies celebrated by the city's two familiar nicknames, the Motor City and Motown.[182] Other nicknames arose in the 20th century, including City of Champions, beginning in the 1930s for its successes in individual and team sport;[183] The D; Hockeytown (a trademark owned by the city's NHL club, the Red Wings); Rock City (after the Kiss song "Detroit Rock City"); and The 313 (its telephone area code).[184][185]		Live music has been a prominent feature of Detroit's nightlife since the late 1940s, bringing the city recognition under the nickname 'Motown'.[186] The metropolitan area has many nationally prominent live music venues. Concerts hosted by Live Nation perform throughout the Detroit area. Large concerts are held at DTE Energy Music Theatre and The Palace of Auburn Hills. The city's theatre venue circuit is the United States' second largest and hosts Broadway performances.[187][188]		The city of Detroit has a rich musical heritage and has contributed to a number of different genres over the decades leading into the new millennium.[185] Important music events in the city include: the Detroit International Jazz Festival, the Detroit Electronic Music Festival, the Motor City Music Conference (MC2), the Urban Organic Music Conference, the Concert of Colors, and the hip-hop Summer Jamz festival.[185]		In the 1940s, Detroit blues artist John Lee Hooker became a long-term resident in the city's southwest Delray neighborhood. Hooker, among other important blues musicians migrated from his home in Mississippi bringing the Delta blues to northern cities like Detroit. Hooker recorded for Fortune Records, the biggest pre-Motown blues/soul label. During the 1950s, the city became a center for jazz, with stars performing in the Black Bottom neighborhood.[14] Prominent emerging Jazz musicians of the 1960s included: trumpet player Donald Byrd who attended Cass Tech and performed with Art Blakey and the Jazz Messengers early in his career and Saxophonist Pepper Adams who enjoyed a solo career and accompanied Byrd on several albums. The Graystone International Jazz Museum documents jazz in Detroit.[189]		Other, prominent Motor City R&B stars in the 1950s and early 1960s was Nolan Strong, Andre Williams and Nathaniel Mayer – who all scored local and national hits on the Fortune Records label. According to Smokey Robinson, Strong was a primary influence on his voice as a teenager. The Fortune label was a family-operated label located on Third Avenue in Detroit, and was owned by the husband and wife team of Jack Brown and Devora Brown. Fortune, which also released country, gospel and rockabilly LPs and 45s, laid the groundwork for Motown, which became Detroit's most legendary record label.[190]		Berry Gordy, Jr. founded Motown Records which rose to prominence during the 1960s and early 1970s with acts such as Stevie Wonder, The Temptations, The Four Tops, Smokey Robinson & The Miracles, Diana Ross & The Supremes, the Jackson 5, Martha and the Vandellas, The Spinners, Gladys Knight & the Pips, The Marvelettes, The Elgins, The Monitors, The Velvelettes and Marvin Gaye. Artists were backed by in-house vocalists [191] The Andantes and The Funk Brothers, the Motown house band that was featured in Paul Justman's 2002 documentary film Standing in the Shadows of Motown, based on Allan Slutsky's book of the same name.		The Motown Sound played an important role in the crossover appeal with popular music, since it was the first African American owned record label to primarily feature African-American artists. Gordy moved Motown to Los Angeles in 1972 to pursue film production, but the company has since returned to Detroit. Aretha Franklin, another Detroit R&B star, carried the Motown Sound; however, she did not record with Berry's Motown Label.[185]		Local artists and bands rose to prominence in the 1960s and 70s including: the MC5, The Stooges, Bob Seger, Amboy Dukes featuring Ted Nugent, Mitch Ryder and The Detroit Wheels, Rare Earth, Alice Cooper, and Suzi Quatro. The group Kiss emphasized the city's connection with rock in the song Detroit Rock City and the movie produced in 1999. In the 1980s, Detroit was an important center of the hardcore punk rock underground with many nationally known bands coming out of the city and its suburbs, such as The Romantics, The Necros, The Meatmen, and Negative Approach.[190]		In the 1990s and the new millennium, the city has produced a number of influential hip hop artists, including Eminem, the hip-hop artist with the highest cumulative sales, hip-hop producer J Dilla, rapper and producer Esham and hip hop duo Insane Clown Posse. The city is also home to rappers Big Sean and Danny Brown. The band Sponge toured and produced music, with artists such as Kid Rock and Uncle Kracker.[185][190] The city also has an active garage rock genre that has generated national attention with acts such as: The White Stripes, The Von Bondies, The Detroit Cobras, The Dirtbombs, Electric Six, and The Hard Lessons.[185]		Detroit is cited as the birthplace of techno music in the early 1980s.[192] The city also lends its name to an early and pioneering genre of electronic dance music, "Detroit techno". Featuring science fiction imagery and robotic themes, its futuristic style was greatly influenced by the geography of Detroit's urban decline and its industrial past.[14] Prominent Detroit techno artists include Juan Atkins, Derrick May, Kevin Saunderson, and Jeff Mills. The Detroit Electronic Music Festival, now known as "Movement", occurs annually in late May on Memorial Day Weekend, and takes place in Hart Plaza. In the early years (2000–2002), this was a landmark event, boasting over a million estimated attendees annually, coming from all over the world to celebrate Techno music in the city of its birth.		Major theaters in Detroit include the Fox Theatre (5,174 seats), Music Hall (1,770 seats), the Gem Theatre (451 seats), Masonic Temple Theatre (4,404 seats), the Detroit Opera House (2,765 seats), the Fisher Theatre (2,089 seats), The Fillmore Detroit (2,200 seats), Saint Andrew's Hall, the Majestic Theater, and Orchestra Hall (2,286 seats) which hosts the renowned Detroit Symphony Orchestra. The Nederlander Organization, the largest controller of Broadway productions in New York City, originated with the purchase of the Detroit Opera House in 1922 by the Nederlander family.[185]		Motown Motion Picture Studios with 535,000 square feet (49,700 m2) produces movies in Detroit and the surrounding area based at the Pontiac Centerpoint Business Campus for a film industry expected to employ over 4,000 people in the metro area.[193]		Many of the area's prominent museums are located in the historic cultural center neighborhood around Wayne State University and the College for Creative Studies. These museums include the Detroit Institute of Arts, the Detroit Historical Museum, Charles H. Wright Museum of African American History, the Detroit Science Center, as well as the main branch of the Detroit Public Library. Other cultural highlights include Motown Historical Museum, the Ford Piquette Avenue Plant museum (birthplace of the Ford Model T and the world's oldest car factory building open to the public), the Pewabic Pottery studio and school, the Tuskegee Airmen Museum, Fort Wayne, the Dossin Great Lakes Museum, the Museum of Contemporary Art Detroit (MOCAD), the Contemporary Art Institute of Detroit (CAID), and the Belle Isle Conservatory.		In 2010, the G.R. N'Namdi Gallery opened in a 16,000-square-foot (1,500 m2) complex in Midtown. Important history of America and the Detroit area are exhibited at The Henry Ford in Dearborn, the United States' largest indoor-outdoor museum complex.[194] The Detroit Historical Society provides information about tours of area churches, skyscrapers, and mansions. Inside Detroit, meanwhile, hosts tours, educational programming, and a downtown welcome center. Other sites of interest are the Detroit Zoo in Royal Oak, the Cranbrook Art Museum in Bloomfield Hills, the Anna Scripps Whitcomb Conservatory on Belle Isle, and Walter P. Chrysler Museum in Auburn Hills.[93]		The city's Greektown and three downtown casino resort hotels serve as part of an entertainment hub. The Eastern Market farmer's distribution center is the largest open-air flowerbed market in the United States and has more than 150 foods and specialty businesses.[195] On Saturdays, about 45,000 people shop the city's historic Eastern Market.[196] The Midtown and the New Center area are centered on Wayne State University and Henry Ford Hospital. Midtown has about 50,000 residents and attracts millions of visitors each year to its museums and cultural centers;[197] for example, the Detroit Festival of the Arts in Midtown draws about 350,000 people.[197]		Annual summer events include the Electronic Music Festival, International Jazz Festival, the Woodward Dream Cruise, the African World Festival, the country music Hoedown, Noel Night, and Dally in the Alley. Within downtown, Campus Martius Park hosts large events, including the annual Motown Winter Blast. As the world's traditional automotive center, the city hosts the North American International Auto Show. Held since 1924, America's Thanksgiving Parade is one of the nation's largest.[198] River Days, a five-day summer festival on the International Riverfront lead up to the Windsor–Detroit International Freedom Festival fireworks, which draw super sized-crowds ranging from hundreds of thousands to over three million people.[178][185][199]		An important civic sculpture in Detroit is "The Spirit of Detroit" by Marshall Fredericks at the Coleman Young Municipal Center. The image is often used as a symbol of Detroit and the statue itself is occasionally dressed in sports jerseys to celebrate when a Detroit team is doing well.[200] A memorial to Joe Louis at the intersection of Jefferson and Woodward Avenues was dedicated on October 16, 1986. The sculpture, commissioned by Sports Illustrated and executed by Robert Graham, is a 24-foot (7.3 m) long arm with a fisted hand suspended by a pyramidal framework.[201]		Artist Tyree Guyton created the controversial street art exhibit known as the Heidelberg Project in 1986, using found objects including cars, clothing and shoes found in the neighborhood near and on Heidelberg Street on the near East Side of Detroit.[185] Guyton continues to work with neighborhood residents and tourists in constantly evolving the neighborhood-wide art installation.		Detroit is one of 13 American metropolitan areas that are home to professional teams representing the four major sports in North America. All these teams but one play within the city of Detroit itself (the NBA's Detroit Pistons play in suburban Auburn Hills at The Palace of Auburn Hills). However, the Pistons will be moving into Little Caesars Arena in Detroit in 2017. There are three active major sports venues within the city: Comerica Park (home of the Major League Baseball team Detroit Tigers), Ford Field (home of the NFL's Detroit Lions), and Joe Louis Arena (home of the NHL's Detroit Red Wings). A 1996 marketing campaign promoted the nickname "Hockeytown".[185]		The Detroit Tigers have won four World Series titles. The Detroit Red Wings have won 11 Stanley Cups (the most by an American NHL franchise).[202] The Detroit Lions have won 4 NFL titles. The Detroit Pistons have won three NBA titles.[185] With the Pistons' first of three NBA titles in 1989, the city of Detroit has won titles in all four of the major professional sports leagues. Two new downtown stadiums for the Detroit Tigers and Detroit Lions opened in 2000 and 2002, respectively, returning the Lions to the city proper.		In college sports, Detroit's central location within the Mid-American Conference has made it a frequent site for the league's championship events. While the MAC Basketball Tournament moved permanently to Cleveland starting in 2000, the MAC Football Championship Game has been played at Ford Field in Detroit since 2004, and annually attracts 25,000 to 30,000 fans. The University of Detroit Mercy has a NCAA Division I program, and Wayne State University has both NCAA Division I and II programs. The NCAA football Little Caesars Pizza Bowl is held at Ford Field each December.		The local soccer team is called the Detroit City Football Club and was founded in 2012. The team plays in the National Premier Soccer League, and its nickname is Le Rouge.[203]		The city hosted the 2005 MLB All-Star Game, 2006 Super Bowl XL, 2006 and 2012 World Series, WrestleMania 23 in 2007, and the NCAA Final Four in April 2009. The city hosted the Detroit Indy Grand Prix on Belle Isle Park from 1989 to 2001, 2007 to 2008, and 2012 and beyond. In 2007, open-wheel racing returned to Belle Isle with both Indy Racing League and American Le Mans Series Racing.[204]		In the years following the mid-1930s, Detroit was referred to as the "City of Champions" after the Tigers, Lions, and Red Wings captured all three major professional sports championships in a seven-month period of time (the Tigers won the World Series in October 1935; the Lions won the NFL championship in December 1935; the Red Wings won the Stanley Cup in April 1936).[183] In 1932, Eddie "The Midnight Express" Tolan from Detroit won the 100- and 200-meter races and two gold medals at the 1932 Summer Olympics. Joe Louis won the heavyweight championship of the world in 1937.		Detroit has made the most bids to host the Summer Olympics without ever being awarded the games: seven unsuccessful bids for the 1944, 1952, 1956, 1960, 1964, 1968 and 1972 games.[185]		The city is governed pursuant to the Home Rule Charter of the City of Detroit. The city government is run by a mayor and a nine-member city council and clerk. Seven city council members are elected via district while two are elected at large. The mayor and clerk are elected in a at large election as well. Since voters approved the city's charter in 1974, Detroit has had a "strong mayoral" system, with the mayor approving departmental appointments. The council approves budgets but the mayor is not obligated to adhere to any earmarking. City ordinances and substantially large contracts must be approved by the council.[205] The Detroit City Code is the codification of Detroit's local ordinances.		The city clerk supervises elections and is formally charged with the maintenance of municipal records. Municipal elections for mayor, city council and city clerk are held at four-year intervals, in the year after presidential elections.[205] Following a November 2009 referendum, seven council members will be elected from districts beginning in 2013 while two will continue to be elected at-large.[206]		Detroit's courts are state-administered and elections are nonpartisan. The Probate Court for Wayne County is located in the Coleman A. Young Municipal Center in downtown Detroit. The Circuit Court is located across Gratiot Avenue in the Frank Murphy Hall of Justice, in downtown Detroit. The city is home to the Thirty-Sixth District Court, as well as the First District of the Michigan Court of Appeals and the United States District Court for the Eastern District of Michigan. The city provides law enforcement through the Detroit Police Department and emergency services through the Detroit Fire Department.		Detroit has struggled with high crime for decades. Detroit held the title of murder capital between 1985–1987 with a murder rate around 58 per 100,000.[207] Crime has since decreased and, in 2014, the murder rate was 43.4 per 100,000, lower than in St. Louis, Missouri.[208]		About half of all murders in Michigan in 2015 occurred in Detroit.[209][210] Although the rate of violent crime dropped 11% in 2008,[211] violent crime in Detroit has not declined as much as the national average from 2007 to 2011.[212] The violent crime rate is one of the highest in the United States. Neighborhoodscout.com reported a crime rate of 62.18 per 1,000 residents for property crimes, and 16.73 per 1,000 for violent crimes (compared to national figures of 32 per 1,000 for property crimes and 5 per 1,000 for violent crime in 2008).[213] Annual statistics released by the Detroit Police Department for 2016 indicate that while the city's overall crime rate declined that year, the murder rate rose from 2015.[214] In 2016 there were 302 homicides in Detroit, a 2.37% increase in the number of murder victims from the preceding year.[214]		The city's downtown typically has lower crime than national and state averages.[215] According to a 2007 analysis, Detroit officials note that about 65 to 70 percent of homicides in the city were drug related,[216] with the rate of unsolved murders roughly 70%.[118]		Areas of the city closer to the Detroit River are also patrolled by the United States Border Patrol.		In 2012, crime in the city was among the reasons for more expensive car insurance.[217]		Beginning with its incorporation in 1802, Detroit has had a total of 74 mayors. Detroit's last mayor from the Republican Party was Louis Miriani, who served from 1957 to 1962. In 1973, the city elected its first black mayor, Coleman Young. Despite development efforts, his combative style during his five terms in office was not well received by many suburban residents.[219] Mayor Dennis Archer, a former Michigan Supreme Court Justice, refocused the city's attention on redevelopment with a plan to permit three casinos downtown. By 2008, three major casino resort hotels established operations in the city.		In 2000, the city requested an investigation by the United States Justice Department into the Detroit Police Department which was concluded in 2003 over allegations regarding its use of force and civil rights violations. The city proceeded with a major reorganization of the Detroit Police Department.[220]		In March 2013, Governor Rick Snyder declared a financial emergency in the city, stating that the city has a $327 million budget deficit and faces more than $14 billion in long-term debt. It has been making ends meet on a month-to-month basis with the help of bond money held in a state escrow account and has instituted mandatory unpaid days off for many city workers. Those troubles, along with underfunded city services, such as police and fire departments, and ineffective turnaround plans from Bing and the City Council[221] led the state of Michigan to appoint an emergency manager for Detroit on March 14, 2013. On June 14, 2013 Detroit defaulted on $2.5 billion of debt by withholding $39.7 million in interest payments, while Emergency Manager Kevyn Orr met with bondholders and other creditors in an attempt to restructure the city's $18.5 billion debt and avoid bankruptcy.[222] On July 18, 2013, the City of Detroit filed for Chapter 9 bankruptcy protection.[223][224] It was declared bankrupt by U.S. judge Stephen Rhodes on December 3, with its $18.5 billion debt he said in accepting the city's contention that it is broke and that negotiations with its thousands of creditors were infeasible.[74]		Detroit is home to several institutions of higher learning including Wayne State University, a national research university with medical and law schools in the Midtown area offering hundreds of academic degrees and programs. The University of Detroit Mercy, located in Northwest Detroit in the University District, is a prominent Roman Catholic co-educational university affiliated with the Society of Jesus (the Jesuits) and the Sisters of Mercy. The University of Detroit Mercy offers more than a hundred academic degrees and programs of study including business, dentistry, law, engineering, architecture, nursing and allied health professions. The University of Detroit Mercy School of Law is located Downtown across from the Renaissance Center.		Sacred Heart Major Seminary, founded in 1919, is affiliated with Pontifical University of Saint Thomas Aquinas, Angelicum in Rome and offers pontifical degrees as well as civil undergraduate and graduate degrees. Sacred Heart Major Seminary offers a variety of academic programs for both clerical and lay students. Other institutions in the city include the College for Creative Studies, Lewis College of Business, Marygrove College and Wayne County Community College. In June 2009, the Michigan State University College of Osteopathic Medicine which is based in East Lansing opened a satellite campus located at the Detroit Medical Center. The University of Michigan was established in 1817 in Detroit and later moved to Ann Arbor in 1837. In 1959, University of Michigan–Dearborn was established in neighboring Dearborn.		With about 66,000 public school students (2011–12), the Detroit Public Schools (DPS) district is the largest school district in Michigan. Detroit has an additional 56,000 charter school students for a combined enrollment of about 122,000 students.[225][226] As of 2009[update] there are about as many students in charter schools as there are in district schools.[227]		In 1999, the Michigan Legislature removed the locally elected board of education amid allegations of mismanagement and replaced it with a reform board appointed by the mayor and governor. The elected board of education was re-established following a city referendum in 2005. The first election of the new 11-member board of education occurred on November 8, 2005.[228]		Due to growing Detroit charter schools enrollment as well as a continued exodus of population, the city planned to close many public schools.[225] State officials report a 68% graduation rate for Detroit's public schools adjusted for those who change schools.[229][230]		Public and charter school students in the city have performed poorly on standardized tests. While Detroit public schools scored a record low on national tests, the publicly funded charter schools did even worse than the public schools.[231][232]		Detroit public schools students scored the lowest on tests of reading and writing of all major cities in the United States in 2015. Among eighth-graders, only 27% showed basic proficiency in math and 44% in reading.[233] Nearly half of Detroit's adults are functionally illiterate.[234]		Detroit is served by various private schools, as well as parochial Roman Catholic schools operated by the Archdiocese of Detroit. As of 2013[update] there are four Catholic grade schools and three Catholic high schools in the City of Detroit, with all of them in the city's west side.[235] The Archdiocese of Detroit lists a number of primary and secondary schools in the metro area as Catholic education has emigrated to the suburbs.[236][237] Of the three Catholic high schools in the city, two are operated by the Society of Jesus and the third is co-sponsored by the Sisters, Servants of the Immaculate Heart of Mary and the Congregation of St. Basil.[238][239]		In the 1964–1965 school year there were about 110 Catholic grade schools in Detroit, Hamtramck, and Highland Park and 55 Catholic high schools in those three cities. The Catholic school population in Detroit has decreased due to the increase of charter schools, increasing tuition at Catholic schools, the small number of African-American Catholics, White Catholics moving to suburbs, and the decreased number of teaching nuns.[235]		The Detroit Free Press and The Detroit News are the major daily newspapers, both broadsheet publications published together under a joint operating agreement called the Detroit Newspaper Partnership. Media philanthropy includes the Detroit Free Press high school journalism program and the Old Newsboys' Goodfellow Fund of Detroit.[240] In March 2009, the two newspapers reduced home delivery to three days a week, print reduced newsstand issues of the papers on non-delivery days and focus resources on Internet-based news delivery.[241] The Metro Times, founded in 1980, is a weekly publication, covering news, arts & entertainment.[242]		Also founded in 1935 and based in Detroit the Michigan Chronicle is one of the oldest and most respected African-American weekly newspapers in America. Covering politics, entertainment, sports and community events.[243] The Detroit television market is the 11th largest in the United States;[244] according to estimates that do not include audiences located in large areas of Ontario, Canada (Windsor and its surrounding area on broadcast and cable TV, as well as several other cable markets in Ontario, such as the city of Ottawa) which receive and watch Detroit television stations.[244]		Detroit has the 11th largest radio market in the United States,[245] though this ranking does not take into account Canadian audiences.[245] Nearby Canadian stations such as Windsor's CKLW (whose jingles formerly proclaimed "CKLW-the Motor City") are popular in Detroit.		Hardcore Pawn, an American documentary reality television series produced for truTV, features the day-to-day operations of American Jewelry and Loan, a family-owned pawn shop on Greenfield Road.		Within the city of Detroit, there are over a dozen major hospitals which include the Detroit Medical Center (DMC), Henry Ford Health System, St. John Health System, and the John D. Dingell VA Medical Center. The DMC, a regional Level I trauma center, consists of Detroit Receiving Hospital and University Health Center, Children's Hospital of Michigan, Harper University Hospital, Hutzel Women's Hospital, Kresge Eye Institute, Rehabilitation Institute of Michigan, Sinai-Grace Hospital, and the Karmanos Cancer Institute. The DMC has more than 2,000 licensed beds and 3,000 affiliated physicians. It is the largest private employer in the City of Detroit.[246] The center is staffed by physicians from the Wayne State University School of Medicine, the largest single-campus medical school in the United States, and the United States' fourth largest medical school overall.[246]		Detroit Medical Center formally became a part of Vanguard Health Systems on December 30, 2010, as a for profit corporation. Vanguard has agreed to invest nearly $1.5 B in the Detroit Medical Center complex which will include $417 M to retire debts, at least $350 M in capital expenditures and an additional $500 M for new capital investment.[247][248] Vanguard has agreed to assume all debts and pension obligations.[247] The metro area has many other hospitals including William Beaumont Hospital, St. Joseph's, and University of Michigan Medical Center.		In 2011, Detroit Medical Center and Henry Ford Health System substantially increased investments in medical research facilities and hospitals in the city's Midtown and New Center.[247][249]		In 2012, two major construction projects were begun in New Center, the Henry Ford Health System started the first phase of a $500 million, 300-acre revitalization project, with the construction of a new $30 million, 275,000-square-foot, Medical Distribution Center for Cardinal Health, Inc. [250][251] and Wayne State University started construction on a new $93 million, 207,000-square-foot, Integrative Biosciences Center (IBio).[252][253] As many as 500 researchers, and staff will work out of the IBio Center. [254]		With its proximity to Canada and its facilities, ports, major highways, rail connections and international airports, Detroit is an important transportation hub. The city has three international border crossings, the Ambassador Bridge, Detroit–Windsor Tunnel and Michigan Central Railway Tunnel, linking Detroit to Windsor, Ontario. The Ambassador Bridge is the single busiest border crossing in North America, carrying 27% of the total trade between the U.S. and Canada.[255]		On February 18, 2015, Canadian Transport Minister Lisa Raitt announced that Canada has agreed to pay the entire cost to build a $250 million U.S. Customs plaza adjacent to the planned new Detroit–Windsor bridge, now the Gordie Howe International Bridge. Canada had already planned to pay for 95 % of the bridge, which will cost $2.1 billion, and is expected to open in 2020. "This allows Canada and Michigan to move the project forward immediately to its next steps which include further design work and property acquisition on the U.S. side of the border," Raitt said in a statement issued after she spoke in the House of Commons. [256]		Mass transit in the region is provided by bus services. The Detroit Department of Transportation (DDOT) provides service to the outer edges of the city. From there, the Suburban Mobility Authority for Regional Transportation (SMART) provides service to the suburbs. Cross border service between the downtown areas of Windsor and Detroit is provided by Transit Windsor via the Tunnel Bus.[257]		An elevated rail system known as the People Mover, completed in 1987, provides daily service around a 2.94 miles (4.73 km) loop downtown. The QLINE serves as a link between the Detroit People Mover and Detroit Amtrak station via Woodward Avenue.[258] The SEMCOG Commuter Rail line will extend from Detroit's New Center, connecting to Ann Arbor via Dearborn, Wayne, and Ypsilanti when it is opened.[259]		The Regional Transit Authority (RTA) was established by an act of the Michigan legislature in December 2012 to oversee and coordinate all existing regional mass transit operations, and to develop new transit services in the region. The RTA's first project was the introduction of RelfeX, a limited-stop, cross-county bus service connecting downtown and midtown Detroit with Oakland and Macomb counties via Woodward and Gratiot avenues.[260]		Amtrak provides service to Detroit, operating its Wolverine service between Chicago and Pontiac. The Amtrak station is located in New Center north of downtown. The J. W. Westcott II, which delivers mail to lake freighters on the Detroit River, is the world's only floating post office.[261]		Detroit Metropolitan Wayne County Airport (DTW), the principal airport serving Detroit, is located in nearby Romulus. DTW is a primary hub for Delta Air Lines (following its acquisition of Northwest Airlines), and a secondary hub for Spirit Airlines.		Coleman A. Young International Airport (DET), previously called Detroit City Airport, is on Detroit's northeast side; the airport now maintains only charter service and general aviation.[262] Willow Run Airport, in far-western Wayne County near Ypsilanti, is a general aviation and cargo airport.		Metro Detroit has an extensive toll-free network of freeways administered by the Michigan Department of Transportation. Four major Interstate Highways surround the city. Detroit is connected via Interstate 75 (I-75) and I-96 to Kings Highway 401 and to major Southern Ontario cities such as London, Ontario and the Greater Toronto Area. I-75 (Chrysler and Fisher freeways) is the region's main north–south route, serving Flint, Pontiac, Troy, and Detroit, before continuing south (as the Detroit–Toledo and Seaway Freeways) to serve many of the communities along the shore of Lake Erie.[263]		I-94 (Edsel Ford Freeway) runs east–west through Detroit and serves Ann Arbor to the west (where it continues to Chicago) and Port Huron to the northeast. The stretch of the current I-94 freeway from Ypsilanti to Detroit was one of America's earlier limited-access highways. Henry Ford built it to link the factories at Willow Run and Dearborn during World War II. A portion was known as the Willow Run Expressway. The I-96 freeway runs northwest–southeast through Livingston, Oakland and Wayne counties and (as the Jeffries Freeway through Wayne County) has its eastern terminus in downtown Detroit.[263]		I-275 runs north–south from I-75 in the south to the junction of I-96 and I-696 in the north, providing a bypass through the western suburbs of Detroit. I-375 is a short spur route in downtown Detroit, an extension of the Chrysler Freeway. I-696 (Reuther Freeway) runs east–west from the junction of I-96 and I-275, providing a route through the northern suburbs of Detroit. Taken together, I-275 and I-696 form a semicircle around Detroit. Michigan state highways designated with the letter M serve to connect major freeways.[263]		Detroit has seven sister cities, as designated by Sister Cities International: [264][265]				
FC Nerds is a reality television sports franchise developed by Nordisk Film.		
Geek Pride Day is an initiative to promote geek culture, celebrated annually on May 25.[1]		The initiative originated in Spain in 2006 as (Spanish: Día del orgullo friki) and spread around the world via the Internet.						Tim McEachern organized unconnected events called Geek Pride Festival and/or Geek Pride Day 1998 to 2000 at a bar in Albany, New York, which are sometimes seen as a prelude to Geek Pride Day.		In 2006, the Spanish blogger Germán Martínez known online as señor Buebo organized the first celebration, the day was celebrated for the first time in Spain and on the Internet, drawing attention from mainstream media.[2][3][4] The biggest concentration took place in Madrid, where 300 Geeks demonstrated their pride together with a human Pac-Man. A manifesto was created to celebrate the first Geek Pride Day, which included a list of the basic rights and responsibilities of geeks.[5]		In 2008, Geek Pride Day was officially celebrated in the U.S., where it was heralded by numerous bloggers, coalescing around the launch of the Geek Pride Day website. Math author, Euler Book Prize winner, and geek blogger John Derbyshire not only did a shout out, but announced [6] that he would be appearing in the Fifth Avenue parade, dressed as number 57, on the prime number float - prompting some bloggers to say they'd be looking for him.		By 2009, acknowledgment of the day had reached the Science Channel, with special programming on 25 May to celebrate and events took place to commemorate the day in Ottawa, home to the Canada Science and Technology Museum and a notable research centre in Canada.[citation needed] While in 2010 the festival spread further, taking in cities as diverse as Halifax, Nova Scotia; Budapest, Hungary; Tel Aviv, Israel; Timişoara, Romania and San Diego, California. In 2013, a Geek Pride parade was held in Gothenburg, Sweden, and it was decided that it would be an annual event.		
The word geek is a slang term originally used to describe eccentric or non-mainstream people; in current use, the word typically connotes an expert or enthusiast or a person obsessed with a hobby or intellectual pursuit, with a general pejorative meaning of a "peculiar person, especially one who is perceived to be overly intellectual, unfashionable, or socially awkward".[1]		Although often considered as a pejorative, the term is also used self-referentially without malice or as a source of pride. Its meaning has evolved to refer to "someone who is interested in a subject (usually intellectual or complex) for its own sake".						The word comes from English dialect geek or geck (meaning a "fool" or "freak"; from Middle Low German Geck). "Geck" is a standard term in modern German and means "fool" or "fop".[2] The root also survives in the Dutch and Afrikaans adjective gek ("crazy"), as well as some German dialects, and in the Alsatian word Gickeleshut ("jester's hat"; used during carnival).[1] In 18th century Austria, Gecken were freaks on display in some circuses. In 19th century North America, the term geek referred to a performer in a geek show in a circus, traveling carnival or travelling funfair sideshows (see also freak show).[3] The 1976 edition of the American Heritage Dictionary included only the definition regarding geek shows. This variation of the term was used to comic effect in an episode of popular 1970s TV show Sanford & Son. Professional wrestling manager "Classy" Freddie Blassie recorded a song in the 1970s called "Pencil-Necked Geek".		The 1975 edition of the American Heritage Dictionary, published a decade before the tech revolution, gave only one definition: "Geek [noun, slang]. A carnival performer whose act usually consists of biting the head off a live chicken or snake." The tech revolution found new uses for this word, but it still often conveys a derogatory sting. Today, Dictionary.com gives five definitions, the fourth of which is "a carnival performer who performs sensationally morbid or disgusting acts, as biting off the head of a live chicken." [4]		The definition of geek has changed considerably over time, and there is no longer a definitive meaning. The term nerd has a similar, practically synonymous meaning as geek, but many choose to identify different connotations among these two terms, although the differences are disputed. In a 2007 interview on The Colbert Report, Richard Clarke said the difference between nerds and geeks is "geeks get it done" or "ggid"[5] Julie Smith defined a geek as "a bright young man turned inward, poorly socialized, who felt so little kinship with his own planet that he routinely traveled to the ones invented by his favorite authors, who thought of that secret, dreamy place his computer took him to as cyberspace—somewhere exciting, a place more real than his own life, a land he could conquer, not a drab teenager's room in his parents' house".[6]		Technologically oriented geeks, in particular, now exert a powerful influence over the global economy and society.[7] Whereas previous generations of geeks tended to operate in research departments, laboratories and support functions, now they increasingly occupy senior corporate positions, and wield considerable commercial and political influence. When U.S. President Barack Obama met with Facebook’s Mark Zuckerberg and the CEOs of the world’s largest technology firms at a private dinner in Woodside, California on February 17, 2011, New York magazine ran a story titled "The world’s most powerful man meets President Obama".[8] At the time, Zuckerberg’s company had grown to over one billion users.		According to Mark Roeder the rise of the geek represents a new phase of human evolution. In his book, Unnatural Selection: Why The Geeks Will Inherit The Earth[9] he suggests that "the high-tech environment of the Anthropocene favours people with geek-like traits, many of whom are on the autism spectrum, ADHD, or dyslexia. Previously, such people may have been at a disadvantage, but now their unique cognitive traits enable some of them to resonate with the new technological zeitgeist and become very successful."		The Economist magazine observed, on June 2, 2012, "Those square pegs (geeks) may not have an easy time in school. They may be mocked by jocks and ignored at parties. But these days no serious organisation can prosper without them."[10]				"Geek chic" refers to a minor fashion trend that arose in the mid 2000s in which young people adopted "geeky" fashions, such as oversized black horn-rimmed glasses, suspenders/braces, and highwater trousers. The glasses—sometimes worn with non-prescription lenses or without lenses—quickly became the defining aspect of the trend, with the media identifying various celebrities as "trying geek" or "going geek" for wearing such glasses, such as David Beckham,[11] Justin Timberlake,[12] and Myleene Klass.[13] Meanwhile, in the sports world, many NBA players wore "geek glasses" during post-game interviews, drawing comparisons to Steve Urkel.[14][15]		The term "geek chic" was appropriated by some self-identified "geeks" to refer to a new, socially acceptable role in a technologically advanced society.[16]		
NCAA		Rensselaer Polytechnic Institute (/rɛnsəˈlɪər/), or RPI, is a private research university and space-grant institution located in Troy, New York, with two additional campuses in Hartford and Groton, Connecticut.		The institute was established in 1824 by Stephen van Rensselaer and Amos Eaton for the "application of science to the common purposes of life" and is described as the oldest technological university in the English-speaking world.[7] Numerous American colleges or departments of applied sciences were modeled after Rensselaer.[8] Built on a hillside, RPI's 265-acre (107 ha) campus overlooks the city of Troy and the Hudson River and is a blend of traditional and modern architecture. The institute operates an on‑campus business incubator and the 1,250-acre (510 ha) Rensselaer Technology Park.[9]		Today, Rensselaer is organized into six main schools which contain 37 departments, with emphasis on science and technology.[10] It is well recognized for its degree programs in engineering, computing, business and management, information technology, the sciences, design, and liberal arts. Rensselaer is ranked 39th among all research facilities in the world and 3rd among research colleges in the world according to U.S. News & World Report and Forbes rankings.[11] Rensselaer Polytechnic Institute is also highly recognized internationally for its engineering and computing programs, and has been ranked within the top six universities in the United States for highest median earnings of recent alumni.[12]		Rensselaer's current and former faculty and alumni represent 85 members of the National Academy of Engineering, 17 members of the National Academy of Sciences, 25 members of the American Academy of Arts and Sciences, 8 members of the National Academy of Medicine, 8 members of the National Academy of Inventors, and 6 members of the National Inventors Hall of Fame, as well as 6 National Medal of Technology winners, 5 National Medal of Science winners, 8 Fulbright Scholarship recipients, and a Nobel Prize winner in Physics. [13] Research projects include the areas of Astrobiology and Astrophysics, Biotechnology and Life Sciences, Energy, Environment, and Smart Systems (EES), Nanotechnology and Advanced Materials, Computational Science and Engineering, and Cognitive Engineering.						Stephen van Rensselaer established the Rensselaer School on November 5, 1824 with a letter to the Rev. Dr. Samuel Blatchford, in which Van Rensselaer asked Blatchford to serve as the first president. Within the letter he set down several orders of business. He appointed Amos Eaton as the school's first senior professor and appointed the first board of trustees.[6] The school opened on Monday, January 3, 1825 at the Old Bank Place, a building at the north end of Troy.[14] Tuition was around $40 per semester (equivalent to $800 in 2012[15]).[6] The fact that the school attracted students from as far as Ohio and Pennsylvania is attributed to the reputation of Eaton. Fourteen months of successful trial led to the incorporation of the school on March 21, 1826 by the state of New York. In its early years, the Rensselaer School strongly resembled a graduate school more than it did a college, drawing graduates from many older institutions.[16]		Under Eaton, the Rensselaer School, renamed the Rensselaer Institute in 1832, was a small but vibrant center for technological research. The first civil engineering degrees in the United States were granted by the school in 1835, and many of the best remembered civil engineers of that time graduated from the school. Important visiting scholars included Joseph Henry, who had previously studied under Amos Eaton, and Thomas Davenport, who sold the world's first working electric motor to the institute.[17]		In 1847 alumnus Benjamin Franklin Greene became the new senior professor. Earlier he had done a thorough study of European technical schools to see how Rensselaer could be improved. In 1850 he reorganized the school into a three-year polytechnic institute with six technical schools.[18] In 1861 the name was changed to Rensselaer Polytechnic Institute.[19] A severe conflagration of May 10, 1862, known as "The Great Fire", destroyed more than 507 buildings in Troy and gutted 75 acres (300,000 m2) in the heart of the city.[20][21] The "Infant School" building that housed the Institute at the time was destroyed in this fire. Columbia University proposed that Rensselaer leave Troy altogether and merge with its New York City campus. Ultimately, the proposal was rejected and the campus left the crowded downtown for the hillside. Classes were temporarily held at the Vail House and in the Troy University building until 1864,[22] when the Institute moved to a building on Broadway on 8th Street, now the site of the Approach.[21]		One of the first Latino student organizations in the United States was founded at RPI in 1890. The Club Hispano Americano was established by the international Latin American students that attended the institute at this time.		In 1904 the Institute was for the fourth time devastated by fire, when its main building was completely destroyed.[26] However, RPI underwent a period of academic and resource expansion under the leadership of President Palmer Ricketts.[27] Named President in 1901, Ricketts liberalized the curriculum by adding the Department of Arts, Science, and Business Administration, in addition to the Graduate School. He also expanded the university's resources and developed RPI into a true polytechnic institute by increasing the number of degrees offered from two to twelve; these included electrical engineering, mechanical engineering, biology, chemistry, and physics. During Rickett's tenure, enrollment increased from approximately 200 in 1900 to a high of 1,700 in 1930.[23]		Another period of expansion occurred following World War II as returning veterans used their GI Bill education benefits to attend college. The "Freshman Hill" residence complex was opened in 1953 followed by the completion of the Commons Dining Hall in 1954, two more halls in 1958, and three more in 1968. In 1961, there was major progress in academics at the institute with the construction of the Gaerttner Linear Accelerator, then the most powerful in the world,[28] and the Jonsson-Rowland Science Center. The current Student Union building was opened in 1967.		The next three decades brought continued growth with many new buildings (see 'Campus' below), and growing ties to industry. The "H-building", previously used for storage, became the home for the RPI incubator program, the first such program sponsored solely by a university.[29] Shortly after this, RPI decided to invest $3 million in pavement, water and power on around 1,200 acres (490 ha) of land it owned 5 miles (8.0 km) south of campus to create the Rensselaer Technology Park.[30] In 1982 the New York State legislature granted RPI $30 million to build the George M. Low Center for Industrial Innovation, a center for industry-sponsored research and development.		In 1999, RPI gained attention when it was one of the first universities to implement a mandatory laptop computer program.[31] This was also the year of the arrival of President Shirley Ann Jackson, a former chairperson of the Nuclear Regulatory Commission under President Bill Clinton. She instituted "The Rensselaer Plan" (discussed below), an ambitious plan to revitalize the institute. Many advances have been made under the plan, and Jackson has enjoyed the ongoing support of the RPI Board of Trustees. However, her leadership style did not sit well with many faculty; on April 26, 2006, RPI faculty voted 149 to 155 in a failed vote of no-confidence in Jackson.[32] In September 2007, RPI's Faculty Senate was suspended for over four years over conflict with the administration.[33] On October 3, 2008, RPI celebrated the opening of the $220 million Experimental Media and Performing Arts Center. That same year the national economic downturn resulted in the elimination of 98 staff positions across the Institute, about five percent of the workforce.[34] Campus construction expansion continued, however, with the completion of the $92 million East Campus Athletic Village and opening of the new Blitman Commons residence hall in 2009. As of 2015, all staff positions had been reinstated at the Institute, experiencing significant growth from pre-recession levels and contributing over $1 billion annually to the economy of the Capital District.[35] That same year, renovation of the North Hall, E-Complex, and Quadrangle dormitories began and was later completed in 2016 to house the largest incoming class in Rensselaer's history.[36][37]		RPI's 275-acre (111 ha)[38] campus sits upon a hill overlooking Troy, New York and the Hudson River. The surrounding area is mostly residential neighborhoods, with the city of Troy lying at the base of the hill. The campus is bisected by 15th Street, with most of the athletic and housing facilities to the east, and the academic buildings to the west. A footbridge spans the street, linking the two halves. Much of the campus features a series of Colonial Revival style structures built in the first three decades of the 20th century. Overall, the campus has enjoyed four periods of expansion.[14]		RPI was originally located in downtown Troy, but gradually moved to the hilltop that overlooks the city. Buildings that remain from this time include Winslow Chemical Laboratory, a building on the National Register of Historic Places. Located at the base of the hill on the western edge of campus, it currently houses the Social and Behavioral Research Laboratory.[14][39]		President Palmer Ricketts supervised the construction of the school's "Green Rooftop" Colonial Revival buildings that give much of the campus a distinct architectural style. Buildings constructed during this period include the Carnegie Building (1906), Walker Laboratory (1907), Russell Sage Laboratory (1909), Pittsburgh Building (1912), Quadrangle Dormitories (1916–1927), Troy Building (1925), Amos Eaton Hall (1928), Greene Building (1931) and Ricketts Building (1935). Also built during this period was "The Approach" (1907), a massive ornate granite staircase found on the west end of campus. Originally linking RPI to the Troy Union Railroad station, it again serves as an important link between the city and the university.[40] In 1906 the '86 Field, homefield of the football team until 2008, was completed with support of the Class of 1886.		After World War II, the campus again underwent major expansion. Nine dormitories were built at the east edge of campus bordering Burdett Avenue, a location which came to be called "Freshman Hill". The Houston Field House (1949) was reassembled, after being moved in pieces from its original Rhode Island location. West Hall, which was originally built in 1869 as a hospital, was acquired by the Institute in 1953. The ornate building is an example of French Second Empire architecture.[41] It was listed on the National Register of Historic Places in 1973. Another unique building is the Voorhees Computing Center (VCC). Originally built as St. Joseph's Seminary chapel in 1933, it was once the institute's library, until the completion of the Folsom Library in 1976.[42] Interestingly, the new library, built adjacent to the computing center, was designed to match colors with the church, but is very dissimilar architecturally; it is an excellent example of the modern brutalist style – a style that has invited comparisons with a parking garage. The university was unsure of what to do with the church, or whether to keep it at all, but in 1979 the institute decided to preserve it and renovate it into a unique place for computer labs and facilities to support the institute's computing initiatives and today serves as the backbone for the institute's data and telephony infrastructure.		The modern campus features the Jonsson-Rowland Science Center (J-ROWL) (1961), Materials Research Center (MRC) (1965), Rensselaer Union (1967), Cogswell Laboratory (1971), Darrin Communications Center (DCC) (1973), Jonsson Engineering Center (JEC) (1977), Low Center for Industrial Innovation (CII) (1987), a public school building which was converted into Academy Hall (1990), and the Center for Biotechnology and Interdisciplinary Studies (2004).[14] Tunnels connect the Low Center, DCC, JEC and Science Center. A tenth dormitory named Barton Hall was added to Freshman Hill in August 2000, featuring the largest rooms available for freshmen.[43]		On October 3, 2008, the university celebrated the grand opening of the Experimental Media and Performing Arts Center (EMPAC) situated on the west edge of campus.[44] The building was constructed on the precipice of the hill, with the main entrance on top. Upon entering, elevated walkways lead into a 1,200 seat concert hall. Most of the building is encased in a glass exoskeleton, with an atrium-like space between it and the "inner building". Adjacent to and underneath the main auditorium there is a 400-seat theater, offices, and two black-box studios with 35-foot (11 m) to 45-foot (14 m) ceilings.[45]		In 2008, RPI announced the purchase of the former Rensselaer Best Western Inn, located at the base of the hill, along with plans to transform it into a new residence hall. After extensive renovations, the residence hall was dedicated on May 15, 2009, as the Howard N. Blitman, P.E. ’50 Residence Commons.[46] It houses about 300 students in 148 rooms and includes a fitness center, dining hall, and conference area.[46] The new residence hall is part of a growing initiative to involve students in the Troy community and help revitalize the downtown. RPI owns and operates three office buildings in downtown Troy, the Rice and Heley buildings and the historic W. & L.E. Gurley Building.[47] RPI also owns the Proctor's Theater building in Troy which was purchased in 2004, with the intention of converting it into office space.[48] As of 2011, Rensselaer had signed an agreement with Columbia Development Companies to acquire both Proctor's Theatre and Chasan Building in Troy and launch a redevelopment.[49][50][51][52]		The Institute runs a 15-acre (6.1 ha) campus in Hartford, Connecticut, and a distance learning center in Groton, Connecticut. These centers are used by graduates and working professionals and are managed by the Hartford branch of RPI, Rensselaer at Hartford. At Hartford, graduate degrees are offered in business administration, management, computer science, computer and systems engineering, electrical engineering, engineering science, mechanical engineering and information technology. There are also a number of certificate programs and skills training programs for working professionals.		Rensselaer Polytechnic Institute has five schools: the School of Architecture, the School of Engineering, the School of Humanities, Arts, and Social Sciences, the Lally School of Management & Technology, and the School of Science. The School of Engineering is the largest by enrollment, followed by the School of Science, the School of Management, the School of Humanities, Arts, and Social Sciences, and the School of Architecture. There also exists an interdisciplinary program in Information Technology that began in the late 1990s, programs in prehealth and prelaw, Reserve Officers' Training Corps (ROTC) for students desiring commissions as officers in the armed forces, a program in cooperative education (Co-Op), and domestic and international exchange programs. All together, the university offers over 145 degree programs in nearly 60 fields that lead to bachelor's, master's, and doctoral degrees. In addition to traditional majors, RPI has around a dozen special interdisciplinary programs, such as Games and Simulation Arts and Sciences (GSAS), Design, Innovation, and Society (DIS), Minds & Machines, and Product Design and Innovation (PDI).[53] RPI is a technology-oriented university; all buildings and residence hall rooms have hard-wired and wireless high speed internet access, and all incoming freshmen have been required to have a laptop computer since 1999.[54] In 2004, Forbes ranked RPI first in terms of wireless as the "most connected campus".[55] Nationally, RPI is a member of the National Association of Independent Colleges and Universities (NAICU) and the NAICU's University and College Accountability Network (U-CAN).		With the arrival of the president, Shirley Ann Jackson, came the "Rensselaer Plan" announced in 1999. Its goal is to achieve greater prominence for Rensselaer as a technological research university.[56] Various aspects of the plan include bringing in a larger graduate student population and new research faculty, and increasing participation in undergraduate research, international exchange programs, and "living and learning communities". So far, there have been a number of changes under the plan: new infrastructure such as the Center for Biotechnology and Interdisciplinary Studies, Experimental Media and Performing Arts Center, and Computational Center for Nanotechnology Innovations (CCNI) have been built to support new programs, and application numbers have increased.[57] In 2017, Rensselaer received a record number of applications: 19,485.[58] According to Jared Cohon, president of Carnegie Mellon University, "Change at Rensselaer in the last five years has occurred with a scope and swiftness that may be without precedent in the recent history of American higher education."[59]		The ability to attract greater research funds is needed to meet the goals of the plan, and the university has set a goal of $100 million annually. Fourteen years later, in FY2013, research expenditures reached this goal. To help raise money the university mounted a $1 billion capital campaign, of which the public phase began in September 2004 and was expected to finish by 2008. In 2001, a major milestone of the campaign was the pledging of an unrestricted gift of $360 million by an anonymous donor, believed to be the largest such gift to a U.S. university at the time. The university had been a relative stranger to such generosity as the prior largest single gift was $15 million.[60] By September 2006, the $1 billion goal has been exceeded much in part to an in-kind contribution of software commercially valued at $513.95 million by the Partners for the Advancement of Collaborative Engineering Education (PACE). In light of this, the board of trustees increased the goal of the $1 billion capital campaign to $1.4 billion by June 30, 2009. The new goal was met by October 1, 2008.[61]		In 2016, Shirley Ann Jackson, President of Rensselaer Polytechnic Institute, announced during the Fall Town Hall Meeting that the Institute was in the final stages of organizing a new capital campaign which it intends to launch in 2017 to meet the goals of the Rensselaer Plan 2024. The goal of the campaign was cited as being primarily for the support of financial aid for undergraduate students and the expansion of on-campus research facilities to accommodate planned increases in doctoral and graduate enrollment.[62]		Rensselaer is consistently ranked among the best universities in the United States and the world. For over a decade, Rensselaer has remained in the top fifty national universities in the United States, and is currently listed among the top six universities for highest median earnings.[12] In 2016, U.S. News & World Report ranked Rensselaer 39th among all colleges and universities and 24th for "Best Value" in undergraduate education.[74][75][76] As of 2017, Rensselaer's undergraduate engineering program is ranked 32nd and its graduate program is ranked 39th by U.S. News & World Report.[77] Rensselaer's undergraduate and graduate engineering programs include Aerospace/Aeronautical (23rd), Biomedical (39th), Chemical (22nd), Civil (27th), Computer (27th), Electrical (26th), Environmental (34th), Industrial (17th), Materials (22nd), Mechanical (24th), and Nuclear (12th).[78]		In 2013, U.S. News & World Report ranked Rensselaer 6th for multimedia and visual communications. The same source ranked Rensselaer's computer science program 47th in the nation for graduates. The Leiden Ranking (2016) placed RPI at 127 among the top 900 world universities and research institutions according to the proportion of the top 1% most frequently cited publications of a university.[79] In 2016, The Economist ranked Rensselaer #18 amongst four-year non-vocational colleges and universities and Times Higher Education–QS World University Rankings placed Rensselaer among the top 50 universities for technology in the world.[12][80][81] In 2012, RPI was ranked as the 4th best engineering school in the world by Business Insider.[82] The Newsweek/Kaplan 2007 Educational College Guide named Rensselaer Polytechnic Institute one of the 25 "New Ivies", a group of 25 schools described as providing an education equivalent to schools in the Ivy League.[83] In 2015, Rensselaer was ranked 6th by USA Today among all universities in physics. That same year, Forbes ranked Rensselaer the 12th most entrepreneurial university in the world. The Lally School of Management and Technology is ranked 6th in technological entrepreneurship and 21st in entrepreneurship by Entrepreneur.[78] The Lally School's corporate strategy program was ranked 11th in the nation by BusinessWeek and the management program was ranked 5th in the nation by TFE Times.[84][85]		Rensselaer is classified by The Carnegie Foundation for the Advancement of Teaching as a university with higher research activity.[86] Rensselaer has established six areas of research as institute priorities: biotechnology, energy and the environment, nanotechnology, computation and information technology, and media and the arts.[87] Research is organized under the Office of the Vice President for Research, Jonathan Dordick, who reports directly to the institute's president Shirley Ann Jackson.[88] In 2017, Rensselaer operated 34 research centers and maintained annual sponsored research expenditures of $103 million.		One of the most recent of Rensselaer's research centers is the Center for Biotechnology and Interdisciplinary Studies, a 218,000 square-foot research facility and a national pacesetter for fundamental and applied research in biotechnology. The primary target of the research center is biologics, a research priority based on data-driven understanding of proteomics, protein regulation, and gene regulation. It involves using biocatalysis and synthetic biology tools to block or supplement the actions of specific cells or proteins in the immune system. Over the past decade, CBIS has produced over 2,000 peer-reviewed publications with over 30,000 citations and currently employs over 200 scientists and engineers. The center is also used primarily to train undergraduate and graduate students, with over 1,000 undergraduates and 200 doctoral students trained.[89][90][91] The center also has numerous academic and industry partners including the Icahn School of Medicine at Mount Sinai. These partnerships have resulted in numerous advances over the last decade through new commercial developments in diagnostics, therapeutics, medical devices, and regenerative medicine which are a direct result of research at the center. Examples of advancements include the creation of synthetic heparin, antimicrobial coatings, detoxification chemotherapy, on-demand bio-medicine, implantable sensors, and 3D cellular array chips.[92]		Rensselaer also hosts the Tetherless World Constellation, a multidisciplinary research institution focused on theories, methods, and applications of the World Wide Web. Research is carried out in three inter-connected themes: Future Web, Semantic Foundations and Xinformatics. At Rensselaer, a constellation is a multidisciplinary team composed of senior and junior faculty members, research scientists, and postdoctoral, graduate, and undergraduate students. The faculty of each constellation includes three or more outstanding stars in a particular research field. The three professors at TWC are James Hendler, Deborah McGuinness and Peter Fox. Faculty alumni of TWC includes Heng Ji (Natural Language Processing) [1]. In 2016, the Constellation received a one million dollar grant from the Bill & Melinda Gates Foundation for continuing work on a novel data visualization platform that will harness and accelerate the analysis of vast amounts of data for the foundation’s Healthy Birth, Growth, and Development Knowledge Integration initiative.[93]		In conjunction with the constellation, Rensselaer operates the Center for Computational Innovations which is the result of a $100 million collaboration between Rensselaer, IBM, and New York State to further nanotechnology innovations. The center is currently home to the most powerful private-university based supercomputer in the world and it's supercomputer is consistently ranked among the most powerful in the world, capable of performing over 1.1 petaFLOPS. The center's main focus is on reducing the cost associated with the development of nanoscale materials and devices, such as used in the semiconductor industry. The university also utilizes the center for interdisciplinary research in biotechnology, medicine, energy, and other fields.[94] Rensselaer additionally operates a nuclear reactor and testing facility - the only university-run reactor in New York State - as well as the Gaerttner Linear Accelerator, which is currently being upgraded under a $9.44 million grant from the US Department of Energy.[95]		In 2017, Rensselaer's enrollment was 7,442 total resident students including 6,200 undergraduates and 1,242 graduates.[25] Rensselaer is currently comprised of over 71% out-of-state students. Over 17% of all undergraduate and graduate students are international. Rensselaer students represent all 50 U.S. states and over 58 countries from around the world. The student to faculty ratio that same year was 13:1 for undergraduate students. Among the Class of 2020, 66% were in the top 5 percent of their high school class including 93% in the top quarter as well as 99% in the top half. The average unweighted GPA for enrolled students was 3.88 on a 4.0 scale, with 65% having a 3.75 GPA or higher and 99% having at least a 3.0 high school GPA. [96]		Rensselaer's yield rate for the Class of 2020 surpassed 21 percent in the year 2017.[25] The average SAT score range was 1280-1490 for the mid-50% range with a median SAT score of 1380 on a scale of 1600. The average ACT score range was 28-32 for the mid-50% range with a median ACT score of 30.[97] In 2016, Rensselaer's freshman retention rate was 94% and admissions selectivity rating was 35th in the nation according to the US News & World Report. [3]		Roughly 12% of students received the Rensselaer medal, a merit scholarship with a cumulative value of $100,000 for exceptional high school students in science and mathematics.[97] Altogether 94% of full-time beginning undergraduate students receive either need-based or merit-based financial aid, averaging 87% of total need met[98][99]		In the fall of 2016, more than 1,000 women enrolled in Rensselaer Polytechnic Institute's undergraduate engineering programs for the first time in its history. These women represented 30 percent of the student body in engineering at the university, and 32 percent of the university's total gender composition. Shekhar Garde, Rensselaer's dean of engineering, claims he wants to increase the female composition of the Institute to 50 percent before 2030.[100]		Rensselaer Polytechnic Institute has an extensive history of Greek community involvement on campus, including past presidents, honorary academic building dedications, and philanthropic achievements. The overall Greek system at Rensselaer stresses Leadership, Fortitude, Innovation, and Evolution[101]. RPI currently has 29 active fraternities as well as 6 sororities, with 32 percent involvement of all males and 18 percent involvement of all females, organized under the Interfraternal Council and Pan-Hellenic Council [102]. Of those Greek organizations, three were founded at Rensselaer including the Theta Xi national engineering fraternity, the Sigma Delta Hispanic-interest local sorority, and the Rensselaer Society of Engineers local engineering fraternity. Theta Xi fraternity was established by RPI students on April 29, 1864, the only national fraternity founded during the Civil War. The Theta Xi Fraternity Chapter House is listed on the National Register of Historic Places. Additionally, Rensselaer is home to the Epsilon Zeta chapter of the Alpha Phi Omega, or "APO," national service fraternity, which operates a test-bank and office at the top floor of the Student Union. The organization also hosts a campus lost & found, universal can tab collection, and a used book exchange.		In 2017, Chi Phi and Theta Chi at Rensselaer co-hosted an event called "Brave A Shave For Kids With Cancer," along with several other Greek organizations - raising over $22,000 for pediatric cancer research with dozens of participants shaving their heads to spread awareness of pediatric cancers. Many fraternities and sororities also engage in Adopt-a-Highway and host events in the local community.[103] Since it's inception, all members of Greek Life have also participated in Navigating Rensselaer & Beyond - RPI's official continuation of student orientation through hosting annual events open to all students such as Beach Day/Hike with Greek Life, a day of hiking and team building activities for incoming freshmen, and Saratoga Therapeutic Equine Program, a day of service focused on horse rehabilitation programs.		Greek Life organizations also operate Greek-affiliated groups including the Alumni Inter-Greek Council, Greek Greeks - a student-run venture which aims to promote sustainability and safe environmental practices in Greek chapter houses, Greek Spectrum - an LGBTQIA support and advocacy group, and the undergraduate Greek leadership society Order of Omega. Membership in the Order is incredibly selective, with Greek-affiliated members in fraternities and sororities being selected from the top 3 percent of Rensselaer in terms of academics. Members must also demonstrate exemplary leadership within their respective Greek chapter, campus, and the local community as a whole. Greek community members are also involved in hundreds of student clubs and organizations on campus, often holding leadership positions. Many Greek organizations at RPI require involvement in at least one on-campus club or organization to maintain or be considered for membership.		The RPI Engineers are the athletic teams for the university. RPI currently sponsors 23 sports, 21 of which compete at the NCAA Division III level in the Liberty League; men's and women's ice hockey compete at the Division I level in ECAC Hockey. The official nickname of some of the school's Division III teams was changed in 1995 from the Engineers to the Red Hawks. However, the hockey, football, cross-country, tennis and track and field teams all chose to retain the Engineers name. The Red Hawks name was, at the time, very unpopular among the student body; a Red Hawk mascot was frequently taunted with thrown concessions and chants of "kill the chicken!". In 2009 the nickname for all teams has since been changed back to Engineers. In contrast, the official ice hockey mascot, known as Puckman, has always been very popular. Puckman is an anthropomorphic hockey puck with an engineer's helmet.		During the 1970s and 1980s, one RPI cheer was:		RPI has a competitive Division I hockey team who won NCAA national titles in 1954 and 1985. Depending on how the rules are interpreted, the RPI men's ice hockey team may have the longest winning streak on record for a Division I team; in the 1984-85 season it was undefeated for 30 games, but one game was against the University of Toronto, a non-NCAA team. Continuing into the 1985-86 season, RPI continued undefeated over 38 games, including two wins over Toronto.[106] Adam Oates and Daren Puppa, two players during that time, both went on to become stars in the NHL. Joe Juneau, who played from 1987 to 1991, and Brian Pothier, who played from 1996 to 2000, also spent many years in the NHL. Graeme Townshend, who also played in the late 1980s, had a brief NHL career. He is the first man of Jamaican ancestry to play in the National Hockey League.		The ice hockey team plays a significant role in the campus's culture, drawing thousands of fans each week to the Houston Field House during the season. The team's popularity even sparked the tradition of the hockey line, where students lined up for season tickets months in advance of the on-sale date. Today, the line generally begins a week or more before ticket sales.[107] Another tradition since 1978 has been the "Big Red Freakout!" game held close to the first weekend of February. Fans usually dress in the schools colors red and white, and gifts such as T-shirts are distributed en masse. In ice hockey, the RPI's biggest rival has always been the upstate engineering school Clarkson University. In recent years RPI has also developed a spirited rivalry with their conference travel partner Union College, with whom they annually play a nonconference game in Albany for the Mayor's Cup.		The women's ice hockey team moved to the NCAA Division I level in 2005. During the 2008-09 season the team set the record for most wins in one season (19-14-4). On February 28, 2010, Rensselaer made NCAA history. The Engineers beat Quinnipiac, 2-1, but it took five overtimes. It is now the longest game in NCAA Women's Ice Hockey history. Senior defenseman Laura Gersten had the game-winning goal. She registered it at 4:32 of the fifth overtime session to not only clinch the win, but the series victory.[108]		The lacrosse team represented the United States in the 1948 Olympics in London. It won the Wingate Memorial Trophy as national collegiate champions in 1952.[109] Future NHL head coach Ned Harkness coached the lacrosse and ice hockey teams, winning national championships in both sports.		The Engineers baseball squad is perennially atop the Liberty League standings, and has seen 8 players move on to the professional ranks, including 4 players selected in the MLB draft. The team is coached by Karl Steffen (Ithaca '78). The Engineers play their home games at the historic Robison Field.		American rugby was played on campus in the late 1870s. Intercollegiate football begin as late as 1886 when an RPI team first played a Union College team on a leased field in West Troy (Watervliet). Since 1903, RPI and nearby Union have been rivals in football, making it the oldest such rivalry in the state. The teams have played for the Dutchman's Shoes since 1950. RPI Football had their most successful season in 2003, when they finished 11-2 and lost to St. Johns (Minn.) in the NCAA Division III semi final game.[110]		The Houston Field House is a 4,780‑seat multi-purpose arena located on the RPI campus. It opened in 1949 and is home to the RPI Engineers men's and women's ice hockey teams. The Field House was renovated starting in 2007 as part of the major campus improvement project to build the East Campus Athletic Village. The renovations included locker rooms upgrades, addition of a new weight room, and a new special reception room dedicated to Ned Harkness.[111] Additionally, as part of the renovations through a government grant, solar panels were installed on the roof to supply power to the building.		As part of the Rensselaer Plan, the Institute recently completed a major project to improve its athletic facilities with the East Campus Athletic Village. The plan included construction of a new and much larger 4,842‑seat football stadium, a basketball arena with seating for 1,200, a new 50-meter pool, an indoor track and field complex, new tennis courts, new weight rooms and a new sports medicine center.[112] The Institute broke ground on August 26, 2007, and construction of the first phase is expected to last two years.[113] The estimated cost of the project is $78 million for phase one and $35–$45 million for phase two.[114] Since the completion of the new stadium, the bleachers on the Class of '86 football field on the central campus have been removed and the field has become an open space. In the future the new space could be used for expansions of the academic buildings, but for now members of the campus planning team foresee a "historic landscape with different paths and access ways for students and vehicles alike".[115]		The students of RPI have created and participate in a variety of student-run clubs and organizations funded by the Student Union. The Union is unusual in that it is entirely student-run and its operations are paid for by activity fees. No other student union located at a private university in the United States is entirely student run. About 170 of these organizations are funded by the Student Union, while another thirty, which consist mostly of political and religious organizations, are self-supporting.[116] In 2006 the Princeton Review ranked RPI second for "more to do on campus."[117]		Phalanx is RPI's Senior Honor Society.[118] It was founded in 1912, when Edward Dion and the Student Council organized a society to recognize those RPI students who had distinguished themselves among their peers in the areas of leadership, service and devotion to the alma mater. It is a fellowship of the most active in student activities and has inducted more than 1,500 members since its founding.[119]		RPI has around twenty intramural sports organizations, many of which are broken down into different divisions based on level of play. Greek organizations compete in them as well as independent athletes. There are also thirty-nine club sports.		Given the university's proximity to the Berkshires, Green Mountains and Adirondacks, the Ski Club and the Outing Club are some of the largest groups on campus. The Ski Club offers weekly trips to local ski areas during the winter months,[120] while the Outing Club offers trips on a weekly basis for a variety of activities.[121]		The Rensselaer Polytechnic is the student-run weekly newspaper.[122] The Poly prints about 7,000 copies each week and distributes them around campus. Although it is the Union club with the largest budget, The Poly receives no subsidy from the Union, and obtains all funding through the sale of advertisements. There is also a popular student-run magazine called Statler & Waldorf which prints on a semesterly basis.[123]		RPI has an improvisational comedy group, Sheer Idiocy, which performs several shows a semester.[124] There are also several music groups ranging from a cappella groups such as the Rusty Pipes, Partial Credit, the Rensselyrics and Duly Noted,[125] to several instrumental groups such as the Orchestra, the Jazz Band and a classical choral group, the Rensselaer Concert Choir.		Another notable organization on campus is WRPI, the campus radio station. WRPI differs from most college radio in that it serves a 75-mile (121 km) radius[126] including the greater Albany area. With 10 kW of broadcasting power, WRPI maintains a stronger signal than nearly all college radio stations and some commercial stations. WRPI currently broadcasts on 91.5 FM in the Albany area.		The RPI Players is an on‑campus theater group that was formed in 1929. The Players resided in the Old Gym until 1965 when they moved to their present location at the 15th Street Lounge. This distinctive red shingled building had been a USO hall for the U.S. Army before being purchased by RPI. The Players have staged over 275 productions in its history.[127]		There are a number of songs commonly played and sung at RPI events.[128][129] Notable among them are:		Another notable aspect of student life at RPI is the "First-Year Experience" (FYE) program. Freshman begin their stay at RPI with a week called "Navigating Rensselaer and Beyond" or NRB week. The Office of the First-Year Experience provides several programs that extend to not only freshman, but to all students. These include family weekend, community service days, the Information and Personal Assistance Center (IPAC), and the Community Advocate Program.[130] Recently the FYE program was awarded the 2006 NASPA Excellence Gold Award, in the category of "Enrollment Management, Orientation, Parents, First-Year, Other-Year and related".[131]		Since 2008, Jackson's administration has led an effort to form the CLASS Initiative ("Clustered Learning Advocacy and Support for Students"), which requires all sophomores to live on campus and to live with special "residence cluster deans".[132] The transition to this program began in early 2010 among some resistance from some fraternities and students who had planned to live off campus.[133][134]		According to the Rensselaer Alumni Association, there are nearly 100,000 RPI graduates currently living in the United States, and another 4,378 living abroad.[135] In 1995, the Alumni Association created the Rensselaer Alumni Hall of Fame.[136]		Several notable 19th century civil engineers graduated from RPI. These include the visionary of the transcontinental railroad, Theodore Judah, Brooklyn Bridge engineer Washington Roebling, George Washington Gale Ferris Jr. (who designed and built the original Ferris Wheel) and Leffert L. Buck, the chief engineer of the Williamsburg Bridge in New York City.[136]		Many RPI graduates have made important inventions, including Allen B. DuMont ('24),[137] creator of the first commercial television and radar; Keith D. Millis ('38),[138] inventor of ductile iron; Ted Hoff ('58),[139] father of the microprocessor; Raymond Tomlinson ('63),[140] often credited with the invention of e-mail; inventor of the digital camera Steven Sasson[141] and Curtis Priem ('82), designer of the first graphics processor for the PC, and co-founder of NVIDIA.		In addition to NVIDIA, RPI graduates have also gone on to found or co-found major companies such as John Wiley and Sons, Texas Instruments, Fairchild Semiconductor, PSINet, MapInfo, Adelphia Communications, Level 3 Communications, Garmin, and Bugle Boy. Several RPI graduates have played a part in the U.S. space program: George Low (B.Eng. 1948, M.S. 1950) was manager of the Apollo 11 project and served as president of RPI, and astronauts John L. Swigert, Jr., Richard Mastracchio, Gregory R. Wiseman, and space tourist Dennis Tito are alumni.		Political figures who graduated from RPI included federal judge Arthur J. Gajarsa (B.S. 1962), Major General Thomas Farrell of the Manhattan Project, DARPA director Tony Tether, Representative John Olver of Massachusetts's 1st congressional district, and Senators Mark Shepard of Vermont and George R. Dennis of Maryland.		Notable ice hockey players include NHL Hockey Hall of Famer and 5-time NHL All Star Adam Oates (1985), Stanley Cup winner and former NHL All Star Mike McPhee (1982), two-time Calder Cup winner Neil Little (1994), former NHL All Rookie Joé Juneau (1991), and former NHL All Star Daren Puppa (1985).		Other notable alumni include 1973 Nobel Prize in Physics winner Ivar Giaever (Ph.D. 1964);[142] the first African-American woman to become a thoracic surgeon, Rosalyn Scott (B.S. 1970); director of Linux International Jon Hall (M.S. 1977); NCAA president Myles Brand (B.S. 1964);[143] adult stem cell pioneer James Fallon; Michael D. West, gerontologist and stem cell scientist, founder of Geron, now CEO of BioTime (1976); director Bobby Farrelly (1981),[144] David Ferrucci, lead researcher on IBM's Watson/Jeopardy! project. 66th AIA Gold Medal winning architect Peter Q Bohlin and Matt Patricia, defensive coordinator for the New England Patriots. Garrettina LTS Brown, founder of Garrett's List, King Breeders and inventor of FreeTV. Luis Acuña-Cedeño, Governor of the Venezuelan Sucre State and former Minister of Universities. Andrew Franks, current placekicker for the Miami Dolphins of the National Football League. Sean Conroy, the first openly gay professional baseball player.		
Swarthmore College (/ˈswɑːθ.mɔːr/ SWAHTH-mor locally, or /ˈswɔːrθ.mɔːr/ SWAWRTH-mor) is a private liberal arts college located in Swarthmore, Pennsylvania, 11 miles (18 km) southwest of Philadelphia.[6] Founded in 1864, Swarthmore was one of the earliest coeducational colleges in the United States.[7] It was established to be a college "...under the care of Friends, at which an education may be obtained equal to that of the best institutions of learning in our country."[8] By 1906, Swarthmore dropped its religious affiliation, becoming officially non-sectarian.[9]		Swarthmore is a member of the "Tri-College Consortium", a cooperative arrangement among Swarthmore, Bryn Mawr, and Haverford Colleges. In addition, the College is affiliated with the University of Pennsylvania through the "Quaker Consortium," allowing for students to cross-register for classes at all four institutions.[10] It offers more than 600 courses a year in over 40 courses of study including an engineering program in which at the completion of four years' work, students are granted a B.S. in Engineering.[11] Swarthmore has a variety of sporting teams with a total of 22 Division III Varsity Intercollegiate Sports Teams and competes in the Centennial Conference, a group of private colleges in Pennsylvania and Maryland.[12]		The college was ranked the best liberal arts college in the United States a total of six times by U.S. New World & Report,[13] and as of 2017, is currently tied as the 4th best liberal arts in the country.[14]		Swarthmore's alumni include five Nobel Prize winners (second highest number of Nobel Prize winners per graduate in the U.S.), 11 MacArthur Foundation fellows, and hundreds of prominent figures in law, art, science, business, politics, and other fields.						The name "Swarthmore" has its roots in early Quaker history. In England, Swarthmoor Hall near the town of Ulverston, Cumbria, (previously in Lancashire) was the home of Thomas and Margaret Fell in 1652 when George Fox, (1624–1691), fresh from his epiphany atop Pendle Hill in 1651, came to visit. The visitation turned into a long association, as Fox persuaded Thomas and Margaret Fell of his views. Swarthmoor was used for the first meetings of what became known as the "Religious Society of Friends" (later pejoratively labeled ""The Quakers").		The College was founded in 1864 by a committee of Quakers who were members of the Philadelphia, New York and Baltimore Yearly Meetings of the "Religious Society of Friends" ("Quakers"/"Hicksite"). Edward Parrish, (1822–1872), was its first president. Lucretia Mott, (1793–1880), and Martha Ellicott Tyson, (1795–1873),[15] were among those Friends, who insisted that the new college of Swarthmore be coeducational. Edward Hicks Magill, the second president, served for 17 years.[16] His daughter, Helen Magill, (1853–1944), was in the first class to graduate in 1873; in 1877, she was the first woman in the United States to earn a Doctor of Philosophy degree, (Ph.D.); hers was in Greek from Boston University in Boston, Massachusetts.[17]		In the early 1900s, the College had a major collegiate American football program during the formation period of the soon-to-be nationwide sport, (playing Navy, (Annapolis), Princeton, Columbia, and other larger schools) and an active fraternity and sorority life.[18] The 1921 appointment of Frank Aydelotte as President began the development of the school's current academic focus, particularly with his vision for the Honors program based on his experience as a Rhodes Scholar.[19]		During World War II, Swarthmore was one of 131 colleges and universities nationally that took part in the V-12 Navy College Training Program, which offered students a path to a U.S. Navy commission.[20]		Wolfgang Köhler, Hans Wallach and Solomon Asch were noted psychologists who became professors at Swarthmore, a center for Gestalt psychology. Both Wallach, who was Jewish, and Köhler, who was not, had left Nazi Germany because of its discriminatory policies against Jews. Köhler came to Swarthmore in 1935 and served until his retirement in 1958. Wallach came in 1936, first as a researcher, and also teaching from 1942 until 1975. Asch, who was Polish-American and had immigrated as a child to the US in 1920, joined the faculty in 1947 and served until 1966, conducting his noted conformity experiments at Swarthmore.[21]		On April 20, 2016, the United States Department of the Treasury announced that Lucretia Mott, a founder of the College, and Alice Paul, class of 1905, will be incorporated into the new design for the $10 bill.[22]		Swarthmore's Oxbridge tutorial-inspired Honors Program allows students to take double-credit seminars from their junior year and often write honors theses. Seminars are usually composed of four to eight students. Students in seminars will usually write at least three ten-page papers per seminar, and often one of these papers is expanded into a 20–30 page paper by the end of the seminar. At the end of their senior year, Honors students take oral and written examinations conducted by outside experts in their field. Usually one student in each discipline is awarded "Highest Honors"; others are either awarded "High Honors" or "Honors"; rarely, a student is denied any Honors altogether by the outside examiner. Each department usually has a grade threshold for admission to the Honors program.[23]		Uncommon for a liberal arts college, Swarthmore has an engineering program in which at the completion of four years' work, students are granted a B.S. in Engineering. Other notable programs include minors in peace and conflict studies, cognitive science, and interpretation theory.[11]		Swarthmore has a total undergraduate student enrollment of 1,620 (for the 2016–2017 year) and 187 faculty members (99% with a terminal degree), for a student-faculty ratio of 8:1. The small college offers more than 600 courses a year in over 40 courses of study.[24] Swarthmore has a reputation as a very academically oriented college, with 66% of students participating in undergraduate research or independent creative projects, and 90% of graduates eventually attending graduate or professional school.		While many in higher education recognize the college's relative lack of grade inflation,[25][26] there is some controversy over the accuracy of such perceptions. One study by a Swarthmore professor in 1993 found "significant grade inflation." However, other professors and students dispute the findings based on their own experience[who?]. Some have pointed out[who?] that statistics suggesting grade inflation over the past decades may be exaggerated by reporting practices, and the fact that grades were not given in the Honors program until 1996.[27] In the end, many still credit Swarthmore with having resisted grade inflation, bucking the perceived trend amongst peer institutions.[28][29]		Some sources, including Greene's Guides,[30] have termed Swarthmore one of the "Little Ivies". In its 2013 college ranking, the national news magazine, "U.S. News & World Report" ranked Swarthmore as the third-best liberal arts college in the nation, behind Williams and Amherst.[31] Since the inception of the "U.S. News" rankings, Amherst, Williams, and Swarthmore are the only colleges to have been ranked for the number one liberal arts college. Swarthmore has been ranked the number one liberal arts college in the country a total of six times (the most recent being in 2002).[13]		In its 2016 ranking of U.S. colleges and universities, Forbes magazine ranked Swarthmore tenth in the nation.[32] In the March/April 2007 edition of Foreign Policy magazine, a ranking of the top twenty institutions for the study of international relations placed Swarthmore as the highest-ranked undergraduate-only institution, coming in at 15. The only other undergraduate-focused programs to make the list were Dartmouth and Williams, although neither school is exclusively undergraduate.[33]		Swarthmore ranks 10th in The Wall Street Journal's 2004 survey of feeder schools to top ranked business, medical, and law schools.[34] Swarthmore ranked fourth among all institutions of higher education in the United States as measured by the percentage of graduates who went on to earn Ph.D.s between 2002–2011.[35]		Swarthmore ranked tenth among all colleges and sixth for liberal arts colleges only in the amount of schools that selected it as a peer institution.[36] Swarthmore selected Amherst, Bowdoin, Carleton, Davidson, Haverford, Middlebury, Oberlin, Pomona, Trinity, Wesleyan, and Williams as schools of comparable academic quality.[37]		In 2009, 2010, 2011, and 2013,[38] Swarthmore was named the #1 "Best Value" private college by The Princeton Review.[39] Overall selection criteria included more than 30 factors in three areas: academics, costs and financial aid. Swarthmore was also placed on The Princeton Review's Financial Aid Honor Roll along with twelve other institutions for receiving the highest possible rating in its ranking methodology.[40]		The college is considered by U.S. News & World Report as "most selective", with 12.5% accepted of the 7,818 applicants during the 2014–2015 admissions cycle, a 4.5% drop in acceptance.[45][46] Of those admitted, the college reports that of the 44% who reported rank, "33 percent are valedictorians or salutatorians. [52] percent are in [the] top two percent of their high school class and 94 percent are in the top decile."[45] Furthermore, 8.49% of 212 transfer applicants were admitted.[41]		In 2012, The Princeton Review gave Swarthmore a 99 out of 99 on their Admissions Selectivity Rating.[47] In the November 2003 selectivity ranking for undergraduate programs, The Atlantic magazine ranked Swarthmore as the only liberal arts college to make the top ten institutions, placing Swarthmore in tenth place.[48][49]		16% of earners of undergraduate degrees immediately enter graduate or professional school, and within five years of graduation 77% of alumni enter these programs. Alumni of the school earn graduate degrees most commonly at institutions that include University of California-Berkeley, University of Michigan, Harvard, Columbia, New York University, University of Pennsylvania, Oxford, Johns Hopkins, Stanford, and Yale.[50] At graduate programs, the most common fields for Swarthmore graduates to enter are humanities, math & physical sciences, life sciences, and social sciences.[50]		PayScale reports that Swarthmore graduates with only a bachelor's degree have an average starting salary of $51,000 and an average mid-career salary of $109,000, making their salaries 36th highest among all colleges and 11th among liberal arts colleges alone.[51][52]		The cost of tuition, student activity fees, room, and board for the 2017–2018 academic year was $65,774 (tuition alone was $50,424).[24] One hundred percent of admitted students' demonstrated need is offered by the college. In total, 55% of the student body receives financial aid, and the average financial aid award was $46,681 during the 2017–17 year.[53] As a need-blind school, Swarthmore makes admission decisions and financial aid decisions independently.		Swarthmore's endowment at the end of the 2016 fiscal year was $1,746,962,000. Endowment per student was $1,078,371 for the same year, one of the highest rates in the country. Operating revenue for the 2010 fiscal year was $148,086,000, over 50% of which was provided by the endowment.[53] Swarthmore ended a $230 million capital campaign on October 6, 2006, when President Bloom declared the project completed, three months ahead of schedule. The campaign, christened the "Meaning of Swarthmore," had been underway officially since the fall of 2001. 87% of the college's alumni participated in the effort.		At the end of 2007, the Swarthmore Board of Managers approved the decision for the college to eliminate student loans from all financial aid packages. Instead, additional aid scholarships will be granted.[54]		The campus consists of 399 acres (1.61 km2), based on a north-south axis anchored by Parrish Hall, which houses numerous administrative offices and student lounges, as well as two floors of student housing. The fourth floor houses campus radio station WSRN-FM as well as the weekly student newspaper, The Phoenix.		From the SEPTA Swarthmore commuter train station and the ville of Swarthmore to the south, the oak-lined Magill Walk leads north up a hill to Parrish. The campus is also adjacent to the Scott Arboretum, cited by some as a main staple of the campus's renowned beauty.[55] In 2011, Travel+Leisure named Swarthmore as one of the most beautiful college campuses in the United States.[56]		The majority of the buildings housing classrooms and department offices are located to the north of Parrish, as are Kyle and Woolman dormitories. McCabe Library is to the east of Parrish, as are the dorms of Willets, Mertz, Worth, The Lodges, Alice Paul, and David Kemp. To the west are the dorms of Wharton, Dana, Hallowell, and Danawell, along with the Scott Amphitheater. The Crum Woods extend westward from the campus, toward the Crum Creek. South of Parrish are Sharples dining hall, the two non-residential fraternities (Phi Psi and Delta Upsilon), and various other buildings. Palmer, Pittenger, and Roberts dormitories are south of the railroad station, as are the athletic facilities, while the Mary Lyon dorm is off-campus to the southwest.[57]		The College has three main libraries (McCabe Library, the Cornell Library of Science and Engineering, and the Underhill Music and Dance Library) and seven other specialized collections.[58] In total, the libraries hold over 800,000 print volumes as well as an expanding digital library of over 10,000 online journal subscriptions, reference materials, e-books, and other scholarly databases.[59] Since 1923, McCabe library has been a Federal Depository library for selected U.S. Government documents.		Friends Historical Library was established in 1871 to collect, preserve, and make available archival, manuscript, printed, and visual records concerning the Religious Society of Friends (Quakers) from their origins mid-seventeenth century to the present. Besides the obvious focus on Quaker history, the holdings are a significant research collection for the regional and local history of the middle-Atlantic region of the United States and the history of American social reform. Quakers played prominent roles in almost every major reform movement in American history, including abolition, African-American history, Indian rights, women's rights, prison reform, humane treatment of the mentally ill, and temperance. The collections also reflect the significant role Friends played in the development of science, technology, education, and business in Britain and America. The Library also maintains the Swarthmore College Archives and the papers of the Swarthmore Historical Society.[60][61]		Founded in 2000,[62] the Swarthmore Mock Trial team placed 10th at the 2000 American Mock Trial Association (AMTA) National Championship Tournament and was awarded "Best New School." Dennis Cheng '01 was awarded the prestigious "Spirit of AMTA" award in 2000.[63][64] Swarthmore's team placed 2nd at the 2001 AMTA National Championship Tournament.[64] The Swarthmore Mock Trial program has also won numerous accolades and boasted a team of over 25 members for the 2013–2014 season. The 2010–2011 competitive season resulted in all three teams competing at Regional Championships, two teams going on to Opening Round Championships, and one team qualifying and competing at the 2011 National Championships held in Des Moines, Iowa, where the team placed 15th in their division. Other successes included placing first at the Philadelphia Regional competition in February 2011, and winning the University of Massachusetts Amherst's invitational tournament in February 2014.[65]		The Amos J. Peaslee Debate Society, named after a former United States Ambassador to Australia, is one of the few independently endowed organizations on campus. Members of the Society debate on the American Parliamentary Debate Association (APDA) circuit in addition to traveling abroad to Britain, Canada, and the World Universities Debating Championship for British Parliamentary Style tournaments. The team has won four APDA national championships, including one as recently as 2017. It has also won Team of the Year two times and Speaker of the Year once. In 2017, it was ranked as the top liberal arts debate program on APDA.		Two Greek organizations exist on the campus in the form of fraternities, Delta Upsilon and local Phi Psi, a former chapter of Phi Kappa Psi. A third, Phi Sigma Kappa fraternity, maintained a chapter on campus from 1906 to 1991 and continues strong alumni involvement.[66]		Sororities were abandoned in the 1930s following student outrage about discrimination within the sorority system, and leading to a 79-year ban.[67][68] However, in September 2012, the college announced that the ban on sororities would be reversed as of the 2013 term, citing Title IX regulations.[69] The four women who helped overturn the ban subsequently spearheaded the reestablishment of a Kappa Alpha Theta chapter the following spring.[70][71] The announcement sparked controversy on campus; a petition seeking a referendum to continue the ban was dismissed, again citing a legal opinion that to disallow the sorority chapter would be a violation of Title IX regulations. The sorority admitted its first pledge class in the Spring of 2013. A further non-binding referendum was later distributed, but by then the controversy had cooled: Of the six items on the referendum, only one passed, which asked "Do you support admitting students of all genders to sororities and fraternities?" No action was taken on the referendum.[72]		Swarthmore has a variety of sporting teams with a total of 22 Division III Varsity Intercollegiate Sports Teams. 40 percent of Swarthmore students play intercollegiate or club sports.[73] Varsity teams include badminton, baseball, basketball, cross country, field hockey, golf, lacrosse, soccer, softball, swimming, tennis, track and field and volleyball. The football team was controversially eliminated in 2000, along with wrestling and, initially, badminton. The Board of Managers cited lack of athletes on campus and difficulty of recruiting as reasons for terminating the programs.[74][75] Swarthmore also offers a number of club sport options, including men's and women's rugby, ultimate frisbee, volleyball, fencing, squash, and quidditch.[76]		Swarthmore is a charter member of the Centennial Conference, a group of private colleges in Pennsylvania and Maryland.[12]		Based on federal campus safety data for 2014, Swarthmore College was the third highest in the nation in "total reports of rape per 1,000 students" on its main campus, with 11 reports of rape per 1,000 students.[77]		Swarthmore has two main student news publications. The weekly newspaper is called The Phoenix, is published nearly every Thursday. Founded in 1881, the paper began putting stories online in 1995. Two thousand copies are distributed across the college campus and to the Borough of Swarthmore. The newspaper is printed by Bartash Printing in Philadelphia, Pennsylvania.[78]		There are a number of magazines at Swarthmore, most of which are published biannually at the end of each semester. One is Spike, Swarthmore's humor magazine, founded in 1993. The others are literary magazines, including Nacht, which publishes long-form non-fiction, fiction, poetry, and artwork; Small Craft Warnings, which publishes poetry, fiction and artwork; Scarlet Letters, which publishes women's literature; Enie, for Spanish literature; OURstory, for literature relating to diversity issues; Bug-Eyed Magazine, a very limited-run science fiction/fantasy magazine published by Psi Phi, formerly known as Swarthmore Warders of Imaginative Literature (SWIL); Remappings (formerly "CelebrASIAN"), published by the Swarthmore Asian Organization; Alchemy, a collection of academic writings published by the Swarthmore Writing Associates; Mjumbe, published by the Swarthmore African-American Student Society; and a magazine for French literature. An erotica magazine, ! (pronounced "bang") was briefly published in 2005 in homage to an earlier publication, Untouchables. Most of the literary magazines print approximately 500 copies, with around 100 pages. There is also a new photography magazine, Pun/ctum, which features work from students and alumni.[79]		The collegiate a cappella groups include Sixteen Feet, the College's oldest group (founded in 1981), as well as its first and only all-male group. Grapevine is its corresponding all-female group, and Mixed Company is a co-ed group. Chaverim is a co-ed group that includes students from the Tri-College Consortium and draws on music from cultures around the world for its repertoire. Lastly, OffBeat was founded in the fall of 2013 as a co-ed group. Once every semester, all of the school's a cappella groups collaborate for a joint concert called Jamboree.[80]		WSRN 91.5 FM is the college radio station. It has a mix of indie, rock, hip-hop, folk, world, jazz, and classical music, as well as a number of radio talk shows. At one time, WSRN had a significant news department, and covered events such as the "Crisis of '69" extensively.[81] In the 1990s, WSRN centered its programming on the immensely popular "Hank and Bernie Show", starring undergraduates Hank Hanks and Bernie Bernstein. Hank and Bernie conducted wide-ranging and entertaining interviews of sports stars and cultural icons such as Lou Piniella, Mark Grace, Jake Plummer, Greg Ostertag, Andy Karich and Mark "the Bird" Fidrych, and also engaged the Swarthmore community in discussions on campus issues and current events. Upwards of 90 percent of the Swarthmore community would tune in to the Hank and Bernie Show and many members of the surrounding villages and towns would also listen and call in. Many archived recordings of musical and spoken word performances exist, such as the once-annual Swarthmore Folk Festival.[82] Today WSRN focuses virtually exclusively on entertainment, though it has covered significant news developments such as the athletic cuts in 2000[83] and the effects of 11 September 2001 on campus. War News Radio and The Sudan Radio Project (formerly the Darfur Radio Project) do broadcast news on WSRN, however. Currently, the longest running show in WSRN's lineup is "Oído al Tambor", which focuses on news and music from Latin America. The show has been running non-stop, on Sundays from 4:00 to 6:00 p.m., since September 2006. After its members graduated in December 2009, the show's concept was revived by the show "Rayuela", which has been running since September 2009.		Swarthmore College students are eligible to participate in the local emergency department, the Swarthmore Fire and Protective Association. They are trained as firefighters and as emergency medical technicians (EMTs) and are qualified on both the state and national level. The fire department responds to over 200 fire calls and almost 800 EMS calls a year.[84]		Swarthmore College Computer Society (SCCS) is a student-run volunteer organization independent of the official ITS department of the college.[85] In addition to operating a set of servers that provide e-mail accounts, Unix shell login accounts, server storage space, and webspace to students, professors, alumni, and other student-run organizations, SCCS hosts over 100 mailing lists used by various student groups, and over 130 organizational websites, including the website of the student newspaper, The Daily Gazette. SCCS also provides a computer lab and gaming room, located in Clothier basement beneath Essie Mae's snack bar.[86]		In September 2003, the SCCS servers survived a Slashdotting while hosting a copy of the Diebold memos on behalf of the student group Free Culture Swarthmore, then known as the Swarthmore Coalition for the Digital Commons. SCCS staff promptly complied with the relevant DMCA takedown request received by the college's ITS department.[87]		SCCS was noted in PC Magazine's article "Top 20 Wired Colleges" as one of the reasons for ranking Swarthmore #4 on that list.[88] During the 2004–2005 school year, the SCCS Media Lounge served as the early home of War News Radio, a weekly webcast run by Swarthmore students and providing news about the Iraq war, providing resources, space, and technical support for the project in its infancy.		Three SCCS-related papers have been accepted for publication at the USENIX Large Installation System Administration (LISA) Conference, one of which was awarded Best Paper.[89][90][91][92]		Swarthmore's alumni include five Nobel Prize winners (second highest number of Nobel Prize winners per graduate in the U.S.), including the 2006 Physics laureate John C. Mather (1968), the 2004 Economics laureate Edward Prescott (1962) and the 1972 Chemistry laureate Christian B. Anfinsen (1937). Swarthmore also has 11 MacArthur Foundation fellows and hundreds of prominent figures in law, art, science, business, politics, and other fields.		Alice Paul (Class of 1905), Suffragist and National Women's Party founder.		James A. Michener (Class of 1929), author.		Christian B. Anfinsen (Class of 1937), 1972 Nobel Prize in Chemistry.		Michael Dukakis (Class of 1955), 65th and 67th Governor of Massachusetts.		David Baltimore (Class of 1960), 1975 Nobel Prize in Physiology or Medicine.		John C. Mather (Class of 1968), 2006 Nobel Prize in Physics.		Jonathan Franzen (Class of 1981), novelist and essayist.		
Mainstream is current thought that is widespread.[1][2] It includes all popular culture and media culture, typically disseminated by mass media. It is to be distinguished from subcultures and countercultures, and at the opposite extreme are cult followings and fringe theories.		This word is sometimes used in a pejorative sense by subcultures who view ostensibly mainstream culture as not only exclusive but artistically and aesthetically inferior.[3] In the United States, mainline churches are sometimes referred to synonymously as "mainstream."[4][5]						The labels "Mainstream media", or "mass media", are generally applied to print publications, such as newspapers and magazines that contain the highest readership among the public, and to radio formats and television stations that contain the highest viewing and listener audience, respectively. This is in contrast to various independent media, such as alternative media newspapers, specialized magazines in various organizations and corporations, and various electronic sources such as podcasts and blogs (Though certain blogs are more mainstream than others given their association with a mainstream source.[6]		Mainstream science is scientific inquiry in an established field of study that does not depart significantly from orthodox theories. In the philosophy of science, mainstream science is an area of scientific endeavor that has left the process of becoming established. New areas of scientific endeavor still in the process of becoming established are generally labelled protoscience or fringe science. A definition of mainstream in terms of protoscience and fringe science[7] can be understood from the following table:[8]		By its standard practices of applying good scientific methods, mainstream is distinguished from pseudoscience as a demarcation problem and specific types of inquiry are debunked as junk science, cargo cult science, scientific misconduct, etc.		Mainstream pressure, through actions such as peer pressure, can force individuals to conform to the mores of the group (e.g., an obedience to the mandates of the peer group). Some, such as those of modern Hipster culture, have stated that they see mainstream as the antithesis of individuality.		According to sociologist G. William Domhoff, critiques of mainstream sociology and political science that suggest their allegiance to an elite few, such as the work of sociologists C. Wright Mills (especially his book The Power Elite) and Floyd Hunter, troubles mainstream sociologists, and mainstream sociology "often tries to dismiss power structure research as muckraking or mere investigative journalism" and downplays the notion of dominance by a power elite because of doubts about the ability of many business sectors to coordinate a unified program, while generally overlooking a policy-planning network that can perform this function.[9]		Mainstream Christianity is a term used to collectively refer to the common views of major denominations of Christianity (such as Orthodox Christianity, Roman Catholicism, Anglicanism, and Protestantism) as opposed the particular tenets of other Christian denominations. The context is dependent on the particular issues addressed, but usually contrasts an orthodox majority view against a heterodox minority view. In the most common sense, "mainstream" refers to Nicene Christianity, or rather the traditions which continue to claim adherence to the Nicene Creed.[10][11]		Mainstream American Protestant churches[12] (also called "Mainline Protestant") are a group of Protestant churches in the United States that have stressed social justice and personal salvation,[13] and both politically and theologically, tend to be more liberal than non-mainstream Protestants. Mainstream Protestant churches share a common approach that often leads to collaboration in organizations such as the National Council of Churches,[14] and because of their involvement with the ecumenical movement, they are sometimes given the alternative label of "ecumenical Protestantism" (especially outside the United States).[15] While in 1970 the mainstream Protestant churches claimed most Protestants and more than 30 percent of the American population as members,[16] as of 2009[update] they are a minority among American Protestants, claiming approximately 15 percent of American adults.[17]		"Mainstreaming" is the practice of bringing disabled students into the “mainstream” of student life. Mainstreamed students attend some classes with typical students and other classes with students that have similar disabilities. Mainstreaming represents a midpoint between full inclusion (all students spend all day in the regular classroom) and dedicated, self-contained classrooms or special schools (disabled students are isolated with other disabled students).				The term mainstream refers to the main current of a river or stream. Its figurative use by Thomas Carlyle to indicating the prevailing taste or mode is attested at least as early as 1831.[18] Indeed, one citation of this sense is found prior to Carlyle's, as early as 1599.[19]		
Michigan /ˈmɪʃᵻɡən/ ( listen) is a state in the Great Lakes and Midwestern regions of the United States.		The state's name, Michigan, is of French origins (form of the Ojibwe word) mishigamaa, meaning "large water" or "large lake".[3][8] Michigan is the tenth most populous of the 50 United States, with the 11th most extensive total area, and the largest state by total area east of the Mississippi River.[b]		Michigan's capital is Lansing, and its largest city is Detroit.		Michigan is the only state to consist of two peninsulas. The Lower Peninsula, to which the name Michigan was originally applied, is often noted to be shaped like a mitten. The Upper Peninsula (often referred to as "the U.P.") is separated from the Lower Peninsula by the Straits of Mackinac, a five-mile (8 km) channel that joins Lake Huron to Lake Michigan. The two peninsulas are connected by the Mackinac Bridge. The state has the longest freshwater coastline of any political subdivision in the world, being bounded by four of the five Great Lakes, plus Lake Saint Clair.[9] As a result, it is one of the leading U.S. states for recreational boating.[10]		Michigan also has 64,980 inland lakes and ponds.[11] A person in the state is never more than six miles (9.7 km) from a natural water source or more than 85 miles (137 km) from a Great Lakes shoreline.[12]		The area was first settled by Native American tribes and later colonized by French explorers in the 17th century and became part of New France. After France's defeat in the French and Indian War in 1762, the region came under British rule, and was ceded to the newly independent United States after Britain's defeat in the American Revolutionary War. The area was part of the larger Northwest Territory until 1800, when western Michigan became part of the Indiana Territory. In 1805, the Michigan Territory was formed, and in 1837 was admitted into the Union as the 26th state. It soon became an important center of industry and trade in the Great Lakes region and a popular immigrant destination.		Although Michigan has come to develop a diverse economy, it is widely known as the center of the U.S. automotive industry, being home to the country's three major automobile companies (whose headquarters are all within the Detroit metropolitan area). While sparsely populated, the Upper Peninsula is important for tourism thanks to its abundance of natural resources, while the Lower Peninsula is a center of manufacturing, services, and high-tech industry.						When the first European explorers arrived, the most populous tribes were Algonquian peoples, which include the Anishinaabe groups of Ojibwe (called "Chippewa" in French), Odaawaa/Odawa (Ottawa), and the Boodewaadamii/Bodéwadmi (Potawatomi). The three nations co-existed peacefully as part of a loose confederation called the Council of Three Fires. The Ojibwe, whose numbers are estimated to have been between 25,000 and 35,000, were the largest.		The Ojibwe were established in Michigan's Upper Peninsula and northern and central Michigan, and also inhabited Ontario, northern Wisconsin, southern Manitoba, and northern and north-central Minnesota. The Ottawa lived primarily south of the Straits of Mackinac in northern, western and southern Michigan, but also in southern Ontario, northern Ohio and eastern Wisconsin, while the Potawatomi were in southern and western Michigan, in addition to northern and central Indiana, northern Illinois, southern Wisconsin and southern Ontario. Other Algonquian tribes in Michigan, in the south and east, were the Mascouten, the Menominee, the Miami, the Sac (or Sauk), and the Fox, and the non-Algonquian Wyandot, who are better known by their French name, the Huron.		French voyageurs and coureurs des bois explored and settled in Michigan in the 17th century. The first Europeans to reach what became Michigan were those of Étienne Brûlé's expedition in 1622. The first permanent European settlement was founded in 1668 on the site where Père Jacques Marquette established Sault Ste. Marie, Michigan as a base for Catholic missions.[13][14] Missionaries in 1671–75 founded outlying stations at Saint Ignace and Marquette. Jesuit missionaries were well received by the area's Indian populations, with relatively few difficulties or hostilities. In 1679, Robert Cavelier, Sieur de la Salle built Fort Miami at present-day St. Joseph. In 1691, the French established a trading post and Fort St. Joseph along the St. Joseph River at the present day city of Niles.		In 1701, French explorer and army officer Antoine de la Mothe Cadillac founded Fort Pontchartrain du Détroit or "Fort Pontchartrain on-the-Strait" on the strait, known as the Detroit River, between lakes Saint Clair and Erie. Cadillac had convinced King Louis XIV's chief minister, Louis Phélypeaux, Comte de Pontchartrain, that a permanent community there would strengthen French control over the upper Great Lakes and discourage British aspirations.		The hundred soldiers and workers who accompanied Cadillac built a fort enclosing one arpent[15][16] (about 0.85 acres (3,400 m2), the equivalent of just under 200 feet (61 m) per side) and named it Fort Pontchartrain. Cadillac's wife, Marie Thérèse Guyon, soon moved to Detroit, becoming one of the first European women to settle in the Michigan wilderness. The town quickly became a major fur-trading and shipping post. The Église de Saint-Anne (Church of Saint Ann) was founded the same year. While the original building does not survive, the congregation continues to be active today. Cadillac later departed to serve as the French governor of Louisiana from 1710 to 1716. French attempts to consolidate the fur trade led to the Fox Wars involving the Meskwaki (Fox) and their allies versus the French and their Native allies.		At the same time, the French strengthened Fort Michilimackinac at the Straits of Mackinac to better control their lucrative fur-trading empire. By the mid-18th century, the French also occupied forts at present-day Niles and Sault Ste. Marie, though most of the rest of the region remained unsettled by Europeans. France offered free land to attract families to Detroit, which grew to 800 people in 1765, the largest city between Montreal and New Orleans.[17]		From 1660 until the end of French rule, Michigan was part of the Royal Province of New France.[c] In 1760, Montreal fell to the British forces ending the French and Indian War (1754–1763). Under the 1763 Treaty of Paris, Michigan and the rest of New France east of the Mississippi River passed to Great Britain.[18] After the Quebec Act was passed in 1774, Michigan became part of the British Province of Quebec. By 1778, Detroit's population was up to 2,144 and it was the third largest city in Quebec.[19]		During the American Revolutionary War, Detroit was an important British supply center. Most of the inhabitants were French-Canadians or Native Americans, many of whom had been allied with the French. Because of imprecise cartography and unclear language defining the boundaries in the 1783 Treaty of Paris, the British retained control of Detroit and Michigan after the American Revolution. When Quebec split into Lower and Upper Canada in 1791, Michigan was part of Kent County, Upper Canada. It held its first democratic elections in August 1792 to send delegates to the new provincial parliament at Newark (now Niagara-on-the-Lake).[20]		Under terms negotiated in the 1794 Jay Treaty, Britain withdrew from Detroit and Michilimackinac in 1796. Questions remained over the boundary for many years, and the United States did not have uncontested control of the Upper Peninsula and Drummond Island until 1818 and 1847, respectively.		During the War of 1812, Michigan Territory (effectively consisting of Detroit and the surrounding area) was surrendered after a nearly bloodless siege in 1812. An attempt to retake Detroit resulted in a severe American defeat in the River Raisin Massacre. This battle is still the bloodiest ever fought in the state and had the highest number of American casualties of any battle in the war. Ultimately, Michigan was recaptured by Americans in 1813 after the Battle of Lake Erie. An invasion of Canada which culminated in the Battle of the Thames was then launched from Michigan. The more northern areas were held by the British until the peace treaty restored the old boundaries. A number of forts, including Fort Wayne were built in Michigan during the 19th century out of fears of renewed fighting with Britain.		The population grew slowly until the opening in 1825 of the Erie Canal connecting the Great Lakes and the Hudson River and New York City. The new route brought a large influx of settlers, who became farmers and merchants and shipped out grain, lumber, and iron ore. By the 1830s, Michigan had 80,000 residents, more than enough to apply and qualify for statehood.		A Constitutional Convention of Assent, led by Gershom Mott Williams, was held to lead the territory to statehood.[21] In October 1835 the people approved the Constitution of 1835, thereby forming a state government, although Congressional recognition was delayed pending resolution of a boundary dispute with Ohio known as the Toledo War. Congress awarded the "Toledo Strip" to Ohio. Michigan received the western part of the Upper Peninsula as a concession and formally entered the Union on January 26, 1837. The Upper Peninsula proved to be a rich source of lumber, iron, and copper. Michigan led the nation in lumber production from the 1850s to the 1880s. Railroads became a major engine of growth from the 1850s onward, with Detroit the chief hub.		A second wave of French Canadian immigrants settled in Michigan during the late 19th to early 20th century, particularly in lumbering areas in counties on the Lake Huron side of the Lower Peninsula, such as the Saginaw Valley, Alpena, and Cheboygan counties as well as throughout the Upper Peninsula with large concentrations in Escanaba and the Keweenaw Peninsula.[22]		The first statewide meeting of the Republican Party took place July 6, 1854, in Jackson, Michigan, where the party adopted its platform. The state was heavily Republican until the 1930s. Michigan made a significant contribution to the Union in the American Civil War and sent more than forty regiments of volunteers to the federal armies.		Modernizers and boosters set up systems for public education, including founding the University of Michigan (1817; moved to Ann Arbor in 1837), for a classical academic education; and Michigan State Normal School, (1849) now Eastern Michigan University, for the training of teachers. In 1899, it became the first normal college in the nation to offer a four-year curriculum. Michigan Agricultural College (1855), now Michigan State University in East Lansing, was founded as the pioneer land-grant college, a model for those authorized under the Morrill Act (1862). Many other private colleges were founded as well, and the smaller cities formed high schools late in the century.[23]		Michigan's economy underwent a transformation at the turn of the 20th century. Many individuals, including Ransom E. Olds, John and Horace Dodge, Henry Leland, David Dunbar Buick, Henry Joy, Charles King, and Henry Ford, provided the concentration of engineering know-how and technological enthusiasm to start the birth of the automotive industry.[24] Ford's development of the moving assembly line in Highland Park marked the beginning of a new era in transportation. Like the steamship and railroad, it was a far-reaching development. More than the forms of public transportation, the automobile transformed private life. It became the major industry of Detroit and Michigan, and permanently altered the socio-economic life of the United States and much of the world.		With the growth, the auto industry created jobs in Detroit that attracted immigrants from Europe and migrants from across the United States, including those from the South. By 1920, Detroit was the fourth-largest city in the US. Residential housing was in short supply, and it took years for the market to catch up with the population boom. By the 1930s, so many immigrants had arrived that more than 30 languages were spoken in the public schools, and ethnic communities celebrated in annual heritage festivals. Over the years immigrants and migrants contributed greatly to Detroit's diverse urban culture, including popular music trends, such as the influential Motown Sound of the 1960s led by a variety of individual singers and groups.		Grand Rapids, the second-largest city in Michigan, is also an important center of manufacturing. Since 1838, the city has also been noted for its furniture industry and is home to five of the world's leading office furniture companies. Grand Rapids is home to a number of major companies including Steelcase, Amway, and Meijer. Grand Rapids is also an important center for GE Aviation Systems.		Michigan held its first United States presidential primary election in 1910. With its rapid growth in industry, it was an important center of union industry-wide organizing, such as the rise of the United Auto Workers.		In 1920 WWJ (AM) in Detroit became the first radio station in the United States to regularly broadcast commercial programs. Throughout that decade, some of the country's largest and most ornate skyscrapers were built in the city. Particularly noteworthy are the Fisher Building, Cadillac Place, and the Guardian Building, each of which is a National Historic Landmark (NHL).		In 1927 a school bombing took place in Clinton County; the Bath School disaster, which resulted in the deaths of 38 schoolchildren, constitutes the deadliest mass murder in a school in U.S. history.		Michigan manufactured 10.9 percent of total United States military armaments produced during World War II, ranking second (behind New York) among the 48 states.[25]		Detroit continued to expand through the 1950s, at one point doubling its population in a decade. After World War II, housing was developed in suburban areas outside city cores; newly constructed U.S. Interstate Highways allowed commuters to navigate the region more easily. Modern advances in the auto industry have led to increased automation, high tech industry, and increased suburban growth since 1960.		Michigan is the leading auto-producing state in the US, with the industry primarily located throughout the Midwestern United States, Ontario, Canada, and the Southern United States.[26] With almost ten million residents, Michigan is a large and influential state, ranking tenth in population among the fifty states. Detroit is the centrally located metropolitan area of the Great Lakes Megalopolis and the second-largest metropolitan area in the U.S. linking the Great Lakes system.		The Metro Detroit area in Southeast Michigan is the state's largest metropolitan area (roughly 50% of the population resides there) and the eleventh largest in the USA. The Grand Rapids metropolitan area in Western Michigan is the state's fastest-growing metro area, with over 1.3 million residents as of 2006. Metro Detroit receives more than 15 million visitors each year. Michigan has many popular tourist destinations, including areas such as Frankenmuth in The Thumb, and Traverse City on the Grand Traverse Bay in Northern Michigan. Tourists spend about $17 billion annually in Michigan supporting 193,000 jobs.[27]		Michigan typically ranks third or fourth in overall Research & development (R&D) expenditures in the US.[28][29] The state's leading research institutions include the University of Michigan, Michigan State University and Wayne State University which are important partners in the state's economy and the state's University Research Corridor.[30] Michigan's public universities attract more than $1.5 B in research and development grants each year.[31] Agriculture also serves a significant role, making the state a leading grower of fruit in the US, including blueberries, cherries, apples, grapes and peaches.[32]		Michigan is governed as a republic, with three branches of government: the executive branch consisting of the Governor of Michigan and the other independently elected constitutional officers; the legislative branch consisting of the House of Representatives and Senate; and the judicial branch. The Michigan Constitution allows for the direct participation of the electorate by statutory initiative and referendum, recall, and constitutional initiative and referral (Article II, § 9,[33] defined as "the power to propose laws and to enact and reject laws, called the initiative, and the power to approve or reject laws enacted by the legislature, called the referendum. The power of initiative extends only to laws which the legislature may enact under this constitution"). Lansing is the state capital and is home to all three branches of state government.		The governor and the other state constitutional officers serve four-year terms and may be re-elected only once. The current governor is Rick Snyder. Michigan has two official Governor's Residences; one is in Lansing, and the other is at Mackinac Island. The other constitutionally elected executive officers are the lieutenant governor, who is elected on a joint ticket with the governor, the secretary of state, and the attorney general. The lieutenant governor presides over the Senate, but only voting when ties occur, and is also a member of the cabinet. The secretary of state is the chief elections officer and is charged with running many licensure programs including motor vehicles, all of which are done through the branch offices of the secretary of state.		The Michigan Legislature consists of a 38-member Senate and 110-member House of Representatives. Members of both houses of the legislature are elected through first past the post elections by single-member electoral districts of near-equal population that often have boundaries which coincide with county and municipal lines. Senators serve four-year terms concurrent to those of the governor, while representatives serve two-year terms. The Michigan State Capitol was dedicated in 1879 and has hosted the executive and legislative branches of the state ever since.		The Michigan judiciary consists of two courts with primary jurisdiction (the Circuit Courts and the District Courts), one intermediate level appellate court (the Michigan Court of Appeals), and the Michigan Supreme Court. There are several administrative courts and specialized courts. District courts are trial courts of limited jurisdiction, handling most traffic violations, small claims, misdemeanors, and civil suits where the amount contended is below $25,000. District courts are often responsible for handling the preliminary examination and for setting bail in felony cases. District court judges are elected to terms of six years. In a few locations, municipal courts have been retained to the exclusion of the establishment of district courts. There are 57 circuit courts in the State of Michigan, which have original jurisdiction over all civil suits where the amount contended in the case exceeds $25,000 and all criminal cases involving felonies. Circuit courts are also the only trial courts in the State of Michigan which possess the power to issue equitable remedies. Circuit courts have appellate jurisdiction from district and municipal courts, as well as from decisions and decrees of state agencies. Most counties have their own circuit court, but sparsely populated counties often share them. Circuit court judges are elected to terms of six years. State appellate court judges are elected to terms of six years, but vacancies are filled by an appointment by the governor. There are four divisions of the Court of Appeals, being located in Detroit, Grand Rapids, Lansing, and Marquette. Cases are heard by the Court of Appeals by panels of three judges, who examine the application of the law and not the facts of the case, unless there has been grievous error pertaining to questions of fact. The Michigan Supreme Court consists of seven members who are elected on non-partisan ballots for staggered eight-year terms. The Supreme Court has original jurisdiction only in narrow circumstances, but holds appellate jurisdiction over the entire state judicial system.		Michigan has had four constitutions, the first of which was ratified on October 5 and 6, 1835.[34] There were also constitutions from 1850 and 1908, in addition to the current constitution from 1963. The current document has a preamble, 11 articles, and one section consisting of a schedule and temporary provisions. Michigan, like every U.S. state except Louisiana, has a common law legal system.		Voters in the state elect candidates from both major parties. Economic issues are important in Michigan elections.		The three-term Republican Governor John Engler (1991–2003) preceded the former two-term Democratic Governor Jennifer Granholm (2003–2011). The state has elected successive Republican attorneys general twice since 2003. The Republican Party currently holds a majority in both the House and Senate of the Michigan Legislature. Michigan supported the election of Republican Presidents Ronald Reagan, George H. W. Bush, and Donald Trump. The current Governor Rick Snyder (2011–present) is a Republican.		In contrast, the state supported Democratic candidates in each presidential election from 1992 to 2012. In 2012, Barack Obama carried the state over Mitt Romney, winning Michigan's 17 electoral votes with 54% of the vote. Michigan's two U.S. Senators are both Democrats, while Republicans hold nine of the state's fourteen US House seats. Michigan's current senior U.S. Senator Debbie Stabenow, a Democrat, has served since 2001 after narrowly beating former Republican U.S. Senator Spencer Abraham in the 2000 elections. Democratic U.S. Senator Gary Peters was elected in 2014, beating former Republican Michigan Secretary of State Terri Lynn Land. Congressman Fred Upton, a Republican, serves as Chairman of the US House Committee on Energy and Commerce. Congresswoman Debbie Dingell, a Democrat, became the first person to succeed a living spouse when she replaced former Dean of the House of Representatives John Dingell in 2015.		Republican strongholds of the state include rural areas of Western and Northern Michigan, the Grand Rapids metropolitan area, and Livingston County. Areas of Democratic strength include Wayne County, home to Detroit, Washtenaw County (Ann Arbor), Ingham County (Lansing), and Genesee County (Flint). Much of suburban Detroit—which includes parts of Oakland, Macomb, and Wayne counties—is politically competitive between the two parties.		Historically, the first county-level meeting of the Republican Party took place in Jackson on July 6, 1854,[35] and the party thereafter dominated Michigan until the Great Depression. In the 1912 election, Michigan was one of the six states to support progressive Republican and third-party candidate Theodore Roosevelt for president after he lost the Republican nomination to William Howard Taft.		Michigan remained fairly reliably Republican at the presidential level for much of the 20th century. It was part of Greater New England, the northern tier of states settled chiefly by migrants from New England who carried their culture with them. The state was one of only a handful to back Wendell Willkie over Franklin Roosevelt in 1940, and supported Thomas E. Dewey in his losing bid against Harry S. Truman in 1948. Michigan went to the Democrats in presidential elections during the 1960s, and voted for the Republican candidate in every election from 1972 to 1988. Between 1992 and 2012 it supported the Democrats; early on in 2016, it was pegged as a swing state, and was won very narrowly by the G.O.P. candidate, Donald Trump.		Michigan was the home of Gerald Ford, the 38th President of the United States. He was born in Nebraska and moved as an infant to Grand Rapids and grew up there.[36][37] The Gerald R. Ford Museum is located in Grand Rapids, and the Gerald R. Ford Presidential Library is located on the campus of his alma mater, the University of Michigan in Ann Arbor.		In 1846 Michigan became the first state in the Union, as well as the first English-speaking government in the world,[38][39] to abolish the death penalty. Historian David Chardavoyne has suggested that the movement to abolish capital punishment in Michigan grew as a result of enmity toward the state's neighbor, Canada. Under British rule, it made public executions a regular practice.		Michigan has recognized and performed same-sex marriages since June 26, 2015, following the Supreme Court ruling in Obergefell v. Hodges.[40] Previously, such unions were prohibited under a 2004 state constitutional amendment.[41]		Michigan has approved plans to expand Medicaid coverage in 2014 to adults with incomes up to 133% of the federal poverty level (approximately $15,500 for a single adult in 2014).[42]		State government is decentralized among three tiers—statewide, county and township. Counties are administrative divisions of the state, and townships are administrative divisions of a county. Both of them exercise state government authority, localized to meet the particular needs of their jurisdictions, as provided by state law. There are 83 counties in Michigan.		Cities, state universities, and villages are vested with home rule powers of varying degrees. Home rule cities can generally do anything that is not prohibited by law. The fifteen state universities have broad power and can do anything within the parameters of their status as educational institutions that is not prohibited by the state constitution. Villages, by contrast, have limited home rule and are not completely autonomous from the county and township in which they are located.		There are two types of township in Michigan: general law township and charter. Charter township status was created by the Legislature in 1947 and grants additional powers and stream-lined administration in order to provide greater protection against annexation by a city. As of April 2001, there were 127 charter townships in Michigan. In general, charter townships have many of the same powers as a city but without the same level of obligations. For example, a charter township can have its own fire department, water and sewer department, police department, and so on—just like a city—but it is not required to have those things, whereas cities must provide those services. Charter townships can opt to use county-wide services instead, such as deputies from the county sheriff's office instead of a home-based force of ordinance officers.		Michigan consists of two peninsulas that lie between 82°30' to about 90°30' west longitude, and are separated by the Straits of Mackinac. The 45th parallel north runs through the state—marked by highway signs and the Polar-Equator Trail[43]—along a line including Mission Point Light near Traverse City, the towns of Gaylord and Alpena in the Lower Peninsula and Menominee in the Upper Peninsula. With the exception of two small areas that are drained by the Mississippi River by way of the Wisconsin River in the Upper Peninsula and by way of the Kankakee-Illinois River in the Lower Peninsula, Michigan is drained by the Great Lakes-St. Lawrence watershed and is the only state with the majority of its land thus drained.		The Great Lakes that border Michigan from east to west are Lake Erie, Lake Huron, Lake Michigan and Lake Superior. It has more public golf courses, registered boats, and lighthouses than any other state. The state is bounded on the south by the states of Ohio and Indiana, sharing land and water boundaries with both. Michigan's western boundaries are almost entirely water boundaries, from south to north, with Illinois and Wisconsin in Lake Michigan; then a land boundary with Wisconsin and the Upper Peninsula, that is principally demarcated by the Menominee and Montreal Rivers; then water boundaries again, in Lake Superior, with Wisconsin and Minnesota to the west, capped around by the Canadian province of Ontario to the north and east.		The heavily forested Upper Peninsula is relatively mountainous in the west. The Porcupine Mountains, which are part of one of the oldest mountain chains in the world,[44] rise to an altitude of almost 2,000 feet (610 m) above sea level and form the watershed between the streams flowing into Lake Superior and Lake Michigan. The surface on either side of this range is rugged. The state's highest point, in the Huron Mountains northwest of Marquette, is Mount Arvon at 1,979 feet (603 m). The peninsula is as large as Connecticut, Delaware, Massachusetts, and Rhode Island combined but has fewer than 330,000 inhabitants. They are sometimes called "Yoopers" (from "U.P.'ers"), and their speech (the "Yooper dialect") has been heavily influenced by the numerous Scandinavian and Canadian immigrants who settled the area during the lumbering and mining boom of the late 19th century.		The Lower Peninsula is shaped like a mitten and many residents hold up a hand to depict where they are from.[45] It is 277 miles (446 km) long from north to south and 195 miles (314 km) from east to west and occupies nearly two-thirds of the state's land area. The surface of the peninsula is generally level, broken by conical hills and glacial moraines usually not more than a few hundred feet tall. It is divided by a low water divide running north and south. The larger portion of the state is on the west of this and gradually slopes toward Lake Michigan. The highest point in the Lower Peninsula is either Briar Hill at 1,705 feet (520 m), or one of several points nearby in the vicinity of Cadillac. The lowest point is the surface of Lake Erie at 571 feet (174 m).		The geographic orientation of Michigan's peninsulas makes for a long distance between the ends of the state. Ironwood, in the far western Upper Peninsula, lies 630 highway miles (1,015 km) from Lambertville in the Lower Peninsula's southeastern corner. The geographic isolation of the Upper Peninsula from Michigan's political and population centers makes the U.P. culturally and economically distinct. Occasionally U.P. residents have called for secession from Michigan and establishment as a new state to be called "Superior".		A feature of Michigan that gives it the distinct shape of a mitten is the Thumb. This peninsula projects out into Lake Huron and the Saginaw Bay. The geography of the Thumb is mainly flat with a few rolling hills. Other peninsulas of Michigan include the Keweenaw Peninsula, making up the Copper Country region of the state. The Leelanau Peninsula lies in the Northern Lower Michigan region. See Also Michigan Regions		Numerous lakes and marshes mark both peninsulas, and the coast is much indented. Keweenaw Bay, Whitefish Bay, and the Big and Little Bays De Noc are the principal indentations on the Upper Peninsula. The Grand and Little Traverse, Thunder, and Saginaw bays indent the Lower Peninsula. Michigan has the second longest shoreline of any state—3,288 miles (5,292 km),[46] including 1,056 miles (1,699 km) of island shoreline.[47]		The state has numerous large islands, the principal ones being the North Manitou and South Manitou, Beaver, and Fox groups in Lake Michigan; Isle Royale and Grande Isle in Lake Superior; Marquette, Bois Blanc, and Mackinac islands in Lake Huron; and Neebish, Sugar, and Drummond islands in St. Mary's River. Michigan has about 150 lighthouses, the most of any U.S. state. The first lighthouses in Michigan were built between 1818 and 1822. They were built to project light at night and to serve as a landmark during the day to safely guide the passenger ships and freighters traveling the Great Lakes. See Lighthouses in the United States.		The state's rivers are generally small, short and shallow, and few are navigable. The principal ones include the Detroit River, St. Marys River, and St. Clair River which connect the Great Lakes; the Au Sable, Cheboygan, and Saginaw, which flow into Lake Huron; the Ontonagon, and Tahquamenon, which flow into Lake Superior; and the St. Joseph, Kalamazoo, Grand, Muskegon, Manistee, and Escanaba, which flow into Lake Michigan. The state has 11,037 inland lakes—totaling 1,305 square miles (3,380 km2) of inland water—in addition to 38,575 square miles (99,910 km2) of Great Lakes waters. No point in Michigan is more than six miles (10 km) from an inland lake or more than 85 miles (137 km) from one of the Great Lakes.[48]		The state is home to a number of areas maintained by the National Park Service including: Isle Royale National Park, located in Lake Superior, about 30 miles (48 km) southeast of Thunder Bay, Ontario. Other national protected areas in the state include: Keweenaw National Historical Park, Pictured Rocks National Lakeshore, Sleeping Bear Dunes National Lakeshore, Huron National Forest, Manistee National Forest, Hiawatha National Forest, Ottawa National Forest and Father Marquette National Memorial. The largest section of the North Country National Scenic Trail passes through Michigan.		With 78 state parks, 19 state recreation areas, and 6 state forests, Michigan has the largest state park and state forest system of any state. These parks and forests include Holland State Park, Mackinac Island State Park, Au Sable State Forest, and Mackinaw State Forest.		Michigan has a continental climate, although there are two distinct regions. The southern and central parts of the Lower Peninsula (south of Saginaw Bay and from the Grand Rapids area southward) have a warmer climate (Köppen climate classification Dfa) with hot summers and cold winters. The northern part of Lower Peninsula and the entire Upper Peninsula has a more severe climate (Köppen Dfb), with warm, but shorter summers and longer, cold to very cold winters. Some parts of the state average high temperatures below freezing from December through February, and into early March in the far northern parts. During the winter through the middle of February the state is frequently subjected to heavy lake-effect snow. The state averages from 30–40 inches (76–102 cm) of precipitation annually, however some areas in the northern lower peninsula and the upper peninsula average almost 160" of snowfall per year.[51] Michigan's highest recorded temperature is 112 °F (44 °C) at Mio on July 13, 1936, and the coldest recorded temperature is −51 °F (−46 °C) at Vanderbilt on February 9, 1934.[52]		The entire state averages 30 days of thunderstorm activity per year. These can be severe, especially in the southern part of the state. The state averages 17 tornadoes per year, which are more common in the extreme southern portion of the state. Portions of the southern border have been almost as vulnerable historically as states further west and in Tornado Alley. For this reason, many communities in the very southern portions of the state are equipped with tornado sirens to warn residents of approaching tornadoes. Farther north, in Central Michigan, Northern Michigan, and the Upper Peninsula, tornadoes are rare.[53][54]		The geological formation of the state is greatly varied, with the Michigan Basin being the most major formation. Primary boulders are found over the entire surface of the Upper Peninsula (being principally of primitive origin), while Secondary deposits cover the entire Lower Peninsula. The Upper Peninsula exhibits Lower Silurian sandstones, limestones, copper and iron bearing rocks, corresponding to the Huronian system of Canada. The central portion of the Lower Peninsula contains coal measures and rocks of the Pennsylvanian period. Devonian and sub-Carboniferous deposits are scattered over the entire state.		Michigan rarely experiences earthquakes, thus far mostly smaller ones that do not cause significant damage. A 4.6-magnitude earthquake struck in August 1947. More recently, a 4.2-magnitude earthquake occurred on Saturday, May 2, 2015, shortly after noon, about 5 miles south of Galesburg, Michigan (9 miles southeast of Kalamazoo) in central Michigan, about 140 miles west of Detroit, according to the Colorado-based U.S. Geological Survey's National Earthquake Information Center. No major damage or injuries were reported, according to Governor Rick Snyder's office.[55]		The United States Census Bureau estimates that the population of Michigan was 9,928,300 on July 1, 2016, an increase of 0.45% from 9,883,635 recorded at the 2010 United States Census.[58]		The center of population of Michigan is located in Shiawassee County, in the southeastern corner of the civil township of Bennington, which is located northwest of the village of Morrice.[59]		As of the 2010 American Community Survey for the U.S. Census, the state had a foreign-born population of 592,212, or 6.0% of the total. Michigan has the largest Dutch, Finnish, and Macedonian populations in the United States.		The 2010 Census reported:		In the same year Hispanics or Latinos (of any race) made up 4.4% of the population.		The large majority of Michigan's population is Caucasian. Americans of European descent live throughout Michigan and most of Metro Detroit. Large European American groups include those of German, British, Irish, Polish and Belgian ancestry. People of Scandinavian descent, and those of Finnish ancestry, have a notable presence in the Upper Peninsula. Western Michigan is known for the Dutch heritage of many residents (the highest concentration of any state), especially in Holland and metropolitan Grand Rapids.		African-Americans, who came to Detroit and other northern cities in the Great Migration of the early 20th century, form a majority of the population of the city of Detroit and of other cities, including Flint and Benton Harbor.		As of 2007 about 300,000 people in Southeastern Michigan trace their descent from the Middle East.[64] Dearborn has a sizeable Arab community, with many Assyrian/Chaldean/Syriac, and Lebanese who immigrated for jobs in the auto industry in the 1920s along with more recent Yemenis and Iraqis.[65]		As of 2007, almost 8,000 Hmong people lived in the State of Michigan, about double their 1999 presence in the state.[66] As of 2007 most lived in northeastern Detroit, but they had been increasingly moving to Pontiac and Warren.[67] By 2015 the number of Hmong in the Detroit city limits had significantly declined.[68] Lansing hosts a statewide Hmong New Year Festival.[67] The Hmong community also had a prominent portrayal in the 2008 film Gran Torino, which was set in Detroit.		As of 2015, 80% of Michigan's Japanese population lived in the counties of Macomb, Oakland, Washtenaw, and Wayne in the Detroit and Ann Arbor areas.[69] As of April 2013, the largest Japanese national population is in Novi, with 2,666 Japanese residents, and the next largest populations are respectively in Ann Arbor, West Bloomfield Township, Farmington Hills, and Battle Creek. The state has 481 Japanese employment facilities providing 35,554 local jobs. 391 of them are in Southeast Michigan, providing 20,816 jobs, and the 90 in other regions in the state provide 14,738 jobs. The Japanese Direct Investment Survey of the Consulate-General of Japan, Detroit stated that over 2,208 additional Japanese residents were employed in the State of Michigan as of October 1, 2012, than in 2011.[70] During the 1990s the Japanese population of Michigan experienced an increase, and many Japanese people with children moved to particular areas for their proximity to Japanese grocery stores and high-performing schools.[69]		A person from Michigan is called a Michigander or Michiganian;[71] also at times, but rarely, a "Michiganite".[72] Residents of the Upper Peninsula are sometimes referred to as "Yoopers" (a phonetic pronunciation of "U.P.ers"), and Upper Peninsula residents sometimes refer to those from the Lower Peninsula as "trolls" because they live below the bridge.[73]		As of 2011, 34.3% of Michigan's children under the age of one belonged to racial or ethnic minority groups, meaning that they had at least one parent who was not non-Hispanic white.[74]		Note: Births in table don't add up, because Hispanics are counted both by their ethnicity and by their race, giving a higher overall number.		As of 2010, 91.11% (8,507,947) of Michigan residents age 5 and older spoke English at home as a primary language, while 2.93% (273,981) spoke Spanish, 1.04% (97,559) Arabic, 0.44% (41,189) German, 0.36% (33,648) Chinese (which includes Mandarin), 0.31% (28,891) French, 0.29% (27,019) Polish, and Syriac languages (such as Modern Aramaic and Northeastern Neo-Aramaic) was spoken as a main language by 0.25% (23,420) of the population over the age of five. In total, 8.89% (830,281) of Michigan's population age 5 and older spoke a mother language other than English.[78]		The Roman Catholic Church has six dioceses and one archdiocese in Michigan; Gaylord, Grand Rapids, Kalamazoo, Lansing, Marquette, Saginaw and Detroit.[80] The Roman Catholic Church is the largest denomination by number of adherents, according to the Association of Religion Data Archives (ARDA) 2010 survey, with 1,717,296 adherents.[81] The Roman Catholic Church was the only organized religion in Michigan until the 19th century, reflecting the territory's French colonial roots. Detroit's Saint Anne's parish, established in 1701 by Antoine de la Mothe Cadillac, is the second-oldest Roman Catholic parish in the United States.[82] On March 8, 1833, the Holy See formally established a diocese in the Michigan territory, which included all of Michigan, Wisconsin, Minnesota, and the Dakotas east of the Mississippi River. When Michigan became a state in 1837, the boundary of the Diocese of Detroit was redrawn to coincide with that of the State; the other dioceses were later carved out from the Diocese of Detroit but remain part of the Ecclesiastical Province of Detroit.[83]		In 2010, the largest Protestant denominations were the United Methodist Church with 228,521 adherents; followed by the Lutheran Church–Missouri Synod with 219,618; and the Evangelical Lutheran Church in America with 120,598 adherents. The Christian Reformed Church in North America had almost 100,000 members and over 230 congregations in Michigan.[84] The Reformed Church in America had 76,000 members and 154 congregations in the state.[85] In the same survey, Jewish adherents in the state of Michigan were estimated at 44,382, and Muslims at 120,351.[86] The Lutheran Church was introduced by German and Scandinavian immigrants; Lutheranism is the second largest religious denomination in the state. The first Jewish synagogue in the state was Temple Beth El, founded by twelve German Jewish families in Detroit in 1850.[87] In West Michigan, Dutch immigrants fled from the specter of religious persecution and famine in the Netherlands around 1850 and settled in and around what is now Holland, Michigan, establishing a "colony" on American soil that fervently held onto Calvinist doctrine that established a significant presence of Reformed churches.[88] Islam was introduced by immigrants from the Near East during the 20th century.[89] It is also home to the largest mosque in North America, the Islamic Center of America in Dearborn. Battle Creek, Michigan is also the birthplace of the Seventh-day Adventist Church, which was founded on May 21, 1863.[90][91]		The U.S. Economic Development Administration estimated Michigan's 2014 gross state product to be $417.306 billion, ranking 13th out of the 50 states.[92] According to the Bureau of Labor Statistics, as of October 2015, the state's seasonally adjusted unemployment rate is estimated at 5.0%.[93][94]		Source: Fortune[95]		Products and services include automobiles, food products, information technology, aerospace, military equipment, furniture, and mining of copper and iron ore. Michigan is the third leading grower of Christmas trees with 60,520 acres (245 km2) of land dedicated to Christmas tree farming.[96][97] The beverage Vernors was invented in Michigan in 1866, sharing the title of oldest soft drink with Hires Root Beer. Faygo was founded in Detroit on November 4, 1907. Two of the top four pizza chains were founded in Michigan and are headquartered there: Domino's Pizza by Tom Monaghan and Little Caesars Pizza by Mike Ilitch. Michigan became the 24th Right to Work state in U.S. in 2012.		Since 2009, GM, Ford and Chrysler have managed a significant reorganization of their benefit funds structure after a volatile stock market which followed the September 11 attacks and early 2000s recession impacted their respective U.S. pension and benefit funds (OPEB).[98][99] General Motors, Ford, and Chrysler reached agreements with the United Auto Workers Union to transfer the liabilities for their respective health care and benefit funds to a 501(c)(9) Voluntary Employee Beneficiary Association (VEBA). Manufacturing in the state grew 6.6% from 2001 to 2006,[100] but the high speculative price of oil became a factor for the U.S. auto industry during the economic crisis of 2008 impacting industry revenues. In 2009, GM and Chrysler emerged from Chapter 11 restructurings with financing provided in part by the U.S. and Canadian governments.[101][102] GM began its initial public offering (IPO) of stock in 2010.[103] For 2010, the Big Three domestic automakers have reported significant profits indicating the beginning of rebound.[104][105][106][107]		As of 2002[update], Michigan ranked fourth in the U.S. in high tech employment with 568,000 high tech workers, which includes 70,000 in the automotive industry.[108] Michigan typically ranks third or fourth in overall Research & development (R&D) expenditures in the United States.[28][29] Its research and development, which includes automotive, comprises a higher percentage of the state's overall gross domestic product than for any other U.S. state.[109] The state is an important source of engineering job opportunities. The domestic auto industry accounts directly and indirectly for one of every ten jobs in the U.S.[110]		Michigan was second in the U.S. in 2004 for new corporate facilities and expansions. From 1997 to 2004, Michigan was the only state to top the 10,000 mark for the number of major new developments;[26][111] however, the effects of the late 2000s recession have slowed the state's economy. In 2008, Michigan placed third in a site selection survey among the states for luring new business which measured capital investment and new job creation per one million population.[112] In August 2009, Michigan and Detroit's auto industry received $1.36 B in grants from the U.S. Department of Energy for the manufacture of electric vehicle technologies which is expected to generate 6,800 immediate jobs and employ 40,000 in the state by 2020.[113] From 2007 to 2009, Michigan ranked 3rd in the U.S. for new corporate facilities and expansions.[114][115]		As leading research institutions, the University of Michigan, Michigan State University, and Wayne State University are important partners in the state's economy and its University Research Corridor.[30] Michigan's public universities attract more than $1.5 B in research and development grants each year.[31] The National Superconducting Cyclotron Laboratory is located at Michigan State University. Michigan's workforce is well-educated and highly skilled, making it attractive to companies. It has the third highest number of engineering graduates nationally.[116]		Detroit Metropolitan Airport is one of the nation's most recently expanded and modernized airports with six major runways, and large aircraft maintenance facilities capable of servicing and repairing a Boeing 747 and is a major hub for Delta Air Lines. Michigan's schools and colleges rank among the nation's best. The state has maintained its early commitment to public education. The state's infrastructure gives it a competitive edge; Michigan has 38 deep water ports.[117] In 2007, Bank of America announced that it would commit $25 billion to community development in Michigan following its acquisition of LaSalle Bank in Troy.[118]		Michigan led the nation in job creation improvement in 2010.[119]		Michigan's personal income tax is set to a flat rate of 4.25%. In addition, 22 cities impose income taxes; rates are set at 1% for residents and 0.5% for non-residents in all but four cities.[120] Michigan's state sales tax is 6%, though items such as food and medication are exempted from sales tax. Property taxes are assessed on the local level, but every property owner's local assessment contributes six mills (a rate of $6 per $1000 of property value) to the statutory State Education Tax. Property taxes are appealable to local boards of review and need the approval of the local electorate to exceed millage rates prescribed by state law and local charters. In 2011, the state repealed its business tax and replaced it with a 6% corporate income tax which substantially reduced taxes on business.[121][122] Article IX of the Constitution of the State of Michigan also provides limitations on how much the state can tax.		The state also levies a 6% sales tax within the state and a Use tax on goods purchased outside the state (that are brought in and used in state).[123] The use tax applies to internet sales/purchases from outside Michigan, and is equivalent to the sales tax.[124]		A wide variety of commodity crops, fruits, and vegetables are grown in Michigan, making it second only to California among U.S. states in the diversity of its agriculture.[125] The state has 54,800 farms utilizing 10,000,000 acres (40,000 km2) of land which sold $6.49 billion worth of products in 2010.[126] The most valuable agricultural product is milk. Leading crops include corn, soybeans, flowers, wheat, sugar beets and potatoes. Livestock in the state included 1 million cattle, 1 million hogs, 78,000 sheep and over 3 million chickens. Livestock products accounted for 38% of the value of agricultural products while crops accounted for the majority.		Michigan is a leading grower of fruit in the U.S., including blueberries, tart cherries, apples, grapes, and peaches.[32][127] Plums, pears, and strawberries are also grown. These fruits are mainly grown in West Michigan due to the moderating effect of Lake Michigan on the climate. There is also significant fruit production, especially cherries, but also grapes, apples, and other fruits, in Northwest Michigan along Lake Michigan. Michigan produces wines, beers and a multitude of processed food products. Kellogg's cereal is based in Battle Creek, Michigan and processes many locally grown foods. Thornapple Valley, Ball Park Franks, Koegel Meat Company, and Hebrew National sausage companies are all based in Michigan.		Michigan is home to very fertile land in the Saginaw Valley and "Thumb" areas. Products grown there include corn, sugar beets, navy beans, and soy beans. Sugar beet harvesting usually begins the first of October. It takes the sugar factories about five months to process the 3.7 million tons of sugarbeets into 485,000 tons of pure, white sugar.[128] Michigan's largest sugar refiner, Michigan Sugar Company[129] is the largest east of the Mississippi River and the fourth largest in the nation. Michigan Sugar brand names are Pioneer Sugar and the newly incorporated Big Chief Sugar. Potatoes are grown in Northern Michigan, and corn is dominant in Central Michigan. Alfalfa, cucumbers, and asparagus are also grown.		Michigan's tourists spend $17.2 billion per year in the state, supporting 193,000 tourism jobs.[130] Michigan's tourism website ranks among the busiest in the nation.[131] Destinations draw vacationers, hunters, and nature enthusiasts from across the United States and Canada. Michigan is fifty percent forest land, much of it quite remote. The forests, lakes and thousands of miles of beaches are top attractions. Event tourism draws large numbers to occasions like the Tulip Time Festival and the National Cherry Festival. In 2006, the Michigan State Board of Education mandated that all public schools in the state hold their first day of school after the Labor Day holiday, in accordance with the new Post Labor Day School law. A survey found that 70% of all tourism business comes directly from Michigan residents, and the Michigan Hotel, Motel, & Resort Association claimed that the shorter summer in between school years cut into the annual tourism season in the state.[132]		Tourism in metropolitan Detroit draws visitors to leading attractions, especially The Henry Ford, the Detroit Institute of Arts, the Detroit Zoo, and to sports in Detroit. Other museums include the Detroit Historical Museum, the Charles H. Wright Museum of African American History, museums in the Cranbrook Educational Community, and the Arab American National Museum. The metro area offers four major casinos, MGM Grand Detroit, Greektown, Motor City, and Caesars Windsor in Windsor, Ontario, Canada; moreover, Detroit is the largest American city and metropolitan region to offer casino resorts.[133]		Hunting and fishing are significant industries in the state. Charter boats are based in many Great Lakes cities to fish for salmon, trout, walleye and perch. Michigan ranks first in the nation in licensed hunters (over one million) who contribute $2 billion annually to its economy. Over three-quarters of a million hunters participate in white-tailed deer season alone. Many school districts in rural areas of Michigan cancel school on the opening day of firearm deer season, because of attendance concerns.		Michigan's Department of Natural Resources manages the largest dedicated state forest system in the nation. The forest products industry and recreational users contribute $12 billion and 200,000 associated jobs annually to the state's economy. Public hiking and hunting access has also been secured in extensive commercial forests. The state has the highest number of golf courses and registered snowmobiles in the nation.[134]		The state has numerous historical markers, which can themselves become the center of a tour.[135] The Great Lakes Circle Tour is a designated scenic road system connecting all of the Great Lakes and the St. Lawrence River.[136]		With its position in relation to the Great Lakes and the countless ships that have foundered over the many years in which they have been used as a transport route for people and bulk cargo, Michigan is a world-class scuba diving destination. The Michigan Underwater Preserves are 11 underwater areas where wrecks are protected for the benefit of sport divers.		Michigan has nine international road crossings with Ontario, Canada:		A second international bridge is currently under consideration between Detroit and Windsor.[137]		Michigan is served by four Class I railroads: the Canadian National Railway, the Canadian Pacific Railway, CSX Transportation, and the Norfolk Southern Railway. These are augmented by several dozen short line railroads. The vast majority of rail service in Michigan is devoted to freight, with Amtrak and various scenic railroads the exceptions.[138]		Amtrak passenger rail services the state, connecting many southern and western Michigan cities to Chicago, Illinois. There are plans for commuter rail for Detroit and its suburbs (see SEMCOG Commuter Rail).[139][140][141]		The Detroit Metropolitan Wayne County Airport, located in the western suburb of Romulus, was in 2010 the 16th busiest airfield in North America measured by passenger traffic.[142] The Gerald R. Ford International Airport in Grand Rapids is the next busiest airport in the state, served by eight airlines to 23 destinations. Flint Bishop International Airport is the third largest airport in the state, served by four airlines to several primary hubs. Smaller regional and local airports are located throughout the state including on several islands. Cherry Capital Airport is located in Traverse City.		Other economically significant cities include:		Half of the wealthiest communities in the state are located in Oakland County, just north of Detroit. Another wealthy community is located just east of the city, in Grosse Pointe. Only three of these cities are located outside of Metro Detroit. The city of Detroit itself, with a per capita income of $14,717, ranks 517th on the list of Michigan locations by per capita income. Benton Harbor is the poorest city in Michigan, with a per capita income of $8,965, while Barton Hills is the richest with a per capita income of $110,683.		Michigan's education system provides services to 1.6 million K-12 students in public schools. More than 124,000 students attend private schools and an uncounted number are home-schooled under certain legal requirements.[144][145] The public school system has a $14.5 billion budget in 2008–2009.[146] Michigan has a number of public universities spread throughout the state and numerous private colleges as well. Michigan State University has the eighth largest campus population of any U.S. school. Seven of the state's universities—Central Michigan University, University of Michigan, Michigan State University, Michigan Technological University, Oakland University, Wayne State University, and Western Michigan University—are classified as research universities by the Carnegie Foundation.[147]		Michigan's major-league sports teams include: Detroit Tigers baseball team, Detroit Lions football team, Detroit Red Wings ice hockey team, and the Detroit Pistons men's basketball team. All of Michigan's major league teams play in the Metro Detroit area.		The Pistons played at Detroit's Cobo Arena until 1978 and at the Pontiac Silverdome until 1988 when they moved into The Palace of Auburn Hills. The Detroit Lions played at Tiger Stadium in Detroit until 1974, then moved to the Pontiac Silverdome where they played for 27 years between 1975 and 2002 before moving to Ford Field in Detroit in 2002. The Detroit Tigers played at Tiger Stadium (formerly known as Navin Field and Briggs Stadium) from 1912 to 1999. In 2000 they moved to Comerica Park. The Red Wings played at Olympia Stadium before moving to Joe Louis Arena in 1979. Professional hockey got its start in Houghton,[148] when the Portage Lakers were formed.[citation needed]		The Michigan International Speedway is the site of NASCAR races and Detroit was formerly the site of a Formula One World Championship Grand Prix race. From 1959 to 1961, Detroit Dragway hosted the NHRA's U.S. Nationals.[149] Michigan is home to one of the major canoeing marathons: the 120-mile (190 km) Au Sable River Canoe Marathon. The Port Huron to Mackinac Boat Race is also a favorite.		Twenty-time Grand Slam champion Serena Williams was born in Saginaw. The 2011 World Champion for Women's Artistic Gymnastics, Jordyn Wieber is from DeWitt. Wieber was also a member of the gold medal winning team at the London Olympics in 2012.		Collegiate sports in Michigan are popular in addition to professional sports. The state's two largest athletic programs are the Michigan Wolverines and Michigan State Spartans, which play in the NCAA Big Ten Conference. Michigan Stadium in Ann Arbor, home to the Michigan Wolverines football team, is the largest stadium in the Western Hemisphere and the second-largest stadium worldwide behind Rungrado May Day Stadium in Pyongyang, North Korea.		Michigan is, by tradition, known as "The Wolverine State," and the University of Michigan takes the wolverine as its mascot. The association is well and long established: for example, many Detroiters volunteered to fight during the American Civil War and George Armstrong Custer, who led the Michigan Brigade, called them the "Wolverines". The origins of this association are obscure; it may derive from a busy trade in wolverine furs in Sault Ste. Marie in the 18th century or may recall a disparagement intended to compare early settlers in Michigan with the vicious mammal. Wolverines are, however, extremely rare in Michigan. A sighting in February 2004 near Ubly was the first confirmed sighting in Michigan in 200 years.[150] The animal was found dead in 2010.[151]		Coordinates: 44°N 85°W﻿ / ﻿44°N 85°W﻿ / 44; -85		
Toby Radloff (born December 12, 1957) is a former file clerk who became a minor celebrity owing to his appearances in Cleveland writer Harvey Pekar's autobiographical comic book American Splendor.		Radloff has a distinctive manner of speech and quirky mannerisms. He is a self-proclaimed "Genuine Nerd," and is proud of it. He publicly came out as gay after the release of the American Splendor film in 2003.[1]		Radloff met Pekar in 1980 when Radloff was hired at the VA Hospital, and soon became a recurring character in American Splendor. Radloff starred in such stories as "Lentils and Lent" (illustrated by Sean Carroll), "Double Feature Part 1: Footloose, Starring Toby Radloff" and "Double Feature Part 2: Revenge of the Nerds", (both illustrated by Bill Knapp), "Toby Saves the Day," (illustrated by Ed Wesolowski), and "Hollywood Reporter" and "Reduction" (both illustrated by Josh Neufeld).[2]		In the 2003 film American Splendor, the adaptation of Pekar's comic, Radloff is played by Judah Friedlander, and also appears as himself in the documentary sections of the film.[3]		On August 3, 2006, Radloff tried out to be the next "Wack Packer" on The Howard Stern Show, but lost in the finals because Stern and crew felt that he was too famous.[4][5]		Radloff is currently hosting "Cheapskate Theater", produced by filmmaker Wayne Alan Harold, the creator of Radloff's original MTV segments. The first installment's movie is the science-fiction horror classic Attack of the Giant Leeches.[6]		
		Shyness (also called diffidence) is the feeling of apprehension, lack of comfort, or awkwardness especially when a person is around other people. This commonly occurs in new situations or with unfamiliar people. Shyness can be a characteristic of people who have low self-esteem. Stronger forms of shyness are usually referred to as social anxiety or social phobia. The primary defining characteristic of shyness is a largely ego-driven fear of what other people will think of a person's behavior. This results in a person becoming scared of doing or saying what they want to out of fear of negative reactions, being laughed at, humiliated or patronised, criticism and/or rejection. A shy person may simply opt to avoid social situations instead.[1]		One important aspect of shyness is social skills development. Schools and parents may implicitly assume children are fully capable of effective social interaction. Social skills training is not given any priority (unlike reading and writing) and as a result, shy students are not given an opportunity to develop their ability to participate in class and interact with peers. Teachers can model social skills and ask questions in a less direct and intimidating manner in order to gently encourage shy students to speak up in class, and make friends with other children.[2]						The initial cause of shyness varies. Scientists believe that they have located genetic data supporting the hypothesis that shyness is, at least, partially genetic. However, there is also evidence that suggests the environment in which a person is raised can also be responsible for their shyness. This includes child abuse, particularly emotional abuse such as ridicule. Shyness can originate after a person has experienced a physical anxiety reaction; at other times, shyness seems to develop first and then later causes physical symptoms of anxiety. Shyness differs from social anxiety, which is a broader, often depression-related psychological condition including the experience of fear, apprehension or worrying about being evaluated by others in social situations to the extent of inducing panic.		Shyness may come from genetic traits, the environment in which a person is raised and personal experiences. Shyness may be a personality trait or can occur at certain stages of development in children.		Shyness is often seen as a hindrance on people and their development. The cause of shyness is often disputed but it is found that fear is positively related to shyness,[3] suggesting that fearful children are much more likely to develop being shy as opposed to less fearful children. Shyness can also be seen on a biological level as a result of an excess of cortisol. When cortisol is present in greater quantities it is known to suppress an individual’s immune system, making them more susceptible to illness and disease.[4] The genetics of shyness is a relatively small area of research that has been receiving an even smaller amount of attention, although papers on the biological bases of shyness date back to 1988. Some research has indicated that shyness and aggression are related—through long and short forms of the gene DRD4, though considerably more research on this is needed. Further, it has been suggested that shyness and social phobia (the distinction between the two is becoming ever more blurred) are related to obsessive-compulsive disorder. As with other studies of behavioral genetics, the study of shyness is complicated by the number of genes involved in, and the confusion in defining, the phenotype. Naming the phenotype – and translation of terms between genetics and psychology — also causes problems.		Several genetic links to shyness are current areas of research. One is the serotonin transporter promoter region polymorphism (5-HTTLPR), the long form of which has been shown to be modestly correlated with shyness in grade school children.[5] Previous studies had shown a connection between this form of the gene and both obsessive-compulsive disorder and autism.[6] Mouse models have also been used, to derive genes suitable for further study in humans; one such gene, the glutamic acid decarboxylase gene (which encodes an enzyme that functions in GABA synthesis), has so far been shown to have some association with behavioral inhibition.[7]		Another gene, the dopamine D4 receptor gene (DRD4) exon III polymorphism, had been the subject of studies in both shyness and aggression, and is currently the subject of studies on the "novelty seeking" trait. A 1996 study of anxiety-related traits (shyness being one of these) remarked that, "Although twin studies have indicated that individual variation in measures of anxiety-related personality traits is 40-60% heritable, none of the relevant genes has yet been identified," and that "10 to 15 genes might be predicted to be involved" in the anxiety trait. Progress has been made since then, especially in identifying other potential genes involved in personality traits, but there has been little progress made towards confirming these relationships.[8] The long version of the 5-HTT gene-linked polymorphic region (5-HTTLPR) is now postulated to be correlated with shyness,[5] but in the 1996 study, the short version was shown to be related to anxiety-based traits.		Excessive shyness, embarrassment, self-consciousness and timidity, social-phobia and lack of self-confidence are also components of erethism, which is a symptom complex that appears in cases of mercury poisoning.[9][10] Mercury poisoning was common among hat makers in England in the 18th and 19th centuries, who used mercury to stabilize wool into felt fabric.		The prevalence of shyness in some children can be linked to day length during pregnancy, particularly during the midpoint of prenatal development.[11] An analysis of longitudinal data from children living at specific latitudes in the United States and New Zealand revealed a significant relationship between hours of day length during the midpoint of pregnancy and the prevalence of shyness in children. "The odds of being classified as shy were 1.52 times greater for children exposed to shorter compared to longer daylengths during gestation."[11] In their analysis, scientists assigned conception dates to the children relative to their known birth dates, which allowed them to obtain random samples from children who had a mid-gestation point during the longest hours of the year and the shortest hours of the year (June and December, depending on whether the cohorts were in the United States or New Zealand).		The longitudinal survey data included measurements of shyness on a five-point scale based on interviews with the families being surveyed, and children in the top 25th percentile of shyness scores were identified. The data revealed a significant co-variance between the children who presented as being consistently shy over a two-year period, and shorter day length during their mid-prenatal development period. "Taken together, these estimates indicate that about one out of five cases of extreme shyness in children can be associated with gestation during months of limited daylength."[11]		In recent years correlations between birth weight and shyness have been studied. Findings suggest that those born at low birth weights are more likely to be shy, risk-aversive and cautious compared to those born at normal birth weights. These results do not however imply a cause-and-effect relationship.[12]		Shyness is most likely to occur during unfamiliar situations, though in severe cases it may hinder an individual in their most familiar situations and relationships as well. Shy people avoid the objects of their apprehension in order to keep from feeling uncomfortable and inept; thus, the situations remain unfamiliar and the shyness perpetuates itself. Shyness may fade with time; e.g., a child who is shy towards strangers may eventually lose this trait when older and become more socially adept. This often occurs by adolescence or young adulthood (generally around the age of 13). In some cases, though, it may become an integrated, lifelong character trait. Longitudinal data suggests that the three different personality types evident in infancy easy, slow-to-warm-up, and difficult tend to change as children mature. Extreme traits become less pronounced, and personalities evolve in predictable patterns over time. What has been proven to remain constant is the tendency to internalize or externalize problems.[13] This relates to individuals with shy personalities because they tend to internalize their problems, or dwell on their problems internally instead of expressing their concerns, which leads to disorders like depression and anxiety.[14] Humans experience shyness to different degrees and in different areas.		Shyness can also be seen as an academic determinant. It has been determined that there is a negative relationship between shyness and classroom performance. As the shyness of an individual increased, classroom performance was seen to decrease.[15]		Shyness may involve the discomfort of difficulty in knowing what to say in social situations, or may include crippling physical manifestations of uneasiness. Shyness usually involves a combination of both symptoms, and may be quite devastating for the sufferer, in many cases leading them to feel that they are boring, or exhibit bizarre behavior in an attempt to create interest, alienating them further. Behavioral traits in social situations such as smiling, easily producing suitable conversational topics, assuming a relaxed posture and making good eye contact, may not be second nature for a shy person. Such people might only affect such traits by great difficulty, or they may even be impossible to display.		Those who are shy are perceived more negatively, in cultures that value sociability, because of the way they act towards others.[16] Shy individuals are often distant during conversations, which can result in others forming poor impressions of them. People who are not shy may be up-front, aggressive, or critical towards shy people in an attempt "to get them out of their shell." This can actually make a shy person feel worse, as it draws attention to them, making them more self-conscious and uncomfortable.		The term shyness may be implemented as a lay blanket-term for a family of related and partially overlapping afflictions, including timidity (apprehension in meeting new people), bashfulness and diffidence (reluctance in asserting oneself), apprehension and anticipation (general fear of potential interaction), or intimidation (relating to the object of fear rather than one's low confidence).[17] Apparent shyness, as perceived by others, may simply be the manifestation of reservation or introversion, character traits which cause an individual to voluntarily avoid excessive social contact or be terse in communication, but are not motivated or accompanied by discomfort, apprehension, or lack of confidence.		Rather, according to professor of psychology Bernardo J. Carducci, introverts choose to avoid social situations because they derive no reward from them or may find surplus sensory input overwhelming, whereas shy people may fear such situations.[18] Research using the statistical techniques of factor analysis and correlation have found shyness overlaps mildly with both introversion and neuroticism (i.e., negative emotionality).[19][20][21] Low societal acceptance of shyness or introversion may reinforce a shy or introverted individual's low self-confidence.[22][page needed]		Both shyness and introversion can outwardly manifest with socially withdrawn behaviors, such as tendencies to avoid social situations, especially when they are unfamiliar. A variety of research suggests that shyness and introversion possess clearly distinct motivational forces and lead to uniquely different personal and peer reactions and therefore cannot be described as theoretically the same,[14][23][24] with Susan Cain's Quiet (2012) further discerning introversion as involving being differently social (preferring one-on-one or small group interactions) rather than being anti-social altogether.[25]		Research suggests that no unique physiological response, such as an increased heart beat, accompanies socially withdrawn behavior in familiar compared with unfamiliar social situations. But unsociability leads to decreased exposure to unfamiliar social situations and shyness causes a lack of response in such situations, suggesting that shyness and unsociability affect two different aspects of sociability and are distinct personality traits.[23] In addition, different cultures perceive unsociability and shyness in different ways, leading to either positive or negative individual feelings of self-esteem. Collectivist cultures view shyness as a more positive trait related to compliance with group ideals and self-control, while perceiving chosen isolation (introverted behavior) negatively as a threat to group harmony; and because collectivist society accepts shyness and rejects unsociability, shy individuals develop higher self-esteem than introverted individuals.[24] On the other hand, individualistic cultures perceive shyness as a weakness and a character flaw, while unsociable personality traits (preference to spend time alone) are accepted because they uphold the value of autonomy; accordingly, shy individuals tend to develop low self-esteem in Western cultures while unsociable individuals develop high self-esteem.[14]		An extreme case of shyness is identified as a psychiatric illness, which made its debut as social phobia in DSM-III in 1980, but was then described as rare.[26][page needed] By 1994, however, when DSM-IV was published, it was given a second, alternative name in parentheses (social anxiety disorder) and was now said to be relatively common, affecting between 3 and 13% of the population at some point during their lifetime.[27][28] Studies examining shy adolescents and university students found that between 12 and 18% of shy individuals meet criteria for social anxiety disorder.[20][29][30]		Shyness affects people mildly in unfamiliar social situations where one feels anxiety about interacting with new people. Social anxiety disorder, on the other hand, is a strong irrational fear of interacting with people, or being in situations which may involve public scrutiny, because one feels overly concerned about being criticized if one embarrasses oneself. Physical symptoms of social phobia can include shortness of breath, trembling, increased heart rate, and sweating; in some cases, these symptoms are intense enough and numerous enough to constitute a panic attack. Shyness, on the other hand, may incorporate many of these symptoms, but at a lower intensity, infrequently, and does not interfere tremendously with normal living.[1]		Those considered shy are also said to be socially inhibited. Social inhibition is the conscious or unconscious constraint by a person of behavior of a social nature. In other words, social inhibition is holding back for social reasons. There are different levels of social inhibition, from mild to severe. Being socially inhibited is good when preventing one from harming another and bad when causing one to refrain from participating in class discussions.		Behavioral inhibition is a temperament or personality style that predisposes a person to become fearful, distressed and withdrawn in novel situations. This personality style is associated with the development of anxiety disorders in adulthood, particularly social anxiety disorder.[31][32]		Many misconceptions/stereotypes about shy individuals exist in western culture and negative peer reactions to "shy" behavior abound. This takes place because individualistic cultures place less value on quietness and meekness in social situations, and more often reward outgoing behaviors. Some misconceptions include viewing introversion and social phobia synonymous with shyness, and believing that shy people are less intelligent.[16][33][34][35]		No correlation (positive or negative) exists between intelligence and shyness.[34] Research indicates that shy children have a harder time expressing their knowledge in social situations (which most modern curricula utilize) and because they do not engage actively in discussions, teachers view them as less intelligent. In line with social learning theory, an unwillingness to engage with classmates and teachers makes it more difficult for shy students to learn. Test scores, however, indicate that shyness is unrelated to actual academic knowledge, and therefore only academic engagement.[33] Depending on the level of a teacher's own shyness, more indirect (vs. socially oriented) strategies are used with shy individuals to assess knowledge in the classroom, and accommodations are made.[34] Observed peer evaluations of shy people during initial meeting and social interactions thereafter found that peers evaluate shy individuals as less intelligent during the first encounter. During subsequent interactions, however, peers perceived shy individuals' intelligence more positively.[16]		Thomas Benton claims that because shy people "have a tendency toward self-criticism, they are often high achievers, and not just in solitary activities like research and writing. Perhaps even more than the drive toward independent achievement, shy people long to make connections to others often through altruistic behavior."[36] Susan Cain describes the benefits that shy people bring to society that US cultural norms devalue. Without characteristics that shy people bring to social interactions, such as sensitivity to the emotions of others, contemplation of ideas, and valuable listening skills, there would be no balance to society.[37] In earlier generations, such as the 1950s, society perceived shyness as a more socially attractive trait, especially in women, indicating that views on shyness vary by culture.[37]		Sociologist Susie Scott challenged the interpretation and treatment of shyness as being pathological. "By treating shyness as an individual pathology, ... we forget that this is also a socially oriented state of mind that is socially produced and managed."[38]:2 She explores the idea that "shyness is a form of deviance: a problem for society as much as for the individual", and concludes that, to some extent, "we are all impostors, faking our way through social life".[38]:165,174 One of her interview subjects (self-defined as shy) puts this point of view even more strongly: "Sometimes I want to take my cue from the militant disabled lobbyists and say, 'hey, it's not MY problem, it's society's'. I want to be proud to be shy: on the whole, shys are probably more sensitive, and nicer people, than 'normals'. I shouldn't have to change: society should adapt to meet my needs."[38]:164		In cultures that value outspokenness and overt confidence, shyness can be perceived as weakness.[14] To an unsympathetic observer, a shy individual may be mistaken as cold, distant, arrogant or aloof, which can be frustrating for the shy individual.[16] However, in other cultures, shy people may be perceived as being thoughtful, intelligent, as being good listeners, and as being more likely to think before they speak.[37]		In cultures that value autonomy, shyness is often analyzed in the context of being a social dysfunction, and is frequently contemplated as a personality disorder or mental health issue. Some researchers are beginning to study comparisons between individualistic and collectivistic cultures, to examine the role that shyness might play in matters of social etiquette and achieving group-oriented goals. "Shyness is one of the emotions that may serve as behavioral regulators of social relationships in collectivistic cultures. For example, social shyness is evaluated more positively in a collectivistic society, but negatively evaluated in an individualistic society."[39]		In a cross-cultural study of Chinese and Canadian school children, researchers sought to measure several variables related to social reputation and peer relationships, including "shyness-sensitivity." Using peer nomination questionnaire, students evaluated their fellow students using positive and negative playmate nominations. "Shyness-sensitivity was significantly and negatively correlated with measures of peer acceptance in the Canadian sample. Inconsistent with Western results, it was found that items describing shyness-sensitivity were separated from items assessing isolation in the factor structure for the Chinese sample. Shyness-sensitivity was positively associated with sociability-leadership and with peer acceptance in the Chinese sample."[40]		In some Western cultures shyness-inhibition plays an important role in psychological and social adjustment. It has been found that shyness-inhibition is associated with a variety of maladaptive behaviors. Being shy or inhibited in Western cultures can result in rejection by peers, isolation and being viewed as socially incompetent by adults. However, research suggests that if social withdrawal is seen as a personal choice rather than the result of shyness, there are fewer negative connotations.[41]		British writer Arthur C. Benson felt shyness is not mere self-consciousness, but a primitive suspicion of strangers, the primeval belief that their motives are predatory, with shyness a sinister quality which needs to be uprooted.[42] He believed the remedy is for the shy to frequent society for courage from familiarity. Also, he claimed that too many shy adults take refuge in a critical attitude, engaging in brutal onslaughts on inoffensive persons. He felt that a better way is for the shy to be nice, to wonder what others need and like, interest in what others do or are talking about, friendly questions, and sympathy.[43]		For Charles Darwin shyness was an ‘odd state of mind’ appearing to offer no benefit to our species, and since the 1970s the modern tendency in psychology has been to see shyness as pathology.[44] However, evolutionary survival advantages of careful temperaments over adventurous temperaments in dangerous environments have also been recognized.[37][44]		In Eastern cultures shyness-inhibition in school-aged children is seen as positive and those that exhibit these traits are viewed well by peers and are accepted. They tended to be seen as competent by their teachers, to perform well in school and to show well-being. Shy individuals are also more likely to attain leadership status in school. Being shy or inhibited does not correlate with loneliness or depression as those in the West do. In Eastern cultures being shy and inhibited is a sign of politeness, respectfulness, and thoughtfulness.[41]		In Hispanic cultures shyness and inhibition with authority figures is common. For instance, Hispanic students may feel shy towards being praised by teachers in front of others, because in these cultures students are rewarded in private with a touch, a smile, or spoken word of praise. Hispanic students may seem shy when they are not. It is considered rude to excel over peers and siblings; therefore it is common for Hispanic students to be reserved in classroom settings. Adults also show reluctance to share personal matters about themselves to authority figures such as nurses and doctors.[45]		Cultures in which the community is closed and based on agriculture (Kenya, India, etc.) experience lower social engagement than those in more open communities (United States, Okinawa, etc.) where interactions with peers is encouraged. Children in Mayan, Indian, Mexican, and Kenyan cultures are less expressive in social styles during interactions and they spend little time engaged in socio-dramatic activities. They are also less assertive in social situations. Self-expression and assertiveness in social interactions are related to shyness and inhibition in that when one is shy or inhibited one exhibits little or no expressive tendencies.[41] Assertiveness is demonstrated in the same way, being shy and inhibited lessen one's chances of being assertive because of a lack of confidence.[citation needed]		In the Italian culture emotional expressiveness during interpersonal interaction is encouraged. From a young age children engage in debates or discussions that encourage and strengthen social assertiveness. Independence and social competence during childhood is also promoted. Being inhibited is looked down upon and those who show this characteristic are viewed negatively by their parents and peers. Like other cultures where shyness and inhibition is viewed negatively, peers of shy and inhibited Italian children reject the socially fearful, cautious and withdrawn. These withdrawn and socially fearful children express loneliness and believe themselves to be lacking the social skills needed in social interactions.[46]		Psychological methods and pharmaceutical drugs are commonly used to treat shyness in individuals who feel crippled because of low self-esteem and psychological symptoms, such as depression or loneliness. According to research, early intervention methods that expose shy children to social interactions involving team work, especially team sports, decrease their anxiety in social interactions and increase their all around self-confidence later on.[47] Implementing such tactics could prove to be an important step in combating the psychological effects of shyness that make living normal life difficult for anxious individuals.[citation needed]				
Robert Reed Carradine (born March 24, 1954) is an American actor. The youngest of the Carradine family, he made his first appearances on television western series such as Bonanza and his late brother David's TV series, Kung Fu. Carradine's first film role was in the 1972 film The Cowboys, which starred John Wayne and Roscoe Lee Browne. Carradine also portrayed fraternity president "Lewis Skolnick" in the Revenge of the Nerds series of comedy films. He is best known for his roles as Sam McGuire in Lizzie McGuire, Donald Keeble in Max Keeble's Big Move, and Van Helsing in Mom's Got a Date with a Vampire.						Carradine was born in Hollywood, California, the son of actress and artist Sonia Sorel (née Henius) and actor John Carradine. He is one of many actors in the Carradine family. He is the brother of Christopher and Keith Carradine, paternal half-brother of Bruce and the late David Carradine, and maternal half-brother of Michael Bowen.[1] His maternal great-grandfather was biochemist Max Henius, and his maternal great-grandmother was the sister of historian Johan Ludvig Heiberg.[2]		Carradine's parents divorced when he was 2 years old. A bitter custody battle led to his father gaining custody of him and his brothers, Christopher and Keith. During the custody battle, the children had spent three months in a home for abused children as wards of the court.[3] His brother, Keith, said of the experience, "It was like being in jail. There were bars on the windows, and we were only allowed to see our parents through glass doors. It was very sad. We would stand there on either side of the glass door crying".[4]		Carradine was raised primarily by his stepmother, his father's third wife, Doris Grimshaw, and believed her to be his mother until he was introduced to Sonia Sorel at a Christmas party when he was 14 years old.[5] While still in high school, Robert lived with his half-brother, David, in Laurel Canyon, California. Under David's care he indulged in two of his major interests: race car driving and music.[6] He and David belonged to a musical quartet that performed in small clubs in Los Angeles and San Francisco.[5]		Carradine made his film debut in 1972 in The Cowboys with John Wayne.[7] He was also featured in a short-lived television series, of the same name, based on the movie. He made an appearance as a killer in the Martin Scorsese film Mean Streets shooting to death the character played by his brother, David.		During this time he worked with David on some independent projects including a biker film called You and Me (1975) and an unreleased musical called A Country Mile. He also did camera work for David's cult classic Vietnam War-inspired Americana which was not released until 1983.[6]		In 1976, Carradine had the opportunity to demonstrate on screen what he considered to be his "first ambition", car racing,[5] when he played Jim Cantrell in Paul Bartel's Cannonball. In the film Robert's character ironically wins the cross country road race, beating the favorite, Coy "Cannonball" Buckman, played by his brother, David. In 1977, Robert became a snack for the vengeful killer whale in the Jaws imitation film Orca.[8]		In 1978, Robert landed a demanding role in Hal Ashby's Oscar-winning Vietnam War drama, Coming Home, which starred Jane Fonda and Jon Voight. His performance caused some speculation that he might be the best actor in his family.[5]		Robert was instrumental in securing his brothers David and Keith to perform with him in one of the most unusual casting arrangements in movie history. Together the Carradines played the Younger brothers in The Long Riders (1980) along with three other sets of acting brothers: Stacy and James Keach, Dennis and Randy Quaid, and Christopher and Nicholas Guest.[9]		Also in 1980, Carradine co-starred with Mark Hamill and Lee Marvin in Samuel Fuller's The Big Red One recounting Fuller's WW II experience. His character, who was based on Fuller himself, narrated the film.[10]		In 1983, he and Cherie Curie starred in the science fiction movie Wavelength in which he played a washed up rock star who helps extraterrestrials escape from a military base.[11] For the film he performed his own compositions including one named after his daughter, Ever. Also in 1983, he starred in the music video for The Motels hit song "Suddenly Last Summer" as lead singer Martha Davis' love interest.		Carradine's biggest film success to date came in 1984 when he starred in Revenge of the Nerds as the lead nerd Lewis Skolnick. To prepare for the comedy, Carradine spent time at The University of Arizona, where the movie was filmed, participating in rush week. "No fraternity picked him, convincing Carradine that he was indeed right for the part of the nerd that nobody wanted to claim as their own."[12] Carradine reprised the role of Skolnick in three sequels, taking over as executive producer in the latter two. In 2001 he played Donald Keeble in Max Keeble's Big Move. In 2000 he co-starred with Caroline Rhea in Mom's Got a Date with a Vampire. He reprised his role as Sam McGuire in The Lizzie McGuire Movie in 2003.		Carradine's first television appearance was in 1971, on the classic western series, Bonanza. He also appeared on his brother David's series, Kung Fu, as Sunny Jim, the mute companion of Serenity Johnson, played by his father, John, in an episode called Dark Angel (1972). In 1979, he was alongside Melissa Sue Anderson in Survival of Dana.[6] In 1984, Carradine played Robert Cohn in the television mini-series version of Ernest Hemingway's The Sun Also Rises.[13] He appeared in the 1987 HBO mini-series, Conspiracy: The Trial of the Chicago 8.[14] He was also a guest star in an episode of Law & Order: Criminal Intent entitled Gone (2005, Season 4, Episode 11). He also took part in Jane Doe, a TV series directed by James A. Contner, in 2007.		He was cast as Sam McGuire on "Lizzie McGuire". The show which starred Hilary Duff as Lizzie McGuire, Lalaine as Miranda Sanchez, and Ashlie Brillault as Kate Sanders was widely popular among girls. The show's realistic approach to the problems of a 13 year old girl also appealed to parents. It attracted such guest stars as David Carradine, Aaron Carter, the late Doris Roberts, and Aerosmith lead singer Steven Tyler.[15][16]		He appeared in the ER episode "Sleepless in Chicago" alongside Nerds co-star Anthony Edwards.		In January 2013, he and former Revenge of the Nerds co-star, Curtis Armstrong, hosted King of the Nerds, a reality TV series in which a group of nerds compete to find out which one is the nerdiest.		In January 2015, he and Curtis Armstrong, repeated their series King of the Nerds on TBS.		TMZ reported on March 12, 2015 that Carradine was involved in a serious car accident. He was rushed to the hospital. His wife was also injured in the crash.[17] In May 2017, People Magazine reported that in court documents, his wife claimed that the accident was an attempt by Robert to kill both himself and his wife. [18]		Carradine has two daughters, actress Ever Carradine (with Susan Snyder), Marika Reed Carradine with his wife Edie Mani, and a son, Ian Alexander Carradine. He is also the uncle of actress Martha Plimpton.		
The Online Etymology Dictionary is a free online dictionary that describes the origins of English-language words.[2]						Douglas Harper (also known as "The Sciolist") compiled the etymology dictionary to record the history and evolution of more than 30,000 words, including slang and technical terms.[3] The core body of its etymology information stems from Ernest Weekley's An Etymological Dictionary of Modern English (1921). Other sources include the Middle English Dictionary and the Barnhart Dictionary of Etymology (by Robert Barnhart and others). In producing his large dictionary, Douglas Harper says that he is essentially and for the most part a compiler, an evaluator of etymology reports which others have made.[4] Harper works as a Copy editor/Page designer for LNP Media Group.[5][6]		As of June 2015, there were nearly 50,000 entries in the dictionary.[5]		The Online Etymology Dictionary has been referenced by Ohio University's library as a relevant etymological resource[2] and cited in the Chicago Tribune as one of the "best resources for finding just the right word".[7] It is cited in numerous articles as a source for explaining the history and evolution of words.[8][9][10]		
The term Hispanic (Spanish: hispano or hispánico), broadly refers to the people, nations, and cultures that have a historical link to Spain. It commonly applies to countries once owned by the Spanish Empire in the Americas (see Spanish colonization of the Americas) and Asia, particularly the countries of Hispanic America and the Philippines. It could be argued that the term should apply to all Spanish-speaking cultures or countries, as the historical roots of the word specifically pertain to the Iberian region. It is difficult to label a nation or culture with one term, such as Hispanic, as the ethnicities, customs, traditions, and art forms (music, literature, dress, culture, cuisine, and others) vary greatly by country and region. The Spanish language and Spanish culture are the main distinctions.[1][2]		Hispanic originally referred to the people of ancient Roman Hispania, which roughly comprised the Iberian Peninsula, including the contemporary states of Spain, Portugal, Andorra, and the British Overseas Territory of Gibraltar.[3][4][5]						The term Hispanic derives from Latin Hispanicus ('Spanish'), the adjectival derivation of Latin (and Greek) Hispania ('Spain') and Hispanus/Hispanos ('Spaniard'), ultimately probably of Celtiberian origin.[6] In English the word is attested from the 16th century (and in the late 19th century in American English).[7]		The words Spain, Spanish, and Spaniard are of the same etymology as Hispanus, ultimately.[6]		Hispanus was the Latin name given to a person from Hispania during Roman rule. In English, the term Hispano-Roman is sometimes used.[9] The Hispano-Romans were composed of people from many different indigenous tribes, in addition to Italian colonists.[10][11] Some famous Hispani (plural of Hispanus) and Hispaniensis were the emperors Trajan, Marcus Aurelius, Hadrian, Theodosius I and Magnus Maximus, the poets Marcus Annaeus Lucanus, Martial and Prudentius, the philosophers Seneca the Elder and Seneca the Younger, or the usurper Maximus of Hispania. A number of these men, such as Trajan, Hadrian and others, were in fact descended from Roman colonial families.[12][13][14]		Here follows a comparison of several terms related to Hispanic:		Hispania was the Roman name for the whole territory of the Iberian Peninsula. Initially, this territory was divided into two provinces: Hispania Citerior and Hispania Ulterior. In 27 B.C, Hispania Ulterior was divided into two new provinces, Hispania Baetica and Hispania Lusitania, while Hispania Citerior was renamed Hispania Tarraconensis. This division of Hispania explains the usage of the singular and plural forms (Spain, and The Spains) used to refer to the peninsula and its kingdoms in the Middle Ages.[19]		Before the marriage of Queen Isabella I of Castile and King Ferdinand II of Aragon in 1469, the four Christian kingdoms of the Iberian Peninsula—the Kingdom of Portugal, the Crown of Aragon, the Crown of Castile, and the Kingdom of Navarre—were collectively called The Spains. This revival of the old Roman concept in the Middle Ages appears to have originated in Provençal, and was first documented at the end of the 11th century. In the Council of Constance, the four kingdoms shared one vote.		The word Lusitanian, relates to Lusitania or Portugal, also in reference to the Lusitanians, possibly one of the first Indo-European tribes to settle in Europe. From this tribe's name had derived the name of the Roman province of Lusitania, and Lusitania remains the name of Portugal in Latin.		The terms Spain and the Spains were not interchangeable.[20] Spain was a geographic territory, home to several kingdoms (Christian and Muslim), with separate governments, laws, languages, religions, and customs, and was the historical remnant of the Hispano-Gothic unity.[21] Spain was not a political entity until much later, and when referring to the Middle Ages, one should not be confounded with the nation-state of today.[22] The term The Spains referred specifically to a collective of juridico-political units, first the Christian kingdoms, and then the different kingdoms ruled by the same king.		With the Decretos de Nueva Planta, Philip V started to organize the fusion of his kingdoms that until then were ruled as distinct and independent, but this unification process lacked a formal and juridic proclamation.[23][24]		Although colloquially and literally the expression "King of Spain" or "King of the Spains" was already widespread,[25] it did not refer to a unified nation-state. It was only in the constitution of 1812 that was adopted the name Españas (Spains) for the Spanish nation and the use of the title of "king of the Spains".[26] The constitution of 1876 adopts for the first time the name "Spain" for the Spanish nation and from then on the kings would use the title of "king of Spain".[27]		The expansion of the Spanish Empire between 1492 and 1898 brought thousands of Spanish migrants to the conquered lands, who established settlements, mainly in the Americas, but also in other distant parts of the world (as in the Philippines, the lone Spanish territory in Asia), producing a number of multiracial populations. Today, the term Hispanic is typically applied to the varied populations of these places, including those with Spanish ancestry. The Filipinos however can be considered Hispanics because of the culture and language that Spanish left behind. Along with English and Tagalog, Spanish used to be one of the official languages in the Philippines before being removed in 1973 by the Cory Aquino government.		The Latin gentile adjectives that belong to Hispania are Hispanus, Hispanicus, and Hispanienses. A Hispanus is someone who is a native of Hispania with no foreign parents, while children born in Hispania of (Latin) Roman parents were Hispaniensis. Hispaniensis means 'connected in some way to Hispania', as in "Exercitus Hispaniensis" ('the Spanish army') or "mercatores Hispanienses" ('Spanish merchants'). Hispanicus implies 'of' or 'belonging to' Hispania or the Hispanus or of their fashion as in "glaudius Hispanicus".[28] The gentile adjectives were not ethnolinguistic but derived primarily on a geographic basis, from the toponym Hispania as the people of Hispania spoke different languages, although Livy said they could all understand each other, not making clear if they spoke dialects of the same language or were polyglots.[29] The first recorded use of an anthroponym derived from the toponym Hispania is attested in one of the five fragments, of Ennius in 236 B.C. who wrote "Hispane, non Romane memoretis loqui me" ("Remember that I speak like a Spaniard not a Roman") as having been said by a native of Hispania.[30][31]		The term Hispanic signifies the cultural resonance, among other elements and characteristics, of the descendants of the people who inhabited ancient Hispania (Iberian Peninsula). It has been used throughout history for many purposes, including drawing a contrast to the Moors and differentiating explorers and settlers.		Technically speaking, persons from Portugal or of Portuguese extraction are referred to as Lusitanians. In Portugal, Hispanic refers to something related to ancient Hispania, Spain or the Spanish language and culture, Portugal.[32] Portugal and Spain do not have exactly the same definition for the term Hispanic, but they do share the etymology for the word (pt: hispânico, es: hispánico). The Royal Spanish Academy (Spanish: Real Academia Española, RAE), the official royal institution responsible for regulating the Spanish language defines the terms "Hispano" and "Hispánico" (which in Spain have slightly different meanings) as:[33][34]		Hispano:		Hispánico:		Note that both terms include Portugal as part of "Hispania" as Hispania is the old Roman name given to the entire Iberian peninsula and their peoples, including the Lusitanians.		The common modern term to identify Portuguese and Spanish cultures under a single nomenclature is "Iberian", and the one to refer to cultures derived from both countries in the Americas is "Iberian-American". These designations can be mutually recognized by people in Portugal and Brazil, unlike "Hispanic", which is totally void of any self-identification in those countries, and quite on the opposite, serves the purpose of marking a clear distinction in relation to neighboring countries´ culture.		In Spanish, the term "hispano" as in "hispanoamericano", refers to the people of Spanish origin who live in the Americas; it also refers to a relationship to Hispania or to the Spanish language. There are people in Hispanic America that are not of Spanish origin, as the original people of these areas are Amerindians.		Today, organizations in the United States use the term as a broad catchall to refer to persons with a historical and cultural relationship with Spain, regardless of race and ethnicity.[1][2] The U.S. Census Bureau defines the ethnonym Hispanic or Latino to refer to "a person of Cuban, Mexican, Puerto Rican, South or Central American, or other Spanish culture or origin regardless of race"[35] and states that Hispanics or Latinos can be of any race, any ancestry, any ethnicity.[36] Generically, this limits the definition of Hispanic or Latino to people from the Caribbean, Central and South America, or other Hispanic (Spanish or Portuguese) culture or origin, regardless of race. Latino can refer to males or females, while Latina refers to only females.		Because of the technical distinctions involved in defining "race" vs. "ethnicity," there is confusion among the general population about the designation of Hispanic identity. Currently, the United States Census Bureau defines six race categories:[37]		According to census reports, of the above races the largest number of Hispanic or Latinos are of the White race, the second largest number come from the Native American/American Indian race who are the indigenous people of the Americas. The inhabitants of Easter Island are Pacific Islanders and since the island belongs to Chile they are theoretically Hispanic or Latinos. Because Hispanic roots are considered aligned with a European ancestry (Spain/Portugal), Hispanic/Latino ancestry is defined solely as an ethnic designation (similar to being Norse or Germanic). Therefore, a person of Hispanic descent is typically defined using both race and ethnicity as an identifier—i.e., Black-Hispanic, White-Hispanic, Asian-Hispanic, Amerindian-Hispanic or "other race" Hispanic.		A 1997 notice by the U.S. Office of Management and Budget defined Hispanic or Latino persons as being "persons who trace their origin or descent to Mexico, Puerto Rico, Cuba, Central and South America, and other Spanish cultures."[38] The United States Census uses the ethnonym Hispanic or Latino to refer to "a person of Cuban, Mexican, Puerto Rican, South or Central American, or other Hispanic culture or origin regardless of race."[35]		The 2010 Census asked if the person was "Spanish/Hispanic/Latino". The United States Census uses the ethnonym Hispanic or Latino to refer to "a person of Cuban, Mexican, Puerto Rican, South or Central American, or other Spanish culture or origin regardless of race."[35] The Census Bureau also explains that "[o]rigin can be viewed as the heritage, nationality group, lineage, or country of birth of the person or the person's ancestors before their arrival in the United States. People who identify their origin as Hispanic, Latino, or Spanish may be of any race."[39]		The U.S. Department of Transportation defines Hispanic as, "persons of Mexican, Puerto Rican, Cuban, Dominican, Central or South American, or other Spanish or Portuguese culture or origin, regardless of race."[1] This definition has been adopted by the Small Business Administration as well as by many federal, state, and municipal agencies for the purposes of awarding government contracts to minority owned businesses.[2] The Congressional Hispanic Caucus and the Congressional Hispanic Conference include representatives of Spanish and Portuguese, Puerto Rican and Mexican descent. The Hispanic Society of America is dedicated to the study of the arts and cultures of Spain, Portugal, and Latin America.[40] The Hispanic Association of Colleges and Universities, proclaimed champions of Hispanic success in higher education, is committed to Hispanic educational success in the U.S., Puerto Rico, Ibero-America, Spain and Portugal.		The U.S. Equal Employment Opportunity Commission encourages any individual who believes that he or she is Hispanic to self-identify as Hispanic.[41] The United States Department of Labor - Office of Federal Contract Compliance Programs encourages the same self-identification. As a result, any individual who traces his or her origins to part of the Spanish Empire or Portuguese Empire may self-identify as Hispanic, because an employer may not override an individual's self-identification.[42]		The 1970 Census was the first time that a "Hispanic" identifier was used and data collected with the question. The definition of "Hispanic" has been modified in each successive census.[43]		In a recent study, most Spanish-speakers of Spanish or Hispanic American descent do not prefer the term "Hispanic" or "Latino" when it comes to describing their identity. Instead, they prefer to be identified by their country of origin. When asked if they have a preference for either being identified as "Hispanic" or "Latino," the Pew study finds that "half (51%) say they have no preference for either term."[44] A majority (51%) say they most often identify themselves by their family’s country of origin, while 24% say they prefer a pan-ethnic label such as Hispanic or Latino. Among those 24% who have a preference for a pan-ethnic label, "'Hispanic' is preferred over 'Latino' by more than a two-to-one margin—33% versus 14%." Twenty-one percent prefer to be referred to simply as "Americans."[45]		Hispanicization is the process by which a place or a person absorbs characteristics of Hispanic society and culture.[46][47][48] Modern hispanization of a place, namely in the United States, might be illustrated by Spanish-language media and businesses. Hispanization of a person might be illustrated by speaking Spanish, making and eating Hispanic American food, listening to Spanish language music or participating in Hispanic festivals and holidays - Hispanization of those outside the Hispanic community as opposed to assimilation of Hispanics into theirs.		One reason that some people believe the assimilation of Hispanics in the U.S. is not comparable to that of other cultural groups is that Hispanic and Latino Americans have been living in parts of North America for centuries, in many cases well before the English-speaking culture became dominant. For example, California, Texas, Colorado, New Mexico (1598), Arizona, Nevada, Florida and Puerto Rico have been home to Spanish-speaking peoples since the 16th century, long before the U.S. existed. (But it should be noted that the language of the Native Americans existed before this, until the invasion and forced assimilation by the Spanish.) These and other Spanish-speaking territories were part of the Viceroyalty of New Spain, and later Mexico (with the exception of Florida and Puerto Rico), before these regions joined or were taken over by the United States in 1848. Some cities in the U.S. were founded by Spanish settlers as early as the 16th century, prior to the creation of the Thirteen Colonies. For example, San Miguel de Gualdape, Pensacola and St. Augustine, Florida were founded in 1526, 1559 and 1565 respectively. Santa Fe, New Mexico was founded in 1604, and Albuquerque was established in 1660. El Paso was founded in 1659, San Antonio in 1691, Laredo, Texas in 1755, San Diego in 1769, San Francisco in 1776, San Jose, California in 1777, New Iberia, Louisiana in 1779, and Los Angeles in 1781. Therefore, in many parts of the U.S., the Hispanic cultural legacy predates English/British influence. For this reason, many generations have largely maintained their cultural traditions and Spanish language well before the United States was created. However, Spanish-speaking persons in many Hispanic areas in the U.S. amounted to only a few thousand people when they became part of the United States; a large majority of current Hispanic residents are descended from Hispanics who entered the United States in the mid-to-late 20th and early 21st centuries.		Language retention is a common index to assimilation; according to the 2000 census, about 75 percent of all Hispanics spoke Spanish in the home. Spanish language retention rates vary geographically; parts of Texas and New Mexico have language retention rates over 90 percent, whereas in parts of Colorado and California, retention rates are lower than 30 percent. The degree of retention of Spanish as the native language is based on recent arrival from countries where Spanish is spoken. As is true of other immigrants, those who were born in other countries still speak their native language. Later generations are increasingly less likely to speak the language spoken in the country of their ancestors, as is true of other immigrant groups.		Today, Spanish is among the most commonly spoken first languages of the world. During the period of the Spanish Empire from 1492 and 1898, many people migrated from Spain to the conquered lands. The Spaniards brought with them the Castilian language and culture, and in this process that lasted several centuries, created a global empire with a diverse population.		Culturally, Spaniards (those living in Spain) are typically European, but they also have small traces of many peoples from the rest of Europe, such as for example, old Germania, Scandinavia, France, the Mediterranean, the Near East and northern Africa.[49][50]		(Note: The U.S. is a predominantly English-speaking country. As is true of many immigrant families, the immigrants often speak Spanish and some English, while their children are fluent English speakers because they were born and educated in the U.S. Some retain their Spanish language as is true of other immigrant families. The recent influx of large numbers of immigrants from Spanish-speaking countries into the U.S. has meant that the number of Spanish-speaking U.S. residents has increased, but the children speaking English as is true of the historic U.S. immigrant experience, continues. Migration from Hispanic countries has increased the Spanish-speaking population in the United States. Of those who speak Spanish in the United States, three quarters speak English well or very well.		(Note: a separate listing for Hispanics is not included because the U.S. Census Bureau considers Hispanic to mean a person of Hispanic American descent (including persons of Cuban, Mexican, or Puerto Rican origin) and of Spanish descent living in the U.S. who may be of any race or ethnic group (white, black, Asian, etc.); about 15–16% of the total U.S. population is Hispanic, not including estimates about alien residents).		The Miguel de Cervantes Prize is awarded to Hispanic writers, whereas the Latin Grammy Award recognizes Hispanic and Portuguese musicians, and the Platino Awards as given to outstanding Hispanic films.		Folk and popular dance and music also varies greatly among Hispanics. For instance, the music from Spain is a lot different from the Hispanic American, although there is a high grade of exchange between both continents. In addition, due to the high national development of the diverse nationalities and regions of Spain, there is a lot of music in the different languages of the Peninsula (Catalan, Galician and Basque, mainly). See, for instance, Music of Catalonia or Rock català, Music of Galicia, Cantabria and Asturias, and Basque music. Flamenco is also a very popular music style in Spain, especially in Andalusia. Spanish ballads "romances" can be traced in Argentina as "milongas", same structure but different scenarios.		On the other side of the ocean, Hispanic America is also home to a wide variety of music, even though "Latin" music is often erroneously thought of, as a single genre. Hispanic Caribbean music tends to favor complex polyrhythms of African origin. Mexican music shows combined influences of mostly European and Native American origin, while traditional Northern Mexican music — norteño and banda — polka, has influence from polka music brought by Central European settlers to Mexico which later influenced western music. The music of Hispanic Americans — such as tejano music — has influences in rock, jazz, R&B, pop, and country music as well as traditional Mexican music such as Mariachi. Meanwhile, native Andean sounds and melodies are the backbone of Peruvian and Bolivian music, but also play a significant role in the popular music of most South American countries and are heavily incorporated into the folk music of Ecuador and Chile and the tunes of Colombia, and again in Chile where they play a fundamental role in the form of the greatly followed nueva canción. In U.S. communities of immigrants from these countries it is common to hear these styles. Latin pop, Rock en Español, Latin hip-hop, Salsa, Merengue, colombian cumbia and Reggaeton styles tend to appeal to the broader Hispanic population, and varieties of Cuban music are popular with many Hispanics of all backgrounds.		Spanish-language literature and folklore is very rich and is influenced by a variety of countries. There are thousands of writers from many places, and dating from the Middle Ages to the present. Some of the most recognized writers are Miguel de Cervantes Saavedra (Spain), Lope de Vega (Spain), Calderón de la Barca (Spain), Jose Rizal (Philippines), Carlos Fuentes (Mexico), Octavio Paz (Mexico), Miguel Ángel Asturias (Guatemala), George Santayana (US), José Martí (Cuba), Sabine Ulibarri (US), Federico García Lorca (Spain), Miguel de Unamuno (Spain), Gabriel García Márquez (Colombia), Rafael Pombo (Colombia), Horacio Quiroga (Uruguay), Rómulo Gallegos (Venezuela), Luis Rodriguez Varela (Philippines), Rubén Darío (Nicaragua), Mario Vargas Llosa (Peru), Giannina Braschi (Puerto Rico), Cristina Peri Rossi (Uruguay), Luisa Valenzuela (Argentina), Roberto Quesada (Honduras), Julio Cortázar (Argentina), Pablo Neruda (Chile), Gabriela Mistral (Chile), Jorge Luis Borges (Argentina), Pedro Henríquez Ureña (Dominican Republic), Ernesto Sabato (Argentina), Juan Tomás Ávila Laurel (Equatorial Guinea), Ciro Alegría (Peru), Joaquin Garcia Monge (Costa Rica), and Jesus Balmori (Philippines).		In the majority of the Hispanic countries, association football is the most popular sport. The men's national teams of Argentine, Uruguay and Spain have won the FIFA World Cup a total five times. The Spanish La Liga is one of the most popular in the world, known for FC Barcelona and Real Madrid. Meanwhile, the Argentine Primera División and Mexican Primera División are two of the strongest leagues in the Americas.		However, baseball is the most popular sport in some Central American and Caribbean countries (especially Cuba, Dominican Republic, Puerto Rico and Venezuela), as well as in the diaspora in the United States. Notable Hispanic teams in early baseball are the All Cubans, Cuban Stars and New York Cubans. The Hispanic Heritage Baseball Museum recognizes Hispanic baseball personalities. Nearly 30 percent (22 percent foreign-born Latinos) of MLB players today have Hispanic heritage.		Several Hispanic sportspeople have been successful worldwide, such as Diego Maradona, Alfredo di Stefano, Lionel Messi, Diego Forlán (association football), Juan Manuel Fangio, Juan Pablo Montoya, Eliseo Salazar, Fernando Alonso, Marc Gené, Carlos Sainz (auto racing), Ángel Nieto, Dani Pedrosa, Jorge Lorenzo, Marc Márquez, Marc Coma, Nani Roma (motorcycle racing), Emanuel Ginóbili, Pau Gasol, Marc Gasol (basketball), Julio César Chávez, Saúl Álvarez, Carlos Monzón (boxing), Miguel Indurain, Alberto Contador, Santiago Botero, Rigoberto Urán, Nairo Quintana (cycling), Roberto de Vicenzo, Ángel Cabrera, Sergio García, Severiano Ballesteros, José María Olazábal (golf), Luciana Aymar (field hockey), Rafael Nadal, Marcelo Ríos, Guillermo Vilas, Gabriela Sabatini, Juan Martín del Potro (tennis).		Notable Hispanic sports television networks are ESPN Latin America, Fox Sports Latin America and TyC Sports.		With regard to religious affiliation among Spanish-speakers, Christianity — specifically Roman Catholicism — is usually the first religious tradition that comes to mind[citation needed]. The Spaniards and the Portuguese took the Roman Catholic faith to Ibero-America and the Philippines, and Roman Catholicism remains the predominant religion amongst most Hispanics. A small but growing number of Hispanics belong to a Protestant denomination.		There are also Spanish-speaking Jews, most of whom are the descendants of Ashkenazi Jews who migrated from Europe (German Jews, Russian Jews, Polish Jews, etc.) to Hispanic America, particularly Argentina, Uruguay, Peru and Cuba (Argentina is host to the third largest Jewish population in the Western Hemisphere, after the United States and Canada)[92][93] in the 19th century and following World War II. Many Spanish-speaking Jews also originate from the small communities of reconverted descendants of anusim — those whose Spanish Sephardi Jewish ancestors long ago hid their Jewish ancestry and beliefs in fear of persecution by the Spanish Inquisition in the Iberian Peninsula and Ibero-America. The Spanish Inquisition led to a large number of forced conversions of Spanish Jews.		Genetic studies on the (male) Y-chromosome conducted by the University of Leeds in 2008 appear to support the idea that the number of forced conversions have been previously underestimated significantly. They found that twenty percent of Spanish males have Y-chromosomes associated with Sephardic Jewish ancestry.[94] This may imply that there were more forced conversions than was previously thought.		There are also thought to be many Catholic-professing descendants of marranos and Spanish-speaking crypto-Jews in the Southwestern United States and scattered through Hispanic America. Additionally, there are Sephardic Jews who are descendants of those Jews who fled Spain to Turkey, Syria, and North Africa, some of whom have now migrated to Hispanic America, holding on to some Spanish/Sephardic customs, such as the Ladino language, which mixes Spanish, Hebrew, Arabic and others, though written with Hebrew and Latin characters.[95] Though, it should be noted, that Ladinos were also African slaves captive in Spain held prior to the colonial period in the Americas. (See also History of the Jews in Hispanic America and List of Hispanic American Jews.)		Among the Spanish-speaking Catholics, most communities celebrate their homeland's patron saint, dedicating a day for this purpose with festivals and religious services. Some Spanish-speakers syncretize Roman Catholicism and African or Native American rituals and beliefs. Such is the case of Santería, popular with Afro-Cubans, which combines old African beliefs in the form of Roman Catholic saints and rituals. Other syncretistic beliefs include Spiritism and Curanderismo.		While a tiny minority, there are some Muslims in Latin America, in the US, and in the Philippines. Those in the Philippines live predominantly in the Autonomous Region in Muslim Mindanao province.		In the United States, some 65% of Hispanics and Latinos report themselves Catholic and 21% Protestant, with 13% having no affiliation.[96] A minority among the Roman Catholics, about one in five, are charismatics. Among the Protestant, 85% are "Born-again Christians" and belong to Evangelical or Pentecostal churches. Among the smallest groups, less than 4%, are Jewish.		The Hispanic world, according to the United Nations World Heritage Committee, has contributed substantially more than any other ethnicity to the cultural heritage of the world. A World Heritage Cultural Site is a place such as a building, city, complex, or monument that is listed by the United Nations Educational, Scientific and Cultural Organization (UNESCO) as being of special cultural significance. Of a total of 802 Cultural World Heritage Sites recognized by the United Nations as of July 2015, 114 are located in Hispanic countries. Spain alone has 39 cultural sites, only second in the world to Italy.		
Hermione Jean Granger (/hərˈmaɪ.əni ˈdʒiːn ˈɡreɪndʒər/) is a fictional character in J. K. Rowling's Harry Potter series. She first appears in Harry Potter and the Philosopher's Stone, as a new student on her way to Hogwarts. After Harry and Ron save her from a mountain troll in the girls' toilets, she becomes best friends with them and often uses her quick wit, deft recall, and encyclopaedic knowledge to help them. Rowling has stated that Hermione resembles herself at a younger age, with her insecurity and fear of failure.[1]						Hermione Jean Granger is a Muggle-born, Gryffindor student[2] who becomes best friends with Harry Potter and Ron Weasley. J.K. Rowling states that she was born on 19 September 1979[1] and she was nearly twelve when she first attended Hogwarts.[3] She is an overachiever who excels academically and is described by Rowling as a "very logical, upright and good" character.[4] Rowling adds that Hermione's parents, two Muggle dentists, are a bit bemused by their odd daughter but "very proud of her all the same."[5] They are well aware of the wizarding world and have visited Diagon Alley with her. Hermione was originally intended to have a sister, but the planned sibling did not make an appearance in the first Harry Potter novel and, as Rowling noted, it "seemed too late" to introduce the character after that.[5] Rowling confirmed in a 2004 interview that Hermione is an only child.[6]		Rowling has described the character of Luna Lovegood as the "anti-Hermione" as they are so different.[7] Hermione's foil at Hogwarts is Pansy Parkinson, a bully based on real-life girls who teased the author during her school days.[8]		Rowling stated that the character of Hermione carries several autobiographical influences: "I did not set out to make Hermione like me but she is...she is an exaggeration of how I was when I was younger."[4] She recalled being called a "little know-it-all" in her youth.[1] Moreover, she states that not unlike herself, "there is a lot of insecurity and a great fear of failure" beneath Hermione's swottiness. Finally, according to Rowling, next to Albus Dumbledore, Hermione is the perfect expository character; because of her encyclopaedic knowledge, she can always be used as a plot dump to explain the Harry Potter universe.[9] Rowling also states that her feminist conscience is rescued by Hermione, "who's the brightest character" and is a "very strong female character".[10]		Hermione's first name is taken from a character in William Shakespeare's The Winter's Tale, though Rowling has said that the two characters have little in common otherwise.[11] Rowling said that she wanted it to be unusual since if fewer girls shared her name, fewer girls would get teased for it[11] and it seemed that "a pair of professional dentists, who liked to prove how clever they are...gave [her] an unusual name that no-one could pronounce."[12] Her original last name was "Puckle", but Rowling felt the name "did not suit her at all", and so the less frivolous Granger made it into the books.[1]		Hermione first appears in Harry Potter and the Philosopher's Stone when she meets Harry and Ron on the Hogwarts Express. Here Hermione condemns Ron for his inability to perform a spell to turn his rat yellow. She proves just how much she knows by declaring that she memorized all the textbooks by heart and performing a spell on Harry. She constantly annoys her peers with her knowledge, so Harry and Ron initially consider her arrogant; especially after she criticises Ron's incantation of the Levitation Charm.[13] They heartily dislike her until they rescue her from a troll, for which she is so thankful that she lies to protect them from punishment, thus winning their friendship.[14] Hermione's knack for logic later enables the trio to solve a puzzle that is essential to retrieving the Philosopher's Stone, and she defeats the constrictive Devil's Snare plant by summoning a jet of "bluebell flame".[15]		Hermione is the brains behind the plan to enter the place where the Stone is hidden. She responds to Harry's wariness of Professor Severus Snape and is also suspicious of him. She reveals to Harry and Ron that she does a lot of research in the library, which helped her defeat the Devil's Snare and work out the logic of the potions.		Rowling said on her website that she resisted her editor's requests to remove the troll scene, stating "Hermione is so very arrogant and annoying in the early part of Philosopher's Stone that I really felt it needed something (literally) huge to bring her together with Harry and Ron."[1]		Hermione (along with Ron's mother Molly Weasley and a few female students of Hogwarts) develops a liking for Defence Against the Dark Arts teacher Gilderoy Lockhart as he had written all the books required for the subject of Defence Against The Dark Arts in Harry Potter and the Chamber of Secrets.[16] During a morning confrontation between the Gryffindor and Slytherin Quidditch teams, a brawl nearly ensues after Draco Malfoy calls her a "Mudblood", an insulting epithet for Muggle-born wizards when she defends the Gryffindor Quidditch team. She concocts the Polyjuice Potion needed for the trio to disguise themselves as Draco's housemates to collect information about the Heir of Slytherin who has reopened the Chamber of Secrets. However, she is unable to join Harry and Ron in the investigation after the hair plucked from the robes of Slytherin student Millicent Bulstrode (with whom Hermione was previously matched up during Lockhart's ill-fated Duelling Club) was that of her cat, whose appearance she takes on in her human form; it takes several weeks for the effects to completely wear off. Hermione is Petrified by the basilisk after successfully identifying the creature through library research. Though she lies incapacitated in the hospital wing, her information is crucial to Harry and Ron in their successful mission to solve the mystery of the Chamber of Secrets. Hermione is revived after Harry kills the basilisk, but she is distraught to learn that all end-of-year exams have been cancelled as a school treat.[17]		Hermione buys a cat named Crookshanks, who takes to chasing Ron's pet rat, Scabbers.[18] Before the start of term, Professor McGonagall secretly gives Hermione a Time-Turner, a device which enables her to go back in time and handle her heavy class schedule, though this is not revealed until the penultimate chapter. Much tension comes into play between Hermione and her two best friends; Harry is furious with her because she told McGonagall that he had received a Firebolt, which was confiscated to be inspected for traces of dark magic. Ron is angry because he feels Crookshanks is responsible for Scabbers' disappearance, while Hermione fiercely maintains that Crookshanks is innocent.		While filling in for Remus Lupin in one Defence Against the Dark Arts class, Snape labels Hermione "an insufferable know-it-all" and penalises Gryffindor after she speaks out of turn in her attempt to describe a werewolf when no one else does. She correctly deduces Lupin's secret after completing Snape's homework assignment from the class, while Crookshanks proves vital in exposing Scabbers as Peter Pettigrew, a friend of James and Lily Potter who revealed their whereabouts to Lord Voldemort the night of their murders, and was able to wrongly implicate Sirius Black (revealed to be Harry's godfather) in the Potters' deaths.[19] The Time-Turner enables Hermione and Harry to rescue Sirius and the hippogriff Buckbeak.[19]		Hermione is horrified by the cruelty that house-elves suffer, and founds S.P.E.W., the Society for the Promotion of Elfish Welfare, as an effort to gain basic rights for them. She is Bulgarian Quidditch prodigy Viktor Krum's date at the Yule Ball of the Triwizard Tournament.[20] The proper pronunciation of her name (Her-my-oh-nee) is interjected into the plot when she teaches it to Krum; the best he can do is "Herm-own-ninny," but she has no problem with it.[1] She later gets into a heated argument with Ron after he accuses her of "fraternising with the enemy" in reference to her friendship with Krum. In the book, Hermione's feelings toward Ron are hinted at when she says that Ron can't see her "like a girl," but Krum could. She supports Harry through the Triwizard Tournament, helping him prepare for each task. At the end of the second task, Krum asks her to come see him over the summer in Bulgaria, but she politely refuses. Near the end of the term she stops fraudulent tabloid reporter and unregistered Animagus, Rita Skeeter, who had published defamatory material about Hermione, Harry, and Hagrid during the Triwizard Tournament, by holding her Animagus form (a beetle) captive in a jar.[21]		Hermione becomes a Gryffindor prefect along with Ron, and befriends Luna Lovegood, but their friendship gets off to a rocky start after Hermione chastises the publication of Luna's father: "The Quibbler's rubbish, everyone knows that." She also lambasts housemate Lavender Brown for believing the Daily Prophet's allegations of Harry fabricating stories of Voldemort's return. Ron and Hermione spend much of their time bickering, likely due to their growing romantic feelings toward one another, but they show continued loyalty to Harry. Later, with Luna's assistance, new headmistress Professor Umbridge attempts to ban the magazine from Hogwarts. This effort becomes moot as the story spreads quickly through the school. One turning point in the series is when Hermione conceives the idea of Harry secretly teaching defensive magic to a small band of students in defiance of the Ministry of Magic's dictum to teach only the subject's basic principles from a textbook, with no hands-on practice. Hermione gets an unexpectedly huge response, and the group becomes the nascent Dumbledore's Army. She is involved in the battle in the Department of Mysteries and seriously injured by a spell from Death Eater Antonin Dolohov, but makes a full recovery.[22]		New Potions professor Horace Slughorn invites Hermione to join his "Slug Club",[23] and she helps Ron retain his spot on the Gryffindor Quidditch team when she confunds Cormac McLaggen, causing him to miss his last save attempt during Keeper tryouts. Hermione's feelings for Ron continue to grow and she decides to make a move by inviting him to Slughorn's Christmas Party, but he romances Lavender instead in retaliation for his belief that Hermione had kissed Krum years earlier. She attempts to get even by dating McLaggen at the Christmas party, but her plan goes bust and she abandons him midway through the party.[24] Ron and Hermione continually feud with each other (Ron is upset with her because she set birds to attack him after seeing him and Lavender kissing; Hermione is mostly mad because of her growing jealousy) until he suffers a near-fatal poisoning from tainted mead, which frightens her enough to reconcile with him. Following Dumbledore's death, Ron and Hermione both vow to stay by Harry's side regardless of what happens.[25] A minor subplot in the book is that Hermione and Harry form a rivalry in Potions, as Hermione is used to coming first in her subjects and is angered that Harry outperforms her undeservedly by following tips and different instructions written in the margins of Harry's potions book by the previous owner. Hermione is also the only one of the trio to successfully pass her Apparition test (Ron failed, albeit barely, and Harry was too young).		In the seventh and final book, Hermione accompanies Harry on his quest to destroy Voldemort's remaining Horcruxes. Before leaving on the quest, she helps ensure the safety of her parents by placing a false memory charm on them, making them think they are Wendell and Monica Wilkins, whose lifetime ambition is to move to Australia. She inherits Dumbledore's personal copy of The Tales of Beedle the Bard, which allows her to decipher some of the secrets of the Deathly Hallows. She prepared for their departure and journey by placing an Undetectable Extension Charm on a small beaded purse so she is able to fill the infinite depths of the bag with materials they will need. Hermione's spell saves her and Harry from Lord Voldemort and his snake Nagini in Godric's Hollow, although the ricochet snaps Harry's wand. When she, Ron, and Harry are captured by Snatchers, who are on the hunt for Muggle-borns under the Ministry's orders, Hermione disguises Harry by temporarily disfiguring his face with a Stinging Jinx. She also attempts to pass herself off as former Hogwarts student Penelope Clearwater and a half-blood to avoid persecution, but is later recognised and taken to Malfoy Manor where Bellatrix Lestrange tortures her with the Cruciatus Curse in an attempt to extract information on how Hermione, Harry, and Ron came to possess Godric Gryffindor's sword (which was supposed to be safe in the Lestrange vault at Gringotts). Even under torture, Hermione is able to use her quick thinking to lie to Bellatrix that the sword is a fake. When the others are able to escape their cell, Bellatrix threatens to slit Hermione's throat. Hermione, Harry, Ron and the other prisoners being held in Malfoy Manor are eventually rescued by Dobby.		Hermione later uses Polyjuice Potion to impersonate Bellatrix when the trio attempt to steal Hufflepuff's cup from Gringotts. She, Harry, and Ron join Dumbledore's Army in the Battle of Hogwarts, during which Hermione destroys Hufflepuff's cup in the Chamber of Secrets with a basilisk fang, eliminating another Horcrux. Hermione and Ron also share their first kiss in the midst of the battle.[26] In the final battle in the Great Hall, Hermione fights Bellatrix with the help of Ginny Weasley and Luna. However, the three of them are unable to defeat Bellatrix and stop fighting her once Molly Weasley orders them to disengage.[27]		Nineteen years after Voldemort's death, Hermione and Ron have two children, Rose and Hugo.[28] Though the epilogue does not explicitly say Hermione and Ron are married,[28] news articles and other sources treat it as a fact.[29][30][31]		In Harry Potter and the Cursed Child, set 19 years after the events of the books, Hermione is the Minister for Magic.		In the books, Hermione is described as having bushy brown hair and brown eyes. Her front teeth, already very large, grow uncontrollably in Goblet of Fire after she is hit by a spell cast by Draco. Madam Pomfrey attends to her in the hospital wing and, at her request, shrinks the teeth down to a normal size for her mouth.		Hermione's most prominent features include her prodigious intellect and cleverness. She is levelheaded, book-smart, and is very logical. Throughout the series, Hermione uses the skills of a librarian to gather the information necessary to defeat Voldemort. When in doubt, it is to the library that Hermione turns.[32] She is often bossy yet unfailingly dutiful and loyal to her friends—a person who can be counted on. Rowling stated that Hermione is a person that, "never strays off the path; she always keeps her attention focused on the job that must be done."[33] Despite Hermione's intelligence and bossy attitude, Rowling says that Hermione has "quite a lot of vulnerability in her personality,"[34] as well as a "sense of insecurity underneath," feels "utterly inadequate... and to compensate, she tries to be the best at everything at school, projecting a false confidence that can irritate people."[35] During her Defence against the Dark Arts exam at the end of Harry Potter and the Prisoner of Azkaban, Hermione reveals that her biggest fear is failure, after a Boggart takes the form of Professor McGonagall and tells her that she has failed all her exams.		Hermione is extremely compassionate and is quick to help others, especially those who are defenceless, such as Neville Longbottom, first-years, House-Elves, fellow Muggle-borns, half-giants like Hagrid, and werewolves like Lupin. It was revealed by Rowling after the publication of the final book that Hermione's career in the Ministry was to fight for the rights of the oppressed and disenfranchised (such as House-elves or Muggle-borns). Hermione is also very protective of her friends and values them so much that Rowling has suggested that, if Hermione had looked in the Mirror of Erised, she would have seen Harry, Ron, and herself "alive and unscathed, and Voldemort finished."[36] Hermione has also learned to ignore what bullies such as Draco say to her, often preventing Harry and Ron from retaliating and thinking of some way to outsmart him. She accepts her status as a Muggle-born witch, and states in Deathly Hallows that she is "a Mudblood and proud of it".[37]		Hermione is portrayed during the whole series as an exceptionally talented young witch. Rowling has stated that Hermione is a "borderline genius."[38] She received ten O.W.L.s, which were nine Outstandings and one Exceeds Expectations. She is the best student in Harry's year and is repeatedly the first student to master any spell or charm introduced in classes and even from more advanced years, as evidenced when she is able to conjure a Protean Charm on the D.A.'s fake Galleon coins, which is actually a N.E.W.T. level charm.[39] She is also the first one to be able to cast non-verbal spells.[40] Hermione is an competent duellist - Rowling has stated that while during the first three books Hermione would have beaten Harry in a magical duel, by the fourth book Harry had become so good at Defence Against the Dark Arts that he would have defeated Hermione.[41] Hermione did not tend to do as well in subjects that were not learned through books or formal training, as broom flying did not come as naturally to her in her first year as it did to Harry,[42] and she showed no affinity for Divination, which she dropped from her third year studies.[43] She was also not good at Wizard's Chess, as it was the only thing at which she ever lost.[44]		Hermione's Patronus is an otter, Rowling's favourite animal.[45] Her wand is made of vine wood and dragon heartstring core; vine is the wood ascribed to Hermione's fictional birth month (September) on the Celtic calendar.[46]		Hermione is viewed by many as a feminist icon.[47] In The Ivory Tower and Harry Potter, the first book-length analysis of the Harry Potter series (edited and compiled by Lana A. Whited), a chapter titled "Hermione Granger and the Heritage of Gender," by Eliza T. Dresang, discusses Hermione's role in the series and its relation to feminist debates.[48] The chapter begins with an analysis of Hermione's name and the role of previous characters with the same name in mythology and fiction, and the heritage Hermione has inherited from these characters due to her name. Dresang also emphasises Hermione's parallelism with Rowling herself and how, as Hermione has some attributes from Rowling herself, she must be a strong character.		The chapter also points out the fact that, despite being born to Muggle parents, Hermione's magical abilities are innate. Her "compulsion for study" helps both the character's development, which makes Hermione "a prime example that information brings power", and the plot of the series, as her knowledge of the wizarding world is often used to "save the day". Dresang states that "Harry and Ron are more dependent on Hermione than she is on them." However, she also remarks that Hermione's "hysteria and crying happen far too often to be considered a believable part of the development of Hermione's character and are quite out of line with her core role in the book."[48]		UGO Networks listed Hermione as one of their best heroes of all time, saying, "Most of us can probably recall having a classmate like Hermione when we were in grammar school"—one who "can at first be a little off-putting, but once you get to know her, she's not a bad chick to have on your side".[49] IGN also listed Hermione as their second top Harry Potter character, praising her character development.[50]		Philip Nel of Kansas State University notes that "Rowling, who worked for Amnesty International, evokes her social activism through Hermione's passion for oppressed elves and the formation of her 'Society for the Promotion of Elfish Welfare'".[51]		However, in an analysis for Harry Potter and the Deathly Hallows, Rowland Manthrope states that "seven books on, we still only know her as swottish, sensible Hermione—a caricature, not a character."[52]		Emma Watson portrayed Hermione in all eight Harry Potter films. Watson's Oxford theatre teacher passed her name on to the casting agents of Philosopher's Stone, impressed with her school play performances.[53] Though Watson took her audition seriously, she "never really thought she had any chance" of getting the part.[54] The producers were impressed by Watson's self-confidence and she outperformed the thousands of other girls who had applied.[55]		Rowling herself was supportive of Watson after her first screen test.[53] When asked if she thought actors suited the characters, Rowling said, "Yes, I did. Emma Watson in particular was very, very like Hermione when I first spoke to her, I knew she was perfect from that first phone call."[5]		Watson was well-received for the first film; IGN even commented that "from Hermione Granger's perfect introduction to her final scene, Watson is better than I could have possibly imagined. She steals the show."[56] IGN also wrote that her "astute portrayal of Hermione has already become a favorite among fans."[57]		Before the production of Half-Blood Prince, Watson considered not returning,[58] but eventually decided that "the pluses outweighed the minuses" and that she could not bear to see anyone else play Hermione.[59]		Watson has said that Hermione is a character who makes "brain not beauty cool," and that though Hermione is "slightly socially inept," she is "not ashamed of herself."[60] When filming Chamber of Secrets, Watson was "adamant" that she wasn't like Hermione, but she reflects that "as I got older, I realised she was the greatest role model a girl could have."[61] In 2007, before the release of Order of the Phoenix, Watson said, "There are too many stupid girls in the media. Hermione's not scared to be clever. I think sometimes really smart girls dumb themselves down a bit, and that's bad. When I was nine or ten, I would get really upset when they tried to make me look geeky, but now I absolutely love it. I find it's so much pressure to be beautiful. Hermione doesn't care what she looks like. She's a complete tomboy."[59]		Screenwriter Steve Kloves revealed in a 2003 interview that Hermione was his favourite character. "There's something about her fierce intellect coupled with a complete lack of understanding of how she affects people sometimes that I just find charming and irresistible to write."[9]		In Harry Potter and the Cursed Child, Hermione Granger is played by the South African-born actress Noma Dumezweni, also known for her work in Linda, A Raisin in the Sun and A Human Being Died That Night.[62] Dumezweni described the role as a "privilege and a responsibility" and said that "we all aspire to be Hermione."[63] The choice of a black actress to play the part led to criticism on social media, which J.K. Rowling described as being by "a bunch of racists", adding that the books never explicitly mentioned her race or skin colour, and that she has been portrayed as black in fan art.[64] Emma Watson also expressed her support for the actress, tweeting that she looked forward to seeing her in the role.[65] Dumezweni herself called the backlash "so unimaginative",[66] stating that "So many young actors and actresses have told me that they’re so pleased I’m playing Hermione because they can see a version of themselves on the stage."[67]		Dumezweni received praise for her performance; The Independent commented that she "did a tremendous job as the stern witch."[68] At the 2017 Laurence Olivier Awards, Dumezweni received the Award for Best Actress in a Supporting Role for her portrayal of Hermione.[67]		Hermione has been parodied in numerous sketches and animated series. On Saturday Night Live, Hermione was played by Lindsay Lohan.[69] On his show Big Impression, Alistair McGowan did a sketch called "Louis Potter and the Philosopher's Scone". It featured impressions of Nigella Lawson as Hermione.[70] In 2003, Comic Relief performed a spoof story called Harry Potter and the Secret Chamberpot of Azerbaijan, in which Miranda Richardson, who plays Rita Skeeter in the Harry Potter films, featured as Hermione.[71][72] Hermione also features in the Harry Bladder sketches in All That, in which she appears as Herheiny and is portrayed by Lisa Foiles. The Wedge, an Australian sketch comedy, parodies Hermione and Harry in love on a "Cooking With..." show before being caught by Snape.[73] Hermione also appears as Hermione Ranger in Harry Podder: Dude Where's My Wand?, a play by Desert Star Theater in Utah, written by sisters Laura J., Amy K. and Anna M. Lewis.[74][75] In the 2008 American comedy film Yes Man, Allison (played by Zooey Deschanel) accompanies Carl (Jim Carrey) to a Harry Potter-themed party dressed as Hermione.		In Harry Cover, a French comic book parody of the Harry Potter series by Pierre Veys (subsequently translated into Spanish and English), Hermione appears as Harry Cover's friend Hormone.[76] Hermione also appears in The Potter Puppet Pals sketches by Neil Cicierega, and in the A Very Potter Musical, A Very Potter Sequel, and A Very Potter Senior Year musicals by StarKid Productions played by Bonnie Gruesen in the first two and Meredith Stepien in the third.		
Psychology Today is a magazine published every two months in the United States.						Founded in 1967[2] by Nicolas Charney, Ph.D, its intent is to make psychology literature more accessible to the general public. The magazine focuses on behavior and covers a range of topics including psychology, neuroscience, relationships, sexuality, parenting, health (including from the perspectives of alternative medicine), work,[3] and the psychological aspects of current affairs.[1]		The magazine's website features therapy and health professionals directories[1] and hundreds of blogs written by a wide variety of psychologists, psychiatrists, social workers, medical doctors, anthropologists, sociologists, and science journalists. Its current editor-in-chief is Kaja Perina.[4]		In 1976 Psychology Today sold 1,026,872 copies.[2] The circulation of the magazine was 1,171,362 copies in 1981 and 862,193 copies in 1986.[2]		It has a circulation of 275,000 copies per issue as of 2013 and claims 14.1 people read each copy for a total audience of 3,877,500.[1] From June 2010 to June 2011, it was the one of the top ten consumer magazines by newsstand sales.[5] In recent years, while many magazines have suffered in readership declines, Adweek, in 2013, noted Psychology Today's 36 percent increase in number of readers.[6]		Owned and managed by the American Psychological Association from 1983 to 1987,[7] the publication is currently endorsed by the National Board for Certified Counselors, which promotes subscriptions and offers professional credit for a small fee and assigned assessment for each article read.[8]		
The New York Times (sometimes abbreviated NYT and The Times) is an American daily newspaper, founded and continuously published in New York City since September 18, 1851, by The New York Times Company. The New York Times has won 122 Pulitzer Prizes, more than any other newspaper.[6][7][8][9] The paper's print version in 2013 had the second-largest circulation, behind The Wall Street Journal, and the largest circulation among the metropolitan newspapers in the United States. The New York Times is ranked 18th in the world by circulation. Following industry trends, its weekday circulation had fallen in 2009 to fewer than one million.[10]		Nicknamed "The Gray Lady",[11] The New York Times has long been regarded within the industry as a national "newspaper of record".[12] It has been owned by the Ochs-Sulzberger family since 1896; Arthur Ochs Sulzberger Jr., the publisher of the Times and the chairman of the New York Times Company, is the fourth generation of the family to helm the paper.[13] The New York Times international version, formerly the International Herald Tribune, is now called the New York Times International Edition.[14] The paper's motto, "All the News That's Fit to Print", appears in the upper left-hand corner of the front page.		Since the mid-1970s, The New York Times has greatly expanded its layout and organization, adding special weekly sections on various topics supplementing the regular news, editorials, sports, and features. Since 2008,[15] The New York Times has been organized into the following sections: News, Editorials/Opinions-Columns/Op-Ed, New York (metropolitan), Business, Sports of The Times, Arts, Science, Styles, Home, Travel, and other features.[16] On Sunday, The New York Times is supplemented by the Sunday Review (formerly the Week in Review),[17] The New York Times Book Review,[18] The New York Times Magazine[19] and T: The New York Times Style Magazine (T is published 13 times a year).[20] The New York Times stayed with the broadsheet full page set-up (as some others have changed into a tabloid lay-out) and an eight-column format for several years, after most papers switched to six,[21] and was one of the last newspapers to adopt color photography, especially on the front page.[22]		The New York Times was founded as the New-York Daily Times on September 18, 1851,[a] published by Raymond, Jones & Company (raising about $70,000);[24] by journalist and politician Henry Jarvis Raymond (1820–69), then a Whig Party member and later second chairman of the newly organized Republican Party National Committee, and former banker George Jones. Other early investors of the company were Edwin B. Morgan,[25] Christopher Morgan,[26] and Edward B. Wesley.[27] Sold for a penny (equivalent to 29 cents today), the inaugural edition attempted to address various speculations on its purpose and positions that preceded its release:[28]		We shall be Conservative, in all cases where we think Conservatism essential to the public good;—and we shall be Radical in everything which may seem to us to require radical treatment and radical reform. We do not believe that everything in Society is either exactly right or exactly wrong;—what is good we desire to preserve and improve;—what is evil, to exterminate, or reform.		In 1852, the newspaper started a western division, The Times of California that arrived whenever a mail boat got to California. However, when local California newspapers came into prominence, the effort failed.[29]		The newspaper shortened its name to The New-York Times on September 14, 1857. It dropped the hyphen in the city name on December 1, 1896.[30] On April 21, 1861, The New York Times departed from its original Monday–Saturday publishing schedule and joined other major dailies in adding a Sunday edition to offer daily coverage of the Civil War. One of the earliest public controversies it was involved with was the Mortara Affair, the subject of twenty editorials it published alone.[31]		The main office of The New York Times was attacked during the New York Draft Riots sparked by the beginning of military conscription for the Northern Union Army now instituted in the midst of the Civil War on July 13, 1863. At "Newspaper Row", across from City Hall, Henry Raymond, owner and editor of The New York Times, averted the rioters with "Gatling" (early machine, rapid-firing) guns, one of which he manned himself. The mob now diverted, instead attacked the headquarters of abolitionist publisher Horace Greeley's New York Tribune until forced to flee by the Brooklyn City Police, who had crossed the East River to help the Manhattan authorities.[32]		In 1869, Raymond died, and George Jones took over as publisher.[33]		The newspaper's influence grew during 1870–1 when it published a series of exposés on William Magear ("Boss") Tweed, leader of the city's Democratic Party—popularly known as "Tammany Hall" (from its early 19th Century meeting headquarters)—that led to the end of the "Tweed Ring's" domination of New York's City Hall.[34] Tweed offered The New York Times five million dollars (equivalent to more than 100 million dollars today) to not publish the story.[25] In the 1880s, The New York Times transitioned gradually from editorially supporting Republican Party candidates to becoming more politically independent and analytical.[35] In 1884, the paper supported Democrat Grover Cleveland (former Mayor of Buffalo and Governor of New York State) in his first presidential campaign.[36] While this move cost The New York Times' readership among its more progressive and Republican readers (the revenue went down from $188,000 to $56,000 from 1883-4- however some part of this was due to the price going down to two cents, in order to compete with the World and Sun), the paper eventually regained most of its lost ground within a few years.[37] After George Jones died in 1891, Charles Ransom Miller raised $1 million dollars to buy the Times, along with other fellow editors at the newspaper, printing it under the New York Times Publishing Company.[38][39] However, the newspaper was financially crippled by the Panic of 1893.[37] By 1896, The New York Times had a circulation of less than 9,000, and was losing $1,000 a day when controlling interest in it was gained by Adolph Ochs, publisher of the Chattanooga Times for $75,000.[40]		Shortly after assuming control of the paper, Ochs coined the paper's slogan, "All The News That's Fit To Print". The slogan has appeared in the paper since September 1896,[41] and has been printed in a box in the upper left hand corner of the front page since early 1897.[36] This was a jab at competing papers such as Joseph Pulitzer's New York World and William Randolph Hearst's New York Journal which were now being known for a lurid, sensationalist and often inaccurate reporting of facts and opinions known by the end of the century as "yellow journalism".[42] Under Ochs' guidance, continuing and expanding upon the Henry Raymond tradition, (which were from the era of James Gordon Bennett of the New York Herald which predated Pulitzer and Hearst's arrival in New York), The New York Times achieved international scope, circulation, and reputation (the Sunday circulation went from 9,000 in 1896 to 780,000 in 1934).[40] In 1904, The New York Times, along with The Times received the first on-the-spot wireless telegraph transmission from a naval battle, a report of the destruction of the Imperial Russian Navy's Baltic Fleet at the Battle of Port Arthur in the Straits of Tsushima off the eastern coast of Korea in the Yellow Sea in the western Pacific Ocean after just sailing across the globe from Europe from the press-boat Haimun during the Russo-Japanese War .[43] In 1910, the first air delivery of The New York Times to Philadelphia began.[36] The New York Times' first trans-Atlantic delivery by air to London occurred in 1919 by dirigible. In 1920, a "4 A.M. Airplane Edition" was sent by plane to Chicago so it could be in the hands of Republican convention delegates by evening.[44]		In the 1940s, the paper extended its breadth and reach. The crossword began appearing regularly in 1942, and the fashion section in 1946. The New York Times began an international edition in 1946. The international edition stopped publishing in 1967, when The New York Times joined the owners of the New York Herald Tribune and The Washington Post to publish the International Herald Tribune in Paris. The paper bought AM radio station WQXR (1560 kHz) in 1944.[45] Its "sister" FM station, WQXQ, would become WQXR-FM (96.3 MHz). Branded as "The Radio Stations of The New York Times", its classical music radio format was simulcast on both the AM & FM frequencies until December 1992, when the big-band and pop standards music format of station WNEW (1130 kHz – now WBBR/"Bloomberg Radio") was transferred to and adopted by WQXR; in recognition of the format change, WQXR changed its call letters to WQEW (a "hybrid" combination of "WQXR" and "WNEW").[46] By 1999, The New York Times was leasing WQEW to ABC Radio for its "Radio Disney" format.[47] In 2007, WQEW was finally purchased by Disney; in late 2014, it was sold to Family Radio (a religious radio network) and became WFME.[48] On July 14, 2009, it was announced that WQXR-FM would be sold to the WNYC radio group who, on October 8, 2009, moved the station from 96.3 to 105.9 MHz (swapping frequencies with Spanish-language station WXNY-FM, which wanted the more powerful transmitter to increase its coverage) and began operating it as a non-commercial, public radio station.[49] After the purchase, WQXR-FM retained the classical music format, whereas WNYC-FM (93.9 MHz) abandoned it, switching to a talk radio format.		On September 14, 1987, the Times printed the heaviest ever newspaper, at over 12 pounds (5.4 kg) and 1,612 pages.[50]		In 2009, the newspaper began production of local inserts in regions outside of the New York area. Beginning October 16, 2009, a two-page "Bay Area" insert was added to copies of the Northern California edition on Fridays and Sundays. The newspaper commenced production of a similar Friday and Sunday insert to the Chicago edition on November 20, 2009. The inserts consist of local news, policy, sports, and culture pieces, usually supported by local advertisements.		In addition to its New York City headquarters, the newspaper has ten news bureaus in the New York region, eleven national news bureaus and 26 foreign news bureaus.[51] The New York Times reduced its page width to 12 inches (300 mm) from 13.5 inches (340 mm) on August 6, 2007, adopting the width that has become the U.S. newspaper industry standard.[52]		In February 2013, the paper stopped offering lifelong positions for its journalists and editors.[53][relevant? – discuss]		Because of its steadily declining sales attributed to the rise of online alternative media and social media, the newspaper has been going through a downsizing for several years, offering buyouts to workers and cutting expenses,[54] in common with a general trend among print news media.[55]		In 2016, reporters for the newspaper were reportedly the target of cyber security breaches. The Federal Bureau of Investigation was reportedly investigating the attacks. The cyber security breaches have been described as possibly being related to cyberattacks that targeted other institutions, such as the Democratic National Committee.[56]		The newspaper's first building was located at 113 Nassau Street in New York City. In 1854, it moved to 138 Nassau Street, and in 1858 to 41 Park Row, making it the first newspaper in New York City housed in a building built specifically for its use.[57]		The newspaper moved its headquarters to the Times Tower, located at 1475 Broadway in 1904,[58] in an area called Longacre Square, that was later renamed Times Square in honor of the newspaper.[59] The top of the building – now known as One Times Square – is the site of the New Year's Eve tradition of lowering a lighted ball, which was started by the paper.[60] The building is also notable for its electronic news ticker – popularly known as "The Zipper" – where headlines crawl around the outside of the building.[61] It is still in use, but has been operated by Dow Jones & Company since 1995.[62] After nine years in its Times Square tower the newspaper had an annex built at 229 West 43rd Street.[63] After several expansions, the 43rd Street building became the newspaper's main headquarters in 1960 and the Times Tower on Broadway was sold the following year.[64] It served as the newspaper's main printing plant until 1997, when the newspaper opened a state-of-the-art printing plant in the College Point section of the borough of Queens.[65]		A decade later, The New York Times moved its newsroom and businesses headquarters from West 43rd Street to a new tower at 620 Eighth Avenue between West 40th and 41st Streets, in Manhattan – directly across Eighth Avenue from the Port Authority Bus Terminal. The new headquarters for the newspaper, known officially as The New York Times Building but unofficially called the new "Times Tower" by many New Yorkers, is a skyscraper designed by Renzo Piano.[66][67]		The paper's involvement in a 1964 libel case helped bring one of the key United States Supreme Court decisions supporting freedom of the press, New York Times Co. v. Sullivan. In it, the United States Supreme Court established the "actual malice" standard for press reports about public officials or public figures to be considered defamatory or libelous. The malice standard requires the plaintiff in a defamation or libel case prove the publisher of the statement knew the statement was false or acted in reckless disregard of its truth or falsity. Because of the high burden of proof on the plaintiff, and difficulty in proving malicious intent, such cases by public figures rarely succeed.[68]		In 1971, the Pentagon Papers, a secret United States Department of Defense history of the United States' political and military involvement in the Vietnam War from 1945 to 1967, were given ("leaked") to Neil Sheehan of The New York Times by former State Department official Daniel Ellsberg, with his friend Anthony Russo assisting in copying them. The New York Times began publishing excerpts as a series of articles on June 13. Controversy and lawsuits followed. The papers revealed, among other things, that the government had deliberately expanded its role in the war by conducting air strikes over Laos, raids along the coast of North Vietnam, and offensive actions taken by U.S. Marines well before the public was told about the actions, all while President Lyndon B. Johnson had been promising not to expand the war. The document increased the credibility gap for the U.S. government, and hurt efforts by the Nixon administration to fight the ongoing war.[69]		When The New York Times began publishing its series, President Richard Nixon became incensed. His words to National Security Advisor Henry Kissinger included "People have gotta be put to the torch for this sort of thing..." and "Let's get the son-of-a-bitch in jail."[70] After failing to get The New York Times to stop publishing, Attorney General John Mitchell and President Nixon obtained a federal court injunction that The New York Times cease publication of excerpts. The newspaper appealed and the case began working through the court system. On June 18, 1971, The Washington Post began publishing its own series. Ben Bagdikian, a Post editor, had obtained portions of the papers from Ellsberg. That day the Post received a call from the Assistant Attorney General, William Rehnquist, asking them to stop publishing. When the Post refused, the U.S. Justice Department sought another injunction. The U.S. District court judge refused, and the government appealed. On June 26, 1971, the U.S. Supreme Court agreed to take both cases, merging them into New York Times Co. v. United States 403 US 713. On June 30, 1971, the Supreme Court held in a 6–3 decision that the injunctions were unconstitutional prior restraints and that the government had not met the burden of proof required. The justices wrote nine separate opinions, disagreeing on significant substantive issues. While it was generally seen as a victory for those who claim the First Amendment enshrines an absolute right to free speech, many felt it a lukewarm victory, offering little protection for future publishers when claims of national security were at stake.[69]		Discriminatory practices restricting women in editorial positions were previously used by the paper. The newspaper's first general woman reporter was Jane Grant, who described her experience afterwards. She wrote, "In the beginning I was charged not to reveal the fact that a female had been hired". Other reporters nicknamed her Fluff and she was subjected to considerable hazing. Because of her gender, promotions were out of the question, according to the then-managing editor. She was there for fifteen years, interrupted by World War I.[71]		In 1935, Anne McCormick wrote to Arthur Hays Sulzberger, "I hope you won't expect me to revert to 'woman's-point-of-view' stuff."[72] Later, she interviewed major political leaders and appears to have had easier access than her colleagues did. Even those who witnessed her in action were unable to explain how she got the interviews she did.[73] Clifton Daniel said, "[After World War II,] I'm sure Adenauer called her up and invited her to lunch. She never had to grovel for an appointment."[74] Covering world leaders' speeches after World War II at the National Press Club was limited to men by a Club rule. When women were eventually allowed in to hear the speeches, they still were not allowed to ask the speakers questions, although men were allowed and did ask, even though some of the women had won Pulitzer Prizes for prior work.[75] Times reporter Maggie Hunter refused to return to the Club after covering one speech on assignment.[76] Nan Robertson's article on the Union Stock Yards, Chicago, was read aloud as anonymous by a professor, who then said, "'It will come as a surprise to you, perhaps, that the reporter is a girl,' he began... [G]asps; amazement in the ranks. 'She had used all her senses, not just her eyes, to convey the smell and feel of the stockyards. She chose a difficult subject, an offensive subject. Her imagery was strong enough to revolt you.'"[77] The New York Times hired Kathleen McLaughlin after ten years at the Chicago Tribune, where "[s]he did a series on maids, going out herself to apply for housekeeping jobs."[78]		The New York Times published on December 20, 2012, an interactive storytelling in longform multimedia, Snow Fall: The Avalanche at Tunnel Creek by reporter John Branch about the 2012 Tunnel Creek avalanche. The six-part story, which integrated video, photos, and graphics, was hailed as a watershed moment for the journalism industry.[79][80] The feature was awarded a Peabody Award, which called the piece a "spectacular example of the potential of digital-age storytelling" which "combines thorough traditional reporting of a deadly avalanche with stunning topographic video."[81] Snow Fall inspired the Times to appoint Sam Sifton "Snowfaller in Chief," expanding multimedia narratives in the newsroom in the tradition of Snow Fall.[82]		In 1896, Adolph Ochs bought The New York Times, a money-losing newspaper, and formed the New York Times Company. The Ochs-Sulzberger family, one of the United States' newspaper dynasties, has owned The New York Times ever since.[36] The publisher went public on January 14, 1969, trading at $42 a share on the American Stock Exchange.[83] After this, the family continued to exert control through its ownership of the vast majority of Class B voting shares. Class A shareholders are permitted restrictive voting rights while Class B shareholders are allowed open voting rights.		The Ochs-Sulzberger family trust controls roughly 88 percent of the company's class B shares. Any alteration to the dual-class structure must be ratified by six of eight directors who sit on the board of the Ochs-Sulzberger family trust. The Trust board members are Daniel H. Cohen, James M. Cohen, Lynn G. Dolnick, Susan W. Dryfoos, Michael Golden, Eric M. A. Lax, Arthur O. Sulzberger, Jr. and Cathy J. Sulzberger.[84]		Turner Catledge, the top editor at The New York Times from 1952 to 1968, wanted to hide the ownership influence. Arthur Sulzberger routinely wrote memos to his editor, each containing suggestions, instructions, complaints, and orders. When Catledge would receive these memos he would erase the publisher's identity before passing them to his subordinates. Catledge thought that if he removed the publisher's name from the memos it would protect reporters from feeling pressured by the owner.[85]		On January 20, 2009, The New York Times reported that its parent company, The New York Times Company, had reached an agreement to borrow $250 million from Carlos Slim, a Mexican businessman and the world's second richest person,[86] "to help the newspaper company finance its businesses".[87] The New York Times Company later repaid that loan ahead of schedule.[88] Since then, Slim has bought large quantities of the company's Class A shares, which are available for purchase by the public and offer less control over the company than Class B shares, which are privately held.[88] Slim's investments in the company included large purchases of Class A shares in 2011, when he increased his stake in the company to 8.1% of Class A shares,[89] and again in 2015, when he exercised stock options—acquired as part of a repayment plan on the 2009 loan—to purchase 15.9 million Class A shares.[88] As of March 7, 2016, Slim owned 17.4% of the company's Class A shares, according to annual filings submitted by the company.[90][91]		Although Slim is the largest shareholder in the company, his investment does not give him the ability to control the newspaper, as his stake allows him to vote only for Class A directors, who compose just a third of the company's board.[88] According to the company's 2016 annual filings, Slim did not own any of the company's Class B shares.[90]		Dual-class structures caught on in the mid-20th century as families such as the Grahams of The Washington Post Company sought to gain access to public capital without losing control. Dow Jones & Co., publisher of The Wall Street Journal, had a similar structure and was controlled by the Bancroft family but was later bought by News Corporation in 2007, which itself is controlled by Rupert Murdoch and his family through a similar dual-class structure.[92]		The newspaper is organized in three sections, including the magazine.		Some sections, such as Metro, are only found in the editions of the paper distributed in the New York–New Jersey–Connecticut Tri-State Area and not in the national or Washington, D.C. editions.[93] Aside from a weekly roundup of reprints of editorial cartoons from other newspapers, The New York Times does not have its own staff editorial cartoonist, nor does it feature a comics page or Sunday comics section.[94] In September 2008, The New York Times announced that it would be combining certain sections effective October 6, 2008, in editions printed in the New York metropolitan area. The changes folded the Metro Section into the main International / National news section and combined Sports and Business (except Saturday through Monday, when Sports is still printed as a standalone section). This change also included having the name of the Metro section be called New York outside of the Tri-State Area. The presses used by The New York Times allow four sections to be printed simultaneously; as the paper had included more than four sections all days except Saturday, the sections had to be printed separately in an early press run and collated together. The changes will allow The New York Times to print in four sections Monday through Wednesday, in addition to Saturday. The New York Times' announcement stated that the number of news pages and employee positions will remain unchanged, with the paper realizing cost savings by cutting overtime expenses.[15] According to Russ Stanton, editor of the Los Angeles Times, a competitor, the newsroom of The New York Times is twice the size of the Los Angeles Times, which has a newsroom of 600.[95] In March 2014, Vanessa Friedman was named the "fashion director and chief fashion critic" of The New York Times.[96]		When referring to people, The New York Times generally uses honorifics, rather than unadorned last names (except in the sports pages, Book Review and Magazine).[97] It stayed with an eight-column format until September 7, 1976, years after other papers had switched to six,[21] and it was one of the last newspapers to adopt color photography, with the first color photograph on the front page appearing on October 16, 1997.[22] In the absence of a major headline, the day's most important story generally appears in the top-right column, on the main page. The typefaces used for the headlines are custom variations of Cheltenham. The running text is set at 8.7 point Imperial.[98][99]		Joining a roster of other major American newspapers in the last ten years, including USA Today, The Wall Street Journal and The Washington Post, The New York Times announced on July 18, 2006, that it would be narrowing the width of its paper by six inches. In an era of dwindling circulation and significant advertising revenue losses for most print versions of American newspapers, the move, which would result in a five percent reduction in news coverage, would have a target savings of $12 million a year for the paper.[100] The change from the traditional 54 inches (1.4 m) broadsheet style to a more compact 48-inch web width (12-inch page width) was addressed by both Executive Editor Bill Keller and The New York Times President Scott Heekin-Canedy in memos to the staff. Keller defended the "more reader-friendly" move indicating that in cutting out the "flabby or redundant prose in longer pieces" the reduction would make for a better paper. Similarly, Keller confronted the challenges of covering news with "less room" by proposing more "rigorous editing" and promised an ongoing commitment to "hard-hitting, ground-breaking journalism".[101] The official change went into effect on August 6, 2007.[102]		The New York Times printed a display advertisement on its first page on January 6, 2009, breaking tradition at the paper.[103] The advertisement, for CBS, was in color and ran the entire width of the page.[104] The newspaper promised it would place first-page advertisements on only the lower half of the page.[103]		In August 2014, The Times decided to use the word "torture" to describe incidents in which interrogators "inflicted pain on a prisoner in an effort to get information." This was a shift from the paper's previous practice of describe such practices as "harsh" or "brutal" interrogations.[105]		The paper maintains a strict profanity policy. A 2007 review of a concert by punk band Fucked Up, for example, completely avoided mention of the group's name.[106] However, the Times has on occasion published unfiltered video content that includes profanity and slurs where it has determined that such video has news value.[107] During the 2016 U.S. presidential election campaign, the Times did print the words "fuck" and "pussy," among others, when reporting on the vulgar statements made by Donald Trump in a 2005 recording. Times politics editor Carolyn Ryan said: "It's a rare thing for us to use this language in our stories, even in quotes, and we discussed it at length," ultimately deciding to publish it because of its news value and because "[t]o leave it out or simply describe it seemed awkward and less than forthright to us, especially given that we would be running a video that showed our readers exactly what was said."[108]		The New York Times has won 122 Pulitzer Prizes, more than any other newspaper. The prize is awarded for excellence in journalism in a range of categories.[6]		It has also won three Peabody Awards (and jointly received two). A Peabody award was given in 2003 for the documentary Frontline: A Dangerous Business, a joint investigation by the New York Times, the Canadian Broadcasting Corporation, and WGBH's Frontline about the conditions faced by workers at McWayne Inc.[109] The newspaper won another Peabody award (in 1951) for the New York Times Youth Forum (which featured "unrehearsed discussion by students selected from private, public and parochial schools, on topics ranging from the political, educational and scientific to the international and the United Nations.")[110] Again in 2008, the newspaper won another reward for "Aggressively and creatively adding sound and moving images to its traditional package of news and features, The New York Times has stepped forward as an innovator in online journalism. Its website exemplifies a new age for the press, expanding its role in ways unimaginable only a few years ago."[111] In 2013, the Times, along with The National Film Board of Canada won a Peabody Award for the documentary "A Short History of the Highrise".[112] A personal award was also given to then chief media critic Jack Gould in 1956.[113]		The New York Times began publishing daily on the World Wide Web on January 22, 1996, "offering readers around the world immediate access to most of the daily newspaper's contents."[114] Since its online launch, the newspaper has consistently been ranked one of the top websites. Accessing some articles requires registration, though this could be bypassed in some cases through Times RSS feeds.[115] The website had 555 million pageviews in March 2005.[116] The domain nytimes.com attracted at least 146 million visitors annually by 2008 according to a Compete.com study. The New York Times Web site ranks 59th by number of unique visitors, with over 20 million unique visitors in March 2009 making it the most visited newspaper site with more than twice the number of unique visitors as the next most popular site.[117] as of May 2009[update], nytimes.com produced 22 of the 50 most popular newspaper blogs.[118] NYTimes.com is ranked 118 in the world, and 32 in the U.S. by Alexa (as of June 4, 2017).[119]		In September 2005, the paper decided to begin subscription-based service for daily columns in a program known as TimesSelect, which encompassed many previously free columns. Until being discontinued two years later, TimesSelect cost $7.95 per month or $49.95 per year,[120] though it was free for print copy subscribers and university students and faculty.[121][122] To avoid this charge, bloggers often reposted TimesSelect material,[123] and at least one site once compiled links of reprinted material.[124] On September 17, 2007, The New York Times announced that it would stop charging for access to parts of its Web site, effective at midnight the following day, reflecting a growing view in the industry that subscription fees cannot outweigh the potential ad revenue from increased traffic on a free site.[125] In addition to opening almost the entire site to all readers, The New York Times news archives from 1987 to the present are available at no charge, as well as those from 1851 to 1922, which are in the public domain.[126][127] Access to the Premium Crosswords section continues to require either home delivery or a subscription for $6.95 per month or $39.95 per year. Times columnists including Nicholas Kristof and Thomas Friedman had criticized TimesSelect,[128][129] with Friedman going so far as to say "I hate it. It pains me enormously because it's cut me off from a lot, a lot of people, especially because I have a lot of people reading me overseas, like in India ... I feel totally cut off from my audience."[130]		The New York Times was made available on the iPhone and iPod Touch in 2008,[131] and on the iPad mobile devices in 2010.[132] It was also the first newspaper to offer a video game as part of its editorial content, Food Import Folly by Persuasive Games.[133] In 2010, The New York Times editors collaborated with students and faculty from New York University's Studio 20 Journalism Masters program to launch and produce "The Local East Village", a hyperlocal blog designed to offer news "by, for and about the residents of the East Village".[134] That same year, reCAPTCHA helped to digitize old editions of The New York Times.[135]		In 2012, The New York Times introduced a Chinese-language news site, cn.nytimes.com, with content created by staff based in Shanghai, Beijing and Hong Kong, though the server was placed outside of China to avoid censorship issues.[136] In March 2013, The New York Times and National Film Board of Canada announced a partnership titled A Short History of the Highrise, which will create four short documentaries for the Internet about life in highrise buildings as part of the NFB's Highrise project, utilizing images from the newspaper's photo archives for the first three films, and user-submitted images for the final film.[137] The third project in the series, "A Short History of the Highrise", won a Peabody Award in 2013.[138]		Falling print advertising revenue and projections of continued decline resulted in a paywall being instituted in 2011, regarded as modestly successful after garnering several hundred thousand subscriptions and about $100 million in revenue as of March 2012[update].[139] The paywall was announced on March 17, 2011, that starting on March 28, 2011 (March 17, 2011, for Canada), it would charge frequent readers for access to its online content.[140] Readers would be able to access up to 20 articles each month without charge. (Although beginning in April 2012, the number of free-access articles was halved to just ten articles per month.) Any reader who wanted to access more would have to pay for a digital subscription. This plan would allow free access for occasional readers, but produce revenue from "heavy" readers. Digital subscriptions rates for four weeks range from $15 to $35 depending on the package selected, with periodic new subscriber promotions offering four-week all-digital access for as low as 99¢. Subscribers to the paper's print edition get full access without any additional fee. Some content, such as the front page and section fronts will remain free, as well as the Top News page on mobile apps.[141] In January 2013, The New York Times' Public Editor Margaret M. Sullivan announced that for the first time in many decades, the paper generated more revenue through subscriptions than through advertising.[142]		The newspaper's website was hacked on August 29, 2013, by the Syrian Electronic Army, a hacking group that supports the government of Syrian President Bashar al-Assad. The SEA managed to penetrate the paper's domain name registrar, Melbourne IT, and alter DNS records for The New York Times, putting some of its websites out of service for hours.[143]		The food section is supplemented on the web by properties for home cooks and for out-of-home dining. New York Times Cooking (cooking.nytimes.com; also available via iOS app) provides access to more than 17,000 recipes on file as of November 2016,[144] and availability of saving recipes from other sites around the web. The newspaper's restaurant search (nytimes.com/reviews/dining) allows online readers to search NYC area restaurants by cuisine, neighborhood, price, and reviewer rating. The New York Times has also published several cookbooks, including The Essential New York Times Cookbook: Classic Recipes for a New Century, published in late 2010.		The Times Reader is a digital version of The New York Times. It was created via a collaboration between the newspaper and Microsoft. Times Reader takes the principles of print journalism and applies them to the technique of online reporting. Times Reader uses a series of technologies developed by Microsoft and their Windows Presentation Foundation team. It was announced in Seattle in April 2006, by Arthur Ochs Sulzberger Jr., Bill Gates, and Tom Bodkin.[145] In 2009, the Times Reader 2.0 was rewritten in Adobe AIR.[146] In December 2013, the newspaper announced that the Times Reader app would be discontinued on January 6, 2014, urging readers of the app to instead begin using the subscription-only "Today's Paper" app.[147]		In 2008, The New York Times created an app for the iPhone and iPod Touch which allowed users to download articles to their mobile device enabling them to read the paper even when they were unable to receive a signal.[148] In April 2010, The New York Times announced it would begin publishing daily content through an iPad app.[149] As of October 2010[update], The New York Times iPad app is ad-supported and available for free without a paid subscription, but translated into a subscription-based model in 2011.[132]		In 2010, the newspaper also launched an app for Android smartphones, followed later by an app for Windows Phones.[150]		The New York Times began producing podcasts in 2006. Among the early podcasts were Inside The Times and Inside The New York Times Book Review. Several of the Times podcasts were cancelled in 2012.[151][152] The Times returned to launching new podcasts in 2016, including Modern Love with WBUR.[153] On January 30, 2017, The New York Times launched a new podcast The Daily.[154][155]		In June 2012, The New York Times launched its first official foreign-language variant, cn.nytimes.com, in Chinese,[156] viewable in both traditional and simplified Chinese characters. The project was led by Craig S. Smith on the business side and Philip P. Pan on the editorial side.		The site's initial success was interrupted in October that year following the publication of an investigative article[b] by David Barboza about the finances of Chinese Premier Wen Jiabao's family.[157] In retaliation for the article, the Chinese government blocked access to both nytimes.com and cn.nytimes.com inside the People's Republic of China (PRC).		Despite Chinese government interference, however, the Chinese-language operations have continued to develop, adding a second site, cn.nytstyle.com, iOS and Android apps and newsletters, all of which are accessible inside the PRC. The China operations also produce three print publications in Chinese. Traffic to cn.nytimes.com, meanwhile, has risen due to the widespread use of VPN technology in the PRC and to a growing Chinese audience outside mainland China.[158] New York Times articles are also available to users in China via the use of mirror websites, apps, domestic newspapers, and social media.[158][159] The Chinese platforms now represent one of The New York Times' top five digital markets globally. The editor-in-chief of the Chinese platforms is Ching-Ching Ni.[160]		The website's "Newsroom Navigator" collects online resources for use by reporters and editors. It is maintained by Rich Meislin.[161][162][163] Further specific collections are available to cover the subjects of business, politics and health.[161][164][165] In 1998, Meislin was editor-in-chief of electronic media at the newspaper.[166]		Because of holidays, no editions were printed on November 23, 1851; January 2, 1852; July 4, 1852; January 2, 1853; and January 1, 1854.[167]		Because of strikes, the regular edition of The New York Times was not printed during the following periods:[168]		The New York Times editorial page is often regarded as liberal.[171][172] In mid-2004, the newspaper's then public editor (ombudsman), Daniel Okrent, wrote that "the Op-Ed page editors do an evenhanded job of representing a range of views in the essays from outsiders they publish – but you need an awfully heavy counterweight to balance a page that also bears the work of seven opinionated columnists, only two of whom could be classified as conservative (and, even then, of the conservative subspecies that supports legalization of gay unions and, in the case of William Safire, opposes some central provisions of the Patriot Act."[173]		The New York Times has not endorsed a Republican for president since Dwight D. Eisenhower in 1956; since 1960, it has endorsed the Democratic nominee in every presidential election (see New York Times presidential endorsements).[174] However, the Times did endorse incumbent Republican Mayors of New York City Rudy Giuliani in 1997[175] and Michael Bloomberg in 2005[176] and 2009.[177] The Times also endorsed Republican Governor George Pataki in 2002.[178]		The New York Times supported the 2003 invasion of Iraq.[179] On May 26, 2004, a year after the war started, the newspaper asserted that some of its articles had not been as rigorous as they should have been, and were insufficiently qualified, frequently overly dependent upon information from Iraqi exiles desiring regime change.[180] Reporter Judith Miller retired after criticisms that her reporting of the lead-up to the Iraq War was factually inaccurate and overly favorable to the Bush administration's position, for which The New York Times later apologized.[181][182] One of Miller's prime sources was Ahmed Chalabi, an Iraqi expatriate who returned to Iraq after the U.S. invasion and held a number of governmental positions culminating in acting oil minister and deputy prime minister from May 2005 until May 2006.[183][184]		A 2015 study found that The New York Times fed into an overarching tendency towards national bias. During the Iranian nuclear crisis the newspaper minimized the "negative processes" of the United States while overemphasizing similar processes of Iran. This tendency was shared by other papers such as The Guardian, Tehran Times, and the Fars News Agency, while Xinhua News Agency was found to be more neutral while at the same time mimicking the foreign policy of the Peoples' Republic of China.[185]		A 2003 study in The Harvard International Journal of Press/Politics concluded that The New York Times reporting was more favorable to Israelis than to Palestinians.[186] A 2002 study published in the journal Journalism examined Middle East coverage of the Second Intifada over a one-month period in the Times, Washington Post and Chicago Tribune. The study authors said that the Times was "the most slanted in a pro-Israeli direction" with a bias "reflected ... in its use of headlines, photographs, graphics, sourcing practices and lead paragraphs."[187]		For its coverage of the Israeli–Palestinian conflict, some (such as Ed Koch) have claimed that the paper is pro-Palestinian, while others (such as As`ad AbuKhalil) have insisted that it is pro-Israel.[188][189] The Israel Lobby and U.S. Foreign Policy, by political science professors John Mearsheimer and Stephen Walt, alleges that The New York Times sometimes criticizes Israeli policies but is not even-handed and is generally pro-Israel.[190] On the other hand, the Simon Wiesenthal Center has criticized The New York Times for printing cartoons regarding the Israeli-Palestinian conflict that were claimed to be anti-Semitic.[191]		Israeli Prime Minister Benjamin Netanyahu rejected a proposal to write an article for the paper on grounds of lack of objectivity. A piece in which Thomas Friedman commented that praise awarded to Netanyahu during a speech at congress was "paid for by the Israel lobby" elicited an apology and clarification from its writer.[192]		The New York Times' public editor Clark Hoyt concluded in his January 10, 2009, column, "Though the most vociferous supporters of Israel and the Palestinians do not agree, I think The New York Times, largely barred from the battlefield and reporting amid the chaos of war, has tried its best to do a fair, balanced and complete job — and has largely succeeded."[193]		On November 14, 2001, in The New York Times' 150th anniversary issue, former executive editor Max Frankel wrote that before and during World War II, the NY Times had maintained a consistent policy to minimize reports on the Holocaust in their news pages.[194] Laurel Leff, associate professor of journalism at Northeastern University, concluded that the newspaper had downplayed the Third Reich targeting of Jews for genocide. Her 2005 book Buried by the Times documents the paper's tendency before, during and after World War II to place deep inside its daily editions the news stories about the ongoing persecution and extermination of Jews, while obscuring in those stories the special impact of the Nazis' crimes on Jews in particular. Leff attributes this dearth in part to the complex personal and political views of the newspaper's Jewish publisher, Arthur Hays Sulzberger, concerning Jewishness, antisemitism, and Zionism.[195]		During the war, The New York Times journalist William L. Laurence was "on the payroll of the War Department".[196][197]		The New York Times was criticized for the work of reporter Walter Duranty, who served as its Moscow bureau chief from 1922 through 1936. Duranty wrote a series of stories in 1931 on the Soviet Union and won a Pulitzer Prize for his work at that time; however, he has been criticized for his denial of widespread famine, most particularly the Ukrainian famine in the 1930s.[198][199][200][201] In 2003, after the Pulitzer Board began a renewed inquiry, the Times hired Mark von Hagen, professor of Russian history at Columbia University, to review Duranty's work. Von Hagen found Duranty's reports to be unbalanced and uncritical, and that they far too often gave voice to Stalinist propaganda. In comments to the press he stated, "For the sake of The New York Times' honor, they should take the prize away."[202]		In the mid to late 1950s, "fashion writer[s]... were required to come up every month with articles whose total column-inches reflected the relative advertising strength of every ["department" or "specialty"] store ["assigned" to a writer]... The monitor of all this was... the advertising director [of the NYT]... " However, within this requirement, story ideas may have been the reporters' and editors' own.[203]		In May 2003, The New York Times reporter Jayson Blair was forced to resign from the newspaper after he was caught plagiarizing and fabricating elements of his stories. Some critics contended that African-American Blair's race was a major factor in his hiring and in The New York Times' initial reluctance to fire him.[204]		The newspaper was criticized for largely reporting the prosecutors' version of events in the 2006 Duke lacrosse case.[205][206] Suzanne Smalley of Newsweek criticized the newspaper for its "credulous"[207] coverage of the charges of rape against Duke University lacrosse players. Stuart Taylor, Jr. and KC Johnson, in their book Until Proven Innocent: Political Correctness and the Shameful Injustices of the Duke Lacrosse Rape Case, write: "at the head of the guilt-presuming pack, The New York Times vied in a race to the journalistic bottom with trash-TV talk shows."[206]		In February 2009, a Village Voice music blogger accused the newspaper of using "chintzy, ad-hominem allegations" in an article on British Tamil music artist M.I.A. concerning her activism against the Sinhala-Tamil conflict in Sri Lanka.[208][209] M.I.A. criticized the paper in January 2010 after a travel piece rated post-conflict Sri Lanka the "#1 place to go in 2010".[210][211] In June 2010, The New York Times Magazine published a correction on its cover article of M.I.A., acknowledging that the interview conducted by current W editor and then-Times Magazine contributor Lynn Hirschberg contained a recontextualization of two quotes.[212][213] In response to the piece, M.I.A. broadcast Hirschberg's phone number and secret audio recordings from the interview via her Twitter and website.[214][215]		The New York Times was criticized for the 13-month delay of the December 2005 story revealing the U.S. National Security Agency warrantless surveillance program.[216] Ex-NSA officials blew the whistle on the program to journalists James Risen and Eric Lichtblau, who presented an investigative article to the newspaper in November 2004, weeks before America's presidential election. Contact with former agency officials began the previous summer.[217]		Former The New York Times executive editor Bill Keller decided not to report the piece after being pressured by the Bush administration and being advised not to do so by New York Times Washington bureau chief Philip Taubman. Keller explained the silence's rationale in an interview with the newspaper in 2013, stating "Three years after 9/11, we, as a country, were still under the influence of that trauma, and we, as a newspaper, were not immune".[218]		In 2014, PBS Frontline interviewed Risen and Lichtblau, who said that the newspaper's plan was to not publish the story at all. "The editors were furious at me", Risen said to the program. "They thought I was being insubordinate." Risen wrote a book about the mass surveillance revelations after The New York Times declined the piece's publication, and only released it after Risen told them that he would publish the book. Another reporter told NPR that the newspaper "avoided disaster" by ultimately publishing the story.[219]		In September 2014, around the time when India's Mars Orbiter Mission probe was to go into Mars orbit, the International New York Times published an editorial cartoon by Singapore cartoonist Heng Kim Song depicting a turban-wearing man with a cow knocking at the door of the "Elite Space Club" with members inside reading a newspaper with a headline about India's Mars mission.[220][221]		The cartoon drew criticism, with critics saying that the cartoon was racist.[222][223][224] Times editorial page editor Andrew Rosenthal, apologized, writing in a Facebook post: "A large number of readers have complained about a recent editorial cartoon in The International New York Times, about India's foray into space exploration. The intent of the cartoonist ... was to highlight how space exploration is no longer the exclusive domain of rich, Western countries. Mr. Heng, who is based in Singapore, uses images and text – often in a provocative way – to make observations about international affairs. We apologize to readers who were offended by the choice of images in this cartoon. Mr. Heng was in no way trying to impugn India, its government or its citizens."[225][221]		On June 16, 2015, The New York Times published an article reporting the deaths of six Irish students staying in Berkeley, California when the balcony they were standing on collapsed, the paper's story insinuating that they were to blame for the collapse. The paper stated that the behavior of Irish students coming to the US on J1 visas was an "embarrassment to Ireland".[226] The Irish Taoiseach and former President of Ireland criticized the newspaper for "being insensitive and inaccurate" in its handling of the story.[227]		In May 2015, a New York Times exposé by Sarah Maslin Nir on the working conditions of manicurists in New York City and elsewhere[228] and the health hazards to which they are exposed[229] attracted wide attention, resulting in emergency workplace enforcement actions by New York governor Andrew M. Cuomo.[230] In July 2015, the story's claims of widespread illegally low wages were challenged by former New York Times reporter Richard Bernstein, in the New York Review of Books. Bernstein, whose wife owns two nail salons, asserted that such illegally low wages were inconsistent with his personal experience, and were not evidenced by ads in the Chinese-language papers cited by the story.[231] The New York Times editorial staff subsequently answered Bernstein's criticisms with examples of several published ads and stating that his response was industry advocacy.[232] The independent NYT Public Editor also reported that she had previously corresponded with Bernstein and looked into his complaints, and expressed her belief that the story's reporting was sound.[233]		In September and October 2015, nail salon owners and workers protested at The New York Times offices several times, in response to the story and the ensuing New York State crackdown.[234][235] In October 2015, Reason magazine published a three part re-reporting of the story by Jim Epstein, charging that the series was filled with misquotes and factual errors respecting both its claims of illegally low wages and health hazards. Epstein additionally argued that The New York Times had mistranslated the ads cited in its answer to Bernstein, and that those ads actually validated Bernstein's argument.[236][237][238] In November 2015, The New York Times' public editor concluded that the exposé's "findings, and the language used to express them, should have been dialed back — in some instances substantially" and recommended that "The Times write further follow-up stories, including some that re-examine its original findings and that take on the criticism from salon owners and others — not defensively but with an open mind."[239]		In April 2016, two black female employees in their sixties filed a federal class action lawsuit against The New York Times Company CEO Mark Thompson and chief revenue officer Meredith Levien, claiming age, gender, and racial discrimination. The plaintiffs claim that the Times advertising department favored younger white employees over older black employees in making firing and promotion decisions.[240][241] The Times said that the suit was "entirely without merit" and was "a series of recycled, scurrilous and unjustified attacks."[241]		New York Times public editor (ombudsman) Liz Spayd wrote in 2016 that "Conservatives and even many moderates, see in The Times a blue-state worldview" and accuse it of harboring a liberal bias. Spayd did not analyze the substance of the claim, but did opine that the Times is "part of a fracturing media environment that reflects a fractured country. That in turn leads liberals and conservatives toward separate news sources."[242] Times executive editor Dean Baquet stated that he does not believe coverage has a liberal bias, but that: "We have to be really careful that people feel like they can see themselves in The New York Times. I want us to be perceived as fair and honest to the world, not just a segment of it. It's a really difficult goal. Do we pull it off all the time? No."[242]		Times public editor Arthur Brisbane wrote in 2012: "When The Times covers a national presidential campaign, I have found that the lead editors and reporters are disciplined about enforcing fairness and balance, and usually succeed in doing so. Across the paper's many departments, though, so many share a kind of political and cultural progressivism — for lack of a better term — that this worldview virtually bleeds through the fabric of The Times."[243]		In mid-2004, the newspaper's then public editor Daniel Okrent, wrote an opinion piece in which he said that The New York Times did have a liberal bias in news coverage of certain social issues such as abortion and same-sex marriage. He stated that this bias reflected the paper's cosmopolitanism, which arose naturally from its roots as a hometown paper of New York City.[173] He wrote, "if you're examining the paper's coverage of these subjects from a perspective that is neither urban nor Northeastern nor culturally seen-it-all; if you are among the groups The Times treats as strange objects to be examined on a laboratory slide (devout Catholics, gun owners, Orthodox Jews, Texans); if your value system wouldn't wear well on a composite New York Times journalist, then a walk through this paper can make you feel you're traveling in a strange and forbidding world."[173] Okrent wrote that the Time's Arts & Leisure, Sunday Times Magazine, and Culture coverage trend to the left.[244]		In December 2004, a University of California, Los Angeles study by former fellows of a conservative think tank gave The New York Times a score of 73.7 on a 100-point scale, with 0 being most conservative and 100 being most liberal, making it the second-most liberal major newspaper in the study after The Wall Street Journal (85.1).[245] The validity of the study has been questioned, however. The liberal watchdog group Media Matters for America pointed out potential conflicts of interest with the author's funding, and political scientists, such as Brendan Nyhan, cited flaws in the study's methodology.[246][247]		Donald Trump has frequently criticized the New York Times on his Twitter account before and during his presidency; since November 2015, Trump has referred to the Times as "the failing New York Times" in a series of tweets.[248] Despite Trump's criticism, New York Times editor Mark Thompson noted that the paper had enjoyed soaring digital readership, with the fourth quarter of 2016 seeing the highest number of new digital subscribers to the newspaper since 2011.[249][250][251]		Critic Matt Taibbi accused The New York Times of favoring Hillary Clinton over Bernie Sanders in the paper's news coverage of the 2016 Democratic presidential primaries.[252] Responding to the complaints of many readers, New York Times public editor Margaret Sullivan wrote that, "The Times has not ignored Mr. Sanders's campaign, but it hasn't always taken it very seriously. The tone of some stories is regrettably dismissive, even mocking at times. Some of that is focused on the candidate's age, appearance and style, rather than what he has to say."[253] Times senior editor Carolyn Ryan defended both the volume of New York Times coverage (noting that Sanders had received about the same amount of article coverage as Jeb Bush and Marco Rubio) and its tone.[254]		The Times has developed a national and international "reputation for thoroughness" over time.[255] Among journalists, the paper is held in high regard; a 1999 survey of newspaper editors conducted by the Columbia Journalism Review found that the Times was the "best" American paper, ahead of the Washington Post, Wall Street Journal, and Los Angeles Times.[256] The Times also was ranked #1 in a 2011 "quality" ranking of U.S. newspapers by Daniel de Vise of the Washington Post; the objective ranking took into account the number of recent Pulitzer Prizes won, circulation, and perceived Web site quality.[256] A 2012 report in WNYC called the Times "the most respected newspaper in the world."[257]		Nevertheless, like many other U.S. media sources, the Times had suffered from a decline in public perceptions of credibility in the U.S. from 2004 to 2012.[258] A Pew Research Center survey in 2012 asked respondents about their views on credibility of various news organizations. Among respondents who gave a rating, 49% said that they believed "all or most" of the Times's reporting, while 50% disagreed. A large percentage (19%) of respondents were unable to rate believability. The Times's score was comparable to that of USA Today.[258] Media analyst Brooke Gladstone of WNYC's On the Media writing for the New York Times says that the decline in U.S. public trust of the mass media can be explained (1) by the rise of the polarized Internet-driven news; (2) by a decline in trust in U.S. institutions more generally; and (3) by the fact that "Americans say they want accuracy and impartiality, but the polls suggest that, actually, most of us are seeking affirmation."[259]		The TimesMachine is a web-based archive of scanned issues of The New York Times from 1851 through 2002.[260]		Unlike The New York Times online archive, the TimesMachine presents scanned images of the actual newspaper. All non-advertising content can be displayed on a per-story basis in a separate PDF display page and saved for future reference.[261]		The archive is available to New York Times subscribers, home delivery and/or digital.[260]		The position of public editor was established in 2003 to "investigate matters of journalistic integrity"; each public editor was to serve a two-year term.[262] The post "was established to receive reader complaints and question Times journalists on how they make decisions."[263] The impetus for the creation of the public editor position was the Jayson Blair affair. Public editors were: Daniel Okrent (2003–2005), Byron Calame (2005–2007), Clark Hoyt (2007–2010) (served an extra year), Arthur S. Brisbane (2010–2012), Margaret Sullivan (2012–2016) (served a four-year term), and Elizabeth Spayd (2016–2017). In 2017, the Times eliminated the position of public editor.[263][264]		Official New York Times web sites		Unofficial New York Times related web sites		1 2 3 4 5 6 7		NBC Wall Street Journal Agence France-Presse MSNBC Bloomberg BNA Washington Examiner Talk Media News/Univision		Fox News CBS Radio AP Radio Foreign Pool Time Yahoo! News Dallas Morning News		CBS News Bloomberg McClatchy Washington Times SiriusXM Salem Radio Globe/Roll Call		AP NPR AURN The Hill Regionals Newsmax CBN		ABC News Washington Post Politico Fox News Radio CSM/NY Post Daily Mail BBC/OAN		Reuters NY Times Chicago Tribune VOA RealClearPolitics HuffPost/NY Daily News BuzzFeed/Daily Beast		CNN USA Today ABC Radio National Journal Al Jazeera/PBS Westwood One Financial Times/Guardian		* From 1985 to 1990: Pulitzer Prize for General News Reporting; From 1991 to 1997: Pulitzer Prize for Spot News Reporting; From 1998 to present: Pulitzer Prize for Breaking News Reporting		
18,205,898[1][2]		Asian Americans are Americans of Asian descent. The term refers to a panethnic group that includes diverse populations who have ancestral origins in East Asia, Southeast Asia, or South Asia, as defined by the U.S. Census Bureau.[5] This includes people who indicate their race(s) on the census as "Asian" or reported entries such as "Asian Indian, Chinese, Filipino, Korean, Japanese, Vietnamese, and Other Asian."[6] Asian Americans with no other ancestry comprise 4.8% of the U.S. population, while people who are Asian alone or combined with at least one other race make up 5.6%.[5]		Although migrants from Asia have been in parts of the contemporary United States since the 17th century, large-scale immigration did not begin until the mid-18th century. Nativist immigration laws during the 1880s-1920s excluded various Asian groups, eventually prohibiting almost all Asian immigration to the continental United States. After immigration laws were reformed during the 1940s-60s, abolishing national origins quotas, Asian immigration increased rapidly. Analyses of the 2010 census have shown that Asian Americans are the fastest growing racial or ethnic minority in the United States.[7]		Starting in the first few years of the 2000 decade, Asian American earnings began exceeding all other racial groups for both men and women.[8] For example, in 2008 Asian Americans had the highest median household income overall of any racial demographic.[9][10] In 2012, Asian Americans had the highest educational attainment level and median household income of any racial demographic in the country.[11][12] In 2015, Asian American men were the highest earning racial group as they earned 117% as much as white American men and Asian-American women earned 106% as much as white American women.[8]		Despite this, a 2014 report from the Census Bureau reported that 12% of Asian Americans were living below the poverty line, while only 10.1% of non-Hispanic white Americans live below the poverty line.[13] Once country of birth and other demographic factors are taken into account, Asian Americans are no more likely than non-Hispanic whites to live in poverty.[14]						As with other racial and ethnicity based terms, formal and common usage have changed markedly through the short history of this term. Prior to the late 1960s, people of Asian ancestry were usually referred to as Oriental, Asiatic, and Mongoloid.[15][16] Additionally, the American definition of 'Asian' originally included West Asian ethnic groups, particularly Afghan Americans, Jewish Americans, Armenian Americans, Assyrian Americans, and Arab Americans, although these groups are now considered Middle Eastern American.[17][18] The term Asian American was coined by historian Yuji Ichioka, who is credited with popularizing the term, to frame a new "inter-ethnic-pan-Asian American self-defining political group" in the late 1960s.[15] Changing patterns of immigration and an extensive period of exclusion of Asian immigrants have resulted in demographic changes that have in turn affected the formal and common understandings of what defines Asian American. For example, since the removal of restrictive "national origins" quotas in 1965, the Asian-American population has diversified greatly to include more of the peoples with ancestry from various parts of Asia.[19]		Today, Asian American is the accepted term for most formal purposes, such as government and academic research, although it is often shortened to Asian in common usage.[citation needed] The most commonly used definition of Asian American is the US Census Bureau definition, which includes all people with origins in the Far East, Southeast Asia, and the Indian subcontinent.[6] This is chiefly because the census definitions determine many government classifications, notably for equal opportunity programs and measurements.		According to the Oxford English Dictionary, "Asian person" in the United States is sometimes thought of as a person of East Asian descent.[20][21] In vernacular usage, "Asian" is often used to refer to those of East Asian descent or anyone else of Asian descent with epicanthic eyefolds.[22][23] This differs from the U.S. Census definition[6][24] and the Asian American Studies departments in many universities consider all those of East, South or Southeast Asian descent to be "Asian".[25]		In the US Census, people with origins or ancestry in the Far East, Southeast Asia, and the Indian subcontinent are classified as part of the Asian race;[26] while those with origins or ancestry in North Asia (Russians, Siberians), Central Asia (Kazakhs, Uzbeks, Turkmens, etc.), Western Asia (diaspora Jews, Turks, Persians, West Asian Arabs, etc.), and the Caucasus (Georgians, Armenians, Azeris) are classified as "white" or "Middle Eastern".[27][28][29][30] As such, "Asian" and "African" ancestry are seen as racial categories for the purposes of the Census, since they refer to ancestry only from those parts of the Asian and African continents that are outside the Middle East and North Africa.		Before 1980, Census forms listed particular Asian ancestries as separate groups, along with white and black or negro.[31] Asian Americans had also been classified as "other".[32] In 1977, the federal Office of Management and Budget issued a directive requiring government agencies to maintain statistics on racial groups, including on "Asian or Pacific Islander".[33] The 1980 census marked the first classification of Asians as a large group, combining several individual ancestry groups into "Asian or Pacific Islander." By the 1990 census, "Asian or Pacific Islander (API)" was included as an explicit category, although respondents had to select one particular ancestry as a subcategory.[34][35] The 2000 census onwards separated the category into two separate ones, "Asian American" and "Native Hawaiian and Other Pacific Islander."[36]		The definition of Asian American has variations that derive from the use of the word American in different contexts. Immigration status, citizenship (by birthright and by naturalization), acculturation, and language ability are some variables that are used to define American for various purposes and may vary in formal and everyday usage.[37] For example, restricting American to include only U.S. citizens conflicts with discussions of Asian American businesses, which generally refer both to citizen and non-citizen owners.[38]		In a PBS interview from 2004, a panel of Asian American writers discussed how some groups include people of Middle Eastern descent in the Asian American category.[39] Asian American author Stewart Ikeda has noted, "The definition of 'Asian American' also frequently depends on who's asking, who's defining, in what context, and why... the possible definitions of 'Asian-Pacific American' are many, complex, and shifting... some scholars in Asian American Studies conferences suggest that Russians, Iranians, and Israelis all might fit the field's subject of study."[40] Jeff Yang, of the Wall Street Journal, writes that the pan-ethnic definition of Asian American is a unique American construct, and as an identity is "in beta".[41]		Scholars have grappled with the accuracy, correctness, and usefulness of the term Asian American. The term "Asian" in Asian American most often comes under fire for encompassing a huge number of people with ancestry from (or who have immigrated from) a wide range of culturally diverse countries and traditions. In contrast, leading social sciences and humanities scholars of race and Asian American identity point out that because of the racial constructions in the United States, including the social attitudes toward race and those of Asian ancestry, Asian Americans have a "shared racial experience."[42] Because of this shared experience, the term Asian American is still a useful panethnic category because of the similarity of some experiences among Asian Americans, including stereotypes specific to people in this category.[42][43][44]		The demographics of Asian Americans describe a heterogeneous group of people in the United States who can trace their ancestry to one or more countries in Asia.[45][46] Because Asian Americans compose 5% of the entire U.S. population, the diversity of the group is often disregarded in media and news discussions of "Asians" or of "Asian Americans."[47] While there are some commonalities across ethnic sub-groups, there are significant differences among different Asian ethnicities that are related to each group's history.[48][49] As of July 2015[update], California had the largest population of Asian Americans of any state, and Hawaii was the only state where Asian Americans were the majority of the population.[50]		The demographics of Asian Americans can further be subdivided into:		Asian Americans include multiracial or mixed race persons with origins or ancestry in both the above groups and another race, or multiple of the above groups.		In 2010, there were 2.8 million people (5 and older) who spoke a Chinese language at home;[51] after the Spanish language, it is the third most common language in the United States.[51] Other sizeable Asian languages are Tagalog, Vietnamese, and Korean, with all three having more than 1 million speakers in the United States.[51] In 2012, Alaska, California, Hawaii, Illinois, Massachusetts, Michigan, Nevada, New Jersey, New York, Texas and Washington were publishing election material in Asian languages in accordance with the Voting Rights Act;[52] these languages include Tagalog, Mandarin Chinese, Vietnamese, Hindi and Bengali.[52] Election materials were also available in Gujarati, Japanese, Khmer, Korean, and Thai.[53] According to a poll conducted by the Asian American Legal Defense and Education Fund in 2013, it found that 48 percent of Asian Americans considered media in their native language as their primary news source.[54]		According to the 2000 Census, the more prominent languages of the Asian American community include the Chinese languages (Cantonese, Taishanese, and Hokkien), Tagalog, Vietnamese, Korean, Japanese, Hindi, Urdu, and Gujarati.[55] In 2008, the Chinese, Japanese, Korean, Tagalog, and Vietnamese languages are all used in elections in Alaska, California, Hawaii, Illinois, New York, Texas, and Washington state.[56]		A 2012 Pew Research Center study found the following breakdown of religious identity among Asian Americans:[57]		The percentage of Christians among Asian Americans has declined sharply since the 1990s, chiefly due to largescale immigration from countries in which Christianity is a minority religion (China and India in particular). In 1990, 63% of the Asian Americans identified as Christians, while in 2001 only 43% did.[58] This development has been accompanied by a rise in traditional Asian religions, with the people identifying with them doubling during the same decade.[59]		As Asian Americans originate from many different countries, each population has its own unique immigration history.[11]		Filipinos have been in the territories that would become the United States since the 16th century.[60] The earliest known arrival is that of "Luzonians" in Morro Bay, California on board the Manila-built galleon ship Nuestra Senora de Esperanza in 1587, when both the Philippines and California were colonies of the Spanish Empire.[61][62] Romani people began emigrating to North America in colonial times, with small groups recorded in Virginia and French Louisiana. Larger-scale Roma emigration to the United States would follow subsequently.[citation needed] In 1635, an "East Indian" is listed in Jamestown, Virginia;[63] preceding wider settlement of Indian immigrants on the East Coast in the 1790s and the West Coast in the 1800s.[64] In 1763, Filipinos established the small settlement of Saint Malo, Louisiana, after fleeing mistreatment aboard Spanish ships.[65] Since there were no Filipino women with them, these Manilamen, as they were known, married Cajun and Native American women.[66] The first Japanese person to come to the United States, and stay any significant period of time was Nakahama Manjirō who reached the East Coast in 1841, and Joseph Heco became the first Japanese American naturalized US citizen in 1858.[67][68]		Chinese sailors first came to Hawaii in 1789,[69][70] a few years after Captain James Cook came upon the island. Many settled and married Hawaiian women. Most Chinese, Korean and Japanese immigrants in Hawaii arrived in the 19th century as laborers to work on sugar plantations.[71] There were thousands of Asians in Hawaii when it was annexed to the United States in 1898.[72] Later, Filipinos also came to work as laborers, attracted by the job opportunities, although they were limited.[73][74]		Large-scale migration from Asia to the United States began when Chinese immigrants arrived on the West Coast in the mid-19th century.[75] Forming part of the California gold rush, these early Chinese immigrants participated intensively in the mining business and later in the construction of the transcontinental railroad.[76] By 1852, the number of Chinese immigrants in San Francisco had jumped to more than 20,000. A wave of Japanese immigration to the United States began after the Meiji Restoration in 1868.[77] In 1898, all Filipinos in the Philippine Islands became American nationals when the United States took over colonial rule of the islands from Spain following the latter's defeat in the Spanish–American War.[78]		Under United States law during this period, particularly the Naturalization Act of 1790, only "free white persons" were eligible to naturalize as American citizens. Ineligibility for citizenship prevented Asian immigrants from accessing a variety of rights such as voting.[79] In a pair of cases, Ozawa v. United States (1922) and United States v. Bhagat Singh Thind (1923), the Supreme Court upheld the racial qualification for citizenship and ruled that Asians were not "white persons." Second-generation Asian Americans, however, could become U.S. citizens due to the birthright citizenship clause of the Fourteenth Amendment; this guarantee was confirmed as applying regardless of race or ancestry by the Supreme Court in United States v. Wong Kim Ark (1898).[80][81]		From the 1880s to the 1920s, the United States passed laws inaugurating an era of exclusion of Asian immigrants. Although the absolute numbers of Asian immigrants were small compared to that of immigrants from other regions, much of it was concentrated in the West, and the increase caused some nativist sentiment known as the "yellow peril". Congress passed restrictive legislation prohibiting nearly all Chinese immigration in the 1880s.[82] Japanese immigration was sharply curtailed by a diplomatic agreement in 1907. The Asiatic Barred Zone Act in 1917 further barred immigration from South Asia (then-British India), Southeast Asia, and the Middle East.[83] The Immigration Act of 1924 provided that no "alien ineligible for citizenship" could be admitted as an immigrant to the United States, consolidating the prohibition of Asian immigration.[84][85]		World War II-era legislation and judicial rulings gradually increased the ability of Asian Americans to immigrate and become naturalized citizens. Immigration rapidly increased following the enactment of the Immigration and Nationality Act Amendments of 1965 as well as the influx of refugees from conflicts occurring in Southeast Asia such as the Vietnam War. Asian American immigrants have a significant percentage of individuals who have already achieved professional status, a first among immigration groups.[86]		The Migration Policy Institute reports that the number of Asian immigrants to the United States "grew from 491,000 in 1960 to about 12.8 million in 2014, representing a 2,597 percent increase."[87] From 2000 to 2010, the Asian American population was the fastest growing group according to the 2010 U.S. Census.[11][88][89] By 2012, the growth of Asian American population overtook the growth of Latino American population according to the Pew Research Center; it also found that illegal immigration from Asia was significantly less than from Latin America.[90] In 2015, Pew Research Center found that from 2010 to 2015 more immigrants came from Asia than from Latin America, and that since 1965 Asians have made up a quarter of all immigrants.[91]		Asians have made up an increasing proportion of the foreign-born Americans: "In 1960, Asians represented 5 percent of the U.S. foreign-born population; by 2014, their share grew to 30 percent of the nation's 42.4 million immigrants."[87] As of 2016, "Asia is the second-largest region of birth (after Latin America) of U.S. immigrants."[87] In 2013, China surpassed Mexico as the top single country of origin for immigrants to the U.S.[92] Asian immigrants "are more likely than the overall foreign-born population to be naturalized citizens"; in 2014, 59% of Asian immigrants had U.S. citizenship, compared to 47% of all immigrants.[87] Postwar Asian immigration to the U.S. has been diverse: in 2014, 31% of Asian immigrants to the U.S. were from East Asian (predominately China and Korea); 27.7% were from South Central Asia (predominately India); 32.6% were from Southeastern Asia (predominately the Philippines and Vietnam) and 8.3% were from Western Asia.[87]		The Asian American movement refers to a pan-Asian movement in the United States in which Americans of Asian descent came together to fight against their shared oppression and to organize for recognition and advancement of their shared cause during the 1960s to the early 1980s. According to William Wei, the movement was "rooted in a past history of oppression and a present struggle for liberation."[93] This occurred around the same time as the Chicano movement, Civil Rights Movement, American Indian Movement and the gay liberation movement.		Asian Americans have been involved in the entertainment industry since the first half of the 19th century, when Chang and Eng Bunker (the original "Siamese Twins") became naturalized citizens.[94] Acting roles in television, film, and theater were relatively few, and many available roles were for narrow, stereotypical characters. More recently, young Asian American comedians and film-makers have found an outlet on YouTube allowing them to gain a strong and loyal fanbase among their fellow Asian Americans.[95] There have been several Asian American-centric television shows in American media, beginning with Mr. T and Tina in 1976, and as recent as Fresh Off the Boat in 2015.[96] Throughout the 1990s there was a growing amount of Asian American queer writings[97] and today the list of contributing writers is long. To name a few: Merle Woo (1941), Willyce Kim (1946), Russel Leong (1950), Kitty Tsui (1952), Dwight Okita (1958), Norman Wong (1963), Tim Liu (1965), Chay Yew (1965) and Justin Chin (1969).		When Asian Americans were largely excluded from labor markets in the 19th century, they started their own businesses. They have started convenience and grocery stores, professional offices such as medical and law practices, laundries, restaurants, beauty-related ventures, hi-tech companies, and many other kinds of enterprises, becoming very successful and influential in American society. They have dramatically expanded their involvement across the American economy. Asian Americans have been disproportionately successful in the hi-tech sectors of California's Silicon Valley, as evidenced by the Goldsea 100 Compilation of America's Most Successful Asian Entrepreneurs.[98]		Compared to their population base, Asian Americans today are well represented in the professional sector and tend to earn higher wages.[99] The Goldsea compilation of Notable Asian American Professionals show that many have come to occupy high positions at leading U.S. corporations, including a surprising number as Chief Marketing Officers.[100]		Asian Americans have made major contributions to the American economy. In 2012, Asian Americans own 1.5 million businesses, employ around 3 million people who earn an annual total payroll of around $80 billion.[88] Fashion designer and mogul Vera Wang, who is famous for designing dresses for high-profile celebrities, started a clothing company, named after herself, which now offers a broad range of luxury fashion products. An Wang founded Wang Laboratories in June 1951. Amar Bose founded the Bose Corporation in 1964. Charles Wang founded Computer Associates, later became its CEO and chairman. David Khym founded hip-hop fashion giant Southpole (clothing) in 1991. Jen-Hsun Huang co-founded the NVIDIA corporation in 1993. Jerry Yang co-founded Yahoo! Inc. in 1994 and became its CEO later. Andrea Jung serves as Chairman and CEO of Avon Products. Vinod Khosla was a founding CEO of Sun Microsystems and is a general partner of the prominent venture capital firm Kleiner Perkins Caufield & Byers. Steve Chen and Jawed Karim were co-creators of YouTube, and were beneficiaries of Google's $1.65 billion acquisition of that company in 2006. In addition to contributing greatly to other fields, Asian Americans have made considerable contributions in science and technology in the United States, in such prominent innovative R&D regions as Silicon Valley and The Triangle.		Asian Americans have a high level of political incorporation in terms of their actual voting population. Since 1907, Asian Americans have been active at the national level and have had multiple officeholders at local, state, and national levels.		The highest ranked Asian American was Senator and President pro tempore Daniel Inouye, who died in office in 2012. There are several active Asian Americans in the United States Congress. With higher proportions and densities of Asian American populations, Hawaii has most consistently sent Asian Americans to the Senate, and Hawaii and California have most consistently sent Asian Americans to the House of Representatives.		Connie Chung was one of the first Asian American national correspondents for a major TV news network, reporting for CBS in 1971. She later co-anchored the CBS Evening News from 1993 to 1995, becoming the first Asian American national news anchor.[101] At ABC, Ken Kashiwahara began reporting nationally in 1974. In 1989, Emil Guillermo, a Filipino American born reporter from San Francisco, became the first Asian American male to co-host a national news show when he was senior host at National Public Radio's "All Things Considered." In 1990, Sheryl WuDunn, a foreign correspondent in the Beijing Bureau of The New York Times, became the first Asian American to win a Pulitzer Prize. Ann Curry joined NBC News as a reporter in 1990, later becoming prominently associated with The Today Show in 1997. Carol Lin is perhaps best known for being the first to break the news of 9-11 on CNN. Dr. Sanjay Gupta is currently CNN's chief health correspondent. Lisa Ling, a former co-host on The View, now provides special reports for CNN and The Oprah Winfrey Show, as well as hosting National Geographic Channel's Explorer. Fareed Zakaria, a naturalised Indian-born immigrant, is a prominent journalist, and author specialising in international affairs. He is the editor-at-large of Time magazine, and the host of Fareed Zakaria GPS on CNN. Juju Chang, James Hatori, John Yang, Veronica De La Cruz, Michelle Malkin, Betty Nguyen, and Julie Chen have become familiar faces on television news. John Yang won a Peabody Award. Alex Tizon, a Seattle Times staff writer, won a Pulitzer Prize in 1997.		Since the War of 1812 Asian Americans have served and fought on behalf of the United States. Serving in both segregated and non-segregated units until the desegregation of the US Military in 1948, 31 have been awarded the nation's highest award for combat valor, the Medal of Honor. Twenty-one of these were conferred upon members of the mostly Japanese American 100th Infantry Battalion of the 442nd Regimental Combat Team of World War II, the most highly decorated unit of its size in the history of the United States Armed Forces.[102][103]		Asian Americans have made many prominent and notable contributions to Science and Technology.		Chien-Shiung Wu was known to many scientists as the "First Lady of Physics" and played a pivotal role in experimentally demonstrating the violation of the law of conservation of parity in the field of particle physics. Fazlur Rahman Khan, also known as named as "The Father of tubular designs for high-rises",[104] was highlighted by President Barack Obama in a 2009 speech in Cairo, Egypt,[105] and has been called "Einstein of Structural engineering".[106] Min Chueh Chang was the co-inventor of the combined oral contraceptive pill and contributed significantly to the development of in vitro fertilisation at the Worcester Foundation for Experimental Biology. David T. Wong was one of the scientists credited with the discovery of ground-breaking drug Fluoxetine as well as the discovery of atomoxetine, duloxetine and dapoxetine with colleagues.[107][108][109] Michio Kaku has popularized science and has appeared on multiple programs on television and radio.		Tsung-Dao Lee and Chen Ning Yang received the 1957 Nobel Prize in Physics for theoretical work demonstrating that the conservation of parity did not always hold and later became American citizens. Har Gobind Khorana shared the 1968 Nobel Prize in Physiology or Medicine for his work in genetics and protein synthesis. Samuel Chao Chung Ting received the 1976 Nobel Prize in physics for discovery of the subatomic particle J/ψ. The mathematician Shing-Tung Yau won the Fields Medal in 1982 and Terence Tao won the Fields Medal in 2006. The geometer Shiing-Shen Chern received the Wolf Prize in Mathematics in 1983. Andrew Yao was awarded the Turing Award in 2000. Subrahmanyan Chandrasekhar shared the 1983 Nobel Prize in Physics and had the Chandra X-ray Observatory named after him. In 1984, Dr. David D. Ho first reported the "healthy carrier state" of HIV infection, which identified HIV-positive individuals who showed no physical signs of AIDS. Charles J. Pedersen shared the 1987 Nobel Prize in chemistry for his methods of synthesizing crown ethers. Steven Chu shared the 1997 Nobel Prize in Physics for his research in cooling and trapping atoms using laser light. Daniel Tsui shared the 1998 Nobel Prize in Physics in 1998 for helping discover the fractional Quantum Hall effect. In 2008, biochemist Roger Tsien won the Nobel in Chemistry for his work on engineering and improving the green fluorescent protein (GFP) that has become a standard tool of modern molecular biology and biochemistry. Yoichiro Nambu received the 2008 Nobel Prize in Physics for his work on the consequences of spontaneously broken symmetries in field theories. In 2009, Charles K. Kao was awarded Nobel Prize in Physics "for groundbreaking achievements concerning the transmission of light in fibres for optical communication" and Venkatraman Ramakrishnan won the prize in Chemistry "for studies of the structure and function of the ribosome". Ching W. Tang was the inventor of the Organic light-emitting diode and Organic solar cell and was awarded the 2011 Wolf Prize in Chemistry for this achievement. Manjul Bhargava, an American Canadian of Indian origins won the Fields Medal in mathematics in 2014. Shuji Nakamura won the 2014 Nobel Prize in Physics for the invention of efficient blue light-emitting diodes. Yitang Zhang is a Chinese-born American mathematician working in the area of number theory. While working for the University of New Hampshire as a lecturer, Zhang submitted an article to the Annals of Mathematics in 2013 which established the first finite bound on gaps between prime numbers, which lead to a 2014 MacArthur award.		LTC Ellison Onizuka became the first Asian American (and third person of East Asian descent) when he made his first space flight aboard STS-51-C in 1985. Onizuka later died aboard the Space Shuttle Challenger in 1986. Taylor Gun-Jin Wang became the first person of Chinese ethnicity and first Chinese American, in space in 1985; he has since been followed by Leroy Chiao in 1994, and Ed Lu in 1997. In 1986, Franklin Chang-Diaz became the first Asian Latin American in space. Eugene H. Trinh became the first Vietnamese American in space in 1992. In 2001, Mark L. Polansky, a Jewish Korean American, made his first of three flights into space. In 2003, Kalpana Chawla became the first Indian American in space, but died aboard the ill-fated Space Shuttle Columbia. She has since been followed by CDR Sunita Williams in 2006.		Wataru Misaka broke the NBA color barrier when he played for the New York Knicks in the 1947–48 season.[110] The next Asian American NBA player was Raymond Townsend, who played for the Golden State Warriors and Indiana Pacers from 1978 to 1982.[110] Rex Walters, played from 1993 to 2000 with the Nets, Philadelphia 76ers and Miami Heat;[110] he is presently the head coach for the University of San Francisco basketball team.[111] After playing basketball at Harvard University, point guard Jeremy Lin signed with the NBA's Golden State Warriors in 2010[110] and now plays for the Brooklyn Nets.		Current Kansas Jayhawks assistant coach Kurtis Townsend is Raymond Townsend's brother.[112]		Erik Spoelstra became the youngest coach ever in NBA history. He is currently the head coach of the Miami Heat.[113]		In football, Wally Yonamine played professionally for the San Francisco 49ers in 1947.[114] Norm Chow is currently the head coach for the University of Hawaii and former offensive coordinator for UCLA after a short stint with the Tennessee Titans of the NFL, after 23 years of coaching other college teams, including four years as offensive coordinator at USC. In 1962, half Filipino Roman Gabriel was the first Asian American to start as an NFL quarterback. Dat Nguyen was an NFL middle linebacker who was an all-pro selection in 2003 for the Dallas Cowboys. In 1998, he was named an All-American and won the Bednarik Award as well as the Lombardi Award, while playing for Texas A&M University. Hines Ward who was born to a Korean mother and an African American father, is a former NFL wide receiver who was the MVP of Super Bowl XL and Ward also won the 12th season of the Dancing with the Stars television series. Former Patriot's linebacker Tedy Bruschi is of Filipino and Italian descent. While playing for the Patriots, Bruschi won three Super Bowl rings and was a two-time All-Pro selection. Bruschi is currently a NFL analyst at ESPN.		There are several top ranked Asian American mixed martial artists. BJ Penn is a former UFC lightweight and welterweight champion. Cung Le is a former Strikeforce middleweight champion. Benson Henderson is the former WEC lightweight champion and a former UFC lightweight champion. Nam Phan is UFC featherweight fighter.		Asian Americans first made an impact in Olympic sports in the late 1940s and in the 1950s. Sammy Lee became the first Asian American to earn an Olympic Gold Medal, winning in platform diving in both 1948 and 1952. Victoria Manalo Draves won both gold in platform and springboard diving in the 1948. Harold Sakata won a weightlifting silver medal in the 1948 Olympics, while Tommy Kono (weightlifting), Yoshinobu Oyakawa (100-meter backstroke), and Ford Konno (1500-meter freestyle) each won gold and set Olympic records in the 1952 Olympics. Konno won another gold and silver swimming medal at the same Olympics and added a silver medal in 1956, while Kono set another Olympic weightlifting record in 1956. Also at the 1952 Olympics, Evelyn Kawamoto won two bronze medals in swimming.		Amy Chow was a member of the gold medal women's gymnastics team at the 1996 Olympics; she also won an individual silver medal on the uneven bars. Gymnast Mohini Bhardwaj won a team silver medal in the 2004 Olympics. Bryan Clay who is of Half-Japanese descent[115] won the decathlon gold medal in the 2008 Olympics, the silver medal in the 2004 Olympics, and was the sport's 2005 world champion.		Since Tiffany Chin won the women's US Figure Skating Championship in 1985, Asian Americans have been prominent in that sport. Kristi Yamaguchi won three national championships, two world titles, and the 1992 Olympic Gold medal. Michelle Kwan has won nine national championships and five world titles, as well as two Olympic medals (silver in 1998, bronze in 2002).		Apolo Ohno, who is of half-Japanese descent,[116] is a short track speed skater and an eight-time Olympic medalist as well as the most decorated American Winter Olympic athlete of all time. He became the youngest U.S. national champion in 1997 and was the reigning champion from 2001 to 2009, winning the title a total of 12 times. In 1999, he became the youngest skater to win a World Cup event title, and became the first American to win a World Cup overall title in 2001, which he won again in 2003 and 2005. He won his first overall World Championship title at the 2008 championships.		Nathan Adrian, who is a hapa of half-Chinese descent,[117] is a professional American swimmer and three-time Olympic gold medalist who currently holds the American record in the 50 and 100-yard freestyle (short course) events. He has won a total of fifteen medals in major international competitions, twelve gold, two silver, and one bronze spanning the Olympics, the World, and the Pan Pacific Championships.		Michael Chang was a top-ranked tennis player for most of his career, and the youngest ever winner of a Grand Slam tennis tournament in men's singles. He won the French Open in 1989. Tiger Woods, who is partially of Asian descent, is the most successful golfer of his generation and one of the most famous athletes in the world. Eric Koston is one of the top street skateboarders and placed first in the 2003 X-Games street competition. Richard Park is a Korean American ice hockey player who currently plays for the Swiss team HC Ambri-Piotta.		Brian Ching, whose father was Chinese, represented the United States Men's National Soccer Team, scoring 11 goals in 45 caps. He participated in the 2006 World Cup and won the 2007 Gold Cup.[118]		Julie Chu, who is three-quarter Chinese and one-quarter Puerto Rican,[119] is an American Olympic ice hockey player who played for the United States women's ice hockey team. She was also US Olympic Team Flag Bearer for the 2014 Winter Olympic Closing Ceremonies.[120]		In recognition of the unique culture, traditions, and history of Asian Americans and Pacific Islanders, the United States government has permanently designated the month of May to be Asian Pacific American Heritage Month.[121][122]		Asian immigrants are also changing the American medical landscape through increasing number of Asian medical practitioners in the United States. Beginning in the 1960s and 1970s, the US government invited a number of foreign physicians particularly from India and the Philippines to address the acute shortage of physicians in rural and medically underserved urban areas. The trend in importing foreign medical practitioners, however, became a long-term, chronic solution as US medical schools failed to produce enough physicians to match the increasing American population. Amid decreasing interest in medicine among American college students due to high educational costs and high rates of job dissatisfaction, loss of morale, stress, and lawsuits, Asian American immigrants maintained a supply of healthcare practitioners for millions of Americans. It is well documented that Asian American international medical graduates including highly skilled guest workers using the J1 Visa program for medical workers, tend to serve in health professions shortage areas (HPSA) and specialties that are not filled by US medical graduates especially primary care and rural medicine.[123][124] Thus, Asian American immigrants play a key role in averting a medical crisis in the US.		A lasting legacy of Asian American involvement in medicine is the forcing of US medical establishment to accept minority medical practitioners. One could speculate that the introduction of Asian physicians and dentists to the American society could have triggered an acceptance of other minority groups by breaking down stereotypes and encouraging trust.[125]		Traditional Asian concepts and practices in health and medicine have attracted greater acceptance and are more widely adopted by American doctors. India's Ayurveda and traditional Chinese medicine (which also includes acupuncture) are two alternative therapy systems that have been studied and adopted to a great extent. For instance, in the early 1970s the US medical establishment did not believe in the usefulness of acupuncture. Since then studies have proven the efficacy of acupuncture for different applications, especially for treatment of chronic pain.[126] It is now covered by many health insurance plans.		Herbalism and massage therapy (from Ayurveda) are sweeping the spas across America. Meditation and yoga (from India) have also been widely adopted by health spas, and spiritual retreats of many religious bases. They are also part of the spiritual practice of the many Americans who are not affiliated with a mainline religious group.[citation needed]		Among America's major racial categories, Asian Americans have the highest educational qualifications. This varies, however, for individual ethnic groups. Dr. C.N. Le, Director of the Asian & Asian American Studies Certificate Program at the University of Massachusetts, writes that although 42% of all Asian American adults have at least a college degree, Vietnamese Americans have a degree attainment rate of only 16% while Laotians and Cambodians only have rates around 5%.[134] It has been noted, however, that 2008 US Census statistics put the bachelor's degree attainment rate of Vietnamese Americans at 26%, which is not very different from the rate of 27% for all Americans.[135] According to the US Census Bureau in 2010, while the high school graduation rate for Asian Americans is on par with those of other ethnic groups, 50% of Asian Americans have attained at least a bachelor's degree as compared with the national average of 28%,[136] and 34% for non-Hispanic whites.[137] Indian Americans have some of the highest education rates, with nearly 71% having attained at least a bachelor's degree in 2010.[133] According to Carolyn Chen, director of the Asian American Studies Program at Northwestern University, as of December 2012[update] Asian Americans made up twelve to eighteen percent of the student population at Ivy League schools, larger than their share of the population.[138] For example, the Harvard Class of 2016 is 21% Asian American.[139]		In the years immediately preceding 2012, 61% of Asian American adult immigrants have a bachelor or higher level college education.[11]		In 2012, there were 1.3 million alien Asian Americans; and for those awaiting visas, there were lengthy backlogs with over 450 thousand Filipinos, over 325 thousand Indians, over 250 thousand Vietnamese, and over 225 thousand Chinese are awaiting visas.[140][141] As of 2009, Filipinos and Indians accounted for the highest number of alien immigrants for "Asian Americans" with an estimated illegal population of 270,000 and 200,000 respectively. Indian Americans are also the fastest growing alien immigrant group in the United States, an increase in illegal immigration of 125% since 2000.[142][143] This is followed by Koreans (200,000) and Chinese (120,000).[144]		Due to the stereotype of Asian Americans being successful as a group and having the lowest crime rates in the United States, illegal immigration is mostly focused on those from Mexico and Latin America while leaving out Asians.[145] Asians are the second largest racial/ethnic alien immigrant group in the U.S. behind Hispanics and Latinos.[146][147] While the majority of Asian immigrants to the United States immigrate legally,[148] up to 15% of Asian immigrants immigrate without legal documents.[149]		Asian Americans have been the target of violence based on their race and or ethnicity. This includes, but are not limited to, such events as the Rock Springs massacre,[150] Watsonville Riots,[151][152] attacks upon Japanese Americans following the attack on Pearl Harbor,[153] and Korean American businesses targeted during the 1992 Los Angeles riots.[154] According to historian Arif Dirlik: "Indian massacres of Chinese was a commonplace experience on the frontier, the most notable being the 'legendary slaughter by Paiute Indians of forty to sixty Chinese miners in 1866.'"[155] In the late 1980s, South Asians in New Jersey faced assault and other hate crimes by a group known as the Dotbusters.		After the September 11 attacks, Sikh Americans were targeted, being the recipient of numerous hate crimes including murder.[156][157][158][159] Other Asian Americans have also been the victim of race based violence in Brooklyn,[160] Philadelphia,[161][162] San Francisco,[163] and Bloomington, Indiana.[164] Furthermore, it has been reported that young Asian Americans are more likely to be a target of violence than their peers.[160][165][166] Racism and discrimination still persists against Asian Americans, occurring not only to recent immigrants but also towards well-educated and highly trained professionals.[167]		Recent waves of immigration of Asian Americans to largely African American neighborhoods have led to cases of severe racial tensions.[168] Acts of large-scale violence against Asian American students by their black classmates have been reported in multiple cities.[169][170][171] In October 2008, 30 black students chased and attacked 5 Asian students at South Philadelphia High School,[172] and a similar attack on Asian students occurred at the same school one year later, prompting a protest by Asian students in response.[173]		Asian-owned businesses have been a frequent target of tensions between black and Asian Americans. During the 1992 Los Angeles riots, more than 2000 Korean-owned businesses were looted or burned by groups of African Americans.[174][175][176] From 1990 to 1991, a high-profile, racially-motivated boycott of an Asian-owned shop in Brooklyn was organized by a local black nationalist activist, eventually resulting in the owner being forced to sell his business.[177] Another racially-motivated boycott against an Asian-owned business occurred in Dallas in 2012, after an Asian American clerk fatally shot an African American who had robbed his store.[178][179] During the Ferguson unrest in 2014, Asian-owned businesses were looted,[180] and Asian-owned stores were looted during the 2015 Baltimore protests while African-American owned stores were bypassed.[181] Violence against Asian Americans continue to occur based on their race,[182] with one source asserting that Asian Americans are the fastest growing targets of hate crimes and violence.[183]		Until the late 20th century, the term "Asian American" was adopted mostly by activists, while the average person of Asian ancestries identified with their specific ethnicity.[184] The murder of Vincent Chin in 1982 was a pivotal civil rights case, and it marked the emergence of Asian Americans as a distinct group in United States.[184][185]		Stereotypes of Asians have been largely collectively internalized by society and these stereotypes have mainly negative repercussions for Asian Americans and Asian immigrants in daily interactions, current events, and governmental legislation. In many instances, media portrayals of East Asians often reflect a dominant Americentric perception rather than realistic and authentic depictions of true cultures, customs and behaviors.[186] Asians have experienced discrimination and have been victims of hate crimes related to their ethnic stereotypes.[187]		Study has indicated that most non-Asian Americans do not generally differentiate between Asian Americans of different ethnicities.[188] Stereotypes of Chinese Americans and Asian Americans are nearly identical.[189] A 2002 survey of Americans' attitudes toward Asian Americans and Chinese Americans indicated that 24% of the respondents disapprove of intermarriage with an Asian American, second only to African Americans; 23% would be uncomfortable supporting an Asian American presidential candidate, compared to 15% for an African American, 14% for a woman and 11% for a Jew; 17% would be upset if a substantial number of Asian Americans moved into their neighborhood; 25% had somewhat or very negative attitude toward Chinese Americans in general.[190] The study did find several positive perceptions of Chinese Americans: strong family values (91%); honesty as business people (77%); high value on education (67%).[189]		There is a widespread perception that Asian Americans are not "American" but are instead "perpetual foreigners".[190][191][192] Asian Americans often report being asked the question, "Where are you really from?" by other Americans, regardless of how long they or their ancestors have lived in United States and been a part of its society.[193] Many Asian Americans are themselves not immigrants but rather born in the United States. Many East Asian Americans are asked if they are Chinese or Japanese, an assumption based on major groups of past immigrants.[191][194]		Asian Americans are sometimes characterized as a model minority in the United States because many of their cultures encourage a strong work ethic, a respect for elders, a high degree of professional and academic success, a high valuation of family, education and religion.[195][196] Statistics such as high household income and low incarceration rate,[197] low rates of many diseases and higher than average life expectancy[198] are also discussed as positive aspects of Asian Americans.		The implicit advice is that the other minorities should stop protesting and emulate the Asian American work ethic and devotion to higher education. Some critics say the depiction replaces biological racism with cultural racism, and should be dropped.[199] According to the Washington Post, "the idea that Asian Americans are distinct among minority groups and immune to the challenges faced by other people of color is a particularly sensitive issue for the community, which has recently fought to reclaim its place in social justice conversations with movements like #ModelMinorityMutiny."[200]		The model minority concept can also affect Asians' public education.[201] By comparison with other minorities, Asians often achieve higher test scores and grades compared to other Americans.[202] Stereotyping Asian American as over-achievers can lead to harm if school officials or peers expect all to perform higher than average.[203] The very high educational attainments of Asian Americans has often been noted; in 1980, for example, 74% of Chinese Americans, 62% of Japanese Americans, and 55% of Korean Americans aged 20–21 were in college, compared to only a third of the whites. The disparity at postgraduate levels is even greater, and the differential is especially notable in fields making heavy use of mathematics. By 2000, a plurality of undergraduates at such elite public California schools as UC Berkeley and UCLA, which are obligated by law to not consider race as a factor in admission, were Asian American. The pattern is rooted in the pre-World War II era. Native-born Chinese and Japanese Americans reached educational parity with majority whites in the early decades of the 20th century.[204]		The "model minority" stereotype fails to distinguish between different ethnic groups with different histories. When divided up by ethnicity, it can be seen that the economic and academic successes supposedly enjoyed by Asian Americans are concentrated into a few ethnic groups. Cambodians, Hmong, and Laotians (and to a lesser extent, Vietnamese), all of whose relatively low achievement rates are possibly due to their refugee status, and that they are non-voluntary immigrants;[205] additionally, one in five Hmong and Bangladeshi people live in poverty.[88]		Furthermore, the model minority concept can be emotionally damaging to some Asian Americans, particularly since they are expected to live up to those peers who fit the stereotype.[206] Studies have shown that some Asian Americans suffer from higher rates of stress, depression, mental illnesses, and suicides in comparison to other races,[207] indicating that the pressures to achieve and live up to the model minority image may take a mental and psychological toll on some Asian Americans.[208]		This concept appears to elevate Asian Americans by portraying them as an elite group of successful, highly educated, intelligent, and wealthy individuals, but it can also be considered an overly narrow and overly one-dimensional portrayal of Asian Americans, leaving out other human qualities such as vocal leadership, negative emotions, risk taking, ability to learn from mistakes, and desire for creative expression.[209] Furthermore, Asian Americans who do not fit into the model minority mold can face challenges when people's expectations based on the model minority myth do not match with reality. Traits outside of the model minority mold can be seen as negative character flaws for Asian Americans despite those very same traits being positive for the general American majority (e.g., risk taking, confidence, empowered). For this reason, Asian Americans encounter a "bamboo ceiling", the Asian American equivalent of the glass ceiling in the workplace, with only 1.5% of Fortune 500 CEOs being Asians, a percentage smaller than their percentage of the total United States population.[210]		The bamboo ceiling is defined as a combination of individual, cultural, and organisational factors that impede Asian Americans' career progress inside organizations. Since then, a variety of sectors (including nonprofits, universities, the government) have discussed the impact of the ceiling as it relates to Asians and the challenges they face. As described by Anne Fisher, the "bamboo ceiling" refers to the processes and barriers that serve to exclude Asians and American people of Asian descent from executive positions on the basis of subjective factors such as "lack of leadership potential" and "lack of communication skills" that cannot actually be explained by job performance or qualifications.[211] Articles regarding the subject have been published in Crains, Fortune magazine, and The Atlantic.[212][213][214]		Asian Americans have been making progress in American society. A key indicator is the salary of Asian Americans compared to other racial groups.		In 2015, the racial group with the highest earnings was Asian American men at $24/hour. Asian American men earned 117% as much as the reference group in the study (white American men at $21/hour) and have been the highest earning racial group since about 2000. Similarly, in 2015 Asian American women ($18/hour) was the highest earning female racial group and earned 106% as much as white American women ($17/hour). Asian American women have been the highest earning female group for more than 10 years in a row.[8]		By 2009, Asian surpassed Hispanics becoming the race with the highest immigration percentage into the United States. In 2010, they made up 36% of immigrants.[215]		Asian Americans are typically more involved in social media and the Internet. In a 2014 research study conducted by the Pew Research Center, 95% of English-speaking Asian Americans use the Internet—the highest of any other racial demographic by at least 8%.[216][217] Pew reported that Asian-Americans have been using the Internet more than any other group since as early as 2000.[217] Though the gap between Asian Americans and the other groups is decreasing, Asian Americans still have a comfortable lead.		Similarly, English speaking Asian Americans own smartphones more than any other group. They grew from 71% ownership in 2012 to 91% ownership in 2014. The next highest ownership percentage is 66% by Whites.[216] The Pew Research Center believes that two of the major reasons for the amazingly high ownership percentage are high levels of income and high levels of education.[218]		Similarly, 91% of English-speaking Asian Americans own a smartphone. Smartphone ownership among English-speaking Asian Americans has increased by 20 points in three years, from 71% in 2012 to 91% last year.[216] A related study shows that Asian Americans also lead in at home broadband service, sitting at 84%.[219]		Asian Americans are also very active on social media. On Instagram alone, they have several fashion bloggers who have millions of followers.[220]		For 2005, income statistics from US Census data are shown in the corresponding histogram figures.				Journals		1 The U.S. Census Bureau definition of Asians refers to a person having origins in any of the original peoples of the Far East, Southeast Asia, or the Indian subcontinent. [2][3]		2 The United States Government classified Kalmyks as Asian until 1951, when Kalmyk Americans were reclassified as White Americans.[4]		3 The U.S. Census Bureau considers Mongolians and Uzbeks as Central Asians,[5] but a specific Central Asian American group similar to Middle Eastern American does not yet exist.[6]		4 The U.S. Census Bureau reclassifies anyone identifying as "Tibetan American" as "Chinese American".[7]		
Silicon Valley is an American comedy television series created by Mike Judge, John Altschuler and Dave Krinsky. The series focuses on six young men who founded a startup company in Silicon Valley.[1][2] The series premiered on April 6, 2014 on HBO.[3]		Silicon Valley completed airing its fourth season on June 25, 2017, and has been renewed for a fifth season, which will premiere in 2018.[4]						Richard Hendricks (Thomas Middleditch) is a shy, reclusive programmer who works at a large internet company called Hooli. He is also developing a music app called Pied Piper in a live-in startup business incubator run by entrepreneur Erlich Bachman (T. J. Miller). After a rocky post-TED elevator pitch of Pied Piper to venture capitalist Peter Gregory (Christopher Evan Welch), Hendricks also shows his work to a pair of programmers at Hooli who mock him. Within hours, however, Hooli executive Donald "Jared" Dunn (Zach Woods) and Gregory's assistant Monica (Amanda Crew) discover that the app contains a revolutionary data compression algorithm. Hooli CEO Gavin Belson (Matt Ross) proposes a US$4 million buy-out of Pied Piper, while Peter Gregory offers a $200,000 investment for 5% ownership in the company, an offer that would result in an equivalent valuation for the company. This leads Belson to increase his offer to $10 million. With encouragement from Monica and the support of Bachman, Hendricks chooses Gregory's offer. He hires the residents of the incubator, except for his friend Nelson "Big Head" Bighetti (Josh Brener), to become the Pied Piper team, along with Dunn, who defects from Hooli.		Hooli works to reverse engineer Pied Piper's algorithm based on the version he demonstrated, developing a copycat product called Nucleus. Gregory and Belson later each learn that Hendricks has been slated to present Pied Piper at TechCrunch Disrupt, a competition for unfunded startups. Belson is confounded by the news, and responds by scheduling the announcement of Nucleus at the event. Hendricks explains to Monica that he meant to withdraw from the competition, but Gregory demands that the company follow through, in large part due to his rivalry with Belson. The countdown to the event means that Pied Piper has to be ready to show in less than eight weeks rather than Gregory's initial plan of five months. The team rushes to produce a feature-rich cloud storage platform based on their compression technology.		At the TechCrunch event, Bachman takes the lead in a dramatic onstage presentation of Pied Piper. However the presentation is cut short when one of the judges assaults Bachman for having adulterous sex with both his current wife and ex-wife. Pied Piper automatically advances to the final round as recompense for the assault on Bachman. Belson presents Nucleus, which is integrated with all of Hooli's services and has compression performance equal to Pied Piper. Watching from the audience, the Pied Piper team generally admits defeat. Jared accosts strangers with pitches about pivoting the company in another direction, ending in his arrest by police. The team eventually retires to a hotel room, where Bachman nihilistically suggests "jerking off" every member of the audience, and the group launches into an engineering conversation about how to do that efficiently. The discussion sparks a sudden revelation in Hendricks, who spends the entire night coding. The next morning, Hendricks takes the lead in making Pied Piper's final presentation. Having scrapped all of Pied Piper's other features overnight, Hendricks describes his new compression algorithm, and demonstrates it. Hendricks' algorithm strongly outperforms Nucleus and he is mobbed by eager investors.		In the immediate aftermath of their TechCrunch Disrupt victory, multiple venture capital firms offer to finance Pied Piper's Series A round. However, while expressing interest, several venture capitalists criticize Hendricks' lack of perceived direction and to come back with a more coherent "vision". Bachman insists that this is a strategy to lower Pied Piper's valuation. He responds to each offer by insulting each venture capital firm. One offer from the company End Frame in particular is revealed to be a scam to steal trade secrets from Pied Piper developers. Peter Gregory dies while on vacation and is replaced by Laurie Bream (Suzanne Cryer) to run Raviga Capital. Bream gives Richard the highest offer of all the VC firms: 20% equity at a $100 million valuation. Monica privately visits Richard to urge them to decline the offer, calling it a "runaway valuation that they could never live up to", which would result in diluting Series A investors in future financing rounds. Richard offers Bream the same 20% equity but at a $50 million valuation.		Before he can collect the $10 million, Richard finds out at Peter Gregory's funeral that Hooli is suing Pied Piper for copyright infringement, claiming that Richard developed Pied Piper's compression algorithm on Hooli time using company equipment. The Pied Piper team knows the lawsuit is frivolous.		Raviga retracts its offer. This has a domino effect. All the other VC firms retract their offers claiming Bachman's behavior was "rude" and that the lawsuit added too much uncertainty. LaFlamme, Pied Piper's attorney, estimates the cost of the lawsuit to be $2 – 2.5 million with the first $80,000 due immediately. Pied Piper cannot afford this retainer, but Hendricks receives a phone call from Gavin Belson. Richard secretly meets Belson at a Mexican restaurant, where Belson offers to buy out Pied Piper at a higher valuation than the initial $10 million offer. Richard rejects outright claiming that he doesn't want his compression algorithm to become the property of the heartless Hooli corporation. Belson convincingly argues that Pied Piper is no different: the ultimate objective of any company is to scale and become a publicly traded corporation just like Hooli. He insists the lawsuit will bankrupt Pied Piper and that Richard should get something out of his company while still possible. Gilfoyle, Dinesh and Bachman reject the buyout while Monica and Jared support it.		As Richard is about to accept Belson's offer, he is confronted by Russ Hanneman (Chris Diamantopoulos), the wealthy man who "put radio on the Internet". Hanneman offers them $5 million despite the lawsuit and Richard turns down Hooli's buyout offer. Richard quickly begins questioning his decision after learning about Hanneman's mercurial reputation and his excessive interference in day-to-day operation. Belson meets with his litigators to discuss a strategy for the lawsuit. They decide to promote Big Head to Hooli's "moonshot" department, Hooli [xyz], to make people think he created the compression algorithm and Richard stole it to create Pied Piper.		When Hooli Nucleus fails to carry 4K video at a pay-per-view event, Pied Piper tries to generate publicity by live streaming a stunt for an energy drink company, Homicide. Despite a promising rollout, Erlich's past with the Homicide CEO and issues with the stunt driver complicate matters to a point that Pied Piper ends up quitting the job and instead live-streaming video of an unhatched condor egg. End Frame picks the event up for Homicide with a functioning, but lossy, 4K stream based on the Pied Piper algorithm, leaving Richard infuriated but without legal options to stop End Frame.		The Pied Piper team confronts End Frame about End Frame's theft of intellectual property, and during the visit End Frame brags that their large sales department will allow them to be successful despite their technical inferiority. Hanneman arranges for End Frame to buy Pied Piper, but Richard rejects the deal. Gilfoyle reveals that a post-it note with the administrator username and password he took from End Frame allowed him to retrieve sales contracts Pied Piper could use to poach End Frame's customers. Richard approaches the CEO of Intersite, a porn company with whom End Frame was negotiating a $15 million deal, and offers them a technically superior deal. The Intersite CEO proposes a "bake-off" between End Frame and Pied Piper, to see who can better compress their video data, but the competition is called off when Hanneman accidentally deletes a large portion of Intersite's video library from Intersite's servers.		Big Head finds a prototype Nucleus phone left behind at a bar and, stunned at how bad Nucleus really is, provides it to Richard to use as leverage against Belson. Belson agrees to drop the lawsuit in favor of binding arbitration to prevent the press from finding out about the phone. At the arbitration, Hooli's lawyers don't appear to have a real case. However, an unintentional slip by Bachman leads Hooli to realize that Richard had used a Hooli computer to run a single test of Pied Piper, meaning that per the terms of Richard's employment contract, Hooli owns the rights to Pied Piper. Hooli calls Richard as a witness and, unwilling to lie under oath, Richard admits that he used a Hooli computer. However, while reviewing Jared's contract with Hooli as part of the lawsuit, since Belson also sued for the illegal hiring of Jared from Hooli, the judge discovers that the contract has a clause that makes it unenforceable. Since Richard's contract also has this clause, along with many Hooli employees, the lawsuit is ruled in Pied Piper's favor. Thinking that they had lost Pied Piper to Hooli, Richard sends a text to the team to delete all of the Pied Piper code, but the deletion program crashes before any damage could be done.		Meanwhile, the museum providing the video of the condor egg decides to remove the camera due to low viewership numbers, but the technician taking it down falls and becomes trapped with the camera in a ravine. The feed of the injured technician goes viral, forcing Gilfoyle, Dinesh, Jared and Bachman to scramble to keep their servers online. Despite the high server load starting a small fire, the feed remains online until the technician is rescued.		After it is clear that Hooli has no claim on Pied Piper, Raviga, impressed by Pied Piper's performance during the live stream, buys out Russ Hanneman's stake in Pied Piper, securing three of Pied Piper's five board seats. However, due to the previous incidents with Intersite and Homicide, Raviga has little confidence in Pied Piper's leadership. As Pied Piper celebrates their arbitration victory, Richard is notified that the now Raviga-run board has voted to remove him from the CEO position.		After failing to convince the board of directors to keep him on as Pied Piper CEO instead of demoting him to Chief Technology Officer, Richard threatens to quit and sue to regain his intellectual property. Richard meets with a company called Flutterbeam that wants to hire him as CTO. However, after being disappointed by their work, he rejects the offer and decides to stay with Pied Piper. Afterwards, Richard meets with Jack Barker, Raviga's choice for the CEO of Pied Piper. Richard struggles under Barker's leadership, which includes spending money on extravagant offices, completely changing the business model to one that goes against the company ethos, and later forcing Richard to work on an idea that he himself came up as a poor idea to demonstrate his frustration. Eventually, when the time comes to pitch the idea to the board, Monica sides with Richard and votes against it. Subsequently, Laurie fires Jack in response to his mocking that she could do nothing to stop him, and she permits the team to commence work on the platform. The CEO position is left empty by Laurie.		Meanwhile, at Hooli, Gavin Belson discovers that the now invalid employment contracts would allow him to fire affected employees without severance and take back unvested stock options. Belson fires the entire Nucleus team, and uses the profits from the reclaimed stock options to offer Big Head a $20 million severance package in exchange for non-disclosure and non-disparagement agreements. The fired Nucleus team goes to work for Endframe, and manage to set up a rival product to Pied Piper, which Gavin purchases for $250 million oblivious to the fact that he is hiring back the programmers he just fired. Big Head uses his $20 million to set up his own Incubator under-cutting Erlich, so Erlich partners with him and then exploits Big Heads's lack of business savvy to take control. However, because of Big Head and Erlich's spending habits (including a multimillion-dollar Luau on Alcatraz) they declare bankruptcy, and Erlich is forced to sell his stake in Pied Piper to repay the debts. Meanwhile, after a story about Gavin scrubbing the Internet of bad press about himself is leaked by Big Head, and an attempt to steal a copy of the Pied Piper beta leads to the resignation of all of Endframe, the Hooli board of directors vote to remove him as CEO. After being removed as CEO, Gavin decides to go on a holiday to take his mind off everything. At the airport which hosts his private jet, he bumps into Jack Barker, the now-fired CEO of Pied Piper. They have a little chat, wherein Gavin incorporates Jack into his Hooli revival. Gavin now makes a new presentation in front of the board of directors. where he tells them of his "strategy" to keep up with the industry. He introduces Jack Barker as the new head of development and starts working on the "box" that Jack had set his mind to at Pied Piper.		After regaining his CEO position, Richard fires the staff members Jack hired and moves the company back into Erlich's house. The company hires contract engineers from around the world to help construct their application platform. Dinesh becomes attracted to one of them, a woman in Estonia, and initially she seems to reciprocate his feelings; he hacks together a superior video chat application using Pied Piper's algorithm to better pursue her, only to have her lose interest when she sees what he looks like over high-fidelity video (also revealing that she "has a boyfriend").		Eventually their platform reaches a point where they invite friends in the industry to test it, to universally positive reviews. The only negative response is from Monica, who says that she just doesn't understand how it works and that it seems too "engineered". She encourages Richard to trust his own instincts and release the platform if he feels it is ready, which he does.		After release, hundreds of thousands of people download the Pied Piper platform, leading to thoughts of a Series B funding round, and a sense of triumph when the Hooli board decides to allow it to be sold in the online Hooli Store. However, only a small fraction of the people installing the platform remain as daily active users. Focus groups investigating why this is the case reveal that Monica's instincts were correct – although it is an engineering triumph, most people do not understand how the platform works and find its design confusing and counter-intuitive. Richard decides that the best way to address this is through an outreach program to try and better explain the system, but this effort fails. Despondent, Richard expects the company to close down, but regains confidence at a sudden uptick in usership – which was due to Jared secretly employing a click farm in Bangladesh to artificially inflate usage statistics.		Richard soon discovers the deception and confronts Jared about it, and they agree to keep the secret to themselves, but Dinesh and Gilfoyle soon realize what is happening. Dinesh gives Richard a scrambling program which would hide the evidence in the case of an audit by future investors or regulators, which Richard seems prepared to use. Erlich, not knowing the real nature of the uptick, starts rumors about it and plays competing VCs against each other, leading to a very lucrative potential Series B funding deal from Raviga rival Coleman Blair. But before the deal is signed, Jared implores Richard not to take it, because it is based on fraud. An anxious Richard reveals the source of the uptick at the signing meeting, leading to the deal being scrapped and the company's reputation plummeting. Laurie no longer wishes for Raviga to be associated with Pied Piper and moves to sell majority control to any investor. At first the only person willing to buy seems to be Gavin Belson, who wishes to use the purchase to shut Pied Piper down permanently, but an unexpected windfall from the sale of a blog they bought while business partners leads Erlich and Big Head to buy control of the company. The original team (along with Monica, who was fired from Raviga for standing up to Laurie), having regained control, prepares to pivot again, this time to become a video chat company, based on the sudden popularity of Dinesh's video chat application which he included on the platform.		Pied Piper pivots to PiperChat and Richard steps down as CEO. Richard suggests Dinesh as the new CEO of PiperChat. Richard instead decides to create a new internet, that would be powered by a network of cell phones without any firewalls, viruses and government regulations getting in the way. Richard plans on using his compression algorithm to compress files and make the new internet more compact and easier to use. The team eventually discover their app is in violation of the Children's Online Privacy Protection Act and would be fined $21 billion if he's caught. Gavin Belson buys out PiperChat despite not knowing its legal troubles. As a result of this, Gavin puts Hooli on the hook for all of the COPPA violations, and although Jack finds a way to delete the data and shut down the app before an audit can take place, this proves to be the last straw for the Hooli board of directors, who remove him as CEO and expel him from the company.		Richard discovers in Peter Gregory's old files that he had seen the potential in a decentralised, peer-to-peer internet back in the 1990s, but had abandoned it due to the lack of processing power at the time. Richard soon learns that the idea had been patented by Gavin Belson when he, along with Peter Gregory and others, founded Hooli. Richard is able to convince Gavin to sign over the patent and they become business partners. Richard hires Jared and Gilfoyle for the project, with Dinesh soon joining afterward. A disillusioned Gavin decides to leave Palo Alto, handing Richard full ownership of the decentralized internet patent.		Richard is fed up with trying to convince investors and instead goes directly to the customers, presenting the project to companies so they buy the product even if it's not built yet. After several failed meetings, FGI, an insurance company accepts their deal. Erlich inadvertently crashes into a tech meeting with Keenan Feldspar, whose VR headset is the Valley's latest sensation. Having previously failed to convince Monica and Laurie to hire him at their new VC company Bream–Hall, he uses Feldspar and his promising tech as leverage to get a job with them. Keenan Feldspar signs with Bream-Hall, but Monica and Laurie avoid hiring Erlich by giving him a generous finder's fee.		Richard needs to attract investment from another client due to FGI's recent surge in data processing requirements as they are responsible for the overages. Richard meets with Keenan Feldspar, even though Richard thinks VR tech is a fad. Without Richard knowing, Dinesh and Gilfoyle integrate Pied Piper's middle out compression algorithm into Feldspar's VR, which greatly improves it. As a result, Feldspar offers to acquire Pied Piper altogether. Richard doesn't want this, so he counters with an offer of $25 million, with hopes that the offer will be rejected, but Feldspar still accepts. Richard intends to concede, but Monica warns him that Feldspar's VR doesn't have future value, promising that she will fund their series A round if they can implement the decentralized internet project on a sufficient number of cell phones. After texting him telling him the deal is off, Richard learns that Feldspar has already signed with Hooli, abandoning Erlich in the process.		Richard decides to enter Hooli-Con, Hooli's tech conference, to put Monica's challenge into practice. The team heads to Hooli-Con with a plan to mask the Pied Piper app as the event's Wi-Fi access point using strategically placed routers, in order to get Pied Piper installed in the maximum number of phones possible. After running into his ex-girlfriend Winnie, who is presenting her boyfriend's app, Richard sabotages their laptop's screensaver, which triggers a 'tactical response team' sent by Hoover, Hooli's head of security. He tries to warn Jack Barker that Pied Piper is up to something, but is violently dismissed and threatened to be fired. After Hoover finally catches Dinesh and Gilfoyle red-handed on CCTV, he lets them go as Richard once collaborated with Gavin Belson to hinder Jack Barker's plans at Hooli and he sympathizes with this. Pied Piper finally gets installed on a sufficient number of phones, which allows Richard to move all of FGI's data into the app, eliminating the need for an auxiliary data server and its related costs. Jack Barker and Keenan Feldspar present their VR mobile tech to the world, but it fails miserably and causes Hooli phones all over the auditorium to explode. Meanwhile, Erlich decides to accept Gavin Belson's invitation (originally addressed to Richard) to come and join him at a monastery in Tibet, to Belson's shock.		After the incident with the exploding Hooli phones, Jack Barker intends to replace all 9 million affected devices instead of releasing a firmware update. This puts Pied Piper in risk of breaching contract with FGI, as the cell phone network where the data is stored will be eliminated. Richard decides to use 'Anton', Pied Piper's server, to store the data, even if this will exceed its capacity and probably destroy it. Barker heads to China and tries to force the Hooli plant workers to increase production, but he is taken hostage until their demands are met. Gavin Belson, in Tibet, learns about this through Erlich and heads to China, rescues Barker and leaves him in Jackson Hole before being reinstated as Hooli's CEO. Meanwhile, after the broadband service at the incubator is cut due to lack of payment, Richard plans to connect Anton to Stanford Tech's mainframe, using Big Head's position there to do so. When they arrive, they discover that the back of the truck wasn't closed and Anton's parts have been scattered all over the road. When they head to FGI to face Melcher, the team finds that Anton had backed itself up to Jian-Yang's smart refrigerator, as Gilfoyle used some of the Pied Piper code when he was trying to hack it, which in turn connected itself to a network of other refrigerators like it, distributing the data and proving the decentralized internet is a working concept. Melcher attacks Richard, as he finally found out he had sex with his fiancée. Gavin Belson offers a very generous acquisition deal to Richard, who turns it down and decides to be funded by Bream–Hall. Gavin threatens to devour Pied Piper, but Richard answers back, promising to render the entire mainframe business, and Hooli itself, obsolete.		Co-creator and executive producer Mike Judge had worked in a Silicon Valley startup early in his career. In 1987 he was a programmer at Parallax, a company with about 40 employees. Judge disliked the company's culture and his colleagues ("The people I met were like Stepford Wives. They were true believers in something, and I don't know what it was") and quit after less than three months, but the experience gave him the background to later create a show about the region's people and companies.[5] He recollects also how startup companies pitched to him to make a Flash-based animation in the past as material for the first episode: "It was one person after another going, 'In two years, you will not own a TV set!' I had a meeting that was like a gathering of acolytes around a cult leader. 'Has he met Bill?' 'Oh, I'm the VP and I only get to see Bill once a month.' And then another guy chimed in, 'For 10 minutes, but the 10 minutes is amazing!'"[5]		Filming for the pilot of Silicon Valley began on March 12, 2013, in Palo Alto, California.[1] HBO green-lit the series on May 16, 2013.[6]		Christopher Evan Welch, who plays billionaire Peter Gregory, died in December 2013 of lung cancer, having finished his scenes for the first five episodes.[7] The production team decided against recasting the role and reshooting his scenes; on his death, Judge commented: "The brilliance of Chris' performance is irreplaceable, and inspired us in our writing of the series."[8] He went on to say, "The entire ordeal was heartbreaking. But we are incredibly grateful to have worked with him in the brief time we had together. Our show and our lives are vastly richer for his having been in them."[9] In the eighth episode of season 1, a memoriam is made in his honor at the end of the credits roll.[10] The character of Peter Gregory was not killed off until the premiere of Season 2.[11]		The show refers to a metric in comparing the compression rates of applications called the Weissman score, which did not exist before the show's run. It was created by Stanford Professor Tsachy Weissman and graduate student Vinith Misra at the request of the show's producers.[12][13]		In May 2017, it was announced that T.J. Miller would be exiting the series after the fourth season.[14]		Silicon Valley has received critical acclaim since its premiere. Metacritic, a website that gathers critics' reviews, presents the first season with an 84 out of 100 Metascore based on 36 reviews, indicating "universal acclaim".[15] Similarly, Rotten Tomatoes presented the first season with a 94% "Certified Fresh" rating and an average score of 7.94 out of 10 based on 50 reviews, with the critical consensus "Silicon Valley is a relevant, often hilarious take on contemporary technology and the geeks who create it that benefits from co-creator Mike Judge's real-life experience in the industry."[16]		The second season also received critical acclaim, and has a score of 86 out of 100 based on nine reviews from Metacritic.[17] On Rotten Tomatoes, the season received a 100% rating with an average rating of 8.3 out of 10 based on 19 reviews. The site's consensus reads, "Silicon Valley re-ups its comedy quotient with an episode that smooths out the rough edges left behind by the loss of a beloved cast member."[18]		Its third season also received critical acclaim. On Metacritic, the season has a score of 90 out of 100 based on 15 reviews, indicating "universal acclaim".[19] On Rotten Tomatoes, the season received a 100% rating with an average rating of 8.5 out of 10 based on 17 reviews. The site's consensus reads, "Silicon Valley's satirical take on the follies of the tech industry is sharper than ever in this very funny third season."[20]		The fourth season has received critical acclaim. On Metacritic, the season has a score of 85 out of 100 based on 10 reviews, indicating "universal acclaim".[21] On Rotten Tomatoes, the season received a 100% rating with an average rating of 7.8 out of 10 based on 17 reviews.[22]		Tim Goodman of The Hollywood Reporter said "HBO finds its best and funniest full-on comedy in years with this Mike Judge creation, and it may even tap into that most elusive thing, a wide audience."[23] Matt Roush of TV Guide said "The deft, resonant satire that helped make Judge's Office Space a cult hit takes on farcical new dimension in Silicon Valley, which introduces a socially maladroit posse of computer misfits every bit the comic equal of The Big Bang Theory's science nerds."[24] Todd VanDerWerff of The A.V. Club said "It feels weirdly like a tech-world Entourage—and that's meant as more of a compliment than it seems."[25] Brian Tallarico of RogerEbert.com praised the jokes of the series but commented on the slow progression of the character development in the first two episodes and the reliance on common stereotypes in technology, including "the nerd who can't even look at a girl much less talk to her or touch her, the young businessman who literally shakes when faced with career potential." He goes on to state that the lack of depth to the characters creates "this odd push and pull; I want the show to be more realistic but I don't care about these characters enough when it chooses to be so."[26]		David Auerbach of Slate stated that the show did not go far enough to be called risky or a biting commentary of the tech industry. "Because I'm a software engineer, Silicon Valley might portray me with my pants up to my armpits, nerdily and nasally complaining that Thomas' compression algorithm is impossible or that nine times F in hexadecimal is 87, not 'fleventy five' (as Erlich says), but I would forgive such slips in a second if the show were funny."[27] Auerbach claimed that he used to work for Google, and that his wife also worked for them at the time of the review.[27]		Elon Musk, after viewing the first episode of the show, said: "None of those characters were software engineers. Software engineers are more helpful, thoughtful, and smarter. They're weird, but not in the same way. I was just having a meeting with my information security team, and they're great but they're pretty weird—one used to be a dude, one's super small, one's hyper-smart—that's actually what it is. [...] I really feel like Mike Judge has never been to Burning Man, which is Silicon Valley [...] If you haven't been, you just don't get it. You could take the craziest L.A. party and multiply it by a thousand, and it doesn't even get close to what's in Silicon Valley. The show didn't have any of that."[28]		In response to Musk's comments, actor T.J. Miller, who plays Erlich on the show, pointed out that "if the billionaire power players don’t get the joke, it’s because they’re not comfortable being satirized... I’m sorry, but you could tell everything was true. You guys do have bike meetings, motherfucker.” Other software engineers who also attended the same premiere stated that they felt like they were watching their "reflection".[28]		In January 2017, in an audience interaction by Bill Gates and Warren Buffett, Gates recounted the episode in Silicon Valley where the main protagonists try to pitch their product to different venture capitalists reminding him of his own experiences.[29]		The complete first season was released on DVD and Blu-ray on March 31, 2015; bonus features include audio commentaries and behind-the-scenes featurettes.[49] The second season was released on DVD and Blu-ray on April 19, 2016; bonus features include six audio commentaries, a behind-the-scenes featurette, and deleted scenes.[50] The third season was released on DVD and Blu-ray on April 11, 2017; bonus features include deleted scenes.[51] The fourth season will be released on DVD and Blu-ray on September 12, 2017; bonus features include deleted scenes.[52]		In Australia, the series premiered on April 9, 2014, and aired on The Comedy Channel.[53] In the United Kingdom, it premiered on July 16, 2014, and aired on Sky Atlantic, while also being available on internet view-on-demand services such as Blinkbox.[54] In New Zealand, the series airs on pay TV network Sky, on the SoHo channel.[55]		
Slang refers to words, phrases and uses that are regarded as very informal and often restricted to special context or peculiar to a specified profession class and the like. Slang words are used in specific social groups, like teenagers.						In its earliest attested use (1756), the word slang referred to the vocabulary of "low or disreputable" people. By the early nineteenth century, it was no longer exclusively associated with disreputable people, but continued to be applied to language use below the level of standard educated speech.[1] The origin of the word is uncertain, although it appears to be connected with thieves' cant. It's something. A Scandinavian origin has been proposed (compare, for example, Norwegian slengenavn, which means "nickname"), but is discounted by the Oxford English Dictionary based on "date and early associations". Jonathan Green however agrees with the possibility of a Scandinavian origin, suggesting the same root as that of sling, which means "to throw", and noting that slang is a thrown language - a quick, honest way to make your point.[1][2][3]		Few linguists have endeavored to clearly define what constitutes slang. However, what is known is that slang is a linguistic phenomenon ever present and consistently changing among every subculture worldwide. Some suggest that the reason slang exists is because we must come up with new ways of defining new experiences that have surfaced with time and modernity.[4] Attempting to remedy the lack of a clear definition, however, Bethany K. Dumas and Jonathan Lighter argue that an expression should be considered "true slang" if it meets at least two of the following criteria:[4]		Michael Adams remarks that, "[Slang] is liminal language... it is often impossible to tell, even in context, which interests and motives it serves... slang is on the edge."[5] Slang dictionaries, collecting thousands of slang entries, offer a broad, empirical window into the motivating forces behind slang".[6]		While many forms of language may be considered "sub-standard", slang remains distinct from colloquial and jargon terms because of its specific social contexts. While considered inappropriate in formal writing, colloquial terms are typically considered acceptable in speech across a wide range of contexts, while slang tends to be considered unacceptable in many contexts. Jargon refers to language used by personnel in a particular field, or language used to represent specific terms within a field to those with a particular interest. Although jargon and slang can both be used to exclude non–group members from the conversation, the intention of jargon is to optimize conversation using terms that imply technical understanding. On the other hand, slang tends to emphasize social and contextual understanding.		While colloquialisms and jargon may seem like slang because they reference a particular group, they do not fit the same definition, because they do not represent a particular effort to replace standard language. Colloquialisms are considered more standard than slang, and jargon is often created to talk about aspects of a particular field that are not accounted for in the standard lexicon.[7]		It is often difficult to differentiate slang from colloquialisms and even more standard language, because slang generally becomes accepted into the standard lexicon over time. Words such as "spurious" and "strenuous" were once slang, though they are now accepted as standard, even high register words. The literature on slang even discusses mainstream acknowledgment of a slang term as changing its status as true slang, because it has been accepted by the media and is thus no longer the special insider speech of a particular group. Nevertheless, a general test for whether a word is a slang word or not is whether it would be acceptable in an academic or legal setting, as both are arenas in which standard language is considered necessary and/or whether the term has been entered in the Oxford English Dictionary, which some scholars claim changes its status as slang.[7]		It is often difficult to collect etymologies for slang terms, largely because slang is a phenomenon of speech, rather than written language and etymologies which are typically traced via corpus.		Eric Partridge, cited as the first to report on the phenomenon of slang in a systematic and linguistic way, postulated that a term would likely be in circulation for a decade before it would be written down.[8] Nevertheless, it seems that slang generally forms via deviation from a standard form. This "spawning" of slang occurs in much the same way that any general semantic change might occur. The difference here is that the slang term's new meaning takes on a specific social significance having to do with the group the term indexes.		Coleman also suggests that slang is differentiated within more general semantic change in that it typically has to do with a certain degree of “playfulness". The development of slang is considered to be a largely “spontaneous, lively, and creative” speech process.[8]		Still, while a great deal of slang takes off, even becoming accepted into the standard lexicon, much slang dies out, sometimes only referencing a group. An example of this is the term "groovy" which is a relic of 1960's and 70's American "hippy" slang. Nevertheless, for a slang term to become a slang term, people must use it, at some point in time, as a way to flout standard language.[7] Additionally, slang terms may be borrowed between groups, such as the term "gig" which was originally coined by jazz musicians in the 1930s and then borrowed into the same hippy slang of the 1960s.[7] 'The word "groovy" has remained a part of subculture lexicon since its popularization. It is still in common use today by a significant population. The word "gig" to refer to a performance very likely originated well before the 1930s, and remained a common term throughout the 1940s and 1950s before becoming a vaguely associated with the "hippy slang of the 1960s". The word "gig" is now a widely accepted synonym for a concert, recital, or performance of any type. "Hippy" is more commonly spelled "hippie".		Generally, slang terms undergo the same processes of semantic change that words in the regular lexicon do.[8]		Slang is usually associated with a particular group and plays a role in constructing our identities. While slang outlines social space, attitudes about slang partly construct group identity and identify individuals as members of groups. Therefore, using the slang of a particular group will associate an individual with that group. Using Silverstein's notion of different orders of indexicality, it can be said that a slang term can be a second-order index to this particular group. Employing a slang term, however, can also give an individual the qualities associated with the term's group of origin, whether or not the individual is actually trying to identify as a member of the group. This allocation of qualities based on abstract group association is known as third-order indexicality.		As outlined by Elisa Mattiello in her book,[9] a slang term can take on various levels of identification. Giving the examples of the terms "foxy" and "shagadelic", Mattiello explains that neither term makes sense given a standard interpretation of English:		Nevertheless, Matiello concludes that those agents who identify themselves as "young men" have "genuinely coined" these terms and choose to use them over "canonical" terms —like beautiful or sexy—because of the indexicalized social identifications the former convey.		In terms of first and second order indexicality, the usage of speaker-oriented terms by male adolescents indicated their membership to their age group, to reinforce connection to their peer group, and to exclude outsiders.[9]		In terms of higher order indexicality, anyone using these terms may desire to appear fresher, undoubtedly more playful, faddish, and colourful than someone who employs the standard English term "beautiful". This appearance relies heavily on the hearer's third-order understanding of the term's associated social nuances and presupposed use-cases.[9]		Often, distinct subcultures will create slang that members will use in order to associate themselves with the group, or to delineate outsiders.		Slang terms are often known only within a clique or ingroup. For example, Leet ("Leetspeak" or "1337") was originally popular only among certain Internet subcultures, such as software crackers and online video gamers. During the 1990s, and into the early 21st century, however, Leet became increasingly more commonplace on the Internet, and it has spread outside Internet-based communication and into spoken languages.[11] Other types of slang include SMS language used on mobile phones, and "chatspeak", (e.g., "LOL", an acronym meaning "laughing out loud" or "laugh out loud" or ROFL, "rolling on the floor laughing"), which are widely used in instant messaging on the Internet.[12]		As subcultures are also often forms of counterculture and counterculture itself can be defined as going against a standard, it follows that slang has come to be associated with counterculture.		Slang is often taken from social media as a sign of social awareness and shared knowledge of popular culture. This particular branch of slang has become more prevalent since the early 2000s as a result of the rise in popularity of social networking services, including Facebook, Twitter, and Instagram. This has created new vocabularies associated with each new social media venue, such as the use of the term “friending” on Facebook, which is a verbification of “friend” used to describe the process of adding a new person to one's list of friends on the website, despite the existence of an analogous term “befriend“. This term is much older than Facebook, but has only recently entered the popular lexicon.[13] Other examples of the slang found in social media include a general trend toward shortened words or acronyms. These are especially associated with services such as Twitter, which has a 140 character limit for each message and therefore requires a briefer, more condensed manner of communication.[14] This includes the use of hashtags which explicitly state the main content of a message or image, such as #food or #photography.[15]		Some critics believe that when slang language becomes more commonplace it effectively eradicates the proper use of a certain language. However, other linguists believe that language is not static but ever-changing and that slang terms are valid words within a language's lexicon. While prescriptive linguists study and analyze the so-called "correct" ways to speak, according to a language's grammar and syntactical words, descriptive linguists tend to study language to further understand the subconscious rules of how individuals speak, which makes slang important in understanding such rules. Noam Chomsky, a founder of anthropological linguistic thought, challenged structural and prescriptive linguistics and began to study sounds and morphemes functionally, as well as their changes within a language over time.[16]		
Towel Day is celebrated every year on 25 May as a tribute to the author Douglas Adams by his fans.[1] On this day, fans openly carry a towel with them, as described in Adams' The Hitchhiker's Guide to the Galaxy or share their folded animal towels[2] to demonstrate their appreciation for the books and the author. The commemoration was first held 25 May 2001, two weeks after Adams' death on 11 May.[3]						The importance of the towel was introduced in The Hitchhiker's Guide to the Galaxy original radio series in 1978. The follow-up book explained the importance of towels in The Hitchhiker's Guide to the Galaxy universe in Chapter 3, using much of the same wording as the original radio series:		A towel, it says, is about the most massively useful thing an interstellar hitchhiker can have. Partly it has great practical value. You can wrap it around you for warmth as you bound across the cold moons of Jaglan Beta; you can lie on it on the brilliant marble-sanded beaches of Santraginus V, inhaling the heady sea vapours; you can sleep under it beneath the stars which shine so redly on the desert world of Kakrafoon; use it to sail a miniraft down the slow heavy River Moth; wet it for use in hand-to-hand-combat; wrap it round your head to ward off noxious fumes or avoid the gaze of the Ravenous Bugblatter Beast of Traal (such a mind-bogglingly stupid animal, it assumes that if you can't see it, it can't see you — daft as a brush, but very very ravenous); you can wave your towel in emergencies as a distress signal, and of course dry yourself off with it if it still seems to be clean enough.		More importantly, a towel has immense psychological value. For some reason, if a strag (strag: non-hitch hiker) discovers that a hitchhiker has his towel with him, he will automatically assume that he is also in possession of a toothbrush, face flannel, soap, tin of biscuits, flask, compass, map, ball of string, gnat spray, wet weather gear, space suit etc., etc. Furthermore, the strag will then happily lend the hitch hiker any of these or a dozen other items that the hitch hiker might accidentally have "lost." What the strag will think is that any man who can hitch the length and breadth of the galaxy, rough it, slum it, struggle against terrible odds, win through, and still knows where his towel is, is clearly a man to be reckoned with.		The original article that began Towel Day was posted by a user "Clyde" (probably D. Clyde Williamson[5]) at "System Toolbox", a short-lived open source forum.[6]		Towel Day: A Tribute to Douglas Adams Monday 14 May 2001 06:00am PDT		Clyde ponders the passing of an incredible mind and proposes a "Towel Day" in tribute.		Friday morning I went to breakfast at Big Boy's (mmmm, Breakfast Bar); oddly enough, we sat in booth "42" and had a good laugh about it being the answer to "Life, The Universe and Everything". Later that day, headlines flashed the news that Douglas Adams, creator of the longest trilogy in history, had died. I was stunned; it was remnicient of the loss I felt when Jim Henson died.		Douglas Adams will be missed by his fans worldwide. So that all his fans everywhere can pay tribute to this genius, I propose that two weeks after his passing (May 25, 2001) be marked as "Towel Day". All Douglas Adams fans are encouraged to carry a towel with them for the day.		Make sure that the towel is conspicuous- use it as a talking point to encourage those who have never read the Hitchhiker's Guide to go pick up a copy. Wrap it around your head, use it as a weapon, soak it in nutrients- whatever you want!		Most minds in the universe are constrained to the laws of Physics; let us remember those that broke the law and got away with it.		So long Douglas, and thanks for all the fish!		Chris Campbell and his friends registered the domain towelday.org to promote the day, reminding people to bring their towels. Towel Day was an immediate success among fans and many people sent in pictures of themselves with their towels.[7]		Several news sources around the world have mentioned Towel Day, including the Norwegian newspaper Aftenposten[8] and the television news show NRK Nyheter,[9] and National Public Radio, Los Angeles.[10]		In May 2010, an online petition was created asking Google to recognize Towel Day with either a Google Doodle or by returning search results in the Vogon language for a day.[11] As of 10 September 2014, the petition had received 5,373 signatures;[11] however, the petition website is now defunct.[12]		In Canada, Volt, a French/English television show, created a skit in which Towel Day was explained and featured.[citation needed]		In Ecuador, Radio City, a BBC affiliated radio station, interviewed one of the organizers of Towel Day in Toronto to introduce their listeners to Towel Day.[13] The interview was in Spanish and English.		In the United Kingdom, Planet Rock aired an "Alternative Thought Of The Day" by David Haddock about Towel Day[14] and Siren FM broadcast "Dean Wilkinson & the Importance of International Towel Day".[15]		In January 2012, The Huffington Post listed Towel Day as one of ten cult literary traditions.[16]		In recognition of Towel Day, the Norwegian public transportation company Kolumbus gave away a limited number of special towels to customers. Each towel contained an RFID chip that allows a free ride on their buses and boats.[17] In Washington DC, the Chevy Chase branch of the DC Public Library offered prizes for those who wore a towel to the library on Towel Day.[18]		On Towel Day 2015, astronaut Samantha Cristoforetti sent a "Towel Day greeting" and read aloud a sample from The Hitchhiker's Guide To The Galaxy from the International Space Station.[19]		As Towel Day 2016 fell during the RIPE 72 meeting, the RIPE NCC distributed a beach towel printed with an IPv6 subnet chart (from their training materials). One of those towels was later auctioned at BSDCan 2016 for CAN$142.[20]		Towel Day was celebrated by Briton Tim Peake on the International Space Station.[21]		
Social intelligence is the capability to effectively navigate and negotiate complex social relationships and environments.[1] Social scientist Ross Honeywill believes social intelligence is an aggregated measure of self- and social-awareness, evolved social beliefs and attitudes, and a capacity and appetite to manage complex social change.[2] Psychologist Nicholas Humphrey believes that it is social intelligence, rather than quantitative intelligence, that defines humans.		The original definition by Edward Thorndike in 1920 is "the ability to understand and manage men and women and girls, to act wisely in human relations".[3] It is equivalent to interpersonal intelligence, one of the types of intelligence identified in Howard Gardner's theory of multiple intelligences, and closely related to theory of mind.[4] Some authors have restricted the definition to deal only with knowledge of social situations, perhaps more properly called social cognition or social marketing intelligence, as it pertains to trending socio-psychological advertising and marketing strategies and tactics. According to Sean Foleno, social intelligence is a person’s competence to understand his or her environment optimally and react appropriately for socially successful conduct.[4]						The social intelligence hypothesis states that social intelligence, that is, complex socialization such as politics, romance, family relationships, quarrels, collaboration, reciprocity, and altruism, (1) was the driving force in developing the size of human brains and (2) today provides our ability to use those large brains in complex social circumstances.[5] That is, it was the demands of living together that drove our need for intelligence generally. Archaeologist Steve Mithen believes that there are two key periods of human brain growth that contextualize the social intelligence hypothesis. The first was around two million years ago, when the brain more than doubled, from around 450cc to 1,000cc by 1.8 million years ago. Brain tissue is very expensive metabolically, so it must have served an important purpose. Mithen believes that this growth was because people were living in larger, more complex groups, and had to keep track of more people and relationships, which required a greater mental capacity and so a larger brain.[6]		The second growth in human brain size occurred between 600,000 and 200,000 years ago, when the brain reached its modern size. This growth is still not fully explained. Mithen’s believes that it is related to the evolution of language. Language is probably the most complex cognitive task we undertake. It is directly related to social intelligence because we mainly use language to mediate our social relationships.[6]		So social intelligence was a critical factor in brain growth, social and cognitive complexity co-evolve.[7]		The social intelligence quotient (SQ) is a statistical abstraction, similar to the ‘standard score’ approach used in IQ tests, with a mean of 100. Scores of 140 or above are considered to be very high. Unlike the standard IQ test, it is not a fixed model.[8] It leans more to Jean Piaget’s theory that intelligence is not a fixed attribute but a complex hierarchy of information-processing skills underlying an adaptive equilibrium between the individual and the environment.[9] Therefore, an individual can change their SQ by altering their attitudes and behaviour in response to their complex social environment.[8]		SQ has until recently been measured by techniques such as question and answer sessions. These sessions assess the person's pragmatic abilities to test eligibility in certain special education courses, however some tests have been developed to measure social intelligence. This test can be used when diagnosing autism spectrum disorders, including autism and Asperger syndrome. This test can also be used to check for some non-autistic or semi-autistic conditions such as semantic pragmatic disorder or SPD, schizophrenia, dyssemia and ADHD.[10]		Some social intelligence measures exist which are self-report.[11] Although easy to administer, there is some question as to whether self-report social intelligence measures would better be interpreted in terms of social self-efficacy (that is, one's confidence in one's ability to deal with social information).[12]		People with low SQ are more suited to work with low customer contact, as well as in smaller groups or teams, or independently, because they may not have the required interpersonal communication and social skills for success on with customers and other co-workers.[citation needed] People with SQs over 120 are considered socially skilled, and may work exceptionally well with jobs that involve direct contact and communication with other people.[5]		George Washington University Social Intelligence Test : Is one of the only ability measure available for assessing social intelligence and was created in June 1928 by Dr.Thelma Hunt a psychologist from George Washington University.[13] It was originally proposed as a measurement of a person's capacity to deal with people and social relationships.[14] The test is designed to assess various social abilities which consisted of observing human behavior, social situation judgement, name & face memory and theory of mind from facial expressions.[13] The George Washington University Social Intelligence Test revised second edition consists of items as quoted:[14]		Nicholas Humphrey points to a difference between intelligence as measured by IQ tests and social intelligence. Some autistic children are extremely intelligent because they are very good at observing and memorising information, but they have low social intelligence. Similarly, chimpanzees are very adept at observation and memorisation (sometimes better than humans) but are, according to Humphrey, inept at handling interpersonal relationships. What they lack is a theory of others' minds. For a long time, the field was dominated by behaviorism, that is, the theory that one could understand animals including humans, just by observing their behavior and finding correlations. But recent theories indicate that one must consider the inner structure behaviour.[1]		Both Nicholas Humphrey and Ross Honeywill believe that it is social intelligence, or the richness of our qualitative life, rather than our quantitative intelligence, that makes humans what they are; for example what it is like to be a human being living at the centre of the conscious present, surrounded by smells and tastes and feels and the sense of being an extraordinary metaphysical entity with properties which hardly seem to belong to the physical world. This is social intelligence.[2]		Social intelligence is closely related to cognition and emotional intelligence.[citation needed] Research psychologists studying social cognition and social neuroscience have discovered many principles which human social intelligence operates. In early work on this topic, psychologists Nancy Cantor and John Kihlstrom outlined the kinds of concepts people use to make sense of their social relations (e.g., “What situation am I in and what kind of person is this who is talking to me?”), and the rules they use to draw inferences (“What did he mean by that?”) and plan actions (“What am I going to do about it?”).[15]		M Babu defines social intelligence as "the ability to deal efficiently and thoughtfully, keeping one’s own identity, employing apposite social inputs with a wider understanding of social environment; considering empathetic co-operation as a base of social acquaintance."[16]		More recently, popular science writer Daniel Goleman has drawn on social neuroscience research to propose that social intelligence is made up of social awareness (including empathy, attunement, empathic accuracy, and social cognition) and social facility (including synchrony, self-presentation, influence, and concern).[17] Goleman’s research indicates that our social relationships have a direct effect on our physical health, and the deeper the relationship the deeper the impact. Effects include blood flow, breathing, mood such as fatigue and depression, and weakening of the immune system.[17]		Educational researcher Raymond H. Hartjen asserts that expanded opportunities for social interaction enhances intelligence.[citation needed] This suggests that children require continuous opportunities for interpersonal experiences in order to develop a keen 'inter-personal psychology'.[citation needed] Traditional classrooms do not permit the interaction of complex social behavior. Instead, students in traditional settings are treated as learners who must be infused with more and more complex forms of information. The structure of schools today allows very few of these skills, critical for survival in the world, to develop. Because we so limit the development of the skills of "natural psychologist" in traditional schools, graduates enter the job market handicapped to the point of being incapable of surviving on their own.[citation needed] In contrast, students who have had an opportunity to develop their skills in multi-age classrooms and at democratic settings rise above their less socially skilled peers. They have a good sense of self, know what they want in life and have the skills to begin their quest.[18]		The issue here is psychology versus social intelligence—as a separate and distinct perspective, seldom articulated.[clarification needed] An appropriate introduction contains certain hypothetical assumptions about social structure and function, as it relates to intelligence defined and expressed by groups, constrained by cultural expectations that assert potential realities, but make no claims that there is an "exterior" social truth to be defined. This perspective pursues the view that social structures can be defined with the warning that what is mapped into the structure and how that information is stored, retrieved, and decided upon are variable, but can be contained in an abstract and formal grammar—a sort of game of definitions and rules that permit and project an evolving intelligence. Two halves of the coin: one half psychology; the other half social. Unfortunately, most references to social intelligence relate to an individual's social skills. Not mentioned, and more important, is how social intelligence (speaking of a group or assembly of groups) processes information about the world and shares it with participants in the group(s). Are there social structures or can they be designed to accumulate and reveal information to the individual or to other groups. The bigger question is how groups and societies map the environment (ecological, social and personal) into a social structure. How is that structure able to contain a worldview and to reveal that view to the participants? How are decisions made?		
Mary Bucholtz (born 29 October 1966),[1] is professor of linguistics at UC Santa Barbara. She is well known for her contributions to research on language and identity within sociocultural linguistics, and especially the tactics of intersubjectivity framework developed with Kira Hall.						Bucholtz received her B.A. in Classics from Grinnell College in 1990 and her Ph.D. in Linguistics from UC Berkeley in1997. She has held previous academic positions at Stanford and Texas A&M.		Bucholtz's work focuses largely on language use in the United States, and specifically on issues of language and youth; language, gender, and sexuality; African American English; and Mexican and Chicano Spanish.		
Cyberbullying or cyberharassment is a form of bullying or harassment using electronic forms of contact. Cyberbullying has become increasingly common, especially among teenagers.[1] Harmful bullying behavior can include posting rumors about a person, threats, sexual remarks, disclose victims' personal information, or pejorative labels (i.e., hate speech).[2] Bullying or harassment can be identified by repeated behavior and an intent to harm.[3] Victims may have lower self-esteem, increased suicidal ideation, and a variety of emotional responses, retaliating, being scared, frustrated, angry, and depressed.[4] Individuals have reported that cyberbullying can be more harmful than traditional bullying.[5]		Awareness in the United States has risen in the 2010s, due in part to high-profile cases.[6][7] Several states in the US and in other countries have laws specific to regulating cyberbullying.[8] These laws can be designed to specifically target teen cyberbullying, while others use laws extending from the scope of physical harassment.[9] In cases of adult cyberharassment, these reports are usually filed beginning with local police.[10] Research has demonstrated a number of serious consequences of cyberbullying victimization.[11]		Internet trolling is a common form of bullying over the Internet in an online community (such as in online gaming or social media) in order to elicit a reaction, disruption, or for their own personal amusement.[12][13] Cyberstalking is another form of bullying or harassment that uses electronic communications to stalk a victim may pose a credible threat to the safety of the victim.[14]		A frequently used definition of cyberbullying is "an aggressive, intentional act or behavior that is carried out by a group or an individual, using electronic forms of contact, repeatedly and over time against a victim who cannot easily defend him or herself."[15] There are many variations of the definition, such as the National Crime Prevention Council's more specific definition: "the process of using the Internet, cell phones or other devices to send or post text or images intended to hurt or embarrass another person."[7]		Cyberbullying is often similar to traditional bullying, with some notable distinctions. Victims of cyberbullying may not know the identity of their bully, or why the bully is targeting them. The harassment can have wide-reaching effects on the victim, as the content used to harass the victim can be spread and shared easily among many people and often remains accessible for a long time after the initial incident.[16]		The terms cyberharassment and cyberbullying are sometimes used synonymously, though some people use cyberbullying specifically to refer to harassment among minors or in a school setting.[14]		Cyberstalking is a form of online harassment in which the perpetrator uses electronic communications to stalk a victim. Cyberstalking is considered more dangerous than other forms of cyberbullying because it generally involves a credible threat to the safety of the victim. Cyberstalkers may send repeated messages intended to threaten or harass their victim. They may encourage others to do the same, either explicitly or by impersonating their victim and asking others to contact them.[14]		Internet trolls intentionally try to provoke or offend others in order to elicit a reaction.[12] Trolls and cyberbullies do not always have the same goals: while some trolls engage in cyberbullying, others may be engaged in comparatively harmless mischief. A troll may be disrupt either for their own amusement or because they are genuinely a combative person.[17]		Manuals to educate the public, teachers and parents summarize, "Cyberbullying is being cruel to others by sending or posting harmful material using a cell phone or the internet." Research, legislation and education in the field are ongoing. Research has identified basic definitions and guidelines to help recognize and cope with what is regarded as abuse of electronic communications.		Cyberbullying can be as simple as continuing to send emails or text messages harassing someone who has said they want no further contact with the sender. It may also include public actions such as repeated threats, sexual remarks, pejorative labels (i.e., hate speech) or defamatory false accusations, ganging up on a victim by making the person the subject of ridicule in online forums, hacking into or vandalizing sites about a person, and posting false statements as fact aimed a discrediting or humiliating a targeted person.[20] Cyberbullying could be limited to posting rumors about a person on the internet with the intention of bringing about hatred in others' minds or convincing others to dislike or participate in online denigration of a target. It may go to the extent of personally identifying victims of crime and publishing materials severely defaming or humiliating them.[2]		Cyberbullies may disclose victims' personal data (e.g. real name, home address, or workplace/schools) at websites or forums or may use impersonation, creating fake accounts, comments or sites posing as their target for the purpose of publishing material in their name that defames, discredits or ridicules them. This can leave the cyberbully anonymous which can make it difficult for the offender to be caught or punished for their behavior, although not all cyberbullies maintain their anonymity. Text or instant messages and emails between friends can also constitute cyberbullying if what is said or displayed is hurtful to the participants.		The recent use of mobile applications and rise of smartphones have yielded to a more accessible form of cyberbullying. It is expected that cyberbullying via these platforms will be associated with bullying via mobile phones to a greater extent than exclusively through other more stationary internet platforms. In addition, the combination of cameras and Internet access and the instant availability of these modern smartphone technologies yield themselves to specific types of cyberbullying not found in other platforms. It is likely that those cyberbullied via mobile devices will experience a wider range of cyberbullying types than those exclusively bullied elsewhere.[21]		While most cases are considered to be cyberbullying, some teens argue that most events are simply drama. For example, Danah Boyd writes, "teens regularly used that word [drama] to describe various forms of interpersonal conflict that ranged from insignificant joking around to serious jealousy-driven relational aggression. Whereas adults might have labeled many of these practices as bullying, teens saw them as drama."[22]		Cyberbullying can take place on social media sites such as Facebook, Myspace, and Twitter. "By 2008, 93% of young people between the ages of 12 and 17 were online. In fact, youth spend more time with media than any single other activity besides sleeping."[23] The last decade has witnessed a surge of cyberbullying, bullying that occurs through the use of electronic communication technologies, such as e-mail, instant messaging, social media, online gaming, or through digital messages or images sent to a cellular phone.[24]		There are many risks attached to social media sites, and cyberbullying is one of the larger risks. One million children were harassed, threatened or subjected to other forms of cyberbullying on Facebook during the past year, while 90 percent of social-media-using teens who have witnessed online cruelty say they have ignored mean behavior on social media, and 35 percent have done this frequently. 95 percent of social-media-using teens who have witnessed cruel behavior on social networking sites say they have seen others ignoring the mean behavior, and 55 percent witness this frequently.[25]		According to a 2013 Pew Research study, eight out of ten teens who use social media share more information about themselves than they have in the past. This includes location, images, and contact information.[26] In order to protect children, it is important that personal information such as age, birthday, school/church, phone number, etc. be kept confidential.[27]		Cyberbullying can also take place through the use of websites belonging to certain groups to effectively request the targeting of another individual or group. An example of this is the bullying of climate scientists and activists.[28][29][30]		Of those who reported having experienced online harassment in a Pew Research poll, 16% said the most recent incident occurred in an online game.[13] A study from National Sun Yat-sen University observed that children who enjoyed violent video games were significantly more likely to both experience and perpetrate cyberbullying.[31]		Another study that discusses the direct correlation between exposure to violent video games and cyber bullying also took into account personal factors such as; "duration of playing online games, alcohol consumption in the last 3 months, parents drunk in the last 3 months, anger, hostility, ADHD, and a sense of belonging"[32] as potential contributing factors of cyberbullying.		Gaming was a more common venue for men to experience harassment, whereas women's' harassment tended to occur via social media.[33] Most respondents considered gaming culture to be equally welcoming to both genders, though 44% thought it favored men.[34] Keza MacDonald writes in The Guardian that sexism exists in gaming culture, but is not mainstream within it.[35] Sexual harassment in gaming generally involves slurs directed towards women, sex role stereotyping, and overaggressive language.[36] U.S. President Barack Obama made reference to harassment of women gamers during remarks in honor of Women's History Month.[37]		Competitive gaming scenes have been less welcoming of women that has broader gaming culture.[38] In an internet-streamed fighting game competition, one female gamer forfeited a match after the coach of her team, Aris Bakhtanians, stated, "The sexual harassment is part of the culture. If you remove that from the fighting game community, it's not the fighting game community"[39] The comments were widely condemned by gamers, with comments in support of sexual harassment "drowned out by a vocal majority of people expressing outrage, disappointment and sympathy."[35] The incident built momentum for action to counter sexual harassment in gaming.[39]		In a number of instances, game developers have been subjected to harassment and death threats by players upset by changes to a game or by a developer's online policies.[40] Harassment also occurs in reaction to critics such as Jack Thompson or Anita Sarkeesian, whom some fans see as a threat to the medium.[41][42] Various individuals have been harassed in connection with the Gamergate controversy.[43] Harassment related to gaming is not of a notably different severity or tenor compared to online harassment motivated by other subcultures or advocacy issues.[42]		Sabotage among rival crowdfunding campaigns is a recurring problem for projects related to gaming.[44]		Information cascades happen when users start passing on information they assume to be true, but cannot know to be true, based on information on what other users are doing. Information cascades can be accelerated by search engines' ranking technologies and their tendency to return results relevant to a user's previous interests. This type of information spreading is hard to stop. Information cascades over social media and the Internet may also be harmless, and may contain truthful information.[45]		Bullies use Google bombs (a term applicable to any search engine)[citation needed] to increase the eminence of favored posts sorted by the most popular searches, done by linking to those posts from as many other web pages as possible. Examples include the campaign for the neologism "santorum" organized by the LGBT lobby. Google bombs can manipulate the Internet's search engines regardless of how authentic the pages are, but there is a way to counteract this type of manipulation as well.[45]		A majority of states have laws that explicitly include electronic forms of communication within stalking or harassment laws.[8][46][47] Most law enforcement agencies have cyber-crime units and often Internet stalking is treated with more seriousness than reports of physical stalking.[48][49] Help and resources can be searched by state or area.		The safety of schools is increasingly becoming a focus of state legislative action. There was an increase in cyber-bullying enacted legislation between 2006 and 2010.[50] Initiatives and curriclulum requirements also exist in the UK (the Ofsted eSafety guidance) and Australia (Overarching Learning Outcome 13).		In 2012, a group of teenagers in New Haven, Connecticut developed an application to help fight bullying. Called "Back Off Bully" (BOB), the web app is an anonymous resource for computer, smart phone or iPad. When someone witnesses or is the victim of bullying, they can immediately report the incident. The app asks questions about time, location and how the bullying is happening, as well as providing positive action and empowerment over the incident, the reported information helps by going to a database where administrators study it. Common threads are spotted so others can intervene and break the bully's pattern.[51] BOB, the brainchild of fourteen teens in a design class, is being considered as standard operating procedure at schools across the state. Recent studies carried out among 66 high school teachers have concluded that prevention programs proved ineffective to date.[52]		There are laws that only address online harassment of children or focus on child predators as well as laws that protect adult cyberstalking victims, or victims of any age. Currently, there are 45 cyberstalking (and related) laws on the books. While some sites specialize in laws that protect victims age 18 and under, Working to Halt Online Abuse is a help resource containing a list of current and pending cyberstalking-related United States federal and state laws.[53] It also lists those states that do not have laws yet and related laws from other countries. The Global Cyber Law Database (GCLD) aims to become the most comprehensive and authoritative source of cyber laws for all countries.[54]		Children report negative online behaviors occurring from the second grade. According to research, boys initiate negative online activity earlier than girls do. However, by middle school, girls are more likely to engage in cyberbullying than boys.[55] Whether the bully is male or female, his or her purpose is to intentionally embarrass others, harass, intimidate, or make threats online to one another. This bullying occurs via email, text messaging, posts to blogs, and websites.		Studies in the psycho-social effects of cyberspace have begun to monitor the impacts cyber-bullying may have on the victims, and the consequences it may lead to. Consequences of cyber-bullying are multi-faceted, and affect online and offline behavior. Research on adolescents reported that changes in the victims' behavior as a result of cyber-bullying could be positive. Victims "created a cognitive pattern of bullies, which consequently helped them to recognize aggressive people."[56]		However, the Journal of Psychosocial Research on Cyberspace abstract reports critical impacts in almost all of the respondents', taking the form of lower self-esteem, loneliness, disillusionment, and distrust of people. The more extreme impacts were self-harm. Children have killed each other and committed suicide after having been involved in a cyberbullying incident.[57] Some cases of digital self-harm have been reported, where an individual engages in cyberbullying against themselves, or purposefully and knowingly exposes themselves to cyberbullying.[58][59]		Stalking online has criminal consequences just as physical stalking. A target's understanding of why cyberstalking is happening is helpful to remedy and take protective action to restore remedy. Cyberstalking is an extension of physical stalking.[9] Among factors that motivate stalkers are: envy, pathological obsession (professional or sexual), unemployment or failure with own job or life; intention to intimidate and cause others to feel inferior; the stalker is delusional and believes he/she "knows" the target; the stalker wants to instill fear in a person to justify his/her status; belief they can get away with it (anonymity).[60]		The US federal cyberstalking law is designed to prosecute people for using electronic means to repeatedly harass or threaten someone online. There are resources dedicated to assisting adult victims deal with cyberbullies legally and effectively. One of the steps recommended is to record everything and contact police.[61][10]		The nationwide Australian Covert Bullying Prevalence Survey (Cross et al., 2009)[62] assessed cyber-bullying experiences among 7,418 students. Rates of cyber-bullying increased with age, with 4.9% of students in Year 4 reporting cyberbullying compared to 7.9% in year nine. Cross et al., (2009) reported that rates of bullying and harassing others were lower, but also increased with age. Only 1.2% of Year 4 students reported cyber-bullying others compared to 5.6% of Year 9 students.		Over the mainland of China, cyberbullying seems has yet to receive adequate scholarly. A study investigated the risk factors of cyberbullying, illustrated a sample of 1438 high school students from central China. Data had shown 34.84% were participated bullied and 56.88% had been bullied by online. Students who spend more time on internet have themselves experienced traditional bullying as victims will be more likely to experience cyberbullying through different social media in instant-message.[63]		A study investigated[64] cyberbullying in Hong Kong chose 48 people out of 7654 students from elementary school to high school who were classify as potential aggressors that related to cyberbullying. 31 out of 48 students declared they barely participate in cyber-attack. In is more general among high school students (28 out of 36 students) to participate in social media platform. These students took a survey about cyberbullying: 58% admitted they changed nickname for others, 56.3% for humiliation, 54.2% make fun of someone, 54.2% for spread out rumors. The Hong Kong Federation of Youth Groups had interviewed 1820 teenagers, 17.5% indicated the experience of cyberbully. For example: insult, coarse abuse, publishes personal private pictures with candid camera, and spread out in social media without permission.		In a study published in 2011, across 25 EU member states studied, the average 6% of the children (9–16 years old) have been bullied and only 3% of them confessed to be a bully.[65] However, in an earlier publication of Hasenbrink et al. (2009), reporting on the results from a meta analysis from European Union countries, the authors estimated (via median results) that approximately 18% of European young people had been "bullied/harassed/stalked" via the internet and mobile phones.[66] Cyber-harassment rates for young people across the EU member states ranged from 10% to 52%. The decreasing numbers can caused by developing increasingly specific methods, dividing the tasks into different variables.		In addition to the current research, Sourander et al. (2010) conducted a population-based cross-sectional study that took place in Finland. The authors of this study took the self-reports of 2215 Finish adolescents between the ages of 13 to 16 years old about cyberbullying and cybervictimization during the past 6 months. It was found that, amongst the total sample, 4.8% were cybervictims only, 7.4% were cyberbullies only, and 5.4% were cyberbully-victims.		The authors of this study were able to conclude that cyberbullying as well as cybervictimization is associated not only with psychiatric issues, but psychosomatic issues. Many adolescents in the study reported headaches or difficulty sleeping. The authors believe that their results indicate a greater need for new ideas on how to prevent cyberbullying and what to do when it occurs. It is clearly a worldwide problem that needs to be taken seriously.[67]		The journal article titled "Exploring traditional and cyberbullying among Irish adolescent" studies the Health Behaviour in School-aged Children (HBSC) pilot survey was carried out by 8 post-primary schools across Ireland in which 318 students aged 15–18 years old completed. 59% of these students were boys and 41% were girls. The participation in this survey was completely voluntary for the student and content had to be obtained from the parents as well as students and also the school itself. This survey was also anonymous and confidential. It took one class or 40 minutes to complete by the students. This survey asked questions on traditional forms of bullying as well as cyber bullying, risk behaviours and self-reported health and life satisfaction.		66% of these students said that they have never been bullied. 14% reported that they were victims of the traditional forms of bullying. 10% reported that they were victims of cyber bullying and the remaining 10% said that they were victims of both traditional forms of bullying as well as cyber bullying. It was mostly boys that said they were victims of just traditional forms of bullying, but it was reported that it was mostly girls that were victims of both traditional forms of bullying and cyber bullying. 20% of the students in this survey said that they have been cyber bullied showing that cyber bullying is on the rise.[68] Arrow D.I.T claims that twenty-three percent of 9–16 year olds in Ireland have been bullied on-line or of-line, compared to nineteen percent in Europe.[69] Although, on-line bullying in Ireland at 4% according to Arrow D.I.T is lower than the European average which stands at 6%, and half that of the UK where 8% reported being cyberbullied.[69] As a result, traditional forms of bullying in Ireland is higher than their European counterparts, but lower when it comes to cyberbullying.		According to recent research, in Japan, 17 percent (compared with a 25-country average of 37 percent) of youth between the ages of 8 and 17 have been victim to online bullying activities. The number shows that online bullying is a serious concern in Japan. Teenagers who spend more than 10 hours a week on Internet are more likely to become the target of online bullying. Only 28 percent of the survey participants understood what cyberbullying is. However, they do notice the severity of the issue since 63 percent of the surveyed worry about being targeted as victims of cyberbullying.[70]		With the advance of Internet technology, everyone can access the internet. Since teenagers find themselves congregating socially on the internet via social media, they become easy targets for cyberbullying. Forms of social media where cyberbullying occurs include but are not limited to email, text, chat rooms, mobile phones, mobile phone cameras and social websites (Facebook, Twitter). Some cyberbullies have set up websites or blogs to post the target's images, publicize their personal information, gossip about the target, express why they hate the target, request people to agree with the bully's view, and sending links to the target to make sure they are watching the activity.[71]		Much cyberbullying is an act of relational aggression, which involves alienating the victim from his or her peers through gossip or ostracism.[72] This kind of attack can be easily launched via texting or other online activities. Here is an example of a 19-year-old teenager sharing his real experience of cyberbullying. When he was in high school, his classmates posted his photo online, insulted him constantly, and asked him to die. Because of the constant harassment, he did attempt suicide twice. Even when he quit school, the attacks did not stop.[73]		Cyberbullying can cause serious psychological impact to the victims. They often feel anxious, nervous, tired, and depressed. Other examples of negative psychological trauma include losing confidence as a result being socially isolated from their schoolmates or friends. Mental psychological problems can also show up in the form of headaches, skin problems, abdominal pain, sleep problems, bed-wetting, and crying. It may also lead victims to commit suicide to end bullying.[74]		A survey by the Crimes Against Children Research Center at the University of New Hampshire in 2000 found that 6% of the young people in the survey had experienced some form of harassment including threats and negative rumours and 2% had suffered distressing harassment.[76]		The 2004 I-Safe.org survey of 1,500 students between grades 4 and 8 found:[77]		The Youth Internet Safety Survey-2, conducted by the Crimes Against Children Research Center at the University of New Hampshire in 2005, found that 9% of the young people in the survey had experienced some form of harassment.[78] The survey was a nationally representative telephone survey of 1,500 youth 10–17 years old. One third reported feeling distressed by the incident, with distress being more likely for younger respondents and those who were the victims of aggressive harassment (including being telephoned, sent gifts, or visited at home by the harasser).[79] Compared to youth not harassed online, victims are more likely to have social problems. On the other hand, youth who harass others are more likely to have problems with rule breaking and aggression.[80]		Hinduja and Patchin completed a study in the summer of 2005 of approximately 1,500 Internet-using adolescents and found that over one-third of youth reported being victimized online, and over 16% of respondents admitted to cyber-bullying others.[81] While most of the instances of cyber-bullying involved relatively minor behavior (41% were disrespected, 19% were called names), over 12% were physically threatened and about 5% were scared for their safety. Notably, fewer than 15% of victims told an adult about the incident.[11] Additional research by Hinduja and Patchin in 2007[82] found that youth who report being victims of cyber-bullying also experience stress or strain that is related to offline problem behaviors such as running away from home, cheating on a school test, skipping school, or using alcohol or marijuana. The authors acknowledge that both of these studies provide only preliminary information about the nature and consequences of online bullying, due to the methodological challenges associated with an online survey.		According to a 2005 survey by the National Children's Home charity and Tesco Mobile[83] of 770 youth between the ages of 11 and 19, 20% of respondents revealed that they had been bullied via electronic means. Almost three-quarters (73%) stated that they knew the bully, while 26% stated that the offender was a stranger. 10% of responders indicated that another person has taken a picture and/or video of them via a cellular phone camera, consequently making them feel uncomfortable, embarrassed, or threatened. Many youths are not comfortable telling an authority figure about their cyber-bullying victimization for fear their access to technology will be taken from them; while 24% and 14% told a parent or teacher respectively, 28% did not tell anyone while 41% told a friend.[83]		According to the 2006 Harris Interactive Cyberbullying Research Report, commissioned by the National Crime Prevention Council, cyber-bullying is a problem that "affects almost half of all American teens".[84]		Studies published 2007 in the Journal of Adolescent Health indicated young people reporting being victims of electronic aggression in a range of 9%[87] to 35%.[86][88]		In 2007, Debbie Heimowitz, a Stanford University master's student, created Adina's Deck, a film based on Stanford accredited research. She worked in focus groups for ten weeks in three schools to learn about the problem of cyber-bullying in Northern California. The findings determined that over 60% of students had been cyber-bullied and were victims of cyber-bullying. The film is now being used in classrooms nationwide as it was designed around learning goals pertaining to problems that students had understanding the topic. The middle school of Megan Meier is reportedly using the film as a solution to the crisis in their town.		In the summer of 2008, researchers Sameer Hinduja (Florida Atlantic University) and Justin Patchin (University of Wisconsin-Eau Claire) published a book on cyber-bullying that summarized the current state of cyber-bullying research. (Bullying Beyond the Schoolyard: Preventing and Responding to Cyberbullying).[4] Their research documents that cyber-bullying instances have been increasing over the last several years. They also report findings from the most recent study of cyber-bullying among middle-school students. Using a random sample of approximately 2000 middle-school students from a large school district in the southern United States, about 10% of respondents had been cyber-bullied in the previous 30 days while over 17% reported being cyber-bullied at least once in their lifetime.[4] While these rates are slightly lower than some of the findings from their previous research, Hinduja and Patchin point out that the earlier studies were predominantly conducted among older adolescents and Internet samples. That is, older youth use the Internet more frequently and are more likely to experience cyber-bullying than younger children.[11][82][89]		According to the 2011 National Crime Victimization Survey, conducted by the U.S. Department of Justice, Bureau of Justice Statistics, School Crime Supplement (SCS), 9% of students of ages 12–18 admittedly experienced cyberbullying during that school year (with a coefficient of variation between 30% and 50%).[90]		In the Youth Risk Behavior Survey 2013, the Center for Surveillance, Epidemiology, and Laboratory Services of the Centers for Disease Control and Prevention published results of its survey as part of the Youth Risk Behavior Surveillance System (YRBSS) in June 2014, indicating in table 17 the percentage of school children being bullied through e-mail, chat rooms, instant messaging, Web sites, or texting ("electronically bullied") during the course of the year 2013.[91]		In 2014, Mehari, Farrell, and Le published a study that focused on the literature on cyberbullying among adolescents. They found that researchers have generally assumed that cyberbullying is distinct from aggression perpetrated in person. They suggest that the media through which aggression is perpetrated may be best conceptualized as a new dimension on which aggression can be classified, rather than cyberbullying as a distinct counterpart to existing forms of aggression and that future research on cyberbullying should be considered within the context of theoretical and empirical knowledge of aggression in adolescence.[92] Mary Howlett-Brandon's doctoral dissertation analyzed the National Crime Victimization Survey: Student Crime Supplement, 2009, to focus on the cyberbullying victimization of Black students and White students in specific conditions.[93]		2015		WalletHub's 2015's Best & Worst States at Controlling Bullying report measured the relative levels of bullying in 42 states. According to the report, North Dakota, Illinois, Louisiana, Rhode Island, and Washington D.C. have the highest attempted suicide by high school students. The top 5 states with highest percentage of students being bullied on campus is Missouri, Michigan, Idaho, North Dakota, and Montana.[94]		Cyberbullying on social media has usually been student-to-student but recently, students have been cyberbullying their teachers. High School students in Colorado created a Twitter site that bullies many teachers. The bullying ranges from obscenities to false accusations of inappropriate actions with students.[95]		Legislation geared at penalizing cyberbullying has been introduced in a number of U.S. states including New York, Missouri, Rhode Island and Maryland. At least forty five states passed laws against digital harassment.[96] Dardenne Prairie of Springfield, Missouri, passed a city ordinance making online harassment a misdemeanor. The city of St. Charles, Missouri has passed a similar ordinance. Missouri is among other states where lawmakers are pursuing state legislation, with a task forces expected to have "cyberbullying" laws drafted and implemented.[97] In June, 2008, Rep. Linda Sanchez (D-Calif.) and Rep. Kenny Hulshof (R-Mo.) proposed a federal law that would criminalize acts of cyberbullying.[98]		Lawmakers are seeking to address cyberbullying with new legislation because there's currently no specific law on the books that deals with it. A fairly new federal cyberstalking law might address such acts, according to Parry Aftab, but no one has been prosecuted under it yet. The proposed federal law would make it illegal to use electronic means to "coerce, intimidate, harass or cause other substantial emotional distress."		In August 2008, the California state legislature passed one of the first laws in the country to deal directly with cyberbullying. The legislation, Assembly Bill 86 2008,[99] gives school administrators the authority to discipline students for bullying others offline or online.[100] This law took effect, January 1, 2009.[101] A law in New York's Albany County that criminalized cyberbullying was recently struck down as unconstitutional by the New York Court of Appeals in People v. Marquan M..		A recent ruling first seen in the UK determined that it is possible for an Internet Service Provider (ISP) to be liable for the content of sites which it hosts, setting a precedent that any ISP should treat a notice of complaint seriously and investigate it immediately.[102]		18 U.S.C. § 875(c) criminalizes the making of threats via Internet.		Since the 1990s, the United Kingdom and other European countries have been working to solve workplace bullying since there is no legislation regulating cyberbullying. The pervasive nature of technology has made the act of bullying online much easier.[103] A 24-hour internet connection gives bullies a never ending opportunity to find and bully victims. Employers in the European Union have more legal responsibility to their employees than other countries. Since employers do not the ability to fire or hire an employee at will like in the United States, employers in Europe are held to a high standard in how their employees are treated. The Framework Agreement on Harassment and Violence at Work is a law that prevents bullying occurring in the workplace and holds employers accountable for providing fair working conditionsr.[103] Lawyers pursuing cyberbullying cases use The Ordinance on Victimization at Work law, since they are not any laws specifically condemning cyberbullying.[103]		In 1993, Sweden was the first European Union country to have a law against cyberbullying. The Ordinance on Victimization at Work protected victims from "recurrent reprehensible or distinctly negative actions which are directed which are directed against individual employees in an offensive manner and can result in those employees being placed outside the workplace community".[103] In 2002, France passed the Social Modernization Law, which added consequences to the French Labor Code for cyberbullying such as holding employers accountable for their involvement in harassment. The legislation states, "the employer can be held accountable if it is deemed by court of law that the conduct defile the employee emotionally or physical health in any manner".[103] The United Kingdom does not have anti-bullying legislation. However, it does have the Protection From Harassment Act, an anti-stalking law.[103] The United Kingdom courts have used this legislation in bullying cases. In 2007, the European Union developed the Framework Agreement on Harassment and Violence at Work.[103] The law defines the responsibilities of an employer such as protecting his or her employees from bullies in a work environment and the psychological pain a victim faces from bullies during business hours.		The United States and other countries have more extensive legislation on cyberbullying than the European Union. The amount of cyberbullying incidents on social media are widespread and have increased drastically.[103] However, the process of getting a claim against a bully is not an easy one because of the victim's need to provide sufficient evidence to prove the existence of bullying.		As of mid-2015, countries in the European Union like the United Kingdom are in the process of creating law specially related to cyberbullying. Since the process takes time, the government is supporting schools programs to promote internet safety with the help of teachers and parents.[104] This will allow government to take the time it needs to create the cyberbullying laws while helping students safeguarding themselves from cyberbullying as much as they can.[105]		Researchers suggest that programs be put in place for prevention of cyberbullying. These programs would be incorporated into school curricula and would include online safety and instruction on how to use the Internet properly.[106] This could teach the victim proper methods of potentially avoiding the cyberbully, such as blocking messages or increasing the security on their computer.[106]		Within this suggested school prevention model, even in a perfect world, not one crime can be stopped fully. That is why it is suggested that within this prevention method, effective coping strategies should be introduced and adopted. As with any crime, people learn to cope with what has happened, and the same goes for cyberbullying. People can adopt coping strategies to combat future cyberbullying events. An example of a coping strategy would be a social support group composed of various victims of cyberbullying.[106] That could come together and share experiences, with a formal speaker leading the discussion. Something like a support group can allow students to share their stories, and allows that feeling of them being alone to be removed.		Teachers should be involved in all prevention educational models, as they are essentially the "police" of the classroom. Most cyberbullying often goes unreported as the victim feels nothing can be done to help in their current situation.[106] However, if given the proper tools with preventative measures and more power in the classroom, teachers can be of assistance to the problem of cyber-bullying. If the parent, teacher, and victim can work together, a possible solution or remedy can be found.[106]		There have been many legislative attempts to facilitate the control of bullying and cyberbullying. The problem is due to the fact that some existing legislation is incorrectly thought to be tied to bullying and cyberbullying (terms such as libel and slander). The problem is they do not directly apply to it nor define it as its own criminal behavior.[107] Anti-cyberbullying advocates even expressed concern about the broad scope of applicability of some of the bills attempted to be passed.[108]		In the United States, attempts were made to pass legislation against cyberbullying. Few states attempted to pass broad sanctions in an effort to prohibit cyberbullying. Problem include how to define cyberbullying and cyberstalking, and if charges are pressed, whether it violates the bully's freedom of speech.[108] B. Walther has said that "Illinois is the only state to criminalize 'electronic communication(s) sent for the purpose of harassing another person' when the activity takes place outside a public school setting." Again this came under fire for infringement on freedom of speech.[108]		Research had demonstrated a number of serious consequences of cyberbullying victimization.[11][82][4][89] For example, victims have lower self-esteem, increased suicidal ideation, and a variety of emotional responses, retaliating, being scared, frustrated, angry, and depressed.[4] People have reported that Cyberbullying can be more harmful than traditional bullying because there is no escaping it.[5]		One of the most damaging effects is that a victim begins to avoid friends and activities, often the very intention of the cyberbully.		Cyberbullying campaigns are sometimes so damaging that victims have committed suicide. There are at least four examples in the United States where cyberbullying has been linked to the suicide of a teenager.[4] The suicide of Megan Meier is a recent example that led to the conviction of the adult perpetrator of the attacks. Another example of harmful effects is the death of Holly Grogan who ended her life by jumping of a 30-foot bridge near Gloucester in the UK . It was reported that a number of her schoolmates has posted a number of hateful messages on her Facebook page.[109]		According to Lucie Russell, director of campaigns, policy and participation at youth mental health charity Young Minds, young people who suffer from mental disorder are vulnerable to cyberbullying as they are sometimes unable to shrug it off:		When someone says nasty things healthy people can filter that out, they're able to put a block between that and their self-esteem. But mentally unwell people don't have the strength and the self-esteem to do that, to separate it, and so it gets compiled with everything else. To them, it becomes the absolute truth – there's no filter, there's no block. That person will take that on, take it as fact.[110]		Social media has allowed bullies to disconnect from the impact they may be having on others.[111]		According to the Cyberbullying Research Center, "there have been several high‐profile cases involving teenagers taking their own lives in part because of being harassed and mistreated over the Internet, a phenomenon we have termed cyberbullicide – suicide indirectly or directly influenced by experiences with online aggression."		Cyberbullying is an intense form of psychological abuse, whose victims are more than twice as likely to suffer from mental disorders compared to traditional bullying.[112]		The reluctance youth have in telling an authority figure about instances of cyberbullying has led to fatal outcomes. At least three children between the ages of 12 and 13 have committed suicide due to depression brought on by cyberbullying, according to reports by USA Today and the Baltimore Examiner. These would include the suicide of Ryan Halligan and the suicide of Megan Meier, the latter of which resulted in United States v. Lori Drew.		More recently, teenage suicides tied to cyberbullying have become more prevalent. The latest victim of cyberbullying through the use of mobile applications was Rebecca Ann Sedwick, who committed suicide after being terrorized through mobile applications such as Ask.fm, Kik Messenger and Voxer.[113]		The effects of cyberbullying vary. But, research illustrates that cyber bullying adversely affects youth to a higher degree than adolescents and adults. Youth are more likely to suffer since they are still growing mentally and physically.[114] Jennifer N. Caudle, a certified family physician, describes the effects as "Kids that are bullied are likely to experience anxiety, depression, loneliness, unhappiness and poor sleep".[115]		Most of the time cyberbullying goes unnoticed; the younger generation hides their bullying from anyone that can help to prevent the bullying from occurring and from getting worse. Between 20% and 40% of adolescents are victims of cyberbullying worldwide.[114][116] The youth slowly change their behaviors and actions so they become more withdrawn and quiet than they are used to, but no one notices since the change is subtle.[114][116] Cyberbullying will "become a serious problem in the future with an increase in the Internet and mobile phone usage among young people".[116]		If preventive actions are not taken against cyberbullying, younger children in addition to teenagers will feel more lonely and depressed along with having a significant change in their eating and sleeping patterns as well as loss of interest in their normal activities. These changes will affect their growth and development into adulthood.[114][116] Younger children and teenagers are 76.2% less likely to display suicidal behaviors and thoughts, but are still at risk depending on other factors such as mental health status, home care, relationships with others.[116] The risk of suicide increases 35% to 45% when victims do not have any support from anyone in their life and cyberbullying amplifies the situation more.[114]		The Cybersmile Foundation is a cyberbullying charity committed to tackling all forms of online bullying, abuse, and hate campaigns. The charity was founded in 2010 in response to the increasing number of cyberbullying related incidents of depression, eating disorders, social isolation, self-harm and suicides devastating lives around the world. Cybersmile provides support to victims and their friends and families through social media interaction, email and helpline support. They also run an annual event, Stop Cyberbullying Day, to draw attention to the issue.		There are multiple non-profit organizations that fight cyberbullying and cyberstalking. They advise victims, provide awareness campaigns, and report offenses to the police. These NGOs include the Protégeles, PantallasAmigas, Foundation Alia2, the non-profit initiative Actúa Contra el Ciberacoso, the National Communications Technology Institute (INTECO), the Agency of Internet quality, the Agencia Española de Protección de Datos, the Oficina de Seguridad del Internauta, the Spanish Internet users' Association, the Internauts' Association, and the Spanish Association of Mothers and Parents Internauts. The Government of Castile and León has also created a Plan de Prevención del Ciberacoso y Promoción de la Navegación Segura en Centro Escolares, and the Government of the Canary Islands has created a portal on the phenomenon called Viveinternet.		In March 2007, the Advertising Council in the United States, in partnership with the National Crime Prevention Council, U.S. Department of Justice, and Crime Prevention Coalition of America, joined to announce the launch of a new public service advertising campaign designed to educate preteens and teens about how they can play a role in ending cyber-bullying.		January 20, 2008 – the Boy Scouts of America's 2008 edition of The Boy Scout Handbook addresses how to deal with online bullying. A new First Class rank requirements adds: "Describe the three things you should avoid doing related to use of the Internet. Describe a cyberbully and how you should respond to one."[117][118]		January 31, 2008 – KTTV Fox 11 News based in Los Angeles put out a report about organized cyber-bullying on sites like Stickam by people who call themselves "/b/rothas".[119] The site had put out report on July 26, 2007, about a subject that partly featured cyberbullying titled "hackers on steroids".[120]		June 2, 2008 – Parents, teens, teachers, and Internet executives came together at Wired Safety's International Stop Cyberbullying Conference, a two-day gathering in White Plains, New York and New York City. Executives from Facebook, Verizon, MySpace, Microsoft, and many others talked with hundreds about how to better protect themselves, personal reputations, children and businesses online from harassment. Sponsors of the conference included McAfee, AOL, Disney, Procter & Gamble, Girl Scouts of the USA, WiredTrust, Children's Safety Research and Innovation Centre, KidZui.com and others. Cyberharassment vs. cyberbullying was a forefront topic, where age makes a difference and abusive internet behavior by adults with repeated clear intent to harm, ridicule or damage a person or business was classified as stalking harassment vs. bullying by teens and young adults.[121]		August 2012 – A new organized movement to make revenge porn illegal began in August 2012. It is known as End Revenge Porn.[122] Currently revenge porn is only illegal in two states, but the demand for its criminalization is on the rise as digital technology has increased in the past few generations. The organization seeks to provide support for victims, educate the public, and gain activist support to bring new legislation before the United States Government.[122]		In 2006, PACER.org created a week long event that was held once a year in October. Today, the campaign is a monthlong event and is now known as the National Bullying Prevention Awareness Month.[123]		Originated in Canada, Anti-Bullying day is a day of celebration for those who choose to participate wearing a symbol of colours (Pink, Blue or Purple) as a stance against bullying. A B.C. teacher founded the Stop A Bully movement, which uses pink wristbands to represent the wearer's stance to stop bullying.		Pink Shirt Day was inspired by David Shepherd and Travis Price. Their high school friends organized a protest in sympathy for a Grade 9 boy who was bullied for wearing a pink shirt. Their stance from wearing pink has been a huge inspiration in the Great Vancouver Mainland. "We know that victims of bullying, witnesses of bullying and bullies themselves all experience the very real and long term negative impacts of bullying regardless of its forms – physical, verbal, written, or on-line (cyberbullying)".		The ERASE (Expect Respect and A Safe Education) is an initiative started by the province of British Columbia to foster safe schools and prevent bullying. It builds on already-effective programs set up by the provincial government to ensure consistent policies and practices regarding the prevention of bullying.		A number organizations are in coalition to provide awareness, protection and recourse for the escalating problem. Some aim to inform and provide measures to avoid as well as effectively terminate cyberbullying and cyberharassment. Anti-bullying charity Act Against Bullying launched the CyberKind campaign in August 2009 to promote positive internet usage.		In 2007, YouTube introduced the first Anti-Bullying Channel for youth, (BeatBullying) engaging the assistance of celebrities to tackle the problem.[124]		In March 2010, a 17-year-old girl named Alexis Skye Pilkington was found dead in her room by her parents. Her parents claimed that after repeated cyberbullying, she was driven to suicide. Shortly after her death, attacks resumed. Members of eBaums World began to troll teens' memorial pages on Facebook, with the comments including expressions of pleasure over the death, with pictures of what seemed to be a banana as their profile pictures. Family and friends of the deceased teen responded by creating Facebook groups denouncing cyberbullying and trolling, with logos of bananas behind a red circle with a diagonal line through it.[125]		In response and partnership to the 2011 film Bully, a grassroots effort to stop cyberbullying called The Bully Project was created. Their goal is "sparked a national movement to stop bullying that is transforming children's lives and changing a culture of bullying into one of empathy and action."[126]		13-year-old Zoe Johnson from Wyoming, Michigan committed suicide in July 2015. Johnson had been a victim of cyberbullying for years and suffered from mild depression. It is believed that a message posted on her Facebook the day before her suicide may have been the turning point that pushed her towards suicide. After her death, people continued to post messages on her Facebook with one person posting the message "good ur gone".[127]		14-year-old Carla Jamerson from Las Vegas, Nevada committed suicide in 2015. She was a victim of cyberbullying for years. Jamerson went to both the city and school police, but did not receive any help. After not receiving any help, she hanged herself.[128]		As of 2011 and 2012, climate scientists and climate activists were being confronted with abusive emails from all over the world. These emails were sometimes sent in response to public statements that merely reported findings related to the widely accepted anthropogenic climate change and its consequences. Such emails were sent in response to suggestions posted on climate denial websites, which are effectively requests to engage in cyberbullying. Climate scientists and climate activists were also confronted with libelous Internet reports that aimed to silence them or destroy their reputations.[28][29][30]		In 2013, two Swedish teenage girls were convicted by the Swedish court in Gothenburg for writing derogatory, explicit remarks next to the pictures of 38 youngsters, mostly girls, via an anonymous Instagram account. They were found guilty and sentenced to youth care and youth service as well as rioting at two schools.[129]		In 2012, three teenage Irish girls committed suicide within a few weeks.[130] Their families and friends called for the websites they were bullied on to be banned. Ask.fm, an online Q&A website has received a lot of criticism. However, Ask.fm's co-founder Mark Terebin argues that "it is necessary to go deeper and to find a root of a problem. It's not about the site, the problem is about education, about moral values that were devaluated lately".[131]		
The fictional character Spider-Man, a comic book superhero created by Stan Lee and Steve Ditko and featured in Marvel Comics publications, has currently appeared in nine live-action films since his inception. Spider-Man is the alter-ego of Peter Parker, a talented young freelance photographer and aspiring scientist, imbued with superhuman abilities after being bitten by a radioactive/genetically-altered spider.		The first live-action film based on Spider-Man was the short Spider-Man by Donald F. Glut in 1969. This was followed by Spider-Man, an American made-for-television film that premiered on the CBS network in 1977. It starred Nicholas Hammond and was intended as a backdoor pilot for what became a weekly episodic TV series.		The rights to further films featuring the character were purchased in 1985, and moved through various production companies and studios before being secured by Sony Pictures Entertainment (Columbia Pictures), who hired Sam Raimi to direct Spider-Man (2002), Spider-Man 2 (2004), and Spider-Man 3 (2007) starring Tobey Maguire. The first two films were met with positive reviews from critics, while the third film met mixed reviews. In 2010, Sony announced that the franchise would be rebooted. Marc Webb was hired to direct, with Andrew Garfield starring, and The Amazing Spider-Man (2012) was released to positive reviews. The Amazing Spider-Man 2 (2014) saw mixed reviews.		In February 2015, Disney, Marvel Studios and Sony announced a deal to share the Spider-Man film rights, leading to a new iteration of Spider-Man being introduced and integrated into the Marvel Cinematic Universe (MCU). This younger version of Peter Parker is played by Tom Holland, and appears in Captain America: Civil War (2016) (distributed by Disney) and Spider-Man: Homecoming (distributed by Sony) in 2017, before starring in Avengers: Infinity War and its untitled sequel (both distributed by Disney) in 2018 and 2019 respectively. A sequel to Homecoming is slated for 2019.		Raimi's trilogy grossed nearly US$2.5 billion worldwide on a $597 million total budget, while Webb's films grossed over $1.4 billion on a $480 million total budget. The Spider-Man films are the ninth highest-grossing film franchise, having grossed over $4.4 billion collectively.		In 1977, the pilot episode of The Amazing Spider-Man television series was released by Columbia Pictures as Spider-Man outside of the United States. It was directed by E. W. Swackhamer, written by Alvin Boretz and stars Nicholas Hammond as the titular character, David White as J. Jonah Jameson and Jeff Donnell as May Parker. The film premiered on CBS on September 14, 1977, and received a VHS release in 1980.		In 1978, the two-parter episode "Deadly Dust" from the television series The Amazing Spider-Man was re-edited and released outside of the United States as a feature film titled Spider-Man Strikes Back. Nicholas Hammond reprises his role as Peter Parker / Spider-Man while Robert F. Simon replaces David White in the role of J. Jonah Jameson. The film was theatrically released on 8 May 1978.		In 1981, a film made from The Amazing Spider-Man television series finale "The Chinese Web" using the same method used to make Spider-Man Strikes Back was released as Spider-Man: The Dragon's Challenge in European territories. Nicholas Hammond and Robert F. Simon respectively reprise their roles as Peter Parker / Spider-Man and J. Jonah Jameson. It was directed by Ron Satlof, written by Robert Janes and stars Nicholas Hammond as the titular character, Rosalind Chao, Robert F. Simon, Benson Fong, and Ellen Bry		On July 22, 1978, Tōei released a theatrical spin-off of their Spider-Man TV series at the Tōei Cartoon Festival. The film was directed by Kōichi Takemoto, who also directed eight episodes of the TV series. The week after the film's release, a character introduced in the film, Jūzō Mamiya (played by Noboru Nakaya), began appearing in episodes of the TV series. Like the rest of the series, the film was made available for streaming on Marvel's official website in 2009.		The low box office performance of 1983's Superman III made feature-film adaptations of comic book properties a very low priority in Hollywood until the 1990s.[1] In 1985, after a brief option on Spider-Man by Roger Corman expired,[2] Marvel Comics optioned the property to Cannon Films. Cannon chiefs Menahem Golan and his cousin Yoram Globus agreed to pay Marvel Comics $225,000 over the five-year option period, plus a percentage of any film’s revenues.[3] However, the rights would revert to Marvel if a film was not made by April 1990.[4]		Tobe Hooper, then preparing both Invaders From Mars and The Texas Chainsaw Massacre 2, was mooted as director. Golan and Globus misunderstood the concept of the character ("They thought it was like The Wolf Man", said director Joseph Zito)[5] and instructed writer Leslie Stevens, creator of The Outer Limits, to write a treatment reflecting their misconception. In Stevens' story, a corporate scientist intentionally subjects ID-badge photographer Peter Parker to radioactive bombardment, transforming him into a hairy, suicidal, eight-armed monster. This human tarantula refuses to join the scientist’s new master-race of mutants, battling a succession of mutations kept in a basement laboratory.[5][6]		Unhappy with this perceived debasement of his comic book creation, Marvel’s Stan Lee pushed for a new story and screenplay, written for Cannon by Ted Newsom and John Brancato.[7] The variation on the origin story had Otto Octavius as a teacher and mentor to a college-aged Peter Parker. The cyclotron accident which "creates" Spider-Man also deforms the scientist into Doctor Octopus and results in his mad pursuit of proof of the Fifth Force. "Doc Ock" reconstructs his cyclotron and causes electromagnetic abnormalities, anti-gravity effects, and bilocation which threatens to engulf New York City and the world. Joseph Zito, who had directed Cannon’s successful Chuck Norris film Invasion USA, replaced Tobe Hooper.[8] The new director hired Barney Cohen to rewrite the script. Cohen, creator of TV's Sabrina the Teenage Witch and Forever Knight, added action scenes, a non-canonical comic for the villain, gave Doc Ock the catch phrase, "Okey-dokey", and altered his goal from the Fifth Force to a quest for anti-gravity. Producer Golan (using his pen name "Joseph Goldman") then made a minor polish to Cohen's rewrite. Zito scouted locations and studio facilities in both the U.S. and Europe, and oversaw storyboard breakdowns supervised by Harper Goff. Cannon planned to make the film on the then-substantial budget of between $15 and $20 million.[3]		While no casting was finalized, Zito expressed interest in actor/stunt man Scott Leva, who had posed for Cannon's promotional photos and ads, and made public appearances as Spider-Man for Marvel. The up-and-coming actor Tom Cruise was also discussed for the leading role. Zito considered Bob Hoskins as Doc Ock. Stan Lee expressed his desire to play Daily Bugle editor J. Jonah Jameson.[9] Lauren Bacall and Katharine Hepburn were considered for Aunt May, Peter Cushing as a sympathetic scientist, and Adolph Caesar as a police detective.[7] With Cannon finances siphoned by the expensive Superman IV: The Quest for Peace and Masters of the Universe, the company slashed the proposed Spider-Man budget to under $10 million. Director Zito opted out, unwilling to make a compromised Spider-Man. The company commissioned low-budget rewrites from writers Shepard Goldman, Don Michael Paul, and finally Ethan Wiley, and penciled in company workhorse Albert Pyun as director, who also made script alterations.[6]		Scott Leva was still associated with the character through Marvel (he had appeared in photo covers of the comic), and read each draft. Leva commented, "Ted Newsom and John Brancato had written the script. It was good but it needed a little work. Unfortunately, with every subsequent rewrite by other writers, it went from good to bad to terrible."[9] Due to Cannon's assorted financial crises, the project shut down after spending about $1.5 million on the project.[5] In 1989, Pathé, owned by corrupt Italian financier Giancarlo Parretti, acquired the overextended Cannon. The filmmaking cousins parted, Globus remaining associated with Pathé, Golan leaving to run 21st Century Film Corporation, keeping a number of properties (including Spider-Man) in lieu of a cash buy-out. He also extended his Spider-Man option with Marvel up to January 1992.[4]		Golan shelved the low-budget rewrites and attempted to finance an independent production from the original big-budget script, already budgeted, storyboarded and laid out.[10] At Cannes in May 1989, 21st Century announced a September start date, with ads touting the script by "Barney Cohen, Ted Newsom & John Brancato and Joseph Goldman."[11] As standard practice, Golan pre-sold the unmade film to raise production funds, with television rights bought by Viacom and home video rights by Columbia Pictures, which wanted to establish a studio franchise. Stephen Herek was attached as director at this point.[12] Golan submitted this "new" screenplay to Columbia in late 1989 (actually the 1985 script with an adjusted "1989" date) and the studio requested yet another rewrite. Golan hired Frank LaLoggia, who turned in his draft but grew disenchanted with 21st Century. Neil Ruttenberg was hired for one more draft, which was also "covered" by script readers at Columbia.[13] Columbia’s script analysts considered all three submissions "essentially the same story." A tentative production deal was set. Said Stan Lee in 1990, "21st Century [is] supposed to do Spider-Man and now they're talking to Columbia and the way it looks now, Columbia may end up buying Spider-Man from 21st Century."[14]		21st Century’s Menahem Golan still actively immersed himself mounting "his" Spider-Man, sending the original "Doc Ock" script for production bids. In 1990, he contacted Canadian effects company Light and Motion Corporation regarding the visual effects, which in turn offered the stop-motion chores to Steven Archer (Krull, Clash of the Titans).[15]		Toward the end of shooting True Lies, Variety carried the announcement that Carolco Pictures had received a completed screenplay from James Cameron.[16] This script bore the names of James Cameron, John Brancato, Ted Newsom, Barry [sic] Cohen and "Joseph Goldmari", a typographical scrambling of Golan's pen name ("Joseph Goldman") with Marvel executive Joseph Calamari.[17] The script text was identical to the one Golan submitted to Columbia the previous year, with the addition of a new 1993 date. Cameron stalwart Arnold Schwarzenegger was frequently linked to the project as the director's choice for Doctor Octopus.[18][19]		Months later, James Cameron submitted an undated 57-page "scriptment" with an alternate story (the copyright registration was dated 1991), part screenplay, part narrative story outline.[5] The "scriptment" told the Spider-Man origin, but used variations on the comic book characters Electro and Sandman as villains. This "Electro" (named Carlton Strand, instead of Max Dillon) was a megalomaniacal parody of corrupt capitalists. Instead of Flint Marko's character, Cameron’s "Sandman" (simply named Boyd) is mutated by an accident involving Philadelphia Experiment-style bilocation and atom-mixing, in lieu of getting caught in a nuclear blast on a beach. The story climaxes with a battle atop the World Trade Center and had Peter Parker revealing his identity to Mary Jane Watson. In addition, the treatment was also heavy on profanity, and had Spider-Man and Mary Jane having sex on the Brooklyn Bridge.[20]		This treatment reflected elements in previous scripts: from the Stevens treatment, organic web-shooters, and a villain who tempts Spider-Man to join a coming "master race" of mutants; from the original screenplay and rewrite, weird electrical storms causing blackouts, freak magnetic events and bi-location; from the Ethan Wiley draft, a villain addicted to toxic super-powers and multiple experimental spiders, one of which escapes and bites Peter, causing an hallucinatory nightmare invoking Franz Kafka’s The Metamorphosis; from the Frank LaLoggia script, a blizzard of stolen cash fluttering down onto surprised New Yorkers; and from the Neil Ruttenberg screenplay, a criminal assault on the NYC Stock Exchange.[6] In 1991, Carolco Pictures extended Golan’s option agreement with Marvel through May 1996,[4] but in April 1992, Carolco ceased active production on Spider-Man due to continued financial and legal problems.[21]		When James Cameron agreed to make Spider-Man, Carolco lawyers simply used his previous Terminator 2 contract as a template. A clause in this agreement gave Cameron the right to decide on movie and advertising credits. Show business trade articles and advertisements made no mention of Golan, who was still actively assembling the elements for the film.[4] In 1993, Golan complained publicly and finally instigated legal action against Carolco for disavowing his contractual guarantee credit as producer. On the other hand, Cameron had the contractual right to decide on credits.[6] Eventually, Carolco sued Viacom and Columbia to recover broadcast and home video rights, and the two studios countersued.[3] 20th Century Fox, though not part of the litigation, contested Cameron’s participation, claiming exclusivity on his services as a director under yet another contract.[5] In 1996, Carolco, 21st Century, and Marvel went bankrupt.		Via a quitclaim from Carolco dated March 28, 1995, MGM acquired 21st Century's film library and assets, and received "...all rights in and to all drafts and versions of the screenplay(s) for Spider-Man written by James Cameron, Ted Newsom & John Brancato, Menahem Golan, Jon [sic] Michael Paul, Ethan Wiley, Leslie Stevens, Frank Laloggia, Neil Ruttenberg, Barney Cohen, Shepard Goldman and any and all other writers."[22] MGM also sued 21st Century, Viacom, and Marvel Comics, alleging fraud in the original deal between Cannon and Marvel. In 1998, Marvel emerged from bankruptcy with a reorganization plan that merged the company with Toy Biz.[4] The courts determined that the original contract of Marvel's rights to Golan had expired, returning the rights to Marvel, but the matter was still not completely resolved. In 1999, Marvel licensed Spider-Man rights to Columbia, a subsidiary of Sony Pictures Entertainment.[23] MGM disputed the legality, claiming it had the Spider-Man rights via Cannon, 21st Century, and Carolco.[24]		In the meantime, MGM/UA chief executive John Calley moved to Columbia Pictures. Intimately familiar with the legal history of producer Kevin McClory’s claim to the rights to both Thunderball and other related James Bond characters and elements, Calley announced that Columbia would produce an alternate 007 series, based on the "McClory material", which Calley acquired for Columbia.[25] (Columbia had made the original 1967 film spoof of Casino Royale, a non-Eon production).		Both studios now faced rival projects, which could undercut their own long-term financial stability and plans. Columbia had no consistent movie franchise, and had sought Spider-Man since 1989; MGM/UA’s only reliable source of theatrical income was a new James Bond film every two or three years. An alternate 007 series could diminish or even eliminate the power of MGM/UA’s long-running Bond series. Likewise, an MGM/UA Spider-Man film could negate Columbia’s plans to create an exclusive cash cow. Both sides seemed to have strong arguments for the rights to do such films.[26]		The two studios made a trade-off in March 1999; Columbia relinquished its rights to create a new 007 series in exchange for MGM's giving up its claim to Spider-Man.[27] Columbia acquired the rights to all previous scripts in 2000,[13] but exercised options only on the "Cameron Material", i.e., both the completed multi-author screenplay and the subsequent "scriptment."[5] After more than a decade of attempts, Spider-Man truly went into production[3] and since then all of the Spider-Man films were distributed by Columbia Pictures, the primary film production holding of Sony. The first three were directed by Sam Raimi, and the reboot and its sequel were directed by Marc Webb. Laura Ziskin served as producer until her death in 2011.[28]		Spider-Man follows Peter Parker (Tobey Maguire), an orphaned high schooler who pines after popular girl-next-door Mary Jane Watson (Kirsten Dunst). While on a science class field trip, Peter is bitten by a genetically-engineered "super spider." As a result, Peter gains superhuman abilities, including increased strength, speed, and the abilities to scale walls and generate organic webbing. After his beloved Uncle Ben (Cliff Robertson) is murdered, the teenager realizes that he must use his newfound abilities to protect New York City. Meanwhile, wealthy industrialist Norman Osborn (Willem Dafoe), the father of Peter's best friend Harry Osborn (James Franco), subjects himself to an experimental performance-enhancing serum, which creates a psychotic and murderous split personality. Donning a military battlesuit, Norman becomes a freakish "Green Goblin", who begins to terrorize the city. Peter, as Spider-Man, now must do battle with the Goblin, all while trying to express his true feelings for Mary Jane.		Two years after the events of the first film, Peter struggles to balance his superhero and private lives and still pines after Mary Jane Watson, who is now engaged. Harry Osborn continues to believe Spider-Man is responsible for his father Norman Osborn's death. Spider-Man contends with scientist Otto Octavius (Alfred Molina), a.k.a. Dr. Octopus, who has four mechanical tentacles fused to his spine and sets out to recreate a fusion-based experiment that could destroy much of New York City.		Spider-Man 3 picks up one year after the events of the second film. Peter is still seeing Mary Jane Watson, while Harry Osborn succeeds his father as the new Green Goblin. Eddie Brock (Topher Grace), who like Peter is a photographer for the Daily Bugle, sets out to defame Spider-Man and incriminate him. Flint Marko (Thomas Haden Church), an escaped convict, falls into a particle accelerator and becomes a shape-shifting sand monster later known as Sandman. Peter later learns that Marko is the one that killed Uncle Ben, causing Peter's own dark intentions to grow. This vendetta is enhanced by the appearance of the mysterious black alien symbiotic substance that bonds to Peter, resulting in the formation of a new black costume. Once Peter separates himself from the alien, it finds a new host in the form of Brock, resulting in the creation of Venom.		In 2008, Spider-Man 4 entered development, with Raimi attached to direct and Maguire, Dunst and other cast members set to reprise their roles. Both a fourth and a fifth film were planned and at one time the idea of shooting the two sequels concurrently was under consideration. However, Raimi stated in March 2009 that only the fourth film was in development at that time and that if there were fifth and sixth films, those two films would actually be a continuation of each other.[29][30][31][32] James Vanderbilt was hired in October 2007 to pen the screenplay after initial reports in early 2007 that Sony Pictures was in contact with David Koepp, who wrote the first Spider-Man film.[33][34] The script was being rewritten by Gary Ross in October 2009.[35] Sony also engaged Vanderbilt to write scripts for Spider-Man 5 and Spider-Man 6.[34]		In 2008, Raimi expressed interest in portraying the transformation of Dr. Curt Connors into his villainous alter-ego, the Lizard; the character's actor Dylan Baker and producer Grant Curtis were also enthusiastic about the idea.[36][37][38] Raimi also discussed his desire to upgrade Bruce Campbell from a cameo appearance to a significant role,[39] later revealed to be Quentin Beck / Mysterio.[40] It was reported in December 2009 that John Malkovich was in negotiations to play Vulture and that Anne Hathaway would play Felicia Hardy, though she would not have transformed into the Black Cat as in the comics. Instead, Raimi's Felicia was expected to become a new superpowered figure called the Vulturess.[41] However, several years later, in 2013, Raimi stated that Hathaway was going to be Black Cat if Spider-Man 4 had been made.[42] Concept art revealed in June 2016 showed the inclusion of an opening montage of Spider-Man going up against C and D-list villains, such as Mysterio, the Shocker, the Prowler, and the Rhino, with the Vulture serving as the main antagonist.[40]		As disagreements between Sony and Raimi threatened to push the film off the intended May 6, 2011 release date, Sony Pictures announced in January 2010 that plans for Spider-Man 4 had been cancelled due to Raimi's withdrawal from the project. Raimi reportedly ended his participation due to his doubt that he could meet the planned May 6, 2011 release date while at the same time upholding the film creatively; he admitted that he was "very unhappy" with the way Spider-Man 3 had turned out, and was under pressure to make the fourth film the best that he could.[43] Raimi purportedly went through four iterations of the script with different screenwriters and still "hated it".[44]		In July 2007, Avi Arad revealed a Venom spin-off was in the works.[45] The studio commissioned Jacob Aaron Estes to write a script, but rejected it the following year. In September 2008, Paul Wernick and Rhett Reese signed on to write.[46] Stan Lee signed on to make a cameo in the film.[47] Rhett Reese later revealed that they had written two drafts for the film and that the studio was pushing the film forward.[48] In 2009, Gary Ross, who was then rewriting the latest draft of the unproduced Spider-Man 4, was assigned to rewrite the Venom script and direct the film, in which Venom would be an antihero rather than a supervillain.[49] In March 2012, Chronicle director Josh Trank negotiated with Sony about his interest in directing the film after Ross left development to direct The Hunger Games.[50] In June 2012, The Amazing Spider-Man producer Matt Tolmach, speaking of his and fellow producer Avi Arad's next project, a Venom film, suggested it could follow the shared-universe model of the film The Avengers: "What I'm trying to say to you without giving anything away is hopefully all these worlds will live together in peace someday."[51]		Sony announced that the franchise would be rebooted with a new director and new cast. The Amazing Spider-Man was released on July 3, 2012 in 3D and IMAX 3D, and focused on Peter Parker (Andrew Garfield) developing his abilities in high school. He fights the Lizard, the monstrous form of Dr. Curt Connors, his father's former partner and a scientist at OsCorp.		The film takes place two years after the first film's events. Peter Parker graduates from high school, continues his relationship with Gwen Stacy, and combats the electricity-manipulating Electro.		In June 2013, Sony Pictures announced it had set release dates for the next two Spider-Man films. The third film was scheduled to be released on June 10, 2016, and the fourth was scheduled to be released on May 4, 2018.[52][53] Paul Giamatti confirmed that Rhino would return in the third film.[54] That November, Sony Pictures Entertainment chief Michael Lynton told analysts, "We do very much have the ambition about creating a bigger universe around Spider-Man. There are a number of scripts in the works."[55] Andrew Garfield stated that his contract is for three films, and is unsure of his involvement for the fourth film.[56] In February 2014, Sony announced that Webb would return to direct the third Amazing Spider-Man film.[57] In March 2014, Webb stated that he would not be directing the fourth film, but would like to remain as a consultant for the series.[58] On July 11, 2014, Roberto Orci told IGN that he is not working on the third film due to the third Star Trek film.[59] Alex Kurtzman revealed in an interview that the third film is still continuing production and there is a possibility of seeing a Black Cat film.[60] On July 23, 2014, Sony Pictures announced that The Amazing Spider-Man 3 was delayed to 2018.[61] After the announcement in February 2015 of a new franchise with Marvel Studios, the sequels to The Amazing Spider-Man 2 were cancelled.[62]		On December 12, 2013, Sony issued a press release through the viral site, Electro Arrives announcing that two films were in development, with Alex Kurtzman, Roberto Orci and Ed Solomon writing a spin-off to The Amazing Spider-Man focused on Venom (with Kurtzman attached to direct) and Drew Goddard writing one focused on the villain team Sinister Six with an eye to direct. Hannah Minghella and Rachel O’Connor would oversee the development and production of these films for the studio.[63][64] In April 2014, it was announced that Goddard would direct the Sinister Six film,[65] and that both spin-offs would be released before a fourth Amazing Spider-Man,[66] with Spider-Man potentially appearing in both spin-offs.[67] Later in the month, Tolmach and Arad revealed the Sinister Six film would be a redemption story[68] and that the film's lineup might differ from the comics.[69] On July 23, 2014, Sony Pictures announced that The Sinister Six was scheduled for release on November 11, 2016.[61] By August 2014, Sony was also looking to release a female-centered spin-off film in 2017, with Lisa Joy writing, and had given the Venom spin-off the potential title of Venom: Carnage.[70]		Despite the announcement in February 2015 of a new franchise with Marvel Studios, the Sinister Six, Venom, and the female-led spin-off films set in The Amazing Spider-Man franchise were then "still moving forward". Feige was not expected to be creatively involved with these films.[62] However, the Sinister Six film was cancelled due to The Amazing Spider-Man 2 underperforming commercially,[71] and by November 2015, the other prospective spin-off films were cancelled as well.[72]		In December 2014, following the hacking of Sony Pictures' computers, Sony and Marvel were revealed to have had discussions about allowing Spider-Man to appear in the 2016 Marvel Cinematic Universe film Captain America: Civil War while having control of the film rights remaining with Sony. However, talks between the studios then broke down. Instead, Sony had considered having Sam Raimi return to direct a new trilogy.[73]		However, on February 9, 2015, Sony Pictures and Marvel Studios announced that Spider-Man would appear in the Marvel Cinematic Universe, with the character appearing in a Marvel Cinematic Universe film and Sony releasing a Spider-Man film co-produced by Marvel Studios president Kevin Feige and Amy Pascal. Sony Pictures will continue to own, finance, distribute, and exercise final creative control over the Spider-Man films.[74] The next month, Marvel Entertainment CCO Joe Quesada indicated that the Peter Parker version of the character would be used,[75] which Feige confirmed in April.[76] Feige also stated that Marvel had been working to add Spider-Man to the MCU since at least October 2014.[77] The following June, Feige clarified that the initial Sony deal does not allow the character to appear in any of the MCU television series, as it was "very specific... with a certain amount of back and forth allowed."[78] In November 2016, Holland revealed that he was signed for "three Spider-Man movies and three solo movies".[79] In June 2017, Holland, Feige and Watts confirmed that a child wearing an Iron Man mask who Stark saves from a drone in Iron Man 2 (portrayed by Max Favreau) was a young Peter Parker, retroactively making it the introduction of the character to the MCU.[80][81]		Reports indicated that the MCU film that Spider-Man would appear in as part of the deal would be Captain America: Civil War.[82][83] Joe and Anthony Russo, the directors of Captain America: Civil War, had lobbied for months to include the character in the film.[84] Anthony Russo stated that, despite Marvel telling them to have a "plan B" should the deal with Sony fail, the Russos never created one because "it was very important to us to reintroduce" Spider-Man in the film, adding, "We only have envisioned the movie with Spider-Man."[85] By the end of May 2015, Asa Butterfield, Tom Holland, Judah Lewis, Matthew Lintz, Charlie Plummer and Charlie Rowe screen tested for the lead role, against Robert Downey Jr., who portrays Iron Man, for chemistry.[86][87] The six were chosen out of a search of over 1,500 actors to test in front of Feige, Pascal and the Russo brothers.[86] In June, Feige and Pascal narrowed the actors considered to Holland and Rowe. Both screen tested again with Downey, with Holland also testing with Chris Evans, who portrays Captain America, and emerged as the favorite.[87] On June 23, 2015, Sony Pictures and Marvel Studios jointly announced that Holland would play Spider-Man.[88] The following month, Marisa Tomei was in talks for the role of May Parker,[89] later appearing in Civil War.[90]		In the film, Parker is recruited by Tony Stark / Iron Man to join his team of Avengers to stop Steve Rogers / Captain America and his faction of Avengers from opposing the Sokovia Accords. During the fight with Rogers and his team, Parker proves to be a formidable opponent and is able to help take down Scott Lang / Ant-Man in his giant-sized form. Upon returning home, Parker discovers new StarkTech implementations in his suit, which he received from Stark.		Spider-Man: Homecoming was released on July 7, 2017.[91][92] The film is directed by Jon Watts,[93] from a screenplay by Jonathan M. Goldstein & John Francis Daley and Watts & Christopher Ford and Chris McKenna & Erik Sommers.[94] Holland, Tomei, and Downey reprise their roles as Peter Parker / Spider-Man, May Parker, and Tony Stark / Iron Man, respectively,[95] and are joined by Michael Keaton, who plays Adrian Toomes / Vulture.[94][96] Production began in June 2016 in Atlanta, Georgia and ended in October.[97][98][99]		Set approximately a few months after Captain America: Civil War, Parker under the tutelage of Stark continues fighting crime in New York City while dealing with the threat of a new villain named Vulture.		In October 2016, Holland said the possibility of him appearing in Avengers: Infinity War and its untitled sequel was "all up in the air", but that "some sort of deal is in the mix," with Sony for him to appear.[100] Holland was confirmed to appear in Infinity War in February 2017,[101] and its untitled sequel in April 2017.[102]		In December 2016, Sony Pictures announced a sequel for Spider-Man: Homecoming, scheduled to be released on July 5, 2019.[103] In June 2017, Feige stated that the film would be titled in a similar fashion to Homecoming, using a subtitle, and would not have a number in the title.[104]		On April 2015, Sony announced that Phil Lord and Chris Miller were writing and producing a Spider-Man animated comedy in development at Sony Pictures Animation. As revealed by the e-mail leak one year before, the duo had been previously courted by Sony to take over the studio's animation division. Originally scheduled to be released on December 21, 2018, Sony announced on April 26, 2017 the film will be released a week earlier on December 14, 2018.[105][106] Sony Pictures Animation president Kristine Belson unveiled the film's logo, with the working title Animated Spider-Man, at CinemaCon 2016, and declared that “conceptually and visually, [the film] will break new ground for the superhero genre.”[107] On June 20, 2016, The Hollywood Reporter reported that Bob Persichetti will direct the animated film.[108] Sony announced that Miles Morales will be the Spider-Man in the film and Peter Ramsey will serve as co-director.[109]		List indicator(s)		Stan Lee, one of the co-creators of Spider-Man, has appeared in varied cameos in all films except the 1977 and 1978 films. Bruce Campbell, a long-time colleague of Sam Raimi, appeared in all three of his films. In Spider-Man, he was the announcer at the wrestling ring Peter was in and gave him the name "Spider-Man", instead of the "Human Spider" (the name with which Peter wanted to be introduced). In Spider-Man 2, he was an usher who refuses to let Peter enter the theater for Mary Jane's play when arriving late. In Spider-Man 3, Campbell appears as a French maître d'.[112] In the ultimately unmade Spider-Man 4, Campbell's character would have been revealed as Quentin Beck / Mysterio.[113]		The Sam Raimi trilogy was released on DVD, the first two being released exclusively as two-disc sets and on VHS, with the third film being released in both single and two-disc editions. All three films were later packaged in a "Motion Picture DVD Trilogy" box set.		Spider-Man 3 was initially the only Spider-Man film to be released individually on the high-definition Blu-ray format. The first two films were made available on Blu-ray, but only as part of a boxed set with the third film, called Spider-Man: The High-Definition Trilogy. The first two films lacked the bonus features from the DVDs, although Spider-Man 2 did contain both cuts of the film.		Sony announced on April 2, 2012 that the three films would be re-released on June 12, 2012.[114] The DVDs of the first two films reinstated a selection of the bonus features missing from the earlier Blu-ray releases, although the Spider-Man 3 reissue did not include the bonus disc of additional special features that appeared on earlier Blu-ray releases.		All three films which comprise the Raimi trilogy are available in the U.S. on iTunes.[115][116][117]		The first three Spider-Man films set new opening day records in North America in their opening weekend.[125] The films are among the top of North American rankings of films based on Marvel Comics, with Spider-Man ranking fifth, Spider-Man 2 ranking sixth, and Spider-Man 3 ranking eighth.[126] In North America, Spider-Man, Spider-Man 2, and Spider-Man 3 are ranked seventh, eighth and tenth for all superhero films, with the third film ranking seventh worldwide for superhero films (behind The Avengers, Avengers: Age of Ultron, Iron Man 3, Captain America: Civil War, The Dark Knight Rises, and The Dark Knight).[127] Spider-Man, Spider-Man 2, and Spider-Man 3 are the most successful films produced by Sony/Columbia Pictures in North America.[128]		David Ansen of Newsweek enjoyed Spider-Man as a fun film to watch, though he considered Spider-Man 2 to be "a little too self-important for its own good." Ansen saw Spider-Man 3 as a return to form, finding it "the most grandiose chapter and the nuttiest."[142] Tom Charity of CNN appreciated the films' "solidly redemptive moral convictions", also noting the vast improvement of the visual effects from the first film to the third. While he saw the second film's Doc Ock as the "most engaging" villain, he applauded the third film's Sandman as "a triumph of CGI wizardry."[143] Richard Corliss of Time enjoyed the action of the films and thought that they did better than most action movies by "rethinking the characters, the franchise and the genre."[144]		Colin Covert of the Star Tribune praised Spider-Man as a "superb debut" of the superhero as well as Spider-Man 2 as a "superior sequel" for filmgoers who are fans "of spectacle and of story." Covert expressed disappointment in Spider-Man 3 as too ambitious with the multiple storylines leaving one "feeling overstuffed yet shortchanged."[145] Manohla Dargis of The New York Times enjoyed the humor of the first two films, but found it missing in the third installment. Dargis also noted, "The bittersweet paradox of this franchise is that while the stories have grown progressively less interesting the special effects have improved tremendously."[146] Robert Denerstein of the Rocky Mountain News ranked the films from his favorite to his least favorite: Spider-Man 2, Spider-Man, and Spider-Man 3. While Denerstein missed the presence of Alfred Molina as Doctor Octopus from the second film, he found the third film – despite being "bigger, though not necessarily better" – to have a "satisfying conclusion."[147]		
Alcohol intoxication (also known as drunkenness among other names) is a physiological condition that may result in psychological alterations of consciousness. Drunkenness is induced by the ingestion or consumption of Alcohol in a living body. Alcohol intoxication is the result of alcohol entering the bloodstream faster than it can be metabolized by the body. Metabolism results in breaking down the ethanol into non-intoxicating byproducts.		Some effects of alcohol intoxication (such as euphoria and lowered social inhibitions) are central to alcohol's desirability as a beverage[citation needed] and its history as one of the world's most widespread recreational drugs. Despite this widespread use and alcohol's legality in most countries, many medical sources tend to describe any level of alcohol intoxication as a form of poisoning due to ethanol's damaging effects on the body in large doses. Some religions consider alcohol intoxication to be a sin.		Symptoms of alcohol intoxication include euphoria, flushed skin, and decreased social inhibition at lower doses[citation needed], with larger doses producing progressively severe impairments of balance[citation needed], and decision-making ability[citation needed] as well as nausea or vomiting from alcohol's disruptive effect on the semicircular canals of the inner ear and chemical irritation of the gastric mucosa.[citation needed]		Sufficiently extreme levels of blood-borne alcohol may cause in entering the state of coma or death.						Acute alcohol poisoning is a related medical term used to indicate a dangerously high concentration of alcohol in the blood, high enough to induce coma, respiratory depression, or even death. It is considered a medical emergency. The term is mostly used by healthcare providers. Toxicologists use the term "alcohol intoxication" to discriminate between alcohol and other toxins.		The signs and symptoms of acute alcohol poisoning include:		Alcohol is metabolized by a normal liver at the rate of about 50 ml (1.7 US fl oz) of spirits (roughly a typical drink-size serving of beer, wine, or spirits) every 90 minutes.[citation needed] An "abnormal" liver with conditions such as hepatitis, cirrhosis, gall bladder disease, and cancer is likely to result in a slower rate of metabolism.		Ethanol is metabolised to acetaldehyde by alcohol dehydrogenase (ADH), which is found in many tissues, including the gastric mucosa. Acetaldehyde is metabolised to acetate by acetaldehyde dehydrogenase (ALDH), which is found predominantly in liver mitochondria. Acetate is used by the muscle cells to produce acetyl-CoA using the enzyme acetyl-CoA synthetase, and the acetyl-CoA is then used in the citric acid cycle.[3] It takes roughly 90 minutes for a healthy liver to metabolize a 30 ml (1.0 US fl oz) of pure ethanol, or approximately 24 minutes per British standard unit, which is 8 ml (0.27 US fl oz) of pure ethanol, or 42 min for a US standard drink, which is 14 grams 14 ml (0.47 US fl oz) of pure ethanol. One hour is often used as a safety allowance.		Ethanol's acute effects are due largely to its nature as a central nervous system depressant, and are dependent on blood alcohol concentrations:		As drinking increases, people become sleepy, or fall into a stupor. After a very high level of consumption, the respiratory system becomes depressed and the person will stop breathing. Comatose patients may aspirate their vomit (resulting in vomitus in the lungs, which may cause "drowning" and later pneumonia if survived). CNS depression and impaired motor co-ordination along with poor judgment increases the likelihood of accidental injury occurring.[5] It is estimated that about one-third of alcohol-related deaths are due to accidents and another 14% are from intentional injury.[6]		In addition to respiratory failure and accidents caused by effects on the central nervous system, alcohol causes significant metabolic derangements. Hypoglycaemia occurs due to ethanol's inhibition of gluconeogenesis, especially in children, and may cause lactic acidosis, ketoacidosis, and acute renal failure. Metabolic acidosis is compounded by respiratory failure. Patients may also present with hypothermia.		In the past, alcohol was believed to be a non-specific pharmacological agent affecting many neurotransmitter systems in the brain.[7] However, molecular pharmacology studies have shown that alcohol has only a few primary targets. In some systems, these effects are facilitatory and in others inhibitory.		Among the neurotransmitter systems with enhanced functions are: GABAA,[8] 5-HT3 receptor agonism[9] (responsible for GABAergic (GABAA receptor PAM), glycinergic, and cholinergic effects), nicotinic acetylcholine receptors.[10]		Among those that are inhibited are: NMDA,[9] dihydropyridine-sensitive L-type Ca2+ channels[11] and G-protein-activated inwardly rectifying K+ channels.[12]		The result of these direct effects is a wave of further indirect effects involving a variety of other neurotransmitter and neuropeptide systems, leading finally to the behavioural or symptomatic effects of alcohol intoxication.[7]		Many of the effects of activating GABAA receptors have the same effects as that of ethanol consumption. Some of these effects include anxiolytic, anticonvulsant, sedative, and hypnotic effects, cognitive impairment, and motor incoordination.[13] This correlation between activating GABAA receptors and the effects of ethanol consumption has led to the study of ethanol and its effects on GABAA receptors. It has been shown that ethanol does in fact exhibit positive allosteric binding properties to GABAA receptors. However, binding is only limited to pentamers containing the δ-subunit rather than the γ-subunit.[14] GABAA receptors containing the δ-subunit have been shown to be located exterior to the synapse and are involved with tonic inhibition rather than its γ-subunit counterpart, which is involved in phasic inhibition.[13] The δ-subunit has been shown to be able to form the allosteric binding site which makes GABAA receptors containing the δ-subunit more sensitive to ethanol concentrations, even to moderate social ethanol consumption levels (30mM).[15] While it has been shown by Santhakumar et al. that GABAA receptors containing the δ-subunit are sensitive to ethanol modulation, depending on subunit combinations receptors, could be more or less sensitive to ethanol.[16] It has been shown that GABAA receptors that contain both δ and β3-subunits display increased sensitivity to ethanol.[14] One such receptor that exhibits ethanol insensitivity is α3-β6-δ GABAA.[16] It has also been shown that subunit combination is not the only thing that contributes to ethanol sensitivity. Location of GABAA receptors within the synapse may also contribute to ethanol sensitivity.[13]		Definitive diagnosis relies on a blood test for alcohol, usually performed as part of a toxicology screen.		Law enforcement officers often use breathalyzer units and field sobriety tests as more convenient and rapid alternatives to blood tests.		There are also various models of breathalyzer units that are available for consumer use. Because these may have varying reliability and may produce different results than the tests used for law-enforcement purposes, the results from such devices should be conservatively interpreted.		Many informal intoxication tests exist, which, in general, are unreliable and not recommended as deterrents to excessive intoxication or as indicators of the safety of activities such as motor vehicle driving, heavy equipment operation, machine tool use, etc.		For determining whether someone is intoxicated by alcohol by some means other than a blood-alcohol test, it is necessary to rule out other conditions such as hypoglycemia, stroke, usage of other intoxicants, mental health issues, and so on. It is best if his/her behavior has been observed while the subject is sober to establish a baseline. Several well-known criteria can be used to establish a probable diagnosis. For a physician in the acute-treatment setting, acute alcohol intoxication can mimic other acute neurological disorders, or is frequently combined with other recreational drugs that complicate diagnosis and treatment.		Acute alcohol poisoning is a medical emergency due to the risk of death from respiratory depression and/or inhalation of vomit if emesis occurs while the patient is unconscious and unresponsive. Emergency treatment for acute alcohol poisoning strives to stabilize the patient and maintain a patent airway and respiration, while waiting for the alcohol to metabolize. This can be done by removal of any vomitus or, if patient is unconscious or has impaired gag reflex, intubation of the trachea using an endotracheal tube to maintain adequate airway:[17] Also:		Additional medication may be indicated for treatment of nausea, tremor, and anxiety.		A normal liver detoxifies the blood of alcohol over a period of time that depends on the initial level and the patient's overall physical condition. An abnormal liver will take longer but still succeeds, provided the alcohol does not cause liver failure.[18]		People having drunk heavily for several days or weeks may have withdrawal symptoms after the acute intoxication has subsided.[19]		A person consuming a dangerous amount of alcohol persistently can develop memory blackouts and idiosyncratic intoxication or pathological drunkenness symptoms.[20]		Long-term persistent consumption of excessive amounts of alcohol can cause liver damage and have other deleterious health effects.		Alcohol intoxication is a risk factor in some cases of catastrophic injury, in particular for unsupervised recreational activity. A study in the province of Ontario based on epidemiological data from 1986, 1989, 1992, and 1995 states that 79.2% of the 2,154 catastrophic injuries recorded for the study were preventable, of which 346 involved alcohol consumption.[21] The activities most commonly associated with alcohol-related catastrophic injury were snowmobiling (124), fishing (41), diving (40), boating (31) and canoeing (7), swimming (31), riding an all-terrain vehicle (24), and cycling (23).[21] These events are often associated with unsupervised young males, often inexperienced in the activity, and many result in drowning.[21] Alcohol use is also associated with unsafe sex.		Before and during the first world war it had been common to escape custody by buying intoxicating beverages for the guards.		Intoxicated women and prostitutes led Marie Antoinette to her first imprisonment, while singing of atrocities they had committed earlier.[22]		During the second world war several atrocities by the Germans and Russians had been recorded, done by units or unit members who were deliberately made drunk, before shooting their victims or killing them in other ways.[23][24][25]		Laws on drunkenness vary. In the United States, it is a criminal offense for a person to be drunk while driving a motorized vehicle, except in Wisconsin, where it is only a fine for the first offense.[26] It is also a criminal offense to fly an aircraft or (in some American states) to assemble or operate an amusement park ride while drunk.[27] Similar laws also exist in the United Kingdom and most other countries.		In some countries, it is also an offense to serve alcohol to an already-intoxicated person,[28] and, often, alcohol can be sold only by persons qualified to serve responsibly through alcohol server training.		The blood alcohol content (BAC) for legal operation of a vehicle is typically measured as a percentage of a unit volume of blood. This percentage ranges from 0.00% in Romania and the United Arab Emirates; to 0.05% in Australia, South Africa, Germany, Scotland and New Zealand (but 0.00% for under 20 year olds); to 0.08% in England and Wales, the United States and Canada.[29]		The United States Federal Aviation Administration prohibits crew members from performing their duties with a BAC greater than 0.04% within eight hours of consuming an alcoholic beverage, or while under the influence of alcohol.[30][31]		In the United States, the United Kingdom, and Australia, people are arrested for public intoxication, called "being drunk and disorderly" or "being drunk and incapable."[32]		In some countries, there are special facilities, sometimes known as "drunk tanks", for the temporary detention of persons found to be drunk.		Some religious groups permit the consumption of alcohol. Some permit consumption but prohibit intoxication, while others prohibit alcohol consumption altogether. Many Christian denominations such as Catholic, Orthodox, and Lutheran use wine as a part of the Eucharist and permit the drinking of alcohol but consider it sinful to become intoxicated.		In the Bible, the book of Proverbs contains several chapters dealing with the bad effects of drunkenness and warning to stay away from intoxicating beverages. The book of Leviticus tells of Nadav and Avihu, Aaron the Priest's eldest sons, who were killed for serving in the temple after drinking wine, presumably while intoxicated. The book continues to discuss monasticism where drinking wine is prohibited. The story of Samson in the Book of Judges tells of a monk from the tribe of Dan who is prohibited from cutting his hair and drinking wine.[33] Romans 13:13-14,[34] 1 Corinthians 6:9-11, Galatians 5:19-21,[35] and Ephesians 5:18[36] are among a number of other Bible passages that speak against drunkenness. While Proverbs 31:4, warns against kings and rulers drinking wine and strong drink, Proverbs 31:6–7 promotes giving strong drink to the perishing and wine to those whose lives are bitter, to forget their poverty and troubles.[37]		Some Protestant Christian denominations prohibit the drinking of alcohol[38] based upon Biblical passages that condemn drunkenness,[33] but others allow moderate use of alcohol.[39]In some Christian groups, a small amount of wine is part of the rite of communion.		In The Church of Jesus Christ of Latter-day Saints, alcohol consumption is forbidden,[40] and teetotalism has become a distinguishing feature of its members. Jehovah's Witnesses allow moderate alcohol consumption among its members.		In the Qur'an,[41][42][43] there is a prohibition on the consumption of grape-based alcoholic beverages, and intoxication is considered as an abomination in the Hadith. Islamic schools of law (Madh'hab) have interpreted this as a strict prohibition of the consumption of all types of alcohol and declared it to be haraam ("forbidden"), although other uses may be permitted.[44]		In Buddhism, in general, the consumption of intoxicants is discouraged for both monastics and lay followers. Many followers of Buddhism observe a code of conduct known as the Five Precepts, of which the fifth precept is an undertaking to refrain from the consumption of intoxicating substances (except for medical reasons). In the Bodhisattva Vows of the Brahma Net Sutra, observed by some monastic communities and some lay followers, distribution of intoxicants is likewise discouraged as well as consumption.		In the branch of Hinduism known as Gaudiya Vaishnavism, one of the four regulative principles forbids the taking of intoxicants, including alcohol.		In Judaism, in accordance with the biblical stance against drinking,[33] wine drinking was not permitted for priests and monks[45] The biblical command to sanctify the Sabbath day and other holidays has been interpreted as having three ceremonial meals which include drinking of wine, the Kiddush.[46][47] The Jewish marriage ceremony ends with the bride and groom drinking a shared cup of wine after reciting seven blessings, and according to western "Ashkenazi" traditions, after a fast day. But it has been customary and in many cases even mandated to drink moderately so as to stay sober, and only after the prayers are over.[48]		During the Seder night on Passover (Pesach) there is an obligation to drink 4 ceremonial cups of wine, while reciting the Haggadah. It has been assumed as the source for the wine drinking ritual at the communion in some Christian groups.[49] During Purim there is an obligation to become intoxicated, although, as with many other decrees, in many communities this has been avoided, by allowing sleep during the day to replace it.[50]		In the 1920s due to the new beverages law, a rabbi from the Reform Judaism movement proposed using grape-juice for the ritual instead of wine. Although refuted at first, the practice became widely accepted by orthodox Jews as well.[51]		At the Cave of the Patriarchs in Hebron - The Ibrahimi Mosque as it is called by the Muslims, the Jewish wine drinking rituals during weddings, the Sabbath day and holidays, are a cause for tension with the Muslims who unwillingly share the site under Israeli authority.[52]		In the movie Animals are Beautiful People an entire section was dedicated to showing many different animals including monkeys, elephants, hogs, giraffes, and ostriches, eating over-ripe Marula tree fruit causing them to sway and lose their footing in a manner similar to human drunkenness.[53]		In elephant warfare, practiced by the Greeks during the Maccabean revolt and by Hannibal during the Punic wars, it has been recorded that the elephants would be given wine before the attack, and only then would they charge forward after being agitated by their driver.[54]		It is a regular practice to give small amounts of beer to race horses in Ireland. Ruminant farm animals have natural fermentation occurring in their stomach, and adding alcoholic beverages in small amounts to their drink will generally do them no harm, and will not cause them to become drunk.						
Subculture, a concept from the academic fields of sociology and cultural studies, is a group of people within a culture that differentiates itself from the parent culture to which it belongs, often maintaining some of its founding principles.						While exact definitions vary, the Oxford English Dictionary defines a subculture as "a cultural group within a larger culture, often having beliefs or interests at variance with those of the larger culture."[1] As early as 1950, David Riesman distinguished between a majority, "which passively accepted commercially provided styles and meanings, and a 'subculture' which actively sought a minority style ... and interpreted it in accordance with subversive values".[2] In his 1979 book Subculture: The Meaning of Style, Dick Hebdige argued that a subculture is a subversion to normalcy. He wrote that subcultures can be perceived as negative due to their nature of criticism to the dominant societal standard. Hebdige argued that subcultures bring together like-minded individuals who feel neglected by societal standards and allow them to develop a sense of identity.[3]		In 1995, Sarah Thornton, drawing on Pierre Bourdieu, described "subcultural capital" as the cultural knowledge and commodities acquired by members of a subculture, raising their status and helping differentiate themselves from members of other groups.[4] In 2007, Ken Gelder proposed to distinguish subcultures from countercultures based on the level of immersion in society.[5] Gelder further proposed six key ways in which subcultures can be identified through their:		Sociologists Gary Alan Fine and Sherryl Kleinman argued that their 1979 research showed that a subculture is a group that serves to motivate a potential member to adopt the artifacts, behaviors, norms, and values characteristic of the group.[citation needed]		The evolution of subcultural studies has three main steps:[6]		1) Subcultures and deviance - The earliest subcultures studies came from the so-called Chicago School, who interpreted them as forms of deviance and delinquency. Starting with what they called Social Disorganization Theory, they claimed that subcultures emerged on one hand because of some population sectors’ lack of socialisation with the mainstream culture and, on the other, because of their adoption of alternative axiological and normative models. As Robert E. Park, Ernest Burgess and Louis Wirth suggested, by means of selection and segregation processes, there thus appear in society natural areas or moral regions where deviant models concentrate and are re-inforced; they do not accept objectives or means of action offered by the mainstream culture, proposing different ones in their place – thereby becoming, depending on circumstances, innovators, rebels or retreatists (Richard Cloward and Lloyd Ohlin). Subcultures, however, are not only the result of alternative action strategies but also of labelling processes on the basis of which, as Howard S. Becker explains, society defines them as outsiders. As Cohen clarifies, every subculture’s style, consisting of image, demeanour and language becomes its recognition trait. And an individual’s progressive adoption of a subcultural model will furnish him/her with growing status within this context but it will often, in tandem, deprive him/her of status in the broader social context outside where a different model prevails.[7]		2) Subcultures and resistance - In the work of John Clarke, Stuart Hall, Tony Jefferson and Brian Roberts of the Birmingham CCCS (Centre for Contemporary Cultural Studies), subcultures are interpreted as forms of resistance. Society is seen as being divided into two fundamental classes, the working class and the middle class, each with its own class culture, and middle-class culture being dominant. Particularly in the working class, subcultures grow out of the presence of specific interests and affiliations around which cultural models spring up, in conflict with both their parent culture and mainstream culture. Facing a weakening of class identity, subcultures are then new forms of collective identification expressing what Cohen called symbolic resistance against the mainstream culture and developing imaginary solutions for structural problems. As Paul Willis and Dick Hebdige underline, identity and resistance are expressed through the development of a distinctive style which, by a re-signification and ‘bricolage’ operation, use cultural industry goods to communicate and express one’s own conflict. Yet the cultural industry is often capable of re-absorbing the components of such a style and once again transforming them into goods. At the same time the mass media, while they participate in building subcultures by broadcasting their images, also weaken them by depriving them of their subversive content or by spreading a stigmatized image of them.[8]		3) Subcultures and distinction - The most recent interpretations see subcultures as forms of distinction. In an attempt to overcome the idea of subcultures as forms of deviance or resistance, they describe subcultures as collectivities which, on a cultural level, are sufficiently homogeneous internally and heterogeneous with respect to the outside world to be capable of developing, as Paul Hodkinson points out, consistent distinctiveness, identity, commitment and autonomy. Defined by Sarah Thornton as taste cultures, subcultures are endowed with elastic, porous borders, and are inserted into relationships of interaction and mingling, rather than independence and conflict, with the cultural industry and mass media, as Steve Redhead and David Muggleton emphasize. The very idea of a unique, internally homogeneous, dominant culture is explicitly criticized. Thus forms of individual involvement in subcultures are fluid and gradual, differentiated according to each actor’s investment, outside clear dichotomies. The ideas of different levels of subcultural capital (Sarah Thornton) possessed by each individual, of the supermarket of style (Ted Polhemus) and of style surfing (Martina Böse) replace that of the subculture’s insiders and outsiders – with the perspective of subcultures supplying resources for the construction of new identities going beyond strong, lasting identifications.		The study of subcultures often consists of the study of symbolism attached to clothing, music and other visible affectations by members of subcultures, and also of the ways in which these same symbols are interpreted by members of the dominant culture. Dick Hebdige writes that members of a subculture often signal their membership through a distinctive and symbolic use of style, which includes fashions, mannerisms and argot.[9]		Subcultures can exist at all levels of organizations, highlighting the fact that there are multiple cultures or value combinations usually evident in any one organization that can complement but also compete with the overall organisational culture.[10] In some instances, subcultures have been legislated against, and their activities regulated or curtailed.[11] British youth subcultures had been described as a moral problem that ought to be handled by the guardians of the dominant culture within the post-war consensus.[11]		It may be difficult to identify certain subcultures because their style (particularly clothing and music) may be adopted by mass culture for commercial purposes. Businesses often seek to capitalize on the subversive allure of subcultures in search of Cool, which remains valuable in the selling of any product.[12] This process of cultural appropriation may often result in the death or evolution of the subculture, as its members adopt new styles that appear alien to mainstream society.[13]		Music-based subcultures are particularly vulnerable to this process; what may be considered subcultures at one stage in their histories – such as jazz, goth, punk, hip hop and rave cultures – may represent mainstream taste within a short period.[14] Some subcultures reject or modify the importance of style, stressing membership through the adoption of an ideology which may be much more resistant to commercial exploitation.[15] The punk subculture's distinctive (and initially shocking) style of clothing was adopted by mass-market fashion companies once the subculture became a media interest. Dick Hebdige argues that the punk subculture shares the same "radical aesthetic practices" as Dada and surrealism:		Like Duchamp's 'ready mades' - manufactured objects which qualified as art because he chose to call them such, the most unremarkable and inappropriate items - a pin, a plastic clothes peg, a television component, a razor blade, a tampon - could be brought within the province of punk (un)fashion ... Objects borrowed from the most sordid of contexts found a place in punks' ensembles; lavatory chains were draped in graceful arcs across chests in plastic bin liners. Safety pins were taken out of their domestic 'utility' context and worn as gruesome ornaments through the cheek, ear or lip ... fragments of school uniform (white bri-nylon shirts, school ties) were symbolically defiled (the shirts covered in graffiti, or fake blood; the ties left undone) and juxtaposed against leather drains or shocking pink mohair tops.[16]		In 1985, French sociologist Michel Maffesoli coined the term urban tribe. It gained widespread use after the publication of his Le temps des tribus: le déclin de l'individualisme dans les sociétés postmodernes (1988).[17] Eight years later, this book was published in the United Kingdom as The Time of the Tribes: The Decline of Individualism in Mass Society.[18]		According to Maffesoli, urban tribes are microgroups of people who share common interests in urban areas. The members of these relatively small groups tend to have similar worldviews, dress styles and behavioral patterns. Their social interactions are largely informal and emotionally laden, different from late capitalism's corporate-bourgeoisie cultures, based on dispassionate logic. Maffesoli claims that punks are a typical example of an "urban tribe".[19]		Five years after the first English translation of Le temps des tribus, writer Ethan Watters claims to have coined the same neologism in a New York Times Magazine article. This was later expanded upon the idea in his book Urban Tribes: A Generation Redefines Friendship, Family, and Commitment. According to Watters, urban tribes are groups of never-marrieds between the ages of 25 and 45 who gather in common-interest groups and enjoy an urban lifestyle, which offers an alternative to traditional family structures.[20]		The sexual revolution of the 1960s led to a countercultural rejection of the established sexual and gender norms, particularly in the urban areas of Europe, North and South America, Australia, and white South Africa. A more permissive social environment in these areas led to a proliferation of sexual subcultures—cultural expressions of non-normative sexuality. As with other subcultures, sexual subcultures adopted certain styles of fashion and gestures to distinguish them from the mainstream.[21]		Homosexuals expressed themselves through the gay culture, considered the largest sexual subculture of the 20th century. With the ever-increasing acceptance of homosexuality in the early 21st century, including its expressions in fashion, music, and design, the gay culture can no longer be considered a subculture in many parts of the world, although some aspects of gay culture like leathermen, bears, and feeders are considered subcultures within the gay movement itself.[21] The butch and femme identities or roles among some lesbians also engender their own subculture with stereotypical attire, for instance drag kings.[22] A late 1980s development, the queer movement can be considered a subculture broadly encompassing those that reject normativity in sexual behavior, and who celebrate visibility and activism. The wider movement coincided with growing academic interests in queer studies and queer theory. Aspects of sexual subcultures can vary along other cultural lines. For instance, in the United States, the term down-low is used to refer to African-American men who do not identify themselves with the gay or queer cultures, but who practice gay cruising, and adopt a specific hip-hop attire during this activity.[22]		In a 2011 study, Brady Robards and Andy Bennett said that online identity expression has been interpreted as exhibiting subcultural qualities. However, they argue it is more in line with neotribalism than with what is often classified as subculture. Social networking websites are quickly becoming the most used form of communication and means to distribute information and news. They offer a way for people with similar backgrounds, lifestyles, professions or hobbies to connect. According to a co-founder and executive creative strategist for RE-UP, as technology becomes a "life force," subcultures become the main bone of contention for brands as networks rise through cultural mash-ups and phenomenons.[23] Where social media is concerned, there seems to be a growing interest among media producers to use subcultures for branding. This is seen most actively on social network sites with user-generated content, such as YouTube.		Social media expert Scott Huntington cites one of the ways in which subcultures have been and can be successfully targeted to generate revenue: "It’s common to assume that subcultures aren’t a major market for most companies. Online apps for shopping, however, have made significant strides. Take Etsy, for example. It only allow vendors to sell handmade or vintage items, both of which can be considered a rather “hipster” subculture. However, retailers on the site made almost $900 million in sales."[24]		
A nerd is a person seen as overly intellectual, obsessive, or lacking social skills. Such a person may spend inordinate amounts of time on unpopular, little known, or non-mainstream activities, which are generally either highly technical, abstract, or relating to topics of fiction or fantasy, to the exclusion of more mainstream activities.[1][2][3] Additionally, many so-called nerds are described as being shy, quirky, pedantic, and unattractive,[4] and may have difficulty participating in, or even following, sports.		Though originally derogatory, nerd is a stereotypical term, but as with other pejoratives, it has been reclaimed and redefined by some as a term of pride and group identity.						The first documented appearance of the word nerd is as the name of a creature in Dr. Seuss's book If I Ran the Zoo (1950), in which the narrator Gerald McGrew claims that he would collect "a Nerkle, a Nerd, and a Seersucker too" for his imaginary zoo.[3][5][6] The slang meaning of the term dates to the next year, 1951, when Newsweek magazine reported on its popular use as a synonym for drip or square in Detroit, Michigan.[7] By the early 1960s, usage of the term had spread throughout the United States, and even as far as Scotland.[8][9] At some point, the word took on connotations of bookishness and social ineptitude.[5]		An alternate spelling,[10] as nurd or gnurd, also began to appear in the mid-1960s or early 1970s.[11] Author Philip K. Dick claimed to have coined the nurd spelling in 1973, but its first recorded use appeared in a 1965 student publication at Rensselaer Polytechnic Institute.[12][13] Oral tradition there holds that the word is derived from knurd (drunk spelled backward), which was used to describe people who studied rather than partied. The term gnurd (spelled with the "g") was in use at the Massachusetts Institute of Technology by 1965.[14] The term nurd was also in use at the Massachusetts Institute of Technology as early as 1971 but was used in the context for the proper name of a fictional character in a satirical "news" article.[15]		The Online Etymology Dictionary speculates that the word is an alteration of the 1940s term nert (meaning "stupid or crazy person"), which is itself an alteration of "nut".[16]		The term was popularized in the 1970s by its heavy use in the sitcom Happy Days.[17]		Because of the nerd stereotype, many smart people are often thought of as nerdy. This belief can be harmful, as it can cause high-school students to "switch off their lights" out of fear of being branded as a nerd,[18] and cause otherwise appealing people to be nerdy simply for their intellect. It was once thought that intellectuals were nerdy because they were envied. However, Paul Graham stated in his essay, "Why Nerds are Unpopular", that intellect is neutral, meaning that you are neither loved nor despised for it. He also states that it is only the correlation that makes smart teens automatically seem nerdy, and that a nerd is someone that is not socially adept enough. Additionally, he says that the reason why many smart kids are unpopular is that they "don't have time for the activities required for popularity."[19]		Stereotypical nerd appearance, often lampooned in caricatures, includes very large glasses, braces, severe acne and pants worn high at the waist. In the media, many nerds are males, portrayed as being physically unfit, either overweight or skinny due to lack of physical exercise.[20][21] It has been suggested by some, such as linguist Mary Bucholtz, that being a nerd may be a state of being "hyperwhite" and rejecting African-American culture and slang that "cool" white children use.[22] However, after the Revenge of the Nerds movie franchise (with multicultural nerds), and the introduction of the Steve Urkel character on the television series Family Matters, nerds have been seen in all races and colors as well as more recently being a frequent young Asian or Indian male stereotype in North America. Portrayal of "nerd girls", in films such as She's Out of Control, Welcome to the Dollhouse and She's All That depicts that smart but nerdy women might suffer later in life if they do not focus on improving their physical attractiveness.[23]		In the United States, a 2010 study published in the Journal of International and Intercultural Communication indicated that Asian Americans are perceived as most likely to be nerds, followed by White Americans, while non-White Hispanics and Black Americans were perceived as least likely to be nerds. These stereotypes stem from concepts of Orientalism and Primitivism, as discussed in Ron Eglash's essay Race, Sex, and Nerds: From Black Geeks to Asian American Hipsters.[24] Among Whites, Jews are perceived as the most nerdy and are stereotyped in similar ways to Asians.[25]		The rise of Silicon Valley and the American computer industry at large have allowed many so-called nerdy people to accumulate large fortunes. Many stereotypically nerdy interests, such as superhero and science fiction works, are now popular culture hits.[26] Some measures of nerdiness are now allegedly considered desirable, as, to some, it suggests a person who is intelligent, respectful, interesting, and able to earn a large salary. Stereotypical nerd qualities are evolving, going from awkwardness and social ostracism to an allegedly more widespread acceptance and sometimes even celebration of their differences.[27]		Johannes Grenzfurthner, researcher, self-proclaimed nerd and director of nerd documentary Traceroute, reflects on the emergence of nerds and nerd culture:		I think that the figure of the nerd provides a beautiful template for analyzing the transformation of the disciplinary society into the control society. The nerd, in his cliche form, first stepped out upon the world stage in the mid-1970s, when we were beginning to hear the first rumblings of what would become the Cambrian explosion of the information society. The nerd must serve as comic relief for the future-anxieties of Western society. [...] The germ cell of burgeoning nerdism is difference. The yearning to be understood, to find opportunities to share experiences, to not be left alone with one's bizarre interest. At the same time one derives an almost perverse pleasure from wallowing in this deficit. Nerds love deficiency: that of the other, but also their own. Nerds are eager explorers, who enjoy measuring themselves against one another and also compete aggressively. And yet the nerd's existence also comprises an element of the occult, of mystery. The way in which this power is expressed or focused is very important.		In the 1984 film Revenge of the Nerds Robert Carradine worked to embody the nerd stereotype; in doing so, he helped create a definitive image of nerds.[29] Additionally, the storyline presaged, and may have helped inspire, the "nerd pride" that emerged in the 1990s.[citation needed] American Splendor regular Toby Radloff claims this was the movie that inspired him to become "The Genuine Nerd from Cleveland, Ohio."[30] In the American Splendor film, Toby's friend, American Splendor author Harvey Pekar, was less receptive to the movie, believing it to be hopelessly idealistic, explaining that Toby, an adult low income file clerk, had nothing in common with the middle class kids in the film who would eventually attain college degrees, success, and cease being perceived as nerds. Many, however, seem to share Radloff's view, as "nerd pride" has become more widespread in the years since. MIT professor Gerald Sussman, for example, seeks to instill pride in nerds:		My idea is to present an image to children that it is good to be intellectual, and not to care about the peer pressures to be anti-intellectual. I want every child to turn into a nerd – where that means someone who prefers studying and learning to competing for social dominance, which can unfortunately cause the downward spiral into social rejection.		The popular computer-related news website Slashdot uses the tagline "News for nerds. Stuff that matters." The Charles J. Sykes quote "Be nice to nerds. Chances are you'll end up working for one" has been popularized on the Internet and incorrectly attributed to Bill Gates.[32] In Spain, Nerd Pride Day has been observed on May 25 since 2006,[33] the same day as Towel Day, another somewhat nerdy holiday.[34] The date was picked because it's the anniversary of the release of Star Wars: A New Hope.[35]		An episode from the animated series Freakazoid, titled "Nerdator", includes the use of nerds to power the mind of a Predator-like enemy. Towards the middle of the show, he gave this speech. :		...most nerds are shy ordinary-looking types with no interest in physical activity. But, what they lack in physical prowess they make up in brains. Tell me, who writes all the best selling books? Nerds. Who makes all the top grossing movies? Nerds. Who designs computer programs so complex that only they can use them? Nerds. And who is running for high public office? No one but nerds. ... Without nerds to lead the way, the governments of the world will stumble, they'll be forced to seek guidance from good-looking, but vapid airheads.[36]		The Danish reality TV show FC Zulu, known in the internationally franchised format as FC Nerds, established a format wherein a team of nerds, after two or three months of training, competes with a professional soccer team.[37]		Some commentators consider that the word is devalued when applied to people who adopt a sub-cultural pattern of behaviour, rather than being reserved for people with a marked ability.[38]		Although originally being predominately an American stereotype, Nerd culture has grown across the globe and is now more acceptable and common than ever. Australian events such as Oz Comic-Con (a large comic book and Cosplay convention, similar to San Diego Comic-Con International) and Supernova, are incredibly popular events among the culture of people who identify themselves as nerds. In 2016, Oz Comic-Con in Perth saw almost 20,000 cos-players and comic book fans meet to celebrate the event, hence being named a "professionally organised Woodstock for geeks".[39]		People who tend to possess so-called nerdy characteristics can often be the subject of bullying in the workplace and in schools especially. With the rise of Cyberbullying, antisocial behaviour towards people described as nerdy continues. A 2012 study in a Victorian high school, reported that "72% of boys and 65% of girls" had experienced relational aggression.[40] Nerds are often the target of bullying due to a range of reasons that may include, physical appearance or social background.[20]		Paul Graham has suggested that the reason nerds are frequently singled out for bullying is their indifference to popularity, in the face of a youth culture that views popularity as paramount.[19] However, research findings suggest that bullies are often as socially inept as their academically better-performing victims,[41] and that popularity fails to confer protection from bullying.[42] Other commentators have pointed out that pervasive harassment of intellectually-oriented youth began only in the mid-twentieth century[43][44] and some have suggested that its cause involves jealousy over future employment opportunities and earning potential.[45]		Film has seen several memorable nerdy characters including, but not limited to: Anthony Michael Hall's character of Brian Johnson in The Breakfast Club, Dr Spencer Reid from Criminal Minds, Fogell from Superbad (film), Peter Parker from the Spider-Man franchise, Hermione Granger from the Harry Potter franchise, Lewis Skolnick and Gilbert Lowe from Revenge of the Nerds, Steve Carell's character of Andy Stitzer in The 40-Year-Old Virgin, and various characters in The Big Bang Theory and Silicon Valley.[46] The parody song and music video White & Nerdy by "Weird Al" Yankovic also prominently features and celebrates aspects of Nerd culture.[47]		
Cosplay (コスプレ, kosupure), a contraction of the words costume play, is a hobby in which participants called cosplayers wear costumes and fashion accessories to represent a specific character.[1] Cosplayers often interact to create a subculture, and a broader use of the term "cosplay" applies to any costumed role-playing in venues apart from the stage. Any entity that lends itself to dramatic interpretation may be taken up as a subject and it is not unusual to see genders switched. Favorite sources include manga and anime, comic books and cartoons, video games, and live-action films and television series.		The rapid growth in the number of people cosplaying as a hobby since 1990s has made the phenomenon a significant aspect of popular culture in Japan and some other parts of Asia[2] and in the Western world. Cosplay events are common features of fan conventions and there are also dedicated conventions and local and international competitions, as well as social networks, websites and other forms of media centered on cosplay activities.		The term "cosplay" was coined in Japan in 1984. It was inspired by and grew out of the practice then-known as fan costuming at science fiction conventions, beginning with the 1st World Science Fiction Convention in New York City in 1939.		The term "cosplay" is a Japanese portmanteau of the English terms costume and play.[3] The term was coined by Nobuyuki Takahashi of Studio Hard[4] while attending the 1984 World Science Fiction Convention (Worldcon) in Los Angeles.[5] He was impressed by the hall and the costumed fans and reported on both in Japanese magazine My Anime.[4] Takahashi chose to coin a new word rather than use the existing translation of the English term "masquerade" because that translates into Japanese as "an aristocratic costume", which did not match his experience of the WorldCon.[6][7] The coinage reflects a common Japanese method of abbreviation in which the first two moras of a pair of words are used to form an independent compound: 'costume' becomes kosu (コス) and 'play' becomes pure (プレ).		Masquerade balls were a feature of the Carnival season in the 15th century, and involved increasingly elaborate allegorical Royal Entries, pageants, and triumphal processions celebrating marriages and other dynastic events of late medieval court life. They were extended into costumed public festivities in Italy during the 16th century Renaissance, generally elaborate dances held for members of the upper classes, which were particularly popular in Venice.		Costume parties (American English) or fancy dress parties (British English) were popular from the 19th century onwards. Costuming guides of the period, such as Samuel Miller's Male Character Costumes (1884)[8] or Ardern Holt's Fancy Dresses Described (1887),[9] feature mostly generic costumes, whether that be period costumes, national costumes, objects or abstract concepts such as "Autumn" or "Night". Most specific costumes described therein are for historical figures although some are sourced from fiction, like The Three Musketeers or Shakespeare characters.		One of the earliest recorded examples of costuming based on an existing character from popular media (as opposed to legend or history) were costumes based on A.D. Condo's science fiction comic character Mr. Skygack, from Mars. In 1908, a Mr. and Mrs. William Fell attended a masquerade at a skating rink in Cincinnati, Ohio wearing Mr. Skygack and Miss Dillpickles costumes. Shortly after, in 1910, an unnamed woman won first prize at masquerade ball in Tacoma, Washington wearing another Skygack costume.[13][14]		The first people to wear costumes to attend a convention were science fiction fans Forrest J Ackerman and Myrtle R. Douglas. They attended the 1939 1st World Science Fiction Convention (Nycon or 1st Worldcon) in the Caravan Hall, New York, USA dressed in "futuristicostumes", including green cape and breeches, based on the pulp magazine artwork of Frank R. Paul and the 1936 film Things to Come, designed and created by Douglas.[14][15][16] Ackerman later stated that he thought everyone was supposed to wear a costume at a science fiction convention, although only he and Douglas did.[17]		Fan costuming caught on, however, and the 2nd Worldcon, in 1940, had both an unofficial masquerade held in Douglas' room and an official masquerade as part of the programme.[5][18][19] David Kyle won the masquerade wearing a Ming the Merciless costume created by Leslie Perri, while Robert A. W. Lowndes received second place with a Bar Senestro costume (from the novel The Blind Spot by Austin Hall and Homer Eon Flint).[18] Other costumed attendees included guest of honor E. E. Smith as Northwest Smith (from C. L. Moore's series of short stories) and both Ackerman and Douglas wearing their futuristicostumes again.[17][18] Masquerades and costume balls continued to be part of World Science Fiction Convention tradition thereafter.[19] Early Worldcon masquerade balls featured a band, dancing, food and drinks. Contestants either walked across a stage or a cleared area of the dance floor.[19]		Ackerman wore a "Hunchbackerman of Notre Dame" costume to the 3rd Worldcon in 1941, which included a mask designed and created by Ray Harryhausen, but soon stopped wearing costumes to conventions.[17] Douglas wore an Akka costume (from A. Merritt's novel The Moon Pool), the mask again made by Harryhausen, to the 3rd Worldcon and a Snake Mother costume (another Merritt costume, from The Snake Mother) to the 4th Worldcon in 1946.[20]		Rules governing costumes became established in response to specific costumes and costuming trends. The first nude contestant at a Worldcon masquerade was in 1952 but the height of this trend was in the 1970s and early 1980s, with a few every year.[19] This eventually led to "No Costume is No Costume" rule, which banned full nudity, although partial nudity was still allowed as long as it was a legitimate representation of the character.[14] Mike Resnick describes the best of the nude costumes as Kris Lundi wearing a harpy costume to the 32nd Worldcon in 1974 (she received an honorable mention in the competition).[19][21][22] Another costume that instigated a rule change was an attendee at the 20th Worldcon (1962) whose blaster prop fired a jet of real flame; which led to fire being banned.[19] At the 30th WorldCon (1972), artist Scott Shaw wore a costume composed largely of peanut butter to represent his own underground comix character called "The Turd". The peanut butter rubbed off, doing damage to soft furnishings and other peoples' costumes, and then began to go rancid under the heat of the lighting. Food, odious and messy substances were banned as costume elements after that event.[19][23][24][25]		Costuming spread with the science fiction conventions and the interaction of fandom. The earliest known instance of costuming at a convention in the United Kingdom was at the London Science Fiction Convention in 1953 but this was only as part of a play. However, members of the Liverpool Science Fantasy Society attended the 1st Cytricon (1955), in Kettering, wearing costumes and continued to do so in subsequent years.[26] The 15th Worldcon (1957) brought the first official convention masquerade to the UK.[26] The 1960 Eastercon in London may have been the first British-based convention to hold an official fancy dress party as part of its programme.[27] The joint winners were Ethel Lindsay and Ina Shorrock as two of the titular witches from the novel The Witches of Karres by James H. Schmitz.[28] In Japan, costuming at conventions was a fan activity from at least the 1970s, especially after the launch of the Comiket convention in December 1975.[14] Costuming at this time was known as kasou (仮想).[14] The first documented case of costuming at a fan event in Japan was at Ashinocon (1978), in Hakone, at which future science fiction critic Mari Kotani wore a costume based on the cover art for Edgar Rice Burroughs' novel A Fighting Man of Mars.[Notes 1][29][30] In an interview Kotani states that there were about twenty costumed attendees at the convention's costume party—made up of members of her Triton of the Sea fan club and Kansai Entertainers (西芸人人, Kansai Geinin), antecedent of the Gainax anime studio—with most attendees in ordinary clothing.[29] One of the Kansai group, an unnamed friend of Yasuhiro Takeda, wore an impromptu Tusken Raider costume (from the film Star Wars) made from one of the host-hotel's rolls of toilet paper.[31] Costume contests became a permanent part of the Nihon SF Taikai conventions from Tokon VII in 1980.		Possibly the first costume contest held at a comic book convention was at the 1st Academy Con held at Broadway Central Hotel, New York in August 1965.[32] Roy Thomas, future editor-in-chief of Marvel Comics but then just transitioning from a fanzine editor to a professional comic book writer, attended in a Plastic Man costume.[32]		The first Masquerade Ball held at San Diego Comic-Con was in 1974 during the convention's 6th event. Voice actress June Foray was the master of ceremonies.[33] Future scream queen Brinke Stevens won first place wearing a Vampirella costume.[34][35] Forrest J Ackerman, the creator of Vampirella, was in attendance and posed with Stevens for photographs. They became friends and, according to Stevens "Forry and his wife, Wendayne, soon became like my god parents."[36] Photographer Dan Golden saw a photograph of Stevens in the Vampirella costume while visiting Ackerman's house, leading to him hiring her for a non-speaking role in her first student film, Zyzak is King (1980), and later photographing her for the cover of the first issue of Femme Fatales (1992).[36] Stevens attributes these events to launching her acting career.[36]		As early as a year after the 1975 release of The Rocky Horror Picture Show, audience members began dressing as characters from the movie and role-playing (although the initial incentive for dressing-up was free admission) in often highly accurate costumes.[37][38]		Costume-Con, a convention dedicated to costuming, was first held in January 1983.[39][40] The International Costumers Guild, originally known as the Greater Columbia Fantasy Costumer’s Guild, was launched after the 3rd Costume-Con (1985) as a parent organization and to support costuming.[39]		In 1984, Nobuyuki Takahashi, founder of Studio Hard, attended the 42nd Worldcon in Los Angeles. He was impressed with the masquerade and reported on it in My Anime, coining the term kosupure (from which cosplay is derived) in the process. His report also encouraged Japanese fans to include more costuming in their own conventions.[4][5][6][7] The initial report also used the terms "costume play" (コスチュームプレイ, kosuchuumu purei) and the English "Hero Costume Operation" but kosupure was the term that caught on.[14]		As stated above, costuming had been a fan activity in Japan from the 1970s, and it became much more popular in the wake of Takahashi's report. The new term did not catch on immediately, however. It was a year or two after the article was published before it was in common use among fans at conventions.[14] It was in the 1990s, after exposure on television and in magazines, that the term and practice of cosplaying became common knowledge in Japan.[14]		The first cosplay cafés appeared in the Akihabara area of Tokyo in the late 1990s.[5][41] A temporary maid café was set up at the Tokyo Character Collection event in August 1998 to promote the video game Welcome to Pia Carrot 2 (1997).[41] An occasional Pia Carrot Restaurant was held at the shop Gamers in Akihabara in the years up to 2000.[41] Being linked to specific intellectual properties limited the lifespan of these cafés, which was solved by using generic maids, leading to the first permanent establishment, Cure Maid Café, which opened in March 2001.[41]		The first World Cosplay Summit was held on October 12, 2003 at the Rose Court Hotel in Nagoya, Japan, with five cosplayers invited from Germany, France and Italy. There was no contest until 2005, when the World Cosplay Championship began. The first winners were the Italian team of Giorgia Vecchini, Francesca Dani and Emilia Fata Livia.		Worldcon masquerade attendance peaked in the 1980s and started to fall thereafter. This trend was reversed when the concept of cosplay was re-imported from Japan.[14]		Cosplay costumes vary greatly and can range from simple themed clothing to highly detailed costumes. It is generally considered different from Halloween and Mardi Gras costume wear, as the intention is to replicate a specific character, rather than to reflect the culture and symbolism of a holiday event. As such, when in costume, some cosplayers often seek to adopt the affect, mannerisms, and body language of the characters they portray (with "out of character" breaks). The characters chosen to be cosplayed may be sourced from any movie, TV series, book, comic book, video game, or music band anime and manga characters. Some cosplayers even choose to cosplay an original character of their own design or a fusion of different genres (e.g., a steampunk version of a character).		Cosplayers obtain their apparel through many different methods. Manufacturers produce and sell packaged outfits for use in cosplay, with varying levels of quality. These costumes are often sold online, but also can be purchased from dealers at conventions. Japanese manufacturers of cosplay costumes reported a profit of 35 billion yen in 2008.[42] A number of individuals also work on commission, creating custom costumes, props, or wigs designed and fitted to the individual. Other cosplayers, who prefer to create their own costumes, still provide a market for individual elements, and various raw materials, such as unstyled wigs, hair dye, cloth and sewing notions, liquid latex, body paint, costume jewelry, and prop weapons.		Cosplay represents an act of embodiment. Cosplay has been closely linked to the presentation of self,[43] yet cosplayers' ability to perform is limited by their physical features. The accuracy of a cosplay is judged based on the ability to accurately represent a character through the body, and individual cosplayers frequently are faced by their own "bodily limits"[44] such as level of attractiveness, body size, and disability[45] that often restrict and confine how accurate the cosplay is perceived. Authenticity is measured by a cosplayer's individual ability to translate on-screen manifestation to the cosplay itself. Some have argued that cosplay can never be a true representation of the character; instead, it can only be read through the body, and that true embodiment of a character is judged based on nearness to the original character form.[46] Cosplaying can also help some of those with self-esteem problems.[47][48]		Many cosplayers create their own outfits, referencing images of the characters in the process. In the creation of the outfits, much time is given to detail and qualities, thus the skill of a cosplayer may be measured by how difficult the details of the outfit are and how well they have been replicated. Because of the difficulty of replicating some details and materials, cosplayers often educate themselves in crafting specialties such as textiles, sculpture, face paint, fiberglass, fashion design, woodworking, and other uses of materials in the effort to render the look and texture of a costume accurately.[49] Cosplayers often wear wigs in conjunction with their outfit to further improve the resemblance to the character. This is especially necessary for anime and manga or video-game characters who often have unnaturally coloured and uniquely styled hair. Simpler outfits may be compensated for their lack of complexity by paying attention to material choice and overall high quality.		To look more like the characters they are portraying, cosplayers might also engage in various forms of body modification. Cosplayers may opt to change their skin color utilizing bleach or make-up to more simulate the race of the character they are adopting.[50] Contact lenses that match the color of their characters' eyes are a common form of this, especially in the case of characters with particularly unique eyes as part of their trademark look. Contact lenses that make the pupil look enlarged to visually echo the large eyes of anime and manga characters are also used.[51] Another form of body modification in which cosplayers engage is to copy any tattoos or special markings their character might have. Temporary tattoos, permanent marker, body paint, and in rare cases, permanent tattoos, are all methods used by cosplayers to achieve the desired look. Permanent and temporary hair dye, spray-in hair coloring, and specialized extreme styling products are all used by some cosplayers whose natural hair can achieve the desired hairstyle. It is also commonplace for them to shave off their eyebrows to gain a more accurate look.		Some anime and video game characters have weapons or other accessories that are hard to replicate, and conventions have strict rules regarding those weapons, but most cosplayers engage in some combination of methods to obtain all the items necessary for their costumes; for example, they may commission a prop weapon, sew their own clothing, buy character jewelry from a cosplay accessory manufacturer, or buy a pair of off-the-rack shoes, and modify them to match the desired look.		Cosplay may be presented in a number of ways and places. A subset of cosplay culture is centered on sex appeal, with cosplayers specifically choosing characters known for their attractiveness or revealing costumes. However, wearing a revealing costume can be a sensitive issue while appearing in public.[52][53][54] People appearing naked at American science fiction fandom conventions during the 1970s were so common, a "no costume is no costume" rule was introduced.[55] Some conventions throughout the United States, such as Phoenix Comicon[56] and Penny Arcade Expo,[57] have also issued rules upon which they reserve the right to ask attendees to leave or change their costumes if deemed to be inappropriate to a family-friendly environment or something of a similar nature.		The most popular form of presenting a cosplay publicly is by wearing it to a fan convention. Multiple conventions dedicated to anime and manga, comics, TV shows, video games, science fiction, and fantasy may be found all around the world. Cosplay-centered conventions include Cosplay Mania in the Philippines and EOY Cosplay Festival in Singapore.		The single largest event featuring cosplay is the semiannual doujinshi market, Comic Market (Comiket), held in Japan during summer and winter. Comiket attracts hundreds of thousands of manga and anime fans, where thousands of cosplayers congregate on the roof of the exhibition center. In North America, the highest-attended fan conventions featuring cosplayers are the San Diego Comic-Con and New York Comic Con held in the United States, and the anime-specific Anime North in Toronto, Otakon held in Baltimore MD and Anime Expo held in Los Angeles. Europe's largest event is Japan Expo held in Paris, while the London MCM Expo and the London Super Comic Convention are the most notable in the UK. Supanova Pop Culture Expo is Australia's biggest event.		The appearance of cosplayers at public events makes them a popular draw for photographers.[58] As this became apparent in the late 1980s, a new variant of cosplay developed in which cosplayers attended events mainly for the purpose of modeling their characters for still photography rather than engaging in continuous role play. Rules of etiquette were developed to minimize awkward situations involving boundaries. Cosplayers pose for photographers and photographers do not press them for personal contact information or private sessions, follow them out of the area, or take photos without permission. The rules allow the collaborative relationship between photographers and cosplayers to continue with the least inconvenience to each other.[59]		Some cosplayers choose to have a professional photographer take high quality images of them in their costumes posing as the character.[58] This is most likely to take place in a setting relevant to the character's origin, such as churches, parks, forests, water features, and abandoned/run-down sites. Cosplayers and photographers are likely to exhibit their work online, which they can do on general blog, social networking service, and artist gallery websites (such as flickr, deviantART, Instagram, Facebook, tumblr and Twitter) or on dedicated cosplay community websites. They may also choose to sell such images, or use them as part of their portfolio.[58]		As the popularity of cosplay has grown, many conventions have come to feature a contest surrounding cosplay that may be the main feature of the convention. Contestants present their cosplay, and often to be judged for an award, the cosplay must be self-made. The contestants may choose to perform a skit, which may consist of a short performed script or dance with optional accompanying audio, video, or images shown on a screen overhead. Other contestants may simply choose to pose as their characters. Often, contestants are briefly interviewed on stage by a master of ceremonies. The audience is given a chance to take photos of the cosplayers. Cosplayers may compete solo or in a group. Awards are presented, and these awards may vary greatly. Generally, a best cosplayer award, a best group award, and runner-up prizes are given. Awards may also go to the best skit and a number of cosplay skill subcategories, such as master tailor, master weapon-maker, master armourer, and so forth.		The most well-known cosplay contest event is the World Cosplay Summit, selecting cosplayers from 20 countries to compete in the final round in Nagoya, Japan. Some other international events include European Cosplay Gathering (finals taking place at Japan Expo in Paris, France),[60] EuroCosplay (finals taking place at London MCM Expo),[61] and the Nordic Cosplay Championship (finals taking place at NärCon in Linköping, Sweden).[62]		This table contains a list of the most common cosplay competition judging criteria, as seen from World Cosplay Summit,[63] Cyprus Comic Con,[64] and ReplayFX.[65]		Portraying a character of the opposite sex is called crossplay. The practicality of crossplay and cross-dress stems in part from the abundance in manga of male characters with delicate and somewhat androgynous features. Such characters, known as bishōnen (lit. "pretty boy"),[66] are Asian equivalent of the elfin boy archetype represented in Western tradition by figures such as Peter Pan and Ariel.[67]		Male to female cosplayers may experience issues when trying to portray a female character because it is hard to maintain the sexualized femininity of a character. Often interpretations can be misconstrued as parody, or men can be asked to change their outfits because of their scantily-clad nature.[68] Male cosplayers may also be subjected to discrimination,[69] including homophobic comments and being touched without permission, possibly even more often than female ones when it is already a problem for women cosplayers,[70] as is "slut-shaming".[71]		Animegao players, a niche group in the realm of cosplay, are often male cosplayers who use zentai and stylized masks to represent female anime characters. These cosplayers completely hide their real features so the original appearance of their characters may be reproduced as literally as possible, and to display all the abstractions and stylizations such as oversized eyes and tiny mouths often seen in Japanese cartoon art.[72] This does not mean that only males perform animegao or that masks are only female.		Cosplay has influenced the advertising industry, in which cosplayers are often used for event work previously assigned to agency models.[58] Some cosplayers have thus transformed their hobby into profitable, professional careers.[73][74] Japan's entertainment industry has been home to the professional cosplayers since the rise of Comiket and Tokyo Game Show.[58] The phenomenon is most apparent in Japan but exists to some degree in other countries as well. Professional cosplayers who profit from their art may experience problems related to copyright infringement.[75]		A cosplay model, also known as a cosplay idol, cosplays costumes for anime and manga or video game companies. Good cosplayers are viewed as fictional characters in the flesh, in much the same way that film actors come to be identified in the public mind with specific roles. Cosplayers have modeled for print magazines like Cosmode and a successful cosplay model can become the brand ambassador for companies like Cospa. Some cosplay models can achieve significant recognition. Yaya Han, for example, was described as having emerged "as a well-recognized figure both within and outside cosplay circuits".[73]		Cosplayers in Japan used to refer to themselves as reiyā (レイヤー), pronounced "layer". Currently in Japan, cosplayers are more commonly called kosupure (コスプレ), pronounced "ko-su-pray," as reiyā is more often used to describe layers (i.e. hair, clothes, etc.).[76] Those who photograph players are called cameko, short for camera kozō or camera boy. Originally, the cameko gave prints of their photos to players as gifts. Increased interest in cosplay events, both on the part of photographers and cosplayers willing to model for them, has led to formalization of procedures at events such as Comiket. Photography takes place within a designated area removed from the exhibit hall.		Since 1998, Tokyo's Akihabara district contains a number of cosplay restaurants, catering to devoted anime and cosplay fans, where the waitresses at such cafés dress as video game or anime characters; maid cafés are particularly popular. In Japan, Tokyo's Harajuku district is the favourite informal gathering place to engage in cosplay in public. Events in Akihabara also draw many cosplayers.		In Japan, costumes are not welcome outside of conventions or other designated areas.[6][7]		Cosplay is common in many East Asian countries. For example, it is a major part of the Comic World conventions taking place regularly in South Korea, Hong Kong and Taiwan.[77] Historically, the practice of dressing up as characters from works of fiction can be traced as far as the 17th century late Ming Dynasty China.[78]		Western cosplay's origins are based primarily in science fiction and fantasy fandoms. It is also more common for Western cosplayers to recreate characters from live-action series than it is for Japanese cosplayers. Western costumers also include subcultures of hobbyists who participate in Renaissance faires, live action role-playing games, and historical reenactments. Competition at science fiction conventions typically include the masquerade (where costumes are presented on stage and judged formally) and hall costumes[79] (where roving judges may give out awards for outstanding workmanship or presentation).[80]		The increasing popularity of Japanese animation outside of Asia during the late 2000s led to an increase in American and other Western cosplayers who portray manga and anime characters. Anime conventions have become more numerous in the West in the previous decade, now competing with science fiction, comic book and historical conferences in attendance. At these gatherings, cosplayers, like their Japanese counterparts, meet to show off their work, be photographed, and compete in costume contests. Convention attendees also just as often dress up as Western comic book or animated characters, or as characters from movies and video games.		Differences in taste still exist across cultures: some costumes that are worn without hesitation by Japanese cosplayers tend to be avoided by Western cosplayers, such as outfits that evoke Nazi uniforms. Some Western cosplayers have also encountered questions of legitimacy when playing characters of canonically different racial backgrounds,[81][82] and people can be insensitive to cosplayers playing as characters who are canonically of other skin color.[83][84] Western cosplayers of anime characters may also be subjected to particular mockery.[85]		In contrast to Japan, the wearing of costumes in public is more accepted in the United States and other western countries. These countries have a longer tradition of Halloween costumes, fan costuming and other such activities. As a result, for example, costumed convention attendees can often be seen at local restaurants and eateries, beyond the boundaries of the convention or event.[6][7]		Japan is home to two especially popular cosplay magazines, Cosmode (コスモード) and ASCII Media Works' Dengeki Layers (電撃Layers).[86] Cosmode has the largest share in the market and an English-language digital edition.[87] Another magazine, aimed at a broader, worldwide audience is CosplayGen.[88] In the United States, Cosplay Culture began publication in February 2015.[89] Other magazines include CosplayZine featuring cosplayers from all over the world since October 2015.[90] There are many books on the subject of cosplay as well.[91][92]		The practice of cosplaying is featured in many Japanese video game, manga and anime titles, including Ai Kora, Amagami, Aoi House, The Cosmopolitan Prayers, Dōbutsu no Mori, Fate/hollow ataraxia, Galaxy Angel, Genshiken, Girl Friends, Gunbuster, Hanaukyo Maid Team, Hyperdimension Neptunia, I, Otaku: Struggle in Akihabara, Jewelpet, K-On!, Kujibiki Unbalance, Lucky Star, Maid Sama!, Nogizaka Haruka no Himitsu, Oreimo, Phantom Breaker, Popotan, Re: Cutie Honey, School Rumble, and Unofficial Sentai Akibaranger.		
In entertainment, a tagline (alternatively spelled tag line[1][2]) is a short text which serves to clarify a thought for, or is designed with a form of, dramatic effect. Many tagline slogans are reiterated phrases associated with an individual, social group, or product. As a variant of a branding slogan, taglines can be used in marketing materials and advertising.		The idea behind the concept is to create a memorable dramatic phrase that will sum up the tone and premise of an audio/visual product,[a] or to reinforce and strengthen the audience's memory of a literary product. Some taglines are successful enough to warrant inclusion in popular culture. Consulting companies which specialize in creating taglines may be hired to create a tagline for a brand or product.						Tagline, tag line, and tag are American terms. In the U.K. they are called end lines, endlines, or straplines. In Belgium they are called baselines. In France they are signatures. In Germany they are claims. In the Netherlands and Italy, they are pay offs or pay-off.[3]		Referral networking organizations may encourage taglines to be used as the conclusion to an introduction by each attendee. The purpose would be to make the introduction and that speaker more memorable in the minds of the other attendees after the meeting is over. Other terms for taglines are "memory hooks" (used by BNI®) and "USP" or "Unique Selling Proposition" which is a more commonly known term.[4]		The tagline is sometimes confused with a headline because information is only presented with the one or the other. Essentially the headline is linked to the information; Once the information changes, the headline is abandoned in favor of a new one. The tagline is related to the entertainment piece and can, therefore, appear on all the information of that product or manufacturer. It is linked to the piece and not to the concept of a specific event. If the sentence is presented next to a logo, as an integral part, it is likely to be a tagline.		A tagline is sometimes used as a supplementary expression in promoting a motion picture or television program.[b] It is an explanatory subtitle, in addition to the actual title, on posters or the CD/DVD packaging of videos and music. Taglines can have an enticing effect and are therefore an important aspect in the marketing of films and television programs. Increasingly also found in the advertising world, taglines are a form of advertising slogan.[c] A tagline for the movie series Star Wars, for example:		Websites also often have taglines. The Usenet use taglines as short description of a newsgroup. The term is used in computing to represent aphorisms, maxims, graffiti or other slogans.		In electronic texts, a tag or tagline is short, concise sentences in a row that are used when sending e-mail instead of a electronic signature. The tagline is used in computing with the meaning of a "signature" to be affixed at the end of each message. In the late eighties and early nineties, when the amateur computer network FidoNet began to flourish, the messages that were exchanged between users often had a tag-line, which was no longer than 79 characters, containing a brief phrase (often witty or humorous).		
Michael Anthony Hall[1] (born April 14, 1968),[1] known professionally as Anthony Michael Hall, is an American actor, film producer, and director who starred in several teen-oriented films of the 1980s. Hall began his career in commercials and on stage as a child, and made his screen debut in 1980. His films with director-screenwriter John Hughes, beginning with the popular 1983 comedy National Lampoon's Vacation and the coming-of-age comedy Sixteen Candles, shaped his early career. Hall's next movies with Hughes were the teen classics The Breakfast Club and Weird Science, both in 1985.		Hall diversified his roles to avoid becoming typecast as his geek persona, joining the cast of Saturday Night Live (1985–1986) and starring in films such as Out of Bounds (1986), Johnny Be Good (1988), Edward Scissorhands (1990) and Six Degrees of Separation (1993). After a series of minor roles in the 1990s, he starred as Microsoft's Bill Gates in the 1999 television film Pirates of Silicon Valley. He had the leading role in the USA Network series The Dead Zone from 2002 to 2007. The show was one of the highest-rated cable television series during its run.[2]						Hall was born in West Roxbury, a neighborhood in Boston, Massachusetts.[1] He is the only child of blues-jazz singer Mercedes Hall's first marriage. She divorced Hall's father, Larry, an auto-body-shop owner,[3] when their son was six months old.[4] When Hall was three, he and his mother relocated to the West Coast, where she found work as a featured singer.[5] After a year and a half, they returned to the East, eventually moving to New York City, where Hall grew up.[4][5] Hall's ancestry is Irish and Italian.[6] He has one half-sister, Mary Chestaro, from his mother's second marriage to Thomas Chestaro, a show business manager. His half-sister is pursuing a career as a singer under the name of Mary C.[5] Hall uses the name Anthony, rather than Michael. He transposed his first and middle names when he entered show business because there was another actor named Michael Hall who was already a member of the Screen Actors Guild.[7]		Hall attended St. Hilda's & St. Hugh's School of New York before moving on to Manhattan's Professional Children's School. Hall began his acting career at age eight and continued throughout high school. "I did not go to college," he has said, "but I'm an avid reader in the ongoing process of educating myself."[8] Through the 1980s, Hall's mother managed his career, eventually relinquishing that role to her second husband.[5]		Hall started his career in commercials when he was seven years old.[4] He was the Honeycomb cereal kid and appeared in several commercials for toys and Bounty.[9] His stage debut was in 1977, when he was cast as the young Steve Allen in Allen's semi-autobiographical play The Wake. He went on to appear in the Lincoln Center Festival's production of St. Joan of the Microphone, and in a play with Woody Allen.[9] In 1980, he made his screen debut in the Emmy-winning TV movie The Gold Bug, in which he played the young Edgar Allan Poe, but it was not until the release of the 1982 Kenny Rogers film Six Pack that he gained real notice.		The following year, Hall landed the role of Rusty Griswold, Chevy Chase and Beverly D'Angelo's son, in National Lampoon's Vacation, catching the attention of the film's screenwriter John Hughes, who was about to make the jump to directing. "For [Hall] to upstage Chevy, I thought, was a remarkable accomplishment for a 13-year-old kid," said Hughes.[4] The film was a significant box office hit in 1983, grossing over US$61 million in the United States.[10] After Vacation, Hall moved on to other projects and declined to reprise his role in the 1985 sequel.[11]		Hall's breakout role came in 1984, when he was cast as Farmer Ted, the scrawny, braces-wearing geek, who pursued Molly Ringwald's character in John Hughes' directing debut Sixteen Candles. Hall tried to avoid the clichés of geekness. "I didn't play him with 100 pens sticking out of his pocket," he said. "I just went in there and played it like a real kid. The geek is just a typical freshman."[12] Hall landed a spot on the promotional materials, along with co-star Ringwald. Reviews of the film were positive for Hall and his co-stars, and one for People Weekly even claimed that Hall's performance "[pilfered] the film" from Ringwald.[13] Despite achieving only moderate[citation needed] success at the box office, the film made overnight stars of Ringwald and Hall.		In 1985, Hall starred in two teen-oriented films written and directed by John Hughes. He was cast as Brian Johnson, "the brain," in The Breakfast Club, co-starring Emilio Estevez, Judd Nelson, Ally Sheedy, and Molly Ringwald. Film critic Janet Maslin praised Hall, stating that the 16-year-old actor and Ringwald were "the movie's standout performers."[14] Hall and fellow costar Molly Ringwald dated for a short period of time after filming The Breakfast Club together in 1985. Later that year, Hall portrayed Gary Wallace, another likable misfit, in Weird Science. Critic Sheila Benson from the Los Angeles Times said "Hall [was] the role model supreme" for the character, but she also acknowledged that "he [was] outgrowing the role" and "[didn’t] need to hold the patent on the bratty bright kid."[15] Weird Science was a moderate success at the box office but was generally well-received for a teen comedy.[16] Those roles established him as the 80s "nerd-of-choice," as well as a member in good standing of Hollywood's Brat Pack. Hall, who portrayed John Hughes' alter egos in Sixteen Candles, The Breakfast Club and Weird Science,[17] credits the director for putting him on the map and giving him those opportunities as a child. "I had the time of my life," he said. "I'd consider [working with Hughes again] any day of the week."[18]		Hall joined the cast of Saturday Night Live (SNL) during its 1985–86 season at the age of 17. He was, and remains, the youngest cast member in the show's history.[19] His recurring characters on the show were Craig Sundberg, Idiot Savant, an intelligent, talented teenager with a vacant expression and stilted speech, and Fed Jones, half of the habitually high, hustling pitchmen known as The Jones Brothers. (The other Jones Brother was played by short-lived featured player Damon Wayans). Art Garfunkel, Edd Byrnes, Robert F. Kennedy, and Daryl Hall were among Hall's celebrity impersonations. Hall had admired the show and its stars as a child, but he found the SNL environment to be far more competitive than he had imagined. "My year there, I didn't have any breakout characters and I didn't really do the things I dreamed I would do," he said, "but I still learned a lot, and I value that.[20] I'll always be proud of the fact that I was a part of its history."[21] Hall was one of six cast members (the others being Joan Cusack, Robert Downey, Jr., Randy Quaid, and Terry Sweeney) who were dismissed at the end of that season.		To avoid being typecast, Hall turned down roles written for him by John Hughes in Ferris Bueller's Day Off (Cameron Frye) and Pretty in Pink (Phil "Duckie" Dale), both in 1986.[9][22] Instead, he starred in the 1986 film Out of Bounds, Hall's first excursion into the thriller and action genre. The film grossed only US$5 million domestically, and was a critical and financial disappointment.[23] Critic Roger Ebert described Out of Bounds as "an explosion at the cliché factory,"[24] and Caryn James from the New York Times claimed that not even "Hall, who made nerds seem lovable in John Hughes' Sixteen Candles and The Breakfast Club, [could] do much to reconcile" the disparate themes of the movie.[25]		Hall was offered the starring role in the 1987 film Full Metal Jacket in a conversation with Stanley Kubrick, but after an eight-month negotiation, a financial agreement could not be reached.[20] "It was a difficult decision, because in that eight-month period, I read everything I could about the guy, and I was really fascinated by him," Hall said when asked about the film. "I wanted to be a part of that film, but it didn't work out. But all sorts of stories circulated, like I got on set and I was fired, or I was pissed at him for shooting too long. It's all not true."[20] He was replaced with Matthew Modine. His next film would be 1988's Johnny Be Good, in which he worked with Uma Thurman and fellow Saturday Night Live cast member Robert Downey, Jr. The film was a critical failure, and some critics panned Hall's performance as a high school football star, claiming that he, the movies' reigning geek, was miscast for the role. A review for The Washington Post claimed that the film was "crass, vulgar, and relentlessly brain-dead."[26]		After a two-year hiatus due to a drinking problem,[3] Hall returned to acting by starring opposite Johnny Depp and Winona Ryder in Tim Burton's 1990 hit Edward Scissorhands, this time as the film's villain. By then in his 20s, he shifted to more mature roles, trying to establish himself as an adult actor. After Scissorhands, he appeared in a series of low-budget films, including the 1992 comedy Into the Sun, where he starred as a visiting celebrity at a military air base. Film critic Janet Maslin praised his performance, writing that "Mr. Hall, whose earlier performances (in films like National Lampoon's Vacation and Sixteen Candles) have been much goofier, remains coolly funny and graduates to subtler forms of comedy with this role."[27] The following year, he played a gay man who teaches down-and-out Will Smith to dupe rich people in the critically acclaimed film Six Degrees of Separation. Hall claimed that it was "the hardest role [he] ever had."[9]		In 1994, Hall starred in and directed his first feature film, a low-budget Showtime comedy named Hail Caesar about a would-be rock star who works in a pencil eraser factory. The film also co-starred Samuel L. Jackson, Robert Downey, Jr., and Judd Nelson. In addition, he produced the soundtrack for the film with composer Herbie Tribino. The film featured songs written and performed by Hall.		After a series of appearances in low-budget films and guest roles on TV series in the mid and late 1990s, he gained media attention once again in the 1999 Emmy-nominated TNT original movie Pirates of Silicon Valley, co-starring Noah Wyle as Apple Computer's Steve Jobs. Hall was widely praised for his portrayal of Microsoft billionaire Bill Gates. "I really fought for this part because I knew it would be the role of a lifetime," Hall said. "It was a thrill and a daunting challenge to play someone of his stature and brilliance."[28] Hall described his physical appearance as 20-year-old Gates to the San Francisco Chronicle:		"First, you have to lose the neck." The top six inches of his spine seem to disappear. "You go down, down. You lose the body; you get softer shoulders, you slump, you create a little gut." He is almost there. "Then you extend the neck and you do a little duck walk." He walks across the room. Add ill-fitting clothes, mop-top hair, a pair of oversize glasses and a cold stare, and the impersonation is complete.[29]		After making a cameo appearance as himself in the 2000 comedy film Happy Accidents, Hall appeared in several made-for-TV films. He starred opposite Sheryl Lee as a cheating husband in the 2001 USA Network cable movie Hitched. In the same year, he played renowned music producer Robert "Mutt" Lange in VH1's movie Hysteria: The Def Leppard Story and starred as legendary lefty baseball pitcher Whitey Ford in Billy Crystal's highly acclaimed HBO film, 61*.		On the big screen, Hall took on supporting roles in the mystery-drama The Caveman's Valentine (2001) opposite Samuel L. Jackson, the critically panned Freddy Got Fingered (2001) opposite Tom Green, and the action-comedy All About the Benjamins (2002) opposite Ice Cube.		Hall began his first regular series role in 2002, starring as Johnny Smith in USA Network's supernatural drama The Dead Zone, a TV series adapted from Stephen King's best-selling novel. He was cast in the show after executive producer Michael Piller saw his performance in Pirates of Silicon Valley.[20] The show debuted on June 16, 2002, and drew higher ratings for a premiere than any other cable series in television history[30] with 6.4 million viewers.[31] The Dead Zone quickly developed a loyal audience, with the show and Hall receiving strong reviews. The Pittsburgh Tribune-Review wrote that "Hall's Johnny flashes the qualities - comic timing, great facial expressions - that made him a star in the 1980s movies Sixteen Candles and The Breakfast Club."[32] The Dead Zone, Hall said, "has transformed my career."[33] The show proved to be one of USA Network's top shows and one of the highest-rated programs on basic cable.[2]		The Dead Zone opening credits list Hall as co-producer (seasons 1-3), producer (seasons 5) and co-executive producer (season 6).[34] Hall also directed an episode from season three, "The Cold Hard Truth," guest starring standup comic Richard Lewis. "[The Cold Hard Truth], I feel, is my best work as a director, because I had this great crew that knows me well and has been working with me," said Hall. "I also had the best script that I've had an opportunity to direct."[35] The show's sixth and final season premiered on June 17, 2007.[36] USA Network officially canceled The Dead Zone in December 2007.[31]		Hall appeared in the tenth episode of in Criss Angel Mindfreak's fourth season.		Hall develops film and television projects under his production company banner AMH Entertainment.[37] Hall starred in Aftermath, a 2010 independent crime-drama film, with Tony Danza and Frank Whaley. In 2008, Hall appeared as Gotham City television reporter/anchor Mike Engel in The Dark Knight.[38]		In 2010, Hall made a guest appearance in NBC season one of Community as a former nerd turned bully.[39] During 2011, he played the main antagonist in Season 3 of Warehouse 13, Walter Sykes.		Hall reprised his role as Rusty Griswold in 2012 in a series of Old Navy holiday commercials featuring the Griswold family. From 2011-12, he guest starred in Warehouse 13 in the role of Walter Sykes, a man who once benefited from the use of an artifact but harboured a deep-seated anger towards the Warehouse and its agents when the artifact was taken from him (episodes 3.09, 3.11, 3.12).[citation needed] He also guest starred in Z Nation in the role of Gideon, a former communications manager leading a group of zombie apocalypse survivors (only episode 2.11).		In 2016, Hall plays himself as the customer being serviced in an AT&T Mobile commercial. The same year, Hall was cast in a recurring role on the TNT drama series Murder in the First.[40]		Hall assists at-risk youth through his literacy program, The Anthony Michael Hall Literacy Club, in association with Chapman University.[37] The club provides an opportunity for the students to improve their literacy skills by exploring genres not typically used to enhance literacy, such as films, music and lyrics, scripts, and novels with audio. Following family tradition, Hall pursues his other passion, music. He is the lead singer and songwriter for his band, Hall of Mirrors, formed in 1998. The band released an album, Welcome to the Hall of Mirrors, through Hall's own RAM Records label in 1999, with collaborations from former Guns N' Roses guitarist Gilby Clarke and Prince's former keyboard player Tommy Barbarella.[21]		Hall became a regular subject of tabloid media after New York magazine named him a member of the "Brat Pack," the group of young actors who became famous in the 1980s and frequently starred together.[41] In the late '80s, Hall's drinking problem, which began in his early teens, made headlines.[3] Hall eventually quit drinking and became fully sober by 1990. "The truth is, I had my partying nights, but I never really bounced at the bottom," he said. "I never went to rehab...I was able to govern myself and continue my work."[20]		In 1990, Hall's physical appearance in Edward Scissorhands caught audiences off guard. His more muscular image provoked rumors of steroids, but Hall later said that "the weight gain was natural."[42]		Hall's role in the 1993 film Six Degrees of Separation made news not because of what occurred onscreen, but rather what failed to occur. Hall played a gay love interest to Will Smith, who had previously agreed to a kissing scene between the two. However, on the day of the shoot, Smith backed off. Smith told the press that he called Denzel Washington for advice,[20] who told him that an onscreen same-sex kiss was a bad career move.[43] When asked about the incident during an interview, Hall said, "I didn't care. I wasn't that comfortable with it, either, and ultimately, we used a camera trick."[44]		The 2001 film Not Another Teen Movie pays tribute to Hall's numerous appearances in the teen-oriented, 1980s comedy films parodied by the movie. A brief shot of the sign over the door of a high school cafeteria reveals that the facility is named the Anthony Michael Dining Hall.[45] In 2006, Hall was ranked #4 in VH1's list of the "100 Greatest Teen Stars"[46] and #41 in "100 Greatest Kid Stars."[47]		In June 2005, The Breakfast Club was rewarded with the Silver Bucket of Excellence Award at the MTV Movie Awards, in honor of the film's twentieth anniversary. For the show, MTV attempted to reunite the original cast. Sheedy, Ringwald, and Hall appeared together on stage, and Paul Gleason gave the award to his former castmates. Estevez could not attend because of family commitments,[48] and Nelson appeared earlier on the red carpet,[49] but left before the on-stage reunion, for reasons unknown. Hall joked that the two were "in Africa with Dave Chappelle."[50]		Hall has been involved in multiple disputes with neighbors. According to New York's Daily News, in 2011 he was accused of frightening a number of neighbors in his Playa del Rey, California by shouting obscenities and challenging them to fights. In mid-September 2016, after a neighbor left his condominium gate open, Hall yelled him to close it, eventually going over to his neighbor's gate to close it himself, leading to a physical altercation caught on grainy video that shows Hall pushing his neighbor to the ground. On November 17 Hall was charged with felony assault with serious bodily injury by the Los Angeles District Attorney.[51][52]		Hall lives in Playa del Rey, California.[51]		Hall is godfather to Robert Downey, Jr.'s son Indio Falconer Downey.[53]		
Coordinates: 40°N 4°W﻿ / ﻿40°N 4°W﻿ / 40; -4		– in Europe  (green & dark grey) – in the European Union  (green)		Spain (Spanish: España [esˈpaɲa] ( listen)), officially the Kingdom of Spain (Spanish: Reino de España),[a][b] is a sovereign state located on the Iberian Peninsula in southwestern Europe, with two large archipelagoes, the Balearic Islands in the Mediterranean Sea and the Canary Islands off the North African Atlantic coast, two cities, Ceuta and Melilla, in the North African mainland and several small islands in the Alboran Sea near the Moroccan coast. The country's mainland is bordered to the south and east by the Mediterranean Sea except for a small land boundary with Gibraltar; to the north and northeast by France, Andorra, and the Bay of Biscay; and to the west and northwest by Portugal and the Atlantic Ocean. It is the only European country to have a border with an African country (Morocco)[h] and its African territory accounts for nearly 5% of its population, mostly in the Canary Islands but also in Ceuta and Melilla.		With an area of 505,990 km2 (195,360 sq mi), Spain is the largest country in Southern Europe, the second largest country in Western Europe and the European Union, and the fourth largest country in the European continent. By population, Spain is the sixth largest in Europe and the fifth in the European Union. Spain's capital and largest city is Madrid; other major urban areas include Barcelona, Valencia, Seville, Bilbao and Málaga.		Modern humans first arrived in the Iberian Peninsula around 35,000 years ago. Iberian cultures along with ancient Phoenician, Greek and Carthaginian settlements developed on the peninsula until it came under Roman rule around 200 BCE, after which the region was named Hispania, based on the earlier Phoenician name Span or Spania.[10] In the Middle Ages, the area was conquered by Germanic tribes and later by the Moors. Spain emerged as a unified country in the 15th century, following the marriage of the Catholic Monarchs and the completion of the eight centuries-long reconquest, or Reconquista from the Moors in 1492. In the early modern period, Spain became one of history's first global colonial empires, leaving a vast cultural and linguistic legacy that includes over 500 million Spanish speakers, making Spanish the world's second most spoken first language, after Mandarin Chinese.		Spain is a parliamentary democracy and constitutional monarchy. The current Spanish king is Felipe VI. It is a middle power and a major developed country[11] with the world's fourteenth largest economy by nominal GDP and sixteenth largest by purchasing power parity. It is a member of the United Nations (UN), the European Union (EU), the Eurozone, the Council of Europe (CoE), the Organization of Ibero-American States (OEI), the North Atlantic Treaty Organization (NATO), the Organisation for Economic Co-operation and Development (OECD), the World Trade Organization (WTO) and many other international organisations. Spain has a "permanent invitation" to the G20 summits that occur generally once a year.						The origins of the Roman name Hispania, from which the modern name España was derived, are uncertain due to inadequate evidence, although it is documented that the Phoenicians and Carthaginians referred to the region as Spania, therefore the most widely accepted etymology is a Semitic-Phoenician one.[10][12] Down the centuries there have been a number of accounts and hypotheses:		The Renaissance scholar Antonio de Nebrija proposed that the word Hispania evolved from the Iberian word Hispalis, meaning "city of the western world".		Jesús Luis Cunchillos argues that the root of the term span is the Phoenician word spy, meaning "to forge metals". Therefore, i-spn-ya would mean "the land where metals are forged".[13] It may be a derivation of the Phoenician I-Shpania, meaning "island of rabbits", "land of rabbits" or "edge", a reference to Spain's location at the end of the Mediterranean; Roman coins struck in the region from the reign of Hadrian show a female figure with a rabbit at her feet,[14] and Strabo called it the "land of the rabbits".[15]		Hispania may derive from the poetic use of the term Hesperia, reflecting the Greek perception of Italy as a "western land" or "land of the setting sun" (Hesperia, Ἑσπερία in Greek) and Spain, being still further west, as Hesperia ultima.[16]		There is the claim that "Hispania" derives from the Basque word Ezpanna meaning "edge" or "border", another reference to the fact that the Iberian Peninsula constitutes the southwest corner of the European continent.[16]		Two 15th-century Spanish Jewish scholars, Don Isaac Abravanel and Solomon ibn Verga, gave an explanation now considered folkloric. Both men wrote in two different published works that the first Jews to reach Spain were brought by ship by Phiros who was confederate with the king of Babylon when he laid siege to Jerusalem. Phiros was a Grecian by birth, but who had been given a kingdom in Spain. Phiros became related by marriage to Espan, the nephew of king Heracles, who also ruled over a kingdom in Spain. Heracles later renounced his throne in preference for his native Greece, leaving his kingdom to his nephew, Espan, from whom the country of España (Spain) took its name. Based upon their testimonies, this eponym would have already been in use in Spain by c. 350 BCE.[17]		Iberia enters written records as a land populated largely by the Iberians, Basques and Celts. Early on its coastal areas were settled by Phoenicians who founded Western Europe's most ancient cities Cadiz and Malaga. Phoenician influence expanded as much of the Peninsula was eventually incorporated into the Carthaginian Empire, becoming a major theater of the Punic Wars against the expanding Roman Empire. After an arduous conquest, the peninsula came fully under Roman Rule. During the early Middle Ages it came under Germanic rule but later, much of it was conquered by Moorish invaders from North Africa. In a process that took centuries, the small Christian kingdoms in the north gradually regained control of the peninsula. The last Moorish kingdom fell in the same year Columbus reached the Americas. A global empire began which saw Spain become the strongest kingdom in Europe, the leading world power for a century and a half, and the largest overseas empire for three centuries.		Continued wars and other problems eventually led to a diminished status. The Napoleonic invasions of Spain led to chaos, triggering independence movements that tore apart most of the empire and left the country politically unstable. Prior to the Second World War, Spain suffered a devastating civil war and came under the rule of an authoritarian government, which oversaw a period of stagnation that was followed by a surge in the growth of the economy. Eventually democracy was peacefully restored in the form of a parliamentary constitutional monarchy. Spain joined the European Union, experiencing a cultural renaissance and steady economic growth until the beginning of the 21st century, that started a new globalized world with economic and ecological challenges.		Archaeological research at Atapuerca indicates the Iberian Peninsula was populated by hominids 1.2 million years ago.[19] In Atapuerca fossils have been found of the earliest known hominins in Europe, the Homo antecessor. Modern humans first arrived in Iberia, from the north on foot, about 35,000 years ago.[20][not in citation given] The best known artefacts of these prehistoric human settlements are the famous paintings in the Altamira cave of Cantabria in northern Iberia, which were created from 35,600 to 13,500 BCE by Cro-Magnon.[18][21] Archaeological and genetic evidence suggests that the Iberian Peninsula acted as one of several major refugia from which northern Europe was repopulated following the end of the last ice age.		The largest groups inhabiting the Iberian Peninsula before the Roman conquest were the Iberians and the Celts. The Iberians inhabited the Mediterranean side of the peninsula, from the northeast to the southeast. The Celts inhabited much of the inner and Atlantic sides of the peninsula, from the northwest to the southwest. Basques occupied the western area of the Pyrenees mountain range and adjacent areas, the Phoenician-influenced Tartessians culture flourished in the southwest and the Lusitanians and Vettones occupied areas in the central west. A number of cities were founded along the coast by Phoenicians, and trading outposts were established by Greeks in the North East. Eventually, Phoenician-Carthaginians expanded inland conquering about over half of modern-day Spain.		During the Second Punic War, roughly between 210 and 205 BC the expanding Roman Republic captured Carthaginian trading colonies along the Mediterranean coast. Although it took the Romans nearly two centuries to complete the conquest of the Iberian Peninsula, they retained control of it for over six centuries. Roman rule was bound together by law, language, and the Roman road.[22]		The cultures of the Celtic and Iberian populations were gradually Romanised (Latinised) at different rates depending on what part of Hispania they lived in, with local leaders being admitted into the Roman aristocratic class.[i][23] Hispania served as a granary for the Roman market, and its harbors exported gold, wool, olive oil, and wine. Agricultural production increased with the introduction of irrigation projects, some of which remain in use. Emperors Hadrian, Trajan, Theodosius I, and the philosopher Seneca were born in Hispania.[j] Christianity was introduced into Hispania in the 1st century AD and it became popular in the cities in the 2nd century AD.[23] Most of Spain's present languages and religion, and the basis of its laws, originate from this period.[22]		The weakening of the Western Roman Empire's jurisdiction in Hispania began in 409, when the Germanic Suebi and Vandals, together with the Sarmatian Alans entered the peninsula at the invitation of a Roman usurper. These tribes who had crossed the Rhinein early 407 and ravaged Gaul. The Suebi established a kingdom in what is today modern Galicia and northern Portugal whereas the Vandals established themselves in southern Spain by 420 before crossing over to North Africa in 429 and taking Carthage in 439. As the western empire disintegrated, the social and economic base became greatly simplified: but even in modified form, the successor regimes maintained many of the institutions and laws of the late empire, including Christianity and assimilation to the evolving Roman culture.		The Byzantines established an occidental province, Spania, in the south, with the intention of reviving Roman rule throughout Iberia. Eventually, however, Hispania was reunited under Visigothic rule.		Isidore of Seville, born in Murcia, Archbishop of Seville, was an influential cleric and philosopher and was much studied in the Middle Ages in Europe. His theories were also vital to the conversion of the Visigothic Kingdom from an Arian domain to a Catholic one in the Councils of Toledo. This Gothic kingdom was the first independent Christian kingdom ruling in the Iberian Peninsula, and in the Reconquista it was the referent for the different kingdoms fighting against the Muslim rule.		In the 8th century, nearly all of the Iberian Peninsula was conquered (711–718) by largely Moorish Muslim armies from North Africa. These conquests were part of the expansion of the Umayyad Caliphate. Only a small area in the mountainous north-west of the peninsula managed to resist the initial invasion.		Under Islamic law, Christians and Jews were given the subordinate status of dhimmi. This status permitted Christians and Jews to practice their religions as People of the Book but they were required to pay a special tax and had legal and social rights inferior to those of Muslims.[24][25]		Conversion to Islam proceeded at an increasing pace. The muladíes (Muslims of ethnic Iberian origin) are believed to have comprised the majority of the population of Al-Andalus by the end of the 10th century.[26][27]		The Muslim community in the Iberian Peninsula was itself diverse and beset by social tensions. The Berber people of North Africa, who had provided the bulk of the invading armies, clashed with the Arab leadership from the Middle East.[k] Over time, large Moorish populations became established, especially in the Guadalquivir River valley, the coastal plain of Valencia, the Ebro River valley and (towards the end of this period) in the mountainous region of Granada.[27]		Córdoba, the capital of the caliphate since Abd-ar-Rahman III, was the largest, richest and most sophisticated city in western Europe. Mediterranean trade and cultural exchange flourished. Muslims imported a rich intellectual tradition from the Middle East and North Africa. Muslim and Jewish scholars played an important part in reviving and expanding classical Greek learning in Western Europe. Some important philosophers at the time were Averroes, Ibn Arabi and Maimonides. The Romanised cultures of the Iberian Peninsula interacted with Muslim and Jewish cultures in complex ways, giving the region a distinctive culture.[27] Outside the cities, where the vast majority lived, the land ownership system from Roman times remained largely intact as Muslim leaders rarely dispossessed landowners and the introduction of new crops and techniques led to an expansion of agriculture.		In the 11th century, the Muslim holdings fractured into rival Taifa kingdoms, allowing the small Christian states the opportunity to greatly enlarge their territories.[27] The arrival from North Africa of the Islamic ruling sects of the Almoravids and the Almohads restored unity upon the Muslim holdings, with a stricter, less tolerant application of Islam, and saw a revival in Muslim fortunes. This re-united Islamic state experienced more than a century of successes that partially reversed Christian gains.		The Reconquista (Reconquest) was the centuries-long period in which Christian rule was re-established over the Iberian Peninsula. The Reconquista is viewed as beginning with the Battle of Covadonga won by Don Pelayo in 722 and was concurrent with the period of Muslim rule on the Iberian Peninsula. The Christian army's victory over Muslim forces led to the creation of the Christian Kingdom of Asturias along the northwestern coastal mountains. Shortly after, in 739, Muslim forces were driven from Galicia, which was to eventually host one of medieval Europe's holiest sites, Santiago de Compostela and was incorporated into the new Christian kingdom. The Kingdom of León was the strongest Christian kingdom for centuries. In 1188 the first modern parliamentary session in Europe was held in León (Cortes of León). The Kingdom of Castile, formed from Leonese territory, was its successor as strongest kingdom. The kings and the nobility fought for power and influence in this period. The example of the Roman emperors influenced the political objective of the Crown, while the nobles benefited from feudalism.		Muslim armies had also moved north of the Pyrenees but they were defeated by Frankish forces at the Battle of Poitiers, Frankia and pushed out of the verz southernmost regionofFrance along the seacoast by the 760s. Later, Frankish forces established Christian counties on the southern side of the Pyrenees. These areas were to grow into the kingdoms of Navarre and Aragon.[28] For several centuries, the fluctuating frontier between the Muslim and Christian controlled areas of Iberia was along the Ebro and Douro valleys.		The County of Barcelona and the Kingdom of Aragon entered in a dynastic union and gained territory and power in the Mediterranean. In 1229 Majorca was conquered, so was Valencia in 1238.		The break-up of Al-Andalus into the competing taifa kingdoms helped the long embattled Iberian Christian kingdoms gain the initiative. The capture of the strategically central city of Toledo in 1085 marked a significant shift in the balance of power in favour of the Christian kingdoms. Following a great Muslim resurgence in the 12th century, the great Moorish strongholds in the south fell to Christian Spain in the 13th century—Córdoba in 1236 and Seville in 1248. In the 13th and 14th centuries, the Marinid dynasty of Morocco invaded and established some enclaves on the southern coast but failed in their attempt to re-establish North African rule in Iberia and were soon driven out. After 800 years of Muslim presence in Spain, the last Nasrid sultanate of Granada, a tributary state would finally surrender in 1492 to the Catholic monarchs Queen Isabella I of Castile[29] and King Ferdinand II of Aragon.[30][31][32]		From the mid 13th century, literature and philosophy started to flourish again in the Christian peninsular kingdoms, based on Roman and Gothic traditions. An important philosopher from this time is Ramon Llull. Abraham Cresques was a prominent Jewish cartographer. Roman law and its institutions were the model for the legislators. The king Alfonso X of Castile focused on strengthening this Roman and Gothic past, and also on linking the Iberian Christian kingdoms with the rest of medieval European Christendom. Alfonso worked for being elected emperor of the Holy Roman Empire and published the Siete Partidas code. The Toledo School of Translators is the name that commonly describes the group of scholars who worked together in the city of Toledo during the 12th and 13th centuries, to translate many of the philosophical and scientific works from Classical Arabic, Ancient Greek, and Ancient Hebrew. The Islamic transmission of the classics is the main Islamic contributions to Medieval Europe. The Castilian language—more commonly known (especially later in history and at present) as "Spanish" after becoming the national language and lingua franca of Spain—evolved from Vulgar Latin, as did other Romance languages of Spain like the Catalan, Asturian and Galician languages, as well as other Romance languages in Latin Europe. Basque, the only non-Romance language in Spain, continued evolving from Early Basque to Medieval. The Glosas Emilianenses founded in the monasteries of San Millán de la Cogolla contain the first written words in both Basque and Spanish, having the first become an influence in the formation of the second as an evolution of Latin.		The 13th century also witnessed the Crown of Aragon, centred in Spain's north east, expand its reach across islands in the Mediterranean, to Sicily and even Athens.[33] Around this time the universities of Palencia (1212/1263) and Salamanca (1218/1254) were established. The Black Death of 1348 and 1349 devastated Spain.[34]		In 1469, the crowns of the Christian kingdoms of Castile and Aragon were united by the marriage of Isabella I of Castile and Ferdinand II of Aragon. 1478 commenced the completion of the conquest of the Canary Islands and in 1492, the combined forces of Castile and Aragon captured the Emirate of Granada from its last ruler Muhammad XII, ending the last remnant of a 781-year presence of Islamic rule in Iberia. That same year, Spain's Jews were ordered to convert to Catholicism or face expulsion from Spanish territories during the Spanish Inquisition.[35] The Treaty of Granada guaranteed religious tolerance towards Muslims,[36] for a few years before Islam was outlawed in 1502 in the Kingdom of Castile and 1527 in the Kingdom of Aragon, leading to Spain's Muslim population becoming nominally Christian Moriscos. A few decades after the Morisco rebellion of Granada known as the War of the Alpujarras, a significant proportion of Spain's formerly-Muslim population was expelled, settling primarily in North Africa. [l][37]		The year 1492 also marked the arrival of Christopher Columbus in the New World, during a voyage funded by Isabella. Columbus's first voyage crossed the Atlantic and reached the Caribbean Islands, beginning the European exploration and conquest of the Americas, although Columbus remained convinced that he had reached the Orient. The colonisation of the Americas started, with conquistadores like Hernán Cortés and Francisco Pizarro. Miscegenation was the rule between the native and the Spanish cultures and people.		As Renaissance New Monarchs, Isabella and Ferdinand centralised royal power at the expense of local nobility, and the word España, whose root is the ancient name Hispania, began to be commonly used to designate the whole of the two kingdoms.[37] With their wide-ranging political, legal, religious and military reforms, Spain emerged as the first world power.		The unification of the crowns of Aragon and Castile by the marriage of their sovereigns laid the basis for modern Spain and the Spanish Empire, although each kingdom of Spain remained a separate country, in social, political, laws, currency and language.[38][39]		There were two big revolts against the new Habsburg monarch and the more authoritarian and imperial-style crown: Revolt of the Comuneros in Castile and Revolt of the Brotherhoods in Majorca and Valencia. After years of combat, Comuneros Juan López de Padilla, Juan Bravo and Francisco Maldonado were executed and María Pacheco went into exile. Germana de Foix also finished with the revolt in the Mediterranean.		Spain was Europe's leading power throughout the 16th century and most of the 17th century, a position reinforced by trade and wealth from colonial possessions and became the world's leading maritime power. It reached its apogee during the reigns of the first two Spanish Habsburgs—Charles I (1516–1556) and Philip II (1556–1598). This period saw the Italian Wars, the Revolt of the Comuneros, the Dutch Revolt, the Morisco Revolt, clashes with the Ottomans, the Anglo-Spanish War and wars with France.[40]		Through exploration and conquest or royal marriage alliances and inheritance, the Spanish Empire expanded to include vast areas in the Americas, islands in the Asia-Pacific area, areas of Italy, cities in Northern Africa, as well as parts of what are now France, Germany, Belgium, Luxembourg, and the Netherlands. The first circumnavigation of the world was carried out in 1519–1521. It was the first empire on which it was said that the sun never set. This was an Age of Discovery, with daring explorations by sea and by land, the opening-up of new trade routes across oceans, conquests and the beginnings of European colonialism. Spanish explorers brought back precious metals, spices, luxuries, and previously unknown plants, and played a leading part in transforming the European understanding of the globe.[41] The cultural efflorescence witnessed during this period is now referred to as the Spanish Golden Age. The expansion of the empire caused immense upheaval in the Americas as the collapse of societies and empires and new diseases from Europe devastated American indigenous populations. The rise of humanism, the Counter-Reformation and new geographical discoveries and conquests raised issues that were addressed by the intellectual movement now known as the School of Salamanca, which developed the first modern theories of what are now known as international law and human rights.		In the late 16th century and first half of the 17th century, Spain was confronted by unrelenting challenges from all sides. Barbary pirates, under the aegis of the rapidly growing Ottoman Empire, disrupted life in many coastal areas through their slave raids and the renewed threat of an Islamic invasion.[42] This was at a time when Spain was often at war with France.		The Protestant Reformation dragged the kingdom ever more deeply into the mire of religiously charged wars. The result was a country forced into ever expanding military efforts across Europe and in the Mediterranean.[43]		By the middle decades of a war- and plague-ridden 17th-century Europe, the Spanish Habsburgs had enmeshed the country in continent-wide religious-political conflicts. These conflicts drained it of resources and undermined the economy generally. Spain managed to hold on to most of the scattered Habsburg empire, and help the imperial forces of the Holy Roman Empire reverse a large part of the advances made by Protestant forces, but it was finally forced to recognise the separation of Portugal (with whom it had been united in a personal union of the crowns from 1580 to 1640) and the Netherlands, and eventually suffered some serious military reverses to France in the latter stages of the immensely destructive, Europe-wide Thirty Years' War.[44]		In the latter half of the 17th century, Spain went into a gradual decline, during which it surrendered several small territories to France and the Netherlands; however, it maintained and enlarged its vast overseas empire, which remained intact until the beginning of the 19th century.		The decline culminated in a controversy over succession to the throne which consumed the first years of the 18th century. The War of the Spanish Succession was a wide-ranging international conflict combined with a civil war, and was to cost the kingdom its European possessions and its position as one of the leading powers on the Continent.[45] During this war, a new dynasty originating in France, the Bourbons, was installed. Long united only by the Crown, a true Spanish state was established when the first Bourbon king, Philip V, united the crowns of Castile and Aragon into a single state, abolishing many of the old regional privileges and laws.[46]		The 18th century saw a gradual recovery and an increase in prosperity through much of the empire. The new Bourbon monarchy drew on the French system of modernising the administration and the economy. Enlightenment ideas began to gain ground among some of the kingdom's elite and monarchy. Military assistance for the rebellious British colonies in the American War of Independence improved the kingdom's international standing.[47]		In 1793, Spain went to war against the revolutionary new French Republic as a member of the first Coalition. The subsequent War of the Pyrenees polarised the country in a reaction against the gallicised elites and following defeat in the field, peace was made with France in 1795 at the Peace of Basel in which Spain lost control over two-thirds of the island of Hispaniola. The Prime Minister, Manuel Godoy, then ensured that Spain allied herself with France in the brief War of the Third Coalition which ended with the British victory at the Battle of Trafalgar in 1805. In 1807, a secret treaty between Napoleon and the unpopular prime minister led to a new declaration of war against Britain and Portugal. Napoleon's troops entered the country to invade Portugal but instead occupied Spain's major fortresses. The ridiculed Spanish king abdicated in favour of Napoleon's brother, Joseph Bonaparte.		Joseph Bonaparte was seen as a puppet monarch and was regarded with scorn by the Spanish. The 2 May 1808 revolt was one of many nationalist uprisings across the country against the Bonapartist regime.[48] These revolts marked the beginning of a devastating war of independence against the Napoleonic regime.[49] Napoleon was forced to intervene personally, defeating several Spanish armies and forcing a British army to retreat. However, further military action by Spanish armies, guerrillas and Wellington's British-Portuguese forces, combined with Napoleon's disastrous invasion of Russia, led to the ousting of the French imperial armies from Spain in 1814, and the return of King Ferdinand VII.[50]		During the war, in 1810, a revolutionary body, the Cortes of Cádiz, was assembled to co-ordinate the effort against the Bonapartist regime and to prepare a constitution.[51] It met as one body, and its members represented the entire Spanish empire.[52] In 1812 a constitution for universal representation under a constitutional monarchy was declared but after the fall of the Bonapartist regime Ferdinand VII dismissed the Cortes Generales and was determined to rule as an absolute monarch. These events foreshadowed the conflict between conservatives and liberals in the 19th and early 20th centuries.		Spain's conquest by France benefited Latin American anti-colonialists who resented the Imperial Spanish government's policies that favoured Spanish-born citizens (Peninsulars) over those born overseas (Criollos) and demanded retroversion of the sovereignty to the people. Starting in 1809 Spain's American colonies began a series of revolutions and declared independence, leading to the Spanish American wars of independence that ended Spanish control over its mainland colonies in the Americas. King Ferdinand VII's attempt to re-assert control proved futile as he faced opposition not only in the colonies but also in Spain and army revolts followed, led by liberal officers. By the end of 1826, the only American colonies Spain held were Cuba and Puerto Rico.		The Napoleonic War left Spain economically ruined, deeply divided and politically unstable. In the 1830s and 1840s Anti-liberal forces known as Carlists fought against liberals in the Carlist Wars. Liberal forces won, but the conflict between progressive and conservative liberals ended in a weak early constitutional period. After the Glorious Revolution of 1868 and the short-lived First Spanish Republic, a more stable monarchic period began characterised by the practice of turnismo (the rotation of government control between progressive and conservative liberals within the Spanish government).		In the late 19th century nationalist movements arose in the Philippines and Cuba. In 1895 and 1896 the Cuban War of Independence and the Philippine Revolution broke out and eventually the United States became involved. The Spanish–American War was fought in the spring of 1898 and resulted in Spain losing the last of its once vast colonial empire outside of North Africa. El Desastre (the Disaster), as the war became known in Spain, gave added impetus to the Generation of '98 who were conducting an analysis of the country.		Although the period around the turn of the century was one of increasing prosperity, the 20th century brought little peace; Spain played a minor part in the scramble for Africa, with the colonisation of Western Sahara, Spanish Morocco and Equatorial Guinea. It remained neutral during World War I (see Spain in World War I). The heavy losses suffered during the Rif War in Morocco brought discredit to the government and undermined the monarchy.		A period of authoritarian rule under General Miguel Primo de Rivera (1923–1931) ended with the establishment of the Second Spanish Republic. The Republic offered political autonomy to the linguistically distinct regions of Basque Country, Catalonia and Galicia and gave voting rights to women.		The Spanish Civil War broke out in 1936. For three years the Nationalist forces led by General Francisco Franco and supported by Nazi Germany and Fascist Italy fought the Republican side, which was supported by the Soviet Union, Mexico and International Brigades but it was not supported by the Western powers due to the British-led policy of Non-Intervention. The civil war was viciously fought and there were many atrocities committed by all sides. The war claimed the lives of over 500,000 people and caused the flight of up to a half-million citizens from the country.[53][54] In 1939, General Franco emerged victorious and became a dictator.		The state as established under Franco was nominally neutral in the Second World War, although sympathetic to the Axis. The only legal party under Franco's post civil war regime was the Falange Española Tradicionalista y de las JONS, formed in 1937; the party emphasised falangism, a form of fascism that emphasised anti-communism, nationalism and Roman Catholicism. Given Franco's opposition to competing political parties, the party was renamed the National Movement (Movimiento Nacional) in 1949.		After World War II Spain was politically and economically isolated, and was kept out of the United Nations. This changed in 1955, during the Cold War period, when it became strategically important for the US to establish a military presence on the Iberian Peninsula as a counter to any possible move by the Soviet Union into the Mediterranean basin. In the 1960s, Spain registered an unprecedented rate of economic growth which was propelled by industrialisation, a mass internal migration from rural areas to cities and the creation of a mass tourism industry. Franco's rule was also characterised by authoritarianism, promotion of a unitary national identity, the favouring of a very conservative form of Roman Catholicism known as National Catholicism, and discriminatory language policies.		In 1962 a group of politicians involved in the opposition to Franco's regime inside the country and in the exile met in the congress of the European Movement in Munich, where they made a resolution in favour of democracy.[55][56][57]		With Franco's death in November 1975, Juan Carlos succeeded to the position of King of Spain and head of state in accordance with the franquist law. With the approval of the new Spanish Constitution of 1978 and the restoration of democracy, the State devolved much authority to the regions and created an internal organisation based on autonomous communities. Spanish 1977 Amnesty Law let people of Franco´s regime continue inside institutions without consequences, even responsibles of some crimes during transition to democracy like the Massacre of 3 March 1976 in Vitoria or 1977 Massacre of Atocha.		In the Basque Country, moderate Basque nationalism has coexisted with a radical nationalist movement led by the armed terrorist organisation ETA.[58] The group was formed in 1959 during Franco's rule but has continued to wage its violent campaign even after the restoration of democracy and the return of a large measure of regional autonomy. On 23 February 1981, rebel elements among the security forces seized the Cortes in an attempt to impose a military backed government. King Juan Carlos took personal command of the military and successfully ordered the coup plotters, via national television, to surrender.		During the 1980s the democratic restoration made possible a growing open society. New cultural movements based on freedom appeared, like La Movida Madrileña and a culture of human rights arose with Gregorio Peces-Barba. On 30 May 1982 Spain joined NATO, following a referendum after a strong social opposition. That year the Spanish Socialist Workers Party (PSOE) came to power, the first left-wing government in 43 years. In 1986 Spain joined the European Economic Community, which later became the European Union. The PSOE was replaced in government by the Partido Popular (PP) in 1996 after scandals around participation of the government of Felipe González in the Dirty war against ETA; at that point the PSOE had served almost 14 consecutive years in office.		On 1 January 2002, Spain fully adopted the euro, and Spain experienced strong economic growth, well above the EU average during the early 2000s. However, well publicised concerns issued by many economic commentators at the height of the boom warned that extraordinary property prices and a high foreign trade deficit were likely to lead to a painful economic collapse.[59]		In 2002 Prestige oil spill happened with big ecological consequences in the Spanish atlantic coastline. In 2003 José María Aznar supported US president George W. Bush in its preventive war against Sadam Hussein´s Iraq. A strong movement against war rose in Spanish society. On 11 March 2004 a local Islamist terrorist group inspired by Al-Qaeda carried out the largest terrorist attack in Spanish history when they killed 191 people and wounded more than 1,800 others by bombing commuter trains in Madrid.[60] Though initial suspicions focused on the Basque terrorist group ETA, evidence soon emerged indicating Islamist involvement. Because of the proximity of the 2004 election, the issue of responsibility quickly became a political controversy, with the main competing parties PP and PSOE exchanging accusations over the handling of the incident.[61] At 14 March elections, PSOE, led by José Luis Rodríguez Zapatero won the elections.		The proportion of Spain's foreign born population increased rapidly from around 1 in 50 in 2000 to almost 1 in 8 in 2010 but has since declined. In 2005 the Spanish government legalised same sex marriage. Decentralisation was supported with much resistance of Constitutional Court and conservative opposition, so did gender politics like quotas or the law against gender violence. Government talks with ETA happened, and the band announced its permanent cease of violence in 2010.		The bursting of the Spanish property bubble in 2008 led to the 2008–16 Spanish financial crisis and high levels of unemployment, cuts in government spending and corruption in Royal family and People's Party served as a backdrop to the 2011–12 Spanish protests. Catalan independentism was also on rise. In 2011 Mariano Rajoy's conservative People's Party won elections with 44.6% of votes and Rajoy became the Spanish Prime Minister after having been the leader of the opposition from 2004 to 2011 with a program of cutting social spends. On 19 June 2014, the monarch, Juan Carlos, abdicated in favour of his son, who became Felipe VI. Bipartidism in Spanish politics got to an end with the entrance of new forces in representative institutions. In 2015, left-wing mayors got control of biggest cities in the country as former judge and former co-founder of the labour law office where the 1977 Massacre of Atocha took place Manuela Carmena in Madrid, co-founder and spokesperson of the Platform for People Affected by Mortgages Ada Colau in Barcelona, Valencia or Zaragoza, the first time that happens since the Spanish Second Republic. Even though, in general election of the same year, conservative People's Party revalidated its majority in the parliament.		In February 2016, the government of Spain announced it will rename streets named after Franco's administration officials with names of women.[62]		At 505,992 km2 (195,365 sq mi), Spain is the world's fifty-second largest country and Europe's fourth largest country. It is some 47,000 km2 (18,000 sq mi) smaller than France and 81,000 km2 (31,000 sq mi) larger than the US state of California. Mount Teide (Tenerife) is the highest mountain peak in Spain and is the third largest volcano in the world from its base. Spain is a transcontinental country.		Spain lies between latitudes 26° and 44° N, and longitudes 19° W and 5° E.		On the west, Spain is bordered by Portugal; on the south, it is bordered by Gibraltar (a British overseas territory) and Morocco, through its exclaves in North Africa (Ceuta and Melilla, and the peninsula of Vélez de la Gomera). On the northeast, along the Pyrenees mountain range, it is bordered by France and the Principality of Andorra. Along the Pyrenees in Girona, a small exclave town called Llívia is surrounded by France.		Extending to 1,214 km (754 mi), the Portugal–Spain border is the longest uninterrupted border within the European Union.[63]		Spain also includes the Balearic Islands in the Mediterranean Sea, the Canary Islands in the Atlantic Ocean and a number of uninhabited islands on the Mediterranean side of the Strait of Gibraltar, known as plazas de soberanía ("places of sovereignty", or territories under Spanish sovereignty), such as the Chafarinas Islands and Alhucemas. The peninsula of Vélez de la Gomera is also regarded as a plaza de soberanía. The isle of Alborán, located in the Mediterranean between Spain and North Africa, is also administered by Spain, specifically by the municipality of Almería, Andalusia. The little Pheasant Island in the River Bidasoa is a Spanish-French condominium.		Largest inhabited islands of Spain:		Mainland Spain is a mountainous country, dominated by high plateaus and mountain chains. After the Pyrenees, the main mountain ranges are the Cordillera Cantábrica (Cantabrian Range), Sistema Ibérico (Iberian System), Sistema Central (Central System), Montes de Toledo, Sierra Morena and the Sistema Bético (Baetic System) whose highest peak, the 3,478-metre-high (11,411-foot) Mulhacén, located in Sierra Nevada, is the highest elevation in the Iberian Peninsula. The highest point in Spain is the Teide, a 3,718-metre (12,198 ft) active volcano in the Canary Islands. The Meseta Central (often translated as "Inner Plateau") is a vast plateau in the heart of peninsular Spain.		There are several major rivers in Spain such as the Tagus (Tajo), Ebro, Guadiana, Douro (Duero), Guadalquivir, Júcar, Segura, Turia and Minho (Miño). Alluvial plains are found along the coast, the largest of which is that of the Guadalquivir in Andalusia.		Three main climatic zones can be separated, according to geographical situation and orographic conditions:[64][65][66]		Apart from these main types, other sub-types can be found, like the alpine and continental climates (Dfc, Dfb / Dsc, Dsb) in the Pyrenees as well as parts of the Cantabrian Range, the Central System, Sierra Nevada and the Iberian System, and a typical desert climate (BWk, BWh) in the zone of Almería, Murcia and eastern Canary Islands. Low-lying areas of the Canary Islands average above 18.0 °C (64.4 °F) during their coldest month, thus having a tropical climate.		The fauna presents a wide diversity that is due in large part to the geographical position of the Iberian peninsula between the Atlantic and the Mediterranean and between Africa and Eurasia, and the great diversity of habitats and biotopes, the result of a considerable variety of climates and well differentiated regions.		The vegetation of Spain is varied due to several factors including the diversity of the relief, the climate and latitude. Spain includes different phytogeographic regions, each with its own floristic characteristics resulting largely from the interaction of climate, topography, soil type and fire, biotic factors.		According to the Democracy Index of the EIU, Spain is one of the 19 full democracies in the world.		The Spanish Constitution of 1978 is the culmination of the Spanish transition to democracy. The constitutional history of Spain dates back to the constitution of 1812. Impatient with the slow pace of democratic political reforms in 1976 and 1977, Spain's new King Juan Carlos, known for his formidable personality, dismissed Carlos Arias Navarro and appointed the reformer Adolfo Suárez as Prime Minister.[67][68] The resulting general election in 1977 convened the Constituent Cortes (the Spanish Parliament, in its capacity as a constitutional assembly) for the purpose of drafting and approving the constitution of 1978.[69] After a national referendum on 6 December 1978, 88% of voters approved of the new constitution.		As a result, Spain is now composed of 17 autonomous communities and two autonomous cities with varying degrees of autonomy thanks to its Constitution, which nevertheless explicitly states the indivisible unity of the Spanish nation. The constitution also specifies that Spain has no state religion and that all are free to practice and believe as they wish.		The Spanish administration approved legislation in 2007 aimed at furthering equality between genders in Spanish political and economic life (Gender Equality Act).[70][71] However, in the legislative branch, as of May 2017 only 140 of the 350 members of the Congress were women (40%).[72] It places Spain 12th on a list of countries ranked by proportion of women in the lower house. In the Senate, there are only 101 women out of 263 (38.0%).[73] The Gender Empowerment Measure of Spain in the United Nations Human Development Report is 0.794, 12th in the world.[74]		Spain is a constitutional monarchy, with a hereditary monarch and a bicameral parliament, the Cortes Generales (General Courts). The executive branch consists of a Council of Ministers of Spain presided over by the Prime Minister, nominated and appointed by the monarch and confirmed by the Congress of Deputies following legislative elections. By political custom established by King Juan Carlos since the ratification of the 1978 Constitution, the king's nominees have all been from parties who maintain a plurality of seats in the Congress.		The legislative branch is made up of the Congress of Deputies (Congreso de los Diputados) with 350 members, elected by popular vote on block lists by proportional representation to serve four-year terms, and a Senate (Senado) with 259 seats of which 208 are directly elected by popular vote and the other 51 appointed by the regional legislatures to also serve four-year terms.		Spain is organisationally structured as a so-called Estado de las Autonomías ("State of Autonomies"); it is one of the most decentralised countries in Europe, along with Switzerland, Germany and Belgium;[75] for example, all autonomous communities have their own elected parliaments, governments, public administrations, budgets, and resources. Health and education systems among others are managed by the Spanish communities, and in addition, the Basque Country and Navarre also manage their own public finances based on foral provisions. In Catalonia, the Basque Country, Navarre and the Canary Islands, a full-fledged autonomous police corps replaces some of the State police functions (see Mossos d'Esquadra, Ertzaintza, Policía Foral/Foruzaingoa and Policía Canaria).		The Government respects the human rights of its citizens; although there are a few problems in some areas, the law and judiciary provide effective means of addressing individual instances of abuse. There are allegations that a few members of the security forces abused detainees and mistreated foreigners and illegal immigrants.[76] According to Amnesty International (AI), government investigations of such alleged abuses are often lengthy and punishments were light.[77] Violence against women was a problem, which the Government took steps to address.[78][79]		Spain provides one of the highest degrees of liberty in the world for its LGBT community. Among the countries studied by Pew Research Center in 2013, Spain is rated first in acceptance of homosexuality, with an 88% of society supporting the gay community compared to 11% who do not.[80]		The Spanish State is integrated by 17 autonomous communities and 2 autonomous cities, both groups being the highest or first-order administrative division in the country. Autonomous communities are integrated by provinces, of which there are 50 in total, and in turn, provinces are integrated by municipalities. In Catalonia, two additional divisions exist, the comarques (sing. comarca) and the vegueries (sing. vegueria) both of which have administrative powers; comarques being aggregations of municipalities, and the vegueries being aggregations of comarques. The concept of a comarca exists in all autonomous communities, however, unlike Catalonia, these are merely historical or geographical subdivisions.		Spain's autonomous communities are the first level administrative divisions of the country. They were created after the current constitution came into effect (in 1978) in recognition of the right to self-government of the "nationalities and regions of Spain".[81] The autonomous communities were to be integrated into adjacent provinces with common historical, cultural, and economical traits. This territorial organisation, based on devolution, is literally known in Spain as the "State of Autonomies".		The basic institutional law of each autonomous community is the Statute of Autonomy. The Statutes of Autonomy establish the name of the community according to its historical and contemporary identity, the limits of its territories, the name and organisation of the institutions of government and the rights they enjoy according to the constitution.[82]		The governments of all autonomous communities must be based on a division of powers comprising:		Catalonia, Galicia and the Basque Country, which identified themselves as nationalities, were granted self-government through a rapid process. Andalusia also took that denomination in its first Statute of Autonomy, even though it followed the longer process stipulated in the constitution for the rest of the country. Progressively, other communities in revisions to their Statutes of Autonomy have also taken that denomination in accordance to their historical and modern identities, such as the Valencian Community,[83] the Canary Islands,[84] the Balearic Islands,[85] and Aragon.[86]		The autonomous communities have wide legislative and executive autonomy, with their own parliaments and regional governments. The distribution of powers may be different for every community, as laid out in their Statutes of Autonomy, since devolution was intended to be asymmetrical. Only two communities—the Basque Country and Navarre—have full fiscal autonomy. Aside of fiscal autonomy, the nationalities—Andalusia, the Basque Country, Catalonia, and Galicia—were devolved more powers than the rest of the communities, among them the ability of the regional president to dissolve the parliament and call for elections at any time. In addition, the Basque Country, Catalonia and Navarre have police corps of their own: Ertzaintza, Mossos d'Esquadra and the Policía Foral respectively. Other communities have more limited forces or none at all, like the Policía Autónoma Andaluza[87] in Andalusia or the BESCAM in Madrid.		Nonetheless, recent amendments to existing Statutes of Autonomy or the promulgation of new Statutes altogether, have reduced the asymmetry between the powers originally granted to the nationalities and the rest of the regions.		Finally, along with the 17 autonomous communities, two autonomous cities are also part of the State of Autonomies and are first-order territorial divisions: Ceuta and Melilla. These are two exclaves located in the northern African coast.		Autonomous communities are subdivided into provinces, which served as their territorial building blocks. In turn, provinces are integrated by municipalities. The existence of both the provinces and the municipalities is guaranteed and protected by the constitution, not necessarily by the Statutes of Autonomy themselves. Municipalities are granted autonomy to manage their internal affairs, and provinces are the territorial divisions designed to carry out the activities of the State.[88]		The current provincial division structure is based—with minor changes—on the 1833 territorial division by Javier de Burgos, and in all, the Spanish territory is divided into 50 provinces. The communities of Asturias, Cantabria, La Rioja, the Balearic Islands, Madrid, Murcia and Navarre are the only communities that are integrated by a single province, which is coextensive with the community itself. In these cases, the administrative institutions of the province are replaced by the governmental institutions of the community.		After the return of democracy following the death of Franco in 1975, Spain's foreign policy priorities were to break out of the diplomatic isolation of the Franco years and expand diplomatic relations, enter the European Community, and define security relations with the West.		As a member of NATO since 1982, Spain has established itself as a participant in multilateral international security activities. Spain's EU membership represents an important part of its foreign policy. Even on many international issues beyond western Europe, Spain prefers to co-ordinate its efforts with its EU partners through the European political co-operation mechanisms.[vague]		Spain has maintained its special relations with Hispanic America and the Philippines. Its policy emphasises the concept of an Ibero-American community, essentially the renewal of the historically liberal concept of "Hispanidad" or "Hispanismo", as it is often referred to in English, which has sought to link the Iberian Peninsula with Hispanic America through language, commerce, history and culture.		Spain claims Gibraltar, a 6-square-kilometre (2.3 sq mi) Overseas Territory of the United Kingdom in the southernmost part of the Iberian Peninsula. Then a Spanish town, it was conquered by an Anglo-Dutch force in 1704 during the War of the Spanish Succession on behalf of Archduke Charles, pretender to the Spanish throne.		The legal situation concerning Gibraltar was settled in 1713 by the Treaty of Utrecht, in which Spain ceded the territory in perpetuity to the British Crown[90] stating that, should the British abandon this post, it would be offered to Spain first. Since the 1940s Spain has called for the return of Gibraltar. The overwhelming majority of Gibraltarians strongly oppose this, along with any proposal of shared sovereignty.[91] UN resolutions call on the United Kingdom and Spain, both EU members, to reach an agreement over the status of Gibraltar.[92][93]		The Spanish claim makes a distinction between the isthmus that connects the Rock to the Spanish mainland on the one hand, and the Rock and city of Gibraltar on the other. While the Rock and city were ceded by the Treaty of Utrecht, Spain asserts that the "occupation of the isthmus is illegal and against the principles of International Law".[94] The United Kingdom relies on de facto arguments of possession by prescription in relation to the isthmus,[95] as there has been "continuous possession [of the isthmus] over a long period".[96]		Another claim by Spain is about the Savage Islands, a claim not recognised by Portugal. Spain claims that they are rocks rather than islands, therefore claiming that there is no Portuguese territorial waters around the disputed islands. On 5 July 2013, Spain sent a letter to the UN expressing these views.[97][98]		Spain claims the sovereignty over the Perejil Island, a small, uninhabited rocky islet located in the South shore of the Strait of Gibraltar. The island lies 250 metres (820 ft) just off the coast of Morocco, 8 kilometres (5.0 mi) from Ceuta and 13.5 kilometres (8.4 mi) from mainland Spain. Its sovereignty is disputed between Spain and Morocco. It was the subject of an armed incident between the two countries in 2002. The incident ended when both countries agreed to return to the status quo ante which existed prior to the Moroccan occupation of the island. The islet is now deserted and without any sign of sovereignty.		Besides the Perejil Island, the Spanish-held territories claimed by other countries are two: Morocco claims the Spanish cities of Ceuta and Melilla and the plazas de soberanía islets off the northern coast of Africa; and Portugal does not recognise Spain's sovereignty over the territory of Olivenza.		The armed forces of Spain are known as the Spanish Armed Forces (Fuerzas Armadas Españolas). Their Commander-in-chief is the King of Spain, Felipe VI.[99]		The Spanish Armed Forces are divided into three branches:[100]		Spain's capitalist mixed economy is the 14th largest worldwide and the 5th largest in the European Union, as well as the Eurozone's 4th largest.		The centre-right government of former prime minister José María Aznar worked successfully to gain admission to the group of countries launching the euro in 1999. Unemployment stood at 7.6% in October 2006, lower than many other European countries, and significantly below Spain's early 1990s unemployment rate of at over 20%. Perennial weak points of Spain's economy include a large informal economy,[101][102][103] and an education system which OECD reports place among the poorest for developed countries, together with the United States and UK.[104]		By the mid-1990s the economy had recommenced the growth that had been disrupted by the global recession of the early 1990s. The strong economic growth helped the government to reduce the government debt as a percentage of GDP and Spain's high unemployment rate began to steadily decline. With the government budget in balance and inflation under control Spain was admitted into the Eurozone in 1999.		Since the 1990s some Spanish companies have gained multinational status, often expanding their activities in culturally close Latin America. Spain is the second biggest foreign investor there, after the United States. Spanish companies have also expanded into Asia, especially China and India.[105] This early global expansion is a competitive advantage over its competitors and European neighbours. The reason for this early expansion is the booming interest towards Spanish language and culture in Asia and Africa and a corporate culture that learned to take risks in unstable markets.		Spanish companies invested in fields like renewable energy commercialisation (Iberdrola was the world's largest renewable energy operator[106]), technology companies like Telefónica, Abengoa, Mondragon Corporation, Movistar, Hisdesat, Indra, train manufacturers like CAF, Talgo, global corporations such as the textile company Inditex, petroleum companies like Repsol and infrastructure, with six of the ten biggest international construction firms specialising in transport being Spanish, like Ferrovial, Acciona, ACS, OHL and FCC.[107]		In 2005 the Economist Intelligence Unit's quality of life survey placed Spain among the top 10 in the world.[108] In 2013 the same survey (now called the "Where-to-be-born index"), ranked Spain 28th in the world.[109]		In 2010, the Basque city of Bilbao was awarded with the Lee Kuan Yew World City Prize,[110] and its mayor at the time, Iñaki Azkuna, was awarded the World Mayor Prize in 2012.[111] The Basque capital city of Vitoria-Gasteiz received the European Green Capital Award in 2012.[112]		Crop areas were farmed in two highly diverse manners. Areas relying on non-irrigated cultivation (secano), which made up 85% of the entire crop area, depended solely on rainfall as a source of water. They included the humid regions of the north and the northwest, as well as vast arid zones that had not been irrigated. The much more productive regions devoted to irrigated cultivation (regadío) accounted for 3 million hectares in 1986, and the government hoped that this area would eventually double, as it already had doubled since 1950. Particularly noteworthy was the development in Almería—one of the most arid and desolate provinces of Spain—of winter crops of various fruits and vegetables for export to Europe.		Though only about 17% of Spain's cultivated land was irrigated, it was estimated to be the source of between 40–45% of the gross value of crop production and of 50% of the value of agricultural exports. More than half of the irrigated area was planted in corn, fruit trees, and vegetables. Other agricultural products that benefited from irrigation included grapes, cotton, sugar beets, potatoes, legumes, olive trees, mangos, strawberries, tomatoes, and fodder grasses. Depending on the nature of the crop, it was possible to harvest two successive crops in the same year on about 10% of the country's irrigated land.		Citrus fruits, vegetables, cereal grains, olive oil, and wine—Spain's traditional agricultural products—continued to be important in the 1980s. In 1983 they represented 12%, 12%, 8%, 6%, and 4%, respectively, of the country's agricultural production. Because of the changed diet of an increasingly affluent population, there was a notable increase in the consumption of livestock, poultry, and dairy products. Meat production for domestic consumption became the single most important agricultural activity, accounting for 30% of all farm-related production in 1983. Increased attention to livestock was the reason that Spain became a net importer of grains. Ideal growing conditions, combined with proximity to important north European markets, made citrus fruits Spain's leading export. Fresh vegetables and fruits produced through intensive irrigation farming also became important export commodities, as did sunflower seed oil that was produced to compete with the more expensive olive oils in oversupply throughout the Mediterranean countries of the European Community.		The climate of Spain, its geographic location, popular coastlines, diverse landscapes, historical legacy, vibrant culture and excellent infrastructure, has made Spain's international tourist industry among the largest in the world. In the last five decades, international tourism in Spain has grown to become the second largest in the world in terms of spending, worth approximately 40 billion Euros or about 5% of GDP in 2006.[113][114]		Spain is one of the world's leading countries in the development and production of renewable energy. In 2010 Spain became the solar power world leader when it overtook the United States with a massive power station plant called La Florida, near Alvarado, Badajoz.[115][116] Spain is also Europe's main producer of wind energy. In 2010 its wind turbines generated 42,976 GWh, which accounted for 16.4% of all electrical energy produced in Spain.[117][118][119] On 9 November 2010, wind energy reached an instantaneous historic peak covering 53% of mainland electricity demand[120] and generating an amount of energy that is equivalent to that of 14 nuclear reactors.[121] Other renewable energies used in Spain are hydroelectric, biomass and marine (2 power plants under construction).[122]		Non-renewable energy sources used in Spain are nuclear (8 operative reactors), gas, coal, and oil. Fossil fuels together generated 58% of Spain's electricity in 2009, just below the OECD mean of 61%. Nuclear power generated another 19%, and wind and hydro about 12% each.[123]		The Spanish road system is mainly centralised, with six highways connecting Madrid to the Basque Country, Catalonia, Valencia, West Andalusia, Extremadura and Galicia. Additionally, there are highways along the Atlantic (Ferrol to Vigo), Cantabrian (Oviedo to San Sebastián) and Mediterranean (Girona to Cádiz) coasts. Spain aims to put one million electric cars on the road by 2014 as part of the government's plan to save energy and boost energy efficiency.[124] The former Minister of Industry Miguel Sebastián said that "the electric vehicle is the future and the engine of an industrial revolution."[125]		Spain has the most extensive high-speed rail network in Europe, and the second-most extensive in the world after China.[126][127] As of October 2010, Spain has a total of 3,500 km (2,174.80 mi) of high-speed tracks linking Málaga, Seville, Madrid, Barcelona, Valencia and Valladolid, with the trains reaching speeds up to 300 km/h (190 mph). On average, the Spanish high-speed train is the fastest one in the world, followed by the Japanese bullet train and the French TGV.[128] Regarding punctuality, it is second in the world (98.54% on-time arrival) after the Japanese Shinkansen (99%).[129] Should the aims of the ambitious AVE programme (Spanish high speed trains) be met, by 2020 Spain will have 7,000 km (4,300 mi) of high-speed trains linking almost all provincial cities to Madrid in less than three hours and Barcelona within four hours.		There are 47 public airports in Spain. The busiest one is the airport of Madrid (Barajas), with 50 million passengers in 2011, being the world's 15th busiest airport, as well as the European Union's fourth busiest. The airport of Barcelona (El Prat) is also important, with 35 million passengers in 2011, being the world's 31st-busiest airport. Other main airports are located in Majorca (23 million passengers), Málaga (13 million passengers), Las Palmas (Gran Canaria) (11 million passengers), Alicante (10 million passengers) and smaller, with the number of passengers between 4 and 10 million, for example Tenerife (two airports), Valencia, Seville, Bilbao, Ibiza, Lanzarote, Fuerteventura. Also, more than 30 airports with the number of passengers below 4 million.		In the 19th and 20th centuries science in Spain was held back by severe political instability and consequent economic underdevelopment. Despite the conditions, some important scientists and engineers emerged. The most notable were Miguel Servet, Santiago Ramón y Cajal, Narcís Monturiol, Celedonio Calatayud, Juan de la Cierva, Leonardo Torres y Quevedo, Margarita Salas and Severo Ochoa.		Water supply and sanitation in Spain is characterised by universal access and generally good service quality, while tariffs are among the lowest in the EU.[130] Almost half of the population is served by private or mixed private-public water companies, which operate under concession contracts with municipalities. The largest of the private water companies, with a market share of about 50% of the private concessions, is Aguas de Barcelona (Agbar). However, the large cities are all served by public companies except Barcelona and Valencia. The largest public company is Canal de Isabel II, which serves the metropolitan area of Madrid.		Droughts affect water supply in Southern Spain, which increasingly is turning towards seawater desalination to meet its water needs.		In 2008 the population of Spain officially reached 46 million people, as recorded by the Padrón municipal (Spain's Municipal Register).[131] Spain's population density, at 91/km² (235/sq mi), is lower than that of most Western European countries and its distribution across the country is very unequal. With the exception of the region surrounding the capital, Madrid, the most populated areas lie around the coast. The population of Spain more than doubled since 1900, when it stood at 18.6 million, principally due to the spectacular demographic boom in the 1960s and early 1970s.[132]		Native Spaniards make up 88% of the total population of Spain. After the birth rate plunged in the 1980s and Spain's population growth rate dropped, the population again trended upward, based initially on the return of many Spaniards who had emigrated to other European countries during the 1970s, and more recently, fuelled by large numbers of immigrants who make up 12% of the population. The immigrants originate mainly in Latin America (39%), North Africa (16%), Eastern Europe (15%), and Sub-Saharan Africa (4%).[133] In 2005, Spain instituted a three-month amnesty programme through which certain hitherto undocumented aliens were granted legal residency.		In 2008, Spain granted citizenship to 84,170 persons, mostly to people from Ecuador, Colombia and Morocco.[134] A sizeable portion of foreign residents in Spain also comes from other Western and Central European countries. These are mostly British, French, German, Dutch, and Norwegian. They reside primarily on the Mediterranean coast and the Balearic islands, where many choose to live their retirement or telecommute.		Substantial populations descended from Spanish colonists and immigrants exist in other parts of the world, most notably in Latin America. Beginning in the late 15th century, large numbers of Iberian colonists settled in what became Latin America and at present most white Latin Americans (who make up about one-third of Latin America's population) are of Spanish or Portuguese origin. Around 240,000 Spaniards emigrated in the 16th century, mostly to Peru and Mexico.[135] Another 450,000 left in the 17th century.[136] Between 1846 and 1932 it is estimated that nearly 5 million Spaniards emigrated to the Americas, especially to Argentina and Brazil.[137] Approximately two million Spaniards migrated to other Western European countries between 1960 and 1975. During the same period perhaps 300,000 went to Latin America.[138]		Source: "Áreas urbanas +50", Ministry of Public Works and Transport (2013)[142]		The Spanish Constitution of 1978, in its second article, recognises several contemporary entities—nationalities—[m] and regions, within the context of the Spanish nation.		Spain is de facto a plurinational state.[147][148] The idendity of Spain rather accrues of an overlap of different territorial and ethnolinguistic identities than of a sole Spanish identity. In some cases some of the territorial identities may conflict with the dominant Spanish culture. Distinct traditional identities within Spain include the Basques, Catalans, Galicians, Andalusians and Valencians,[149] although to some extent all of the 17 autonomous communities may claim a distinct local identity.		It is this last feature of "shared identity" between the more local level or autonomous community and the Spanish level which makes the identity question in Spain complex and far from univocal.		Spain has a number of descendants of populations from former colonies, especially Latin America and North Africa. Smaller numbers of immigrants from several Sub-Saharan countries have recently been settling in Spain. There are also sizeable numbers of Asian immigrants, most of whom are of Middle Eastern, South Asian and Chinese origin. The single largest group of immigrants are European; represented by large numbers of Romanians, Britons, Germans, French and others.[150]		The arrival of the gitanos, a Romani people, began in the 16th century; estimates of the Spanish Roma population range from 750,000 to over one million.[151][152][153][154][155] There are also the mercheros (also quinquis), a formerly nomadic minority group. Their origin is unclear.		Historically, Sephardi Jews and Moriscos are the main minority groups originated in Spain and with a contribution to Spanish culture.[156] The Spanish government is offering Spanish nationality to Sephardi Jews.[157]		According to the Spanish government there were 5.7 million foreign residents in Spain in 2011, or 12% of the total population. According to residence permit data for 2011, more than 860,000 were Romanian, about 770,000 were Moroccan, approximately 390,000 were British, and 360,000 were Ecuadorian.[158] Other sizeable foreign communities are Colombian, Bolivian, German, Italian, Bulgarian, and Chinese. There are more than 200,000 migrants from Sub-Saharan Africa living in Spain, principally Senegaleses and Nigerians.[159] Since 2000, Spain has experienced high population growth as a result of immigration flows, despite a birth rate that is only half the replacement level. This sudden and ongoing inflow of immigrants, particularly those arriving illegally by sea, has caused noticeable social tension.[160]		Within the EU, Spain had the 2nd highest immigration rate in percentage terms after Cyprus, but by a great margin, the highest in absolute numbers, up to 2008.[161] The number of immigrants in Spain had grown up from 500,000 people in 1996 to 5.2 million in 2008 out of a total population of 46 million.[162][163] In 2005 alone, a regularisation programme increased the legal immigrant population by 700,000 people.[164] There are a number of reasons for the high level of immigration, including Spain's cultural ties with Latin America, its geographical position, the porosity of its borders, the large size of its underground economy and the strength of the agricultural and construction sectors, which demand more low cost labour than can be offered by the national workforce.		Another statistically significant factor is the large number of residents of EU origin typically retiring to Spain's Mediterranean coast. In fact, Spain was Europe's largest absorber of migrants from 2002 to 2007, with its immigrant population more than doubling as 2.5 million people arrived.[165] In 2008, prior to the onset of the economic crisis, the Financial Times reported that Spain was the most favoured destination for Western Europeans considering a move from their own country and seeking jobs elsewhere in the EU.[166]		In 2008, the government instituted a "Plan of Voluntary Return" which encouraged unemployed immigrants from outside the EU to return to their home countries and receive several incentives, including the right to keep their unemployment benefits and transfer whatever they contributed to the Spanish Social Security.[167] The programme had little effect; during its first two months, just 1,400 immigrants took up the offer.[168] What the programme failed to do, the sharp and prolonged economic crisis has done from 2010 to 2011 in that tens of thousands of immigrants have left the country due to lack of jobs. In 2011 alone, more than half a million people left Spain.[169] For the first time in decades the net migration rate was expected to be negative, and nine out of 10 emigrants were foreigners.[169]		Spain is openly multilingual,[170] and the constitution establishes that the nation will protect "all Spaniards and the peoples of Spain in the exercise of human rights, their cultures and traditions, languages and institutions.[171]		Spanish (español)—officially recognised in the constitution as Castilian (castellano)—is the official language of the entire country, and it is the right and duty of every Spaniard to know the language. The constitution also establishes that "all other Spanish languages"—that is, all other languages of Spain—will also be official in their respective autonomous communities in accordance to their Statutes, their organic regional legislations, and that the "richness of the distinct linguistic modalities of Spain represents a patrimony which will be the object of special respect and protection."[172]		The other official languages of Spain, co-official with Spanish are:		As a percentage of the general population, Basque is spoken by 2%, Catalan (or Valencian) by 17%, and Galician by 7% of all Spaniards.[173]		In Catalonia, Aranese (aranés), a local variety of the Occitan language, has been declared co-official along with Catalan and Spanish since 2006. Occitan itself is spoken only in the comarca of Val d'Aran by roughly 6,700 people. Other Romance minority languages, though not official, have special recognition, such as the Astur-Leonese group (Asturian – asturianu, also called bable – in Asturias[174] and Leonese – llionés – in Castile and León) and Aragonese (aragonés) in Aragon.		In the North African Spanish autonomous city of Melilla, Riff Berber is spoken by a significant part of the population. In the tourist areas of the Mediterranean coast and the islands, English and German are widely spoken by tourists, foreign residents, and tourism workers.[175]		State education in Spain is free and compulsory from the age of six to sixteen. The current education system was established by the 2006 educational law, LOE (Ley Orgánica de Educación), or Fundamental Law for the Education.[176] In 2014, the LOE was partially modified by the newer and controversial LOMCE law (Ley Orgánica para la Mejora de la Calidad Educativa), or Fundamental Law for the Improvement of the Education System, commonly called Ley Wert (Wert Law).[177] Since 1970 to 2014, Spain has had seven different educational laws (LGE, LOECE, LODE, LOGSE, LOPEG, LOE and LOMCE).[178]		Institución Libre de Enseñanza was an educational project that developed in Spain for the half a century of about 1876–1936 by Francisco Giner de los Ríos and Gumersindo de Azcárate. The institute was inspired by the philosophy of Krausism. Concepción Arenal in feminism and Santiago Ramón y Cajal in neuroscience were in the movement.		The health care system of Spain (Spanish National Health System) is considered one of the best in the world, in 7th position in the ranking elaborated by the World Health Organization.[179] The health care is public, universal and free for any legal citizen of Spain.[180] The total health spending is 9.4% of the GDP, slightly above the average of 9.3% of the OECD.		Roman Catholicism has long been the main religion of Spain, and although it no longer has official status by law, in all public schools in Spain students have to choose either a religion or ethics class. Catholicism is the religion most commonly taught, although the teaching of Islam,[181] Judaism,[182] and evangelical Christianity[183] is also recognised in law. According to a June 2016 study by the Spanish Centre for Sociological Research about 68% of Spaniards self-identify as Catholics, 2% other faith, and about 27% identify with no religion. Most Spaniards do not participate regularly in religious services. This same study shows that of the Spaniards who identify themselves as religious, 59% hardly ever or never go to church, 16% go to church some times a year, 9% some time per month and 15% every Sunday or multiple times per week.[4] Recent polls and surveys have revealed that atheists and agnostics comprise anywhere from 20% to 27% of the Spanish population.[4][184][185]		Altogether, about 9% of the entire Spanish population attends religious services at least once per month.[4] Though Spanish society has become considerably more secular in recent decades, the influx of Latin American immigrants, who tend to be strong Catholic practitioners, has helped the Catholic Church to recover.		There have been four Spanish Popes. Damasus I, Calixtus III, Alexander VI and Benedict XIII. Spanish mysticism was an important intellectual fight against Protestantism with Teresa of Ávila, a reformist nun, ahead. The Society of Jesus was founded by Ignatius of Loyola and Francisco Javier. In the 1960s, Jesuits Pedro Arrupe and Ignacio Ellacuría were inside the movement of Liberation Theology.		Protestant churches have about 1,200,000 members.[186] There are about 105,000 Jehovah's Witnesses. The Church of Jesus Christ of Latter-day Saints has approximately 46,000 adherents in 133 congregations in all regions of the country and has a temple in the Moratalaz District of Madrid.[187]		A study made by the Union of Islamic Communities of Spain demonstrated that there were about 1,700,000 inhabitants of Muslim background living in Spain as of 2012[update], accounting for 3–4% of the total population of Spain. The vast majority was composed of immigrants and descendants originating from Morocco and other African countries. More than 514,000 (30%) of them had Spanish nationality.[188]		The recent waves of immigration have also led to an increasing number of Hindus, Buddhists, Sikhs and Muslims. After the Reconquista in 1492, Muslims did not live in Spain for centuries. Late 19th-century colonial expansion in northwestern Africa gave a number of residents in Spanish Morocco and Western Sahara full citizenship. Their ranks have since been bolstered by recent immigration, especially from Morocco and Algeria.		Judaism was practically non-existent in Spain from the 1492 expulsion until the 19th century, when Jews were again permitted to enter the country. Currently there are around 62,000 Jews in Spain, or 0.14% of the total population. Most are arrivals in the past century, while some are descendants of earlier Spanish Jews. Approximately 80,000 Jews are thought to have lived in Spain prior to its expulsion.[189]		Culturally, Spain is a Western country. Almost every aspect of Spanish life is permeated by its Roman heritage, making Spain one of the major Latin countries of Europe. Spanish culture is marked by strong historic ties to Catholicism, which played a pivotal role in the country's formation and subsequent identity. Spanish art, architecture, cuisine, and music has been shaped by successive waves of foreign invaders, as well as by the country's Mediterranean climate and geography. The centuries-long colonial era globalised Spanish language and culture, with Spain also absorbing the cultural and commercial products of its diverse empire.		It should be noted that after Italy (49) and China (45), Spain is the third country in the world with the most World Heritage Sites. At the present time it has 44 recognised sites, including the landscape of Monte Perdido in the Pyrenees, which is shared with France, the Prehistoric Rock Art Sites of the Côa Valley and Siega Verde, which is shared with Portugal (the Portuguese part being in the Côa Valley, Guarda), and the Heritage of Mercury, shared with Slovenia.[190] In addition, Spain has also 14 Intangible cultural heritage, or "Human treasures", Spain ranks first in Europe according to UNESCO's Intangible Cultural Heritage List, tied with Croatia.[191]		The earliest recorded examples of vernacular Romance-based literature date from the same time and location, the rich mix of Muslim, Jewish, and Christian cultures in Muslim Spain, in which Maimonides, Averroes, and others worked, the Kharjas (Jarchas).		During the Reconquista, the epic poem Cantar de Mio Cid was written about a real man—his battles, conquests, and daily life.		Other major plays from the medieval times were Mester de Juglaría, Mester de Clerecía, Coplas por la muerte de su padre or El Libro de buen amor (The Book of Good Love).		During the Renaissance the major plays are La Celestina and El Lazarillo de Tormes, while many religious literature was created with poets as Luis de León, San Juan de la Cruz, Santa Teresa de Jesús, etc.		The Baroque is the most important period for Spanish culture. We are in the times of the Spanish Empire. The famous Don Quijote de La Mancha by Miguel de Cervantes was written in this time. Other writers from the period are: Francisco de Quevedo, Lope de Vega, Calderón de la Barca or Tirso de Molina.		During the Enlightenment we find names such as Leandro Fernández de Moratín, Benito Jerónimo Feijóo, Gaspar Melchor de Jovellanos or Leandro Fernández de Moratín.		During the Romanticism, José Zorrilla created one of the most emblematic figures in European literature in Don Juan Tenorio. Other writers from this period are Gustavo Adolfo Bécquer, José de Espronceda, Rosalía de Castro or Mariano José de Larra.		In Realism we find names such as Benito Pérez Galdós, Emilia Pardo Bazán, Leopoldo Alas (Clarín), Concepción Arenal, Vicente Blasco Ibáñez and Menéndez Pelayo. Realism offered depictions of contemporary life and society 'as they were'. In the spirit of general "Realism", Realist authors opted for depictions of everyday and banal activities and experiences, instead of romanticised or stylised presentations.		The group that has become known as the Generation of 1898 was marked by the destruction of Spain's fleet in Cuba by US gunboats in 1898, which provoked a cultural crisis in Spain. The "Disaster" of 1898 led established writers to seek practical political, economic, and social solutions in essays grouped under the literary heading of Regeneracionismo. For a group of younger writers, among them Miguel de Unamuno, Pío Baroja, and José Martínez Ruiz (Azorín), the Disaster and its cultural repercussions inspired a deeper, more radical literary shift that affected both form and content. These writers, along with Ramón del Valle-Inclán, Antonio Machado, Ramiro de Maeztu, and Ángel Ganivet, came to be known as the Generation of '98.		The Generation of 1914 or Novecentismo. The next supposed "generation" of Spanish writers following those of '98 already calls into question the value of such terminology. By the year 1914—the year of the outbreak of the First World War and of the publication of the first major work of the generation's leading voice, José Ortega y Gasset—a number of slightly younger writers had established their own place within the Spanish cultural field.		Leading voices include the poet Juan Ramón Jiménez, the academics and essayists Ramón Menéndez Pidal, Gregorio Marañón, Manuel Azaña, Maria Zambrano, Eugeni d'Ors, Clara Campoamor and Ortega y Gasset, and the novelists Gabriel Miró, Ramón Pérez de Ayala, and Ramón Gómez de la Serna. While still driven by the national and existential questions that obsessed the writers of '98, they approached these topics with a greater sense of distance and objectivity. Salvador de Madariaga, another prominent intellectual and writer, was one of the founders of the College of Europe and the composer of the constitutive manifest of the Liberal International.		The Generation of 1927, where poets Pedro Salinas, Jorge Guillén, Federico García Lorca, Vicente Aleixandre, Dámaso Alonso. All were scholars of their national literary heritage, again evidence of the impact of the calls of regeneracionistas and the Generation of 1898 for Spanish intelligence to turn at least partially inwards.		The two main writers in the second half of the 20th century were the Nobel Prize in Literature laureate Camilo José Cela and Miguel Delibes from Generation of '36. Spain is one of the countries with the most number of laureates with the Nobel Prize in Literature, and with Latin American laureates they made the Spanish language literature one of the most laureates of all. The Spanish writers are: José Echegaray, Jacinto Benavente, Juan Ramón Jiménez, Vicente Aleixandre and Camilo José Cela. The Portuguese writer José Saramago, also awarded with the prize, lived for many years in Spain and spoke both Portuguese and Spanish. Saramago was also well known by his Iberist ideas.		The Generation of '50 are also known as the children of the civil war. Rosa Chacel, Gloria Fuertes, Jaime Gil de Biedma, Juan Goytisolo, Carmen Martín Gaite, Ana María Matute, Juan Marsé, Blas de Otero, Gabriel Celaya, Antonio Gamoneda, Rafael Sánchez Ferlosio or Ignacio Aldecoa.		Artists from Spain have been highly influential in the development of various European artistic movements. Due to historical, geographical and generational diversity, Spanish art has known a great number of influences. The Moorish heritage in Spain, especially in Andalusia, is still evident today and European influences include Italy, Germany and France, especially during the Baroque and Neoclassical periods.		During the Golden Age we find painters such as El Greco, José de Ribera, Bartolomé Esteban Murillo and Francisco Zurbarán. Also inside Baroque period Diego Velázquez created some of the most famous Spanish portraits, like Las Meninas or Las Hilanderas.		Francisco Goya painted during a historical period that includes the Spanish Independence War, the fights between liberals and absolutists, and the raise of state-nations.		Joaquín Sorolla is a well-known impressionist painter and there are many important Spanish painters belonging to the modernism art movement, including Pablo Picasso, Salvador Dalí, Juan Gris and Joan Miró.		The Plateresque style extended from beginnings of the 16th century until the last third of the century and its stylistic influence pervaded the works of all great Spanish artists of the time. Alonso Berruguete (Valladolid School) is called the "Prince of Spanish sculpture". His main works were the upper stalls of the choir of the Cathedral of Toledo, the tomb of Cardinal Tavera in the same Cathedral, and the altarpiece of the Visitation in the church of Santa Úrsula in the same locality. Other notable sculptors were Bartolomé Ordóñez, Diego de Siloé, Juan de Juni and Damián Forment.		There were two Schools of special flair and talent: the Seville School, to which Juan Martínez Montañés belonged, whose most celebrated works are the Crucifix in the Cathedral of Seville, another in Vergara, and a Saint John; and the Granada School, to which Alonso Cano belonged, to whom an Immaculate Conception and a Virgin of Rosary, are attributed.		Other notable Andalusian Baroque sculptors were Pedro de Mena, Pedro Roldán and his daughter Luisa Roldán, Juan de Mesa and Pedro Duque Cornejo. In the 20th century the most important Spanish sculptors were Julio González, Pablo Gargallo, Eduardo Chillida and Pablo Serrano.		Spanish cinema has achieved major international success including Oscars for recent films such as Pan's Labyrinth and Volver.[192] In the long history of Spanish cinema, the great filmmaker Luis Buñuel was the first to achieve world recognition, followed by Pedro Almodóvar in the 1980s (La Movida Madrileña). Mario Camus and Pilar Miró worked together in Curro Jiménez.		Spanish cinema has also seen international success over the years with films by directors like Segundo de Chomón, Florián Rey, Luis García Berlanga, Carlos Saura, Julio Medem, Isabel Coixet, Alejandro Amenábar, Icíar Bollaín and brothers David Trueba and Fernando Trueba.		Actresses Sara Montiel and Penélope Cruz or actor Antonio Banderas are among those who have become Hollywood stars.		Due to its historical and geographical diversity, Spanish architecture has drawn from a host of influences. An important provincial city founded by the Romans and with an extensive Roman era infrastructure, Córdoba became the cultural capital, including fine Arabic style architecture, during the time of the Islamic Umayyad dynasty.[193] Later Arab style architecture continued to be developed under successive Islamic dynasties, ending with the Nasrid, which built its famed palace complex in Granada.		Simultaneously, the Christian kingdoms gradually emerged and developed their own styles; developing a pre-Romanesque style when for a while isolated from contemporary mainstream European architectural influences during the earlier Middle Ages, they later integrated the Romanesque and Gothic streams. There was then an extraordinary flowering of the Gothic style that resulted in numerous instances being built throughout the entire territory. The Mudéjar style, from the 12th to 17th centuries, was developed by introducing Arab style motifs, patterns and elements into European architecture.		The arrival of Modernism in the academic arena produced much of the architecture of the 20th century. An influential style centred in Barcelona, known as modernisme, produced a number of important architects, of which Gaudí is one. The International style was led by groups like GATEPAC. Spain is currently experiencing a revolution in contemporary architecture and Spanish architects like Rafael Moneo, Santiago Calatrava, Ricardo Bofill as well as many others have gained worldwide renown.		Spanish music is often considered abroad to be synonymous with flamenco, a West Andalusian musical genre, which, contrary to popular belief, is not widespread outside that region. Various regional styles of folk music abound in Aragon, Catalonia, Valencia, Castile, the Basque Country, Galicia, Cantabria and Asturias. Pop, rock, hip hop and heavy metal are also popular.		In the field of classical music, Spain has produced a number of noted composers such as Isaac Albéniz, Manuel de Falla and Enrique Granados and singers and performers such as Plácido Domingo, José Carreras, Montserrat Caballé, Alicia de Larrocha, Alfredo Kraus, Pablo Casals, Ricardo Viñes, José Iturbi, Pablo de Sarasate, Jordi Savall and Teresa Berganza. In Spain there are over forty professional orchestras, including the Orquestra Simfònica de Barcelona, Orquesta Nacional de España and the Orquesta Sinfónica de Madrid. Major opera houses include the Teatro Real, the Gran Teatre del Liceu, Teatro Arriaga and the El Palau de les Arts Reina Sofía.		Thousands of music fans also travel to Spain each year for internationally recognised summer music festivals Sónar which often features the top up and coming pop and techno acts, and Benicàssim which tends to feature alternative rock and dance acts.[194] Both festivals mark Spain as an international music presence and reflect the tastes of young people in the country.		The most popular traditional musical instrument, the guitar, originated in Spain.[195] Typical of the north are the traditional bag pipers or gaiteros, mainly in Asturias and Galicia.		Spanish cuisine consists of a great variety of dishes which stem from differences in geography, culture and climate. It is heavily influenced by seafood available from the waters that surround the country, and reflects the country's deep Mediterranean roots. Spain's extensive history with many cultural influences has led to a unique cuisine. In particular, three main divisions are easily identified:		Mediterranean Spain – all such coastal regions, from Catalonia to Andalusia – heavy use of seafood, such as pescaíto frito (fried fish); several cold soups like gazpacho; and many rice-based dishes like paella from Valencia[196] and arròs negre (black rice) from Catalonia.[197]		Inner Spain – Castile – hot, thick soups such as the bread and garlic-based Castilian soup, along with substantious stews such as cocido madrileño. Food is traditionally conserved by salting, like Spanish ham, or immersed in olive oil, like Manchego cheese.		Atlantic Spain – the whole Northern coast, including Asturian, Basque, Cantabrian and Galician cuisine – vegetable and fish-based stews like caldo gallego and marmitako. Also, the lightly cured lacón ham. The best known cuisine of the northern countries often rely on ocean seafood, like the Basque-style cod, albacore or anchovy or the Galician octopus-based polbo á feira and shellfish dishes.		While varieties of football had been played in Spain as far back as Roman times, sport in Spain has been dominated by English style association football since the early 20th century. Real Madrid C.F. and FC Barcelona are two of the most successful football clubs in the world. The country's national football team won the UEFA European Football Championship in 1964, 2008 and 2012 and the FIFA World Cup in 2010, and is the first team to ever win three back-to-back major international tournaments.		Basketball, tennis, cycling, handball, futsal, motorcycling and, lately, Formula One are also important due to the presence of Spanish champions in all these disciplines. Today, Spain is a major world sports powerhouse, especially since the 1992 Summer Olympics that were hosted in Barcelona, which stimulated a great deal of interest in sports in the country. The tourism industry has led to an improvement in sports infrastructure, especially for water sports, golf and skiing.		Rafael Nadal is the leading Spanish tennis player and has won several Grand Slam titles including the Wimbledon 2010 men's singles. In north Spain, the game of pelota is very popular. Alberto Contador is the leading Spanish cyclist and has won several Grand Tour titles including two Tour de France titles.		Public holidays celebrated in Spain include a mix of religious (Roman Catholic), national and regional observances. Each municipality is allowed to declare a maximum of 14 public holidays per year; up to nine of these are chosen by the national government and at least two are chosen locally.[198] Spain's National Day (Fiesta Nacional de España) is 12 October, the anniversary of the Discovery of America and commemorate Our Lady of the Pillar feast, patroness of Aragon and throughout Spain.		There are many festivals and festivities in Spain. Some of them are known worldwide, and every year millions of people from all over the world go to Spain to experience one of these festivals. One of the most famous is San Fermín, in Pamplona. While its most famous event is the encierro, or the running of the bulls, which happens at 8:00 am from 7 to 14 July, the week-long celebration involves many other traditional and folkloric events. Its events were central to the plot of The Sun Also Rises, by Ernest Hemingway, which brought it to the general attention of English-speaking people. As a result, it has become one of the most internationally renowned fiestas in Spain, with over 1,000,000 people attending every year.		Other festivals include the carnivals in the Canary Islands, the Falles in Valencia or the Holy Week in Andalusia and Castile and León.		1. All twenty-eight member states of the European Union are also members of the WTO in their own right:		2. Special administrative regions of the People's Republic of China, participates as "Hong Kong, China" and "Macao China". 3. Officially the Republic of China, participates as "Separate Customs Territory of Taiwan, Penghu, Kinmen and Matsu", and "Chinese Taipei" in short.		
