Employee exit management is the process used within many businesses to terminate employees contracts in a professional manner. It applies to employees who have resigned and those that have been terminated by the company.		When an employee is terminated there are a number of considerations that an organization needs to make in order to cleanly end the relationship between the company and the employee. The company as a legal entity has a responsibility to the employee which may extend beyond the period of employment and this is the primary focus of the exit procedure.[1]		As part of computer security, the process will also ensure that access privileges are revoked when a person leaves, and may also cover other issues such as the recovery of equipment, keys and credit cards to ensure that security integrity be maintained.						Employees may terminate their contract by resigning or an employer may terminate the contract by dismissing an employee.[2]		Short term employment is when an employee is hired for less than one year. These will normally end automatically once the end date is reached. However, the employer may wish to end the contract earlier or extend the contract.		A temporary contract is usually around 3 months long. Employers may recruit temporary employees during busy periods such as Christmas. At the end of the temporary contract the employer may wish to offer a permanent position or extend the temporary contract. Temporary contract will also automatically end once the end date is reached.		Employees maybe hired on a contractor basis for the completing of a certain task. Once the task is completed or the event takes place the contract ends.this type of contract employees falls under seasonal works.		Dismissal is when you end the contract of an employee. This must be done in a fair way.		There are different types of dismissal:[3]		If the employer dismisses you from 6 April 2009 onwards, they should follow the procedures which are laid out in the Acas Code of Practice on disciplinary and grievance procedures.		The procedure should have the following steps:[4]		An employer may dismiss you if you are unable to complete the job to the required standard or you have the capability to complete to a high standard but for some reason or another you are unable to.		There is no specific process by law an employer must follow in order to dismiss an individual however it must be carried out fairly.		Misconduct includes persistent lateness or unauthorised absence. It is based on serious misconduct or gross misconduct. Serious misconduct includes poor performance for which the employee should be provided with a formal warning which states that not improving could lead to dismissal. Gross misconduct includes theft, physical violence, serious insubordination and gross negligence for which the employee may be dismissed immediately.[5]		An employee may be dismissed due to medical reasons. The employer must pay for the employee whilst dismissed for a period not exceeding 20 weeks. The employer may wish to provide the employee with alternative work for the period of time, which is suited to your medical needs.[6]		Redundancy is when you dismiss an employee because you no longer need anyone to do their job.[7]		Both company Liquidation and Administration can lead to the end of the company. Company administration intention is to help the company repay its debts and avoid insolvency. Liquidation is when the company is forced to sell all of its assets before the company vanishes as a whole. This will result in the employees being made redundant.[8] You’ll normally be entitled to statutory redundancy pay if you’re an employee and you’ve been working for your current employer for 2 years or more.		You’ll get:[9]		Length of service is capped at 20 years and weekly pay is capped at £475. The maximum amount of statutory redundancy pay is £14,250.		
This is a list of countries by employment rate (the proportion of working age adults employed, here with working age between 15 and 64 years old). The information is based largely on data from the OECD as at 2013 and can be expanded as new data (with sources) is found. Employment data for the European Union countries can be retrieved from Eurostat [1].		Due to different methods for calculating the unemployment rate, the employment rate can be used as a supplemental method of comparison.		
Professional certification, trade certification, or professional designation, often called simply certification or qualification, is a designation earned by a person to assure qualification to perform a job or task. Not all certifications that use post-nominal letters are an acknowledgement of educational achievement, or an agency appointed to safeguard the public interest.						A certification is a third-party attestation of an individual's level of knowledge or proficiency in a certain industry or profession. They are granted by authorities in the field, such as professional societies and universities, or by private certificate-granting agencies. Most certifications are time-limited; some expire after a period of time (e.g., the lifetime of a product that required certification for use), while others can be renewed indefinitely as long as certain requirements are met. Renewal usually requires ongoing education to remain up-to-date on advancements in the field, evidenced by earning the specified number of continuing education credits (CECs), or continuing education units (CEUs), from approved professional development courses.		Many certification programs are affiliated with professional associations, trade organizations, or private vendors interested in raising industry standards. Certificate programs are often created or endorsed by professional associations, but are typically completely independent from membership organizations. Certifications are very common in fields such as aviation, construction, technology, environment, and other industrial sectors, as well as healthcare, business, real estate, and finance.		According to The Guide to National Professional Certification Programs (1997) by Phillip Barnhart, "certifications are portable, since they do not depend on one company's definition of a certain job" and they provide protential employers with "an impartial, third-party endorsement of an individual's professional knowledge and experience".[1]		Certification is different from professional licensure. In the United States, licenses are typically issued by state agencies, whereas certifications are usually awarded by professional societies or educational institutes. Obtaining a certificate is voluntary in some fields, but in others, certification from a government-accredited agency may be legally required to perform certain jobs or tasks. In other countries, licenses are typically granted by professional societies or universities and require a certificate after about three to five years and so on thereafter. The assessment process for certification may be more comprehensive than that of licensure, though sometimes the assessment process is very similar or even the same, despite differing in terms of legal status.		The American National Standards Institute (ANSI) defines the standard for being a certifying agency as meeting the following two requirements:		The Institute for Credentialing Excellence (ICE) is a U.S.-based organization that sets standards for the accreditation of personnel certification and certificate programs based on the Standards for Educational and Psychological Testing, a joint publication of the American Educational Research Association (AERA), the American Psychological Association (APA), and the National Council on Measurement in Education (NCME). Many members of the Association of Test Publishers (ATP) are also certification organizations.		There are three general types of certification. Listed in order of development level and portability, they are: corporate (internal), product-specific, and profession-wide.		Corporate, or "internal" certifications, are made by a corporation or low-stakes organization for internal purposes. For example, a corporation might require a one-day training course for all sales personnel, after which they receive a certificate. While this certificate has limited portability – to other corporations, for example – it is the most simple to develop.		Product-specific certifications are more involved, and are intended to be referenced to a product across all applications. This approach is very prevalent in the information technology (IT) industry, where personnel are certified on a version of software or hardware. This type of certification is portable across locations (for example, different corporations that use that software), but not across other products. Another example could be the certifications issued for shipping personnel, which are under international standards even for the recognition of the certification body, under the International Maritime Organization (IMO).		The most general type of certification is profession-wide. Certification in the medical profession is often offered by particular specialties. In order to apply professional standards, increase the level of practice, and protect the public, a professional organization might establish a certification. This is intended to be portable to all places a certified professional might work. Of course, this generalization increases the cost of such a program; the process to establish a legally defensible assessment of an entire profession is very extensive. An example of this is a Certified Public Accountant (CPA), which would not be certified for just one corporation or one piece of accountancy software but for general work in the profession.		Many universities grant professional certificates as an award for the completion of an educational program. The curriculum of a professional certificate is most often in a focused subject matter. Many professional certificates have the same curriculum as master's degrees in the same subject. Many other professional certificates offer the same courses as master's degrees in the same subject, but require the student to take fewer total courses to complete the program. Some professional certificates have a curriculum that more closely resembles a baccalaureate major in the same field. The typical professional certificate program is between 200-300 class-hours in size. It is uncommon for a program to be larger or smaller than that. Most professional certificate programs are open enrollment, but some have admissions processes. A few universities put some of their professional certificates into a subclass they refer to as advanced professional certificates.		Some of the more commonly offered professional certificates include:		Advanced professional certificates are professional credentials designed to help professionals enhance their job performance and marketability in their respective fields. In many other countries, certificates are qualifications in higher education. In the United States, a certificate may be offered by an institute of higher education. These certificates usually signify that a student has reached a standard of knowledge of a certain vocational subject. Certificate programs can be completed more quickly than associate degrees and often do not have general education requirements.		An advanced professional certificate is a result of an educational process designed for individuals. Certificates are designed for both newcomers to the industry as well as seasoned professionals. Certificates are awarded by an educational program or academic institution. Completion of a certificate program indicates completion of a course or series of courses with a specific concentration that is different from an educational degree program. Course content for an advanced certificate is set forth through a variety of sources i.e. faculty, committee, instructors, and other subject matter experts in a related field. The end goal of an advanced professional certificate is so that professionals may demonstrate knowledge of course content at the end of a set period in time.		Institutions that offer advance professional certificates in various fields and industries include:		There are many professional bodies for accountants and auditors throughout the world; some of them are legally recognized in their jurisdictions. Public accountants are the accountancy and control experts that are legally certified in different jurisdictions to work in public practices, certifying accounts as statutory auditors, eventually selling advice and services to other individuals and businesses. Today, however, many work within private corporations, financial industry, and government bodies.		Cf. Accountancy qualifications and regulation		Aviators are certified through theoretical and in-flight examinations. Requirements for certifications are quite equal in most countries and are regulated by each National Aviation Authority. The existing certificates or pilot licenses are:		Licensing in these categories require not only examinations but also a minimum number of flight hours. All categories are available for Fixed-Wing Aircraft (airplanes) and Rotatory-Wing Aircraft (helicopters). Within each category, aviators may also obtain certifications in:		Usually, aviators must be certified also in their log books for the type and model of aircraft they are allowed to fly. Currency checks as well as regular medical check-ups with a frequency of 6 months, 12 months, or 36 months, depending on the type of flying permitted, are obligatory. An aviator can fly only if holding:		In Europe, the ANSP, ATCO & ANSP technicians are certified according to ESARRs [2] (according to EU regulation 2096/2005 "Common Requirements").		In the United States, several communications certifications are conferred by the Electronics Technicians Association.		Certification is often used in the professions of software engineering and information technology.		Conferred by the International Dance Council CID[24] at UNESCO, the International Certification of Dance Studies[25] is awarded to students who have completed 150 hours of classes in a specific form of dance for Level 1. Another 150 hours are required for Level 2 and so on till Level 10. This is the only international certification for dance since the International Dance Council CID[24] is the official body for all forms of dance; it is usually given in addition to local or national certificates, that is why it is colloquially called "the dancer's passport". Students cannot apply for this certification directly - they have to ask their school to apply on their behalf. This certification is awarded free of charge, there is no cost other than membership fees.		International Dance Council CID[24] at UNESCO administers the International Certification of Dance Studies.		In the United States, several electronics certifications are provided by the Electronics Technicians Association.		The Federal Emergency Management Agency's EMI offers credentials and training opportunities for United States citizens. Students do not have to be employed by FEMA or be federal employees for some of the programs.[26]		Professional engineering is any act of planning, designing, composing, measuring, evaluating, inspecting, advising, reporting, directing or supervising, or managing any of the foregoing, that requires the application of engineering principles and that concerns the safeguarding of life, health, property, economic interests, the public interest or the environment.		Facility management can be defined as an aspect of engineering management science that deals with the planning, designing, coordination of space and maintenance of a built environment to enhance quality service management system. Service Quality System includes activities like security, maintenance, catering, and external as well as internal cleaning. In general, it is also the coordination and harmonization of various specialist disciplines to create the best possible working environment for staff.		Facility management is an interdisciplinary field devoted to the coordination of space, infrastructure, people and organization, often associated with the administration of office blocks, arenas, schools, convention centers, shopping complexes, hospitals, hotels, etc. However, FM facilitates on a wider range of activities than just business services and these are referred to as non-core functions.		A warehouse management system (WMS) is a part of the supply chain and primarily aims to control the movement and storage of materials within a warehouse and process the associated transactions, including shipping, receiving, putaway and picking. The systems also direct and optimize stock putaway based on real-time information about the status of bin utilization. A WMS monitors the progress of products through the warehouse. It involves the physical warehouse infrastructure, tracking systems, and communication between product stations.		More precisely, warehouse management involves the receipt, storage and movement of goods, (normally finished goods), to intermediate storage locations or to a final customer. In the multi-echelon model for distribution, there may be multiple levels of warehouses. This includes a central warehouse, a regional warehouses (serviced by the central warehouse) and potentially retail warehouses (serviced by the regional warehouses).		IECEx[32] covers the highly specialized field of explosion protection associated with the use of equipment in areas where flammable gases, liquids and combustible dusts may be present. This system provides the assurance that equipment is manufactured to meet safety standards, and that services such as installation, repair and overhaul also comply with IEC International Standards on safety. The United Nations, via UNECE (United Nations Economic Commission for Europe), recommends the IEC and IECEx as the world’s best practice model for the verification of conformity to International Standards. It published a “Common Regulatory Framework” encompassing the use of IEC International Standards developed by IEC TC (Technical Committee) 31: Equipment for explosive atmospheres, with proof of compliance demonstrated by IECEx.		AG (Accredited Genealogist) conferred by the International Commission for the Accreditation of Professional Genealogists (ICAPGen). CG (Certified Genealogist) conferred by the Board for Certification of Genealogists (BCG). CGL (Certified Genealogical Lecturer) conferred by the Board for Certification of Genealogists (BCG).		In the United States, insurance professionals are licensed separately by each state. Many individuals seek one or more certifications to distinguish themselves from their peers. The most recognizable certifications are issued by five organizations:		National Association of Mutual Insurance Companies		National Registry of Workers' Compensation Specialists		Professional Liability Underwriting Society (PLUS)		TESOL is a large field of employment with widely varying degrees of regulation. Most provision worldwide is through the state school system of each individual country, and as such, the instructors tend to be trained primary- or secondary school teachers who are native speakers of the language of their pupils, and not of English. Though native speakers of English have been working in non-English speaking countries in this capacity for years, it was not until the last twenty-five years or so that there was any widespread focus on training particularly for this field. Previously, workers in this sort of job were anyone from backpackers hoping to earn some extra travel money to well-educated professionals in other fields doing volunteer work, or retired people. These sort of people are certainly still to be found, but there are many who consider TESOL their main profession.		One of the problems[according to whom?] facing these full-time teachers is the absence of an international governing body for the certification or licensure of English language teachers. However, Cambridge University and its subsidiary body UCLES are pioneers in trying to get some degree of accountability and quality control to consumers of English courses, through their CELTA and DELTA programs. Trinity College London has equivalent programs, the CertTESOL and the LTCL DipTESOL. They offer initial certificates in teaching, in which candidates are trained in language awareness and classroom techniques, and given a chance to practice teaching, after which feedback is reported. Both institutions have as a follow-up a professional diploma, usually taken after a year or two in the field. Although the initial certificate is available to anyone with a high school education, the diploma is meant to be a post-graduate qualification and in fact can be incorporated into a master's degree program.		An increasing number of attorneys are choosing to be recognized as having special expertise in certain fields of law. According to the American Bar Association, a lawyer who is a certified specialist has been recognized by an independent professional certifying organization as having an enhanced level of skill and expertise, as well as substantial involvement in an established legal specialty. These organizations require a lawyer to demonstrate special training, experience and knowledge to ensure that the lawyer's recognition is meaningful and reliable. Lawyer conduct with regard to specialty certification is regulated by the states.		NBLSC is an American Bar Association (ABA) accredited organization providing Board Certification for US Lawyers. Board Certification is a rigorous testing and approval process that officially recognizes the extensive education and courtroom experience of attorneys. NBLSC provides Board Certification for Trial Lawyers & Trial Attorneys, Civil Lawyers, Criminal Lawyers, Family Lawyers and Social Security Disability Lawyers.		Logistician is the profession in the logistics and transport sectors, including sea, air, land and rail modes. Professional qualification for logisticians usually carries post-nominal letters. Common examples include:		[45] (CSCB),		Churches have their own process of who may use various religious titles. Protestant churches typically require a Masters of Divinity, accreditation by the denomination and ordination by the local church in order for a minister to become a "Reverend". Those qualifications may or may not also give government authorization to solemnize marriages		Board certification is the process by which a physician in the United States documents by written, practical or computer based testing, illustrating a mastery of knowledge and skills that define a particular area of medical specialization. The American Board of Medical Specialties, a not-for-profit organization, assists 24 approved medical specialty boards in the development and use of standards in the ongoing evaluation and certification of physicians.		Medical specialty certification in the United States is a voluntary process. While medical licensure sets the minimum competency requirements to diagnose and treat patients, it is not specialty specific.[46] Board certification demonstrates a physician’s exceptional expertise in a particular specialty or sub-specialty of medical practice.		Patients, physicians, health care providers, insurers and quality organizations regard certification as an important measure of a physician’s knowledge, experience and skills to provide quality health care within a given specialty.		Other professional certifications include certifications such as medical licenses, Membership of the Royal College of Physicians, Fellowship of the Royal College of Physicians and Surgeons of Canada, nursing board certification, diplomas in social work. The Commission for Certification in Geriatric Pharmacy certifies pharmacists that are knowledgeable about principles of geriatric pharmacotherapy and the provision of pharmaceutical care to the elderly. Additional certifying bodies relating to the medical field include:		NCPRP stands for “National Certified Peer Recovery Professional”, and the NCPRP credential and exam were developed in collaboration with the International Certification Board of Recovery Professionals (ICBRP) and is currently being administered by PARfessionals.		PARfessionals is a professional organization and all of the available courses are professional development and pre-certification courses.		The NCPRP credential and exam focus primarily on the concept of peer recovery through mental health and addiction recovery. It has the main purpose of training student-candidates on how to become peer recovery professionals who can provide guidance, knowledge or assistance for individuals who have had similar experiences.[50]		Through the support of the SJM Family Foundation, PARfessionals has developed the first globally recognized online training program for peer recovery professionals.[51]		Each student-candidate must complete several key steps which include initial registration; the pre-certification review course; and all applicable sections of the official application in order to become eligible to complete the final step, which is the NCPRP certification exam.[52]		The NCPRP credential is obtained once a participant successfully passes the NCPRP certification exam by the second attempt and is valid for five years.[53]		Certification is of significant importance in the project management (PM) industry. Certification refers to the evaluation and recognition of the skills, knowledge and competence of a practitioner in the field.		Project management certifications come in a variety of flavors:		Combination of competence-based, knowledge-based, and experience-based		Knowledge-based		There are 15 professional associations from around the world offering the 'Accredited in Public Relations' (APR) designation and one offering the 'Accredited Business Communicator' (ABC) designation. See Communications above for communications-related certifications (CMP and SCMP)		Customer relationship management (CRM) is a system for managing a company’s interactions with current and future customers. It often involves using technology to organize, automate and synchronize sales, marketing, customer service, and technical support.		Conferred by the National Speakers Association, the Certified Speaking Professional (CSP)[63] is the speaking profession's international measure of professional platform competence. This certification is awarded by the National Speakers Association.[64] Only about 10% of the speakers who belong to the Global Speakers Federation (GSF) hold this designation. Those who have earned their certification have done so by demonstrating a track record of experience and expertise.		Supply chain management (SCM) is the management of the flow of goods. It includes the movement and storage of raw materials, work-in-process inventory, and finished goods from point of origin to point of consumption. Interconnected or interlinked networks, channels and node businesses are involved in the provision of products and services required by end customers in a supply chain.[2] Supply chain management has been defined as the "design, planning, execution, control, and monitoring of supply chain activities with the objective of creating net value, building a competitive infrastructure, leveraging worldwide logistics, synchronizing supply with demand and measuring performance globally.		SCM draws heavily from the areas of operations management, logistics, procurement, and information technology, and strives for an integrated approach.		Australian Institute of Certified Practising Trainers administers the Certified Practising Trainer (CPT). Conferred by the Australian Institute of Certified Practising Trainers,[65] this certification is the hallmark for professional trainers.[citation needed]		Many political commentators, often criticize professional or occupational licensing, especially medical and legal licensing, for restricting the supply of services and therefore making them more expensive, often putting them out of reach of the poor.[66][67]		The current proliferation of IT certifications (both offered and attained){{1}}, like the FSI's IT baseline protection certification, has led some technologists to question their value. Proprietary content that has been distributed on the Internet allows some to gain credentials without the implied depth or breadth of expertise. Certifying agencies have responded in various ways: Some now incorporate hands-on elements, anti-cheating methodologies or have expanded their content. Others have expired and restructured their certificate programs or raised their fees to deter abuse.		Certification programs that take into account length of service and demonstrated experience via industry peer and employer recommendation avoid some of the issues associated with purely passing an examination; however, certification remains a contentious issue.		Also, some professional certifications require a criminal record check before the certification can be approved. The presence of a criminal history when applying for certification may be grounds for denial of certification.		
Workplace bullying is a persistent pattern of mistreatment from others in the workplace that causes either physical or emotional harm.[1] It can include such tactics as verbal, nonverbal, psychological, physical abuse and humiliation. This type of workplace aggression is particularly difficult because, unlike the typical school bully, workplace bullies often operate within the established rules and policies of their organization and their society. In the majority of cases, bullying in the workplace is reported as having been by someone who has authority over their victim. However, bullies can also be peers, and occasionally subordinates.[2] Research has also investigated the impact of the larger organizational context on bullying as well as the group-level processes that impact on the incidence and maintenance of bullying behaviour.[3] Bullying can be covert or overt. It may be missed by superiors; it may be known by many throughout the organization. Negative effects are not limited to the targeted individuals, and may lead to a decline in employee morale and a change in organizational culture.[4]						The first known documented use of "workplace bullying" is in 1992 in a book by Andrea Adams called Bullying at Work: How to Confront and Overcome It.[5][6]		While there is no universally accepted formal definition of workplace bullying, several researchers have endeavoured to define it:		Because it can occur in a variety of contexts and forms, it is also useful to define workplace bullying by the key features that these behaviours possess. Bullying is characterized by:[17]		This distinguishes bullying from isolated behaviours and other forms of job stress and allows the term workplace bullying to be applied in various contexts and to behaviours that meet these characteristics. Many observers agree that bullying is often a repetitive behaviour. However, some experts who have dealt with a great many people who report abuse also categorize some once-only events as bullying, for example with cases where there appear to be severe sequelae.[18] Expanding the common understanding of bullying to include single, severe episodes also parallels the legal definitions of sexual harassment in the US.		According to Pamela Lutgin-Sandvik,[19] the lack of unifying language to name the phenomenon of workplace bullying is a problem because without a unifying term or phrase, individuals have difficulty naming their experiences of abuse, and therefore have trouble pursuing justice against the bully. Unlike sexual harassment, which named a specific problem and is now recognized in law of many countries (including U.S.), workplace bullying is still being established as a relevant social problem and is in need of a specific vernacular.		Euphemisms intended to trivialize bullying and its impact on bullied people: incivility, disrespect, difficult people, personality conflict, negative conduct, and ill treatment. Bullied people are labelled as insubordinate when they resist the bullying treatment.		There is no exact definition for bullying behaviours in workplace, which is why different terms and definitions are common. For example, mobbing is a commonly used term in France and Germany, where it refers to a "mob" of bullies, rather than a single bully; this phenomenon is not often seen in other countries.[20][not in citation given] In the United States, aggression and emotional abuse are frequently used terms, whereas harassment is the term preferred in Finland. Workplace bullying is primarily used in Australia, UK, and Northern Europe.[21][not in citation given]		Bosses are the most common bullies. In fact, approximately 72% of bullies outrank their victims.[22] Statistics[23] from the 2007 WBI-Zogby survey show that 13% of U.S. employees report being bullied currently, 24% say they have been bullied in the past and an additional 12% say they have witnessed workplace bullying. Nearly half of all American workers (49%) report that they have been affected by workplace bullying, either being a target themselves or having witnessed abusive behaviour against a co-worker.		Although socio-economic factors may play a role in the abuse, researchers from the Project for Wellness and Work-Life[9] suggest that "workplace bullying, by definition, is not explicitly connected to demographic markers such as sex and ethnicity".[9] Because one in ten employees experiences workplace bullying, the prevalence of this issue is cause for great concern, even as initial data about this issue are reviewed.		According to the 2010 National Health Interview Survey Occupational Health Supplement (NHIS-OHS), the national prevalence rate for workers reporting having been threatened, bullied, or harassed by anyone on the job was 8%.[24]		In 2008, Dr. Judy Fisher-Blando[25] wrote a doctoral research dissertation on Aggressive behaviour: Workplace Bullying and Its Effect on Job Satisfaction and Productivity.[26] The scientific study determined that almost 75% of employees surveyed had been affected by workplace bullying, whether as a target or a witness. Further research showed the types of bullying behaviour, and organizational support.		In terms of gender, the Workplace Bullying Institute (2007)[23] states that women appear to be at greater risk of becoming a bullying target, as 57% of those who reported being targeted for abuse were women. Men are more likely to participate in aggressive bullying behaviour (60%), however when the bully is a woman her target is more likely to be a woman as well (71%).[27]		In the research of Samnani and Singh (2012), it concludes the findings from previous 20 years' literature and claims that in terms of the gender factor, inconsistent findings could not support the differences across gender.		The NHIS-OHS confirms the previous finding, as higher prevalence rates for being threatened, bullied, or harassed were identified for women (9%) compared with men (7%).[24]		Race also may play a role in the experience of workplace bullying. According to the Workplace Bullying Institute (2007),[23] the comparison of reported combined bullying (current + ever bullied) prevalence percentages reveals the pattern from most to least:		The reported rates of witnessing bullying were:		The percentages of those reporting that they have neither experienced nor witnessed mistreatment were		Research psychologist Tony Buon published one of the first reviews of bullying in China in the prestigious Journal PKU Business Review in 2005.[28]		Higher prevalence rates for experiencing a hostile work environment were identified for divorced or separated workers compared to married workers, widowed workers, and never married workers .[24]		Higher prevalence rates for experiencing a hostile work environment were identified for workers with some college education or workers with high school diploma or GED, compared to workers with less than a high school education.[24]		Lower prevalence rates for experiencing a hostile work environment were identified for workers aged 65 and older compared to workers in other age groups.[24]		With respect to age, conflicting findings have been reported. A study by Einarsen and Skogstad (1996) indicates older employees tend to be more likely to be bullied than younger ones.		Among industry groups, workers with higher prevalence rates of a hostile work environment, compared to all adults employed at some time in a 12-month period leading up to a survey in 2010 (8%), were in public administration (16%) and retail trade industries (10%). Lower prevalence rates of a hostile work environment were reported among those working in construction (5%); finance and insurance (5%); manufacturing (5%); and professional, scientific, and technical services industries (6%).[24]		For occupational groups, workers in protective service reported a higher prevalence rate (25%) of hostile work environments compared to the prevalence rate for all adults employed at some time in the past 12 months. Workers in community and social service occupations also experienced a relatively high rate (16%). Lower prevalence rates were observed among architecture and engineering (4%), computer and mathematical (4%), business and financial operations (5%), and construction and extraction (5%) occupations.[24]		Researchers Caitlin Buon and Tony Buon have suggested that attempts to profile ‘the bully’ have been damaging [29] They state that the "bully" profile is that ‘the bully’ is always aware of what they are doing, deliberately sets out to harm their ‘victims’, targets a particular individual or type of person and has some kind of underlying personality flaw, insecurity or disorder. But this is unproven and lacks evidence. The researchers suggest referring to workplace bullying as generic harassment along with other forms of non-specific harassment and this would enable employees to use less emotionally charged language and start a dialogue about their experiences rather than being repelled by the spectre of being labelled as a pathological predator or having to define their experiences as the victims of such a person. Tony Buon and Caitlin Buon also suggest that the perception and profile of the workplace bully is not facilitating interventions with the problem. They suggest that to make significant progress and achieve behaviour change over the long term then organisations and individuals need to embrace the notion that everyone must all potentially house ‘the bully’ within them and their organisations. It exists in workplace cultures, belief systems, interactions and emotional competencies and cannot be transformed if externalization and demonization continue the problem by profiling ‘the bully’ rather than talking about behaviours and interpersonal interactions.[29]		From the research by Hoel, it is clear that most of the perpetrators are supervisors, the second one is peers, subordinates and customers follow which found from Hoel's research.[30] So three main relationships among the participants in workplace bullying can be indicated as:		Bullying behaviour shows as an abuse of power between supervisors and subordinates in workplace. Supervisors release their own pressure to bully subordinates with their higher power due to workplace bullying. It is always related to management style of the supervisors. An authoritative management style is accompanied by a kind of bullying behaviours which can make subordinates fear so that supervisors can become authority themselves. On the other hand, some researchers agree that bullying behaviours is a positive performance in workplace. Workplace bullying can attribute to the organizational power and control. It is also a representative of power and control. if an organization want to improve this situation in workplace, strategies and policies must be improved. Lacking of policy in bullying like low-monitoring or no punishment will result in tolerating in organization. Bullying behaviours in workplace also exist among colleagues. They can be either the ‘target’ or perpetrator. If workplace bullying happens among the co-workers, witness will take side between target and perpetrator. Perpetrators always win, because witnesses do not want to be the next target. This way, it does encourage perpetrators to continue this behaviour. In addition, the sense of the injustice by targets might become another perpetrator to bully other colleagues who have less power than them. Varitia who is a workplace bullying researcher investigate that 20% of interviewees who experienced workplace bullying thought the reason why they became a target is they are different from others.[31] In a word, bullying can increase more bullying in workplace. The third relationship in workplace is between employees and customers. Although it takes a little part, it plays a significant role about the efficiency of the organization. If an employee work with unhealthy emotion, it will affect the quality of the service seriously. This relationship is closely related to emotion label. Lots of examples can be listed from our daily life, like customers are ignored by shop assistants, patients are shouted by nurses in the hospital and so on. On the other hand, customers might despise the employees, especially blue-collar job, such as gas station assistants. Bullying behaviours in workplace can generate effect mutually between the employees and customers. The Fourth relationship in workplace is between organization or its institution or its system and the employees. In the article of Andreas Liefooghe (2012), it notes that a lot of employees describe their organization as bully. It is not environmental factors facilitating the bullying but it is the bullying itself. Tremendous power imbalance enables company to "legitimately exercise" their power in the way of monitoring and controlling as bullying. The terms of the bullying "traditionally" imply to interpersonal relationship. Talking about bullying in interpersonal level is legitimate, but talking about the exploitation, justice and subjugation as bullying of organization would be "relatively ridiculous" or not taken as serious. Bullying is sometimes more than purely interpersonal issue.		Bullying is seen to be prevalent in organisations where employees and managers feel that they have the support, or at least the implicit blessing, of senior managers to carry on their abusive and bullying behaviour.[4] Furthermore, new managers will quickly come to view this form of behaviour as acceptable and normal if they see others get away with it and are even rewarded for it.[32]		When bullying happens at the highest levels, the effects may be far reaching. That people may be bullied irrespective of their organisational status or rank, including senior managers, indicates the possibility of a negative domino effect, where bullying may be cascaded downwards as the targeted supervisors might offload their own aggression on their subordinates. In such situations, a bullying scenario in the boardroom may actually threaten the productivity of the entire organisation.[33]		According to a research investigating the acceptability of the bullying behaviour across from different cultures (Power et al., 2013), it clearly shows that culture could also serve as an influencing factor. The difference on the cultural dimension across different cultures could affect the perception on the acceptable behaviour. National-level factors, such as culture, may also represent a predictor of workplace bullying (Harvey et al., 2009; Hoel et al., 1999; Lutgen-Sandvik et al., 2007).		Humane orientation is negatively associated with the acceptability of bullying for WRB (Work related bullying). Performance orientation is positively associated with the acceptance of bullying. Future orientation is negatively associated with the acceptability of bullying. A culture of femininity suggests that individuals tend to value interpersonal relationships to a greater degree.		Three broad dimensions have been mentioned in relation to workplace bullying: power distance; masculinity versus femininity; and individualism versus collectivism (Lutgen-Sandvik et al., 2007).		In Confucian Asia, which has a higher performance orientation than Latin America and Sub-Saharan Africa, bullying may be seen as an acceptable price to pay for performance. The value Latin America holds for personal connections with employees and the higher humane orientation of Sub-Saharan Africa may help to explain their distaste for bullying. A culture of individualism in the US implies competition, which may increase the likelihood of workplace bullying situations.		Ashforth discussed potentially destructive sides of leadership and identified what he referred to as petty tyrants, i.e.Leaders who exercise a tyrannical style of management, resulting in a climate of fear in the workplace.[34] Partial or intermittent negative reinforcement can create an effective climate of fear and doubt.[35] When employees get the sense that bullies "get away with it", a climate of fear may be the result.[33] Several studies have confirmed a relationship between bullying, on the one hand, and an autocratic leadership and an authoritarian way of settling conflicts or dealing with disagreements, on the other. An authoritarian style of leadership may create a climate of fear, where there is little or no room for dialogue and where complaining may be considered futile.[32]		In a study of public-sector union members, approximately one in five workers reported having considered leaving the workplace as a result of witnessing bullying taking place. Rayner explained these figures by pointing to the presence of a climate of fear in which employees considered reporting to be unsafe, where bullies had "got away with it" previously despite management knowing of the presence of bullying.[33]		The workplace bully is often expert at knowing how to work the system. They can spout all the current management buzzwords about supportive management but basically use it as a cover. By keeping their abusive behaviour hidden, any charges made by individuals about his or her bullying will always come down to your word against his. They may have a kiss up kick down personality, wherein they are always highly cooperative, respectful, and caring when talking to upper management but the opposite when it comes to their relationship with those whom they supervise.[36] Bullies tend to ingratiate themselves to their bosses while intimidating subordinates.[37][38] They may be socially popular with others in management, including those who will determine their fate. Often, a workplace bully will have mastered kiss up kick down tactics that hide their abusive side from superiors who review their performance.[39]		As a consequence of this kiss up kick down strategy:[40]		The most typical reactions to workplace bullying are to do with the survival instinct - "fight or flight" - and these are probably a victim’s healthier responses to bullying. Flight is a legitimate and valid response to bullying. It is very common, especially in organizations in which upper management cannot or will not deal with the bullying. In hard economic times, however, flight may not be an option, and fighting may be the only choice.[41]		Fighting the bullying can require near heroic action, especially if the bullying targets just one or two individuals. It can also be a difficult challenge. There are some times when confrontation is called for. First, there is always a chance that the bully boss is labouring under the impression that this is the way to get things done and does not recognize the havoc being wreaked on subordinates.[41]		With some variations, the following typology of workplace bullying behaviours has been adopted by a number of academic researchers. The typology uses five different categories.[42] [43]		Research by the Workplace Bullying Institute, suggests that the following are the 25 most common workplace bullying tactics:[44]		According to Bassman, common abusive workplace behaviours are:[45]		According to Hoel and Cooper, common abusive workplace behaviours are:[46]		Abusive cyberbullying in the workplace can have serious socioeconomic and psychological consequences on the victim. Workplace cyberbullying can lead to sick leave due to depression which in turn can lead to loss of profits for the organisation.[47]		Several aspects of academia, such as the generally decentralized nature of academic institutions[48][49] and the particular recruitment and career procedures,[50] lend themselves to the practice of bullying and discourage its reporting and mitigation.		Bullying has been identified as prominent in blue collar jobs including on the oil rigs and in mechanic shops and machine shops. It is thought that intimidation and fear of retribution cause decreased incident reports. This is also an industry dominated by males, where disclosure of incidents are seen as effeminate, which, in the socioeconomic and cultural milieu of such industries, would likely lead to a vicious circle. This is often used in combination with manipulation and coercion of facts to gain favour among higher ranking administrators.[51][non-primary source needed]		A culture of bullying is common in information technology (IT), leading to high sickness rates, low morale, poor productivity and high staff turnover.[52] Deadline-driven project work and stressed-out managers take their toll on IT workers.[53]		Bullying in the medical profession is common, particularly of student or trainee doctors. It is thought that this is at least in part an outcome of conservative traditional hierarchical structures and teaching methods in the medical profession which may result in a bullying cycle.		Bullying has been identified as being particularly prevalent in the nursing profession although the reasons are not clear. It is thought that relational aggression (psychological aspects of bullying such as gossiping and intimidation) are relevant. Relational aggression has been studied amongst girls but not so much amongst adult women.[54][55]		School teachers are commonly the subject of bullying but they are also sometimes the originators of bullying within a school environment.		Bullying in the legal profession is believed to be more common than in some other professions. It is believed that its adversarial, hierarchical tradition contributes towards this.[56] Women, trainees and solicitors who have been qualified for five years or less are more impacted, as are ethnic minority lawyers and lesbian, gay and bisexual lawyers.[57]		Bullying exists to varying degrees in the military of some countries, often involving various forms of hazing or abuse by higher members of the military hierarchy.		Tim Field suggested that workplace bullying takes these forms:[58]		Adult bullying can come in an assortment of forms. There are about five distinctive types of adult bullies. A narcissistic bully is described as a self-centred person whose egotism is frail and possesses the need to put others down. An impulsive bully is someone who acts on bullying based on stress or being upset at the moment. A physical bully uses physical injury and the threat of harm to abuse their victims, while a verbal bully uses demeaning and cynicism to debase their victims. Lastly, a secondary adult bully is portrayed as a person that did not start the initial bullying but participates in afterwards to avoid being bullied themselves ("Adult Bullying"). [61]		Workplace bullying is reported to be far more prevalent than perhaps commonly thought.[62] For some reason, workplace bullying seems to be particularly widespread in healthcare organizations; 80% of nurses report experiencing workplace bullying.[62] Similar to the school environment for children, the work environment typically places groups of adult peers together in a shared space on a regular basis. In such a situation, social interactions and relationships are of great importance to the function of the organizational structure and in pursuing goals. The emotional consequences of bullying put an organization at risk of losing victimized employees.[62] Bullying also contributes to a negative work environment, is not conducive to necessary cooperation and can lessen productivity at various levels.[62] Bullying in the workplace is associated with negative responses to stress.[62] The ability to manage emotions, especially emotional stress, seems to be a consistently important factor in different types of bullying. The workplace in general can be a stressful environment, so a negative way of coping with stress or an inability to do so can be particularly damning. Workplace bullies may have high social intelligence and low emotional intelligence (EI).[63] In this context, bullies tend to rank high on the social ladder and are adept at influencing others. The combination of high social intelligence and low empathy is conducive to manipulative behaviour, such that Hutchinson (2013) describes workplace bullying to be.[63] In working groups where employees have low EI, workers can be persuaded to engage in unethical behaviour.[63] With the bullies' persuasion, the work group is socialized in a way that rationalizes the behaviour, and makes the group tolerant or supportive of the bullying.[63] Hutchinson & Hurley (2013) make the case that EI and leadership skills are both necessary to bullying intervention in the workplace, and illustrates the relationship between EI, leadership and reductions in bullying. EI and ethical behaviour among other members of the work team have been shown to have a significant impact on ethical behaviour of nursing teams.[64] Higher EI is linked to improvements in the work environment and is an important moderator between conflict and reactions to conflict in the workplace.[62] The self-awareness and self-management dimensions of EI have both been illustrated to have strong positive correlations with effective leadership and the specific leadership ability to build healthy work environments and work culture.[62]		Abusive supervision overlaps with workplace bullying in the workplace context. Research suggests that 75% of workplace bullying incidents are perpetrated by hierarchically superior agents. Abusive supervision differs from related constructs such as supervisor bullying and undermining in that it does not describe the intentions or objectives of the supervisor.[65]		A power and control model has been developed for the workplace, divided into the following categories:[66]		Workplace mobbing overlaps with workplace bullying. The concept originated from the study of animal behaviour. It concentrates on bullying by a group.		Workplace bullying overlaps to some degree with workplace incivility but tends to encompass more intense and typically repeated acts of disregard and rudeness. Negative spirals of increasing incivility between organizational members can result in bullying,[67] but isolated acts of incivility are not conceptually bullying despite the apparent similarity in their form and content. In case of bullying, the intent of harm is less ambiguous, an unequal balance of power (both formal and informal) is more salient, and the target of bullying feels threatened, vulnerable and unable to defend himself or herself against negative recurring actions.[42][43]		In 2005, psychologists Belinda Board and Katarina Fritzon at the University of Surrey, UK, interviewed and gave personality tests to high-level British executives and compared their profiles with those of criminal psychiatric patients at Broadmoor Hospital in the UK. They found that three out of eleven personality disorders were actually more common in executives than in the disturbed criminals. They were:		They described these business people as successful psychopaths and the criminals as unsuccessful psychopaths.[68]		According to leading leadership academic Manfred F.R. Kets de Vries, it seems almost inevitable these days that there will be some personality disorders in a senior management team.[69]		Industrial/organizational psychology research has also examined the types of bullying that exist among business professionals and the prevalence of this form of bullying in the workplace as well as ways to measure bullying empirically.[70]		Narcissism, lack of self-regulation, lack of remorse and lack of conscience have been identified as traits displayed by bullies. These traits are shared with psychopaths, indicating that there is some theoretical cross-over between bullies and psychopaths.[71] Bullying is used by corporate psychopaths as a tactic to humiliate subordinates.[72] Bullying is also used as a tactic to scare, confuse and disorient those who may be a threat to the activities of the corporate psychopath[72] Using meta data analysis on hundreds of UK research papers, Boddy concluded that 36% of bullying incidents were caused by the presence of corporate psychopaths. According to Boddy there are two types of bullying:[73]		A corporate psychopath uses instrumental bullying to further his goals of promotion and power as the result of causing confusion and divide and rule.		People with high scores on a psychopathy rating scale are more likely to engage in bullying, crime and drug use than other people.[74] Hare and Babiak noted that about 29 per cent of corporate psychopaths are also bullies.[75] Other research has also shown that people with high scores on a psychopathy rating scale were more likely to engage in bullying, again indicating that psychopaths tend to be bullies in the workplace.[74]		A workplace bully or abuser will often have issues with social functioning. These types of people often have psychopathic traits that are difficult to identify in the hiring and promotion process. These individuals often lack anger management skills and have a distorted sense of reality. Consequently, when confronted with the accusation of abuse, the abuser is not aware that any harm was done.[76]		In 2007, researchers Catherine Mattice and Brian Spitzberg at San Diego State University, USA, found that narcissism revealed a positive relationship with bullying. Narcissists were found to prefer indirect bullying tactics (such as withholding information that affects others' performance, ignoring others, spreading gossip, constantly reminding others of mistakes, ordering others to do work below their competence level, and excessively monitoring others' work) rather than direct tactics (such as making threats, shouting, persistently criticizing, or making false allegations). The research also revealed that narcissists are highly motivated to bully, and that to some extent, they are left with feelings of satisfaction after a bullying incident occurs.[77]		According to Namie, Machiavellians manipulate and exploit others to advance their perceived personal agendas but he emphasizes that they are not mentally ill. They do not have a personality disorder, schizophrenia and neither are they psychopaths. In his view, Machiavellianism represents the core of workplace bullying.[78]		According to Gary and Ruth Namie, as well as Tracy, et al.,[79] workplace bullying can harm the health of the targets of bullying. Organizations are beginning to take note of workplace bullying because of the costs to the organization in terms of the health of their employees.		According to scholars at The Project for Wellness and Work-Life at Arizona State University, "workplace bullying is linked to a host of physical, psychological, organizational, and social costs." Stress is the most predominant health effect associated with bullying in the workplace. Research indicates that workplace stress has significant negative effects that are correlated to poor mental health and poor physical health, resulting in an increase in the use of "sick days" or time off from work (Farrell & Geist-Martin, 2005).		The negative effects of bullying are so severe that posttraumatic stress disorder (PTSD) and even suicide[80] are not uncommon. Tehrani[81] found that 1 in 10 targets experience PTSD, and that 44% of her respondents experienced PTSD similar to that of battered women and victims of child abuse. Matthiesen and Einarsen[82] found that up to 77% of targets experience PTSD.		In addition, co-workers who witness workplace bullying can also have negative effects, such as fear, stress, and emotional exhaustion.[11] Those who witness repetitive workplace abuse often choose to leave the place of employment where the abuse took place. Workplace bullying can also hinder the organizational dynamics such as group cohesion, peer communication, and overall performance.		According to the 2012 survey conducted by Workplace Bullying Institute (516 respondents), Anticipation of next negative event is the most common psychological symptom of workplace bullying reported by 80%. Panic attacks afflict 52%. Half (49%) of targets reported being diagnosed with clinical depression. Sleep disruption, loss of concentration, mood swings, and pervasive sadness and insomnia were more common (ranging from 77% to 50%). Nearly three-quarters (71%) of targets sought treatment from a physician. Over half (63%) saw a mental health professional for their work-related symptoms. Respondents reported other symptoms that can be exacerbated by stress: migraine headaches (48%), irritable bowel disorder (37%), chronic fatigue syndrome (33%) and sexual dysfunction (27%).		Several studies have attempted to quantify the cost of bullying to an organization.		Research by Dr. Dan Dana has shown organizations suffer a large financial cost by not accurately managing conflict and bullying type behaviours. He has developed a tool to assist with calculating the cost of conflict.[84] In addition, researcher Tamara Parris discusses how employers need to be more attentive in managing various discordant behaviours in the workplace, such as, bullying, as it not only creates a financial cost to the organization, but also erodes the company's human resources assets.[85]		
Unemployment benefits (depending on the jurisdiction also called unemployment insurance or unemployment compensation) are payments made by the state or other authorized bodies to unemployed people. In the United States, benefits are funded by a compulsory governmental insurance system, not taxes on individual citizens. Depending on the jurisdiction and the status of the person, those sums may be small, covering only basic needs, or may compensate the lost time proportionally to the previous earned salary.		Unemployment benefits are generally given only to those registering as unemployed, and often on conditions ensuring that they seek work and do not currently have a job, and are validated as being laid off and not fired for cause in most states.		In some countries, a significant proportion of unemployment benefits are distributed by trade/labour unions, an arrangement known as the Ghent system.						The first unemployment benefit scheme was introduced in the United Kingdom with the National Insurance Act 1911 under the Liberal Party government of H. H. Asquith. The popular measures were to combat the increasing influence of the Labour Party among the country's working-class population. The Act gave the British working classes a contributory system of insurance against illness and unemployment. It only applied to wage earners, however, and their families and the unwaged had to rely on other sources of support, if any.[1] Key figures in the implementation of the Act included Robert Laurie Morant, and William Braithwaite.		By the time of its implementation, the benefit was criticized by communists, who thought such insurance would prevent workers from starting a revolution, while employers and tories saw it as a "necessary evil".[2]		The scheme was based on actuarial principles and it was funded by a fixed amount each from workers, employers, and taxpayers. It was restricted to particular industries, particularly more volatile ones like shipbuilding, and did not make provision for any dependants. After one week of unemployment, the worker was eligible for receiving 7 shillings/week for up to 15 weeks in a year. By 1913, 2.3 million were insured under the scheme for unemployment benefit.		The Unemployment Insurance Act 1920 created the dole system of payments for unemployed workers.[3] The dole system provided 39 weeks of unemployment benefits to over 11 million workers—practically the entire civilian working population except domestic service, farm workers, railroad men, and civil servants.		Unemployment benefits were introduced in Germany in 1927, and in most European countries in the period after the Second World War with the expansion of the welfare state. Unemployment insurance in the United States originated in Wisconsin in 1932.[4] Through the Social Security Act of 1935, the federal government of the United States effectively encouraged the individual states to adopt unemployment insurance plans.		In Argentina, successive administrations have used a variety of passive and active labour market interventions to protect workers against the consequences of economic shocks and the government's key institutional response to combat the increase in poverty and unemployment created by the crisis was the launch of an active unemployment assistance programme called Plan Jefas y Jefes de Hogar Desocupados (Program for Unemployed Heads of Households).		In Australia, social security benefits, including unemployment benefits, are funded through the taxation system. There is no compulsory national unemployment insurance fund. Rather, benefits are funded in the annual Federal Budget by the National Treasury and are administrated and distributed throughout the nation by the government agency, Centrelink. Benefit rates are indexed to the Consumer Price Index and are adjusted twice a year according to inflation or deflation.		There are two types of payment available to those experiencing unemployment. The first, called Youth Allowance, is paid to young people aged 16–20 (or 15, if deemed to meet the criteria for being considered 'independent' by Centrelink). Youth Allowance is also paid to full-time students aged 16–24, and to full-time Australian Apprenticeship workers aged 16–24. People aged below 18 who have not completed their High School education, are usually required to be in full-time education, undertaking an apprenticeship or doing training to be eligible for Youth Allowance. For single people under 18 years of age living with a parent or parents the basic rate is A$91.60 per week. For over-18- to 20-year-olds living at home this increases to A$110.15 per week. For those aged 18–20 not living at home the rate is A$167.35 per week. There are special rates for those with partners and/or children.		The second kind of payment is called Newstart Allowance and is paid to unemployed people over the age of 21 and under the pension eligibility age. To receive a Newstart payment, recipients must be unemployed, be prepared to enter into an Employment Pathway Plan (previously called an Activity Agreement) by which they agree to undertake certain activities to increase their opportunities for employment, be Australian Residents and satisfy the income test (which limits weekly income to A$32 per week before benefits begin to reduce, until one's income reaches A$397.42 per week at which point no unemployment benefits are paid) and the assets test (an eligible recipient can have assets of up to A$161,500 if he or she owns a home before the allowance begins to reduce and $278,500 if he or she does do not own a home). The rate of Newstart allowance as at the 12th January 2010 for single people without children is A$228 per week, paid fortnightly. (This does not include supplemental payments such as Rent Assistance.) Different rates apply to people with partners and/or children.		The system in Australia is designed to support recipients no matter how long they have been unemployed. In recent years the former Coalition government under John Howard has increased the requirements of the Activity Agreement, providing for controversial schemes such as Work for the Dole, which requires that people on benefits for 6 months or longer work voluntarily for a community organisation regardless of whether such work increases their skills or job prospects. Since the Labor government under Kevin Rudd was elected in 2008, the length of unemployment before one is required to fulfill the requirements of the Activity Agreement (which has been renamed the Employment Pathway Plan) has increased from six to twelve months. There are other options available as alternatives to the Work for the Dole scheme, such as undertaking part-time work or study and training, the basic premise of the Employment Pathway Plan being to keep the welfare recipient active and involved in seeking full-time work.		For people renting their accommodation, unemployment benefits are supplemented by Rent Assistance, which, for single people as at 29 June 2012, begins to be paid when weekly rent is more than A$53.40. Rent Assistance is paid as a proportion of total rent paid (75 cents per dollar paid over $53.40 up to the maximum). The maximum amount of rent assistance payable is A$60.10 per week, and is paid when the total weekly rent exceeds A$133.54 per week. Different rates apply to people with partners and/or children, or who are sharing accommodation.		External links		In Canada, the system now known as Employment Insurance was formerly called Unemployment Insurance. The name was changed in 1996, in order to alleviate perceived negative connotations. In 2015, Canadian workers pay premiums of 1.88%[5] of insured earnings in return for benefits if they lose their jobs.		The Employment and Social Insurance Act was passed in 1935 during the Great Depression by the government of R.B. Bennett as an attempted Canadian unemployment insurance programme. It was, however, ruled unconstitutional by the Supreme Court of Canada as unemployment was judged to be an insurance matter falling under provincial responsibility. After a constitutional amendment was agreed to by all of the provinces, a reference to "Unemployment Insurance" was added to the matters falling under federal authority under the Constitution Act, 1867, and the first Canadian system was adopted in 1940. Because of these problems Canada was the last major Western country to bring in an employment insurance system. It was extended dramatically by Pierre Trudeau in 1971 making it much easier to get. The system was sometimes called the 10/42, because one had to work for 10 weeks to get benefits for the other 42 weeks of the year. It was also in 1971 that the UI program was first opened up to maternity and sickness benefits, for 15 weeks in each case.		The generosity of the Canadian UI programme was progressively reduced after the adoption of the 1971 UI Act. At the same time, the federal government gradually reduced its financial contribution, eliminating it entirely by 1990. The EI system was again cut by the Progressive Conservatives in 1990 and 1993, then by the Liberals in 1994 and 1996. Amendments made it harder to qualify by increasing the time needed to be worked, although seasonal claimants (who work long hours over short periods) turned out to gain from the replacement, in 1996, of weeks by hours to qualify. The ratio of beneficiaries to unemployed, after having stood at around 40 percent for many years, rose somewhat during the 2009 recession but then fell back again to the low 40s.[6] Some unemployed persons are not covered for benefits (e.g. self-employed workers), while others may have exhausted their benefits, did not work long enough to qualify, or quit or were dismissed from their job. The length of time one could take EI has also been cut repeatedly. The 1994 and 1996 changes contributed to a sharp fall in Liberal support in the Atlantic provinces in the 1997 election.		In 2001, the federal government increased parental leave from 10 to 35 weeks, which was added to preexisting maternity benefits of 15 weeks. In 2004, it allowed workers to take EI for compassionate care leave while caring for a dying relative, although the strict conditions imposed make this a little used benefit. In 2006, the Province of Quebec opted out of the federal EI scheme in respect of maternity, parental and adoption benefits, in order to provide more generous benefits for all workers in that province, including self-employed workers. Total EI spending was $19.677 billion for 2011-2012 (figures in Canadian dollars).[7]		Employers contribute 1.4 times the amount of employee premiums. Since 1990, there is no government contribution to this fund. The amount a person receives and how long they can stay on EI varies with their previous salary, how long they were working, and the unemployment rate in their area. The EI system is managed by Service Canada, a service delivery network reporting to the Minister of Employment and Social Development Canada.		A bit over half of EI benefits are paid in Ontario and the Western provinces but EI is especially important in the Atlantic provinces, which have higher rates of unemployment. Many Atlantic workers are also employed in seasonal work such as fishing, forestry or tourism and go on EI over the winter when there is no work. There are special rules for fishermen making it easier for them to collect EI. EI also pays for maternity and parental leave, compassionate care leave, and illness coverage. The programme also pays for retraining programmes (EI Part II) through labour market agreements with the Canadian provinces.		A significant part of the federal fiscal surplus of the Jean Chrétien and Paul Martin years came from the EI system. Premiums were reduced much less than falling expenditures - producing, from 1994 onwards, EI surpluses of several billion dollars per year, which were added to general government revenue.[8] The cumulative EI surplus stood at $57 billion at March 31, 2008,[9] nearly four times the amount needed to cover the extra costs paid during a recession.[10] This drew criticism from Opposition parties and from business and labour groups, and has remained a recurring issue of the public debate. The Conservative Party,[11] chose not to recognize those EI surpluses after being elected in 2006. Instead, the Conservative government cancelled the EI surpluses entirely in 2010, and required EI contributors to make up the 2009, 2010 and 2011 annual deficits by increasing EI premiums. On December 11, 2008, the Supreme Court of Canada rejected a court challenge launched against the federal government by two Quebec unions, who argued that EI funds had been misappropriated by the government.[12]		External links		The level of benefit is set between the minimum wage and the minimum living allowance by individual provinces, autonomous regions and municipalities.[citation needed]		Each Member State of the European Union has its own system and in general a worker should claim unemployment benefits in the country where they last worked. For a person working in a country other than their country of residency (a cross-border worker), they will have to claim benefits in their country of residence. More specific rules and information is available on the Your Europe portal.		Two systems run in parallel, combining a Ghent system and a minimum level of support provided by Kela, an agency of the national government. Unionization rates are high (70%), and union membership comes with membership in an unemployment fund. Additionally, there are non-union unemployment funds. Usually benefits require 26 weeks of 18 hours per week on average, and the unemployment benefit is 60% of the salary and lasts for 500 days.[13] When this is not available, Kela can pay either regular unemployment benefit or labor market subsidy benefits. The former requires a degree and two years of full-time work. The latter requires participation in training, education, or other employment support, which may be mandated on pain of losing the benefit, but may be paid after the regular benefits have been either maxed out or not available.[14] Although the unemployment funds handle the payments, most of the funding is from taxes and compulsory tax-like unemployment insurance charges.		Regardless of whether benefits are paid by Kela or from an unemployment fund, the unemployed person receives assistance from the Työ- ja elinkeinokeskus (TE-keskus, or the "Work and Livelihood Centre"), a government agency which helps people to find jobs and employers to find workers. In order to be considered unemployed, the seeker must register at the TE-keskus as unemployed. If the jobseeker does not have degree, the agency can require the jobseeker to apply to a school.		If the individual does not qualify for any unemployment benefit he may still be eligible for the housing benefit (asumistuki) from Kela and municipal social welfare provisions (toimeentulotuki). They are not unemployment benefits and depend on household income, but they have in practice become the basic income of many long-term unemployed.		France uses a quasi Ghent system, under which unemployment benefits are distributed by an independent agency (UNEDIC) in which unions and Employer organisations are equally represented. UNEDIC is responsible for 3 benefits: ARE, ACA and ASR The main ARE scheme requires a minimum of 122 days membership in the preceding 24 months and certain other requirements before any claims can be made. Employers pay a contribution on top of the pre-tax income of their employees, which together with the employee contribution, fund the scheme.		The maximum unemployment benefit is (as of March 2009) 57.4% of EUR 162 per day (Social security contributions ceiling in 2011), or 6900 euros per month.[15] Claimants receive 57,4% of their average daily salary of the last 12 months preceding unemployment with the average amount being 1,111 euros per month.[16] In France tax and other payroll taxes are paid on unemployment benefits. In 2011 claimants received the allowance for an average 291 days.		Germany has two different types of unemployment benefits.		The unemployment benefit I in Germany is also known as the unemployment insurance. The insurance is administered by the federal employment agency and funded by employee and employer contributions. This in stark contrast to FUTA in the US and other systems; where only employers make contributions. Participation (and thus contributions) are generally mandatory for both employee and employer.		All workers with a regular employment contract (abhängig Beschäftigte), except freelancers and certain civil servants (Beamte), contribute to the system. Since 2006, certain previously excluded workers have been able to opt into the system on a voluntary basis.		The system is financed by contributions from employees and employers. Employees pay 1.5% of their gross salary below the social security threshold and employers pay 1.5% contribution on top of the salary paid to the employee. The contribution level was reduced from 3.25% for employees and employers as part of labour market reforms known as Hartz. Contributions are paid only on earnings up to the social security ceiling (2012: 5,600 EUR).		The system is largely self-financed but also receives a subsidy from the state to run the Jobcenters.		Unemployed workers are entitled to:		Unemployed benefit is paid to workers who have contributed at least during 12 months preceding their loss of a job. The allowance is paid for half of the period that the worker has contributed. Claimants get 60% of their previous net salary (capped at the social security ceiling), or 67% for claimants with children. The maximum benefit is therefore 2964 Euros (in 2012). In 2011 the federal Work Agency had revenues and expenses of 37.5 bn EUR[17]		After a change in German law effective since 2008, provided their job history qualifies them, benefit recipients aged 50 to 54 now receive an unemployment benefit for 15 months, those 55 to 57 for 18 months and those 58 or older receive benefits for 24 months. For those under the age of 50 who have not been employed for more than 30 months in a job which paid into the social security scheme, full unemployment benefit can be received for a maximum period of 12 months. Note how the duration of eligibility is variegated in Germany to account for the difficulty older people have re-entering the job market.		If a worker is not eligible for the full unemployment benefits or after receiving the full unemployment benefit for the maximum of 12 months, he is able to apply for benefits from the so-called Arbeitslosengeld II (Hartz IV) programme, an open-ended welfare programme which, unlike the US system, ensures people do not fall into penury. A person receiving Hartz IV benefits is paid 404 EUR (2016) a month for living expenses plus the cost of adequate housing (including heating) and health care. Couples can receive benefits for each partner including their children. Additionally, children can get "benefits for education and participation". Germany does not have an EBT (electronic benefits transfer) card system in place and, instead, disburses welfare in cash or via direct deposit onto the recipient's bank account. People who receive Hartz 4 are obligated to seek for jobs and can be forced to take part in social programs or Mini jobs in order to receive this Hartz 4 money. Most of these programs and Mini jobs are lasting like a full time job each day, 5 days a week to simulate real jobs.		Unemployment benefits in Greece are administered through ΟΑΕΔ (Greek: Οργανισμός Απασχόλησης Εργατικού Δυναμικού, Manpower Employment Organization) and are available only to laid-off salaried workers with full employment and social security payments during the previous two years. The self-employed do not qualify, and neither do those with other sources of income. The monthly benefit is fixed at the "55% of 25 minimum daily wages", and is currently 360 euros per month,[18][19] with a 10% increase for each under-age child. Recipients are eligible for at most twelve months; the exact duration depends on the collected number of ensema ένσημα, that is social security payment coupons-stamps collected (i.e. days of work) during the 14 months before being laid off; the minimum number of such coupons, under which there is no eligibility, is 125, collected in the first 12 of the 14 aforementioned months. Eligibility since January 1, 2013, has been further constrained in that one applying for unemployment benefits for a second or more time, must not have received more than the equivalent of 450 days of such benefits during the last four years since the last time one had started receiving such benefits; if one has received unemployment benefits in this period for more than 450 days then there is no eligibility while if one has received less, then one is only eligible for at most the remaining days up until the maximum of 450 days is reached.[19]		People aged 18 and over and who are unemployed in Ireland can apply for either the Jobseeker's Allowance (Liúntas do Lucht Cuardaigh Fostaíochta) or the Jobseeker's Benefit (Sochar do Lucht Cuardaigh Fostaíochta). Both are paid by the Department of Social Protection and are nicknamed "the dole".		Unemployment benefit in Ireland can be claimed indefinitely for as long as the individual remains unemployed. The standard payment is €188 per week for those aged 26 and over. For those aged 18 to 24 the rate is €100 per week. For those aged 25 the weekly rate is €144. Payments can be increased if the unemployed has dependents. For each adult dependent, another €124.80 is added, €100 if the recipient (as opposed to the dependent) is aged 18 to 24, and for each child dependent €29.80 is added.		There are more benefits available to unemployed people, usually on a special or specific basis. Benefits include the Rent Supplement, the Mortgage Interest Supplement and the Fuel Allowance, among others. People on a low income (which includes those on JA/JB) are entitled to a Medical Card (although this must be applied for separately from the Health Service Executive) which provides free health care, optical care, limited dental care, aural care and subsidised prescription drugs carrying a €2.50 per item charge to a maximum monthly contribution of €25 per household (as opposed to subsidised services like non medical-card holders).		To qualify for Jobseekers Allowance, claimants must satisfy the "Habitual Residence Condition": they must have been legally in the state (or the Common Travel Area) for two years or have another good reason (such as lived abroad and are returning to Ireland after become unemployed or deported). This condition does not apply to Jobseekers Benefit (which is based on Social Insurance payments).		More information on each benefit can be found here:		In Israel, unemployment benefits are paid by Bituah Leumi (the National Insurance Institute), to which workers must pay contributions. Eligible workers must immediately register with the Employment Service Bureau upon losing their jobs or jeaopordize their eligibility, and the unemployment period is considered to start upon registration with the Employment Service Bureau. To be eligible for unemployment benefits, an employee must have completed a "qualifying period" of work for which unemployment insurance contributions were paid, which varies between 300 and 360 days. Employees who were involuntarily terminated from their jobs or who terminated their own employment and can provide evidence of having done so for a justified reason are eligible for immediately receiving unemployment benefits, while those who are deemed to have terminated their employment of their own volition with no justified reason will only begin receiving unemployment benefits 90 days from the start of their unemployment period. Unemployment benefits are paid daily, with the amount calculated based on the employee's previous income over the past six months, but not exceeding the daily average wage for the first 125 days of payment and two-thirds of the daily average wage from the 126th day onwards. During the unemployment period, the Employment Service Bureau assists in helping locate suitable work and job training, and regularly reporting to the Employment Service Bureau is a condition for continuing to receive unemployment benefits. A person who was offered suitable work or training by the Employment Service Bureau but refused will only receive unemployment benefits 90 days after the date of the refusal, and 30 days' worth of unemployment benefits will be deducted for each subsequent refusal.[20][21][22]		Unemployment benefits in Italy consists mainly in cash transfers based on contributions (Assicurazione Sociale per l'Impiego, ASPI), up to the 75 percent of the previous wages for up to sixteen months. Other measures are:		In the Italian unemployment insurance system all the measures are income-related, and they have an average decommodification level. The basis for entitlement is always employment, with more specific conditions for each case, and the provider is quite always the state. An interesting feature worthy to be discussed is that the Italian system takes in consideration also the economic situation of the employers, and aims as well at relieving them from the costs of crisis.		Unemployment benefits in Japan are called "unemployment insurance" and are closer to the US or Canadian "user pays" system than the taxpayer funded systems in place in countries such as Britain, New Zealand, or Australia. It is paid for by contributions by both the employer and employee.[23]		On leaving a job, employees are supposed to be given a "Rishoku-hyo" document showing their ID number (the same number is supposed to be used by later employers), employment periods, and pay (which contributions are linked to). The reason for leaving is also documented separately. These items affect eligibility, timing, and amount of benefits.[24] The length of time that unemployed workers can receive benefits depends on the age of the employee, and how long they have been employed and paying in.[25]		It is supposed to be compulsory for most full-time employees.[26] If they have been enrolled for at least 6 months and are fired or made redundant, leave the company at the end of their contract, or their contract is non-renewed, the now-unemployed worker will receive unemployment insurance. If a worker quit of their own accord they may have to wait between 1 and 3 months before receiving any payment.		Mexico lacks a national unemployment insurance system, with the only such system being in Mexico City. Unemployed residents of Mexico City are eligible for unemployment benefits for up to 6 months. If a citizen resides elsewhere then all he/she must do is move to Mexico City and get a job for six months to qualify and be an employee.		Unemployment benefits in the Netherlands was introduced in 1949. Separate schemes exist for mainland Netherlands and for the Caribbean Netherlands.		The scheme in mainland Netherlands entails that, according to the Werkloosheidswet (Unemployment Law, WW), employers are responsible for paying the contributions to the scheme, which are deducted from the salary received by the employees. In 2012 the contribution was 4.55% of gross salary up to a ceiling of 4,172 euros per month. The first 1,435.75 euros of an employees gross salaries are not subject to the 4.55% contribution.		Benefits are paid for a maximum period of 38 months and claimants get 75% of last salary for 2 months and 70% thereafter with a maximum benefit of 3128 euros, depending on how long the claimant has been employed previously. Workers older than 50 years who are unemployed for over 2 months are entitled to a special benefit called the IOAW, if they do not receive the regular unemployment benefit (WW).		In New Zealand, Jobseeker Support, previously known as the Unemployment Benefit and also known as "the dole" provides income support for people who are looking for work or training for work. It is one of a number of benefits administered by Work and Income, a service of the Ministry of Social Development.		To get this benefit, a person must meet the conditions and obligations specified in section 88A to 123D Social Security Act 1964. These conditions and obligations cover things such as age, residency status, and availability to work.[27]		The amount that is paid depends on things such as the person's age, income, marital status and whether they have children. It is adjusted annually on 1 April and in response to changes in legislature. Some examples of the maximum after tax weekly rate at 1 April 2011 are:		More information about this benefit and the amounts paid are on the Work and Income website.[29]		External links		The Spanish unemployment benefits system is part of the Social security system of Spain. Benefits are managed by the State Public Employment Agency (SEPE).The basis for entitlement is having contributed for a minimum period during the time preceding unemployment, with further conditions that may be applicable. The system comprises contributory benefits and non-contributory benefits.		Contributory benefits are payable to those unemployed persons with a minimum of 12 months contributions over a period of 6 years preceding unemployment. The benefit is payable for 1/3 of the contribution period. The benefit amount is 70% of the legal reference salary plus additional amounts for persons with dependants. The benefit reduces to 60% of the reference salary after 6 months. The minimum benefit is 497 euros per month and the maximum is 1087,20 euros per month for a single person.[30] The non-contributory allowance is available to those persons who are no longer entitled to the contributory pension and who do not have income above 75% of the national minimum wage.		Sweden uses the Ghent system, under which a significant proportion of unemployment benefits are distributed by union unemployment funds. Unemployment benefits are divided into a voluntary scheme with income related compensation up to a certain level and a comprehensive scheme that provides a lower level of basic support. The voluntary scheme requires a minimum of 12 months membership and 6 months employment during that time before any claims can be made. Employers pay a fee on top of the pre-tax income of their employees, which together with membership fees, fund the scheme (see Unemployment funds in Sweden).		The maximum unemployment benefit is (as of July 2016) SEK 980 per day. During the first 200 days the unemployed will receive 80 percent of his or her normal income during the last 12 months. From day 201-300 this goes down to 70 percent and from day 301-450 the insurance covers 65 percent of the normal income (only available for parents to children under the age of 18). In Sweden tax is paid on unemployment benefits, so the unemployed will get a maximum of about SEK 10,000 per month during the first 100 days (depending on the municipality tax rate). In other currencies, as of June 2017, this means a maximum of approximately £900, $1,150, or €1,000, each month after tax. Private insurance is also available, mainly through professional organisations, to provide income related compensation that otherwise exceeds the ceiling of the scheme. The comprehensive scheme is funded by tax.		Saudi Arabia is an economic welfare state with free medical care[31] and unemployment benefits.[32] However, the country relies not on taxation but mainly oil revenues to maintain the social and economic services to its populace.		Payment: 2000 SAR (USD $534) for only 12 months for unemployed person from ages 18–35		External links		JSA for a single person is changed annually, and at August 3, 2012 the maximum payable was £71.00 per week for a person aged over 25 and £56.25 per week for a person aged 18–24.[33] The rules for couples where both are unemployed are more complex, but a maximum of £112.55 per week is payable, dependent on age and other factors. Income-based JSA is reduced for people with savings of over £6,000, by a reduction of £1 per week per £250 of savings, up to £16,000. People with savings of over £16,000 are not able to get IB-JSA at all.[34] The British system provides rent payments as part of a separate scheme called Housing Benefit.		Unemployment benefit is commonly referred to as "the dole"; to receive the benefit is to be "on the dole". "Dole" here is an archaic expression meaning "one's allotted portion", from the synonymous Old English word dāl. [35]		In the United States, there are 50 state unemployment insurance programs plus one each in the District of Columbia, Puerto Rico and United States Virgin Islands. Though policies vary by state, unemployment benefits generally pay eligible workers up to $450 per week maximum.[36] Benefits are generally paid by state governments, funded in large part by state and federal payroll taxes levied against employers, to workers who have become unemployed through no fault of their own. Eligibility requirements for unemployment insurance vary by state, but generally speaking, employees not fired for misconduct ("terminated for cause") are eligible for unemployment benefits, while those fired for misconduct (this sometimes can include misconduct committed outside the workplace, such as a problematic social media post or committing a crime) are not.[37] In every state, employees who quit their job without "good cause" are not eligible for unemployment benefits, but the definition of good cause varies by state. In some states, being fired for misconduct permanently bars the employee from receiving unemployment benefits, while in others it only disqualifies the employee for a short period. This compensation is classified as a type of social welfare benefit. According to the Internal Revenue Code, these types of benefits are to be included in a taxpayer's gross income.[38]		The standard time-length of unemployment compensation is six months, although extensions are possible during economic downturns. During the Great Recession, unemployment benefits were extended to 99 weeks.[citation needed]		The Supreme Court held that federal unemployment law is constitutional and does not violate the Tenth Amendment in Steward Machine Company v. Davis, 301 U.S. 548 (1937).		Unemployment insurance is a federal-state program financed through federal and state payroll taxes (federal and state UI taxes).[39] In most states employers pay state and federal unemployment taxes if:		To facilitate this program, the U.S. Congress passed the Federal Unemployment Tax Act (FUTA), which authorizes the Internal Revenue Service (IRS) to collect an annual federal employer tax used to fund state workforce agencies. FUTA covers the costs of administering the Unemployment Insurance and Job Service programs in all states. In addition, FUTA pays one-half of the cost of extended unemployment benefits which are triggered in periods of high state unemployment. FUTA also provides a fund from which states UI funds may borrow to pay benefits. As originally established, the states paid the federal government.[39]		The FUTA tax rate was originally three percent of taxable wages collected from employers who employed at least four employees,[40] and employers could deduct up to 90 percent of the amount due if they paid taxes to a state to support a system of unemployment insurance which met Federal standards,[4] but the rules have recently changed. The FUTA tax rate is now, as of June 30, 2011, 6.0 percent of taxable wages of employees who meet both the above and following criteria,[39] and the taxable wage base is the first $7,000 paid in wages to each employee during a calendar year.[39] Employers who pay the state unemployment tax on time receive an offset credit of up to 5.4 percent regardless of the rate of tax they pay their state. Therefore, the net FUTA tax rate is generally 0.6 percent (6.0 percent - 5.4 percent), for a maximum FUTA tax of $42.00 per employee, per year (.006 X $7,000 = $42.00). State law determines individual state unemployment insurance tax rates.[39] In the United States, unemployment insurance tax rates use experience rating.[41]		Although the taxable wage base for each state/territory is at least $7,000 as mandated by FUTA, only four states or territories still remain at this minimum.[42] These states/territories include Arizona, California, Florida, and Puerto Rico. Florida and Puerto Rico maintain tax rates similar to those of other states, but Arizona and California both have a higher maximum tax rate. Florida's minimum tax rate is 0.1% and the state maximum is 5.4%[43] and in Puerto Rico, employers are taxed between 2.4% and 5.4% depending on their experience rating.[44] As of 2015, Arizona's minimum was 0.03%, but its maximum was 7.79%.[45] California's tax rate on the taxable wage base is currently higher than the federal minimum of 6.0%. Employers are currently on a tax schedule that requires them to pay a minimum of 1.5% and a maximum 6.2% of the taxable wage base.[46] Even with the federal tax credit of 5.4%, Arizona employers could end up paying $175 per employee ((.0779-.054) x $7,000) and California employers could pay $56 per employee ((.062-.054) x $7,000) versus the FUTA maximum of $42.		Within the above constraints, the individual states and territories raise their own contributions and run their own programs. The federal government sets broad guidelines for coverage and eligibility, but states vary in how they determine benefits and eligibility.		Federal rules are drawn by the United States Department of Labor, Employment and Training Administration. For most states, the maximum period for receiving benefits is 26 weeks. There is an extended benefit program (authorized through the Social Security Acts) that may be triggered by the state unemployment rate. Congress has often passed temporary programs to extend benefits during recessions. This was done with the Temporary Extended Unemployment Compensation (TEUC) program in 2002-2003, which has since expired,[47] and remained in force through June 2, 2010, with the Extended Unemployment Compensation 2008 legislation.[48] In July 2010, legislation that provides an extension of federal extended unemployment benefits through November 2010 was signed by the president. The legislation extended benefits for 2.3 million unemployed workers who had exhausted their unemployment benefits.		The federal government lends money to the states for unemployment insurance when states run short of funds which happens when the state's UI fund cannot cover the cost of current benefits. A high unemployment rate shrinks UI tax revenues and increases expenditures on benefits. State UI finances and the need for loans are exacerbated when a state cuts taxes and increases benefits. FUTA loans to state funds are repaid with interest.		Congressional actions to increase penalties for states incurring large debts for unemployment benefits led to state fiscal crises in the 1980s.[citation needed]		To Keynesians, unemployment insurance acts as an automatic stabilizer.[49] Benefits automatically increase when unemployment is high and fall when unemployment is low, smoothing the business cycle; however, others claim that the taxation necessary to support this system serves to decrease employment.[citation needed]		In order to receive benefits, a person must have worked for at least one quarter in the previous year and have been laid-off by an employer. Workers who were temporary or were paid under the table are not eligible for unemployment insurance. If a worker quits without good cause or is fired for misconduct, then they are normally not eligible for UI benefits. There are five common reasons a claim for unemployment benefits are denied: the worker is unavailable for work, the worker quit his or her job without good cause, the worker was fired for misconduct, refusing suitable work, and unemployment resulting from a labor dispute.[50][51] In practice, it is only practical to verify whether the worker quit or was fired. If the worker's claim is denied, then they have the right to appeal. If the worker was fired for misconduct, then the employer has the burden to prove substantially that the termination of employment is a misconduct defined by individual states laws.[52] However, if the employee quit their job, then they must prove that their voluntary separation must be good cause.		Generally, the worker must be unemployed through no fault of his/her own although workers often file for benefits they are not entitled to; when the employer demonstrates that the unemployed person quit or was fired for cause the worker is required to pay back the benefits they received. The unemployed person must also meet state requirements for wages earned or time worked during an established period of time (referred to as a “base period”) to be eligible for benefits. In most states, the base period is usually the first four out of the last five completed calendar quarters prior to the time that the claim is filed.[53] Unemployment benefits are based on reported covered quarterly earnings. The amount of earnings and the number of quarters worked are used to determine the length and value of the unemployment benefit. The average weekly in 2010 payment was $293.[54]		As a result of the American Recovery and Reinvestment Act passed in February 2009, many unemployed people receive up to 99 weeks of unemployment benefits; this may depend on State legislation. Before the passage of the American Recovery and Reinvestment Act, the maximum number of weeks allowed was 26.		It generally takes two weeks for benefit payments to begin, the first being a "waiting week", which is not reimbursed, and the second being the time lag between eligibility for the program and the first benefit actually being paid.		To begin a claim, the unemployed worker must apply for benefits through a state unemployment agency.[53] In certain instances, the employer initiates the process. Generally, the certification includes the affected person affirming that they are "able and available for work", the amount of any part-time earnings they may have had, and whether they are actively seeking work. These certifications are usually accomplished either over the Internet or via an interactive voice response telephone call, but in a few states may be by mail. After receiving an application, the state will notify the individual if they qualify and the rate they will receive every week. The state will also review the reason for separation from employment. Many states require the individual to periodically certify that the conditions of the benefits are still met.		If a worker's reason for separation from their last job is due to some reason other than a "lack of work," a determination will be made about whether they are eligible for benefits. Generally, all determinations of eligibility for benefits are made by the appropriate State under its law or applicable federal laws. If a worker is disqualified or denied benefits, they have the right to file an appeal within an established time-frame. The State will advise a worker of his or her appeal rights. An employer may also appeal a determination if they do not agree with the State's determination regarding the employee's eligibility.[53] If the worker's claim is denied, then they have the right to appeal. If the worker was fired for misconduct, then the employer has the burden to prove substantially that the termination of employment is a misconduct defined by individual states laws.[52] However, if the employee quit their job, then they must prove that their voluntary separation must be good cause. Success rate of unemployment appeals is two-thirds, or 67% of the time for the most claimants.[55][56]		Each Thursday, the Department of Labor issues the Unemployment Insurance Weekly Claims Report.[57] Its headline number is the seasonally adjusted estimate for the initial claims for unemployment for the previous week in the United States. This statistic, because it is published weekly, is depended on as a current indicator of the labor market and the economy generally.		In 2016, the number of people on unemployment benefits fell to around 1.74 the lowest in the last 4 decades. [58]		Twice a year, the Office of Management and Budget delivers an economic assessment of the unemployment insurance program as it relates to budgetary issues.[59] As it relates to the FY 2012 budget, the OMB reports that the insured unemployment rate (IUR) is projected to average 3.6% in both FY 2011 and in FY 2012. State unemployment regular benefit outlays are estimated at $61 billion in FY 2011 and $64.3 billion in FY 2012, down somewhat from Midsession estimates.[59] Outlays from state trust fund accounts are projected to exceed revenues and interest income by $16.0 billion in FY 2011 and $15.1 billion in FY 2012.[59] State trust fund account balances, net of loans, are projected to continue to fall, from -$27.4 billion at the end of FY 2010 to -$62.7 billion at the end of FY 2013, before starting to grow again.[59] Net balances are not projected to become positive again until well beyond FY 2016. Up to 40 states are projected to continue borrowing heavily from the Federal Unemployment Account (FUA) over the next few years.[59] The aggregate loan balance is projected to increase from $40.2 billion at the end of FY 2010 to a peak end-of-year balance of $68.3 billion in FY 2013. Due to the high volume of state loans and increased EB payments, FUA and EUCA are projected to borrow $26.7 billion from the general fund in FY 2011 and an additional $19.4 billion in FY 2012, with neither account projected to return to a net positive balance before 2016.[59] The general fund advances must be repaid with interest.[59]		The economic argument for unemployment insurance comes from the principle of adverse selection. One common criticism of unemployment insurance is that it induces moral hazard, the fact that unemployment insurance lowers on-the-job effort and reduces job-search effort.		Adverse selection refers to the fact that “workers who have the highest probability of becoming unemployed have the highest demand for unemployment insurance.”[60] Adverse selection causes profit maximizing private insurance agencies to set high premiums for the insurance because there is a high likelihood they will have to make payments to the policyholder. High premiums work to exclude many individuals who otherwise might purchase the insurance. “A compulsory government program avoids the adverse selection problem. Hence, government provision of UI has the potential to increase efficiency. However, government provision does not eliminate moral hazard.”[60]		“At the same time, those workers who managed to obtain insurance might experience more unemployment otherwise would have been the case.”[60] The private insurance company would have to determine whether the employee is unemployed through no fault of their own, which is difficult to determine. Incorrect determinations could result in the payout of significant amounts for fraudulent claims or alternately failure to pay legitimate claims. This leads to the rationale that if government could solve either problem that government intervention would increase efficiency.		In the Great Recession, the “moral hazard” issue of whether unemployment insurance—and specifically extending benefits past the maximum 99 weeks—significantly encourages unemployment by discouraging workers from finding and taking jobs was expressed by Republican legislators. Conservative economist Robert Barro found that benefits raised the unemployment rate 2%.[61][62] Disagreeing with Barro's study were Berkeley economist Jesse Rothstein, who found the “vast majority” of unemployment was due to “demand shocks” not “[unemployment insurance]-induced supply reductions.”[62][63] A study by Rothstein of extensions of unemployment insurance to 99 weeks during the Great Recession to test the hypothesis that unemployment insurance discourages people from seeking jobs found the overall effect of UI on unemployment was to raise it by no more than one-tenth of one percent.[64][65]		A November 2011 report by the Congressional Budget Office found that even if unemployment benefits convince some unemployed to ignore job openings, these openings were quickly filled by new entrants into the labor market.[62][66] A survey of studies on unemployment insurance's effect on employment by the Political Economy Research Institute found that unemployed who collected benefits did not find themselves out of work longer than those who didn’t have unemployment benefits; and that unemployed workers did not search for work more or reduce their wage expectations once their benefits ran out.[62][67]		One concern over unemployment insurance increasing unemployment is based on experience rating benefit uses which can sometimes be imperfect. That is, the cost to the employer in increased taxes is less than the benefits that would be paid to the employee upon layoff. The firm in this instance believes that it is more cost effective to lay off the employee, causing more unemployment than under perfect experience rating.[60]		An alternative rationale for unemployment insurance is that it may allow for improving the quality of matches between workers and firms. Marimon and Zilibotti argued that although a more generous unemployment benefit system may indeed increase the unemployment rate, it may also help improve the average match quality.[68] A similar point is made by Mazur who analyzed the welfare and inequality effects of a policy reform giving entitlement for unemployment insurance to quitters.[69] Arash Nekoei and Andrea Weber present empirical evidence from Austria that extending unemployment benefit duration raises wages by improving reemployment firm quality.[70] Similarly, Tatsiramos studied data from European countries and found that although unemployment insurance does increase unemployment duration, the duration of subsequent employment tends to be longer (suggesting better match quality).[71]		An alternative to unemployment insurance intended to reduce the moral hazard costs would introduce mandated individual saving accounts for workers to draw on after being laid off. The plan, by Martin Feldstein would pay any positive account balance at retirement to the employee.[72]		Another issue with unemployment insurance relates to its effects on state budgets. During recessionary time periods, the number of unemployed rises and they begin to draw benefits from the program. The longer the recession lasts, depending on the state’s starting UI program balance, the quicker the state begins to run out of funds. The recession that began in December 2007 and ended in June 2009 has significantly impacted state budgets. According to The Council of State Governments, by March 18, 2011, 32 states plus the Virgin Islands had borrowed nearly $45.7 billion. The Labor Department estimates by the fourth quarter of 2013, as many as 40 states may need to borrow more than $90 billion to fund their unemployment programs and it will take a decade or more to pay off the debt.[73]		Possible policy options for states to shore up the unemployment insurance funds include lowering benefits for recipients and/or raising taxes on businesses. Kentucky took the approach of raising taxes and lowering benefits to attempt to balance its unemployment insurance program. Starting in 2010, a claimant’s weekly benefits will decrease from 68% to 62% and the taxable wage base will increase from $8,000 to $12,000, over a ten-year period. These moves are estimated to save the state over $450 million.[74]		The argument for taxation of social welfare benefits is that they result in a realized gain for a taxpayer. The argument against taxation is that the benefits are generally less than the federal poverty level.		Unemployment compensation has been taxable by the federal government since 1987.[75] Code Section 85 deemed unemployment compensation included in gross income.[76] Federal taxes are not withheld from unemployment compensation at the time of payment unless requested by the recipient using Form W-4V.[75][77] In 2003, Rep. Philip English introduced legislation to repeal the taxation of unemployment compensation, but the legislation did not advance past committee.[75][78] Most states with income tax consider unemployment compensation to be taxable.[75] Prior to 1987, unemployment compensation amounts were excluded from federal gross income.[79] For the US Federal tax year of 2009, as a result of the signing of the American Recovery and Reinvestment Act of 2009 signed by Barack Obama on February 17, 2009 the first $2,400 worth of unemployment income received during the 'tax year' of 2009 will be exempted from being considered as taxable income on the Federal level, when American taxpayers file their 2009 IRS tax return paperwork in early 2010.		Job sharing or work sharing and short time or short-time working refer to situations or systems in which employees agree to or are forced to accept a reduction in working time and pay. These can be based on individual agreements or on government programs in many countries that try to prevent unemployment. In these, employers have the option of reducing work hours to part-time for many employees instead of laying off some of them and retaining only full-time workers. For example, employees in 18 states of the United States can then receive unemployment payments for the hours they are no longer working.[80]		In 2013, it was reported that most U.S. states deliver unemployment benefits to recipients who do not have a bank account through a prepaid debit card.[81] The federal government uses the Direct Express Debit Mastercard prepaid debit card offered by Mastercard and Comerica Bank to give some federal assistance payments to people who do not have bank accounts. Many states have similar programs for unemployment payments and other assistance.		International Labour Organization has adopted the Employment Promotion and Protection against Unemployment Convention, 1988 for promotion of employment against unemployment and social security including unemployment benefit.		
Education is the process of facilitating learning, or the acquisition of knowledge, skills, values, beliefs, and habits. Educational methods include storytelling, discussion, teaching, training, and directed research. Education frequently takes place under the guidance of educators, but learners may also educate themselves.[1] Education can take place in formal or informal settings and any experience that has a formative effect on the way one thinks, feels, or acts may be considered educational. The methodology of teaching is called pedagogy.		Education is commonly divided formally into such stages as preschool or kindergarten, primary school, secondary school and then college, university, or apprenticeship.		A right to education has been recognized by some governments and the United Nations.[2] In most regions, education is compulsory up to a certain age.						Etymologically, the word "education" is derived from the Latin ēducātiō ("A breeding, a bringing up, a rearing") from ēducō ("I educate, I train") which is related to the homonym ēdūcō ("I lead forth, I take out; I raise up, I erect") from ē- ("from, out of") and dūcō ("I lead, I conduct").[3]		Education began in prehistory, as adults trained the young in the knowledge and skills deemed necessary in their society. In pre-literate societies, this was achieved orally and through imitation. Story-telling passed knowledge, values, and skills from one generation to the next. As cultures began to extend their knowledge beyond skills that could be readily learned through imitation, formal education developed. Schools existed in Egypt at the time of the Middle Kingdom.[4]		Plato founded the Academy in Athens, the first institution of higher learning in Europe.[5] The city of Alexandria in Egypt, established in 330 BCE, became the successor to Athens as the intellectual cradle of Ancient Greece. There, the great Library of Alexandria was built in the 3rd century BCE. European civilizations suffered a collapse of literacy and organization following the fall of Rome in CE 476.[6]		In China, Confucius (551–479 BCE), of the State of Lu, was the country's most influential ancient philosopher, whose educational outlook continues to influence the societies of China and neighbours like Korea, Japan, and Vietnam. Confucius gathered disciples and searched in vain for a ruler who would adopt his ideals for good governance, but his Analects were written down by followers and have continued to influence education in East Asia into the modern era.[citation needed]		After the Fall of Rome, the Catholic Church became the sole preserver of literate scholarship in Western Europe. The church established cathedral schools in the Early Middle Ages as centres of advanced education. Some of these establishments ultimately evolved into medieval universities and forebears of many of Europe's modern universities.[6] During the High Middle Ages, Chartres Cathedral operated the famous and influential Chartres Cathedral School. The medieval universities of Western Christendom were well-integrated across all of Western Europe, encouraged freedom of inquiry, and produced a great variety of fine scholars and natural philosophers, including Thomas Aquinas of the University of Naples, Robert Grosseteste of the University of Oxford, an early expositor of a systematic method of scientific experimentation,[7] and Saint Albert the Great, a pioneer of biological field research.[8] Founded in 1088, the University of Bologne is considered the first, and the oldest continually operating university.[9]		Elsewhere during the Middle Ages, Islamic science and mathematics flourished under the Islamic caliphate which was established across the Middle East, extending from the Iberian Peninsula in the west to the Indus in the east and to the Almoravid Dynasty and Mali Empire in the south.		The Renaissance in Europe ushered in a new age of scientific and intellectual inquiry and appreciation of ancient Greek and Roman civilizations. Around 1450, Johannes Gutenberg developed a printing press, which allowed works of literature to spread more quickly. The European Age of Empires saw European ideas of education in philosophy, religion, arts and sciences spread out across the globe. Missionaries and scholars also brought back new ideas from other civilizations – as with the Jesuit China missions who played a significant role in the transmission of knowledge, science, and culture between China and Europe, translating works from Europe like Euclid's Elements for Chinese scholars and the thoughts of Confucius for European audiences. The Enlightenment saw the emergence of a more secular educational outlook in Europe.		In most countries today, full-time education, whether at school or otherwise, is compulsory for all children up to a certain age. Due to this the proliferation of compulsory education, combined with population growth, UNESCO has calculated that in the next 30 years more people will receive formal education than in all of human history thus far.[10]		Formal education occurs in a structured environment whose explicit purpose is teaching students. Usually, formal education takes place in a school environment with classrooms of multiple students learning together with a trained, certified teacher of the subject. Most school systems are designed around a set of values or ideals that govern all educational choices in that system. Such choices include curriculum, organizational models, design of the physical learning spaces (e.g. classrooms), student-teacher interactions, methods of assessment, class size, educational activities, and more.[11][12]		Preschools provide education from ages approximately three to seven, depending on the country when children enter primary education. These are also known as nursery schools and as kindergarten, except in the US, where kindergarten is a term used for primary education.[citation needed] Kindergarten "provide[s] a child-centred, preschool curriculum for three- to seven-year-old children that aim[s] at unfolding the child's physical, intellectual, and moral nature with balanced emphasis on each of them."[13]		Primary (or elementary) education consists of the first five to seven years of formal, structured education. In general, primary education consists of six to eight years of schooling starting at the age of five or six, although this varies between, and sometimes within, countries. Globally, around 89% of children aged six to twelve are enrolled in primary education, and this proportion is rising.[14] Under the Education For All programs driven by UNESCO, most countries have committed to achieving universal enrollment in primary education by 2015, and in many countries, it is compulsory. The division between primary and secondary education is somewhat arbitrary, but it generally occurs at about eleven or twelve years of age. Some education systems have separate middle schools, with the transition to the final stage of secondary education taking place at around the age of fourteen. Schools that provide primary education, are mostly referred to as primary schools or elementary schools. Primary schools are often subdivided into infant schools and junior school.		In India, for example, compulsory education spans over twelve years, with eight years of elementary education, five years of primary schooling and three years of upper primary schooling. Various states in the republic of India provide 12 years of compulsory school education based on a national curriculum framework designed by the National Council of Educational Research and Training.		In most contemporary educational systems of the world, secondary education comprises the formal education that occurs during adolescence. It is characterized by transition from the typically compulsory, comprehensive primary education for minors, to the optional, selective tertiary, "postsecondary", or "higher" education (e.g. university, vocational school) for adults. Depending on the system, schools for this period, or a part of it, may be called secondary or high schools, gymnasiums, lyceums, middle schools, colleges, or vocational schools. The exact meaning of any of these terms varies from one system to another. The exact boundary between primary and secondary education also varies from country to country and even within them but is generally around the seventh to the tenth year of schooling. Secondary education occurs mainly during the teenage years. In the United States, Canada, and Australia, primary and secondary education together are sometimes referred to as K-12 education, and in New Zealand Year 1–13 is used. The purpose of secondary education can be to give common knowledge, to prepare for higher education, or to train directly in a profession.		Secondary education in the United States did not emerge until 1910, with the rise of large corporations and advancing technology in factories, which required skilled workers. In order to meet this new job demand, high schools were created, with a curriculum focused on practical job skills that would better prepare students for white collar or skilled blue collar work. This proved beneficial for both employers and employees, since the improved human capital lowered costs for the employer, while skilled employees received higher wages.		Secondary education has a longer history in Europe, where grammar schools or academies date from as early as the 16th century, in the form of public schools, fee-paying schools, or charitable educational foundations, which themselves date even further back.		Community colleges offer another option at this transitional stage of education. They provide nonresidential junior college courses to people living in a particular area.		Higher education, also called tertiary, third stage, or postsecondary education, is the non-compulsory educational level that follows the completion of a school such as a high school or secondary school. Tertiary education is normally taken to include undergraduate and postgraduate education, as well as vocational education and training. Colleges and universities mainly provide tertiary education. Collectively, these are sometimes known as tertiary institutions. Individuals who complete tertiary education generally receive certificates, diplomas, or academic degrees.		Higher education typically involves work towards a degree-level or foundation degree qualification. In most developed countries, a high proportion of the population (up to 50%) now enter higher education at some time in their lives. Higher education is therefore very important to national economies, both as a significant industry in its own right and as a source of trained and educated personnel for the rest of the economy.		University education includes teaching, research, and social services activities, and it includes both the undergraduate level (sometimes referred to as tertiary education) and the graduate (or postgraduate) level (sometimes referred to as graduate school). Universities are generally composed of several colleges. In the United States, universities can be private and independent like Yale University; public and state-governed like the Pennsylvania State System of Higher Education; or independent but state-funded like the University of Virginia. A number of career specific courses are now available to students through the Internet.		One type of university education is a liberal arts education, which can be defined as a "college or university curriculum aimed at imparting broad general knowledge and developing general intellectual capacities, in contrast to a professional, vocational, or technical curriculum."[15] Although what is known today as liberal arts education began in Europe,[16] the term "liberal arts college" is more commonly associated with institutions in the United States.[17]		Vocational education is a form of education focused on direct and practical training for a specific trade or craft. Vocational education may come in the form of an apprenticeship or internship as well as institutions teaching courses such as carpentry, agriculture, engineering, medicine, architecture and the arts.		In the past, those who were disabled were often not eligible for public education. Children with disabilities were repeatedly denied an education by physicians or special tutors. These early physicians (people like Itard, Seguin, Howe, Gallaudet) set the foundation for special education today. They focused on individualized instruction and functional skills. In its early years, special education was only provided to people with severe disabilities, but more recently it has been opened to anyone who has experienced difficulty learning.[18]		While considered "alternative" today, most alternative systems have existed since ancient times. After the public school system was widely developed beginning in the 19th century, some parents found reasons to be discontented with the new system. Alternative education developed in part as a reaction to perceived limitations and failings of traditional education. A broad range of educational approaches emerged, including alternative schools, self learning, homeschooling, and unschooling. Example alternative schools include Montessori schools, Waldorf schools (or Steiner schools), Friends schools, Sands School, Summerhill School, Walden's Path, The Peepal Grove School, Sudbury Valley School, Krishnamurti schools, and open classroom schools. Charter schools are another example of alternative education, which have in the recent years grown in numbers in the US and gained greater importance in its public education system.[19][20]		In time, some ideas from these experiments and paradigm challenges may be adopted as the norm in education, just as Friedrich Fröbel's approach to early childhood education in 19th-century Germany has been incorporated into contemporary kindergarten classrooms. Other influential writers and thinkers have included the Swiss humanitarian Johann Heinrich Pestalozzi; the American transcendentalists Amos Bronson Alcott, Ralph Waldo Emerson, and Henry David Thoreau; the founders of progressive education, John Dewey and Francis Parker; and educational pioneers such as Maria Montessori and Rudolf Steiner, and more recently John Caldwell Holt, Paul Goodman, Frederick Mayer, George Dennison, and Ivan Illich.		Indigenous education refers to the inclusion of indigenous knowledge, models, methods, and content within formal and non-formal educational systems. Often in a post-colonial context, the growing recognition and use of indigenous education methods can be a response to the erosion and loss of indigenous knowledge and language through the processes of colonialism. Furthermore, it can enable indigenous communities to "reclaim and revalue their languages and cultures, and in so doing, improve the educational success of indigenous students."[21]		Informal learning is one of three forms of learning defined by the Organisation for Economic Co-operation and Development (OECD). Informal learning occurs in a variety of places, such as at home, work, and through daily interactions and shared relationships among members of society. For many learners, this includes language acquisition, cultural norms, and manners. Informal learning for young people is an ongoing process that also occurs in a variety of places, such as out of school time, in youth programs at community centres and media labs.		Informal learning usually takes place outside educational establishments, does not follow a specified curriculum and may originate accidentally, sporadically, in association with certain occasions, from changing practical requirements. It is not necessarily planned to be pedagogically conscious, systematic and according to subjects, but rather unconsciously incidental, holistically problem-related, and related to situation management and fitness for life. It is experienced directly in its "natural" function of everyday life and is often spontaneous.		The concept of 'education through recreation' was applied to childhood development in the 19th century.[22] In the early 20th century, the concept was broadened to include young adults but the emphasis was on physical activities.[23] L.P. Jacks, also an early proponent of lifelong learning, described education through recreation: "A master in the art of living draws no sharp distinction between his work and his play, his labour and his leisure, his mind and his body, his education and his recreation. He hardly knows which is which. He simply pursues his vision of excellence through whatever he is doing and leaves others to determine whether he is working or playing. To himself, he always seems to be doing both. Enough for him that he does it well."[24] Education through recreation is the opportunity to learn in a seamless fashion through all of life's activities.[25] The concept has been revived by the University of Western Ontario to teach anatomy to medical students.[25]		Autodidacticism (also autodidactism) is a contemplative, absorbing process, of "learning on your own" or "by yourself", or as a self-teacher. Some autodidacts spend a great deal of time reviewing the resources of libraries and educational websites. One may become an autodidact at nearly any point in one's life. While some may have been informed in a conventional manner in a particular field, they may choose to inform themselves in other, often unrelated areas. Notable autodidacts include Abraham Lincoln (U.S. president), Srinivasa Ramanujan (mathematician), Michael Faraday (chemist and physicist), Charles Darwin (naturalist), Thomas Alva Edison (inventor), Tadao Ando (architect), George Bernard Shaw (playwright), Frank Zappa (composer, recording engineer, film director), and Leonardo da Vinci (engineer, scientist, mathematician).		In 2012, the modern use of electronic educational technology (also called e-learning) had grown at 14 times the rate of traditional learning.[clarification needed][26] Open education is fast growing to become the dominant form of education, for many reasons such as its efficiency and results compared to traditional methods.[27] Cost of education has been an issue throughout history, and a major political issue in most countries today. Online courses often can be more expensive than face-to-face classes. Out of 182 colleges surveyed in 2009 nearly half said tuition for online courses was higher than for campus-based ones.[28] Many large university institutions are now starting to offer free or almost free full courses such as Harvard, MIT and Berkeley teaming up to form edX. Other universities offering open education are Stanford, Princeton, Duke, Johns Hopkins, Edinburgh, U. Penn, U. Michigan, U. Virginia, U. Washington, and Caltech. It has been called the biggest change in the way we learn since the printing press.[29] Despite favourable studies on effectiveness, many people may still desire to choose traditional campus education for social and cultural reasons.[30]		The conventional merit-system degree is currently not as common in open education as it is in campus universities, although some open universities do already offer conventional degrees such as the Open University in the United Kingdom. Presently, many of the major open education sources offer their own form of certificate. Due to the popularity of open education, these new kind of academic certificates are gaining more respect and equal "academic value" to traditional degrees.[31] Many open universities are working to have the ability to offer students standardized testing and traditional degrees and credentials.[32]		A culture is beginning to form around distance learning for people who are looking to social connections enjoyed on traditional campuses. For example, students may create study groups, meetups, and movements such as UnCollege.		The education sector or education system is a group of institutions (ministries of education, local educational authorities, teacher training institutions, schools, universities, etc.) whose primary purpose is to provide education to children and young people in educational settings. It involves a wide range of people (curriculum developers, inspectors, school principals, teachers, school nurses, students, etc.). These institutions can vary according to different contexts.[33]		Schools deliver education, with support from the rest of the education system through various elements such as education policies and guidelines – to which school policies can refer – curricula and learning materials, as well as pre- and in-service teacher training programmes. The school environment – both physical (infrastructures) and psychological (school climate) – is also guided by school policies that should ensure the well-being of students when they are in school.[33] The Organisation for Economic Co-operation and Development has found that schools tend to perform best when principals have full authority and responsibility for ensuring that students are proficient in core subjects upon graduation. They must also seek feedback from students for quality-assurance and improvement. Governments should limit themselves to monitoring student proficiency.[34]		The education sector is fully integrated into society, through interactions with a large number of stakeholders and other sectors. These include parents, local communities, religious leaders, NGOs, stakeholders involved in health, child protection, justice and law enforcement (police), media and political leadership.[33]		Since 1909, the ratio of children in the developing world attending school has increased. Before then, a small minority of boys attended school. By the start of the 21st century, the majority of all children in most regions of the world attended school.		Universal Primary Education is one of the eight international Millennium Development Goals, towards which progress has been made in the past decade, though barriers still remain.[35] Securing charitable funding from prospective donors is one particularly persistent problem. Researchers at the Overseas Development Institute have indicated that the main obstacles to funding for education include conflicting donor priorities, an immature aid architecture, and a lack of evidence and advocacy for the issue.[35] Additionally, Transparency International has identified corruption in the education sector as a major stumbling block to achieving Universal Primary Education in Africa.[36] Furthermore, demand in the developing world for improved educational access is not as high as foreigners have expected. Indigenous governments are reluctant to take on the ongoing costs involved. There is also economic pressure from some parents, who prefer their children to earn money in the short term rather than work towards the long-term benefits of education.[citation needed]		A study conducted by the UNESCO International Institute for Educational Planning indicates that stronger capacities in educational planning and management may have an important spill-over effect on the system as a whole.[37] Sustainable capacity development requires complex interventions at the institutional, organizational and individual levels that could be based on some foundational principles:		Nearly every country now has Universal Primary Education.		Similarities – in systems or even in ideas – that schools share internationally have led to an increase in international student exchanges. The European Socrates-Erasmus Program[38] facilitates exchanges across European universities. The Soros Foundation[39] provides many opportunities for students from central Asia and eastern Europe. Programs such as the International Baccalaureate have contributed to the internationalization of education. The global campus online, led by American universities, allows free access to class materials and lecture files recorded during the actual classes.		The Programme for International Student Assessment and the International Association for the Evaluation of Educational Achievement objectively monitor and compare the proficiency of students from a wide range of different nations.		Technology plays an increasingly significant role in improving access to education for people living in impoverished areas and developing countries. Charities like One Laptop per Child are dedicated to providing infrastructures through which the disadvantaged may access educational materials.		The OLPC foundation, a group out of MIT Media Lab and supported by several major corporations, has a stated mission to develop a $100 laptop for delivering educational software. The laptops were widely available as of 2008. They are sold at cost or given away based on donations.		In Africa, the New Partnership for Africa's Development (NEPAD) has launched an "e-school program" to provide all 600,000 primary and high schools with computer equipment, learning materials and internet access within 10 years.[40] An International Development Agency project called nabuur.com,[41] started with the support of former American President Bill Clinton, uses the Internet to allow co-operation by individuals on issues of social development.		India is developing technologies that will bypass land-based telephone and Internet infrastructure to deliver distance learning directly to its students. In 2004, the Indian Space Research Organisation launched EDUSAT, a communications satellite providing access to educational materials that can reach more of the country's population at a greatly reduced cost.[42]		Research into LCPS (low-cost private schools) found that over 5 years to July 2013, debate around LCPSs to achieving Education for All (EFA) objectives was polarized and finding growing coverage in international policy.[43] The polarization was due to disputes around whether the schools are affordable for the poor, reach disadvantaged groups, provide quality education, support or undermine equality, and are financially sustainable. The report examined the main challenges encountered by development organizations which support LCPSs.[43] Surveys suggest these types of schools are expanding across Africa and Asia. This success is attributed to excess demand. These surveys found concern for:		The report showed some cases of successful voucher and subsidy programmes; evaluations of international support to the sector are not widespread.[43] Addressing regulatory ineffectiveness is a key challenge. Emerging approaches stress the importance of understanding the political economy of the market for LCPS, specifically how relationships of power and accountability between users, government, and private providers can produce better education outcomes for the poor.		Educational psychology is the study of how humans learn in educational settings, the effectiveness of educational interventions, the psychology of teaching, and the social psychology of schools as organizations. Although the terms "educational psychology" and "school psychology" are often used interchangeably, researchers and theorists are likely to be identified as educational psychologists, whereas practitioners in schools or school-related settings are identified as school psychologists. Educational psychology is concerned with the processes of educational attainment in the general population and in sub-populations such as gifted children and those with specific disabilities.		Educational psychology can in part be understood through its relationship with other disciplines. It is informed primarily by psychology, bearing a relationship to that discipline analogous to the relationship between medicine and biology. Educational psychology, in turn, informs a wide range of specialties within educational studies, including instructional design, educational technology, curriculum development, organizational learning, special education and classroom management. Educational psychology both draws from and contributes to cognitive science and the learning sciences. In universities, departments of educational psychology are usually housed within faculties of education, possibly accounting for the lack of representation of educational psychology content in introductory psychology textbooks (Lucas, Blazek, & Raley, 2006).		Intelligence is an important factor in how the individual responds to education. Those who have higher intelligence tend to perform better at school and go on to higher levels of education.[45] This effect is also observable in the opposite direction, in that education increases measurable intelligence.[46] Studies have shown that while educational attainment is important in predicting intelligence in later life, intelligence at 53 is more closely correlated to intelligence at 8 years old than to educational attainment.[47]		There has been much interest in learning modalities and styles over the last two decades. The most commonly employed learning modalities are:[48]		Other commonly employed modalities include musical, interpersonal, verbal, logical, and intrapersonal.		Dunn and Dunn[49] focused on identifying relevant stimuli that may influence learning and manipulating the school environment, at about the same time as Joseph Renzulli[50] recommended varying teaching strategies. Howard Gardner[51] identified a wide range of modalities in his Multiple Intelligences theories. The Myers-Briggs Type Indicator and Keirsey Temperament Sorter, based on the works of Jung,[52] focus on understanding how people's personality affects the way they interact personally, and how this affects the way individuals respond to each other within the learning environment. The work of David Kolb and Anthony Gregorc's Type Delineator[53] follows a similar but more simplified approach.		Some theories propose that all individuals benefit from a variety of learning modalities, while others suggest that individuals may have preferred learning styles, learning more easily through visual or kinesthetic experiences.[54] A consequence of the latter theory is that effective teaching should present a variety of teaching methods which cover all three learning modalities so that different students have equal opportunities to learn in a way that is effective for them.[55] Guy Claxton has questioned the extent that learning styles such as Visual, Auditory and Kinesthetic(VAK) are helpful, particularly as they can have a tendency to label children and therefore restrict learning.[56][57] Recent research has argued, "there is no adequate evidence base to justify incorporating learning styles assessments into general educational practice."[58]		Educational neuroscience is an emerging scientific field that brings together researchers in cognitive neuroscience, developmental cognitive neuroscience, educational psychology, educational technology, education theory and other related disciplines to explore the interactions between biological processes and education.[59][60][61][62] Researchers in educational neuroscience investigate the neural mechanisms of reading,[61] numerical cognition,[63] attention, and their attendant difficulties including dyslexia,[64][65] dyscalculia,[66] and ADHD as they relate to education. Several academic institutions around the world are beginning to devote resources to the establishment of educational neuroscience research.		As an academic field, philosophy of education is "the philosophical study of education and its problems (...) its central subject matter is education, and its methods are those of philosophy".[67] "The philosophy of education may be either the philosophy of the process of education or the philosophy of the discipline of education. That is, it may be part of the discipline in the sense of being concerned with the aims, forms, methods, or results of the process of educating or being educated; or it may be metadisciplinary in the sense of being concerned with the concepts, aims, and methods of the discipline."[68] As such, it is both part of the field of education and a field of applied philosophy, drawing from fields of metaphysics, epistemology, axiology and the philosophical approaches (speculative, prescriptive or analytic) to address questions in and about pedagogy, education policy, and curriculum, as well as the process of learning, to name a few.[69] For example, it might study what constitutes upbringing and education, the values and norms revealed through upbringing and educational practices, the limits and legitimization of education as an academic discipline, and the relation between education theory and practice.		There is no broad consensus as to what education's chief aim or aims are or should be. Some authors stress its value to the individual, emphasizing its potential for positively influencing students' personal development, promoting autonomy, forming a cultural identity or establishing a career or occupation. Other authors emphasize education's contributions to societal purposes, including good citizenship, shaping students into productive members of society, thereby promoting society's general economic development, and preserving cultural values.[70]		In formal education, a curriculum is the set of courses and their content offered at a school or university. As an idea, curriculum stems from the Latin word for race course, referring to the course of deeds and experiences through which children grow to become mature adults. A curriculum is prescriptive and is based on a more general syllabus which merely specifies what topics must be understood and to what level to achieve a particular grade or standard.		An academic discipline is a branch of knowledge which is formally taught, either at the university–or via some other such method. Each discipline usually has several sub-disciplines or branches, and distinguishing lines are often both arbitrary and ambiguous. Examples of broad areas of academic disciplines include the natural sciences, mathematics, computer science, social sciences, humanities and applied sciences.[71]		Educational institutions may incorporate fine arts as part of K-12 grade curricula or within majors at colleges and universities as electives. The various types of fine arts are music, dance, and theatre.[72]		Instruction is the facilitation of another's learning. Instructors in primary and secondary institutions are often called teachers, and they direct the education of students and might draw on many subjects like reading, writing, mathematics, science and history. Instructors in post-secondary institutions might be called teachers, instructors, or professors, depending on the type of institution; and they primarily teach only their specific discipline. Studies from the United States suggest that the quality of teachers is the single most important factor affecting student performance, and that countries which score highly on international tests have multiple policies in place to ensure that the teachers they employ are as effective as possible.[73][74] With the passing of NCLB in the United States (No Child Left Behind), teachers must be highly qualified. A popular way to gauge teaching performance is to use student evaluations of teachers (SETS), but these evaluations have been criticized for being counterproductive to learning and inaccurate due to student bias.[75]		College basketball coach John Wooden the Wizard of Westwood would teach through quick "This not That" technique. He would show (a) the correct way to perform an action, (b) the incorrect way the player performed it, and again (c) the correct way to perform an action. This helped him to be a responsive teacher and fix errors on the fly. Also, less communication from him meant more time that the player could practice.[76]		It has been argued that high rates of education are essential for countries to be able to achieve high levels of economic growth.[77] Empirical analyses tend to support the theoretical prediction that poor countries should grow faster than rich countries because they can adopt cutting edge technologies already tried and tested by rich countries. However, technology transfer requires knowledgeable managers and engineers who are able to operate new machines or production practices borrowed from the leader in order to close the gap through imitation. Therefore, a country's ability to learn from the leader is a function of its stock of "human capital". Recent study of the determinants of aggregate economic growth have stressed the importance of fundamental economic institutions[78] and the role of cognitive skills.[79]		At the level of the individual, there is a large literature, generally related to the work of Jacob Mincer,[80] on how earnings are related to the schooling and other human capital. This work has motivated a large number of studies, but is also controversial. The chief controversies revolve around how to interpret the impact of schooling.[81][82] Some students who have indicated a high potential for learning, by testing with a high intelligence quotient, may not achieve their full academic potential, due to financial difficulties.[citation needed]		Economists Samuel Bowles and Herbert Gintis argued in 1976 that there was a fundamental conflict in American schooling between the egalitarian goal of democratic participation and the inequalities implied by the continued profitability of capitalist production.[83]		To learn how to add open-license text to Wikipedia articles, please see Wikipedia:Adding open license text to Wikipedia.		
Social comparison theory, initially proposed by social psychologist Leon Festinger in 1954,[1] centers on the belief that there is a drive within individuals to gain accurate self-evaluations. The theory explains how individuals evaluate their own opinions and abilities by comparing themselves to others in order to reduce uncertainty in these domains, and learn how to define the self.		Following the initial theory, research began to focus on social comparison as a way of self-enhancement,[2][3] introducing the concepts of downward and upward comparisons and expanding the motivations of social comparisons.[4]						In the initial theory, Festinger provided nine main hypotheses. First, he stated that humans have a basic drive to evaluate their opinions and abilities and that people evaluate themselves through objective, nonsocial means (Hypothesis I).[1] Second, Festinger stated that if objective, nonsocial means were not available, that people evaluate their opinions and abilities by comparison to other people (Hypothesis II).[1] Next, he hypothesized that the tendency to compare oneself to another person decreases as the difference between their opinions and abilities becomes more divergent.[1] In other words, if someone is much different from you, you are less likely to compare yourself to that person (Hypothesis III). He next hypothesized that there is a unidirectional drive upward in the case of abilities, which is largely absent in opinions.[1] This drive refers to the value that is placed on doing better and better.[5] (Hypothesis IV). Next, Festinger hypothesizes that there are non-social restraints that make it difficult or even impossible to change one's ability and these restraints are largely absent for opinions.[1] In other words, people can change their opinions when they want to but no matter how motivated individuals may be to improve their ability, there may be other elements that make this impossible[5] (Hypothesis V). Festinger goes on to hypothesize that the cessation of comparison with others is accompanied by hostility or derogation to the extent that continued comparison with those persons implies unpleasant consequences (Hypothesis VI). Next, any factors which increase the importance of some particular group as a comparison group from some particular opinion or ability will increase the pressure toward uniformity concerning that ability or opinion within that group. If discrepancies arise between the evaluator and comparison group there is a tendency to reduce the divergence by either attempting to persuade others, or changing their personal views to attain uniformity. However, the importance, relevance and attraction to a comparison group that affects the original motivation for comparison, mediates the pressures towards uniformity (Hypothesis VII). His next hypothesis states that if persons who are very divergent from one's own opinion or ability are perceived as different from oneself on attributes consistent with the divergence, the tendency to narrow the range of comparability becomes stronger (Hypothesis VIII). Lastly, Festinger hypothesized that when there is a range of opinion or ability in a group, the relative strength of the three manifestations of pressures toward uniformity will be different for those who are close to the mode of the group than for those who are distant from the mode. Those close to the mode will have stronger tendencies to change the positions of others, weaker tendencies to narrow the range of comparison, and even weaker tendencies to change their own opinions (Hypothesis IX).[1]		Since its inception, the initial framework has undergone several advances. Key among these are developments in understanding the motivations that underlie social comparisons, and the particular types of social comparisons that are made. Motives that are relevant to social comparison include self-enhancement,[2][3] maintenance of a positive self-evaluation,[6] components of attributions and validation,[7] and the avoidance of closure.[8][9] While there have been changes in Festinger's original concept, many fundamental aspects remain, including the prevalence of the tendency towards social comparison and the general process that is social comparison.		According to Thorton and Arrowood, self-evaluation is one of the functions of social comparison. This is one process that underlies how an individual engages in social comparison.[10] Each individual's specific goals will influence how they engage in social comparison. For self-evaluation, people tend to choose a comparison target that is similar to themselves.[11] Specifically, they are most interested in choosing a target who shares some distinctive characteristic with themselves. Research suggests that most people believe that choosing a similar target helps ensure the accuracy of the self-evaluation. However, individuals do not always act as unbiased self-evaluators, and accurate self-evaluations may not be the primary goal of social comparison.		Individuals may also seek self-enhancement, or to improve their self-esteem.[11] They may interpret, distort, or ignore the information gained by social comparison to see themselves more positively and further their self-enhancement goals. They will also choose to make upward (comparing themselves to someone better off) or downward (comparing themselves to someone worse off) comparisons, depending on which strategy will further their self-enhancement goals. They may also avoid making comparisons period, or avoid making certain types of comparisons. Specifically, when an individual believes that their ability in a specific area is low, they will avoid making upward social comparisons in that area. Unlike for self-evaluation goals, people engaging in social comparison with the goal of self-enhancement may not seek out a target that is similar to themselves. In fact, if a target's similarity is seen as a threat, due to the target outperforming the individual on some dimension, the individual may downplay the similarity of the target to themselves.		Later advances in theory led to self-enhancement being one of the four self-evaluation motives:, along with self-assessment, self-verification, and self-improvement.		Wills introduced the concept of downward comparison in 1981.[3] Downward social comparison is a defensive tendency that is used as a means of self-evaluation. When a person looks to another individual or group that they consider to be worse off than themselves in order to feel better about their self or personal situation, they are making a downward social comparison. Research has suggested that social comparisons with others who are better off or superior, or upward comparisons, can lower self-regard,[12] whereas downward comparisons can elevate self-regard.[13] Downward comparison theory emphasizes the positive effects of comparisons in increasing one's subjective well-being.[3] For example, it has been found that breast cancer patients made the majority of comparisons with patients less fortunate than themselves.[14]		Although social comparison research has suggested that upward comparisons can lower self-regard, Collins indicates that this is not always the case.[15] Individuals make upward comparisons, whether consciously or subconsciously, when they compare themselves with an individual or comparison group that they perceive as superior or better than themselves in order to improve their views of self or to create a more positive perception of their personal reality. Upward social comparisons are made to self-evaluate and self-improve in the hopes that self-enhancement will also occur. In an upward social comparison, people want to believe themselves to be part of the elite or superior, and make comparisons highlighting the similarities between themselves and the comparison group, unlike a downward social comparison, where similarities between individuals or groups are disassociated.[8]		It has also been suggested that upward comparisons may provide an inspiration to improve, and in one study it was found that while breast cancer patients made more downward comparisons, they showed a preference for information about more fortunate others.[16]		Another study indicated that people who were dieting often used upward social comparisons by posting pictures of thinner people on their refrigerators.[15] These pictures served as not only a reminder of an individuals current weight, but also as an inspiration of a goal to be reached. In simple terms, downward social comparisons are more likely to make us feel better about ourselves, while upward social comparisons are more likely to motivate us to achieve more or reach higher.		Aspinwall and Taylor looked at mood, self-esteem, and threat as moderators that drive individuals to choose to make upward or downward social comparisons.[17] Downward comparisons in cases where individuals had experienced a threat to their self-esteem produced more favorable self-evaluations.		Aspinwall and Taylor found that upward social comparisons were good in circumstances where the individuals making the comparisons had high self-esteem, because these types of comparisons provided them with more motivation and hope than downward social comparisons.[17] However, if these individuals had experienced a recent threat or setback to their self-esteem, they reported that upward comparisons resulted in a more negative affect than downward comparisons.		However, people with low self-esteem or people who are experiencing some sort of threat in their life (such as doing poorly in school, or suffering from an illness) tend to favor downward comparisons over upward comparisons. People with low self-esteem and negative affect improve their mood by making downward comparisons. Their mood does not improve as much as it would if they had high self-esteem. Even for people with low self-esteem, these downward social comparisons do improve their negative mood and allow them to feel hope and motivation for their future.		Individuals who have a negative mood improve their mood by making upward social comparisons, regardless of their level of self-esteem. In addition, both individuals with high self-esteem and low self-esteem who are in a positive mood elevate their mood further by making upward comparisons. However, for those who have recently experienced a threat to their self-esteem or a setback in their life, making upward social comparisons instead of downward social comparisons results in a more negative affect. Self-esteem and existence of a threat or setback in an individual's life are two moderators of their response to upward or downward comparisons.		Because individuals are driven upwards in the case of abilities, social comparisons can drive competition among peers.[18] In this regard, the psychological significance of a comparison depends on the social status of an individual, and the context in which their abilities are being evaluated.		Competitiveness resulting from social comparisons may be greater in relation to higher social status because individuals with more status have more to lose. In one study, students in a classroom were presented with a bonus point program where, based on chance, the grades for some students would increase and the grades for others would remain the same. Despite the fact that students could not lose by this program, higher-status individuals were more likely to object to the program, and more likely to report a perceived distributive injustice. It was suggested that this was a cognitive manifestation of an aversion to downward mobility, which has more psychological significance when an individual has more status.[19]		When individuals are evaluated where meaningful standards exist, such as in an academic classroom where students are ranked, then competitiveness increases as proximity to a standard of performance increases. When the only meaningful standard is the top, then high-ranking individuals are most competitive with their peers, and individuals at low and intermediate ranks are equally competitive. However, when both high and low rankings hold significance, then individuals at high and low ranks are equally competitive, and are both more competitive than individuals at intermediate ranks.[20][21]		Several models have been introduced to social comparison, including the self-evaluation maintenance model (SEM),[12] proxy model,[22] the triadic model and the three-selves model.[23]		The SEM model proposes that we make comparisons to maintain or enhance our self-evaluations, focusing on the antagonistic processes of comparison and reflection.[12] Abraham Tesser has conducted research on self-evaluation dynamics that has taken several forms. A self-evaluation maintenance (SEM) model of social behavior focuses on the consequences of another person's outstanding performance on one's own self-evaluation. It sketches out some conditions under which the other's good performance bolsters self-evaluation, i.e., "basking in reflected glory", and conditions under which it threatens self-evaluation through a comparison process.[24]		The proxy model anticipates the success of something that is unfamiliar. The model proposes that if a person is successful or familiar with a task, then he or she would also be successful at a new similar task. The proxy is evaluated based on ability and is concerned with the question "Can I do X?" A proxy's comparison is based previous attributes. The opinion of the comparer and whether the proxy exerted maximum effort on a preliminary task are variables influencing his or her opinion.[8]		The Triadic Model builds on the attribution elements of social comparison, proposing that opinions of social comparison are best considered in terms of 3 different evaluative questions: preference assessment (i.e., "Do I like X?"), belief assessment (i.e., "Is X correct?"), and preference prediction (i.e., "Will I like X?"). In the Triadic Model the most meaningful comparisons are with a person who has already experienced a proxy and exhibits consistency in related attributes or past preferences.[8]		The three-selves model proposes that social comparison theory is a combination of two different theories. One theory is developed around motivation and the factors that influence the type of social comparison information people seek from their environment and the second is about self-evaluation and the factors that influence the effects of social comparisons on the judgments of self.[23] While there has been much research in the area of comparison motives, there has been little in the area of comparative evaluation. Explaining that the self is conceived as interrelated conceptions accessible depending upon current judgment context[25] and taking a cue from Social Cognitive Theory, this model examines the Assimilation effect and distinguishes three classes of working Self-concept ideas: individual selves, possible selves and collective selves.		The media has been found to play a large role in social comparisons. Researchers examining the social effects of the media have used social comparison theory have found that in most cases women tend to engage in upward social comparisons with a target other, which results in more negative feelings about the self. The majority of women have a daily opportunity to make upward comparison by measuring themselves against some form of societal ideal. Social comparisons have become a relevant mechanism for learning about the appearance-related social expectations among peers and for evaluating the self in terms of those standards" (Jones, 2001, P. 647).		Although men do make upward comparisons, research finds that more women make upward comparisons and are comparing themselves with unrealistically high standards presented in the media.[26] As women are shown more mainstream media images of powerful, successful and thin women, they perceive the "ideal" to be the norm for societal views of attractive. Some women have reported making upward comparisons in a positive manner for the purposes of self-motivation, but the majority of upward comparisons are made when the individual is feeling lesser and therefore evoke a negative connotation.		Many criticisms arose regarding Festinger's similarity hypothesis. Deutsch and Krauss[27] argued that people actually seek out dissimilar others in their comparisons maintaining that this is important for providing valuable self-knowledge, as demonstrated in research.[28][29] Ambiguity also circulated about the important dimensions for similarity. Goethals and Darley clarified the role of similarity suggesting that people prefer to compare those who are similar on related attributes such as opinions, characteristics or abilities to increase confidence for value judgments, however those dissimilar in related attributes are preferred when validating one's beliefs.[7]		
Simultaneous recruiting of new graduates or periodic recruiting of new graduates (新卒一括採用, Shinsotsu-ikkatsu-saiyō) is the custom that companies hire new graduates all at once and employ them. This custom was unique to Japan and South Korea, however, a 2010 age discrimination law enforced in South Korea bans employers from discriminating against job-seekers who have recently graduated from high school or university.[1] Now Japan is the only country practising this custom.						In Japan, entry-level jobs are classified further into three categories, that is, entry-level positions for students who have not graduated from high school or university yet, entry-level positions for job-seekers who have recently graduated and entry-level positions for those who have less than 3 years' work experience, however, very few employers post jobs for entry-level positions for job-seekers who have recently graduated. That is why job-seekers who have recently graduated want to apply for entry-level positions for students who have not graduated from high school or university yet.		In Japan, most students hunt for jobs before graduation from university or high school, seeking "informal offers of employment" (内々定, nainaitei) one year before graduation, which will hopefully lead to "formal offer of employment" (内定, naitei) six months later, securing them a promise of employment by the time they graduate. Japanese university students generally begin job hunting all at once in their third year. The government permits companies to begin the selection process and give out informal offers beginning April 1, at the start of the fourth year. These jobs are mainly set to begin on April 1 of the following year. Due to this process, attaining a good position as a regular employee at any other time of year, or any later in life, is extremely difficult.		Since companies prefer to hire new graduates, students who are unsuccessful in attaining a job offer upon graduating often opt to stay in school for another year. According to a survey conducted by Mynavi, nearly 80% of job-seekers who had recently graduated from university had difficulty applying for entry-level positions in Japan.[2] This is in contrast to other countries, where companies do not generally discriminate against those who have recently graduated.		By contrast, potential employees in Japan are largely judged by their educational background. The prestige of the university and high school that a student attends has a marked effect on their ability to find similarly sought-after jobs as adults.		Large companies in particular (e.g. those listed in Nikkei 225), prefer to hire new graduates of prestigious universities "in bulk" to replace retiring workers and groom in-house talent, and the numbers can vary widely from year to year. Employers tend to hire a group of people in a mechanical fashion every year. One example is Toyota; the company hired over 1,500 new graduates in 2010, but this number was barely half of the number employed the year before, and Toyota announced its intention to cut new hires in 2011 further down to 1,200. The company may offer more jobs later on, but those who missed out on the current round of hiring will have a slim chance of gaining a position because they will be overshadowed by fresh graduates.		This practice leaves thousands of young Japanese sidelined in extended studies, part-time jobs, or on unemployment benefits instead of fully participating in the domestic economy and contributes to producing a great number of freeters and neets in Japan.		According to the nonprofit group Lifelink's survey conducted in July, 2013, one in five Japanese college students thought about committing suicide during the job-hunting process.		This custom has been seen to cause many social problems in modern Japan. Students who do not reach a decision about their employment before graduating from university often face great hardships searching for a job after the fact, as the majority of Japanese companies prefer to hire students scheduled to graduate in the spring. In recent years, an increasing number of university seniors looking for jobs have chosen to repeat a year to avoid being placed in the "previous graduate" category by companies. Under the current system, Japanese companies penalize students who study overseas or have already graduated.[citation needed]		Reiko Kosugi, a research director at the Japan Institute for Labor Policy and Training, criticized this process in a 2006 essay in The Asia-Pacific Journal, saying, "If business is in a slump at the point of one's graduation and he cannot get a job, this custom produces inequality of opportunity, and people in this age bracket tend to remain unemployed for a long time."[3] Nagoya University professor Mitsuru Wakabayashi has stated, "If this custom is joined to permanent employment, it produces closed markets of employment, where outplacement is hard, and the employees tend to obey any and all unreasonable demands made by their companies so as not to be fired.",[4] and Yuki Honda, a professor at the University of Tokyo's Graduate School of Education, has said "Whether they get a job when they graduate decides their whole life". Ken Mogi, a Japanese brain scientist, points out that limiting job opportunities would lead to a human rights issue and that Japanese companies cannot secure non-traditional competent people in the current job hunting system.[5]		
Work–life balance is a concept including proper prioritizing between "work" (career and ambition) and "lifestyle" (health, pleasure, leisure, family and spiritual development/meditation). This is related to the idea of lifestyle choice.		The work–leisure dichotomy was invented in the mid-1801s.[1][2] Paul Krassner remarked that anthropologists use a definition of happiness that is to have as little separation as possible "between your work and your play".[3][4] The expression "work–life balance" was first used in the United Kingdom in the late 1970s to describe the balance between an individual's work and personal life.[5] In the United States, this phrase was first used in 1986.		According to 2010 National Health Interview Survey Occupational Health Supplement data, 16% of U.S. workers reported difficulty balancing work and family. Imbalance was more prevalent among workers aged 30–44 (19%) compared with other age groups; non-Hispanic black workers (19%) compared with non-Hispanic white workers (16%), and Hispanic workers (15%); divorced or separated workers (19%) compared with married workers (16%), widowed workers (13%), and never married workers (15%); and workers having a bachelor's degree and higher (18%) compared with workers having a high school diploma or G.E.D. (16%), and workers with less than a high school education (15%). Workers in agriculture, forestry, fishing, and hunting industries (9%) had a lower prevalence rate of work–family imbalance compared to all employed adults (16%). Among occupations, a higher prevalence rate of work–family imbalance was found in legal occupations (26%), whereas a lower prevalence rate was observed for workers in office and administrative support (14%) and farming, forestry, and fishing occupations (10%).[6]						By working in an organization, employees identify, to some extent, with the organization, as part of a collective group.[7] Organizational values, norms and interests become incorporated in the self-concept as employees increase their identification with the organization. However, employees also identify with their outside roles, or their "true self".[8] Examples of these might be parental/caretaker roles, identifications with certain groups, religious affiliations, align with certain values and morals, mass media etc.		Employee interactions with the organization, through other employees, management, customers, or others, reinforces (or resists) the employee identification with the organization.[8] Simultaneously, the employee must manage their "true self" identification. In other words, identity is "fragmented and constructed" through a number of interactions within and out of the organization; employees don’t have just one self.		Most employees identify with not only the organization, but also other facets of their life (family, children, religion, etc.). Sometimes these identities align and sometimes they do not. When identities are in conflict, the sense of a healthy work–life balance may be affected. Organization members must perform identity work so that they align themselves with the area in which they are performing to avoid conflict and any stress as a result.		Work–life conflict is not gender-specific. According to the Center for American Progress, 90 percent of working mothers and 95 percent of working fathers report work–family conflict.[9] However, because of the social norms surrounding each gender role, and how the organization views its ideal worker, men and women handle the work–life balance differently. Organizations play a large part in how their employees deal with work–life balance. Some companies have taken proactive measures in providing programs and initiatives to help their employees cope with work–life balance.		The conflict of work and family can be exacerbated by perceived deviation from the "ideal worker" archetype, leading to those with caretaker roles to be perceived as not as dedicated to the organization. This has a disproportionate impact on working mothers,[10] who are seen as less worthy of training than childless women.[11]		Many authors believe that parents being affected by work–life conflict will either reduce the number of hours one works, where other authors suggest that a parent may run away from family life or work more hours at a workplace.[12] This implies that each individual views work–life conflict differently.		Research conducted by the Kenexa Research Institute (KRI) evaluated how male and female workers perceive work–life balance and found that women are more positive than men in how they perceive their company’s efforts to help them balance work and life responsibilities. The report is based on the analysis of data drawn from a representative sample of 10,000 U.S. workers who were surveyed through WorkTrends, KRI’s annual survey of worker opinions. The results indicated a shift in women’s perceptions about work–life balance. In the past, women often found it more difficult to maintain balance due to the competing pressures at work and demands at home.[13]		"The past two decades have witnessed a sharp decline in men’s provider role, caused in part by growing female labor participation and in part by the weakening of men’s absolute power due to increased rates of unemployment and underemployment" states sociologist Jiping Zuo. She continues on to state that "Women’s growing earning power and commitment to the paid workforce together with the stagnation of men’s social mobility make some families more financially dependent on women. As a result, the foundations of the male dominance structure have been eroded."[14]		Today there are many young women who do not want to just stay at home and do house work, but want to have careers. About 64% of mothers whose youngest child was under age six, and 77% of mothers with a youngest child age 6-17 were employed in 2010, indicating that the majority of women with dependent care responsibilities cannot or do not wish to give up careers. While women are increasingly represented in the work force, they still face challenges balancing work and home life. Both domestic and market labor compete for time and energy. "For women, the results show that only time spent in female housework chores has a significant negative effect on wages".[citation needed]		Many men do not see work alone as providing their lives with full satisfaction; and want a balance between paid work and personal attachments, without being penalized at work.[15][16] These men may desire to work part-time, in order to spend more time with their families.[17][18]		More men are realizing that work is not their only primary source of fulfillment from life. A new study on fatherhood (2010) shows that more men are looking for alternatives to their 40-hour workweek in order to spend more time with their family. Though working less means a smaller paycheck and higher stress levels, men are looking for flexibility just as much as women. However, with an ever-changing society, flexibility is becoming much more apparent. "It seems that some traditional stereotypes are starting to lessen just a bit in terms of who’s responsible for care of the children" says human resource specialist Steve Moore. Traditionalism is becoming less frequent due to what’s actually practical for each individual family.[19]		Men often face unequal opportunity to family life as they are often expected to be the financial supporter of the family unit, "the masculine ideal of a worker unencumbered by caregiving obligations is built into workplace structures and patterns of reward."[20]		Most recently, there has been a shift in the workplace as a result of advances in technology. As Bowswell and Olson-Buchanan stated, "increasingly sophisticated and affordable technologies have made it more feasible for employees to keep contact with work". Employees have many methods, such as emails, computers, and cell phones, which enable them to accomplish their work beyond the physical boundaries of their office. Employees may respond to an email or a voice mail after-hours or during the weekend, typically while not officially "on the job". Researchers have found that employees who consider their work roles to be an important component of their identities will be more likely to apply these communication technologies to work while in their non-work domain.[21]		Some theorists suggest that this blurred boundary of work and life is a result of technological control. Technological control "emerges from the physical technology of an organization".[22] In other words, companies use email and distribute smartphones to enable and encourage their employees to stay connected to the business even when they are not in the office. This type of control, as Barker argues, replaces the more direct, authoritarian control, or simple control, such as managers and bosses. As a result, communication technologies in the temporal and structural aspects of work have changed, defining a "new workplace" in which employees are more connected to the jobs beyond the boundaries of the traditional workday and workplace.[21] The more this boundary is blurred, the higher work-to-life conflict is self-reported by employees.[21]		Employee assistance professionals say there are many causes for this situation ranging from personal ambition and the pressure of family obligations to the accelerating pace of technology.[23] According to a recent study for the Center for Work-Life Policy, 1.7 million people[where?] consider their jobs and their work hours excessive because of globalization.[citation needed]		Mental health is a balancing act that may be affected by four factors: the influence of unfavourable genes, by wounding trauma, by private pressures and most recently by the stress of working.[24] Many people expose themselves unsolicited to the so-called job stress, because the "hard worker" enjoys a very high social recognition. These aspects can be the cause of an imbalance in the areas of life. But there are also other reasons which can lead to such an imbalance.		Remarkable is, for example, the increase in non-occupational activities with obligation character, which include mainly house and garden work, maintenance and support of family members or volunteer activities. All this can contribute to the perception of a chronic lack of time.[25] This time pressure is, amongst others, influenced by their own age, the age and number of children in the household, marital status, the profession and level of employment as well as the income level.[26] The psychological strain, which in turn affects the health, increases due to the strong pressure of time, but also by the complexity of work, growing responsibilities, concern for long-term existential protection and more.[27] The mentioned stresses and strains could lead in the long term to irreversible, physical signs of wear as well as to negative effects on the human cardiovascular and immune systems.[28]		Prominent cultural beliefs that parenthood is the best avenue for a happy fulfilling life may not be justified. In, The Joys of Parenthood Reconsidered, what was found is the opposite, that parents actually suffer worse mental and physical health than childless adults. This is associated with the high costs of parenthood described in the article. Simon states that, "In America we lack institutional supports that would help ease the social and economic burdens associated with parenthood." [29]		Psychoanalysts diagnose uncertainty as the dominant attitude to life in the postmodern society.[30] This uncertainty can be caused by the pressure which is executed from the society to the humans. It is the uncertainty to fail, but also the fear of their own limits, not to achieve something what the society expects, and especially the desire for recognition in all areas of life.[30] In today's society we are in a permanent competition. Appearance, occupation, education of the children - everything is compared to a media staged ideal. Everything should be perfect, because this deep-rooted aversion to all average, the pathological pursuit to excellence - these are old traditions.[30] Who ever wants more - on the job, from the partner, from the children, from themselves - will one day be burned out and empty inside. He is then faced with the realization that perfection does not exist.[31] Who is nowadays empty inside and burned out, is in the common language a Burnout. But due to the definitional problems Burnout is till this date not a recognized illness.[24] An attempt to define this concept more closely, can be: a condition that gets only the passionate, that is certainly not a mental illness but only a grave exhaustion (but can lead to numerous sick days).[24] It can benefit the term that it is a disease model which is socially acceptable and also, to some extent, the individual self-esteem stabilizing. This finding in turn facilitates many undetected depressed people, the way to a qualified treatment.[24] According to experts in the field, in addition to the ultra hard-working and the idealists mainly the perfectionist, the loner, the grim and the thin-skinned are especially endangered of a burnout. All together they usually have a lack of a healthy distance to work.[24]		Another factor is also, that, for example decision-makers in government offices and upper echelons are not allowed to show weaknesses or signs of disease etc., because this would immediately lead to doubts of the ability for further responsibility. Only 20% of managers (e.g. in Germany) do sports regularly and also only 2% keep regularly preventive medical check-up.[32] In such a position other priorities seem to be set and the time is lacking for regular sports. Frightening is that the job has such a high priority, that people waive screening as a sign of weakness. In contrast to that, the burnout syndrome seems to be gaining popularity. There seems nothing to be ashamed to show weaknesses, but quite the opposite: The burnout is part of a successful career like a home for the role model family.[33] Besides that the statement which describes the burnout as a "socially recognized precious version of the depression and despair that lets also at the moment of failure the self-image intact" fits and therefore concludes "Only losers become depressed, burnout against it is a diagnosis for winners, more precisely, for former winners.".[34]		However, it is fact that four out of five Germans complain about too much stress. One in six under 60 swallows at least once a week, a pill for the soul, whether it is against insomnia, depression or just for a bit more drive in the stressful everyday life.[30] The phases of burnout can be described, among other things, first by great ambition, then follows the suppression of failure, isolation and finally, the cynical attitude towards the employer or supervisor. Concerned persons have very often also anxiety disorders and depressions, which are serious mental diseases. Depressions are the predominant causes of the nearly 10,000 suicides that occur alone each year in Germany.[24] The implications of such imbalances can be further measured in figures: In 1993, early retirement due to mental illness still made 15.4 percent of all cases. In 2008, there were already 35.6 percent. Even in the days of illness, the proportion of failures due to mental disorders increased. Statisticians calculated that 41 million absent days in 2008 went to the account of these crises, which led to 3.9 billion euros in lost production costs.[24]		Steven L. Sauter, chief of the Applied Psychology and Ergonomics Branch of the National Institute for Occupational Safety and Health in Cincinnati, Ohio, states that recent studies show that "the workplace has become the single greatest source of stress".[23] Michael Feuerstein, professor of clinical psychology at the Uniformed Services University of the Health Sciences at Bethesda Naval Hospital states, "We're seeing a greater increase in work–related neuroskeletal disorders from a combination of stress and ergonomic stressors".[23] The number of stress-related disability claims by American employees has doubled[when?] according to the Employee Assistance Professionals Association in Arlington, Virginia. Seventy-five to ninety percent of physician visits are related to stress and, according to the American Institute of Stress, the cost to industry has been estimated at $200 billion-$300 billion a year.[23]		Problems caused by stress have become a major concern to both employers and employees. Symptoms of stress are manifested both physiologically and psychologically. Persistent stress can result in cardiovascular disease, sexual health problems, a weaker immune system and frequent headaches, stiff muscles, or backache. It can also result in poor coping skills, irritability, jumpiness, insecurity, exhaustion, and difficulty concentrating. Stress may also perpetuate or lead to binge eating, smoking, and alcohol consumption.[citation needed]		The feeling that simply working hard is not enough anymore is acknowledged by many other American workers. "To get ahead, a seventy-hour work week is the new standard. What little time is left is often divided up among relationships, kids, and sleep." This increase in work hours over the past two decades means that less time will be spent with family, friends, and community as well as pursuing activities that one enjoys and taking the time to grow personally and spiritually.[citation needed]		According to a survey conducted by the National Life Insurance Company, four out of ten U.S. employees state that their jobs are "very" or "extremely" stressful.[23] Those in high-stress jobs are three times more likely than others to suffer from stress-related medical conditions and are twice as likely to quit. The study states that women, in particular, report stress related to the conflict between work and family.		In the study, Work-Family Spillover and Daily Reports of Work and Family Stress in the Adult Labor Force, researchers found that with an increased amount of negative spillover from work to family, the likelihood of reporting stress within the family increased by 74%, and with an increased amount of negative spillover from family to work the likelihood to report stress felt at work increased by 47%.[35]		It has been suggested that very stressful and time-consuming employment cultures can impact the birth rates of a nation. An imbalance of work and life is believed to be one of the causes of the aging of Japan.[citation needed]		Texas Quick, an expert witness at trials of companies who were accused of overworking their employees, states that "when people get worked beyond their capacity, companies pay the price."[23] Although some employers believe that workers should reduce their own stress by simplifying their lives and making a better effort to care for their health, most experts feel that the chief responsibility for reducing stress should be management.		According to Esther M. Orioli, president of Essi Systems, a stress management consulting firm, "Traditional stress-management programs placed the responsibility of reducing stress on the individual rather than on the organization-where it belongs. No matter how healthy individual employees are when they start out, if they work in a dysfunctional system, they’ll burn out."[23]		Work–life balance has been addressed by some employers and has been seen as a benefit to them. Research by Kenexa Research Institute in 2007 shows that those employees who were more favourable toward their organization’s efforts to support work–life balance also indicated a much lower intent to leave the organization, greater pride in their organization, a willingness to recommend it as a place to work and higher overall job satisfaction.[citation needed]		Employers can offer a range of different programs and initiatives, such as flexible working arrangements in the form of part-time, casual and telecommuting work. More proactive employers can provide compulsory leave, strict maximum hours and foster an environment that encourages employees not to continue working after hours.[citation needed].		Studies from Canadian adjunct professor and psychology researcher Yani Likongo demonstrated that sometimes in organizations an idiosyncratic psychological contract is built between the employee and his direct supervisor in order to create an "informal deal" regarding work-life balance. These "deals" support the idea of a constructivist approach including both the employer and the employee, based on a give-and-take situation for both of them.[36]		As of March 2011, paid leave benefits continued to be the most widely available benefit offered by employers in the United States, with paid vacations available to 91 percent of full-time workers in private industry. Access to these benefits, however, varied by employee and establishment characteristics. In private industry, paid vacation benefits were available to only 37 percent of part-time workers. Paid sick leave was available to 75 percent of full-time workers and 27 percent of part-time workers. Paid vacations were available to 90 percent of workers earning wages in the highest 10th percent of private industry employees and only to 38 percent of workers in the lowest 10 percent of private industry wage earners. Access to paid sick leave benefits ranged from 21 percent for the lowest wage category to 87 percent for the highest wage category. These data are from the National Compensation Survey (NCS), which provides comprehensive measures of compensation cost trends and incidence and provisions of employee benefit plans.[37]		It is generally only highly skilled workers that can enjoy such benefits as written in their contracts, although many professional fields would not go so far as to discourage workaholic behaviour.[citation needed] Unskilled workers will almost always have to rely on bare minimum legal requirements.[citation needed] The legal requirements are low in many countries, in particular, the United States. In contrast, the European Union has gone quite far in assuring a legal work–life balance framework, for example pertaining to parental leave and the non-discrimination of part-time workers.[citation needed]		According to Stewart Friedman—professor of Management and founding director of the Wharton School’s Leadership Program and of its Work/Life Integration Project—a "one size fits all" mentality in human resources management often perpetuates frustration among employees. "[It’s not an] uncommon problem in many HR areas where, for the sake of equality, there's a standard policy that is implemented in a way that's universally applicable -- [even though] everyone's life is different and everyone needs different things in terms of how to integrate the different pieces. It's got to be customized."[38]		Friedman’s research indicates that the solution lies in approaching the components of work, home, community, and self as a comprehensive system. Instead of taking a zero-sum approach, Friedman’s Total Leadership program teaches professionals how to successfully pursue "four-way wins"—improved performance across all parts of life.		Although employers are offering many opportunities to help their employees balance work and life, these opportunities may be a catch twenty-two for some female employees. Even if the organization offers part-time options, many women will not take advantage of it as this type of arrangement is often seen as "occupational dead end".[39]		Even with the more flexible schedule, working mothers opt not to work part-time because these positions typically receive less interesting and challenging assignments; taking these assignments and working part-time may hinder advancement and growth. Even when the option to work part-time is available, some may not take advantage of it because they do not want to be marginalized.[39] This feeling of marginalization could be a result of not fitting into the "ideal worker" framework (see: Formation of the "ideal worker" and gender differences).		Additionally, some mothers, after returning to work, experience what is called the maternal wall. The maternal wall is experienced in the less desirable assignments given to the returning mothers. It is also a sense that because these women are mothers, they cannot perform as "ideal workers".[39] If an organization is providing means for working mothers and fathers to better balance their work–life commitments, the general organizational norm needs to shift so the "ideal worker" includes those who must manage a home, children, elderly parents, etc.		Maternity leave and parental leave are leaves of absence for expectant or new mothers (sometimes fathers) for the birth and care of the baby. These policies vary significantly by country (regarding factors such as the length of the leave and what amount of money is paid). They may help create a work–life balance for families, yet in the United States most states do not offer any paid time off for the birth of a child. As of 2015, the US was one of only three countries in the world (the other two being Papua New Guinea and Suriname) that does not have paid maternity leave.[40]		Some new mothers (and fathers) in the US will take unpaid time off, allowed by the Family and Medical Leave Act. The FMLA entitles eligible employees of covered employers to take unpaid, job-protected leave for specified family and medical reasons with continuation of group health insurance coverage under the same terms and conditions as if the employee had not taken leave. Eligible employees are entitled to twelve workweeks of leave in a 12-month period for:		Some states will allow paid time off for maternity leave under the states Temporary Disability Insurance (TDI).[42]		12-month base period up to $959 (2009)		wages in the base year, up to $671, plus dependent allowance of $10 or 7% of weekly benefit for up to 5 dependents (2008)		At the state level, California was the first state to offer paid family leave benefits for its workers. While the benefits only last for 6 weeks [43] this is the first major step for maternity leave in the United States. New Jersey lawmakers are pushing legislation that would make their state the second state to add this worker benefit. Under one New Jersey proposal, workers who take leave would be paid through the state’s temporary disability insurance fund, "augmented by a 0.1 percent charge on workers’ weekly wages."[44] Traditionally, many conservatives have opposed paid family leave, but there is a sign that this mindset is beginning to change. Reverend Paul Schenck, a prominent member of the National Pro-Life Action Center recently stated that he would support paid maternity leave on the assumption that it might encourage women to follow through with their pregnancies instead of having abortions. According to Heyman, "Across the political spectrum, people are realizing these policies have an enormous impact on working families. If you look at the most competitive economies in the world, all the others except the U.S. have these policies in place." [44]		The United States is not as workplace family-oriented as many other wealthy countries. According to a study released by Harvard and McGill University researchers in February 2007, workplace policies for families in the U.S. are weaker than those of all high-income countries and even many middle-and low-income countries.[44] Other differences include the fact that fathers are granted paid paternity leave or paid parental leave in sixty-five countries; thirty one of these countries offer at least fourteen weeks of paid leave. The U.S. does not guarantee this to fathers.(survey) Sweden, Denmark and Norway have the highest level of maternity benefits—Sweden provides 68 weeks paid maternity leave, Norway provides 56 weeks paid maternity leave and Denmark provides 52.[45]		In terms of family guidelines and agreements, corporate policies exclusively refer to marriages among women and men and thereby disregard the situation and the special needs of non-traditional families. These non-traditional families often consist of couples or individuals with lesbian, gay, bisexual or transgender (LGBT) backgrounds that are increasingly under pressure of the community, as their needs within corporations are often paid inadequate attention. Exclusive behavior such as creating environments that do not encourage LGBTs to disclose their sexual orientation, or even neglecting the fact that LGBT are allowed to adopt children, leads to a feeling of isolation and job stress that ultimately negatively effects the WLB of the affected individuals.[46]		As international studies reveal, LGBT-supportive corporate policies strengthen an inclusive environment at the workplace and are therefore beneficial for the affected individuals, and the overall company performance. Reduced discriminatory behavior amongst employees, enhanced job satisfaction, and employee engagement are major reasons for these observations.[47]		However, individual experiences with these kinds of inclusive policies vary, as there seem to be "implementation gaps" between equality and diversity policies, and practice across sectors, workplaces and even within buildings of organizations.[48]		On a macroeconomic level public health policies should be adapted and developed towards more inclusive and diversified approaches regarding minorities such as LGBT, as this is proved to be beneficial to the health of the affected minorities, which in return lowers the cost for the overall public health system.[49]		Religion and spirituality have a major influence in defining employee’s work life-balance. Religion represents an essential issue in diversity management, as the question of accommodating religion at work often raises controversial debate.		Religion is a choice based upon personal belief, but religious and ethical values are often inseparable in the pursuit of one’s livelihood. As a result, religion influences organizations: employees may for example ask for a day-off (Eid, Sabbath), or refuse to work in a company for religious purposes (for example a liquor producer).[50]		Some employers may fear that religion disturbs the "business of business" and thus have a negative impact on the company. In some secular societies such as France, employees are expected to leave their religion at the door of the company.[51]		Poor management of religious diversity may affect employees’ performances if they feel forced to choose between aspects of their religious identity and their jobs. This may also lead to them dissociating themselves from the organization.[51]		Therefore, religious diversity management is essential to ensuring a satisfying work–life balance for employees. The American Title VII of the Civil Rights Act of 1964 states that ‘companies have a duty to provide reasonable religious accommodation’,[52] and religion-based societies in Saudi Arabia or Israel organize religious accommodation with special provisions in government legislation and organizational policies. Some organizations also allow their employees to make up time spent on religious activities out of contractual hours.[51]		Intersectionality infers that oppression against identities is interrelated and not independent. These identities include but are not limited to: race, gender, class, sexual orientation, religion and age. Intersectionality must be understood by companies in order for them to collaborate with their workers in the quest towards providing a greater work life balance. This will result in more highly engaged workers and mutually beneficial gain for all stakeholders. For example: cleaning firms in the UK could recognized that cleaning hours are not conducive for female migrant workers, who are expected to play a more traditional role in the household. In this situation intersectionality is visible through gender (type of job), lower social class (unskilled labour) and cultural beliefs associated with race and ethnicity.[51]		According to a new study by Harvard and McGill University researchers, the United States lags far behind nearly all wealthy countries when it comes to family-oriented workplace policies such as maternity leave, paid sick days and support for breast feeding. Jody Heyman, founder of the Harvard-based Project on Global Working Families and director of McGill’s Institute for Health and Social Policy, states that, "More countries are providing the workplace protections that millions of Americans can only dream of. The United States has been a proud leader in adopting laws that provide for equal opportunity in the workplace, but our work/family protections are among the worst." [53]		This observation is being shared by many Americans today and is considered by many experts to be indicative of the current climate. However, the U.S. Labor Department is examining regulations that give workers unpaid leave to deal with family or medical emergencies (a review that supporters of the FMLA worry might be a prelude to scaling back these protections, as requested by some business groups). Senator Chris Dodd from Connecticut proposed legislation that would enable workers to take six weeks of paid leave. Congress was also expected to reconsider the Healthy Families Act, a bill that would have required employers with at least fifteen employees to provide seven paid sick days per year.[53]		At least 107 countries protect working women’s right to breast-feed and, in at least seventy-three of them, women are paid. The United States does not have any federal legislation guaranteeing mothers the right to breast-feed their infants at work, but 24 states, the District of Columbia and Puerto Rico have laws related to breastfeeding in the workplace.[54]		There is no federal law requiring paid sick days in the United States. When it comes to sick days, 145 countries provide sick days to their employees; 127 provide a week or more per year.		At least 134 countries have laws setting the maximum length of the work week; the U.S. does not have a maximum work week length and does not place any limits on the amount of overtime that an employee is required to work each week. (survey) Sweden, Denmark and Norway have the highest level of maternity benefits—Sweden provides 68 weeks paid maternity leave, Norway provides 56 weeks paid maternity leave and Denmark provides 52.[45]		Even when vacation time is offered in some U.S. companies, some choose not to take advantage of it. A 2003 survey by Management Recruiter International stated that fifty percent of executives surveyed didn’t have plans to take a vacation. They decided to stay at work and use their vacation time to get caught up on their increased workloads.[55]		American workers average approximately ten paid holidays per year while British workers average twenty-five holidays and German employees thirty. Americans are at "work" twelve weeks more a year in total hours than Europeans.		The European Union promotes various initiatives regarding work-life balance and encourages its member states to implement family-friendly policies.[56] In Europe, the Working Time Directive has implemented a maximum 48-hour working week.[57] Many countries have opted for fewer hours. France introduced a 35-hour workweek.[58] Contradictory to the Scandinavian countries, there is no evidence of state policies that absolutely encourage men to take on a larger share of domestic work in France, Portugal, or Britain.[59] In a 2007, the European Quality of Life Survey found that countries in south-eastern Europe had the most common problems with work–life balance. In Croatia and Greece, a little over 70% of working citizens say that they are too tired to do household jobs at least several times a month because of work.[60]		In Britain, legislation has been passed allowing parents of children under six to request a more flexible work schedule. Companies must approve this request as long as it does not damage the business. A 2003 Survey of graduates in the UK revealed that graduates value flexibility even more than wages.[61]		In all twenty-five European Union countries, voters "punish" politicians who try to shrink vacations. "Even the twenty-two days Estonians, Lithuanians, Poles and Slovenians count as their own is much more generous than the leave allotted to U.S. workers." [55] According to a report by the Families and Work Institute, the average vacation time that Americans took each year averaged 14.6 days.		According to Jeremy Reynolds, unions can lobby for benefits, pay, training, safety measures, and additional factors that impact the costs and benefits of work hours. "Unions can also have a more direct impact on hour mismatches through their efforts to change the length of the workday, work week, and work year, and to increase vacation and leave time." This is why workers in countries where there are strong unions usually work fewer hours and have more generous leave policies than workers who are in countries where there are weaker unions.[62]		It is critical to mention that cultural factors influence why and how much we work. As stated by Jeremy Reynolds, "cultural norms may encourage work as an end in itself or as a means to acquiring other things, including consumer products." This might be why Americans are bound to work more than people in other countries. In general, Americans always want more and more, so Americans need to work more in order to have the money to spend on these consumer products.[62]		One of the aspects of happiness is when you can make as little distinction as possible between your work and your play		
In many countries (such as Australia, Canada, New Zealand, United Kingdom, Denmark or the United States), a white-collar worker is a person who performs professional, managerial, or administrative work. White-collar work is performed in an office, cubicle, or other administrative setting. Other types of work are those of a blue-collar worker, whose job requires manual labor and a pink-collar worker, whose labor is related to customer interaction, entertainment, sales, or other service-oriented work. Many occupations blend blue, white and pink (service) industry categorizations.[1]						The term refers to the white dress shirts of male office workers common through most of the nineteenth and twentieth centuries in Western countries, as opposed to the blue overalls worn by many manual laborers.		The term "white collar" is credited to Upton Sinclair, an American writer, in relation to contemporary clerical, administrative, and management workers during the 1930s,[2] though references to white-collar work appear as early as 1935.		Formerly a minority in the agrarian and early industrial societies, white-collar workers have become a majority in industrialized countries due to modernization and outsourcing of manufacturing jobs.[1]		The blue-collar and white-collar descriptors as it pertains to work dress may no longer be an accurate descriptor as office attire has broadened beyond a white shirt and tie. Employees in office environments may wear a variety of colors, may dress business casual or wear casual clothes altogether. In addition work tasks have blurred. "White-collar" employees may perform "blue-collar" tasks (or vice versa). An example would be a restaurant manager who may wear more formal clothing yet still assist with cooking food or taking customers' orders or a construction worker who also performs desk work.		Less physical activity among white-collar workers has been thought to be a key factor in increased life-style related health conditions such as fatigue, obesity, diabetes, hypertension, cancer, and heart disease.[3] Workplace interventions such as alternative activity workstations, sit-stand desks, promotion of stair use are among measures being implemented to counter the harms of sedentary workplace environments.[4] A Cochrane systematic review published in 2016 concluded that "at present there is very low quality evidence that sit-stand desks can reduce sitting at work at the short term. There is no evidence for other types of interventions." Also, evidence was lacking on the long term health benefits of such interventions.[5]		Notes		Further reading		
Career assessments are tools that are designed to help individuals understand how a variety of personal attributes (i.e., interests, values, preferences, motivations, aptitudes and skills), impact their potential success and satisfaction with different career options and work environments. Career assessments have played a critical role in career development and the economy in the last century (Whiston and Rahardja, 2005). Assessments of some or all of these attributes are often used by individuals or organizations, such as university career service centers, career counselors, outplacement companies, corporate human resources staff, executive coaches, vocational rehabilitation counselors, and guidance counselors to help individuals make more informed career decisions.		In part, the popularity for this tool is due to the National Defense Education Act of 1958, which funded career guidance in schools.[1] Focus was put onto tools that would help high school students determine which subjects they may want to focus on to reach a chosen career path. Since 1958, career assessment tool options have exploded.						Career assessments come in many forms and vary along several dimensions. The assessments selected by individuals or administrators vary depending on their personal beliefs regarding the most important criteria when considering career choices, as well as the unique needs of the individual considering a career decision. Some common points of variance are:		Career assessments are designed to discover the skills, aptitude and talents of candidates. A self-assessment can be a useful tool in assessing the areas in which a candidate has strengths and limitations. The results can be useful in helping candidates to choose a career that is in tune with their goals and talents. While the validation of each instrument may vary from test to test, overall these types of assessments have been proven to introduce more career options, increase satisfaction in one’s career plan and increase the understanding of oneself (Prince et al., 2003).		Data as to how often people change careers are unavailable while there's a considerable mythology about it, no systematic studies have been undertaken.[4] However, many people change careers more than once. Some make changes because the career path they chose is no longer viable (to wit, buggy whip makers are no longer in high demand). Or because as they mature throughout their lifespan their interests evolve. The biggest benefit of career assessment, therefore, is that it enables candidates to make the best career decisions to grow both personally and professionally.		To make an assessment of their skills, candidates can pursue many avenues, they can take career interest tests such as the Strong Interest Inventory or the Campbell Interest and Skill inventory, which is based on the Strong.[5] Alternatively, they can conduct a self-assessment; they can use the plethora of career books designed to help with this task. In fact, there are a myriad of helpful books, the most famous of which is, Richard Bolles, "What Color is Your Parachute." In addition, they can seek expert help from career counselors, career coaches or, when warranted, psychologists or other mental health professionals. These professionals use a variety of techniques to determine the talents of candidates. Also, career counselors, career coaches and executive coaches can guide candidates on how to go about planning their career to achieve professional success.		People who are unhappy in their work-life may be uncertain as to where to turn for help.[6] They may have seen career counselors or career coaches or read self-help books and found that their difficulties did not yield to these interventions.[7]		Individuals may be stymied in their careers not only because they lacked career development and job hunting skills but also because they were driven by unconscious factors outside of their awareness.[8]		Psychoanalytically-Informed Career Assessment (also referred to as psychodynamic career assessment) developed in 2000 by Dr. Lynn Friedman, aims to understand the unconscious factors that create conflicts and identify ways to resolve these conflicts.[7]		Focused on individuals who seek career counseling, but end up undermining the process,[8] Psychoanalytically-Informed Career Assessment explores whether the conflicts seen in their careers or career counseling sessions are repeated elsewhere in their lives, for example in school, or with their parents.[9]		Interventions for these individuals might include one or all of the following: career counseling, psychotherapy or psychoanalysis.[8]		Career assessment, in the form of tests and other structured and unstructured tools, can be very useful for those who are uncertain about the array of career possibilities. However, there are some drawbacks to each. At best, the results of individual career assessments provide targeted information that may not address a particular individual's needs. In addition, some of the best individual assessment tools require the help of a qualified professional to ensure the results are interpreted correctly and usefully.		Also, many of the tests are based on the person’s view of himself or herself. If someone is not self-aware, the results may not be accurate.[10] Many times they do not take into account that people have natural blind spots. The test is only as good as its user and individuals are often not clearly aware of their own strengths and weaknesses.		
Career assessments are tools that are designed to help individuals understand how a variety of personal attributes (i.e., interests, values, preferences, motivations, aptitudes and skills), impact their potential success and satisfaction with different career options and work environments. Career assessments have played a critical role in career development and the economy in the last century (Whiston and Rahardja, 2005). Assessments of some or all of these attributes are often used by individuals or organizations, such as university career service centers, career counselors, outplacement companies, corporate human resources staff, executive coaches, vocational rehabilitation counselors, and guidance counselors to help individuals make more informed career decisions.		In part, the popularity for this tool is due to the National Defense Education Act of 1958, which funded career guidance in schools.[1] Focus was put onto tools that would help high school students determine which subjects they may want to focus on to reach a chosen career path. Since 1958, career assessment tool options have exploded.						Career assessments come in many forms and vary along several dimensions. The assessments selected by individuals or administrators vary depending on their personal beliefs regarding the most important criteria when considering career choices, as well as the unique needs of the individual considering a career decision. Some common points of variance are:		Career assessments are designed to discover the skills, aptitude and talents of candidates. A self-assessment can be a useful tool in assessing the areas in which a candidate has strengths and limitations. The results can be useful in helping candidates to choose a career that is in tune with their goals and talents. While the validation of each instrument may vary from test to test, overall these types of assessments have been proven to introduce more career options, increase satisfaction in one’s career plan and increase the understanding of oneself (Prince et al., 2003).		Data as to how often people change careers are unavailable while there's a considerable mythology about it, no systematic studies have been undertaken.[4] However, many people change careers more than once. Some make changes because the career path they chose is no longer viable (to wit, buggy whip makers are no longer in high demand). Or because as they mature throughout their lifespan their interests evolve. The biggest benefit of career assessment, therefore, is that it enables candidates to make the best career decisions to grow both personally and professionally.		To make an assessment of their skills, candidates can pursue many avenues, they can take career interest tests such as the Strong Interest Inventory or the Campbell Interest and Skill inventory, which is based on the Strong.[5] Alternatively, they can conduct a self-assessment; they can use the plethora of career books designed to help with this task. In fact, there are a myriad of helpful books, the most famous of which is, Richard Bolles, "What Color is Your Parachute." In addition, they can seek expert help from career counselors, career coaches or, when warranted, psychologists or other mental health professionals. These professionals use a variety of techniques to determine the talents of candidates. Also, career counselors, career coaches and executive coaches can guide candidates on how to go about planning their career to achieve professional success.		People who are unhappy in their work-life may be uncertain as to where to turn for help.[6] They may have seen career counselors or career coaches or read self-help books and found that their difficulties did not yield to these interventions.[7]		Individuals may be stymied in their careers not only because they lacked career development and job hunting skills but also because they were driven by unconscious factors outside of their awareness.[8]		Psychoanalytically-Informed Career Assessment (also referred to as psychodynamic career assessment) developed in 2000 by Dr. Lynn Friedman, aims to understand the unconscious factors that create conflicts and identify ways to resolve these conflicts.[7]		Focused on individuals who seek career counseling, but end up undermining the process,[8] Psychoanalytically-Informed Career Assessment explores whether the conflicts seen in their careers or career counseling sessions are repeated elsewhere in their lives, for example in school, or with their parents.[9]		Interventions for these individuals might include one or all of the following: career counseling, psychotherapy or psychoanalysis.[8]		Career assessment, in the form of tests and other structured and unstructured tools, can be very useful for those who are uncertain about the array of career possibilities. However, there are some drawbacks to each. At best, the results of individual career assessments provide targeted information that may not address a particular individual's needs. In addition, some of the best individual assessment tools require the help of a qualified professional to ensure the results are interpreted correctly and usefully.		Also, many of the tests are based on the person’s view of himself or herself. If someone is not self-aware, the results may not be accurate.[10] Many times they do not take into account that people have natural blind spots. The test is only as good as its user and individuals are often not clearly aware of their own strengths and weaknesses.		
The Progressive Wage Model (PWM) is a wage structure advocated by the labour movement of Singapore,[1] which is led by the National Trades Union Congress (NTUC), the sole national trade union centre in Singapore. The objective of introducing the Progressive Wage Model is to increase the salaries of workers through the enhancement of skills and improving productivity. Progressive Wage is enforced via business licensing (as opposed to legislation).[2] This model was designed to enable rank and file workers to climb the wage ladder, and provides for a minimum wage. This would then lead to higher wages for the worker and improved overall productivity which helps sustain economic growth.						While the Progressive Wage Model concept was only introduced by the labour movement in June 2012, this remuneration model is actually an amalgamation of various existing programmes and initiatives over the years by the Labour Movement to help workers in Singapore upgrade and up-skill themselves to be able to earn higher wages. These programmes and initiatives include the Skills Redevelopment Programme, the Job Redevelopment Programme, the Best Sourcing Initiatives, and the Inclusive Growth Programme, among others. All of these already help workers in Singapore improve themselves to be able to take on better-paying jobs. However, the Progressive Wage Model takes things a step further by ensuring employers' commitment to their employees' career growth, better wages and increased productivity.		The Progressive Wage Model is based on the key objectives of helping Singaporean workers climb the four ladders of skills upgrading, productivity improvement, career advancement and wage progression. Thus far, seven unionised clusters in Singapore have implemented or are planning to implement a Progressive Wage Model to help workers. The Tripartite Cluster for Cleaners, a working committee formed representatives of the tripartite partners, is pushing for the Progressive Wage Model to help about 10,000 cleaners across various industries. These workers are set to earn a higher entry-level basic wage of between S$1,000 and S$1,200.[3] About half of all cleaners employed under government contracts, or over 3,500 cleaners, currently earn basic wages of at least $1,000 per month. The median wage of cleaners in the civil service was between S$675 and S$950 prior to the introduction of the Progressive Wage Model.		Minister of State Josephine Teo was reported as saying that the government plans to engage only cleaning companies accredited under the National Environment Agency's Enhanced Clean Mark Accreditation Scheme for all contracts called from 1 April 2013.		According to NTUC Secretary General Lim Swee Say, the reason why Singapore can adopt this method of wage improvements through re-skilling and upgrading of workers is based on these factors: the availability of workfare supplements to help the worker while he or she undergoes training to reach the next step of the wage ladder; the establishment of a comprehensive employee training framework in the country; and the government's financial capacity to pay for such training programmes.		The Progressive Wage Model is an enhancement to a basic minimum wage model to help increase the salaries of workers in Singapore.[4] NTUC Secretary General Lim Swee Say was reported as saying that he believed that the shortcomings of a minimum wage system outweigh the benefits. He noted that if the minimum wage was set too low, it would not help workers. On the other hand, if the minimum wage was set too high, that could result in higher unemployment as employers may not be able to afford to pay their workers. He felt the Progressive Wage Model would be a more sustainable approach to helping workers earn better wages.[5]		Labour MP Patrick Tay Teck Guan delivered a speech at the Singapore Budget Debate in parliament in 2013 highlighting how the Progressive Wage Model does not only apply to low wage workers but also to Professional, Managers, and Executives (PMEs).[6] He gave the example of how Singapore Power, a unionized company of The Union of Power and Gas Employees, has implemented a structured path for Technicians to progress to Senior Technician, and thereafter to Technical Officer, Senior Technical Officer and to an Engineer which earns as much as S$7,000 per month.		Singapore's union clusters representing various sectors are working alongside their respective tripartite partners, which include industry players and government officials, to design Progressive Wage Models tailored to meet the needs of each sub-sector. These clusters have set individual targets for progressive wage increases to help especially low-income earners, women and mature workers, the most vulnerable groups of the workforce.		NTUC estimates that the Progressive Wage Model can help about 100,000 workers in the next two to three years from end-2012.[7]		According to Singapore's Ministry of Manpower, Singapore does not have minimum wage and "salary is subject to negotiation and mutual agreement between an employer and an employee or the trade union representing the employees". Singapore is part of the fewer than 10 percent of countries globally that do not have a legislated minimum wage policy. There has not been any outright opposition to the Progressive Wage Model that was introduced in 2012, but several Singaporean politicians and the public have made calls for Singapore to legislate minimum wage to protect low-wage workers. During the Singapore Budget 2013 debate in parliament, Members-of-Parliament such as Inderjit Singh of Ang Mo Kio GRC and Nominated MP Laurence Lien expressed concerns that wages at the bottom of the economic ladder are not increasing quickly enough to help workers cope with a rising cost of living. In an article in The Straits Times in September 2011, Tommy Koh, Ambassador-at-Large for the Singapore government, echoed a recent comment by a journalist from the same paper that "Singapore is a First World country with a Third World wage structure". Koh believed that a minimum wage model would help low-income workers and promote inclusive growth in Singapore society.		
An occupational fatality is a death that occurs while a person is at work or performing work related tasks. Occupational fatalities are also commonly called “occupational deaths” or “work-related deaths/fatalities” and can occur in any industry or occupation.						Common causes of occupational fatalities include falls, machine-related incidents, motor vehicle accidents, electrocution, falling objects, homicides and suicides. Occupational fatalities can be prevented.		In the United States in 2006, 41% of occupational fatalities occurred during a transportation incident, 17% occurred after a worker came into contact with an object or equipment, and 15% occurred as a result of a fall.[1] The remaining 12% of deaths were the result of chemical or environmental exposures (9%) and fires or explosions (3%).[1] Lastly, 15% of all occupational fatalities are the consequences of assault and other violent acts in the workplace.		Many factors contribute to a fatal incident at work. Lack of appropriate employee training and failure to provide and enforce the use of safety equipment are frequent contributors to occupational fatalities. In some cases, employees do receive safety training, but language barriers prevent the employee from fully understanding the safety procedures. Incidents can also be the result of insufficient supervision of inexperienced employees or employees who have taken on a responsibility for which they are not properly trained. Poor worksite organization, staffing and scheduling issues, unworkable policies and practices and workplace culture can all play a role in occupational fatalities. An incident leading to an occupational fatality is generally not the fault of a single person, but the result of a combination of many human and environmental factors.[citation needed]		Although all workers are at risk for occupational fatalities, elderly workers age 65 and older are roughly three times more likely to die at work.[3] Hispanic workers die on the job at a higher rate than non-Hispanic workers. Men account for 92% of occupational deaths.[1]		The majority of occupational deaths occur among men. In one US study, 93% of deaths on the job involved men,[4] with a death rate approximately 11 times higher than women. The industries with the highest death rates are mining, agriculture, forestry, fishing, and construction, all of which employ more men than women.[5] Deaths of members in the military is currently above 90% men.[6]		Occupational fatalities are preventable. Prevention of occupational fatalities depends on the understanding that worker safety is not only the responsibility of the worker, but is the primary responsibility of the employer. Employers must train all employees in the appropriate safety procedures and maintain a safe working environment so that fatalities are less likely to occur.[7] An occupational fatality is not just the fault of the deceased worker; instead, it is the combination of unsafe work environments, insufficient safety training, and negligible employee supervision that contribute fatal incidents. As a result, it is imperative that an employer address all the potential [risk] factors at the workplace and educate all employees in safe work practices and risk awareness.		In order to perform adequate risk assessment of injuries that occur in the workplace, health and safety professionals use resources such as the Haddon Matrix. This model assesses the risks leading up to, during, and after a death in order to prevent future incidents of a similar nature. Employers and employees can learn how to identify risk factors in their work environment in order to avoid incidents that may result in death.		The regulatory organization for occupational injury control and prevention is the Occupational Safety and Health Administration (OSHA). Formed in 1970 as an agency of the United States Department of Labor under the Occupational Safety and Health Act, OSHA exists to prevent occupational injuries and deaths by creating and enforcing standards in the workplace. OSHA standards address employee training programs, safety equipment, employer record keeping and proper maintenance of the work environment. Failure to comply with the OSHA standards can result in workplace inspections and legal action including citations and fines. In very severe cases of employer misconduct, OSHA can “red flag” an operation and send the employer to legal court.[8]		To regulate the millions of workplaces in the United States, OSHA requires that all employers maintain a record of occupational injuries, illnesses and fatalities. Occupational fatalities must be reported to OSHA within eight hours of the incident. Failure to do so can result in legal action against the employer. Employers are responsible for staying current on OSHA standards and enforcing them in their own workplace. State OSHA organizations exist in twenty-eight states and are required to have the same or more rigorous standards than the federal OSHA standards. In these states, employers must abide by their state’s regulations. It is not the responsibility of the employee to stay current on the OSHA standards.		In addition to OSHA, the National Institute for Occupational Safety and Health (NIOSH) was formed under the Occupational Safety and Health Act as a federal research agency to formulate industry recommendations for health and safety. NIOSH is part of the Centers for Disease Control and Prevention (CDC) in the United States Department of Health and Human Services (DHHS). NIOSH analyzes workplace injury and illness data from all fifty states as well as provides support for state-based projects in occupational health and safety.		Under NIOSH, the Fatality Assessment and Control Evaluation (FACE) Program tracks and investigates occupational fatalities in order to provide recommendations for prevention. A voluntary program for individual states created in 1989, FACE is active in California, Iowa, Kentucky, Massachusetts, Michigan, New Jersey, New York, Oregon, and Washington. The primary responsibilities of the state FACE programs are to track occupational fatalities in their state, investigate select fatalities, and provide recommendations for prevention. As part of the prevention efforts, FACE programs also produce extensive prevention education materials that are disseminated to employees, employers, unions, and state organizations.		Nationally, the Census of Fatal Occupational Injuries (CFOI), within the U.S. Department of Labor, compiles national fatality statistics. CFOI is the key, comprehensive system in the surveillance of occupational fatalities in the United States.		Many other non-governmental organizations also work to prevent occupational fatalities. Trade associations and unions play an active role in protecting workers and disseminating prevention information. The National Safety Council also works to prevent occupational fatalities as well as provide resources to employers and employees.		
Under the Canadian Constitution, the responsibility for enacting and enforcing labour laws, including the minimum wage, rests with the ten provinces as well as the three territories which have been granted this power by federal legislation. Some provinces allow lower wages to be paid to liquor servers and other gratuity earners or to inexperienced employees.		The federal government in past years set its own minimum wage rates for workers in federal jurisdiction industries (railways for example). In 1996, however, the federal minimum wage was re-defined to be the general adult minimum wage rate of the province or territory where the work is performed. This means, for example, that a railway company could not legally pay a worker in British Columbia less than C$10.25 per hour regardless of the worker's experience.						In 2013, 39.8% of minimum wage workers were between the ages of 15 and 19; in 1997, it was 36%. 50.2% of workers in this age group were paid minimum wage in 2013, an increase from 31.5% in 1997. Statistics Canada notes that "youth, women and persons with a low level of education were the groups most likely to be paid at minimum wage."[2]		Assuming a 40-hour workweek and 52 paid weeks a year, the gross monthly income of an individual earning the lowest minimum wage in Canada is C$1,857.99 (in Saskatchewan) and the highest minimum wage is C$2,357.15 (in Alberta). Similarly, the yearly gross income of an individual earning the lowest minimum wage in Canada is C$22,295.98 (in Saskatchewan) and the highest minimum wage is C$28,285.80 (in Alberta).[3]		The following table lists the hourly minimum wages for adult workers in each province and territory of Canada. The provinces which have their minimum wages in bold allow for lower wages under circumstances which are described under the "Comments" heading.		Note: The following table can be sorted by Jurisdiction, Wage, or Effective date using the icon.		In Ontario, from 2015 on, the minimum wage shall be increased slightly every year. Minimum wage increases shall be announced on 1 April of each year, based on the Consumer Price Index for Ontario, with the change taking into effect on 1 October of that same year.[18]		On May 30, 2017, it was announced Ontario's minimum wage would rise to $14.00 as of January 1, 2018, and raise to $15.00 as of January 1, 2019.[19][20]		Some critics of the current minimum wage levels in Canada argue that they are insufficient and advocate that the minimum wage is increased to what they consider a living wage. The New Democratic Party in 2007 called for a separate federal minimum wage of C$10 per hour, however, such a change could not be enforced on any employer operating under provincial jurisdiction, unless the province voluntarily agreed to harmonize its own minimum wage with the federal government.[26] On 1 October 2009, M.P. Irene Mathyssen introduced a private member's bill (C-448) to amend the Canada Labour Code with regard to the minimum wage and have the federal minimum wage set to C$12 per hour.[27] Other critics, such as the Canadian Federation of Independent Businesses, contend that minimum wage laws actually hurt the very people they purport to help by causing unemployment for low skilled workers.[28]		
Parental leave or family leave is an employee benefit available in almost all countries.[1] The term "parental leave" generally includes maternity, paternity, and adoption leave. A distinction between "maternity leave" and "parental leave" is sometimes made- maternity leave as the mother's leave time directly before and after childbirth and parental leave being the time given to care for newborns or young children.[2] In some countries and jurisdictions, "family leave" also includes leave provided to care for ill family members. Often, the minimum benefits and eligibility requirements are stipulated by law.		Unpaid parental or family leave is provided when an employer is required to hold an employee's job while that employee is taking leave. Paid parental or family leave provides paid time off work to care for or make arrangements for the welfare of a child or dependent family member. The three most common models of funding are social insurance/social security (where employees, employers, or taxpayers in general contribute to a specific public fund), employer liability (where the employer must pay the employee for the length of leave), and mixed policies that combine both social security and employer liability.[3]		Parental leave has been available as a legal right and/or governmental program for many years, in one form or another. In 2014, the International Labour Organization reviewed parental leave policies in 185 countries and territories, and found that all countries except Papua New Guinea have laws mandating some form of parental leave.[4] A different study showed that of 186 countries examined, 96% offered some pay to mothers during leave, but only 81 of those countries offered the same for fathers.[5] The United States, Suriname, Papua New Guinea, and several island countries in the Pacific Ocean are the only countries that do not require employers to provide paid time off for new parents.[6]		Private employers sometimes provide either or both unpaid and paid parental leave outside of or in addition to any legal mandate.						Amartya Sen and Martha Nussbaum have developed a political model known as the Capabilities approach, where basic freedoms and opportunities are included in economic assessments of a country's well-being, in addition to GDP.[7][8] Nussbaum proposed 10 central capabilities as the minimum requirement for a decent society. In Nussbaum's model, states should provide the resources and freedoms to ensure people have the opportunity to achieve a minimum threshold of each central capability. Universal, paid parental leave is an example resource states can provide so people have the option of starting a family while also working; for instance, under capacity 10 (control of one's environment), the state has a responsibility to ensure all people have "the right to seek employment on an equal basis with others."[8]		Whether parental leave contributes to gender equality depends upon (a) whether laws consider a child to be the responsibility of both parents and (b) whether the leave is equal as to the sexes. An example of a law that does not consider fathers equally responsible is England's parental responsibility law, which is gender-neutral and equal, but baseline applies only to fathers married to the mother at the time of the birth and thus exempts fathers not married to the mother unless someone brings an action on behalf of the child. An equal leave policy would consider the father to be more responsible than the mother for child care after birth, as the time in the womb is counted solely to the mother; in such a policy, mothers would be expected to consume leave for pregnancy, delivery and early lactation that would be matched by time fathers take later in the child's life.		In a 2014 Swiss study, Lanfranconi & Valarino identified one of the important ways Switzerland has considered parental leave policies is related to gender equality, where parental leave "enabl[es] a more equal division of work between men and women by fostering paternal involvement in childcare."[9] Similarly, a 2015 study by Rønsen & Kitterød found that part of the effect of Norwegian parental leave policy "contributed to... a more equal division of paid and unpaid work among parents."[10]		The advancement of gender equality has also been on the political agenda of Nordic countries for decades. Although, all Nordic countries have extended the total leave period, their politics towards father’s quota are different. Iceland, Norway and Sweden have established equal 3 months quotas for the father. The only Nordic country that does not provide fathers with a quota is Denmark. However, the dual earner/dual care model seems to be the direction of all the Nordic countries are moving in the construction of their parental leave systems.[11]		Paid parental leave incentivizes labor market attachment for women both before and after birth, affecting GDP and national productivity, as the workforce is larger.[12][13][10][14] Parental leave increases income at the household level, as well, by supporting dual-earner families.[15]		Paid parental leave incentivizes childbirth, which affects the future workforce. It is thus argued that paid parental leave, in contrast to unpaid parental leave, is harmful to children's welfare because in countries with an aging workforce or countries with Sub-replacement fertility, children are born not because the parents want the child and can meet the child's needs but because children are expected to support their parents. Some see children as responsible for supporting all those in older generations in the society (not just the child's specific parents); their earnings are expected not to be saved for the children's own old age, but to be spent on the earlier generations' demand for social security and pensions for which there was inadequate savings.[16][17]		The neoclassical model of labor markets predicts that if the cost of hiring women of child-bearing years is anticipated to increase (either because the employer is mandated to pay for maternity leave, or because she will be absent from work on public leave), then the "demand" for women in the labor market will decrease. While gender discrimination is illegal, without some kind of remedy, the neoclassical model would predict "statistical discrimination" against hiring women of child-bearing years.[18][19]		If women take long parental leaves, the neoclassical model would predict that their lifetime earnings and opportunities for promotion will be less than their male or childfree counterparts, or the "motherhood penalty."[20] Women may seek out employment sectors that are "family-friendly" (i.e., with generous parental leave policies), resulting in occupational sex segregation.[21] Nielsen, Simonsen, and Verner examine what the different outcomes for women in Denmark are between the "family-friendly" and the "non-family-friendly" sector.[10] In Denmark, the public sector is "family-friendly" because of its generous leave and employee benefits; workers decide which sector to work on based on their preferences and opportunities. The study found that while in the "family-friendly" sector, there was basically no wage loss related to taking parental leave, women did have consistent earnings loss in the "non-family-friendly" private sector for a 1-year leave.[10]		Universal, paid parental leave can be privately funded (i.e., corporations are mandated to absorb the cost of paid parental time off as part of employee benefits) or publicly funded (i.e., transferred directly to workers on leave, like unemployment insurance). Concerns about private funding include the statistical discrimination described above as well as the costs to smaller businesses. Datta Gupta, Smith, & Verneer found in 2008 that, while publicly funded parental leave has benefits, it is very expensive to fund and question if it is the most cost-effective use of funds.[citation needed]		Social norms have historically not included child care in the main responsibilities of fathers. However, in some, mainly western, countries, politicians, and social scientists argue for changing the role of the fathers, and the idea of the ‘new father’ has especially been shaped by the Nordic countries of Scandinavia. The process enables fathers to rationalize their parenting style and align this with what characterizes good care. Even though the mother’s role as main parent has not changed, male parental leave is claimed by its supporters to transform the traditionally gendered father practices and to create a social morality in relation to partners and children. Psychologists however consider that the allegedly positive effects of male parental leave are not supported by research, and warn that it might have negative effects. Norwegian psychology professor Leif Edward Ottesen Kennair believes the father's quota is indefensible from a psychological point of view, and argues that "we must at the very least ask ourselves what the consequences will be when we make a childhood environment that differs from what our species has evolved into." He believes the father's quota is "based on ideology, and only to an extremely limited extent on knowledge," arguing that it is "a social experiment, the effects of which are unknown."[22] It also has to be considered that fathers from different classes see their roles alternatively during their paternity leave. Whereas middle class fathers consider themselves as suitable alternative to the mother having the same competencies, working class men see themselves more as supporters during their leave. In consequence middle class fathers mostly use their leave right after the mother returns to work, meanwhile working class fathers do their leave during the mother's leave.[23]		Typically, the effects of parental leave are improvements in prenatal and postnatal care, including a decrease in infant mortality.[24] The effects of parental leave on the labor market include an increase in employment, changes in wages, and fluctuations in the rate of employees returning to work. Leave legislation can also impact fertility rates.[25]		A study in Germany found that wages decreased by 18 percent for every year an employee spends on parental leave.[25] However, after the initial decrease in wages, the employee’s salary rebounds faster than the salary of someone not offered parental leave.[25] A study of California’s leave policy, the first state in the U.S. to require employers to offer paid parental leave, showed that wages did increase.[26]		Parental leave can lead to greater job security.[25] Studies differ in how this helps return to work after taking time off. Some studies show that if a parent is gone for more than a year after the birth of a child, it decreases the possibility that he or she will return.[25] Other studies of shorter leave periods show that parents no longer need to quit their jobs in order to care for their children, so employment return increases.[26]		It does not appear that parental leave policies have had a significant effect on the gender wage gap, which has remained relatively steady since the late 1980s, despite increasing adoption of parental leave policies.[10]		In the U.S., while the Family and Medical Leave Act of 1993 allows for unpaid parental leave, parents often do not utilize this eligibility to its fullest extent as it is unaffordable. As a result, some studies show that the FMLA has had a limited impact on how much leave new parents take.[27] Though specific amounts can vary, having a child (including the cost of high-quality childcare) costs families approximately $11,000 in the first year.[28] These high costs contribute to new mothers in the United States returning to work quicker than new mothers in European countries; approximately one-third of women in the United States return to work within three months of giving birth, compared to approximately five per cent in the UK, Germany, and Sweden,[29] and just over half of mothers in the United States with a child under the age of one work.[30]		There is some evidence that legislation for parental leave raises the likelihood of women returning to their previous jobs as opposed to finding a new job. This rise is thought to fall to between 10% and 17%. Simultaneously, there is a decrease in the percentage of women who find new jobs which falls between 6% and 11%. Thus, such legislation appears to increase how many women return to work post-childbirth by around 3% or 4%.[31]		Additionally, it appears that parental leave policies do allow women to stay home longer before returning to work as the probability of returning to an old job falls in the second month after childbirth before dramatically rising in the third month. Although this legislation thus appears to have minimal effect on women choosing to take leave, it does appear to increase the time women take in leave.[31]		Maternity leave legislation could pose benefits or harm to employers. The main potential drawback of mandated leave is its potential to disrupt productive activities by raising rates of employee absenteeism. With mandated leave for a certain period of time and facing prolonged absence of the mothers in the workplace, firms will be faced with two options: hire a temp (which could involve training costs) or function with a missing employee. Alternatively, these policies could be positive for employers who previously did not offer leave because they were worried about attracting employees who were disproportionately likely to use maternity leave. Thus, there is potential for these policies to correct market failures.[31] A drawback of rising leave at the societal level, however, is the resulting decrease in female labor supply. In countries with a high demand for labor, including many present-day countries with aging populations, a smaller labor supply is unfavorable.[32]		Something important to note for all the research cited above is that the results typically depend on how leave coverage is defined, and whether the policies are for unpaid or paid leave. Policies guaranteeing paid-leave are considered by some to be dramatically more effective than unpaid-leave policies.[27]		For women individually, long breaks in employment, as would come from parental leave, negatively affects their careers. Longer gaps are associated with reduced lifetime earnings and lower pension disbursements as well as worsened career prospects and reduced earnings. Due to these drawbacks, some countries, notably Norway, have expanded family policy initiatives to increase the father's quota and expand childcare in an effort to work towards greater gender equality.[32]		According to a 2016 study, the expansion of government-funded maternity leave in Norway from 18 to 35 months led mothers to spend more time at home without a reduction in family income.[33]		Although parental leave is increasingly granted to fathers, mothers continue to take the majority of guaranteed parental leave.[34] When guaranteed leave is unpaid, research indicates that men's leave usage is unaffected.[35] While uncommon on a worldwide scale some countries do reserve parts of the paid leave for the father, meaning it can't be transferred to the mother and lapses unless he uses it. Among the earliest countries to actively push for increased usage of paternity leave are the Nordic welfare states - Iceland, Denmark, Sweden, Norway and Finland. These countries lack a unified concept of paternity leave, each imposing different conditions, ratios and timescales, but are regarded as among the most generous in the world, particularly "Norwegian parental leave benefits". Partly in an initiative to combat the "Motherhood penalty," Norway initiated a policy change in the mid-2000s to incentivize paternal leave.		In countries in which leave entitlements include a father's quota, sometimes called the "daddy quota" in Nordic family policy, there has been a more pronounced impact. Research in Norway, which has had a father's quota since 1993, shows that fathers' involvement in childcare has risen. The size of the quota has increased since then, from 4 weeks in 1993 to 14 weeks in 2013 but then being reduced to 10 weeks in 2014. The father's quota has been credited for increasing paternal involvement and challenging gender roles within the family, promoting a more equal division of labor.[32] To evaluate this change, Rønsen & Kitterød looked at the rate and timing of women's return to work after giving birth, and the effect on this of the new parental leave policy. In their 2015 study, Rønsen & Kitterød found women in Norway returned to work significantly faster after the policy change.[36] However, public or subsidized daycare was greatly expanded at the same time, so Rønsen & Kitterød did not find that the "daddy quota" was solely responsible for the timing of work entry. But it can be understood to have an effect on division of household labor by gender when both parents can take time to care for a new baby.[10]		Another impact from fathers taking more leaves is that in Norway, it has been shown to have the potential to either decrease or increase the time women take, depending on whether the mother's and father's childcare are seen as substitutes or complements. If substitute goods, mothers are able to return to work sooner as fathers take some of the childcare responsibility. As for the latter, longer leave for fathers can motivate mothers to also stay home.[10]		Fathers tend to use less parental leave than mothers in the United States as well as in other countries where paid leave is available,[27][37] and this difference may have factors other than the financial constraints which impact both parents. Bygren and Duvander,[37] looking at the use of parental leave by fathers in Sweden, concluded that fathers’ workplace characteristics (including the size of the workplace, whether there were more men or women in the workplace, and whether the workplace was part of the private or public sector) influenced the length of parental leave for fathers, as did the presence of other men who had taken parental leave at an earlier point in time. As of 2016 paternity leave accounts for 25% of paid parental leave in Sweden.		In 2013, Joseph, Pailhé, Recotillet, and Solaz published a natural experiment evaluating a 2004 policy change in France.[38] They were interested in the economic effects of full-time, short paid parental leave. Before the reform, women had a mandatory two-month parental leave, and could take up to three years unpaid parental leave with their job guaranteed, though most women only took the two months. The new policy, complément libre choix d'activité (CLCA), guarantees six months of paid parental leave. The authors found positive effects on employment: compared to women in otherwise similar circumstances before the reform, first-time mothers who took the paid leave after the reform were more likely to be employed after their leave, and less likely to stay out of the labor force. The authors point to similar results of full-time, short paid parental leave observed in Canada in 2008 by Baker and Milligan,[39] and in Germany in 2009 by Kluve and Tamm.[40] However, Joseph, et al., also found that wages were lower (relative to women before the reform) for medium- and highly educated women after the leave, which could be because the women returned to work part-time or because of a "motherhood penalty," where employers discriminate against mothers, taking the six-month leave as a "signal" that the woman will not be as good of an employee because of her mothering responsibilities.		Rasmussen conducted analyzed a similar natural experiment in Denmark with a policy change in 1984 where parental leave increased from 14 to 20 weeks.[41] Rasmussen found the increased length of parental leave had no negative effect on women's wages or employment, and in the short-run (i.e., 12 months) it had a positive effect on women's wages, compared to the shorter leave. There was no difference on children's long-term educational outcomes before and after the policy change.		A Harvard report cited research showing paid maternity leave “facilitates breastfeeding and reduces risk of infection”[24] but is not associated with changes in immunization rate.[42] This research also found that countries with parental leave had lower infant mortality rates.[24] Returning to work within 12 weeks was also associated with less regular medical checkups.[43] Data from 16 European countries during the period 1969-1994 revealed that the decrease of infant mortality rates varied based on length of leave. A 10 week leave was associated with a 1-2% decrease; a 20 week leave with 2-4%; and 30 weeks with 7-9%.[44] The United States, which does not have a paid parental leave law, ranked 56th in the world in 2014 in terms of infant mortality rates, with 6.17 deaths per every 1,000 children born.[45] The research did not find any infant health benefits in countries with unpaid parental leave.		Paid leave, particularly when available prior to childbirth, had a significant effect on birth weight. The frequency of low birth rate decreases under these policies which likely contributes to the decrease in infant mortality rates as low birth weight is strongly correlated with infant death. However, careful analysis reveals that increased birth weight is not the sole reason for the decreased mortality rate.[42]		According to a 2016 study, the expansion of government-funded maternity leave in Norway from 18 to 35 months had little effect on children's schooling.[33] However, when infants bond and have their needs met quickly by caregivers (mothers, fathers, etc) they will become confident and be prepared to have healthy relationships throughout their life.[46]		Children whose mothers did not work in the first 9 months were found to be less ready for school at the age of 3 years. The effects of mother's employment appeared to be the most detrimental when employment started between the sixth and ninth month of life. The reasons for this were uncertain but there is conjecture that there was something unusual for the group of mothers who returned to work in this time period as they represented only 5% of all families studied. Negative impacts in terms of school-readiness were most pronounced when the mother worked at least 30 hours per week. These findings were complicated by many factors, including race, poverty, and how sensitive the mother was considered. The effects were also greater in boys which is explained by the fact that many analysts consider boys more vulnerable to stress in early life.[47]		The same Harvard report also linked paid parental leave and a child’s psychological health. It found that parents with paid parental leave had more intense bonds with their children.[24] Based on research of heterosexual couples, a better father’s immersion in the process of raising a child leads to an enhanced child’s development and furthermore improves the relationship between the two parents.[48] In recent years, various OECD countries drew attention to that topic, especially to the time of the parental leave taken by fathers. Short-term father’s leaves still lead to positive outcomes for the child’s development. However, due to the typically higher income-levels of men mother’s leaves are preferred to father’s leaves since the family forfeits less income when the mother takes off from work.[49]		There are also observable improvements in the mental health of mothers when they are able to return to work later. While the probability of experiencing postpartum depression had no significant statistical change, longer leave (leave over 10 weeks) was associated with decreased severity of depression and decreased number of experienced symptoms. This reduction was, on average, between 5% and 10%.[50]		While studies have shown conflicting results, some research has shown a link between paid parental leave and higher fertility rates. The research looked at women 25–34 years old, who are more likely to be affected by leave legislation. Fertility rates peaked for those between 25-29 and 30-34 across European countries.[25]		The economic consequences of parental leave policies are subject to controversy. According to a 2016 study, the expansion of government-funded maternity leave in Norway from 18 to 35 months had net costs which amounted to 0.25% of GDP, negative redistribution properties and implied a considerable increase in taxes at a cost to economic efficiency.[33] In the U.S., paid family leave tends to lead to a higher employee retention rate and higher incomes for families.[51] Evidence from selected countries in Western Europe suggests that moderate levels of parental leave can encourage mothers to reenter the work force after having children, promoting national economic development.[52]		Some businesses adopt policies that are favorable to workers and public opinion. In their study of maternity leave policies in the United States, Kelly and Dobbin found that public policy surrounding pregnancy as a temporary disability (for instance, California's Family Temporary Disability Insurance program) gave rise to business practices that included maternity leave as a benefit.[53]		Companies are starting to offer paid parental leave as a benefit to some American workers, seeing a profitable aspect of doing so, including: reduced turnover costs, increased productivity from workers, and increased rates of retention among women after childbirth. Some see the increase in paid parental leave as indicative of companies reaching out to women, as more women are working and returning to work after having children, and by doing so these companies generate positive publicity as employers with family-friendly workplaces.[28] Working Mother magazine [3] publishes a list of 100 Best Companies for working mothers each year, a list which is noted not only by the readership of the magazine, but also by corporate America and increasingly by researchers and policy institutes as well.[28] The Institute for Women’s Policy Research[4] issued a report in 2009 encouraging Congress to give federal workers four weeks of paid parental leave.[28] The report cited statistics from the Working Mother 100 Best Company list, using private sector corporations as examples of substantial increase in the retention of new mothers after instituting a longer maternity leave policy. The report also noted that it would take newer workers four years to accrue enough paid leave (sick leave and annual leave) to equal the 12 weeks of unpaid parental leave provided under the FMLA, and that private sector companies which offer paid parental leave have a significant advantage over the federal government in the recruitment and retention of younger workers who may wish to have children.		The Convention on the Elimination of All Forms of Discrimination against Women introduces "maternity leave with pay or with comparable social benefits without loss of former employment, seniority or social allowances".[54] The Maternity Protection Convention C 183 adopted in 2000 by International Labour Organization requires 14 weeks of maternity leave as minimum condition.[55]		National laws vary widely according to the politics of each jurisdiction. As of 2012, only three countries do not mandate paid time off for new parents: Papua New Guinea, Lesotho, and the United States.[56][57]		Unless otherwise specified, the information in the tables below is gathered from the most recent International Labour Organization reports. Maternity leave refers to the legal protection given to the mother immediately after she gives birth (but may also include a period before the birth), paternity leave to legal protection given to the father immediately after the mother gives birth, and parental leave to protected time for childcare (usually for either parent) either after the maternity/paternity leave or directly immediately after birth (for example when the parent is not eligible for maternity/paternity leave, and/or where the time is calculated until the child is a specific age - therefore excluding maternity/paternity leave - usually such jurisdictions protect the job until the child reaches a specific age.[58]) Others allow the parental leave to be transferred into part-time work time. Parental leave is generally available to either parent, except where specified. Leave marked "Unpaid" indicates the job is protected for the duration of the leave. Different countries have different rules regarding eligibility for leave, and long a parent has to have worked at their place of employment prior to giving birth before they are eligible for paid leave. In the European Union, the policies vary significantly by country - with regard to length, to payment, and to how parental leave relates to prior maternity leave - but the EU members must abide by the minimum standards of the Pregnant Workers Directive and Parental Leave Directive.[59]		(% of pay)		(weeks)		(% of pay)		(% of pay)		(% of pay)		As international organizations are not subject to the legislation of any country, they have their own internal legislation on parental leave.		
A jobless recovery or jobless growth is an economic phenomenon in which a macroeconomy experiences growth while maintaining or decreasing its level of employment. The term was coined by the economist Nick Perna in the early 1990s.[1][2]						Economists are still divided about the causes and cures of a jobless recovery: some argue that increased productivity through automation has allowed economic growth without reducing unemployment.[3] Other economists state that blaming automation is an example of the luddite fallacy[4] and that jobless recoveries stem from structural changes in the labor market, leading to unemployment as workers change jobs or industries.[5]		Some have argued that the recent lack of job creation in the United States is due to increased industrial consolidation and growth of monopoly or oligopoly power.[6] The argument is twofold: firstly, small businesses create most American jobs, and secondly, small businesses have more difficulty starting and growing in the face of entrenched existing businesses (compare infant industry argument, applied at the level of industries, rather than individual firms).		In addition to employment growth, population growth must also be considered concerning the perception of jobless recoveries. Immigrants and new entrants to the workforce will often accept lower wages, causing persistent unemployment among those who were previously employed.[7][8]		Surprisingly, the U.S. Bureau of Labor Statistics (BLS) does not offer data-sets isolated to the working-age population (ages 16 to 65).[9] Including retirement age individuals in most BLS data-sets may tend to obfuscate the analysis of employment creation in relation to population growth.[10] Additionally, incorrect assumptions about the term, Labor force, might also occur when reading BLS publications, millions of employable persons are not included within the official definition. The Labor force, as defined by the BLS,[11] is a strict definition of those officially unemployed (U-3),[12] and those who are officially employed (1 hour or more).[13]		The following table and included chart depicts year-to-year employment growth in comparison to population growth for those persons under 65 years of age. As such, baby boomer retirements are removed from the data as a factor for consideration. The table includes the Bureau of Labor Statistics, Current Population Survey, for the Civilian noninstitutional population and corresponding Employment Levels, dating from 1948 and includes October 2013, the age groups are 16 years & over, and 65 years & over.[9] The working-age population is then determined by subtracting those age 65 and over from the Civilian noninstitutional population and Employment Levels respectively. Isolated into the traditional working-age subset, growth in both employment levels and population levels are totaled by decade, an employment percentage rate is also displayed for comparison by decade.		When examined, by decade, the first decade of the 2000s, the United States suffered a 95% jobless rate when compared to the added working age population.		
A labourer is a person who works in one of the construction trades, traditionally considered most reliable manual labor[clarification needed] Laborers are also employed outside of the construction industry, in fields such as road paving, building construction, bridges, tunnels etc. laborers have all blasting, hand tools, power tools, air tools, and small heavy equipment, and act as assistants to other trades as well, when sometimes they need guidance,[1] e.g., operators or cement masons. The 1st century BC engineer Vitruvius writes in detail about laborer practices at that time. In his experience a good crew of laborers is just as valuable as any other aspect of construction. Other than the addition of pneumatics, laborer practices have changed little. With the advent of advanced technology and its introduction into the construction field, the laborers have been quick to include much of this technology as being laborers work.						The following tools are considered a minimum: hammer, pliers w/ side-cutters, utility knife, tape measure, locking pliers, crescent wrench, screwdriver, margin trowel, carpenter's pencil or soapstone, tool belt and one pouch (bag). In addition: a five gallon bucket with additional tools, toolbelt suspenders, water jug and lunchbox are recommended. Most safety equipment that is consumed or work specific, for example hard hat, safety glasses, hearing protection, gloves, fall protection, High-visibility clothing, concrete boots, respirator/dust mask and toe guards[2] are provided by the employer as part of construction site safety. Personal safety equipment, for example full leather boots (some long time laborers believe steel toes are dangerous on the construction site; it is better to have crushed toes than toes cut off by the crushed steel), high strength pants - Carhartt or jeans (some modify thighs with a sacrificial second layer of jean fabric cut from an old pair) - socks, lip balm, and climate specific outerwear (unless laborers are instructed to work in a climate different from what they typically reside in, for example high elevation), are provided by the individual.		Some of the work done by laborers includes:[3]		Much of the work traditionally claimed by laborers is merely work that did not fit into any other workforce's labor classification. These other classifications (in order of prestige) typically include the heavy equipment operators, ironworkers, carpenters, masons, teamsters/truck drivers and hod carriers. In addition, work that typically was shunned by journeymen of other trade unions tradesman/craftsman or was given to their apprentices is generally done by laborers in the absence of apprentices.		An example is the operators who in the division of labor have all the equipment. Most operators will not operate equipment they perceive as lowly such as skid steer, kick-brooms and telescopic handlers, laborers usually are used to operate these unless an operator apprentice is available and demands his right to operate. The same is true for most other trades except the ironworkers who are notorious for protecting their work and not wanting anyone else to touch their steel, tie-wire or Kliens. The advantage to this system is that many laborers gain sufficient experience working with another trade to journeyman-in while earning a higher wage than an apprentice. Many foremen will gradually give a laborer extra responsibility until they are performing at a journeyman level and can enter a more skilled union as a journeyman.		The pay for a union laborer is equal or greater than most work available to anyone with a bachelor's degree {US centric?}, making this one of the few fields where someone without a high school diploma can still earn a living wage. Union, heavy construction and highway construction laborers earn on average (2008 US) $25.47/h compared to 13.72/h for non-union laborers.[4] In addition to paid earnings, union laborers enjoy the benefits of medical insurance, vacation pay, pension plans, representation and vocational schools; totaling $45/hr (2012 US) and some with special skills earn 'over-rate' wages. It is not uncommon for young civil engineers, construction managers and construction engineers to earn less than their apprentice laborers. However, unlike engineers, laborers are not usually employed full-time year round. The additional pay they receive is often balanced out by the lesser unemployment checks they receive while out of work. These unemployment checks supplement the winter pay laborers often earn as independent contractors and under-the-table work. On average young engineers earn (2007 US$) 40,000 to 60,000 while union laborers on average earn 50,000 to 80,000. Engineers are not immune to being out of work, in heavy civil work some are employed on a project basis. They are not guaranteed a place on any subsequent projects, though this is in practice often the case. The value of work put in place by laborers and the value of avoided rework and increased efficiencies produced by the engineers' planning is a balance of resource utilization on any large project. Union laborers earn more than unfree labor and can be an avenue for those who are uneducated and with no resources to become educated and with resources.		There are dangers associated with laboring. Many laborers are severely injured or killed in accidents each year while performing work duties[citation needed]. Many who work as laborers for even a short period of time will suffer from permanent work injuries such as: hearing loss, arthritis, osteoarthritis, back injuries, eye injury, head injury, chemical burn (lime sensitivity), lung disease, missing finger nails and skin scars[citation needed]. Alcoholism, drug use, and drug abuse are common although most companies require drug screening for all new hires.[citation needed] If a laborer is injured on the job they are immediately given a drug test.[citation needed] If the test results are positive then they are ineligible for any Workers' compensation benefits.[citation needed] There is a gray area for the use of marijuana due to medical marijuana prescriptions. Some who have been dismissed for failing a drug test while possessing a prescription have been later reinstated with pay as having been wrongfully terminated. The Laborers' International Union of North America (LIUNA) represents laborers on public and private projects. Some of the business representatives are laborers who have been so severely injured they can no longer labor. With a phone call and a good reason they will be on-site the next morning asking questions and demanding apologies for mistreatment of laborers.[citation needed]		This job, at times, and depending on who is in charge, qualifies for the 3D's, Dirty, Dangerous and Demeaning, or showing global connotation, as the Japanese say it kitanai, kiken, and kitsui.[citation needed] Many other times laboring is a very gratifying job with much fresh air (jobsite air quality) and sunshine.[according to whom?][editorializing] The sheer hardship, drudgery and physical demands of the job ensure that there is always a shortage of good laborers. But, mistakes can be made and laborers have been asked to go forward with ill-made plans.[editorializing][citation needed]		
The largest employers in the world include companies, militaries, and governments.						Non-corporate employers are included in this lists:		The rankings below are the seven private-sector companies providing the most jobs worldwide, according to the 2017 list compiled by Fortune magazine.[5] That list does not include entirely government-owned corporations.				
Professional development is learning to earn or maintain professional credentials such as academic degrees to formal coursework, conferences and informal learning opportunities situated in practice. It has been described as intensive and collaborative, ideally incorporating an evaluative stage.[1] There are a variety of approaches to professional development, including consultation, coaching, communities of practice, lesson study, mentoring, reflective supervision and technical assistance.[2]						The University of Management and Technology notes the use of the phrase "professional development" from 1857 onwards.[citation needed]		In the training of school staff in the United States, "[t]he need for professional development [...] came to the forefront in the 1960's".[3]		A wide variety of people, such as teachers, military officers and non-commissioned officers, health care professionals, lawyers, accountants and engineers engage in professional development. Individuals may participate in professional development because of an interest in lifelong learning, a sense of moral obligation, to maintain and improve professional competence, to enhance career progression, to keep abreast of new technology and practices, or to comply with professional regulatory requirements.[4][5] Many American states have professional development requirements for school teachers. For example, Arkansas teachers must complete 60 hours of documented professional development activities annually.[6] Professional development credits are named differently from state to state. For example, teachers: in Indiana are required to earn 90 Continuing Renewal Units (CRUs) per year;[7] in Massachusetts, teachers need 150 Professional Development Points (PDPs);[8] and in Georgia, must earn 10 Professional Learning Units (PLUs).[9] American and Canadian nurses, as well as those in the United Kingdom, have to participate in formal and informal professional development (earning Continuing education units, or CEUs) in order to maintain professional registration.[10][11][12]		In a broad sense, professional development may include formal types of vocational education, typically post-secondary or poly-technical training leading to qualification or credential required to obtain or retain employment. Professional development may also come in the form of pre-service or in-service professional development programs. These programs may be formal, or informal, group or individualized. Individuals may pursue professional development independently, or programs may be offered by human resource departments. Professional development on the job may develop or enhance process skills, sometimes referred to as leadership skills, as well as task skills. Some examples for process skills are 'effectiveness skills', 'team functioning skills', and 'systems thinking skills'.[13][14]		Professional development opportunities can range from a single workshop to a semester-long academic course, to services offered by a medley of different professional development providers and varying widely with respect to the philosophy, content, and format of the learning experiences. Some examples of approaches to professional development include:[2]		Initial professional development (IPD) is defined as "a period of development during which an individual acquires a level of competence necessary in order to operate as an autonomous professional".[15] Professional associations may recognise the successful completion of IPD by the award of chartered or similar status. Examples of professional bodies that require IPD prior to the award of professional status are the Institute of Mathematics and its Applications,[16] the Institution of Structural Engineers,[17] and the Institution of Occupational Safety and Health.[18]		Continuing professional development (CPD) or continuing professional education (CPE) is continuing education to maintain knowledge and skills. Most professions have CPD obligations. Examples are the Royal Institution of Chartered Surveyors,[19] American Academy of Financial Management,[20] safety professionals with the International Institute of Risk & Safety Management (IIRSM)[21] or the Institution of Occupational Safety and Health (IOSH),[22] and medical and legal professionals, who are subject to continuing medical education or continuing legal education requirements, which vary by jurisdiction.		
The eight-hour day movement or 40-hour week movement, also known as the short-time movement, was started by James Deb and had its origins in the Industrial Revolution in Britain, where industrial production in large factories transformed working life. The use of child labour was common. The working day could range from 10 to 16 hours for six days a week.[1][2]		Robert Owen had raised the demand for a ten-hour day in 1810, and instituted it in his socialist enterprise at New Lanark. By 1817 he had formulated the goal of the eight-hour day and coined the slogan: "Eight hours' labour, Eight hours' recreation, Eight hours' rest". Women and children in England were granted the ten-hour day in 1847. French workers won the 12-hour day after the February Revolution of 1848.[3] A shorter working day and improved working conditions were part of the general protests and agitation for Chartist reforms and the early organisation of trade unions.		The International Workingmen's Association took up the demand for an eight-hour day at its Congress in Geneva in 1866, declaring "The legal limitation of the working day is a preliminary condition without which all further attempts at improvements and emancipation of the working class must prove abortive", and "The Congress proposes eight hours as the legal limit of the working day."		Karl Marx saw it as of vital importance to the workers' health, writing in Das Kapital (1867): "By extending the working day, therefore, capitalist production...not only produces a deterioration of human labour power by robbing it of its normal moral and physical conditions of development and activity, but also produces the premature exhaustion and death of this labour power itself."[4][5]		The first country to adopt eight-hour working day was Soviet Russia. The eight-hour day was introduced on November 11, 1917, four days after the October Revolution, by a Decree of the Soviet government.		The first international treaty to mention it was the Treaty of Versailles in the annex of its thirteen part establishing the International Labour Office, now the International Labour Organization.[6]		The eight-hour day was the first topic discussed by the International Labour Organization which resulted in the Hours of Work (Industry) Convention, 1919 ratified by 52 countries as of 2016.		Although there were initial successes in achieving an eight-hour day in New Zealand and by the Australian labour movement for skilled workers in the 1840s and 1850s, most employed people had to wait to the early and mid twentieth century for the condition to be widely achieved through the industrialised world through legislative action.		The eight-hour day movement forms part of the early history for the celebration of Labour Day, and May Day in many nations and cultures.						In Iran in 1918, the work of reorganizing the trade unions began in earnest in Tehran during the closure of the Iranian constitutional parliament Majles. The printers' union, established in 1906 by Mohammad Parvaneh as the first trade union, in the Koucheki print shop on Nasserieh Avenue in Tehran, reorganized their union under leadership of Russian-educated Seyed Mohammad Dehgan, a newspaper editor and an avowed Communist. In 1918, the newly organised union staged a 14-day strike and succeeded in reaching a collective agreement with employers to institute the eight-hours day, overtime pay, and medical care. The success of the printers' union encouraged other trades to organize. In 1919 the bakers and textile-shop clerks formed their own trade unions.		However the eight-hours day only became as code by a limited governor's decree on 1923 by the governor of Kerman, Sistan and Balochistan, which controlled the working conditions and working hours for workers of carpet workshops in the province. In 1946 the council of ministers issued the first labor law for Iran, which recognized the eight-hour day.		The first company to introduce an eight-hour working day in Japan was the Kawasaki Dockyards in Kobe (now the Kawasaki Shipbuilding Corporation). An eight-hour day was one of the demands presented by the workers during pay negotiations in September 1919. After the company resisted the demands, a slowdown campaign was commenced by the workers on 18 September. After ten days of industrial action, company president Kōjirō Matsukata agreed to the eight-hour day and wage increases on 27 September, which became effective from October. The effects of the action were felt nationwide and inspired further industrial action at the Kawasaki and Mitsubishi shipyards in 1921.[7]		The eight-hour day did not become law in Japan until the passing of the Labor Standards Act in April 1947. Article 32 (1) of the Act specifies a 40-hour week and paragraph (2) specifies an eight-hour day, excluding rest periods.[8]		The 8-hour work day was introduced in Belgium on September 9, 1924.		The Eight-hour day was enacted in France by Georges Clemenceau, as a way to avoid unemployment and diminish communist support. It was succeeded by a strong French support of it during the writing of the International Labour Organization Convention of 1919.[9]		At the turn of the 20th century the eight-hour day was introduced by Ernst Abbe at the Zeiss plants in Jena.		In Portugal a vast wave of strikes occurred in 1919, supported by the National Workers' Union, the biggest labour union organisation at the time. The workers achieved important objectives, including the historic victory of an eight-hour day.		In Russia, the eight-hour day was introduced in 1917, four days after the October Revolution, by a Decree of the Soviet government.		In the region of Alcoy, Spain, a workers strike in 1873 for the eight-hour day followed much agitation from the anarchists. In 1919 in Barcelona, Catalonia, after a 44-day general strike with over 100,000 participants had effectively crippled the Catalan economy, the Government settled the strike by granting all the striking workers demands that included an eight-hour day, union recognition, and the rehiring of fired workers.		The Factory Act of 1833 limited the work day for children in factories. Those aged 9–13 could work only eight hours, 14–18 12 hours. Children under 9 were required to attend school.		In 1884, Tom Mann joined the Social Democratic Federation (SDF) and published a pamphlet calling for the working day to be limited to eight hours. Mann formed an organisation, the Eight Hour League, which successfully pressured the Trades Union Congress to adopt the eight-hour day as a key goal. The British socialist economist Sidney Webb and the scholar Harold Cox co-wrote a book supporting the "Eight Hours Movement" in Britain.[10]		The labour movement in Canada tracked progress in the US and UK. In 1890, the Federation of Labour took up this issue, hoping to organise participation in May Day.[11]		The Mexican Revolution of 1910–1920 produced the Constitution of 1917, which contained Article 123 that gave workers the right to organise labour unions and to strike. It also provided protection for women and children, the eight-hour day, and a living wage. See Mexican labour law.		In the United States, Philadelphia carpenters went on strike in 1791 for the ten-hour day. By the 1830s, this had become a general demand. In 1835, workers in Philadelphia organised the first general strike in North America, led by Irish coal heavers. Their banners read, From 6 to 6, ten hours work and two hours for meals.[12] Labour movement publications called for an eight-hour day as early as 1836. Boston ship carpenters, although not unionised, achieved an eight-hour day in 1842.		In 1864, the eight-hour day quickly became a central demand of the Chicago labour movement. The Illinois legislature passed a law in early 1867 granting an eight-hour day but had so many loopholes that it was largely ineffective. A citywide strike that began on 1 May 1867 shut down the city's economy for a week before collapsing.		On 25 June 1868, Congress passed an eight-hour law for federal employees[13][14] which was also of limited effectiveness. It established an eight-hour workday for labourers and mechanics employed by the Federal Government. President Andrew Johnson had vetoed the act but it was passed over his veto. Johnson told a Workingmen's party delegation that he couldn't directly commit himself to an eight-hour day, he nevertheless told the same delegation that he greatly favoured the "shortest number of hours consistent with the interests of all." According to Richard F. Selcer, however, the intentions behind the law were "immediately frustrated" as wages were cut by 20%.[15]		On 19 May 1869, President Ulysses Grant issued a National Eight Hour Law Proclamation.[16]		In August 1866, the National Labor Union at Baltimore passed a resolution that said, "The first and great necessity of the present to free labour of this country from capitalist slavery, is the passing of a law by which eight hours shall be the normal working day in all States of the American Union. We are resolved to put forth all our strength until this glorious result is achieved."		During the 1870s, eight hours became a central demand, especially among labour organisers, with a network of Eight-Hour Leagues which held rallies and parades. A hundred thousand workers in New York City struck and won the eight-hour day in 1872, mostly for building trades workers. In Chicago, Albert Parsons became recording secretary of the Chicago Eight-Hour League in 1878, and was appointed a member of a national eight-hour committee in 1880.		At its convention in Chicago in 1884, the Federation of Organized Trades and Labor Unions resolved that "eight hours shall constitute a legal day's labour from and after May 1, 1886, and that we recommend to labour organisations throughout this jurisdiction that they so direct their laws as to conform to this resolution by the time named."		The leadership of the Knights of Labor, under Terence V. Powderly, rejected appeals to join the movement as a whole, but many local Knights assemblies joined the strike call including Chicago, Cincinnati and Milwaukee. On 1 May 1886, Albert Parsons, head of the Chicago Knights of Labor, with his wife Lucy Parsons and two children, led 80,000 people down Michigan Avenue, Chicago, in what is regarded as the first modern May Day Parade, with the cry, "Eight-hour day with no cut in pay." In support of the eight-hour day. In the next few days they were joined nationwide by 350,000 workers who went on strike at 1,200 factories, including 70,000 in Chicago, 45,000 in New York, 32,000 in Cincinnati, and additional thousands in other cities. Some workers gained shorter hours (eight or nine) with no reduction in pay; others accepted pay cuts with the reduction in hours.		On 3 May 1886, August Spies, editor of the Arbeiter-Zeitung (Workers Newspaper), spoke at a meeting of 6,000 workers, and afterwards many of them moved down the street to harass strikebreakers at the McCormick plant in Chicago. The police arrived, opened fire, and killed four people, wounding many more. At a subsequent rally on 4 May to protest this violence, a bomb exploded at the Haymarket Square. Hundreds of labour activists were rounded up and the prominent labour leaders arrested, tried, convicted, and executed giving the movement its first martyrs. On 26 June 1893 Illinois Governor John Peter Altgeld set the remaining leader free, and granted full pardons to all those tried claiming they were innocent of the crime for which they had been tried and the hanged men had been the victims of "hysteria, packed juries and a biased judge".		The American Federation of Labor, meeting in St Louis in December 1888, set 1 May 1890 as the day that American workers should work no more than eight hours. The International Workingmen's Association (Second International), meeting in Paris in 1889, endorsed the date for international demonstrations, thus starting the international tradition of May Day.		The United Mine Workers won an eight-hour day in 1898.		The Building Trades Council (BTC) of San Francisco, under the leadership of P. H. McCarthy, won the eight-hour day in 1900 when the BTC unilaterally declared that its members would work only eight hours a day for $3 a day. When the mill resisted, the BTC began organising mill workers; the employers responded by locking out 8,000 employees throughout the Bay Area. The BTC, in return, established a union planing mill from which construction employers could obtain supplies – or face boycotts and sympathy strikes if they did not. The mill owners went to arbitration, where the union won the eight-hour day, a closed shop for all skilled workers, and an arbitration panel to resolve future disputes. In return, the union agreed to refuse to work with material produced by non-union planing mills or those that paid less than the Bay Area employers.		By 1905, the eight-hour day was widely installed in the printing trades – see International Typographical Union (section) – but the vast majority of Americans worked 12- to 14-hour days.		In the 1912 Presidential Election Teddy Roosevelts Progressive Party campaign platform included the eight-hour work day.		On 5 January 1914, the Ford Motor Company took the radical step of doubling pay to $5 a day and cut shifts from nine hours to eight, moves that were not popular with rival companies, although seeing the increase in Ford's productivity, and a significant increase in profit margin (from $30 million to $60 million in two years), most soon followed suit.[17][18][19][20]		In the summer of 1915, amid increased labour demand for World War I, a series of strikes demanding the eight-hour day began in Bridgeport, Connecticut. They were so successful that they spread throughout the Northeast.[21]		The United States Adamson Act in 1916 established an eight-hour day, with additional pay for overtime, for railroad workers. This was the first federal law that regulated the hours of workers in private companies. The United States Supreme Court upheld the constitutionality of the Act in Wilson v. New, 243 U.S. 332 (1917).		The eight-hour day might have been realised for many working people in the US in 1937, when what became the Fair Labor Standards Act (29 U.S. Code Chapter 8) was first proposed under the New Deal. As enacted, the act applied to industries whose combined employment represented about twenty percent of the US labour force. In those industries, it set the maximum workweek at 40 hours,[22] but provided that employees working beyond 40 hours a week would receive additional overtime bonus salaries.[23]		In Puerto Rico in May 1899, while under US administration, General George W. Davis acceded to Island demands and decreed freedom of assembly, speech, press, religion and an eight-hour day for government employees.		The Australian gold rushes attracted many skilled tradesmen to Australia. Some of them had been active in the chartism movement, and subsequently became prominent in the campaign for better working conditions in the Australian colonies. The eight-hour day began in 1856 in the month of May.		The Stonemasons' Society in Sydney issued an ultimatum to employers on 18 August 1855 saying that after six months masons would work only an eight-hour day. Due to the rapid increase in population caused by the gold rushes, many buildings were being constructed, so skilled labour was scarce. Stonemasons working on the Holy Trinity Church and the Mariners' Church (an evangelical mission to seafarers), decided not to wait and pre-emptively went on strike, thus winning the eight-hour day. They celebrated with a victory dinner on 1 October 1855 which to this day is celebrated as a Labour Day holiday in the state of New South Wales. When the six-month ultimatum expired in February 1856, stonemasons generally agitated for a reduction of hours. Although opposed by employers, a two-week strike on the construction of Tooth's Brewery on Parramatta Road proved effective, and stonemasons won an eight-hour day by early March 1856, but with a reduction in wages to match.[24]		Agitation was also occurring in Melbourne where the craft unions were more militant. Stonemasons working on Melbourne University organised to down tools on 21 April 1856 and march to Parliament House with other members of the building trade. The movement in Melbourne was led by veteran chartists and mason James Stephens, T.W. Vine and James Galloway. The government agreed that workers employed on public works should enjoy an eight-hour day with no loss of pay and Stonemasons celebrated with a holiday and procession on Monday 12 May 1856, when about 700 people marched with 19 trades involved. By 1858 the eight-hour day was firmly established in the building industry and by 1860 the eight-hour day was fairly widely worked in Victoria. From 1879 the eight-hour day was a public holiday in Victoria. The initial success in Melbourne led to the decision to organise a movement, to actively spread the eight-hour idea, and secure the condition generally.		In 1903 veteran socialist Tom Mann spoke to a crowd of a thousand people at the unveiling of the Eight Hour Day monument, funded by public subscription, on the south side of Parliament House on Spring St before relocating it in 1923 to the corner of Victoria and Russell Streets outside Melbourne Trades Hall.		It took further campaigning and struggles by trade unions to extend the reduction in hours to all workers in Australia. In 1916 the Victoria Eight Hours Act was passed granting the eight-hour day to all workers in the state. The eight-hour day was not achieved nationally until the 1920s. The Commonwealth Arbitration Court gave approval of the 40-hour five-day working week nationally beginning on 1 January 1948. The achievement of the eight-hour day has been described by historian Rowan Cahill as "one of the great successes of the Australian working class during the nineteenth century, demonstrating to Australian workers that it was possible to successfully organise, mobilise, agitate, and exercise significant control over working conditions and quality of life. The Australian trade union movement grew out of eight-hour campaigning and the movement that developed to promote the principle."		The intertwined numbers 888 soon adorned the pediment of many union buildings around Australia. The Eight Hour March, which began on 21 April 1856, continued each year until 1951 in Melbourne, when the conservative Victorian Trades Hall Council decided to forgo the tradition for the Moomba festival on the Labour Day weekend. In capital cities and towns across Australia, Eight Hour day marches became a regular social event each year, with early marches often restricted to those workers who had won an eight-hour day.		Promoted by Samuel Duncan Parnell as early as 1840, when carpenter Samuel Parnell refused to work more than eight hours a day when erecting a store for merchant George Hunter. He successfully negotiated this working condition and campaigned for its extension in the infant Wellington community. A meeting of Wellington carpenters in October 1840 pledged "to maintain the eight-hour working day, and that anyone offending should be ducked into the harbour". New Zealand is reputed to be the first country in the world to have adopted the eight-hour working day.		Parnell is reported to have said: "There are twenty-four hours per day given us; eight of these should be for work, eight for sleep, and the remaining eight for recreation and in which for men to do what little things they want for themselves." With tradesmen in short supply the employer was forced to accept Parnell's terms. Parnell later wrote, "the first strike for eight hours a day the world has ever seen, was settled on the spot".[25][26]		Emigrants to the new settlement of Dunedin, Otago, while onboard ship decided on a reduction of working hours. When the resident agent of the New Zealand Company, Captain Cargill, attempted to enforce a ten-hour day in January 1849 in Dunedin, he was unable to overcome the resistance of trades people under the leadership of house painter and plumber, Samuel Shaw. Building trades in Auckland achieved the eight-hour day on 1 September 1857 after agitation led by Chartist painter, William Griffin. For many years the eight-hour day was confined to craft tradesmen and unionised workers. Labour Day, which commemorates the introduction of the eight-hour day, became a national public holiday in 1899.		A strike for the eight-hour day was held in May 1919 in Peru. In Uruguay, the eight-hour day was put in place in 1915 of several reforms implemented during the second term of president José Batlle y Ordóñez. It was introduced in Chile on 8 September 1924 at the demand of then-general Luis Altamirano as part of the Ruido de sables that culminated in the September Junta.		New Zealand		United States of America		
Overtime rate is a calculation of hours worked by a worker that exceed those hours defined for a standard workweek. This rate can have different meanings in different countries and jurisdictions, depending on how that jurisdiction's labor law defines overtime. In many jurisdictions, additional pay is mandated for certain classes of workers when this set number of hours is exceeded. In others, there is no concept of a standard workweek or analogous time period, and no additional pay for exceeding a set number of hours within that week.		The overtime rate calculates the ratio between employee overtime with the regular hours in a specific time period. Even if the work is planned or scheduled, it can still be considered overtime if it exceeds what is considered the standard workweek in that jurisdiction.		A high overtime rate is a good indicator of a temporary or permanent high workload, and can be a contentious issue in labor-management relations.[1] It could result in a higher illness rate,[2] lower safety rate,[3] higher labor costs, and lower productivity.						In the United States a standard workweek is considered to be 40 hours. Most waged employees or so-called non-exempt workers under U.S. federal labor and tax law must be paid at a wage rate of 150% of their regular hourly rate for hours that exceed 40 in a week. The start of the pay week can be defined by the employer, and need not be a standard calendar week start (e.g., Sunday midnight). Many employees, especially shift workers in the U.S., have some amount of overtime built into their schedules so that 24/7 coverage can be obtained.		Overtime Rate  = ∑ Overtime Hours ∑ Regular Hours (defined) × 100 % {\displaystyle \textstyle {{\mbox{Overtime Rate }}={\frac {\sum {\mbox{Overtime Hours}}}{\sum {\mbox{Regular Hours (defined)}}}}}\times 100\%}		
Employment discrimination is a form of discrimination based on race, gender, religion, national origin, physical or mental disability, age, sexual orientation, and gender identity by employers. Earnings differentials or occupational differentiation is not in and of itself evidence of employment discrimination. Discrimination can be intended and involve disparate treatment of a group or be unintended, yet create disparate impact for a group.						In neoclassical economics theory, labor market discrimination is defined as the different treatment of two equally qualified individuals on account of their gender, race,[1] age, disability, religion, etc. Discrimination is harmful since it affects the economic outcomes of equally productive workers directly and indirectly through feedback effects.[2] Darity and Mason [1998] summarize that the standard approach used in identifying employment discrimination is to isolate group productivity differences (education, work experience). Differences in outcomes (such as earnings, job placement) that cannot be attributed to worker qualifications are attributed to discriminatory treatment.[3]		In the non-neoclassical view, discrimination is the main source of inequality in the labor market and is seen in the persistent gender and racial earnings disparity in the U.S.[3] Non-neoclassical economists define discrimination more broadly than neoclassical economists. For example, the feminist economist Deborah Figart [1997] defines labor market discrimination as “a multi-dimensional interaction of economic, social, political, and cultural forces in both the workplace and the family, resulting in different outcomes involving pay, employment, and status”.[4] That is, discrimination is not only about measurable outcomes but also about unquantifiable consequences. It is important to note that the process is as important as the outcomes.[4] Furthermore, gender norms are embedded in labor markets and shape employer preferences as well worker preferences; therefore, it is not easy to separate discrimination from productivity-related inequality.[5]		Although labor market inequalities have declined after the U.S. Civil Rights Act of 1964, the movement towards equality has slowed down after the mid-1970s, especially more in gender terms than racial terms.[3][6] The key issue in the debate on employment discrimination is the persistence of discrimination, namely, why discrimination persists in a capitalist economy.[3]		Gender earnings gap or the concentration of men and women workers in different occupations or industries in and of itself is not evidence of discrimination.[2] Therefore, empirical studies seek to identify the extent to which earnings differentials are due to worker qualification differences. Many studies find that qualification differences do not explain more than a portion of the earnings differences. The portion of the earnings gap that cannot be explained by qualifications is then attributed to discrimination. One prominent formal procedure for identifying the explained and unexplained portions of the gender wage differentials or wage gap is the Oaxaca-Blinder decomposition procedure.[2][3]		Another type of statistical evidence of discrimination is gathered by focusing on homogeneous groups. This approach has the advantage of studying economic outcomes of groups with very similar qualifications.[2]		In a well-known longitudinal study, the University of Michigan Law School (U.S.A.) graduates were surveyed between 1987 and 1993, and later between 1994 and 2000 to measure the changes in the wage gap.[7] The group was intentionally chosen to have very similar characteristics. Although the gap in earnings between men and women was very small immediately after graduation, it widened in 15 years to the point that women earned 60 percent of what men earned. Even after factoring in women's choice of working for fewer hours, and worker qualifications and other factors, such as grades in law school and detailed work history data, in 2000 men were ahead of women by 11 percent in their earnings, which might be attributed to discrimination.		Other studies on relatively homogeneous group of college graduates produced a similar unexplained gap, even for the highly educated women, such as Harvard MBAs in the United States. One such study focused on gender wage differences in 1985 between the college graduates.[8] The graduates were chosen from the ones who earned their degree one or two years earlier. The researchers took college major, GPA (grade point average) and the educational institution the graduates attended into consideration. Yet, even after these factors were accounted for, there remained a 10-15 percent pay gap based on gender. Another study based on a 1993 survey of all college graduates had similar results for black and white women regarding gender differences in earnings.[9] Both black women and white women made less money compared to white, non-Hispanic men. However, the results of earnings were mixed for Hispanic and Asian women when their earnings were compared to white, non-Hispanic men. A 2006 study looked at Harvard graduates.[10] The researchers also controlled for educational performance such as GPA, SAT scores and college major, as well as time out of work and current occupation. The results showed 30 percent of the wage gap was unexplained. Therefore, although not all of the unexplained gaps attribute to discrimination, the results of the studies signal gender discrimination, even if these women are highly educated. Human capitalists argue that measurement and data problems contribute to this unexplained gap.[7][8][9][10]		One very recent example of employment discrimination is to be seen among female Chief Financial Officers (CFOs) in the US. Although 62% of accountants and auditors are women, they are only 9% when it comes to the CFO post. According to the research not only are they underrepresented in the profession, but they are also underpaid, 16% less on average.[11]		Audit (or matched pairs) studies are done to examine hiring discrimination. In order to examine racial discrimination, the Urban Institute relied on a matched pairs study.[12] They studied the employment outcomes for Hispanic, white and black men who were between the ages 19–25 in the early 1990s. The job position was entry-level. Thus, they matched pairs of black and white men and pairs of Hispanic and non-Hispanic men as testers. The testers applied for the advertised openings for the new positions. All of the testers were given fabricated resumes where all characteristics but their race/ethnicity was nearly identical. In addition, they went through training sessions for the interviews. If both people in the pair were offered the job or if both were rejected, the conclusion was there was no discrimination. However, if one person from the pair was given the job while the other was rejected, then they concluded there was discrimination. The Institute found out that black men were three times more likely to be refused for a job compared to white men; while the Hispanic men were three times more likely to be discriminated.		The Fair Employment Council of Greater Washington, Inc. did a similar test for women via pairing testers by race.[13] The study found that the white female testers had higher chances of call back for interviews and job offers compared to black female testers. The percentage for interviews was by 10 percent more for the white testers. Among those interviewed, 50 percent white women were offered the job, while only 11 percent of black candidates received jobs offers. The white testers were also offered higher pay for the same job in cases where the same job was also offered to the black testers. The pay difference was 15 cents per hour more for the white candidates. Furthermore, black women were "steered" toward lower level jobs, while white women were even given some higher-level positions that were unadvertised.		A matched-pairs study of homogeneous group audit experiment was done in the restaurants in Philadelphia, United States.[14] Pseudo candidates handed their resumes to a random worker in the restaurants for the resume to be forwarded to the manager, which removed the effect of first impression on the employer. Also, the resumes were written in a three-level scale based on the qualifications of the pseudo applicants and resumes for each qualification level were delivered in three separate weeks. The results showed that male applicants were favored significantly. Men had higher interview callbacks or job offers. In addition, men did even better in high-pay restaurants compared to low-pay ones. In the low-price restaurants, for each man who received a job offer, the woman was rejected 29 percent of the time. There were no such cases where a man did not get the job offer but a woman did. In the high-priced restaurants, when the man got an offer, the woman was rejected 43 percent of the time. The same pattern that signaled discrimination was observed for the interviews. At the high-priced restaurants, women had 40 percent less chance of being interviewed and 50 percent less chance of receiving the job. Therefore, based on this study, it is correct to conclude discrimination in the same job may lead to gender wage discrimination. Note the high-priced restaurants are more likely to offer higher wages and higher tips for its workers compared to those with low prices.[2][3]		Another experiment is the study of the effect of "blind" symphony orchestra auditions by Goldin and Rouse.[15] In this case, the gender of the candidate was not known by the election committee because the auditions were done behind a curtain. Thus, only the skills were considered. As a result, the number of women accepted increased after “blind” auditions from less than 5 percent in 1970 to 25 percent in 1996 in the top five symphony orchestras in the U.S. In other words, a change occurred. This study tests for discrimination directly. The finding implies there was gender discrimination against woman musicians before the adoption of the screen on identity. However, this discriminatory practice was eliminated after the adoption and only qualifications of the individuals were taken into account.[2][3]		Darity and Mason [1998] summarize results of discriminatory behavior observed in other countries on the basis of "correspondence tests".[3] In this type of tests, the researchers design fabricated resumes that signal the ethnicity of the pseudo applicants via the names on the resumes and send these letters to the employers. However, the qualifications written in the resumes are comparable. In England, Afro-American, Indian or Pakistani names were not called back for the interviews but Anglo-Saxons were called.[16] In Australian audits, Greek or Vietnamese names had the same result; Anglo-Saxons were favored.[16] According to the experiment done in the University of Michigan’s study,[17] strikingly, even the “skin shade” and physical features of the individuals had negative effects the further the skin color and physical features were from white characteristics.		Darity and Mason [1998] summarize the court cases on discrimination, in which employers were found guilty and huge awards were rewarded for plaintiffs. They argue that such cases establish the existence of discrimination.[3] The plaintiffs were women or non-whites (St. Petersburg Times, 1997; Inter Press Service, 1996; The Chicago Tribune, 1997; The New York Times, 1993; the Christian Science Monitor, 1983; Los Angeles Times, 1996). Some examples are the following: In 1997, the allegations for the Publix Super Markets were “gender biases in on the job training, promotion, tenure and layoff policies; wage discrimination; occupational segregation; hostile work environment” (St. Petersburg Times, 1997, pp. 77). In 1996, allegations for Texaco were “racially discriminatory hiring, promotion and salary policies” (Inter Press Service, 1996; The Chicago Tribune, 1997, pp. 77). The six black workers, who were the plaintiffs, gave the taped racist comments of the white corporate officials as evidence (Inter Press Service, 1996; The Chicago Tribune, 1997). In 1983, the General Motors Corporation was sued both for gender and racial discrimination (the Christian Science Monitor, 1983). In 1993, the Shoney International was accused of “racial bias in promotion, tenure, and layoff policies; wage discrimination; hostile work environment (The New York Times, 1993, pp. 77) ”. The victims were granted $105 million (The New York Times, 1993). In 1996, the plaintiffs of the Pitney Bowes, Inc. case were granted $11.1 million (Los Angeles Times, 1996).		Neoclassical labor economists explain the existence and persistence of discrimination based on tastes for discrimination and statistical discrimination theories. While overcrowding model moves away from neoclassical theory, the institutional models are non-neoclassical.[2]		The Nobel Prize-winning economist Gary Becker claimed the markets punish the companies that discriminate because it is costly. His argument is as following:[18]		The profitability of the company that discriminates is decreased, and the loss is "directly proportional to how much the employer's decision was based on prejudice, rather than on merit." Indeed, choosing a worker with lower performance (in comparison to salary) causes losses proportional to the difference in performance. Similarly, the customers who discriminate against certain kinds of workers in favor of less effective have to pay more for their services, in the average.[18]		If a company discriminates, it typically loses profitability and market share to the companies that do not discriminate, unless the state limits free competition protecting the discriminators.[19]		However, there is a counter-argument against Becker's claim. As Becker conceptualized, discrimination is the personal prejudice or a "taste" associated with a specific group, originally formulated to explain employment discrimination based on race. The theory is based on the idea that markets punish the discriminator in the long run as discrimination is costly in the long run for the discriminator. There are three types of discrimination, namely: employer, employee and customer.[2][3][6][20]		In the first one, the employer has a taste for discriminating against women and is willing to pay the higher cost of hiring men instead of women. Thus, the non-pecuniary cost brings an additional cost of discrimination in dollar terms; the full cost of employing women is the wage paid plus this additional cost of discrimination. For the total cost of men and women to be equal, women are paid less than men. In the second type, the male employees have a distaste for working with women employees. Because of the non-pecuniary cost, they must be paid more than women. In the third type, the customers or clients have a distaste for being served by woman employees. Therefore, the customers are willing to pay higher prices for a good or a service in order not to be served by women. The as-if non-pecuniary cost is associated with purchasing goods or services from women.[2][20]		Becker's theory states that discrimination cannot exist in the long run because it is costly. However, discrimination seems to persist in the long run; it declined only after the Civil Rights Act, as it was seen in the economic history.[3][6][20] Regardless, it is argued that Becker’s theory holds for occupational segregation. For instance, men are more likely to work as truck drivers, or the female customers are more likely to choose to be served by women lingerie salespersons because of preferences. However, this segregation cannot explain the wage differentials. In other words, occupational segregation is an outcome of group-typing of employment between different groups but consumer discrimination does not cause wage differentials. Thus, customer discrimination theory fails to explain the combination of employment segregation and the wage differentials. However, the data points out the jobs associated with women suffer from lower pay.[3]		Edmund Phelps [1972] introduced the assumption of uncertainty in hiring decisions.[21] When employers make a hiring decision, although they can scrutinize the qualifications of the applicants, they cannot know for sure which applicant would perform better or would be more stable. Thus, they are more likely to hire the male applicants over the females, if they believe on average men are more productive and more stable. This general view affects the decision of the employer about the individual on the basis of information on the group averages.		Blau et al. [2010] point out the harmful consequences of discrimination via feedback effects regardless of the initial cause of discrimination. The non-neoclassical insight that is not part of the statistical discrimination sheds light onto uncertainty. If a woman is given less firm-specific training and is assigned to lower-paid jobs where the cost of her resigning is low based on the general view of women, then this woman is more likely to quit her job, fulfilling the expectations, thus to reinforce group averages held by employers. However, if the employer invests a lot on her, the chance that she will stay is higher.[2]		This non-neoclassical model was first developed by Bergmann.[22] According to the model, outcome of the occupational segregation is wage differentials between two genders. The reasons for segregation may be socialization, individual decisions, or labor market discrimination. Wage differentials occur when the job opportunities or demand for the female-dominated sector is less than the supply of women. According to the evidence, in general female dominated jobs pay less than male dominated jobs. The pay is low because of the high number of women who choose female dominated jobs or they do not have other opportunities.		When there is no discrimination in the market and both female and male workers are equally productive, wages are the same regardless of type of the job, F or M jobs. Assume the equilibrium wages in job F is higher than that of the M jobs. Intuitively, the workers in the less paying job will transfer to the other sector. This movement ceases only when the wages in two sectors are equal. Therefore, when the market is free of discrimination, wages are the same for different types of jobs, provided that there is sufficient time for adjustment and attractiveness of each job is the same.		When there is discrimination in the M jobs against women workers, or when women prefer the F jobs, economic outcomes change. When there is a limit of available M jobs, its supply decreases; thus, wages of the M jobs increase. Because women cannot enter to the M jobs or they choose the F jobs, they “crowd” into F jobs. Consequently, higher supply of F jobs decreases its wage rates. Briefly, segregation causes the gender wage differentials regardless of the equal skills.		Another striking point of overcrowding model is productivity. Since women in the F jobs cost less, it is rational to substitute labor for capital. On the contrary, it is rational to substitute capital for labor in the M jobs. Therefore, overcrowding causes wage differentials and it makes women less productive although they were potentially equally productive initially.[2]		The question of why women prefer working in female-dominated sectors is an important one. Some advocate this choice stems from inherently different talents or preferences; some insist it is due to the differences in socialization and division of labor in the household; some believe it is because of discrimination in some occupations.[2]		Institutional models of discrimination indicate labor markets are not as flexible as it is explained in the competitive models. Rigidities are seen in the institutional arrangements or in the monopoly power. Race and gender differences overlap with labor market institutions. Women occupy certain jobs as versus men.[23] However, institutional models do not explain discrimination but describe how labor markets work to disadvantage women and blacks. Thus, institutional models do not subscribe to the neoclassical definition of discrimination.[24]		The firms hire workers outside or use internal workforce based on worker progress, which plays a role in climbing the promotion ladder. Big firms usually put the workers into groups to have similarity within the groups. When employers think certain groups have different characteristics related to their productivity, statistical discrimination may occur. Consequently, workers might be segregated based on gender and race.[25]		Peter Doeringer and Michael Piore [1971] established the dual labor market model.[25] In this model, primary jobs are the ones with high firm-specific skills, high wages, good promotion opportunities and long-term attachment. On the contrary, secondary jobs are the ones with less skill requirement, lower wages, less promotion opportunities and higher labor turnover. The dual labor market model combined with the gender discrimination suggests that men dominate the primary jobs and that women are over-represented in the secondary jobs.[2]		The difference between primary and secondary jobs also creates productivity differences, such as different level of on-the-job training. Moreover, women have lower incentives for stability since benefits of secondary jobs are less.[25]		Moreover, lack of informal networking from male colleagues, visualizing women in the female dominated jobs and lack of encouragement do affect the economic outcomes for women. They are subject to unintentional institutional discrimination which alters their productivity, promotion and earnings negatively.[2]		The under-representation of women in top-level management might be explained by the “pipeline” argument which states that women are newcomers and it takes time to move toward the upper levels. The other argument is about barriers that prevent women from advance positions. However, some of these barriers are non-discriminatory. Work and family conflicts is an example of why there are fewer females in the top corporate positions.[2]		Yet, both the pipeline and work-family conflict together cannot explain the very low representation of women in the corporations. Discrimination and subtle barriers still count as a factor for preventing women from exploring opportunities. Moreover, it was found out that when the chairman or CEO of the corporation was a woman, the number of women working in the high level positions and their earnings increased around 10-20 percent. The effect of female under-representation on earnings is seen in the 1500 S&P firms studied. The findings indicate women executives earn 45 percent less than male executives based on the 2.5 percent of executives in the sample. Some of the gap is due to seniority, yet mostly it was because of the under-representation of women in CEO, chair or president positions and the fact that women managed smaller companies.[2]		Non-neoclassical economists point out subtle barriers play a huge role in women’s disadvantage. These barriers are difficult to document and to remove. For instance, women are left out of male’s network. Moreover, the general perception is men are better at managing others, which is seen in the Catalyst’s Fortune 1000 survey. The 40 percent of women executives said that they believed man had difficulty when they were managed by women. A separate study found out majority believed in “women, more than men, manifest leadership styles associated with effective performance as leaders,… more people prefer male than female bosses”.[2] In another study in the U.S. about origins of gender division of labor, people were asked these two questions “When jobs are scarce, men should have more right to a job than women?” and “On the whole, men make better political leaders than women do?” Some answers indicated discriminatory act.[26]		Neoclassical economics ignores logical explanations of how self-fulfilling prophecy by the employers affect the motivation and psychology of women and minority groups and thus it alters the decision making of individuals regarding human capital.[3] This is the feedback explanation that correlates with the drop in human capital investment (such as more schooling or training) attainment by women and minorities.[2]		Moreover, power and social relationships link discrimination to sexism and racism, which is ignored in the neoclassical theory. Furthermore, along with the classical and Marxist theory of competition, racial-gender structure of the job is related to the bargaining power and thus wage differential. Therefore, discrimination persists since racial and gender characteristics shape who gets the higher paying jobs, both within and between occupations. In short, the power relationships are embedded in the labor market, which are neglected in the neoclassical approach.[3][27]		In addition, critics have argued that the neoclassical measurement of discrimination is flawed.[4] As Figart [1997] points out, conventional methods do not put gender or race into the heart of the analysis and they measure discrimination as the unexplained residual. As a result, we are not informed about the causes and nature of discrimination. She argues that gender and race should not be marginal to the analysis but at the center and suggests a more dynamic analysis for discrimination. Figart argues gender is more than a dummy variable since gender is fundamental to the economy. Moreover, the segmentation in the labor market, institutional variables and non-market factors affect wage differentials and women dominate low-paid occupations. Again, none of these is because of productivity differentials nor are they the outcome of voluntary choices. Figart also indicates how women’s jobs are associated with unskilled work. For that reason, men don’t like association of “their” jobs with women or femininity, skills are engendered.[4]		Although empirical evidence is a tool to use to prove discrimination, it is important to pay attention to the biases involved in using this tool. The biases might cause under or over-estimation of labor market discrimination. There is lack of information on some individual qualifications which indeed affect their potential productivity. The factors such as motivation or work effort, which affects incomes, are difficult to be scaled. Moreover, information regarding the type of college degree may not be available. In short, all the job qualification related factors are not included to study gender wage gap.[2]		An example for underestimation is the feedback effect of labor market discrimination. That is, women may choose to invest less in human capital such as pursuing a college degree based on the current wage gap, which is also a result of discrimination against women. Another reason may be the childbearing responsibilities of women standing as a negative impact on women's careers since some women may choose to withdraw from the labor market with their own will. By doing so, they give up opportunities, such as the firm-specific training that would have potentially helped with their job promotion or reduction in the wage gap. An example of over-estimation of gender discrimination is men might have been more motivated at work. Therefore, it is wrong to equate unexplained wage gap with discrimination, although most of the gap is a result of discrimination, but not all.[2]		Furthermore, empirical evidence can also be twisted to show that discrimination does not exist or it is so trivial that it can be ignored. This was seen in the results and interpretation of the results of Armed Forces Qualifying Test, (AFQT). Neal and Johnson [1996] claimed the economic differences in the black and white labor markets were due to the "pre-market factors," not to discrimination.[28] Darity and Mason’s [1998] study of the same case disagrees with the findings of Neal and Johnson’s [1996]. They take into account factors such as age family background, school quality and psychology into consideration to make the adjustments.[3]		Although in late 20th-early 21st century discrimination is illegal, historically governments sanctioned and legally supported discrimination against certain groups. The worst forms of discrimination in recent history have been committed by governments elected through popular vote. An example would be the antisemitic practices of Nazi Germany, which human capital theorists claim would not have happened had free markets prevailed, because, according to them, discrimination would have caused losses.[18] The argument is based on the idea that government officials and politicians do not care about losses as much as companies do; as a result they have less incentive not to discriminate.		Another example is the case of U.S. when a law was passed in the early 1900s, based on popular sentiment among the white population, to require applicants for civil service jobs to submit photographs. At the time blacks in the U.S. had started to compete for jobs that had previously been all-white jobs. After the passage of the law, the number of blacks in federal employment plummeted for decades.[19]		Another example is in early 20th century comes from South Africa, where mine owners preferred to hire black workers because they were lower wage workers. In response the white majority successfully persuaded the government to enact laws that highly restricted the blacks' rights to work (see Apartheid).[19]		Blau et al. [2010] sum up the argument for government intervention to address discrimination. First, discrimination prevents equity or fairness, when an equally qualified person does not receive equal treatment as another on account of race or gender. Second, discrimination results in inefficient allocation of resources because workers are not hired, promoted or rewarded based on their skills or productivity.[2]		Becker claimed discrimination in the labor market is costly to employers. His theory is based on the assumption that in order to survive in the existence of competitive markets, employers cannot discriminate in the long run. Strongly believing in the perfect functioning of markets without government or trade union intervention, it was claimed that employer discrimination declines in the long run without political intervention. On the contrary, intervention of human capital investment and regulation of racial interactions make it worse for the disadvantaged groups. Moreover, it was claimed discrimination could only persist due to the "taste" for discrimination and lower education level of blacks explained the labor-market discrimination.[6][20]		However, based on the empirical study, either human capital theory or Becker’s tastes theory does not fully explain racial occupational segregation. That is seen with the increase in black work force in the South as an effect of Civil Rights laws in the 1960s. Therefore, human-capital and "taste-for-discrimination" are not sufficient explanations and government intervention is effective. Becker's claim about employers would not discriminate as it is costly in the competitive markets is weakened by the evidence from real life facts. Sundstrom [1994] points out, it was also costly to violate the social norms since customers could stop buying the employer's goods or services; or the workers could quit working or drop their work effort. Moreover, even if the workers or the customers did not participate in such behaviors, the employer would not take the risk of experimenting by going against the social norms. This was seen from the historical data that compares the economic outcomes for the white and black races.[6]		Women worked in the U.S. industrial sector during the World War II. However, after the war most women quit jobs and returned home for domestic production or traditional jobs. The departure of women from industrial jobs is argued to represent a case of discrimination.[29]		The supply theory claims voluntary movement because women worked due to extraordinary situation and they chose to quit. Their involvement was based on patriotic feelings and their exit depended on personal preferences and it was a response to feminist ideology. On the contrary, demand theory claims working class women changed occupations due to high industrial wages.[29] Tobias and Anderson [1974] present the counter argument for supply theory.[30] Furthermore, there were both housewives and working class women, who had been working prior to the war in different occupations. According to Women's Bureau's interviews, majority of women who had been working wanted to continue to work after the war. Despite their will, they were laid off more than men. Most of them possibly had to choose lower-paying jobs.[29]		The exit pattern shows their quit was not voluntarily. There were pressures women faced, such as change in position to janitorial job, more or new responsibilities at work, and additional or changed shifts that would not fit their schedules, which were all known by the management. Women lay-off rates were higher than men. Briefly, women were treated unequally postwar period at the job market although productivity of women was equal to that of men and women's wage cost was lower.[29]		Supply and demand theories do not provide sufficient explanation regarding women's absence in industrial firms after the war. It is wrong to associate patriotism with the war-time women workers since some housewives quit their jobs at early periods of the war when the country needed their help the most. Some of the housewives were forced to quit as the second highest lay-off rate belonged to them. If their only concern was the well-being of their country at the war time, less persistence to exit would have been observed.[29]		The demand theory partially holds as there were women who worked pre-war time for occupational and wage mobility opportunities. However, these experienced women workers voluntarily quit working more than housewives did. The reason is work-experienced women had many opportunities. However, women with fewer options of where to work, such as African-Americans, older married women, housewives and the ones working in lowest paying jobs, wanted to keep their jobs as long as possible. Thus, their leave was involuntarily.[29]		Although women's job performance at least as good as men's,[citation needed] instead of trying to equalize pays, women's wages were kept below than men’s.[citation needed] Women had higher lay-off rates but also they were not rehired despite the boom in the auto industry. Some argue this was due to the lack of a civil rights movement protecting the rights of women as it did for black men. This explanation is unsatisfactory since it does not explain anti-women worker behavior of the management or lack of protection from unions. Kossoudji et al. [1992] believe it was due to the need for two separate wage and benefits packages for men and women. Women had child care responsibilities such as day care arrangements and maternity leave.[29]		Before the passage of the Civil Rights Act of 1964 in the U.S., employment discrimination was legal and widely practiced. The newspaper ads for various jobs indicated racial and gender discrimination explicitly and implicitly. These behaviors were all built on the assumption that women and blacks were inferior.[3] At the turn of the 21st century, discrimination is still practiced but to a lesser degree and less overtly. The progress on the evident discrimination problem is visible. However, the effect of past is persistent on the economic outcomes, such as historical wage settings that influence current wages. Women are not only under-represented in the high-rank and high-paid jobs, but they are also over-represented in the secondary and lower-paid jobs. The interviews, personal law, wage data and confidential employment records with salaries along with other evidence show gender segregation and its effects on the labor market.[4]		Although there is some inevitable occupational segregation based people’s preferences, discrimination does exist.[2][3] Moreover, persistence of discrimination remains even after government intervention. There is a decline in the wage gap due to three reasons: male wages decreased and women’s wages increased; secondly, the human capital gap between the two genders and experience gap have been closing; thirdly, legal pressures decreased discrimination but there is still inequality in the national economy of the U.S.[3]		The correlation of Civil Rights Act and decrease in discrimination suggests the Act served its purpose. Therefore, it is correct to say leaving discrimination to diminish to the competitive markets is wrong, as Becker had claimed.[3][6] In 1961, Kennedy issued an executive order calling for a presidential commission on the status of women. In 1963, Equal Pay Act, which required the employers to pay the wages to men and women for the same work qualifications, was passed. In 1964, Title VII of the Civil Rights Act with the exception bona fide occupational qualifications (BFOQ) was accepted while the Equal Employment Opportunity Commission (EEOC) responsible to check whether the Equal Pay Act and Title VII were followed. The Title VII of the Civil Rights Act was first written to forbid employment discrimination. Initially it prohibited discrimination on the basis of race, religion and national origin. However, inclusion of the sex accepted last minute. The Title VII addresses both the disparate impact and disparate treatment. In 1965, Executive Order 11246 was passed and in 1967, it was changed to include sex, which prohibited employment discrimination by all employers with federal contracts and subcontracts. In addition, it makes sure affirmative action takes place. In 1986, sexual harassment was accepted as illegal with Supreme Court’s decision. In 1998, the largest sexual harassment settlement was negotiated with $34 million to be paid to female workers of Mitsubishi.		As a result of these government policies occupational segregation decreased. The gender wage gap started to get smaller after the 1980s, most likely due to indirect feedback effects which took time, but an immediate increase in the earnings of blacks was observed in 1964. However, the laws still do not control discrimination fully in terms of hiring, promotion and training programs etc.[2][6]		Executive Order 11246, which is enforced by the Office of Federal Contract Compliance, is an attempt to eliminate the gap between the advantaged and disadvantaged groups on account of gender and race. It requires contractors to observe their employment patterns. If there is under-representation of women and minorities, “goals and timetables” are created to employ more of the disadvantaged groups on account of gender and race. The pros and cons of affirmative action have been discussed. Some believe discrimination does not exist at all or even it does, prohibiting it is enough, and affirmative action is not needed. Some agree that some affirmative action is needed but they have considerations regarding the use of goals and timetables as they might be too strict. Some think strong affirmative action is needed but they are worried if there would be really sincere effort to hire the qualified individuals from the vulnerable groups.[2]		Rodgers et al. [2003] state minimum wage can be used as a tool to combat discrimination, as well and to promote equality.[27] Since discrimination is embedded in the labor market and affects its functioning, and discrimination creates a basis for labor market segregation and for occupational segregation, labor markets institutions and policies can be used to reduce the inequalities. Minimum wage is one of these policies that could be used.[27]		The minimum wage has benefits because it alters the external market wage for women, provides a mechanism for regular increases in the wages and arranges social security. It affects women in the informal sector, which is highly dominated by women partly as an outcome of discrimination, by being a reference point.[27][31][32] However, disadvantages include: first, the wage might be very low when skills and sector aren’t taken into consideration, secondly, adjustment may take time, thirdly, enforcement may not be feasible and finally when there are public spending cuts, the real value of the wage may decline due to social security.[27]		Others have argued that minimum wage simply shifts wage discrimination to employment discrimination. The logic is that if market wages are lower for minorities, then employers have an economic incentive to prefer hiring equally qualified minority candidates, whereas if all workers must be paid the same amount then employers will instead discriminate by not hiring minorities. Minimum wage laws could be responsible for the very high unemployment rate of black teenagers compared to white teenagers.[33]		Some employers have made efforts to reduce the impact of unconscious or unintentional systematic bias.[34] After a study found a substantial increase in hiring equity, some musical organizations have adopted the blind audition; in other fields like software engineering, communications, and design, this has taken the form of an anonymized response to an job application or interview challenge.[35]		The language of job listings has been scrutinized; some phrases or wording are believed to resonate with particular demographics, or stereotypes about particular demographics, and lead to some women and minorities not applying because they can less easily visualized themselves in the position. Examples cited include "rockstar" (which may imply a male) and nurturing vs. dominant language. For example: "Superior ability to satisfy customers and manage company’s association with them" vs. "Sensitive to clients' needs, can develop warm client relationships".[36][37]		Employers concerned about gender and ethnic representation have adopted practices such as measuring demographics over time, setting diversity goals, intentionally recruiting in places beyond those familiar to existing staff, targeting additional recruiting to forums and social circles which are rich in female and minority candidates[38][39] Pinterest has made its statistics and goals public, while increasing efforts at mentorship, identifying minority candidates early, recruiting more minority interns, and adopting a "Rooney Rule" where at least one minority or female candidate must be interviewed for each leadership position, even if they are not in the end hired.[40]		Statistics have found that women typically earn lower salaries than men for the same work, and some of this is due to differences in negotiations - either women do not ask for more money, or their requests are not granted at the same rate as men. The resulting differences can be compounded if future employers use previous salary as a benchmark for the next negotiation. To solve both of these problems, some companies have simply banned salary negotiations and use some other method (such as industry average) to peg the salary for a particular role. Others have made salary information for all employees public within the company, which allows any disparities between employees in the same roles to be detected and corrected.[41] Some research has suggested greater representation of women in the economic modeling of the labor force.[42]		Laws often prohibit discrimination on the basis of:[43]		Employees who complain may be protected against workplace or employment retaliation.[44]		Many countries have laws prohibiting employment discrimination including:		Sometimes these are part of broader anti-discrimination laws which cover housing or other issues.		During the past decade, hiring discrimination was measured by means of the golden standard[45][46] to measure unequal treatment in the labour market, i.e. correspondence experiments. Within these experiments, fictitious job applications that only differ in one characteristic, are sent to real vacancies. By monitoring the subsequent call-back from employers, unequal treatment based on this characteristic can be measured and can be given a causal interpretation.		Pervasive levels of ethnic labour market discrimination are found in Belgium, Greece, Ireland, Sweden and the UK.[47][48][49][50][51] Job candidates with foreign names are found to get 24% to 52% less job interview invitations compared to equal candidates with native names. Interestingly, ethnic discrimination is lower among the high-educated and in larger firms.[51][52] In addition, unequal treatment is found to be heterogeneous by the labour market tightness in the occupation: compared to natives, candidates with a foreign-sounding name are equally often invited to a job interview if they apply for occupations for which vacancies are difficult to fill, but they have to send twice as many applications for occupations for which labor market tightness is low.[47] Recent research shows that ethnic discrimination is nowadays driven by employers' concern that co-workers and customers prefer collaborating with natives.[53] In addition, volunteering has found to be a way out of ethnic discrimination in the labour market.[54]		In 2014, a large correspondence experiment was conducted in Belgium. Two applications of graduates, identical except that one revealed a disability (blindness, deafness or autism), were both sent out to 768 vacancies for which the disabled candidates could be expected to be as productive as their non-disabled counterparts, based on the vacancy information. In addition, the researcher randomly disclosed the entitlement to a substantial wage subsidy in the applications of the disabled candidates. Disabled candidates had a 48% lower chance to receive a positive reaction from the employer side compared with the non-disabled candidates. Potentially due to the fear of the red tape, disclosing a wage subsidy did not affect the employment opportunities of disabled candidates.[55]		While overall no severe levels of discrimination based on female gender is found, unequal treatment is still measured in particular situations, for instance when candidates apply for positions at a higher functional level in Belgium,[56] when they apply at their fertiles ages in France,[57] and when they apply for male-dominated occupations in Austria.[58]		Discrimination based on sexual orientation varies by country. Revealing a lesbian sexual orientation (by means of mentioning an engagement in a rainbow organisation or by mentioning one's partner name) lowers employment opportunities in Cyprus and Greece but has, overall, no negative effect in Sweden and Belgium.[59][60][61][62] In the latter country, even a positive effect of revealing a lesbian sexual orientation is found for women at their fertile ages.		Pervasive levels of age discrimination are found in Belgium, England, France, Spain and Sweden. Job candidates revealing older age are found to get 39% (in Belgium) to 72% (in France) less job interview invitations compared to equal candidates revealing a younger name. Discrimination is heterogeneous by the activity older candidates undertook during their additional post-educational years. In Belgium, they are only discriminated if they have more years of inactivity or irrelevant employment.[63][64][65][66][67][68][69]		Furthermore, European studies provide evidence for hiring discrimination based on former unemployment,[70][71] trade union membership,[72] beauty,[73] HIV,[74] religion,[75] youth delinquency,[76] former underemployment,[71] and former depression.[77] Employment at the army is found to have no causal effect on employment opportunities.[78]		By means of their seminal correspondence experiment, Marianne Bertrand and Sendhil Mullainathan, showed that applications from job candidates with white-sounding names got 50 percent more callbacks for interviews than those with African-American-sounding names in the United States at the start of this millennium.[79] Similarly, a 2009 study found that black applicants for low-wage jobs in New York City were half as likely as whites with equivalent resumes, interpersonal skills, and demographic characteristics.[80]		In Canada, research[81] conducted in 2010 by University of Toronto researchers Philip Oreopoulos and Diane Dechief has found that resumes featuring English-sounding names sent to Canadian employers were more than 35% more likely to receive an interview call-back as compared to resumes featuring Chinese, Indian or Greek-sounding names. The study, supported by Metropolis BC., a federally funded diversity-research agency, was conducted to investigate why recent immigrants are struggling much more in the Canadian job markets than immigrants in the 1970s. In order to test this hypothesis, dozens of identical resumes, with only the name of the applicant changed, was sent to employers in Toronto, Vancouver and Montreal. Of the three cities surveyed, Metro Vancouver employers, both large and small, were the least swayed by the ethnicity of an applicants' name. Resumes submitted to employers here were just 20% more likely to get a callback than those with Chinese or Indian names. Through interviews with Canadian employers, the researchers found that name-based discrimination on application forms were a result of time-pressed employers being concerned that individuals with foreign backgrounds would have inadequate English-language and social skills for the Canadian marketplace.[81]		In 2006, just over one-half (51%) of persons with disabilities were employed, compared to three in four persons without disabilities.[82]		Employment rates are lower (under 40%) for persons with developmental and communication disabilities, whereas employment rates are closer to average for persons with a hearing impairment or for those who have problems with pain, mobility, and agility.[82]		Data from Statistics Canada's Participation and Activity Limitation Survey[82] (PALS) show that, in 2006, one in four unemployed persons with a disability and one in eight persons with a disability who are not in the workforce believe that, in the past five years, they've been refused a job because of their disability. One in twelve employed persons with a disability also reported that they experienced discrimination, with the proportion of discrimination "increasing with the severity of activity limitations".[83]		According to 2011 Statistics Canada data,[84] the gender wage gap in Ontario is 26% for full-time, full-year workers. For every $1.00 earned by a male worker, a female worker earns 74 cents. In 1987, when the Pay Equity Act was passed, the gender wage gap was 36%. It is estimated that as much as 10 to 15% of the gender wage gap is due to discrimination.[85]		The Williams Institute, a national think tank at UCLA School of Law, released a 2011 report[86] that has identified sexual orientation and gender identification discrimination in the workplace. According to the report,[86] between 15–43% of lesbian, gay, bisexual or transgender workers have experienced either being fired, denied promotions or harassment due to their sexual orientation or gender identification.[86] Additionally, only 20 states in the United States of America prohibit discrimination based on sexual orientation and gender identity in the workplace. Wisconsin and New Hampshire prohibit discrimination based on sexual orientation but not gender identity.[87]		A 2013 report[88] was completed by the AARP to identify the impact of age discrimination in the workplace. Of those 1500 individuals who responded to AARP's 2013 Staying Ahead of the Curve survey, almost 64% of those over 45–74 said they have seen or have experienced age discrimination in the workplace. Of those, 92% say it was somewhat or very common in their workplace.[88]		
Contingent work or casual work is an employment relationship which is considered non-permanent. These jobs are typically part time (typically with variable hours), have limited job security, and result in payment on a piece work basis. Contingent work is usually not considered to be a career or part of a career. One of the features of contingent work is that it usually offers little or no opportunity for career development. Contingent workers are also often called freelancers, independent professionals, temporary contract workers, independent contractors, or consultants.[1][2]		Contingent work is not an entirely neutral term as commentators who use the phrase generally consider it to be a social problem.[citation needed] Employment agencies and classified advertising media are more likely to use the phrase casual work, particularly to attract students who wish to earn money during the summer vacation but who would not consider the work as part of a long-term career. All casual work is considered to be contingent work, but not all contingent work is casual. In particular, part time jobs, or jobs in organisations that have a high staff turnover, may be considered contingent work but may not be casual.[citation needed]						The concept of what is now considered to be a job, where one attends work at fixed hours was rare until the Industrial Revolution. Before then, the predominant regular work was in agriculture. Textile workers would often work from home, buying raw cotton from a merchant, spinning it and weaving it into cloth at home, before selling it on.		In the 1770s, cotton mills started to appear in Lancashire, England, using Richard Arkwright's spinning jenny and powered by water wheels. Workers would often work in twelve-hour shifts, six days a week. However, they would still often be paid on a piece work basis, and fines would be deducted from their pay for damage to machinery. Employers could hire and fire pretty much as they pleased, and if employees had any grievance about this, there was very little that they could do about it.		Individual workers were powerless to prevent exploitation by their employers. However, the realisation that all workers generally want the same things, and the benefits of collective bargaining, led to the formation of the first trade unions. As trade unions became larger, their sphere of influence increased, and started to involve political lobbying, resulting in much of the employment law that is now taken for granted.[citation needed]		Manufacturing has declined during the 20th century in the Western world. Many manufacturing organisations that employ large numbers of people have relocated their operations to developing nations. As a result, whenever they do hire staff in Europe or North America, they often need to be able to fire them quickly and keep costs as low as possible, to remain competitive. As a result, some employers may look for loopholes in employment law, or ways of engaging staff that allows them to circumvent union-negotiated employment law, creating what is now known as contingent work.[citation needed]		According to the US Bureau of Labor Statistics (BLS), the nontraditional workforce includes "multiple job holders, contingent and part-time workers, and people in alternative work arrangements".[3] These workers currently represent a substantial portion of the US workforce, and "nearly four out of five employers, in establishments of all sizes and industries, use some form of nontraditional staffing". "People in alternative work arrangements" includes independent contractors, employees of contract companies, workers who are on call, and temporary workers.[3]		Among several other contributing factors, globalization has had a large impact on the growth in using contingent labor. Globalization contributes to rapid growth in industries, increased outsourcing, and a need for flexibility and agility to remain competitive.[4] By engaging contract workers, organizations are able to be agile and save costs. The contingent workforce acts as a variable workforce for companies to select from to perform specific projects or complete specialized projects.[5] Also as organizations make efforts to be more agile and to quickly respond to change in order to be more competitive, they turn to the contingent workforce to have on-demand access to professionals and experts.[6] Organizations also see the opportunity to reduce benefits and retirement costs by engaging the contingent workforce.[5] However, there is risk involved in avoiding these costs if an employee is improperly classified as a contingent worker. Using the contingent workforce is also cost-effective in that using contingent labor allows for adjustments to employment levels and employment costs depending on what kind of expertise and labor is need and at what time it is needed.		Trends in the contingent workforce are also impacted by the economy. A study conducted by the MPS Group shows the relationship between the contingent labor cycle and the state of the economy.[7] In a bullish economy, the demand for contingent labor is strong. This is most likely because organizations are trying to grow with the economy, and using contingent workers allows them to work with experts when needed, without the long-term costs of hiring them.		A knowledge-driven economy also contributes to the growth in the use of the contingent workforce because organizations rely more on their specific and expert knowledge and expertise.[8] As demand increases for highly skilled and knowledgeable people, the expertise of contract workers becomes more attractive.		Contingent workers are at a high risk of being injured or killed on the job. In 2015, 829 fatal injuries (17% of all occupational fatalities) occurred among contract workers, which only represent a subset of contingent workers.[9] Studies have also shown a higher burden of non-fatal occupational injuries and illnesses among contingent workers compared to those in standard employment arrangements.[10][11] There are many possible contributing factors to the high rates of injuries and illnesses among contingent workers. They are often inexperienced and assigned to hazardous jobs and tasks,[12][11][13][14] may be reluctant to object to unsafe working conditions or to advocate for safety measures due to fear of job loss or other repercussions,[13] and they may lack basic knowledge and skills to protect themselves from workplace hazards due to insufficient safety training.[15] According to a joint guidance document released by the Occupational Safety and Health Administration (OSHA) and the National Institute for Occupational Safety and Health (NIOSH), both staffing agencies and host employers (i.e., the clients of staffing agencies) are responsible for providing and maintaining a safe and healthy work environment for contingent workers.[16] Collaborative and interdisciplinary (e.g., epidemiology, occupational psychology, organizational science, economics, law, management, sociology, labor health and safety) research and intervention efforts are needed to protect and promote the occupational safety and health of contingent workers.[17]		Contingent work jobs are widely referred to as McJobs[citation needed]. This term was made popular by Douglas Coupland's novel Generation X: Tales for an Accelerated Culture, and stems from the notion that jobs in McDonald's and other fast food and retail businesses are frequently insecure.		Critics say that it is unfair to tarnish all employment agencies with the brush of contingent work.[citation needed] Some say that temporary work patterns such as self-employment, consultancy and telecommuting can bring benefits of flexibility not just to employers but also employees, can improve work-life balance, and can make it easier for workers to manage family responsibilities. However, it is argued[citation needed] that such benefits are realised only in middle class jobs, whose entry barriers are too high for most workers with below-average earnings.		
A permatemp is a temporary employee who works for an extended period for a single staffing client. The word is a portmanteau of the words permanent and temporary.		There are two types of permatemp employment relationships. In the first form, a public or private employer hires employees as "temporary" or "seasonal" employees, but retains them, often full-time for year after year, often with less pay and without any benefits. These employees often do the same work as permanent employees, but without the same pay, benefits, and labor rights. The second kind of permatemp is an employee of a staffing service provider, payroll agency or Professional Employer Organization, which sends workers to work in a long-term, on-site position for a private company or public employer. The employee is paid by the staffing service provider or agency rather than by the primary employer.		In the United States these agencies are required by the US Internal Revenue Service (IRS) to pay the employer portion of Social Security and Medicare taxes (FICA) and Federal Unemployment Tax (FUTA) in accordance with IRS Publication 15A. U.S. leasing organizations are also required to provide employees with health coverage by the United States Department of Labor, the requirements of the health care offered will change in 2014 to comply with the Affordable Care Act (ObamaCare). Long-term full-time leased employees in the U.S. may also be offered a retirement benefit package with a minimum (leasing) company contribution of at least 10%, IRS Form7003.						Traditionally, a temporary employee is hired to substitute for an employee who is on leave or vacation or to staff a project for which there are insufficient permanent employees to carry out the task. A seasonal employee is hired for the limited time because the work is necessary only for a certain part or season of the year. The normal practice of temporary employment for an agency is one in which the employees have a close relationship with the agency from which they receive their pay. Their work may range from day labor to high-priced consulting. The employee may work for one or several companies, and the working periods may be for days or months at a time, but the working periods often come about irregularly.		"Permatemps" are often distinguished from temporary employees by working for the same company for a long, possibly indefinite amount of time, working the same schedules and hours of regular employees, and by requirements such as "company" training or required attendance at "company" meetings. This is where many Leasing Agencies in the U.S. run afoul of the IRS and US Department of Labor. The IRS, in an effort to close loop holes which allow companies to hire temporary employees and thus avoid federal employee taxes have created a very clear definition of a "Common Law Employee" versus a "permatemp". The IRS definition of a common law employee rests on whom actually controls the work done by the leased employee. IRS Publication 15A explains "Under Common Law Rules anyone who performs services for you is generally your employee if you have the right to control what will be done and how it will be done...What matters is you have the right to control the details of how the services are performed". IRS 15A also defines the role of staffing services with "The staffing service has the right to control and direct the worker's services for the client, including the right to discharge or reassign the worker. The Staffing Service hires the workers, provides them with unemployment insurance and other benefits, and is the employer for employment tax purposes." Further clarification for U.S. employees can be found in IRS Publication 15A Section 2. Misclassification of employees can lead to severe tax liabilities (IRS PUB 15 Circular E) and civil penalties as in the case of Vizcaino v Microsoft. Furthermore, if a "permatemp" actually qualifies as a common law employee, they are entitled to the same fringe benefits their co-workers receive either after one year or after the qualification standard set for regular employees, IRS Publication 15B. IRS Publication 7003 goes so far as to say "An individual who is actually a common law employee of the recipient (the worksite company) will not become an employee of another entity merely because the recipient enters into a formal "leasing agreement' with another entity."		Regular, permanent employees work for a single employer and are paid directly by that employer. In addition to their wages, they may receive benefits, such as subsidized health care, paid vacations, holidays, sick time, or contributions to a retirement plan. Regular employees are sometimes eligible to switch job positions within their companies. Even when employment is "at will," regular employees of large outfits are sometimes protected from abrupt job termination by severance policies, like advance notice in case of layoffs, or formal discipline procedures. They may be eligible to join a union, and may enjoy both social and financial benefits of their employment.		In order to pay the employee, the staffing firm is paid by the worksite company at an agreed upon bill rate, which can be many percentage points higher than the pay rate.		Arguments have been made that when a worker is actually employed full-time, year round, but called a temporary or seasonal employee, the employee is being exploited by being denied the wages, benefits, and employment rights enjoyed by other employees. While it is unknown how common this kind of situation is, class action lawsuits have been decided against Seattle, Washington and King County, Washington. These public sector cases generally involve violation of ordinances or rules limiting the length of service of such workers.		Two California cases address the issues of public employees who were improperly considered "temporary" when they were actually employed as regular, permanent employees. The first case involves the Los Angeles County Fire Department; the second such case concerns the employment practices of the Metropolitan Water District of Southern California. These cases are both class action lawsuits that have been litigated over a number of years. Both cases are near, or in the process of, being settled.		In an Albuquerque, New Mexico, case[citation needed] a federal district judge ruled that an employee who worked full-time for the City of Albuquerque for more than ten years as a "seasonal" supervisor and recreation leader (never earning more than $7.00 per hour and with no benefits) might have had a "property interest" in his employment such that he could not be terminated without a hearing. The judge also certified a conditional class of "similarly situated" city employees employed as temporary or seasonal employees in violation of City ordinances, which limited temporary employees to two years and limited seasonal employees to nine months or less each year.		Staffing through temporary agencies became common in the Silicon Valley technology companies. Permatemping came into vogue simultaneously with the economic bubble of the 1990s. Most recently, General Motors and its subsidiary, Delphi, announced plans to rely on temporary employees. Whether these will be long-term temps, or permatemps, remains to be seen.		General Motors has used "permatemps" for a long time in its lowest management level, Level 6 Supervisor, under a national contract with Kelly Services.		One legal issue and one tax issue, both having to do with permatemps at Microsoft, defined permatemping and also changed it.		In 1996, a class action lawsuit was brought against Microsoft representing thousands of current and former employees that had been classified as temporary and freelance. The monetary value of the suit was determined by how much the misclassified employees could have made if they had been correctly classified and been able to participate in Microsoft's employee stock purchase plan. The case was decided on the basis that the temporary employees had had their jobs defined by Microsoft, worked alongside regular employees doing the same work, and worked for long terms (years, in many cases).		The Microsoft case centered on the language found in Microsoft’s Employee Stock Purchase Plan (ESPP). In that plan, Microsoft defined plan participants (those eligible to participate in the plan) as all “common law employees” on the company’s payroll. The only employees specifically excluded under the language of the ESPP were employees who worked less than five months per year or less than half-time. In reference to the independent contractors, the court held that because Microsoft conceded they were common-law employees and not independent contractors, Microsoft had no legitimate basis for rejecting their benefit claims. With regard to the individuals provided by temporary staffing agencies, the court used the following five factor test in determining whether they were truly “common law employees” and therefore eligible to participate in the plan:		Employees are not entitled to benefits unless they are “eligible” employees under a plan, regardless of whether they meet the common-law test for “employee” status. In other words, a plan need not adopt or incorporate the common law definition of an employee in delineating the scope of its coverage. Employers are free to draft employee benefit plans that leave out certain groups of workers. For example, a plan can exclude contingent workers or workers hired through a third party agency. It can also exclude workers who do not elect to participate in the plan.		The case and subsequent appeals were heard in the United States Court of Appeals for the Ninth Circuit. Before a final ruling could be issued, Microsoft settled the case for US$97 million. The Microsoft permatemps collected their money almost 10 years later.		Simultaneous with Vizcaino, the United States Internal Revenue Service issued a ruling that Microsoft owed millions of dollars in payroll taxes. The IRS determined that permatemp employees were common law employees of Microsoft and the staffing firm's role was simply that of payroll processor.		As a result of the legal and tax rulings, human resources organizations at many companies changed their policies towards temporary employees. Microsoft, for example, decreed that an individual could not be a temp for more than 364 days, and that individuals must be separated from Microsoft for more than 100 days between temporary assignments with the company. Other companies have created policies stating that temporary workers can be assigned to only specific projects that last just a few months. Individuals are often prohibited from taking back-to-back assignments within an agency client company.		When a company requires a break in service of its permatemps, the result is often that those employees regularly cycle between two companies instead of having back-to-back assignments. Other permatemps plan for personal breaks and simply use the time as vacation. In most cases, they are eligible for unemployment insurance as long as they nominally look for work. This form of permatemping may be attractive to those not wanting a steady, full-time, or year-round position, or not wanting to be committed to one position or one employer.		Another arrangement to avoid long-term serial temporary assignments is to "in-source" the work to be done, and not the position that does the work. In this arrangement, a company does not hire a staffing firm to fill a position, but rather hires it to do the work. The staffing firm still must hire the permatemp to do the work, still on-site at the corporation.		Some of these alternative arrangements barely differ from the pre-Vizcaino format for permatemping. Laws and legal rulings continue to define the permatemp-employee relationship. The IRS continues to warn many companies they may owe employment taxes for their temporary workers and employee lawsuits over temping repeat the same arguments.		Due to the 365-day rule, high value contractors (typically in IT) who choose to accept the risk of not receiving benefits and of contract termination in exchange for higher hourly rates are forced out of standard business relationships. This causes problems for both the contractor, who must continually move to new companies, and for the company, which must retrain and familiarize a new contractor with business rules and infrastructure.		Some permatemps also disagree on the effectiveness of lawsuits and new laws to regulate hiring.		Other critics note that the constant job turnover mandated by human resources department policies has the effect of increasing the unemployment rate, which has led to wage deflation in fields with large numbers of permatemps.		In the wake of employee lawsuits, most companies have not increased hiring of staff in positions typically held by permatemps. In fact, rather than risk lawsuits, many firms have decided not to hire within their own country at all, instead turning work formerly done by their pools of permatemps over to outsourcing firms in other countries.		In Microsoft's corporate culture, the presence of permatemps created a caste-like system. That permatemps had socially integrated into the corporate culture and that the company had included permatemps in morale events and gift giving was evidence both in Vizcaino and to the IRS for a communal corporate culture. Policy enforcement that now restricts permatemps from participating in morale events, employee social clubs and the like creates a second class division between regular employees and permatemps. (At one time temporary employees at Microsoft referred to being hired on permanently, with accompanying stock benefits, as "being knighted".)		Many corporations hire temporary employees to do work they deem low-skilled or unimportant. Permatemps hired to do that work may not get the resources that a regular employee would. Permatemps might be forced to share office space, cubicles or phones when regular employees have their own. Employee badges for permatemps might be a different color, and permatemps may be recognized in the corporate e-mail system by dashes or other identifiers appended to their login ID. By declaring positions filled by permatemps to be low-skilled and making it easier for regular employees to identify their co-workers who are permatemps, companies create a sense of elitism in their regular employees. Permatemps, as a group, might be known by epithets such as "dash trash" (referring to an identifier and a dash prepended to an email user account).[1]		Frequently permatemps are highly skilled, excellent workers, particularly in the IT field,[citation needed] but are still not allowed to participate in company events or receive bonuses for work well done. If they earn over the United States Department of Labor minimum for overtime exemption, they may be asked to put in similar overtime hours to benefitted, salaried employees without overtime compensation. Depending on the staffing firm and corporation policies, permatemps may discover themselves in one of several positions, all of which require the same level of work from them as from their coworkers:		
Annual leave is paid time off work granted by employers to employees to be used for whatever the employee wishes. Depending on the employer's policies, differing number of days may be offered, and the employee may be required to give a certain amount of advance notice, may have to coordinate with the employer to be sure that staffing is adequately covered during the employee's absence, and other requirements may have to be met. The vast majority of countries today mandate a minimum amount of paid annual leave by law, though the United States is a notable exception in mandating no minimum paid leave and treating it as a perk rather than a right.						Most countries around the world have labor laws that mandate employers give a certain number of paid time-off days per year to workers. Nearly all Canadian provinces require at least two weeks; in the European Union the countries can set freely the minimum, but it has to be at least 20 days (not including national holidays). Full-time employment in Australia requires twenty annual leave days a year. US law does not require employers to grant any vacation or holidays, and about 25% of all employees receive no paid vacation time or paid holidays.[2]		According to the Bureau of Labor Statistics in the United States, the average paid holidays for full-time employees in small private establishments in 1996 was 7.6 days.[3] Members of the U.S. Armed Services earn 30 vacation days a year, not including national holidays. Although the law does not mandate vacation time, many employers nonetheless offer paid vacation, typically around 10 work days, to attract employees.[4] Under US federal law, employers usually must compensate terminated employees for accrued but unused vacation time. Additionally, many American employers provide paid days off for national holidays, such as Christmas, New Years, Independence Day, Memorial Day, Labor Day, and Thanksgiving.		Countries (such as Italy and Denmark) or particular companies may mandate summer holidays in specific periods.		Argentina has different labor laws, for public employment and private employment. Public employees, have between a minimum of 21 days paid to 45 days paid for vacations (including holidays and weekends). Private employees have since 14 paid days to 28 paid days (including holidays and weekends). In both cases is always relying on the years of service. The more years the worker has the more days of paid vacation will have.		Consecutive holidays refers to holidays that occur in a group without working days in between. In the late 1990s, the Japanese government passed a law that increased the likelihood of consecutive holidays by moving holidays from fixed days to a relative position in a month, such as the second Monday.		
A four-day week is an arrangement where a workplace or school has its employees or students work or attend school over the course of four days rather than the more customary five.[1] This arrangement can be a part of flexible working hours, and is sometimes used to cut costs.		In 2008, employees of the Utah state government all began working ten-hour days from Monday to Thursday.[1][2] By closing state government offices on Fridays, the state expected to save on operating costs such as electricity, heat, air conditioning, and gasoline for state-owned vehicles.[2] Utah ended this practice however, in 2011, with the Utah Legislature overriding Governor Gary Herbert's veto of five-day work week legislation.[3]		Many local governments have had alternative schedules for many years.[4][5][6]		Public schools in Hawaii closed on 17 Fridays in 2010.[7] Over 100 school districts in rural areas in the United States changed the school week to a four-day week; most also extended each school day by an hour or more.[8][9] The changes were often made in order to save money on transportation, heating, and substitute teachers.[8]		More modest attempts to enact a 32-hour workweek (a four-day week and an eight-hour day combined) have remained elusive in the following 80 years despite pockets of residual support.[10]		In Gambia, a four-day workweek was introduced for public officials by president Yahya Jammeh, effective as of 1 February 2013. Working hours became Mondays through Thursday 8 a.m. to 6 p.m., with Friday foreseen as a day of rest to allow residents more time for prayer and agriculture. This regulation was abolished in early 2017 by his successor, president Adama Barrow, who decreed a half-day of work on Fridays.[11]				
Termination of employment is an employee's departure from a job. Termination may be voluntary on the employee's part, or it may be at the hands of the employer, often in the form of dismissal (firing) or a layoff. Dismissal or firing is generally thought to be the fault of the employee, whereas a layoff is generally done for business reasons (for instance a business slowdown or an economic downturn) outside the employee's performance.		Firing carries a stigma in many cultures, and may hinder the jobseeker's chances of finding new employment, particularly if he or she has been sacked from previous jobs. Jobseekers sometimes do not mention jobs from which they were fired on their résumés; accordingly, unexplained gaps in employment, and refusal or failure to contact previous employers are often regarded as "red flags".[1]						Dismissal is when the employer chooses to require the employee to leave, generally for a reason which is the fault of the employee. The most common colloquial terms for dismissal in the United States are "getting fired" or "getting canned" whereas in the United Kingdom the terms "getting the sack" or "getting sacked" are also used.		A less severe form of involuntary termination is often referred to as a layoff (also redundancy or being made redundant in British English). A layoff is usually not strictly related to personal performance, but instead due to economic cycles or the company's need to restructure itself, the firm itself going out of business or a change in the function of the employer (for example, a certain type of product or service is no longer offered by the company and therefore jobs related to that product or service are no longer needed). One type of layoff is the aggressive layoff[citation needed]; in such a situation, the employee is laid off, but not replaced as the job is eliminated.		In an economy based on at-will employment, such as that of the United States, a large proportion of workers may be laid off at some time in their life, and often for reasons unrelated to performance or ethics.[citation needed] Employment termination can also result from a probational period, in which both the employee and the employer reach an agreement that the employer is allowed to lay off the employee if the probational period is not satisfied.		Often,[citation needed] layoffs occur as a result of "downsizing", "reduction in force" or "redundancy". These are not technically classified as firings; laid-off employees' positions are terminated and not refilled, because either the company wishes to reduce its size or operations or otherwise lacks the economic stability to retain the position. In some cases, a laid-off employee may eventually be offered their old position again by his/her respective company, though by this time he or she may have found a new job.		Some companies resort to attrition (voluntary redundancy) as a means to reduce their workforce.[2] Under such a plan, no employees are forced to leave their jobs. However, those who do depart voluntarily are not replaced. Additionally, employees are given the option to resign in exchange for a fixed amount of money, frequently a few years of their salary. Such plans have been carried out by the United States Federal Government under President Bill Clinton during the 1990s,[3] and by the Ford Motor Company in 2005.[4]		However, "layoff" may be specifically addressed and defined differently in the articles of a contract in the case of unionised work.		Some terminations occur as a result of mutual agreement between the employer and employee. When this happens, it is sometimes debatable if the termination was truly mutual. In many of these cases, it was originally the employer's wish for the employee to depart, but the employer offered the mutual termination agreement in order to soften the firing (as in a forced resignation). But there are also times when a termination date is agreed upon before the employment starts (as in an employment contract).		Some types of termination by mutual agreement include:		Firms that wish for an employee to exit of his or her own accord but do not wish to pursue firing or forced resignation, may degrade the employee's working conditions, hoping that he or she will leave "voluntarily".		The employee may be moved to a different geographical location, assigned to an undesirable shift, given too few hours if part time, demoted (or relegated to a menial task), or assigned to work in uncomfortable conditions. Other forms of manipulation may be used, such as being unfairly hostile to the employee, and punishing him or her for things that are deliberately overlooked with other employees.		Often, these tactics are done so that the employer won't have to fill out termination papers in jurisdictions without at-will employment. In addition, with a few exceptions, employees who voluntarily leave generally cannot collect unemployment benefits.		Such tactics may amount to constructive dismissal, which is illegal in some jurisdictions.		Depending on the circumstances, one whose employment has been terminated may or may not be able to be rehired by the same employer.		If the decision to terminate was the employee's, the willingness of the employer to rehire is often contingent upon the relationship the employee had with the employer, the amount of notice given by the employee prior to departure, and the needs of the employer. In some cases, when an employee departed on good terms, s/he may be given special priority by the employer when seeking rehire.		An employee who was fired by an employer may in some cases be eligible for rehire by that same employer, although in some cases it is usually related to staffing issues.		Employment can be terminated without prejudice, meaning the fired employee may be rehired readily for the same or a similar job in the future. This is usually true in the case of layoff.		Conversely, a person's employment can be terminated with prejudice, meaning an employer will not rehire the former employee to a similar job in the future. This can be for many reasons: incompetence, misconduct (such as dishonesty or "zero tolerance" violations), insubordination or "attitude" (personality clashes with peers or bosses).		Termination forms ("pink slips") routinely include a set of check boxes where a supervisor can indicate "with prejudice" or "without prejudice".		For example, public school teachers in New York who are laid off are placed on a Preferred Eligible List for employment in the school district where they were laid off for seven years from the date of layoff. If a teacher who was laid off applies to fill a job opening, he or she is given priority over other applicants.		
The following list provides information relating to the minimum wages of countries in Europe.		The calculations are based on the assumption of a 40-hour working week and a 52-week year, with the exceptions of France (35 hours),[1] San Marino (37.5 hours)[citation needed], Belgium (38 hours)[citation needed], United Kingdom (38.1 hours),[1] Ireland (39 hours),[1] Monaco (39 hours),[2] and Germany (39.1 hours).[1] Most of minimum wages are fixed at a monthly rate, but there are countries where minimum wage is fixed at an hourly rate or weekly rate.						Countries that have main territories in Asia with small or none territories in Europe, but considered European countries due to cultural and historical reasons.		The following list includes states with limited recognition.				
Overqualification is the state of being skilled or educated beyond what is necessary for a job. There can often be high costs for companies associated with training employees. This could be a problem for professionals applying for a job where they significantly exceed the job requirements because potential employers may feel they are using the position as a stepping stone.		In some societies, overqualification has become increasingly common as the proportion of college graduates in a population grows faster than the proportion of jobs in an economy which actually require college-level skills.[1]						The concept of overqualification is often a euphemism used by employers when they do not want to reveal their true reasons for not hiring an applicant. The term "overqualified" can mask age discrimination, but it can also mask legitimate concerns of an employer, such as uncertainty of an applicant's ability to do the job, or concerns that they only want a job on a temporary basis, while they seek another more desirable position.[2] Being overqualified also often means that a person was asking for too high a salary.[3][4] "Overqualified" can also be used to describe a resistance to new technologies, or a pompous approach.[4]		In the definition above, which states that an overqualified person may take a job to gain knowledge and leave the company, this could also apply to all other employees of the same company. The term overqualified, in any definition, should be considered as a subjective term developed by the person doing the evaluation of the applicant based upon their point of view which may in itself be biased. There comes a time in a person's life, when a choice is made to reduce the level of responsibility and one could consider the perceived over qualification as "added value" to the company when the applicant is willing to take a lower-level position, accompanied by a lower salary. When the decision is not based upon factual or unbiased factors, discrimination has occurred.		In the United States, the term "overqualified" has been found by the courts to sometimes be used as a "code word for too old" (i.e., age discrimination) in the hiring process.[5]		The governmental employing institution may have written or unwritten upper qualification limits for a particular position. These limits protect less qualified people like newly graduated students, allowing them to find a job as well. For instance, in countries like Germany or Switzerland, a paid position of a PhD student may normally not be given for an applicant who already has a PhD degree.		Also, a short but successful career may be preferred over longer (so more various different experience) but overall less successful career.		Noluthando Crockett-Ntonga recommends that job applicants address potential concerns such as salary requirements in a cover letter and interview before the employer makes any comments about overqualification.[4] Barbara Moses advises applicants who are described as being overqualified to emphasize their willingness to mentor younger co-workers, and to focus on what attracts them about the position they are applying to rather than emphasizing their ambition or desire to be challenged.[3] Being overqualified can be an asset for employers, especially when the breadth of one's experience enables them to take on additional responsibilities in ways that benefit the employer.[4]		The PhD can reflect overspecialization that manifests itself as a lack of perspective; for example, a PhD might not adequately prepare one for careers in development, manufacturing, or technical management.[6]		In the corporate world, some PhD graduates have been criticized as being unable to turn theories into useful strategies and being unable to work on a team, although PhDs are seen as desirable and even essential in many positions, such as supervisory roles in research, especially PhDs in biomedical sciences.[7]		Even in some college jobs, people can associate negative factors with the PhD, including a lack of focus on teaching, overspecialization, and an undesirable set of professional priorities, often focusing on self-promotion. These forces have led both to an increase in some educational institutions hiring candidates without PhDs as well as a focus on the development of other doctoral degrees, such as the D.A. or Doctor of Arts.[8]		Some employers have reservations about hiring people with PhDs in full-time, entry-level positions but are eager to hire them in temporary positions.[9]		Some argue that this reservation is rather a reaction associated with job insecurity, especially in situations where most of the company leaders hold lower qualifications than the PhD; as part of the wide phenomenon of credential creep.[10]		
Occupational safety and health (OSH), also commonly referred to as occupational health and safety (OHS), occupational health,[1] or workplace health and safety (WHS), is a multidisciplinary field concerned with the safety, health, and welfare of people at work. These terms of course also refer to the goals of this field,[2] so their use in the sense of this article was originally an abbreviation of occupational safety and health program/department etc.		The goals of occupational safety and health programs include to foster a safe and healthy work environment.[3] OSH may also protect co-workers, family members, employers, customers, and many others who might be affected by the workplace environment. In the United States, the term occupational health and safety is referred to as occupational health and occupational and non-occupational safety and includes safety for activities outside of work.[4]		In common-law jurisdictions, employers have a common law duty to take reasonable care of the safety of their employees.[5] Statute law may in addition impose other general duties, introduce specific duties, and create government bodies with powers to regulate workplace safety issues: details of this vary from jurisdiction to jurisdiction.						As defined by the World Health Organization (WHO) "occupational health deals with all aspects of health and safety in the workplace and has a strong focus on primary prevention of hazards."[6] Health has been defined as "a state of complete physical, mental and social well-being and not merely the absence of disease or infirmity."[7] Occupational health is a multidisciplinary field of healthcare concerned with enabling an individual to undertake their occupation, in the way that causes least harm to their health. Health has been defined as It contrasts, for example, with the promotion of health and safety at work, which is concerned with preventing harm from any incidental hazards, arising in the workplace.		Since 1950, the International Labour Organization (ILO) and the World Health Organization (WHO) have shared a common definition of occupational health. It was adopted by the Joint ILO/WHO Committee on Occupational Health at its first session in 1950 and revised at its twelfth session in 1995. The definition reads:		"The main focus in occupational health is on three different objectives: (i) the maintenance and promotion of workers’ health and working capacity; (ii) the improvement of working environment and work to become conducive to safety and health and (iii) development of work organizations and working cultures in a direction which supports health and safety at work and in doing so also promotes a positive social climate and smooth operation and may enhance productivity of the undertakings. The concept of working culture is intended in this context to mean a reflection of the essential value systems adopted by the undertaking concerned. Such a culture is reflected in practice in the managerial systems, personnel policy, principles for participation, training policies and quality management of the undertaking."		Those in the field of occupational health come from a wide range of disciplines and professions including medicine, psychology, epidemiology, physiotherapy and rehabilitation, occupational therapy, occupational medicine, human factors and ergonomics, and many others. Professionals advise on a broad range of occupational health matters. These include how to avoid particular pre-existing conditions causing a problem in the occupation, correct posture for the work, frequency of rest breaks, preventative action that can be undertaken, and so forth.		"Occupational health should aim at: the promotion and maintenance of the highest degree of physical, mental and social well-being of workers in all occupations; the prevention amongst workers of departures from health caused by their working conditions; the protection of workers in their employment from risks resulting from factors adverse to health; the placing and maintenance of the worker in an occupational environment adapted to his physiological and psychological capabilities; and, to summarize, the adaptation of work to man and of each man to his job.		The research and regulation of occupational safety and health are a relatively recent phenomenon. As labor movements arose in response to worker concerns in the wake of the industrial revolution, worker's health entered consideration as a labor-related issue.		In the United Kingdom, the Factory Acts of the early nineteenth century (from 1802 onwards) arose out of concerns about the poor health of children working in cotton mills: the Act of 1833 created a dedicated professional Factory Inspectorate.[9] :41 The initial remit of the Inspectorate was to police restrictions on the working hours in the textile industry of children and young persons (introduced to prevent chronic overwork, identified as leading directly to ill-health and deformation, and indirectly to a high accident rate). However, on the urging of the Factory Inspectorate, a further Act in 1844 giving similar restrictions on working hours for women in the textile industry introduced a requirement for machinery guarding (but only in the textile industry, and only in areas that might be accessed by women or children).[9] :85		In 1840 a Royal Commission published its findings on the state of conditions for the workers of the mining industry that documented the appallingly dangerous environment that they had to work in and the high frequency of accidents. The commission sparked public outrage which resulted in the Mines Act of 1842. The act set up an inspectorate for mines and collieries which resulted in many prosecutions and safety improvements, and by 1850, inspectors were able to enter and inspect premises at their discretion.[10]		Otto von Bismarck inaugurated the first social insurance legislation in 1883 and the first worker's compensation law in 1884 – the first of their kind in the Western world. Similar acts followed in other countries, partly in response to labor unrest.[11]		Although work provides many economic and other benefits, a wide array of workplace hazards also present risks to the health and safety of people at work. These include but are not limited to, "chemicals, biological agents, physical factors, adverse ergonomic conditions, allergens, a complex network of safety risks," and a broad range of psychosocial risk factors.[12] Personal protective equipment can help protect against many of these hazards.		Physical hazards affect many people in the workplace. Occupational hearing loss is the most common work-related injury in the United States, with 22 million workers exposed to hazardous noise levels at work and an estimated $242 million spent annually on worker's compensation for hearing loss disability.[13] Falls are also a common cause of occupational injuries and fatalities, especially in construction, extraction, transportation, healthcare, and building cleaning and maintenance.[14] Machines have moving parts, sharp edges, hot surfaces and other hazards with the potential to crush, burn, cut, shear, stab or otherwise strike or wound workers if used unsafely.[15]		Biological hazards (biohazards) include infectious microorganisms such as viruses and toxins produced by those organisms such as anthrax. Biohazards affect workers in many industries; influenza, for example, affects a broad population of workers.[16] Outdoor workers, including farmers, landscapers, and construction workers, risk exposure to numerous biohazards, including animal bites and stings,[17][18][19] urushiol from poisonous plants,[20] and diseases transmitted through animals such as the West Nile virus and Lyme disease.[21][22] Health care workers, including veterinary health workers, risk exposure to blood-borne pathogens and various infectious diseases,[23][24] especially those that are emerging.[25]		Dangerous chemicals can pose a chemical hazard in the workplace. There are many classifications of hazardous chemicals, including neurotoxins, immune agents, dermatologic agents, carcinogens, reproductive toxins, systemic toxins, asthmagens, pneumoconiotic agents, and sensitizers.[26] Authorities such as regulatory agencies set occupational exposure limits to mitigate the risk of chemical hazards.[27] An international effort is investigating the health effects of mixtures of chemicals. There is some evidence that certain chemicals are harmful at lower levels when mixed with one or more other chemicals. This may be particularly important in causing cancer.[28]		Psychosocial hazards include risks to the mental and emotional well-being of workers, such as feelings of job insecurity, long work hours, and poor work-life balance.[29] A recent Cochrane review - using moderate quality evidence - related that the addition of work-directed interventions for depressed workers receiving clinical interventions reduces the number of lost work days as compared to clinical interventions alone.[30] This review also demonstrated that the addition of cognitive behavioral therapy to primary or occupational care and the addition of a "structured telephone outreach and care management program" to usual care are both effective at reducing sick leave days.[30]		Specific occupational safety and health risk factors vary depending on the specific sector and industry. Construction workers might be particularly at risk of falls, for instance, whereas fishermen might be particularly at risk of drowning. The United States Bureau of Labor Statistics identifies the fishing, aviation, lumber, metalworking, agriculture, mining and transportation industries as among some of the more dangerous for workers.[31] Similarly psychosocial risks such as workplace violence are more pronounced for certain occupational groups such as health care employees, police, correctional officers and teachers.[32]		Construction is one of the most dangerous occupations in the world, incurring more occupational fatalities than any other sector in both the United States and in the European Union.[33][34] In 2009, the fatal occupational injury rate among construction workers in the United States was nearly three times that for all workers.[33] Falls are one of the most common causes of fatal and non-fatal injuries among construction workers.[33] Proper safety equipment such as harnesses and guardrails and procedures such as securing ladders and inspecting scaffolding can curtail the risk of occupational injuries in the construction industry.[35] Due to the fact that accidents may have disastrous consequences for employees as well as organizations, it is of utmost importance to ensure health and safety of workers and compliance with HSE construction requirements. Health and safety legislation in the construction industry involves many rules and regulations. For example, the role of the Construction Design Management (CDM) Coordinator as a requirement has been aimed at improving health and safety on-site.[36]		The 2010 National Health Interview Survey Occupational Health Supplement (NHIS-OHS) identified work organization factors and occupational psychosocial and chemical/physical exposures which may increase some health risks. Among all U.S. workers in the construction sector, 44% had non-standard work arrangements (were not regular permanent employees) compared to 19% of all U.S. workers, 15% had temporary employment compared to 7% of all U.S. workers, and 55% experienced job insecurity compared to 32% of all U.S. workers. Prevalence rates for exposure to physical/chemical hazards were especially high for the construction sector. Among nonsmoking workers, 24% of construction workers were exposed to secondhand smoke while only 10% of all U.S. workers were exposed. Other physical/chemical hazards with high prevalence rates in the construction industry were frequently working outdoors (73%) and frequent exposure to vapors, gas, dust, or fumes (51%).[37]		Agriculture workers are often at risk of work-related injuries, lung disease, noise-induced hearing loss, skin disease, as well as certain cancers related to chemical use or prolonged sun exposure. On industrialized farms, injuries frequently involve the use of agricultural machinery. The most common cause of fatal agricultural injuries in the United States is tractor rollovers, which can be prevented by the use of roll over protection structures which limit the risk of injury in case a tractor rolls over.[38] Pesticides and other chemicals used in farming can also be hazardous to worker health, and workers exposed to pesticides may experience illnesses or birth defects.[39] As an industry in which families, including children, commonly work alongside their families, agriculture is a common source of occupational injuries and illnesses among younger workers.[40] Common causes of fatal injuries among young farm worker include drowning, machinery and motor vehicle-related accidents.[41]		The 2010 NHIS-OHS found elevated prevalence rates of several occupational exposures in the agriculture, forestry, and fishing sector which may negatively impact health. These workers often worked long hours. The prevalence rate of working more than 48 hours a week among workers employed in these industries was 37%, and 24% worked more than 60 hours a week. Of all workers in these industries, 85% frequently worked outdoors compared to 25% of all U.S. workers. Additionally, 53% were frequently exposed to vapors, gas, dust, or fumes, compared to 25% of all U.S. workers.[42]		As the number of service sector jobs has risen in developed countries, more and more jobs have become sedentary, presenting a different array of health problems than those associated with manufacturing and the primary sector. Contemporary problems such as the growing rate of obesity and issues relating to occupational stress, workplace bullying, and overwork in many countries have further complicated the interaction between work and health.		According to data from the 2010 NHIS-OHS, hazardous physical/chemical exposures in the service sector were lower than national averages. On the other hand, potentially harmful work organization characteristics and psychosocial workplace exposures were relatively common in this sector. Among all workers in the service industry, 30% experienced job insecurity in 2010, 27% worked non-standard shifts (not a regular day shift), 21% had non-standard work arrangements (were not regular permanent employees).[43]		Due to the manual labour involved and on a per employee basis, the US Postal Service, UPS and FedEx are the 4th, 5th and 7th most dangerous companies to work for in the US.[44]		According to data from the 2010 NHIS-OHS, workers employed in mining and oil and gas extraction industries had high prevalence rates of exposure to potentially harmful work organization characteristics and hazardous chemicals. Many of these workers worked long hours: 50% worked more than 48 hours a week and 25% worked more than 60 hours a week in 2010. Additionally, 42% worked non-standard shifts (not a regular day shift). These workers also had high prevalence of exposure to physical/chemical hazards. In 2010, 39% had frequent skin contact with chemicals. Among nonsmoking workers, 28% of those in mining and oil and gas extraction industries had frequent exposure to secondhand smoke at work. About two-thirds were frequently exposed to vapors, gas, dust, or fumes at work.[45]		Healthcare workers are exposed to many hazards that can adversely affect their health and well-being. Long hours, changing shifts, physically demanding tasks, violence, and exposures to infectious diseases and harmful chemicals are examples of hazards that put these workers at risk for illness and injury.		According to the Bureau of Labor statistics, U.S. hospitals recorded 253,700 work-related injuries and illnesses in 2011, which is 6.8 work-related injuries and illnesses for every 100 full-time employees.[46] The injury and illness rate in hospitals is higher than the rates in construction and manufacturing – two industries that are traditionally thought to be relatively hazardous.		The Occupational Health Safety Network (OHSN)[47] is a secure electronic surveillance system developed by the National Institute for Occupational Safety and Health (NIOSH) to address health and safety risks among health care personnel. OHSN uses existing data to characterize risk of injury and illness among health care workers. Hospitals and other healthcare facilities can upload the occupational injury data they already collect to the secure database for analysis and benchmarking with other de-identified facilities. NIOSH works with OHSN participants in identifying and implementing timely and targeted interventions. OHSN modules currently focus on three high risk and preventable events that can lead to injuries or musculoskeletal disorders among healthcare personnel: musculoskeletal injuries from patient handling activities;[48] slips, trips, and falls; and workplace violence. OHSN enrollment is open to all healthcare facilities.		The Bureau of Labor Statistics of the United States Department of Labor compiles information about workplace fatalities and non-fatal injuries in the United States. In 1970, an estimated 14,000 workers were killed on the job – by 2010, the workforce had doubled, but workplace deaths were down to about 4,500.[49] Between 1913 and 2013, workplace fatalities dropped by approximately 80%.[50]		The Bureau also compiles information about the most dangerous jobs. According to the census of occupational injuries 4,679 people died on the job in 2014.[51] In 2015, a decline in nonfatal workplace injuries and illnesses was observed, with private industry employers reporting approximately 2.9 million incidents, nearly 48,000 fewer cases than in 2014.[52]		Musculoskeletal injuries accounted for 32% of all employer-reported injuries and illnesses in 2014.[55]		In most countries males comprise the vast majority of workplace fatalities. In the EU as a whole, 94% of death were of males.[56] In the UK the disparity was even greater with males comprising 97.4% of workplace deaths. In the UK there were 171 fatal injuries at work in financial year 2011–2012, compared with 651 in calendar year 1974; the fatal injury rate declined over that period from 2.9 fatalities per 100,000 workers to 0.6 per 100,000 workers[57]		In 2001, the International Labour Organization (ILO) published ILO-OSH 2001, also titled "Guidelines a on occupational safety and health management systems" to assist organizations with introducing OSH management systems.[58] These guidelines encourage continual improvement in employee health and safety, achieved via a constant process of policy, organization, planning & implementation, evaluation, and action for improvement, all supported by constant auditing to determine the success of OSH actions.[58]		The ILO management system was created to assist employers to keep pace with rapidly shifting and competitive industrial environments. The ILO recognizes that national legislation is essential, but sometimes insufficient on its own to address the challenges faced by industry, and therefore elected to ensure free and open distribution of administrative tools in the form of occupational health and safety management system guidance for everyone. This open access forum is intended to provide the tools for industry to create safe and healthy working environments and foster positive safety cultures within the organizations.[citation needed]		OHSAS 18000 is an international occupational health and safety management system specification developed by the London-based BSI Group, a multinational business chiefly concerned with the production and distribution of standards related services. OHSAS 18000 comprises two parts, OHSAS 18001 and 18002 and embraces a number of other publications. OHSAS 18000 is the internationally recognized assessment specification for occupational health and safety management systems. It was developed by a selection of leading trade bodies, international standards and certification bodies to address a gap where no third-party certifiable international standard exists. This internationally recognized specification for occupational health and safety management system operates on the basis of policy, planning, implementation and operation, checking and corrective action, management review, and continual improvement.[citation needed]		The British Standards – Occupational Health and Safety management Systems Requirements Standard BS OHSAS 18001 was developed within the framework of the ISO standards series. Allowing it to integrate better into the larger system of ISO certifications. ISO 9001 Quality Management Systems and ISO 14001 Environmental Management System can work in tandem with BS OHSAS 18001/18002 to complement each other and form a better overall system. Each component of the system is specific, auditable, and accreditable by a third party after review.[59]		Also Standards Australia and the Association Française de Normalisation (AFNOR) in France have developed occupational safety and health management standards.[60]		Guidance note HSG65: Successful Health and Safety Management, published by the British non-departmental public body Health and Safety Executive, was substantially re-written 2013. It now promotes the Plan Do Check Act approach to health and safety management, sharing similarities with BS OHSAS 18001. This achieved a balance between the original systems-based approach, and the more modern behavioural approach to safety management.[61]		Occupational safety and health practice vary among nations with different approaches to legislation, regulation, enforcement, and incentives for compliance. In the EU, for example, some member states promote OSH by providing public monies as subsidies, grants or financing, while others have created tax system incentives for OSH investments. A third group of EU member states has experimented with using workplace accident insurance premium discounts for companies or organisations with strong OSH records.[62]		In Australia, the Commonwealth, four of the six states and both territories have enacted and administer harmonised Work Health and Safety Legislation in accordance with the Intergovernmental Agreement for Regulatory and Operational Reform in Occupational Health and Safety.[63] Each of these jurisdictions has enacted Work Health & Safety legislation and regulations based on the Commonwealth Work Health and Safety Act 2011 and common Codes of Practice developed by Safe Work Australia.[64] Some jurisdictions have also included mine safety under the model approach, however, most have retained separate legislation for the time being. Western Australia intends to adopt a moderated version of the model approach and Victoria has retained its own regime, although the Model WHS laws themselves drew heavily on the Victoria approach.		In Canada, workers are covered by provincial or federal labour codes depending on the sector in which they work. Workers covered by federal legislation (including those in mining, transportation, and federal employment) are covered by the Canada Labour Code; all other workers are covered by the health and safety legislation of the province in which they work. The Canadian Centre for Occupational Health and Safety (CCOHS), an agency of the Government of Canada, was created in 1966 by an Act of Parliament. The act was based on the belief that all Canadians had "...a fundamental right to a healthy and safe working environment." CCOHS is mandated to promote safe and healthy workplaces to help prevent work-related injuries and illnesses. The CCOHS maintains a useful (partial) list of OSH regulations for Canada and its provinces.[65]		per 100,000 full-time employees[66]		In the European Union, member states have enforcing authorities to ensure that the basic legal requirements relating to occupational health and safety are met. In many EU countries, there is strong cooperation between employer and worker organisations (e.g. unions) to ensure good OSH performance as it is recognized this has benefits for both the worker (through maintenance of health) and the enterprise (through improved productivity and quality). In 1996, the European Agency for Safety and Health at Work was founded.		Member states of the European Union have all transposed into their national legislation a series of directives that establish minimum standards on occupational health and safety. These directives (of which there are about 20 on a variety of topics) follow a similar structure requiring the employer to assess the workplace risks and put in place preventive measures based on a hierarchy of control. This hierarchy starts with elimination of the hazard and ends with personal protective equipment.		However, certain EU member states admit to having lacking quality control in occupational safety services, to situations in which risk analysis takes place without any on-site workplace visits and to insufficient implementation of certain EU OSH directives. Based on this, it is hardly surprising that the total societal costs of work-related health problems and accidents vary from 2.6% to 3.8% of GNP between the EU member states.[67]		In Denmark, occupational safety and health is regulated by the Danish Act on Working Environment and cooperation at the workplace.[68] The Danish Working Environment Authority carries out inspections of companies, draws up more detailed rules on health and safety at work and provides information on health and safety at work.[69] The result of each inspection is made public on the web pages of the Danish Working Environment Authority so that the general public, current and prospective employees, customers and other stakeholders can inform themselves about whether a given organization has passed the inspection, should they wish to do so.[70]		In Spain, occupational safety and health is regulated by the Spanish Act on Prevention of Labour Risks. The Ministry of Employment and Social Security is the authority responsible for issues relating to labour environment.[71] The National Institute for Labour Safety and Hygiene is the technical public Organization specialized in occupational safety and health.[72]		In Sweden, occupational safety and health is regulated by the Work Environment Act. The Swedish Work Environment Authority is the government agency responsible for issues relating to the working environment. The agency should work to disseminate information and furnish advice on OSH, has a mandate to carry out inspections, and a right to issue stipulations and injunctions to any non-compliant employer.[73]		In the UK, health and safety legislation is drawn up and enforced by the Health and Safety Executive and local authorities (the local council) under the Health and Safety at Work etc. Act 1974[74][75] (HASAWA). HASAWA introduced (section 2) a general duty on an employer to ensure, so far as is reasonably practicable, the health, safety and welfare at work of all his employees; with the intention of giving a legal framework supporting 'codes of practice' not in themselves having legal force but establishing a strong presumption as to what was reasonably practicable (deviations from them could be justified by appropriate risk assessment). The previous reliance on detailed prescriptive rule-setting was seen as having failed to respond rapidly enough to technological change, leaving new technologies potentially un-regulated or inappropriately regulated.[76][77] HSE has continued to make some regulations giving absolute duties (where something must be done with no 'reasonable practicability' test) but in the UK the regulatory trend is away from prescriptive rules, and towards 'goal setting' and risk assessment. Recent major changes to the laws governing asbestos and fire safety management embrace the concept of risk assessment. The other key aspect of the UK legislation is a statutory mechanism for worker involvement through elected health and safety representatives and health and safety committees. This followed a similar approach in Scandinavia, and that approach has since been adopted in Australia, Canada, New Zealand and Malaysia, for example.		For the UK, the government organisation dealing with occupational health has been the Employment Medical Advisory Service but in 2014 a new occupational health organisation - the Health and Work Service - was created to provide advice and assistance to employers in order to get back to work employees on long-term sick-leave.[78] The service, funded by government, will offer medical assessments and treatment plans, on a voluntary basis, to people on long-term absence from their employer; in return, the government will no longer foot the bill for Statutory Sick Pay provided by the employer to the individual.		In India, the Labour Ministry formulates national policies on occupational safety and health in factories and docks with advice and assistance from Directorate General of Factory Advice Service and Labour Institutes (DGFASLI), and enforces its Policies through inspectorates of factories and inspectorates of dock safety.[79] DGFASLI is the technical arm of the Ministry of Labour & Employment, Government of India and advises the factories on various problems concerning safety, health, efficiency and well - being of the persons at work places.[79] The DGFASLI provides technical support in formulating rules, conducting occupational safety surveys and also for conducting occupational safety training programs.[80]		In Malaysia, the Department of Occupational Safety and Health (DOSH) under the Ministry of Human Resource is responsible to ensure that the safety, health and welfare of workers in both the public and private sector is upheld. DOSH is responsible to enforce the Factories and Machinery Act 1967 and the Occupational Safety and Health Act 1994.		In the People's Republic of China, the Ministry of Health is responsible for occupational disease prevention and the State Administration of Work Safety for safety issues at work. On the provincial and municipal level, there are Health Supervisions for occupational health and local bureaus of Work Safety for safety. The "Occupational Disease Control Act of PRC" came into force on May 1, 2002.[81] and Work safety Act of PRC on November 1, 2002.[82] The Occupational Disease Control Act is under revision. The prevention of occupational disease is still in its initial stage compared with industrialised countries such as the US or UK.		In Singapore, the Ministry of Manpower operates various checks and campaigns against unsafe work practices, such as when working at height, operating cranes and in traffic management. Examples include Operation Cormorant and the Falls Prevention Campaign.[83]		In South Africa the Department of Labour is responsible for occupational health and safety inspection and enforcement in commerce and industry apart from mining and energy production, where the Department of Mineral Resources is responsible.		The main statutory legislation on Health and Safety in the jurisdiction of the Department of Labour is Act No. 85 of 1993: Occupational Health and Safety Act as amended by Occupational Health and Safety Amendment Act, No. 181 Of 1993.		Regulations to the OHS Act include:		In Taiwan, the Occupational Safety and Health Administration of the Ministry of Labor is in charge of occupational safety and health.[92] The matter is governed under the Occupational Safety and Health Act.[93]		2007, official release the document TOSHMS (Taiwan Occupational Safety and Health Management System), which defined the basic rule about occupational safety standard.[94]		In the United States, President Richard Nixon signed the Occupational Safety and Health Act into law on December 29, 1970. The act created the three agencies that administer it. They include the Occupational Safety and Health Administration, National Institute for Occupational Safety and Health, and the Occupational Safety and Health Review Commission.[95] The act authorized the Occupational Safety and Health Administration (OSHA) to regulate private employers in the 50 states, the District of Columbia, and territories.[96] The Act establishing it includes a general duty clause (29 U.S.C. § 654, 5(a)) requiring an employer to comply with the Act and regulations derived from it, and to provide employees with "employment and a place of employment which are free from recognized hazards that are causing or are likely to cause death or serious physical harm to his employees."		OSHA was established in 1971 under the Department of Labor. It has headquarters in Washington, DC and ten regional offices, further broken down into districts, each organized into three sections; compliance, training, and assistance. Its stated mission is to assure safe and healthful working conditions for working men and women by setting and enforcing standards and by providing training, outreach, education and assistance.[97] The original plan was for OSHA to oversee 50 state plans with OSHA funding 50% of each plan. Unfortunately it has not worked out that way. There are currently 26 approved state plans (4 cover only public employees)[49] and no other states want to participate. OSHA manages the plan in the states not participating.[49]		OSHA develops safety standards in the Code of Federal Regulation and enforces those safety standards through compliance inspections conducted by Compliance Officers; enforcement resources are focussed on high-hazard industries. Worksites may apply to enter OSHA's Voluntary Protection Program (VPP); a successful application leads to an on-site inspection ; if this is passed the site gains VPP status and OSHA no longer inspect it annually nor (normally) visit it unless there is a fatal accident or an employee complaint until VPP revalidation (after 3–5 years)[49](VPP sites have injury and illness rates less than half the average for their industry).		It has 73 specialists in local offices to provide tailored information and training to employers and employees at little or no cost[4] Similarly OSHA produces a range of publications, provides advice to employers and funds consultation services available for small businesses.		OSHA's Alliance Program enables groups committed to worker safety and health to work with it to develop compliance assistance tools and resources, share information with workers and employers, and educate them about their rights and responsibilities. OSHA also has a Strategic Partnership Program that zeros in on specific hazards or specific geographic areas.[49] OSHA manages Susan B. Harwood grants to nonprofit companies to train workers and employers to recognize, avoid, and prevent safety and health hazards in the workplace. Grants focus on small business, hard-to-reach workers and high-hazard industries.		The National Institute for Occupational Safety and Health, created under the same act, works closely with OSHA and provides the research behind many of OSHA's regulations and standards.[98]		The roles and responsibilities of OSH professionals vary regionally, but may include evaluating working environments, developing, endorsing and encouraging measures that might prevent injuries and illnesses, providing OSH information to employers, employees, and the public, providing medical examinations, and assessing the success of worker health programs.		In Norway, the main required tasks of an occupational health and safety practitioner include the following:		In the Netherlands, the required tasks for health and safety staff are only summarily defined and include the following:		‘The main influence of the Dutch law on the job of the safety professional is through the requirement on each employer to use the services of a certified working conditions service to advise them on health and safety’.[100] A ‘certified service’ must employ sufficient numbers of four types of certified experts to cover the risks in the organisations which use the service:		In 2004, 37% of health and safety practitioners in Norway and 14% in the Netherlands had an MSc; 44% had a BSc in Norway and 63% in the Netherlands; and 19% had training as an OSH technician in Norway and 23% in the Netherlands.[100]		The main tasks undertaken by the OHS practitioner in the US include:		Knowledge required by the OHS professional in the US include:		Some skills required by the OHS professional in the US include (but are not limited to):		Because different countries take different approaches to ensuring occupational safety and health, areas of OSH need and focus also vary between countries and regions. Similar to the findings of the ENHSPO survey conducted in Australia, the Institute of Occupational Medicine in the UK found that there is a need to put greater emphasis on work-related illness in the UK.[103] In contrast, in Australia and the US, a major responsibility of the OHS professional is to keep company directors and managers aware of the issues that they face in regards to occupational health and safety principles and legislation. However, in some other areas of Europe, it is precisely this which has been lacking: “Nearly half of senior managers and company directors do not have an up-to-date understanding of their health and safety-related duties and responsibilities.”[104]		The terminology used in OSH varies between countries, but generally speaking:		“Hazard”, “risk”, and “outcome” are used in other fields to describe e.g. environmental damage, or damage to equipment. However, in the context of OSH, “harm” generally describes the direct or indirect degradation, temporary or permanent, of the physical, mental, or social well-being of workers. For example, repetitively carrying out manual handling of heavy objects is a hazard. The outcome could be a musculoskeletal disorder (MSD) or an acute back or joint injury. The risk can be expressed numerically (e.g. a 0.5 or 50/50 chance of the outcome occurring during a year), in relative terms (e.g. "high/medium/low"), or with a multi-dimensional classification scheme (e.g. situation-specific risks).[citation needed]		Hazard identification or assessment is an important step in the overall risk assessment and risk management process. It is where individual work hazards are identified, assessed and controlled/eliminated as close to source (location of the hazard) as reasonably as possible. As technology, resources, social expectation or regulatory requirements change, hazard analysis focuses controls more closely toward the source of the hazard. Thus hazard control is a dynamic program of prevention. Hazard-based programs also have the advantage of not assigning or implying there are "acceptable risks" in the workplace. A hazard-based program may not be able to eliminate all risks, but neither does it accept "satisfactory" – but still risky – outcomes. And as those who calculate and manage the risk are usually managers while those exposed to the risks are a different group, workers, a hazard-based approach can by-pass conflict inherent in a risk-based approach.[citation needed]		The information that needs to be gathered from sources should apply to the specific type of work from which the hazards can come from. As mentioned previously, examples of these sources include interviews with people who have worked in the field of the hazard, history and analysis of past incidents, and official reports of work and the hazards encountered. Of these, the personnel interviews may be the most critical in identifying undocumented practices, events, releases, hazards and other relevant information. Once the information is gathered from a collection of sources, it is recommended for these to be digitally archived (to allow for quick searching) and to have a physical set of the same information in order for it to be more accessible. One innovative way to display the complex historical hazard information is with a historical hazards identification map, which distills the hazard information into an easy to use graphical format.		Modern occupational safety and health legislation usually demands that a risk assessment be carried out prior to making an intervention. It should be kept in mind that risk management requires risk to be managed to a level which is as low as is reasonably practical.[citation needed]		This assessment should:		The calculation of risk is based on the likelihood or probability of the harm being realized and the severity of the consequences. This can be expressed mathematically as a quantitative assessment (by assigning low, medium and high likelihood and severity with integers and multiplying them to obtain a risk factor), or qualitatively as a description of the circumstances by which the harm could arise.[citation needed]		The assessment should be recorded and reviewed periodically and whenever there is a significant change to work practices. The assessment should include practical recommendations to control the risk. Once recommended controls are implemented, the risk should be re-calculated to determine if it has been lowered to an acceptable level. Generally speaking, newly introduced controls should lower risk by one level, i.e., from high to medium or from medium to low.[106]		On an international scale, the World Health Organization (WHO) and the International Labour Organization (ILO) have begun focusing on labour environments in developing nations with projects such as Healthy Cities.[107] Many of these developing countries are stuck in a situation in which their relative lack of resources to invest in OSH leads to increased costs due to work-related illnesses and accidents. As a 2007 Factsheet from the European Agency for Safety and Health at Work states: "Countries with less developed OSH systems spend a far higher percentage of GDP on work-related injury and illness – taking resources away from more productive activities ... The ILO estimates that work-related illness and accidents cost up to 10% of GDP in Latin America, compared with just 2.6% to 3.8% in the EU."[108] There is continued use of asbestos, a notorious hazard, in some developing countries. So asbestos-related disease is, sadly, expected to continue to be a significant problem well into the future.		Nanotechnology is an example of a new, relatively unstudied technology. A Swiss survey of one hundred thirty eight companies using or producing nanoparticulate matter in 2006 resulted in forty completed questionnaires. Sixty five per cent of respondent companies stated they did not have a formal risk assessment process for dealing with nanoparticulate matter.[109] Nanotechnology already presents new issues for OSH professionals that will only become more difficult as nanostructures become more complex. The size of the particles renders most containment and personal protective equipment ineffective. The toxicology values for macro sized industrial substances are rendered inaccurate due to the unique nature of nanoparticulate matter. As nanoparticulate matter decreases in size its relative surface area increases dramatically, increasing any catalytic effect or chemical reactivity substantially versus the known value for the macro substance. This presents a new set of challenges in the near future to rethink contemporary measures to safeguard the health and welfare of employees against a nanoparticulate substance that most conventional controls have not been designed to manage.[110]		Occupational health disparities refer to differences in occupational injuries and illnesses that are closely linked with demographic, social, cultural, economic, and/or political factors.[111]		There are multiple levels of training applicable to the field of occupational safety and health (OSH). Programs range from individual non-credit certificates, focusing on specific areas of concern, to full doctoral programs. The University of Southern California was one of the first schools in the US to offer a Ph.D. program focusing on the field. Further, multiple master's degree programs exist, such as that of the Indiana State University who offer a master of science (MS) and a master of arts (MA) in OSH. Graduate programs are designed to train educators, as well as, high-level practitioners. Many OSH generalists focus on undergraduate studies; programs within schools, such as that of the University of North Carolina's online Bachelor of Science in Environmental Health and Safety, fill a large majority of hygienist needs. However, smaller companies often don’t have full-time safety specialists on staff, thus, they appoint a current employee to the responsibility. Individuals finding themselves in positions such as these, or for those enhancing marketability in the job-search and promotion arena, may seek out a credit certificate program. For example, the University of Connecticut's online OSH Certificate,[112] provides students familiarity with overarching concepts through a 15-credit (5-course) program. Programs such as these are often adequate tools in building a strong educational platform for new safety managers with a minimal outlay of time and money. Further, most hygienists seek certification by organizations which train in specific areas of concentration, focusing on isolated workplace hazards. The American Society for Safety Engineers (ASSE), American Board of Industrial Hygiene (ABIH), and American Industrial Hygiene Association (AIHA) offer individual certificates on many different subjects from forklift operation to waste disposal and are the chief facilitators of continuing education in the OSH sector. In the U.S. the training of safety professionals is supported by National Institute for Occupational Safety and Health through their NIOSH Education and Research Centers. In Australia, training in OSH is available at the vocational education and training level, and at university undergraduate and postgraduate level. Such university courses may be accredited by an Accreditation Board of the Safety Institute of Australia. The Institute has produced a Body of Knowledge[113] which it considers is required by a generalist safety and health professional, and offers a professional qualification based on a four-step assessment.		On April 28 The International Labour Organisation celebrates "World Day for Safety and Health"[114] to raise awareness of safety in the workplace. Occurring annually since 2003,[115] each year it focuses on a specific area and bases a campaign around the theme.[116]		
Income bracket is the bandwidth from a basic wage towards all possible salary components and is used to give employees a career perspective and to give the employer the possibility to reward achievements.		In governmental terms, entire populations are divided into income brackets. These brackets are used to categorize demographic data as well as determine levels of taxation and benefits.		
A minimum wage is the lowest remuneration that employers can legally pay their workers. Equivalently, it is the price floor below which workers may not sell their labor. Although minimum wage laws are in effect in many jurisdictions, differences of opinion exist about the benefits and drawbacks of a minimum wage. Supporters of the minimum wage say it increases the standard of living of workers, reduces poverty, reduces inequality, and boosts morale. In contrast, opponents of the minimum wage say it increases poverty, increases unemployment (particularly among unskilled or inexperienced workers) and is damaging to businesses, because excessively high minimum wages require businesses to raise the prices of their product or service to accommodate the extra expense of paying a higher wage.[1][2][3]		Modern minimum wage laws were first passed in New Zealand and Australia in the 1890s. The movement for minimum wages was first motivated as a way to stop the exploitation of workers in sweatshops, by employers who were thought to have unfair bargaining power over them. Over time, minimum wages came to be seen as a way to help lower-income families. Most countries had introduced minimum wage legislation by the end of the 20th century.[4]						Modern minimum wage laws trace their origin to the Ordinance of Labourers (1349), which was a decree by King Edward III that set a maximum wage for laborers in medieval England.[5][6] King Edward III, who was a wealthy landowner, was dependent, like his lords, on serfs to work the land. In the autumn of 1348, the Black Plague reached England and decimated the population.[7] The severe shortage of labor caused wages to soar and encouraged King Edward III to set a wage ceiling. Subsequent amendments to the ordinance, such as the Statute of Labourers (1351), increased the penalties for paying a wage above the set rates.[5]		While the laws governing wages initially set a ceiling on compensation, they were eventually used to set a living wage. An amendment to the Statute of Labourers in 1389 effectively fixed wages to the price of food. As time passed, the Justice of the Peace, who was charged with setting the maximum wage, also began to set formal minimum wages. The practice was eventually formalized with the passage of the Act Fixing a Minimum Wage in 1604 by King James I for workers in the textile industry.[5]		By the early 19th century, the Statutes of Labourers was repealed as increasingly capitalistic England embraced laissez-faire policies which disfavored regulations of wages (whether upper or lower limits).[5] The subsequent 19th century saw significant labor unrest affect many industrial nations. As trade unions were decriminalized during the century, attempts to control wages through collective agreement were made. However, this meant that a uniform minimum wage was not possible. In Principles of Political Economy in 1848, John Stuart Mill argued that because of the collective action problems that workers faced in organisation, it was a justified departure from laissez-faire policies (or freedom of contract) to regulate people's wages and hours by law.		It was not until the 1890s that the first modern legislative attempts to regulate minimum wages were seen in New Zealand and Australia.[8] The movement for a minimum wage was initially focused on stopping sweatshop labor and controlling the proliferation of sweatshops in manufacturing industries.[9] The sweatshops employed large numbers of women and young workers, paying them what were considered to be substandard wages. The sweatshop owners were thought to have unfair bargaining power over their employees, and a minimum wage was proposed as a means to make them pay fairly. Over time, the focus changed to helping people, especially families, become more self-sufficient.[10]		The first modern national minimum wage law was enacted by the government of New Zealand in 1894, followed by Australia in 1896 and the United Kingdom in 1909.[8] In the United States, statutory minimum wages were first introduced nationally in 1938,[13] and they were reintroduced and expanded in the United Kingdom in 1998.[14] There is now legislation or binding collective bargaining regarding minimum wage in more than 90 percent of all countries.[15][4] In the European Union, 22 member states out of 28 currently have national minimum wages.[16] Other countries, such as Sweden, Finland, Denmark, Switzerland, Austria, and Italy, have no minimum wage laws, but rely on employer groups and trade unions to set minimum earnings through collective bargaining.[17][18]		Minimum wage rates vary greatly across many different jurisdictions, not only in setting a particular amount of money—for example $7.25 per hour ($14,500 per year) under certain US state laws (or $2.13 for employees who receive tips, which is known as the tipped minimum wage), $9.47 in the US state of Washington,[19] or £7.50 (for those aged 25+) in the United Kingdom[20]—but also in terms of which pay period (for example Russia and China set monthly minimum wages) or the scope of coverage. Currently the American federal minimum wage rests at seven dollars, twenty-five cents ($7.25) per hour. However, some states do not recognize the minimum wage law such as Louisiana and Tennessee.[21] Other states operate below the federal minimum wage such as Georgia and Wyoming. Some jurisdictions even allow employers to count tips given to their workers as credit towards the minimum wage levels. India was one of the first developing countries to introduce minimum wage policy. It also has one of the most complicated systems with more than 1,200 minimum wage rates.[22]		Customs and extra-legal pressures from governments or labor unions can produce a de facto minimum wage. So can international public opinion, by pressuring multinational companies to pay Third World workers wages usually found in more industrialized countries. The latter situation in Southeast Asia and Latin America was publicized in the 2000s, but it existed with companies in West Africa in the middle of the twentieth century.[23]		Among the indicators that might be used to establish an initial minimum wage rate are ones that minimize the loss of jobs while preserving international competitiveness.[24] Among these are general economic conditions as measured by real and nominal gross domestic product; inflation; labor supply and demand; wage levels, distribution and differentials; employment terms; productivity growth; labor costs; business operating costs; the number and trend of bankruptcies; economic freedom rankings; standards of living and the prevailing average wage rate.		In the business sector, concerns include the expected increased cost of doing business, threats to profitability, rising levels of unemployment (and subsequent higher government expenditure on welfare benefits raising tax rates), and the possible knock-on effects to the wages of more experienced workers who might already be earning the new statutory minimum wage, or slightly more.[25] Among workers and their representatives, political consideration weigh in as labor leaders seek to win support by demanding the highest possible rate.[26] Other concerns include purchasing power, inflation indexing and standardized working hours.		In the United States, the minimum wage promulgated by the Fair Labor Standards Act of 1938. According to the Economic Policy Institute, the minimum wage in the United States would have been $18.28 in 2013 if the minimum wage had kept pace with labor productivity.[27] To adjust for increased rates of worker productivity in the United States, raising the minimum wage to $22 (or more) an hour has been presented.[28][29][30][31]		According to the supply and demand model of the labor market shown in many economics textbooks, increasing the minimum wage decreases the employment of minimum-wage workers.[32] One such textbook states:[33]		If a higher minimum wage increases the wage rates of unskilled workers above the level that would be established by market forces, the quantity of unskilled workers employed will fall. The minimum wage will price the services of the least productive (and therefore lowest-wage) workers out of the market. … the direct results of minimum wage legislation are clearly mixed. Some workers, most likely those whose previous wages were closest to the minimum, will enjoy higher wages. Others, particularly those with the lowest prelegislation wage rates, will be unable to find work. They will be pushed into the ranks of the unemployed.		A firm's cost is an increasing function of the wage rate, the higher the wage rate, the fewer hours an employer will demand of employees. This is because, as the wage rate rises, it becomes more expensive for firms to hire workers and so firms hire fewer workers (or hire them for fewer hours). The demand of labor curve is therefore shown as a line moving down and to the right.[34] Since higher wages increase the quantity supplied, the supply of labor curve is upward sloping, and is shown as a line moving up and to the right.[34] If no minimum wage is in place, wages will adjust until quantity of labor demanded is equal to quantity supplied, reaching equilibrium, where the supply and demand curves intersect. Minimum wage behaves as a classical price floor on labor. Standard theory says that, if set above the equilibrium price, more labor will be willing to be provided by workers than will be demanded by employers, creating a surplus of labor, i.e. unemployment.[34] The basic economic model of markets predicts the same of other commodities (like milk and wheat, for example): Artificially raising the price of the commodity tends to cause the supply of it to increase and the demand for it to lessen. The result is a surplus of the commodity. When there is a wheat surplus, the government buys it. Since the government does not hire surplus labor, the labor surplus takes the form of unemployment, which tends to be higher with minimum wage laws than without them.[23]		The basic supply and demand model implies that by mandating a price floor above the equilibrium wage, minimum wage laws should cause unemployment.[35][36] This is because a greater number of people are willing to work at the higher wage while a smaller number of jobs will be available at the higher wage. Companies can be more selective in those whom they employ thus the least skilled and least experienced will typically be excluded. An imposition or increase of a minimum wage will generally only affect employment in the low-skill labor market, as the equilibrium wage is already at or below the minimum wage, whereas in higher skill labor markets the equilibrium wage is too high for a change in minimum wage to affect employment.[37]		The basic supply and demand theory predicts that raising the minimum wage helps workers whose wages are raised, and hurts people who are not hired (or lose their jobs) when companies cut back on employment. But proponents of the minimum wage hold that the situation is much more complicated than the basic theory can account for. One complicating factor is possible monopsony in the labor market, whereby the individual employer has some market power in determining wages paid. Thus it is at least theoretically possible that the minimum wage may boost employment when affected employees spend more in other sectors of the economy. Though single employer market power is unlikely to exist in most labor markets in the sense of the traditional 'company town,' asymmetric information, imperfect mobility, and the personal element of the labor transaction give some degree of wage-setting power to most firms.[38]		Modern economic theory predicts that although an excessive minimum wage may raise unemployment as it fixes a price above most demand for labor, a minimum wage at a more reasonable level can increase employment, and enhance growth and efficiency. This is because labor markets are monopsonistic and workers persistently lack bargaining power. When poorer workers have more to spend it stimulates effective aggregate demand for goods and services.[39][40]		The argument that a minimum wage decreases employment is based on a simple supply and demand model of the labor market. A number of economists (for example Pierangelo Garegnani,[41] Robert L. Vienneau,[42] and Arrigo Opocher & Ian Steedman[43]), building on the work of Piero Sraffa, argue that that model, even given all its assumptions, is logically incoherent. Michael Anyadike-Danes and Wynne Godley[44] argue, based on simulation results, that little of the empirical work done with the textbook model constitutes a potentially falsifiable theory, and consequently empirical evidence hardly exists for that model. Graham White[45] argues, partially on the basis of Sraffianism, that the policy of increased labor market flexibility, including the reduction of minimum wages, does not have an "intellectually coherent" argument in economic theory.		Gary Fields, Professor of Labor Economics and Economics at Cornell University, argues that the standard textbook model for the minimum wage is ambiguous, and that the standard theoretical arguments incorrectly measure only a one-sector market. Fields says a two-sector market, where "the self-employed, service workers, and farm workers are typically excluded from minimum-wage coverage... [and with] one sector with minimum-wage coverage and the other without it [and possible mobility between the two]," is the basis for better analysis. Through this model, Fields shows the typical theoretical argument to be ambiguous and says "the predictions derived from the textbook model definitely do not carry over to the two-sector case. Therefore, since a non-covered sector exists nearly everywhere, the predictions of the textbook model simply cannot be relied on."[46]		An alternate view of the labor market has low-wage labor markets characterized as monopsonistic competition wherein buyers (employers) have significantly more market power than do sellers (workers). This monopsony could be a result of intentional collusion between employers, or naturalistic factors such as segmented markets, search costs, information costs, imperfect mobility and the personal element of labor markets.[citation needed] In such a case a simple supply and demand graph would not yield the quantity of labor clearing and the wage rate. This is because while the upward sloping aggregate labor supply would remain unchanged, instead of using the upward labor supply curve shown in a supply and demand diagram, monopsonistic employers would use a steeper upward sloping curve corresponding to marginal expenditures to yield the intersection with the supply curve resulting in a wage rate lower than would be the case under competition. Also, the amount of labor sold would also be lower than the competitive optimal allocation.		Such a case is a type of market failure and results in workers being paid less than their marginal value. Under the monopsonistic assumption, an appropriately set minimum wage could increase both wages and employment, with the optimal level being equal to the marginal product of labor.[47] This view emphasizes the role of minimum wages as a market regulation policy akin to antitrust policies, as opposed to an illusory "free lunch" for low-wage workers.		Another reason minimum wage may not affect employment in certain industries is that the demand for the product the employees produce is highly inelastic.[48] For example, if management is forced to increase wages, management can pass on the increase in wage to consumers in the form of higher prices. Since demand for the product is highly inelastic, consumers continue to buy the product at the higher price and so the manager is not forced to lay off workers. Economist Paul Krugman argues this explanation neglects to explain why the firm was not charging this higher price absent the minimum wage.[49]		Three other possible reasons minimum wages do not affect employment were suggested by Alan Blinder: higher wages may reduce turnover, and hence training costs; raising the minimum wage may "render moot" the potential problem of recruiting workers at a higher wage than current workers; and minimum wage workers might represent such a small proportion of a business's cost that the increase is too small to matter. He admits that he does not know if these are correct, but argues that "the list demonstrates that one can accept the new empirical findings and still be a card-carrying economist."[50]		Economists disagree as to the measurable impact of minimum wages in practice. This disagreement usually takes the form of competing empirical tests of the elasticities of supply and demand in labor markets and the degree to which markets differ from the efficiency that models of perfect competition predict.		Economists have done empirical studies on different aspects of the minimum wage, including:[10]		Until the mid-1990s, a general consensus existed among economists, both conservative and liberal, that the minimum wage reduced employment, especially among younger and low-skill workers.[32] In addition to the basic supply-demand intuition, there were a number of empirical studies that supported this view. For example, Gramlich (1976) found that many of the benefits went to higher income families, and that teenagers were made worse off by the unemployment associated with the minimum wage.[52]		Brown et al. (1983) noted that time series studies to that point had found that for a 10 percent increase in the minimum wage, there was a decrease in teenage employment of 1–3 percent. However, the studies found wider variation, from 0 to over 3 percent, in their estimates for the effect on teenage unemployment (teenagers without a job and looking for one). In contrast to the simple supply and demand diagram, it was commonly found that teenagers withdrew from the labor force in response to the minimum wage, which produced the possibility of equal reductions in the supply as well as the demand for labor at a higher minimum wage and hence no impact on the unemployment rate. Using a variety of specifications of the employment and unemployment equations (using ordinary least squares vs. generalized least squares regression procedures, and linear vs. logarithmic specifications), they found that a 10 percent increase in the minimum wage caused a 1 percent decrease in teenage employment, and no change in the teenage unemployment rate. The study also found a small, but statistically significant, increase in unemployment for adults aged 20–24.[53]		Wellington (1991) updated Brown et al.'s research with data through 1986 to provide new estimates encompassing a period when the real (i.e., inflation-adjusted) value of the minimum wage was declining, because it had not increased since 1981. She found that a 10% increase in the minimum wage decreased the absolute teenage employment by 0.6%, with no effect on the teen or young adult unemployment rates.[54]		Some research suggests that the unemployment effects of small minimum wage increases are dominated by other factors.[55] In Florida, where voters approved an increase in 2004, a follow-up comprehensive study after the increase confirmed a strong economy with increased employment above previous years in Florida and better than in the US as a whole.[56] When it comes to on-the-job training, some believe the increase in wages is taken out of training expenses. A 2001 empirical study found that there is "no evidence that minimum wages reduce training, and little evidence that they tend to increase training."[57]		Some empirical studies have tried to ascertain the benefits of a minimum wage beyond employment effects. In an analysis of census data, Joseph Sabia and Robert Nielson found no statistically significant evidence that minimum wage increases helped reduce financial, housing, health, or food insecurity.[58] This study was undertaken by the Employment Policies Institute, a think tank funded by the food, beverage and hospitality industries. In 2012, Michael Reich published an economic analysis that suggested that a proposed minimum wage hike in San Diego might stimulate the city's economy by about $190 million.[59]		The Economist wrote in December 2013: "A minimum wage, providing it is not set too high, could thus boost pay with no ill effects on jobs....America's federal minimum wage, at 38% of median income, is one of the rich world's lowest. Some studies find no harm to employment from federal or state minimum wages, others see a small one, but none finds any serious damage. ... High minimum wages, however, particularly in rigid labour markets, do appear to hit employment. France has the rich world’s highest wage floor, at more than 60% of the median for adults and a far bigger fraction of the typical wage for the young. This helps explain why France also has shockingly high rates of youth unemployment: 26% for 15- to 24-year-olds."[60]		In 1992, the minimum wage in New Jersey increased from $4.25 to $5.05 per hour (an 18.8% increase), while in the adjacent state of Pennsylvania it remained at $4.25. David Card and Alan Krueger gathered information on fast food restaurants in New Jersey and eastern Pennsylvania in an attempt to see what effect this increase had on employment within New Jersey. A basic supply and demand model predicts that relative employment should have decreased in New Jersey. Card and Krueger surveyed employers before the April 1992 New Jersey increase, and again in November–December 1992, asking managers for data on the full-time equivalent staff level of their restaurants both times.[61] Based on data from the employers' responses, the authors concluded that the increase in the minimum wage slightly increased employment in the New Jersey restaurants.[61]		Card and Krueger expanded on this initial article in their 1995 book Myth and Measurement: The New Economics of the Minimum Wage.[62] They argued that the negative employment effects of minimum wage laws are minimal if not non-existent. For example, they look at the 1992 increase in New Jersey's minimum wage, the 1988 rise in California's minimum wage, and the 1990–91 increases in the federal minimum wage. In addition to their own findings, they reanalyzed earlier studies with updated data, generally finding that the older results of a negative employment effect did not hold up in the larger datasets.[63]		In 1996, David Neumark and William Wascher reexamined Card and Krueger's result using administrative payroll records from a sample of large fast food restaurant chains, and reported that minimum wage increases were followed by decreases in employment. An assessment of data collected and analyzed by Neumark and Wascher did not initially contradict the Card and Krueger results,[65] but in a later edited version they found a four percent decrease in employment, and reported that "the estimated disemployment effects in the payroll data are often statistically significant at the 5- or 10-percent level although there are some estimators and subsamples that yield insignificant—although almost always negative" employment effects.[66][67] Neumark and Wascher's conclusions were subsequently rebutted in a 2000 paper by Card and Krueger.[68] A 2011 paper has reconciled the difference between Card and Krueger's survey data and Neumark and Wascher's payroll-based data. The paper shows that both datasets evidence conditional employment effects that are positive for small restaurants, but are negative for large fast-food restaurants.[69]		In 1996 and 1997, the federal minimum wage was increased from $4.25 to $5.15, thereby increasing the minimum wage by $0.90 in Pennsylvania but by just $0.10 in New Jersey; this allowed for an examination of the effects of minimum wage increases in the same area, subsequent to the 1992 change studied by Card and Krueger. A study by Hoffman and Trace found the result anticipated by traditional theory: a detrimental effect on employment.[70]		Further application of the methodology used by Card and Krueger by other researchers yielded results similar to their original findings, across additional data sets.[71] A 2010 study by three economists (Arindrajit Dube of the University of Massachusetts Amherst, William Lester of the University of North Carolina at Chapel Hill, and Michael Reich of the University of California, Berkeley), compared adjacent counties in different states where the minimum wage had been raised in one of the states. They analyzed employment trends for several categories of low-wage workers from 1990 to 2006 and found that increases in minimum wages had no negative effects on low-wage employment and successfully increased the income of workers in food services and retail employment, as well as the narrower category of workers in restaurants.[71][72]		However, a 2011 study by Baskaya and Rubinstein of Brown University found that at the federal level, "a rise in minimum wage have [sic] an instantaneous impact on wage rates and a corresponding negative impact on employment", stating, "Minimum wage increases boost teenage wage rates and reduce teenage employment."[73] Another 2011 study by Sen, Rybczynski, and Van De Waal found that "a 10% increase in the minimum wage is significantly correlated with a 3−5% drop in teen employment."[74] A 2012 study by Sabia, Hansen, and Burkhauser found that "minimum wage increases can have substantial adverse labor demand effects for low-skilled individuals", with the largest effects on those aged 16 to 24.[75]		A 2013 study by Meer and West concluded that "the minimum wage reduces net job growth, primarily through its effect on job creation by expanding establishments ... most pronounced for younger workers and in industries with a higher proportion of low-wage workers."[76] This study by Meer and West was later critiqued for its trends of assumption in the context of narrowly defined low-wage groups.[77] The authors replied to the critiques and released additional data which addressed the criticism of their methodology, but did not resolve the issue of whether their data showed a causal relationship.[78][79] Another 2013 study by Suzana Laporšek of the University of Primorska, on youth unemployment in Europe claimed there was "a negative, statistically significant impact of minimum wage on youth employment."[80] A 2013 study by labor economists Tony Fang and Carl Lin which studied minimum wages and employment in China, found that "minimum wage changes have significant adverse effects on employment in the Eastern and Central regions of China, and result in disemployment for females, young adults, and low-skilled workers".[81][82]		Several researchers have conducted statistical meta-analyses of the employment effects of the minimum wage. In 1995, Card and Krueger analyzed 14 earlier time-series studies on minimum wages and concluded that there was clear evidence of publication bias (in favor of studies that found a statistically significant negative employment effect). They point out that later studies, which had more data and lower standard errors, did not show the expected increase in t-statistic (almost all the studies had a t-statistic of about two, just above the level of statistical significance at the .05 level).[83] Though a serious methodological indictment, opponents of the minimum wage largely ignored this issue; as Thomas Leonard noted, "The silence is fairly deafening."[84]		In 2005, T.D. Stanley showed that Card and Krueger's results could signify either publication bias or the absence of a minimum wage effect. However, using a different methodology, Stanley concluded that there is evidence of publication bias and that correction of this bias shows no relationship between the minimum wage and unemployment.[85] In 2008, Hristos Doucouliagos and T.D. Stanley conducted a similar meta-analysis of 64 U.S. studies on disemployment effects and concluded that Card and Krueger's initial claim of publication bias is still correct. Moreover, they concluded, "Once this publication selection is corrected, little or no evidence of a negative association between minimum wages and employment remains."[86] In 2013, a meta-analysis of 16 UK studies found that the minimum wage has no significant effects on employment. [87] A 2014 meta-analysis found that the minimum wage reduces employment among teenagers. [88]		Minimum wage laws affect workers in most low-paid fields of employment[10] and have usually been judged against the criterion of reducing poverty.[89] Minimum wage laws receive less support from economists than from the general public. Despite decades of experience and economic research, debates about the costs and benefits of minimum wages continue today.[10]		Various groups have great ideological, political, financial, and emotional investments in issues surrounding minimum wage laws. For example, agencies that administer the laws have a vested interest in showing that "their" laws do not create unemployment, as do labor unions whose members' finances are protected by minimum wage laws. On the other side of the issue, low-wage employers such as restaurants finance the Employment Policies Institute, which has released numerous studies opposing the minimum wage.[90][91] The presence of these powerful groups and factors means that the debate on the issue is not always based on dispassionate analysis. Additionally, it is extraordinarily difficult to separate the effects of minimum wage from all the other variables that affect employment.[23]		The following table summarizes the arguments made by those for and against minimum wage laws:		Supporters of the minimum wage claim it has these effects:		Opponents of the minimum wage claim it has these effects:		A widely circulated argument that the minimum wage was ineffective at reducing poverty was provided by George Stigler in 1949:		In 2006, the International Labour Organization (ILO) argued that the minimum wage could not be directly linked to unemployment in countries that have suffered job losses.[4] In April 2010, the Organisation for Economic Co-operation and Development (OECD) released a report arguing that countries could alleviate teen unemployment by "lowering the cost of employing low-skilled youth" through a sub-minimum training wage.[121] A study of U.S. states showed that businesses' annual and average payrolls grow faster and employment grew at a faster rate in states with a minimum wage.[122] The study showed a correlation, but did not claim to prove causation.		Although strongly opposed by both the business community and the Conservative Party when introduced in the UK in 1999, the Conservatives reversed their opposition in 2000.[123] Accounts differ as to the effects of the minimum wage. The Centre for Economic Performance found no discernible impact on employment levels from the wage increases,[124] while the Low Pay Commission found that employers had reduced their rate of hiring and employee hours employed, and found ways to cause current workers to be more productive (especially service companies).[125] The Institute for the Study of Labor found prices in the minimum wage sector rose significantly faster than prices in non-minimum wage sectors, in the four years following the implementation of the minimum wage.[126] Neither trade unions nor employer organizations contest the minimum wage, although the latter had especially done so heavily until 1999.		In 2014, supporters of minimum wage cited a study that found that job creation within the United States is faster in states that raised their minimum wages.[95][127][128] In 2014, supporters of minimum wage cited news organizations who reported the state with the highest minimum-wage garnered more job creation than the rest of the United States.[95][96][129][130][131][132][133]		In 2014, in Seattle, Washington, liberal and progressive business owners who had supported the city's new $15 minimum wage said they might hold off on expanding their businesses and thus creating new jobs, due to the uncertain timescale of the wage increase implementation.[134] However, subsequently at least two of the business owners quoted did expand.[135][136]		The dollar value of the minimum wage loses purchasing power over time due to inflation. Minimum wage laws, for instance proposals to index the minimum wage to average wages, have the potential to keep the dollar value of the minimum wage relevant and predictable.[137]		With regard to the economic effects of introducing minimum wage legislation in Germany in January 2015, recent developments have shown that the feared increase in unemployment has not materialized, however, in some economic sectors and regions of the country, it came to a decline in job opportunities particularly for temporary and part-time workers, and some low-wage jobs have disappeared entirely.[138] Because of this overall positive development, the Deutsche Bundesbank revised its opinion, and ascertained that “the impact of the introduction of the minimum wage on the total volume of work appears to be very limited in the present business cycle”.[139]		According to a 1978 article in the American Economic Review, 90% of the economists surveyed agreed that the minimum wage increases unemployment among low-skilled workers.[140] By 1992 the survey found 79% of economists in agreement with that statement,[141] and by 2000, 45.6% were in full agreement with the statement and 27.9% agreed with provisos (73.5% total).[142][143] The authors of the 2000 study also reweighted data from a 1990 sample to show that at that time 62.4% of academic economists agreed with the statement above, while 19.5% agreed with provisos and 17.5% disagreed. They state that the reduction on consensus on this question is "likely" due to the Card and Krueger research and subsequent debate.[144]		A similar survey in 2006 by Robert Whaples polled PhD members of the American Economic Association (AEA). Whaples found that 46.8% respondents wanted the minimum wage eliminated, 37.7% supported an increase, 14.3% wanted it kept at the current level, and 1.3% wanted it decreased.[145] Another survey in 2007 conducted by the University of New Hampshire Survey Center found that 73% of labor economists surveyed in the United States believed 150% of the then-current minimum wage would result in employment losses and 68% believed a mandated minimum wage would cause an increase in hiring of workers with greater skills. 31% felt that no hiring changes would result.[146]		Surveys of labor economists have found a sharp split on the minimum wage. Fuchs et al. (1998) polled labor economists at the top 40 research universities in the United States on a variety of questions in the summer of 1996. Their 65 respondents were nearly evenly divided when asked if the minimum wage should be increased. They argued that the different policy views were not related to views on whether raising the minimum wage would reduce teen employment (the median economist said there would be a reduction of 1%), but on value differences such as income redistribution.[147] Daniel B. Klein and Stewart Dompe conclude, on the basis of previous surveys, "the average level of support for the minimum wage is somewhat higher among labor economists than among AEA members."[148]		In 2007, Klein and Dompe conducted a non-anonymous survey of supporters of the minimum wage who had signed the "Raise the Minimum Wage" statement published by the Economic Policy Institute. 95 of the 605 signatories responded. They found that a majority signed on the grounds that it transferred income from employers to workers, or equalized bargaining power between them in the labor market. In addition, a majority considered disemployment to be a moderate potential drawback to the increase they supported.[148]		In 2013, a diverse group of 37 economics professors was surveyed on their view of the minimum wage's impact on employment. 34% of respondents agreed with the statement, "Raising the federal minimum wage to $9 per hour would make it noticeably harder for low-skilled workers to find employment." 32% disagreed and the remaining respondents were uncertain or had no opinion on the question. 47% agreed with the statement, "The distortionary costs of raising the federal minimum wage to $9 per hour and indexing it to inflation are sufficiently small compared with the benefits to low-skilled workers who can find employment that this would be a desirable policy", while 11% disagreed.[149]		Economists and other political commentators have proposed alternatives to the minimum wage. They argue that these alternatives may address the issue of poverty better than a minimum wage, as it would benefit a broader population of low wage earners, not cause any unemployment, and distribute the costs widely rather than concentrating it on employers of low wage workers.		A basic income (or negative income tax) is a system of social security that periodically provides each citizen with a sum of money that is sufficient to live on frugally. It is argued that recipients of the basic income would have considerably more bargaining power when negotiating a wage with an employer as there would be no risk of destitution for not taking the employment. As a result, the jobseeker could spend more time looking for a more appropriate or satisfying job, or they could wait until a higher-paying job appeared. Alternately, they could spend more time increasing their skills in university, which would make them more suitable for higher-paying jobs, as well as provide numerous other benefits. Experiments on Basic Income and NIT in Canada and the USA show that people spent more time studying while the program was running.[150]		Proponents argue that a basic income that is based on a broad tax base would be more economically efficient, as the minimum wage effectively imposes a high marginal tax on employers, causing losses in efficiency.[citation needed]		A guaranteed minimum income is another proposed system of social welfare provision. It is similar to a basic income or negative income tax system, except that it is normally conditional and subject to a means test. Some proposals also stipulate a willingness to participate in the labor market, or a willingness to perform community services.[151]		A refundable tax credit is a mechanism whereby the tax system can reduce the tax owed by a household to below zero, and result in a net payment to the taxpayer beyond their own payments into the tax system. Examples of refundable tax credits include the earned income tax credit and the additional child tax credit in the US, and working tax credits and child tax credits in the UK. Such a system is slightly different from a negative income tax, in that the refundable tax credit is usually only paid to households that have earned at least some income. This policy is more targeted against poverty than the minimum wage, because it avoids subsidizing low-income workers who are supported by high-income households (for example, teenagers still living with their parents).[152]		In the United States, earned income tax credit rates, also known as EITC or EIC, vary by state—some are refundable while other states do not allow a refundable tax credit.[153] The federal EITC program has been expanded by a number of presidents including Jimmy Carter, Ronald Reagan, George H.W. Bush, and Bill Clinton.[154] In 1986, President Reagan described the EITC as "the best anti poverty, the best pro-family, the best job creation measure to come out of Congress."[155] The ability of the earned income tax credit to deliver larger monetary benefits to the poor workers than an increase in the minimum wage and at a lower cost to society was documented in a 2007 report by the Congressional Budget Office.[156]		The Adam Smith Institute prefers cutting taxes on the poor and middle class instead of raising wages as an alternative to the minimum wage.[157]		Italy, Sweden, Norway, Finland, and Denmark are examples of developed nations where there is no minimum wage that is required by legislation.[16][18] Such nations, particularly the Nordics, have very high union participation rates.[158] Instead, minimum wage standards in different sectors are set by collective bargaining.[159]		In January 2014, seven Nobel economists—Kenneth Arrow, Peter Diamond, Eric Maskin, Thomas Schelling, Robert Solow, Michael Spence, and Joseph Stiglitz—and 600 other economists wrote a letter to the US Congress and the US President urging that, by 2016, the US government should raise the minimum wage to $10.10. They endorsed the Minimum Wage Fairness Act which was introduced by US Senator Tom Harkin in 2013.[160][161] U.S. Senator Bernie Sanders introduced a bill in 2015 that would raise the minimum wage to $15, and in his 2016 campaign for president ran on a platform of increasing it.[162][163] Although Sanders did not become the nominee, the Democratic National Committee adopted his $15 minimum wage push in their 2016 party platform.[164]		Reactions from former McDonald's USA Ed Rensi about raising minimum wage to $15 is to completely push humans out of the picture when it comes to labor if they are to pay minimum wage at $15 they would look into replacing humans with machines as that would be the more cost-effective than having employees that are ineffective. During an interview on FOX Business Network’s Mornings with Maria, he stated that he believes an increase to $15 an hour would cause job loss at an extraordinary level. Rensi also believes it does not only affect the fast food industry, franchising he sees as the best business model in the United States, it is dependent on people that have low job skills that have to grow and if you cannot pay them a reasonable wage then they are going to be replaced with machines[165]		In late March 2016, Governor of California Jerry Brown reached a deal to raise the minimum wage to $15 by 2022 for big businesses and 2023 for smaller businesses.[166]		In contrast, the relatively high minimum wage in Puerto Rico has been blamed by various politicians and commentators as a highly significant factor in the Puerto Rican government-debt crisis.[167][168][169] One study concluded that "Employers are disinclined to hire workers because the US federal minimum wage is very high relative to the local average".[170]		As of December 2014, unions were exempt from recent minimum wage increases in Chicago, Illinois, SeaTac, Washington, and Milwaukee County, Wisconsin, as well as the California cities of Los Angeles, San Francisco, Long Beach, San Jose, Richmond, and Oakland.[171]		
The minimum wage in the United States is a network of federal, state, and local laws. Employers generally must pay workers the highest minimum wage prescribed by federal, state, and local law. As of July 2016, the federal government mandates a nationwide minimum wage of $7.25 per hour. As of October 2016, there are 29 states with a minimum wage higher than the federal minimum. From 2014 to 2015, nine states increased their minimum wage levels through automatic adjustments, while increases in 11 other states occurred through referendum or legislative action. In real terms, the federal minimum wage peaked near $10.00 per hour in 1968, using 2014 inflation-adjusted dollars.[1]		Beginning in January 2017, Massachusetts and Washington state have the highest minimum wages in the country, at $11.00 per hour.[2] New York City's minimum wage will be $15.00 per hour by the end of 2018.[3] There is a racial difference for support of a higher minimum wage with most black and Hispanic individuals supporting a $15.00 federal minimum wage, and 54% of whites opposing it.[4] In 2015, about 3 percent of White, Asian, and Hispanic or Latino workers earned the federal minimum wage or less. Among Black workers, the percentage was about 4 percent.[5]						In 1912, Massachusetts organized a commission to recommend non-compulsory minimum wages for women and children. Within eight years, at least thirteen U.S. states and the District of Columbia would pass minimum wage laws, with pressure being placed on state legislatures by the National Consumers League in a coalition with other women's voluntary associations and organized labor.[6][7] The United States Supreme Court of the Lochner era consistently invalidated compulsory minimum wage laws. Advocates for these minimum wage laws hoped that they would be upheld under the precedent of Muller v. Oregon, which had upheld maximum working hours laws for women on the grounds that women required special protection which men did not.[7] However, the Court did not extend this principle to minimum wage laws, considering the latter as interfering with the ability of employers to freely negotiate wage contracts with employees.[6]:518		In 1933, the Roosevelt administration made the first attempt at establishing a national minimum wage, when a $0.25 per hour standard was set as part of the National Industrial Recovery Act. However, in the 1935 court case Schechter Poultry Corp. v. United States (295 U.S. 495), the US Supreme Court declared the act unconstitutional, and the minimum wage was abolished. In 1938, the minimum wage was re-established pursuant to the Fair Labor Standards Act, once again at $0.25 per hour ($4.23 in 2015 dollars[9]). In 1941, the Supreme Court upheld the Fair Labor Standards Act in United States v. Darby Lumber Co., holding that Congress had the power under the Commerce Clause to regulate employment conditions.[10]		The 1938 minimum wage law only applied to "employees engaged in interstate commerce or in the production of goods for interstate commerce," but in amendments in 1961 and 1966, the federal minimum wage was extended (with slightly different rates) to employees in large retail and service enterprises, local transportation and construction, state and local government employees, as well as other smaller expansions; a grandfather clause in 1990 drew most employees into the purview of federal minimum wage policy, which now set the wage at $3.80.[11]		In 2006, voters in six states (Arizona, Colorado, Missouri, Montana, Nevada, and Ohio) approved statewide increases in the state minimum wage. The amounts of these increases ranged from $1 to $1.70 per hour and all increases were designed to annually index to inflation.[12] Some politicians in the United States have advocated linking the minimum wage to the Consumer Price Index, thereby increasing the wage automatically each year based on increases to the Consumer Price Index. So far, Ohio, Oregon, Missouri, Vermont and Washington have linked their minimum wages to the consumer price index. Minimum wage indexing also takes place each year in Florida, San Francisco, California, and Santa Fe, New Mexico.[citation needed]		The federal minimum wage in the United States was reset to its current rate of $7.25 per hour in July 2009.[11] Some U.S. territories (such as American Samoa) are exempt. Some types of labor are also exempt: employers may pay tipped labor a minimum of $2.13 per hour, as long as the hour wage plus tip income equals at least the minimum wage. Persons under the age of 20 may be paid $4.25 an hour for the first 90 calendar days of employment (sometimes known as a youth, teen, or training wage) unless a higher state minimum exists.[13] The 2009 increase was the last of three steps of the Fair Minimum Wage Act of 2007, which was signed into law as a rider to the U.S. Troop Readiness, Veterans' Care, Katrina Recovery, and Iraq Accountability Appropriations Act, 2007, a bill that also contained almost $5 billion in tax cuts for small businesses.		In April 2014, the U.S. Senate debated the Minimum Wage Fairness Act (S. 1737; 113th Congress). The bill would have amended the Fair Labor Standards Act of 1938 (FLSA) to increase the federal minimum wage for employees to $10.10 per hour over the course of a two-year period.[14] The bill was strongly supported by President Barack Obama and many of the Democratic Senators, but strongly opposed by Republicans in the Senate and House.[15][16][17] Later in 2014, voters in the Republican-controlled states of Alaska, Arkansas, Nebraska and South Dakota considered ballot initiatives to raise the minimum wage above the national rate of $7.25 per hour, which were successful in all 4 states. The results provided evidence that raising minimum wage has support across party lines.[18]		Since 2012, a growing protest and advocacy movement called "Fight for $15", initially growing out of fast food worker strikes, has advocated for an increase in the minimum wage. On March 27, 2014, Connecticut passed legislation to raise the minimum wage from $8.70 to $10.10 by 2017, making it one of about six states to aim at or above $10.00 per hour.[19] In 2014 and 2015, several cities, including San Francisco, Seattle, Los Angeles, and Washington passed ordinances that gradually increase the minimum wage to $15.00.[20][21] In 2016, New York and California passed legislation that would gradually raise the minimum wage to $15 in each state.[22][23]		As of 2017, recent legislation was passed in multiple states to raise the minimum wage a certain amount in a certain amount of time. California is set to raise their minimum wage to $15.00/hour by January 1, 2023.[24] Colorado is set to raise their minimum wage from $9.30/hour to $12/hour by January 1, 2020, raising $0.90 per year.[25] Seattle passed legislation in 2015 for a raise in minimum wage; for employers of 500 or more employees without heath benefits, the minimum wage will be raised to $15.00/hour by 2017, for employees with heath benefits, the minimum wage will raise to $15.00/hour by 2018, for smaller employees the $15.00/hour wage will be reached at different times. Seattle is one of the first cities to put in place a plan that after $15.00/hour wage is reached, they will continue to increase minimum wage by a certain percentage every year based on inflation changes. [26] New York has also recently passed legislation to increase their minimum wage to $15.00/hour over time, certain counties and larger companies are set on faster plans than others. [27] As there have only been a few places mentioned, cities and states across the United States are putting in place certain legislation to increase the minimum wage for minimum wage workers to a livable wage. Time will only tell the effects on the economy and number of jobs in those cities and states.		On June 2, 2014, the City Council of Seattle, Washington passed a local ordinance to increase the minimum wage of the city to $15.00 per hour,[28] giving the city the highest minimum wage in the United States,[29][30] which will be phased in over seven years, to be fully implemented by 2021.[31] A growing number of California cities have enacted local minimum wage ordinances, including Los Angeles, San Francisco, Oakland, Berkeley, Emeryville, Mountain View, Richmond, and San Jose.		In September 2014, the Los Angeles City Council approved a minimum salary for hotel workers of $15.37 per hour.[32] In April 2016, The Los Angeles Times reported that there is an exemption for unionised workers, and interviewed longtime workers at unionised Sheraton Universal who make $10.00 per hour, whereas non-union employees at a non-union Hilton a few feet away make at least the $15.37 mandated by law for non-unionised employees.[33] Similar exemptions have been adopted in San Francisco, San Jose, Oakland, and Santa Monica.		San Francisco is expected to become the first U.S. city to reach a minimum wage of $15.00 per hour on July 1, 2018, .[34] The minimum wage in Los Angeles and Washington, D.C., will be $15.00 per hour in 2020.[35][36]		On August 18, 2015, the El Cerrito City Council directed city staff to draft a local minimum wage ordinance based on a template provided by a coalition for a county-wide minimum wage effort. The details are not final, but the Council discussed an initial increase of roughly 28–36% ($11.52 – $12.25 or more) by January 1, 2016, with annual increases that will result in a $15.00 hourly wage rate by 2018–2020. The Council did not direct staff to create small business exemptions (or any other exemptions), but a slower phase-in rate may be considered for employees of small businesses. The city will have outreach for residents and business owners to discuss the details of the proposed ordinance. Staff hopes to present a draft for The Council's approval as early as October or November 2015.[37]		As of December 2014, unions were exempt from recent minimum wage increases in Chicago, Illinois, SeaTac, Washington, and Milwaukee County, Wisconsin, as well as the California cities of Los Angeles, San Francisco, Long Beach, San Jose, Richmond, and Oakland.[38] In 2016, the Washington, D.C. Council passed a minimum wage ordinance that included a union waiver, but Mayor Vincent Gray vetoed it. Later that year, the council approved an increase without the union waiver.[39]		Since its inception the purchasing power of the minimum wage has fluctuated. The minimum wage had its highest purchasing power in 1968, when it was $1.60 per hour ($10.88 in 2014 dollars).[9][1] From January 1981 to April 1990, the minimum wage was frozen at $3.35 per hour, then a record-setting minimum wage freeze. From September 1, 1997 through July 23, 2007, the federal minimum wage remained constant at $5.15 per hour, breaking the old record. From the United States Department of Labor. Employment Standards Administration. Wage and Hour Division, the source page has a clickable US map with current and projected state-by-state minimum wage rates for each state. Some government entities, such as counties and cities, observe minimum wages that are higher than the state as a whole. One notable example of this is Santa Fe, New Mexico, whose $9.50 per hour minimum wage was the highest in the nation,[40][41][42] until San Francisco increased its minimum wage to $9.79 in 2009.[43] Another device to increase wages, living wage ordinances, generally apply only to businesses that are under contract to the local government itself.		Since 1984, the purchasing power of the federal minimum wage has decreased. Measured in real terms (adjusted for inflation) using 1984 dollars, the real minimum wage was $3.35 in 1984, $2.90 in 1995, $2.74 in 2005, and $3.23 in 2013. If the minimum wage had been raised to $10.00 in 2013, that would have equated to $4.46 in 1984 dollars.[44][45]		The economic effects of raising the minimum wage are controversial. Adjusting the minimum wage may affect current and future levels of employment, prices of goods and services, economic growth, income inequality and poverty. The interconnection of price levels, central bank policy, wage agreements, and total aggregate demand creates a situation in which the conclusions drawn from macroeconomic analysis are highly influenced by the underlying assumptions of the interpreter.[46]		The law of demand states that—all else being equal—raising the price of any particular good or service will reduce the quantity demanded. Thus, neoclassical economics argues that—all else being equal—raising in the minimum wage will have adverse affects on employment.		Conceptually, if an employer does not believe a worker generates value equal to or in excess of the minimum wage, that worker will not be hired or retained.[47]		Empirical work on fast food workers in the 1990s challenged the neoclassical model. In 1994, economists David Card and Alan Krueger studied employment trends among 410 restaurants in New Jersey and eastern Pennsylvania following New Jersey's minimum wage hike (from $4.25 to $5.05) in April 1992. They found "no indication that the rise in the minimum wage reduced employment."[48] However, a 1995 re-analysis of the evidence by David Neumark found that the increase in New Jersey's minimum wage actually resulted in a 4.6% decrease in employment.[49] Neumark's study relied on payroll records from a sample of large fast-food restaurant chains, whereas the Card-Krueger study relied on business surveys.		Additional research conducted by David Neumark and William Wascher (which surveyed over 100 studies related to the employment effects of minimum wages) found that the majority of peer-reviewed economic research (about two-thirds) showed a positive correlation between minimum wage hikes and increased unemployment—especially for young and unskilled workers. Neumark's analysis further found that, when looking at only the most credible research, 85% of studies showed a positive correlation between minimum wage hikes and increased unemployment.[50]		In February 2014, the Congressional Budget Office (CBO) reported the theoretical effects of a federal minimum wage increase under two scenarios: an increase to $9.00 and an increase to $10.10. According to the report, approximately 100,000 jobs would be lost under the $9.00 option, whereas 500,000 jobs would be lost under the $10.10 option (with a wide range of possible outcomes).[51]		A 2013 Center for Economic and Policy Research (CEPR) review of multiple studies since 2000 indicated that there was "little or no employment response to modest increases in the minimum wage."[52] Another CEPR study in 2014 found that job creation within the United States is faster within states that raised their minimum wage.[53] In 2014, the state with the highest minimum wage in the nation, Washington, exceeded the national average for job growth in the United States.[54]		A 2012 study led by Joseph Sabia, professor of economics at the University of New Hampshire, estimated that the 2004-6 New York State minimum wage increase (from $5.15 to $6.75) resulted in a 20.2% to 21.8% reduction in employment for less-skilled, less-educated workers.[55] Another study conducted by Joseph Sabia, then an assistant professor at American University, found that minimum wages were ineffective at alleviating poverty for single mothers. The study further concluded that a 10% increase in the minimum wage was associated with an 8.8% reduction in employment and an 11.8% reduction in hours for uneducated single mothers.[56]		Research conducted by Richard Burkhauser, professor emeritus of Policy Analysis at Cornell University, concluded that minimum wage increases "significantly reduce the employment of the most vulnerable groups in the working-age population—young adults without a high school degree (aged 20-24), young black adults and teenagers (aged 16-24), and teenagers (aged 16-19)."[57]		A 2007 study by Daniel Aaronson and Eric French concluded that a 10% increase in the minimum wage decreased low-skill employment by 2-4% and total restaurant employment by 1-3%.[58]		The Economist wrote in December 2013: "A minimum wage, providing it is not set too high, could thus boost pay with no ill effects on jobs...Some studies find no harm to employment from federal or state minimum wages, others see a small one, but none finds any serious damage...High minimum wages, however, particularly in rigid labour markets, do appear to hit employment. France has the rich world’s highest wage floor, at more than 60% of the median for adults and a far bigger fraction of the typical wage for the young. This helps explain why France also has shockingly high rates of youth unemployment: 26% for 15- to 24-year-olds."[59]		Conceptually—all else being equal—raising the minimum wage will increase the cost of labor. Thus, employers may accept lower profits, raise their prices, or both. If prices increase, consumers may demand a lesser quantity of the product, substitute other products, or switch to imported products. Marginal producers (those who are barely profitable enough to survive) may be forced out of business if they cannot raise their prices sufficiently to offset the higher cost of labor. Whether the increased income of the workers benefiting from the minimum wage increase can offset these effects is debatable.[47] Federal Reserve Bank of Chicago research from 2007 has shown that restaurant prices rise in response to minimum wage increases.[60]		A 2016 White House report based on "back of envelope calculations and literature review" argued that higher hourly wages led to less crime.[61]		A report by the Council of Economic Advisers claimed that "raising the minimum wage reduces crime by 3 to 5 percent." To get those numbers, the study assumed that "such a minimum wage increase would have no employment impacts, with an employment elasticity of 0.1 the benefits would be somewhat lower."[61]		However, In a 1987 journal article based on actual study data, Masanori Hashimoto noted that minimum wage hikes lead to increased levels of property crime in areas affected by the minimum wage after its increase.[62] According to the article, by decreasing employment in poor communities, total legal trade and production are curtailed. The report also claimed that in order to compensate for the decrease in legal avenues for production and consumption, poor communities increasingly turn to illegal trade and activity.[62]		Whether growth (GDP, a measure of both income and production) increases or decreases depends significantly on whether the income shifted from owners to workers results in an overall higher level of spending. The tendency of a consumer to spend their next dollar is referred to as the marginal propensity to consume or MPC. The transfer of income from higher income owners (who tend to save more, meaning a lower MPC) to lower income workers (who tend to save less, with a higher MPC) can actually lead to an increase in total consumption and higher demand for goods, leading to increased employment.[51] Recent research has shown that higher wages lead to greater productivity. [63]		The CBO reported in February 2014 that income (GDP) overall would be marginally higher after raising the minimum wage, indicating a small net positive increase in growth. Raising the minimum wage to $10.10 and indexing it to inflation would result in a net $2 billion increase in income during the second half of 2016, while raising it to $9.00 and not indexing it would result in a net $1 billion increase in income.[51]		An increase in the minimum wage is a form of redistribution from higher-income persons (business owners or "capital") to lower income persons (workers or "labor") and therefore should reduce income inequality. The CBO estimated in February 2014 that raising the minimum wage under either scenario described above would improve income inequality. Families with income more than 6 times the poverty threshold would see their incomes fall (due in part to their business profits declining with higher employee costs), while families with incomes below that threshold would rise.[51]		Among hourly-paid workers In 2016, 701,000 earned the federal minimum wage and about 1.5 million earned wages below the minimum. Together, these 2.2 million workers represented 2.7% of all hourly-paid workers.[65]		CBO estimated in February 2014 that raising the minimum wage would reduce the number of persons below the poverty income threshold by 900,000 under the $10.10 option versus 300,000 under the $9.00 option.[51]		Research conducted by David Neumark and colleagues found that minimum wages are associated with reductions in the hours and employment of low-wage workers.[66] A separate study by the same researchers found that minimum wages tend to increase the proportion of families with incomes below or near the poverty line.[67] Similarly, a 2002 study led by Richard Vedder, professor of economics at Ohio University, concluded that "The empirical evidence is strong that minimum wages have had little or no effect on poverty in the U.S. Indeed, the evidence is stronger that minimum wages occasionally increase poverty…"[68]		The CBO reported in February 2014 that "[T]he net effect on the federal budget of raising the minimum wage would probably be a small decrease in budget deficits for several years but a small increase in budget deficits thereafter. It is unclear whether the effect for the coming decade as a whole would be a small increase or a small decrease in budget deficits." On the cost side, the report cited higher wages paid by the government to some of its employees along with higher costs for certain procured goods and services. This might be offset by fewer government benefits paid, as some workers with higher incomes would receive fewer government transfer payments. On the revenue side, some would pay higher taxes and others less.[51]		According to a survey conducted by economist Greg Mankiw, 79% of economists agreed that "a minimum wage increases unemployment among young and unskilled workers."[71]		A 2015 survey conducted by the University of New Hampshire Survey Center found that a majority of economists believed raising the minimum wage to $15 per hour would have negative effects on youth employment levels (83%), adult employment levels (52%), and the number of jobs available (76%). Additionally, 67% of economists surveyed believed that a $15 minimum wage would make it harder for small businesses with less than 50 employees to stay in business.[72]		A 2006 survey conducted by Robert Whaples, professor of economics at Wake Forest University, found that, among the economists surveyed[How many?], opinions about the minimum wage were as follows:[73]		In 2014, over 600 economists signed a letter in support of a $10.10 minimum wage increase with research suggesting that a minimum wage increase could have a small stimulative effect on the economy as low-wage workers spend their additional earnings, raising demand and job growth.[74][75][76][77] Also, seven recipients of the Nobel Prize in Economic Sciences were among 75 economists endorsing an increase in the minimum wage for U.S. workers and said "the weight" of economic research shows higher pay doesn’t lead to fewer jobs.[78][79]		According to a February 2013 survey of the University of Chicago IGM Forum, which includes approximately 40 economists:		According to a fall 2000 survey conducted by Fuller and Geide-Stevenson, 73.5% (27.9% of which agreed with provisos) of American economists surveyed[How many?] agreed that minimum wage laws increase unemployment among unskilled and young workers, while 26.5% disagreed with the statement.[81]		Economist Paul Krugman advocated raising the minimum wage moderately in 2013, citing several reasons, including:		Former President Bill Clinton advocated raising the minimum wage in 2014: "I think we ought to raise the minimum wage because it doesn’t just raise wages for the three or four million people who are directly affected by it, it bumps the wage structure everywhere...The estimates are that 35 million Americans would get a pay raise if the federal minimum wage was raised...If you [raise the minimum wage] in a phased way, it always creates jobs. Why? Because people who make the minimum wage or near it are struggling to get by, they spend every penny they make, they turn it over in the economy, they create jobs, they create opportunity, and they take better care of their children. It’s just the right thing to do, but it’s also very good economics."[83]		The Pew Center reported in January 2014 that 73% of Americans supported raising the minimum wage from $7.25 to $10. By party, 53% of Republicans and 90% of Democrats favored this action.[84] Pew found a racial difference for support of a higher minimum wage in 2017 with most blacks and Hispanics supporting a $15.00 federal minimum wage, and 54% of whites opposing it.[85]		A Lake Research Partners poll in February 2012 found the following:		This is a list of the minimum wages (per hour) in each state and territory of the United States, for jobs covered by federal minimum wage laws. If the job is not subject to the federal Fair Labor Standards Act, then state, city, or other local laws may determine the minimum wage.[87] A common exemption to the federal minimum wage is a company having revenue of less than $500,000 per year while not engaging in any interstate commerce.		Under the federal law, workers who receive a portion of their salary from tips, such as waitstaff, are required only to have their total compensation, including tips, meet the minimum wage. Therefore, often, their hourly wage, before tips, is less than the minimum wage.[88] Seven states, and Guam, do not allow for a tip credit.[89] Additional exemptions to the minimum wage include many seasonal employees, student employees, and certain disabled employees as specified by the FLSA.[90]		In addition, some counties and cities within states may observe a higher minimum wage than the rest of the state in which they are located; sometimes this higher wage will apply only to businesses that are under contract to the local government itself, while in other cases the higher minimum will be enforced across the board.		The average US minimum wage per capita (2017) is $8.49 based on the population size of each state and generally represents the average minimum wage experienced by a person working in one of the fifty US states. Cities, counties, districts, and territories are not included in the calculation.[citation needed] As of October 2016, there have been 29 states with a minimum wage higher than the federal minimum. From 2014 to 2015, nine states increased their minimum wage levels through automatic adjustments, while increases in 11 other states occurred through referendum or legislative action.[1] Beginning in January 2017, Massachusetts and Washington state have the highest minimum wages in the country, at $11.00 per hour.[95] New York City's minimum wage will be $15.00 per hour by the end of 2018.[3] |-		For employees working in Prince George's County, the minimum wage is $10.75 per hour, effective October 1, 2016, and increases to $11.50 on October 1, 2017.[138]		For employees working in Montgomery County, the minimum wage is $11.50 per hour starting July 1, 2017.[140] County Council bill 12-16 was enacted on January 17, 2017 to adjust the minimum wage to $15 and base future adjustments on the Consumer Price Index, but was later vetoed by the County Executive.[141][142]		A 2016 law changed the minimum wage over the following six years. "Large" employers have 11 or more employees, and "small" have between 1 and 10. "Downstate" includes Nassau, Suffolk, and Westchester Counties.[166] NYC large employers: $11.00, NYC small employers: $10.50, Downstate employers: $10.00, Upstate employers: $9.70. As of December 31, 2017: NYC large employers: $13.00; NYC small employers: $12.00; Downstate employers: $11.00; Upstate employers: $10.40. As of December 31, 2018: NYC large employers: $15.00; NYC small employers: $13.50; Downstate employers: $12.00; Upstate employers: $11.10. As of December 31, 2019: NYC large employers: $15.00; NYC small employers: $15.00; Downstate employers: $13.00; Upstate employers: $12.50. As of December 31, 2020: NYC large employers: $15.00; NYC small employers: $15.00; Downstate employers: $14.00; Upstate employers: $12.50. As of December 31, 2021: NYC large employers: $15.00; NYC small employers: $15.00; Downstate employers: $15.00; Upstate employers: Set by Commissioner of Labor based on economic conditions, up to $15.00.		Tipped food service workers will be paid $7.50 per hour, or two-thirds of the applicable minimum wage rate rounded to the nearest $0.05, whichever is higher.[citation needed]		Puerto Rico also has minimum wage rates that vary according to the industry. These rates range from a minimum of $5.08 to $7.25 per hour.		In accordance with a law signed on June 27, 2016,[200][201] the minimum wage will be $12.50 per hour as of July 1, 2017; $13.25 per hour as of July 1, 2018; 14.00 per hour as of July 1, 2019; and $15.00 per hour as of July 1, 2020.[202] As of each successive July 1, the minimum wage will increase by the Consumer Price Index for All Urban Consumers in the Washington Metropolitan Statistical Area for the preceding twelve months.[202]		The minimum wage for tipped-employees will increase to $3.47 per hour as of July 1, 2017; $4.17 per hour as of July 1, 2018; $4.87 per hour as of July 1, 2019; and $5.55 per hour as of July 1, 2020.[202]		The minimum wage established by the federal government may be paid to newly hired individuals during their first 90 calendar days of employment, students employed by colleges and universities, and individuals under 18 years of age.[203]		The jobs that are most likely to be directly affected by the minimum wage are those which pay a wage close to the minimum.		According to the May 2006 National Occupational Employment and Wage Estimates, the four lowest-paid occupational sectors in May 2006 (when the federal minimum wage was $5.15 per hour) were the following:[204]		Two years later, in May 2008, when the federal minimum wage was $5.85 per hour and was about to increase to $6.55 per hour in July 2008, these same sectors were still the lowest-paying, but their situation (according to Bureau of Labor Statistics data)[205] was:		In 2006, workers in the following 13 individual occupations received, on average, a median hourly wage of less than $8.00 per hour:[204]		In 2008, only two occupations paid a median wage less than $8.00 per hour:[205]		According to the May 2009 National Occupational Employment and Wage Estimates,[206] the lowest-paid occupational sectors in May 2009 (when the federal minimum wage was $7.25 per hour) were the following:		
Wrongful dismissal, also called wrongful termination or wrongful discharge, is a legal phrase, describing a situation in which an employee's contract of employment has been terminated by the employer if the termination breaches one or more terms of the contract of employment, or a statute provision in employment law. It follows that the scope for wrongful dismissal varies according to the terms of the employment contract, and varies by jurisdiction. The absence of a formal contract of employment does not preclude wrongful dismissal in jurisdictions in which a de facto contract is taken to exist by virtue of the employment relationship. Terms of such a contract may include obligations and rights outlined in an employee handbook. Being terminated for any of the items listed below may constitute wrongful termination:		Wrongful dismissal will tend to arise first as a claim by the employee so dismissed. Many jurisdictions provide tribunals or courts which will hear actions for wrongful dismissal. A proven wrongful dismissal will tend to lead to two main remedies: reinstatement of the dismissed employee and/or monetary compensation for the wrongfully dismissed.		A related situation is constructive dismissal in which an employee feels no choice but to resign from employment for reasons imposed by the employer.		One way to avoid potential liability for wrongful dismissal is to institute an employment probation period after which a new employee is automatically terminated unless there is sufficient justification not to do so. The dismissed employee may still assert a claim, but proof will be more difficult, as the employer may have broad discretion with retaining such a temporary employee.						In the United States, there is no single “wrongful termination” law. Rather there are several state and federal laws and court decisions that define this concept. Employers typically designate their employees to be "employees at will." Even in these cases, however, it is usually deemed a "wrongful termination" to dismiss an employee on a legally prohibited basis. In the United States, wrongful dismissal has become the most common labor claim.[1]		In California if a termination was based on your membership in a group protected from discrimination by law, it would not be legal. An employer may not discriminate or terminate a person because of race, religion, national origin, gender, sexual orientation, disability, medical condition, pregnancy, or age, pursuant to the California’s Fair Employment and Housing Act (FEHA) and Title VII of the Civil Rights Act of 1964.[2]		A 2016 study found "robust evidence that one wrongful-discharge doctrine, the implied-contract exception, reduced state employment rates by 0.8% to 1.7%. The initial impact is largest for female and less-educated workers (those who change jobs frequently), while the longer-term effect is greater for older and more-educated workers (those most likely to litigate). By contrast, we find no robust employment or wage effects of two other widely recognized wrongful-discharge laws: the public-policy and goodfaith exceptions."[3]		In Canadian law, absent a written contract which addresses how to end the employment relationship, the law implies into the employment relationship a term that it will not be ended without "notice" of its termination.		Notice is advanced warning an employer must provide an employee that their employment will be terminated. It is measured in units of time. The employer has the option to provide one of two kinds of notice:		Employers must provide terminated employees the same salary during the notice period, plus any incentive compensation and benefits.[2]		Working notice is legal in Canada. Therefore, if the employee is provided a reasonable amount of working notice, the employer owes the employee no additional money.		Pay in lieu of notice, sometimes referred to as termination pay, is the amount of money the employer must pay the employee if the employer seeks to immediately terminate the employee without working notice.		Employees may be entitled to either statutory or reasonable notice, which ever is greater, but at the very minimum, must receive statutory notice. Provincial legislation such as Ontario's Employment Standards Act, delineates statutory notice by way of a formula.		Reasonable notice, on the other hand, has no formula. The common law dictates how much reasonable notice an employee is entitled to.[3] In this regard, the length of reasonable notice depends on a number of factors, best described by McRuer CJHC in the 1960 Ontario decision of Bardal v Globe & Mail:[4]		There could be no catalogue laid down as to what was reasonable notice in particular classes of cases. The reasonableness of the notice must be decided with reference to each particular case, having regard to the character of the employment, the length of the service of the servant, the age of the servant and the availability of similar employment, having regard to the experience, training and qualifications of the servant.		Notwithstanding the above, the courts are open to creative interpretations of reasonable notice. For example, if an employee was persuaded to leave a job to come to another (i.e. inducement), the courts may take that into account in calculating the employee’s length of service and thus drastically increase the notice period.[5]		The Supreme Court of Canada has significantly expanded the scope of wrongful dismissal in Canadian jurisprudence:		An employer is entitled to dismiss an employee according to the terms of the employment contract. There are oral employment contracts, and written employment contracts, and combinations of oral and written employment contracts. In Canadian common law, there is a basic distinction as to dismissals. There are two basic types of dismissals, or terminations: dismissal with cause and termination without cause. An example of cause would be an employee's behaviour which constitutes a fundamental breach of the terms of the employment contract. Where cause exists, the employer can dismiss the employee without providing any notice. If no cause exists yet the employer dismisses without providing lawful notice, then the dismissal is a wrongful dismissal. A wrongful dismissal will allow the employee to claim monetary damages in an amount that compensates the employee for the wages, commissions, bonuses, profit sharing and other such emoluments the employee would have earned or received during the lawful notice period, minus earnings from new employment obtained during the lawful notice period. In Canadian employment law, in those jurisdictions where a remedy for unjust dismissal is not available, it has long been the rule that reinstatement is not a remedy available to either the employer or the employee—damages must be paid instead.		Although Canadian employment law provides some of the above remedies, each (provincial) jurisdiction may treat employment law differently. It is important to determine which jurisdiction the employment occurs in or is regulated by, then seek appropriate legal advice relevant to that jurisdiction and its particular employment laws.				
An occupational exposure limit is an upper limit on the acceptable concentration of a hazardous substance in workplace air for a particular material or class of materials. It is typically set by competent national authorities and enforced by legislation to protect occupational safety and health. It is an important tool in risk assessment and in the management of activities involving handling of dangerous substances.[1] There are many dangerous substances for which there are no formal occupational exposure limits. In these cases, hazard banding or control banding strategies can be used to ensure safe handling.						Occupational Exposure Limits (OELs) have been established for airborne workplace chemicals by multiple regulatory and authoritative organizations around the world for well over 60 years now. With the changing regulatory arena, shifting centers of manufacturing growth, and the move towards a more global view on occupational hygiene issues, it is important for the Occupational Hygiene profession to understand the current and growing issues impacting the continued viability of OEL’s in our professional practice.[2]		Although peer-reviewed health-based OELs are preferred for establishing safe levels of exposure or for implementing adequate controls to provide worker protection, the lack of publicly available OELs have led to other sources of safe levels to protect workers. Industrial or Occupational Hygienists are often on the front line of anticipating and recognizing the hazards of chemical exposure for workers, and must assess the risk of exposure through the use of OELs so that proper control strategies can be implemented to keep workers below the OEL values. In the absence of OELs however, there are a variety of tools that can and should be used to assess exposure potential of workers. The "Hierarchy of OELs" provides a continuum of occupational exposure limit values that allow assessment of the risk of exposure in order to apply adequate controls.[3]		Personal air sampling is routinely conducted on workers to determine whether exposures are acceptable or unacceptable. These samples are collected and analyzed using validated sampling and analytical methods. These methods are available from OSHA Technical Manual and NIOSH Manual of Analytical Methods [4] Statistical tools are available to assess exposure monitoring data against OELs. The statistical tools are typically free but do require some previous knowledge with statistical concepts. A popular exposure data statistical tool called "IH STAT" is available from AIHA (American Industrial Hygiene Association). IHSTAT has 14 languages including English and is available for free.[5]		Methods for performing occupational exposure assessments can be found in "A Strategy for Assessing and Managing Occupational Exposures, Third Edition Edited by Joselito S. Ignacio and William H. Bullock".[6]		
Sick building syndrome (SBS) is a medical condition where people in a building suffer from symptoms of illness or feel unwell for no apparent reason.[1] The symptoms tend to increase in severity with the time people spent in the building, and improve over time or even disappear when people are away from the building. The main identifying observation is an increased incidence of complaints of symptoms such as headache, eye, nose, and throat irritation, fatigue, and dizziness and nausea.[2] These symptoms appear to be linked to time spent in a building, though no specific illness or cause can be identified. SBS is also used interchangeably with "building-related symptoms", which orients the name of the condition around patients rather than a "sick" building. A 1984 World Health Organization (WHO) report suggested up to 30% of new and remodeled buildings worldwide may be subject of complaints related to poor indoor air quality.[3]		Sick building causes are frequently pinned down to flaws in the heating, ventilation, and air conditioning (HVAC) systems. However, there have been inconsistent findings on whether air conditioning systems result in SBS or not.[4] Other causes have been attributed to contaminants produced by outgassing of some types of building materials, volatile organic compounds (VOC), molds (see mold health issues), improper exhaust ventilation of ozone (byproduct of some office machinery), light industrial chemicals used within, or lack of adequate fresh-air intake/air filtration (see Minimum efficiency reporting value).						Human exposure to bioaerosols has been documented to give rise to a variety of adverse health effects.[5] Building occupants complain of symptoms such as sensory irritation of the eyes, nose, or throat; neurotoxic or general health problems; skin irritation; nonspecific hypersensitivity reactions; infectious diseases;[6] and odor and taste sensations.[7] Exposure to poor lighting conditions has led to general malaise.[8]		Extrinsic allergic alveolitis has been associated with the presence of fungi and bacteria in the moist air of residential houses and commercial offices.[9] A very large 2017 Swedish study [10] correlated several inflammatory diseases of the respiration tract with objective evidence of damp-caused damage in homes.		The WHO has classified the reported symptoms into broad categories, including: mucous membrane irritation (eye, nose, and throat irritation), neurotoxic effects (headaches, fatigue, and irritability), asthma and asthma-like symptoms (chest tightness and wheezing), skin dryness and irritation, gastrointestinal complaints and more.[11]		Several sick occupants may report individual symptoms which do not appear to be connected. The key to discovery is the increased incidence of illnesses in general with onset or exacerbation within a fairly close time frame—usually within a period of weeks. In most cases, SBS symptoms will be relieved soon after the occupants leave the particular room or zone.[12] However, there can be lingering effects of various neurotoxins, which may not clear up when the occupant leaves the building. In some cases—particularly in sensitive individuals—there can be long-term health effects.		It has been suggested[by whom?] that sick building syndrome could be caused by inadequate ventilation, deteriorating fiberglass duct liners, chemical contaminants from indoor or outdoor sources, and biological contaminants, air recycled using fan coils, traffic noise, poor lighting, and buildings located in a polluted urban area.[8] Many volatile organic compounds, which are considered chemical contaminants, can cause acute effects on the occupants of a building. "Bacteria, molds, pollen, and viruses are types of biological contaminants" and can all cause SBS. In addition, pollution from outdoors, such as motor vehicle exhaust, can contribute to SBS.[3] Adult SBS symptoms were associated with a history of allergic rhinitis, eczema and asthma.[13]		A 2015 study concerning the association of SBS and indoor air pollutants in office buildings in Iran found as CO2 levels increase in a building, symptoms like nausea, headaches, nasal irritation, dyspnea, and throat dryness have also been shown to increase.[8] Certain work conditions have been found to be correlated with specific symptoms.  For example, higher light intensity was significantly related to skin dryness, eye pain, and malaise.[8] Higher temperature has also been found to correlate with symptoms such as sneezing, skin redness, itchy eyes and headache, while higher relative humidity has been associated with sneezing, skin redness, and pain of the eyes.[8]		ASHRAE has recognized that polluted urban air, designated within the United States Environmental Protection Agency (EPA)´s air quality ratings as unacceptable requires the installation of treatment such as filtration for which the HVAC practitioners generally apply carbon impregnated filters and their like.		In 1973, in response to the 1973 oil crisis and conservation concerns, ASHRAE Standards 62-73 and 62-81 reduced required ventilation from 10 CFM (4.76 L/S) per person to 5 CFM (2.37 L/S) per person, but this was found to be a contributing factor to sick building syndrome.[14] As of the 2016 revision, ASHRAE ventilation standards call for 5 to 10 CFM of ventilation per occupant (depending on the occupancy type) in addition to ventilation based on the zone floor area delivered to the breathing zone.[15]		One study looked at commercial buildings and their employees, comparing some environmental factors suspected of inducing SBS to a self-reported survey of the occupants,[16] finding that the measured psycho-social circumstances appeared more influential than the tested environmental factors.[17] The list of environmental factors in the study can be found here.[18] Limitations of the study include that it only measured the indoor environment of commercial buildings, which have different building codes than residential buildings, and that the assessment of building environment was based on layman observation of a limited number of factors.		Research has shown that SBS shares several symptoms common in other conditions thought to be at least partially caused by psychosomatic tendencies. The umbrella term 'autoimmune/inflammatory syndrome induced by adjuvants' has been suggested. Other members of the suggested group include Silicosis, Macrophagic myofascitis, The Gulf War syndrome, Post-vaccination phenomena.[19]		Greater effects were found with features of the psychosocial work environment including high job demands and low support. The report concluded that the physical environment of office buildings appears to be less important than features of the psychosocial work environment in explaining differences in the prevalence of symptoms. However, there is still a relationship between sick building syndrome and symptoms of workers regardless of workplace stress.[20]		Excessive work stress or dissatisfaction, poor interpersonal relationships and poor communication are often seen to be associated with SBS, recent studies show that a combination of environmental sensitivity and stress can greatly contribute to sick building syndrome.		Specific work-related stressors are related with specific SBS symptoms. Workload and work conflict are significantly associated with general symptoms (headache, abnormal tiredness, sensation of cold or nausea). While crowded workspaces and low work satisfaction are associated with upper respiratory symptoms.[21]		Engineers are often affected by Sick Building Syndome. One studied case is that of Stephen Danielson, who typically has the ailment for 6 months out of the year. It manifests as a wheeze, commonly known as the Danielson Wheeze.[22]		Specific careers are also associated with specific SBS symptoms. Transport, communication, healthcare, and social workers have highest prevalence of general symptoms. Skin symptoms such as eczema, itching, and rashes on hands and face are associated with technical work. Forestry, agriculture, and sales workers have the lowest rates of sick building syndrome symptoms.[23]		Milton et al. determined the cost of sick leave specific for one business was an estimated $480 per employee, and about five days of sick leave per year could be attributed to low ventilation rates. When comparing low ventilation rate areas of the building to higher ventilation rate areas, the relative risk of short-term sick leave was 1.53 times greater in the low ventilation areas.[24]		Work productivity has been associated with ventilation rates, a contributing factor to SBS, and there's a significant increase in production as ventilation rates increase, by 1.7% for every two-fold increase of ventilation rate.[25]		Sick building syndrome can also occur due to factors of the home.  Laminated flooring can cause more exposure to chemicals and more resulting SBS symptoms compared to stone, tile, and cement flooring.[13] Recent redecorating and new furnishings within the last year were also found to be associated with increased symptoms, along with dampness and related factors, having pets, and the presence of cockroaches.[13] The presence of mosquitoes was also a factor related to more symptoms, though it is unclear if it was due to the presence of mosquitoes or the use of repellents.[13]		While sick building syndrome (SBS) encompasses a multitude of non-specific symptoms, building-related illness (BRI) comprises specific, diagnosable symptoms caused by certain agents (chemicals, bacteria, fungi, etc.). These can typically be identified, measured, and quantified.[26] There are usually 4 causal agents in BRI; 1.) Immunologic, 2.) Infectious, 3.) toxic, and 4.) irritant.[26] For instance, Legionnaire's disease, usually caused by Legionella pneumophila, involves a specific organism which could be ascertained through clinical findings as the source of contamination within a building. SBS does not have any known cure; alleviation consists of removing the affected person from the building associated with non-specific symptoms. BRI, on the other hand, utilizes treatment appropriate for the contaminant identified within the building (e.g., antibiotics for Legionnaire's disease). In most cases, simply improving the indoor air quality (IAQ) of a particular building will attenuate, or even eliminate, the acute symptoms of SBS, while removal of the source contaminant would prove more effective for a specific illness, as in the case of BRI.[27] Building-Related Illness is vital to the overall understanding of Sick Building Syndrome because BRI illustrates a causal path to infection, theoretically. Office BRI may more likely than not be explained by three events: “Wide range in the threshold of response in any population (susceptibility), a spectrum of response to any given agent, or variability in exposure within large office buildings."[28] Isolating any one of the three aspects of office BRI can be a great challenge, which is why those who find themselves with BRI should take three steps, history, examinations, and interventions. History describes the action of continually monitoring and recording the health of workers experiencing BRI, as well as obtaining records of previous building alterations or related activity. Examinations go hand in hand with monitoring employee health. This step is done by physically examining the entire workspace and evaluating possible threats to health status among employees. Interventions follow accordingly based off the results of the Examination and History report.[28]		Some studies have shown a small difference between genders, with women having slightly higher reports of SBS symptoms compared to men.[13] However, many other studies have shown an even higher difference in the report of sick building syndrome symptoms in women compared to men.[8] It is not entirely clear, however, if this is due to biological, social, or occupational factors.		A 2001 study published in the Journal Indoor Air 2001 gathered 1464 office-working participants to increase the scientific understanding of gender differences under the Sick Building Syndrome phenomenon.[30] Using questionnaires, ergonomic investigations, building evaluations, as well as physical, biological, and chemical variables, the investigators obtained results that compare with past studies of SBS and gender. The study team found that across most test variables, prevalence rates were different in most areas, but there was also a deep stratification of working conditions between genders as well. For example, men’s workplace tend to be significantly larger and have all around better job characteristics.  Secondly, there was a noticeable difference in reporting rates, finding that women have higher rates of reporting roughly 20% higher than men. This information was similar to that found in previous studies, indicating a potential difference in willingness to report.[30]		There might be a gender difference in reporting rates of sick building syndrome because women tend to report more symptoms than men do. Along with this, some studies have found that women have a more responsive immune system and are more prone to mucosal dryness and facial erythema. Also, women are alleged by some to be more exposed to indoor environmental factors because they have a greater tendency to have clerical jobs, wherein they are exposed to unique office equipment and materials (example: blueprint machines), whereas men often have jobs based outside of offices.[31]		In the late 1970s, it was noted that nonspecific symptoms were reported by tenants in newly constructed homes, offices, and nurseries. In media it was called "office illness". The term "Sick Building Syndrome" was coined by the WHO in 1986, when they also estimated that 10-30% of newly built office buildings in the West had indoor air problems. Early Danish and British studies reported symptoms.		Poor indoor environments attracted attention. The Swedish allergy study (SOU 1989:76) designated "sick building" as a cause of the allergy epidemic as was feared. In the 1990s, therefore, extensive research into "sick building" was carried out. Various physical and chemical factors in the buildings were examined on a broad front.		The problem was highlighted increasingly in media and was described as a "ticking time bomb". Many studies were performed in individual buildings.		In the 1990s "sick buildings" were contrasted against "healthy buildings". The chemical contents of building materials were highlighted. Many building material manufacturers were actively working to gain control of the chemical content and to replace criticized additives. The ventilation industry advocated above all more well-functioning ventilation. Others perceived ecological construction, natural materials, and simple techniques as a solution.		At the end of the 1990s came an increased distrust of the concept of "sick building". A dissertation at the Karolinska Institutet in Stockholm 1999 questioned the methodology of previous research, and a Danish study from 2005 showed these flaws experimentally. It was suggested that sick building syndrome was not really a coherent syndrome and was not a disease to be individually diagnosed. In 2006 the Swedish National Board of Health and Welfare recommended in the medical journal Läkartidningen that "sick building syndrome" should not be used as a clinical diagnosis. Thereafter, it has become increasingly less common to use terms such as "sick buildings" and "sick building syndrome" in research. However, the concept remains alive in popular culture and is used to designate the set of symptoms related to poor home or work environment engineering. "Sick building" is therefore an expression used especially in the context of workplace health.		Sick building syndrome made a rapid journey from media to courtroom where professional engineers and architects became named defendants and were represented by their respective professional practice insurers. Proceedings invariably relied on expert witnesses, medical and technical experts along with building managers, contractors and manufacturers of finishes and furnishings, testifying as to cause and effect. Most of these actions resulted in sealed settlement agreements, none of these being dramatic. The insurers needed a defense based upon Standards of Professional Practice to meet a court decision that declared—that in a modern, essentially sealed building, the HVAC systems must produce breathing air for suitable human consumption. ASHRAE (American Society of Heating, Refrigeration and Air Conditioning Engineers, currently with over 50,000 international members) undertook the task of codifying its IAQ (Indoor Air Quality) standard.		ASHRAE empirical research determined that "acceptability" was a function of outdoor (fresh air) ventilation rate and used carbon dioxide as an accurate measurement of occupant presence and activity. Building odors and contaminants would be suitably controlled by this dilution methodology. ASHRAE codified a level of 1,000 ppm of carbon dioxide and specified the use of widely available sense-and-control equipment to assure compliance. The 1989 issue of ASHRAE 62.1-1989 published the whys and wherefores and overrode the 1981 requirements that were aimed at a ventilation level of 5,000 ppm of carbon dioxide, (the OAHA workplace limit), federally set to minimize HVAC system energy consumption. This apparently ended the SBS epidemic.		Over time, building materials changed with respect to emissions potential. Smoking vanished and dramatic improvements in ambient air quality, coupled with code compliant ventilation and maintenance, per ASHRAE standards have all contributed to the acceptability of the indoor air environment. With the publication of ASHRAE 62.1-2013 ASHRAE has reactivated 1981 with respect to ventilation rates. Only time and the courts will tell how right, or wrong ASHRAE is.[32][33]		
Structural unemployment is a form of unemployment caused by a mismatch between the skills that workers in the economy can offer, and the skills demanded of workers by employers (also known as the skills gap). Structural unemployment is often brought about by technological changes that make the job skills of many of today's workers obsolete.		Structural unemployment is one of several major categories of unemployment distinguished by economists, including frictional unemployment, cyclical unemployment, involuntary unemployment, and classical unemployment.		Because it requires either migration or re-training, structural unemployment can be long-term and slow to fix.[1]						From an individual perspective, structural unemployment can be due to:		From a larger perspective, there can be a number of reasons for structural unemployment across large numbers of workers:		Large-scale changes in the economy can be particularly challenging. For example, if a large company is the only employer in a given industry for a certain city, when it closes workers will have no competing company to move to, and the local education system and government will be burdened with many people who need job re-training all at once (possibly at the same time the local economy fails to create new jobs due to decreased overall demand).		Employers may also reject workers for reasons unrelated to skills or geography, so for example structural unemployment can also result from discrimination.		While temporary changes in overall demand for labor cause cyclical unemployment, structural unemployment can be caused by temporary changes in demand from different industries. For example, seasonal unemployment often affects farm workers after harvesting is complete, and workers in resort towns after the tourist season ends. The dot-com bubble caused a temporary spike in demand for information technology workers, which was suddenly reversed in 2000-2001.		Structural unemployment is often associated with workers being unable to shift from industry to industry, but it can also happen within industries as technology changes the nature of work within a given field.[3][4] This is a driver of skills gaps as technology and globalization "hollow out" many middle-skill jobs, positions that traditionally have not required a college degree.[5]		Structural unemployment is hard to separate empirically from frictional unemployment, except to say that it lasts longer. As with frictional unemployment, simple demand-side stimulus will not work to easily abolish this type of unemployment.		Seasonal unemployment may be seen as a kind of structural unemployment, since it is a type of unemployment that is linked to certain kinds of jobs (construction work, migratory farm work). The most-cited official unemployment measures erase this kind of unemployment from the statistics using "seasonal adjustment" techniques.		Structural unemployment may also be encouraged to rise by persistent cyclical unemployment: if an economy suffers from long-lasting low aggregate demand, it means that many of the unemployed become disheartened, while their skills (including job-searching skills) become "rusty" and obsolete.[6] Problems with debt may lead to homelessness and a fall into the vicious circle of poverty. This means that they may not fit the job vacancies that are created when the economy recovers. The implication is that sustained high demand may lower structural unemployment.[7][8] This theory of persistence in structural unemployment has been referred to as an example of path dependence or "hysteresis."		There has been considerable debate over how much a role structural unemployment plays in the persistently high unemployment rates seen in much of the world since the 2007-09 global recession. Narayana Kocherlakota, then president of the Federal Reserve Bank of Minneapolis, said in a 2010 speech that as much as 3 percent of the 9.5 percent unemployment rate at the time could be the result of a mismatch.[10] Other studies argued that a skills mismatch was a minor factor, since unemployment rose for nearly all industries and demographic groups during the "Great Recession."[11] A Federal Reserve Bank of New York study found no strong evidence of mismatch for construction workers, a group often prone to structural unemployment because of the regional nature of construction.[12]		Some economists posit that the minimum wage is in part to blame for structural unemployment, although structural unemployment does exist even in the absence of a minimum wage. They assert that because the governmentally imposed minimum wage is higher than some individuals' marginal revenue product in any given job, that those individuals remain unemployed because employers legally cannot pay them what they are "worth."[13] Others believe that in such cases (for example, when a person is intellectually disabled or suffers a debilitating physical condition) it is the responsibility of the state to provide for the citizen in question. When a minimum wage does not exist, more people may be employed, but they may be underemployed and thus unable to fully provide for themselves.		Management professor Peter Cappelli blames poor human resource practices for complaints that not enough qualified job applicants are found, such as replacing skilled HR workers with software that is less capable of matching resumes that exhibit the right combination of skills but without word-for-word alignment with a job posting. (This actually may be a form of frictional unemployment if a match will eventually be made, perhaps with a different employer.) Cappelli also points to a decrease in apprenticeships and hiring from within an organization. Instead, companies attempt to avoid the time and cost of on-the-job training by hiring people from who already have experience doing the same job elsewhere (including at a competitor).[14]		
A resignation is the formal act of giving up or quitting one's office or position. A resignation can occur when a person holding a position gained by election or appointment steps down, but leaving a position upon the expiration of a term is not considered resignation. When an employee chooses to leave a position, it is considered a resignation, as opposed to involuntary termination, which occurs when the employee involuntarily loses a job. Whether an employee resigned or was terminated is sometimes a topic of dispute, because in many situations, a terminated employee is eligible for severance pay and/or unemployment benefits, whereas one who voluntarily resigns may not be eligible. Abdication is the equivalent of resignation of a reigning monarch or pope, or other holder of a non-political, hereditary or similar position.		A resignation is a personal decision to exit a position, though outside pressure exists in many cases. For example, Richard Nixon resigned from the office of President of the United States in August 1974 following the Watergate scandal, when he was almost certain to have been impeached by the United States Congress.		Resignation can be used as a political manoeuvre, as in the Philippines in July 2005, when ten cabinet officials resigned en masse to pressure President Gloria Macapagal Arroyo to follow suit over allegations of electoral fraud. Arroyo's predecessor, Joseph Estrada, was successfully forced out of office during the EDSA Revolution of 2001 as he faced the first impeachment trial held in the country's history.		In 1995, the British Prime Minister, John Major, resigned as Leader of the Conservative Party in order to contest a leadership election with the aim of silencing his critics within the party and reasserting his authority. Having resigned, he stood again and was re-elected. He continued to serve as prime minister until he was defeated in 1997 elections.		Although government officials may tender their resignations, they are not always accepted. This could be a gesture of confidence in the official, as with US President George W. Bush's refusal of his Secretary of Defense Donald Rumsfeld's twice-offered resignation during the Abu Ghraib prison abuse scandal.		However, refusing a resignation can be a method of severe censure if it is followed by dismissal; Alberto Fujimori attempted to resign as President of Peru, but his resignation was refused in order that Congress could impeach him.		For many public figures, primarily departing politicians, resignation is an opportunity to deliver a valedictory resignation speech in which they can elucidate the circumstances of their exit from office and in many cases deliver a powerful speech which often commands much attention. This can be used to great political effect, particularly as, subsequent to resigning, government ministers are no longer bound by collective responsibility and can speak with greater freedom about current issues.[citation needed]		In academia, a university president or the editor of a scientific journal may also resign, particularly in cases where an idea which runs counter to the mainstream is being promoted. In 2006, Harvard president Lawrence Summers resigned after making the provocative suggestion that the underrepresentation of female academics in math and science [1] could be due to factors other than sheer discrimination, such as personal inclination or innate ability.		In a club, society, or other voluntary association, a member may resign from an officer position in that organization or even from the organization itself. In Robert's Rules of Order, this is called a request to be excused from a duty.[1] A resignation may also be withdrawn.[2]		
Cooperative education (or co-operative education) is a structured method of combining classroom-based education with practical work experience. A cooperative education experience, commonly known as a "co-op", provides academic credit for structured job experience. Cooperative education is taking on new importance in helping young people to make the school-to-work transition. Cooperative learning falls under the umbrella of work-integrated learning (alongside internships, service learning and clinical placements) but is distinct as it alternates a school term with a work term in a structured manner, involves a partnership between the academic institution and the employer, and generally is both paid and intended to advance the education of the student.[1]						While at Lehigh University at the beginning of the 20th century, Herman Schneider (1872–1939), engineer, architect, and educator, concluded that the traditional learning space or classroom was insufficient for technical students (Smollins 1999). Schneider observed that several of the more successful Lehigh graduates had worked to earn money before graduation. Gathering data through interviews of employers and graduates, he devised the framework for cooperative education (1901). About that time, Carnegie Technical Schools, now Carnegie Mellon University, opened and thereby minimized the need for Schneider's co-op plan in the region around Lehigh University. However, in 1903 the University of Cincinnati appointed Schneider to their faculty. In 1905 the UC Board of Trustees allowed Schneider to "try this cooperative idea of education for one year only, for the failure of which they would not be held responsible". The cooperative education program was launched in 1906, and became an immediate success. The University of Cincinnati returned to the matter in its September 2005 board meeting, declaring the 100-year trial period of one hundred years of Cooperative Education officially ended, for the success of which the Board resumed full responsibility.		Schneider, beginning from the rank of Assistant Professor, would rise through the rank of Dean of Engineering (1906–1928) to become Interim President (1929–32) of the University of Cincinnati, based largely upon the strength of the co-op program. Throughout his career, he was an advocate for the co-op framework. His thirty years of service to the University of Cincinnati are partly credited for that institution's worldwide fame. In 2006 the University of Cincinnati unveiled a statue of dean Schneider outside the window of his office in Baldwin Hall.		In 1965, The Cooperative Education and Internship Association (CEIA) created "The Dean Herman Schneider Award" in honor of the contributions made by Dean Schneider in cooperative education. The award is given annually to an outstanding educator from faculty or administration. In 2006 The University of Cincinnati established the Cooperative Education Hall of Honor "to give a permanent place of honor to individuals and organizations that have made a significant qualitative difference in the advancement of Cooperative Education for the benefit of students".		In 1909, seeing the possibility of co-op education, Northeastern University began using co-op in their engineering program, becoming only the second institution in America to do so. By 1921, Antioch College had adapted the co-op practices to their liberal arts curricula, for which reason many called co-op the "Antioch Plan." In 1919 the General Motors Institute (GMI) was opened following this model to train new General Motors hires. This school was later renamed Kettering University.[2]		The Drexel University four-year co-op program launched in the College of Engineering in 1919 with the participation of just three academic majors. This stemmed from the University’s founder Anthony J. Drexel's belief that Drexel University should prepare its men and women for successful careers through an education that balanced classroom theory with real world practice. In 1925, the five-year co-op program took hold in the chemical engineering department, which would later become the foundation of Drexel's cooperative education program. Today, the cooperative education program supports students of more than 75 different disciplines, making it one of the largest programs in the nation.		In 1922, Northeastern University emphasized its commitment to co-op by extending it to the College of Business Administration. As new colleges opened at Northeastern, such as the College of Liberal Arts (1935) and College of Education (1953), they became co-op schools as well. By the 1980s, Northeastern was the acknowledged leader in co-op education across the world.(Smollins 1999)		In 1926, Dean Schneider invited those interested in forming an Association of Co-operative Colleges (ACC) to the University of Cincinnati for the first convention. The idea took hold, and was followed by three more annual conventions. In 1929, the Society for the Promotion of Engineering Education, now called American Society for Engineering Education (ASEE), formed the Division of Cooperative Engineering Education, incorporating the membership of the ACC (Auld 1972).		In 1957, the first Canadian co-operative education program began at the University of Waterloo with an inaugural class of 75. This program was seen as a joke and was not expected to succeed, however it quickly became a model for other co-op programs across Canada. These programs were based on both the sandwich education model popularized in Britain and the new American co-op programs. Canadian co-op programs generally follow a four-month school system interspersed with four month work terms. This common system allows employers to hire students from multiple institutions with common timelines and training programs.[3]		In 1961, the Ford and Edison Foundations commissioned a study of co-operative education, published as Work-study college programs; appraisal and report of the study of cooperative education, (James Warner Wilson and Edward H Lyons, New York: Harper). That study resulted in the formation of the National Commission for Cooperative Education (NCCE). NCCE remains today to promote and lobby for co-operative education in the United States. Its membership comprises sponsoring corporations and organizations (not individuals) from academia and business.		Within Canada, the need for connections between co-op programs became clear by 1973. The Canadian Association for Co-operative Education (CAFCE) began with 29 educators from 15 institutions. In its first form, it did not include any employers or industry representatives. The institutions felt that they should decide on an integrative plan for co-op education prior to admitting employers as members. In 1977, employers, HR representatives and recruiters began to join CAFCE.[3]		By 1962, about 150 academic institutions used co-op education, in one form or another. Many were outside of engineering. The need for professional support of non-engineering programs became obvious, and the membership of ASEE, in 1963, began the Cooperative Education Association. To reflect its membership more accurately, it was eventually (sometime in the 1990s or early 2000s) named the Cooperative Education and Internship Association, it remains today as the professional association for co-operative education outside of ASEE.		Much of those early efforts of NCCE focused on lobbying and promoting co-operative education. In 1965, the federal Higher Education Act provided support specifically for co-operative education. Funding continued from the federal government through 1992, when Congress ended its support of co-operative education. In all, a total of over $220 million was appropriated by the federal government toward co-operative education.(Carlson 1999)		In Canada, regulation of co-operative education programs is overseen by CAFCE. Programs can apply for accreditation after the first class of co-op students has graduated. In order to be accredited, 30% of time spent in the program must be devoted to work experience, and each experience must last at least 12 weeks.[4]		In 1979, educators from Australia, Britain, Canada, and the United States (Northeastern's President, Kenneth Ryder), met to discuss work-related programs in their respective countries. In 1981 and 1982, this group, headed by President Ryder, convened an international conference on cooperative education. In 1983, several college and university presidents, educational specialists, and employers from around the world (including Australia, Canada, Hong Kong, the Netherlands, the Philippines, the United States and the United Kingdom) formed the World Council and Assembly on Cooperative Education to foster co-operative education around the world. In 1991, it renamed itself the World Association for Cooperative Education (WACE). By 2005, that Association boasted a membership of over 1,000 individuals from 43 different countries.		Co-operative education is common in most Australian high schools, and has been integrated into many university courses as a part of making up final grades. Australian institutions often refer to co-operative education as Work Placement, VET or Prac.[5][6] All of these involve students going out into their field of choice and joining that field of a set number of weeks in unpaid work. This unpaid work goes towards credits for graduation in both school and universities Australia wide.[7] The Australian government have been funding this programme due to the success in highly regarded applicants that have come from doing the work placement. Many companies in Australia are more inclined to hire an individual who has had proper training within their specific field than those who have not, which has created many more successful applicants and jobs within Australia.[8]		From its beginnings in Cincinnati in 1906, cooperative education has evolved into a program offered at the secondary and post-secondary levels in two predominant models (Grubb & Villeneuve 1995). In one model, students alternate a semester of academic coursework with an equal amount of time working, repeating this cycle several times until graduation. The parallel method splits the day between school and work, typically structured to accommodate the student's class schedule. Thus, like school-to-work (STW), the co-op model includes school-based and work-based learning and, in the best programs, "connecting activities" such as seminars and teacher-coordinator work site visits. These activities help students explicitly connect work and learning.		Other models, such as the sandwich model and the American-style semester model instead have students work a 40-hour work week for a set amount of time, typically between 12 weeks and six months. After this period is over, students return to the classroom for an academic semester after which they may have another work term. This cycle often repeats multiple times, adding a year or more to the students’ university career. In this model, students’ do not receive a summer break from school but instead are either working or in school for 12 months of the year.[3] Before or during this work experience students may complete activities designed to maximize their learning on the job, such as online workplace conduct courses or reflective activities.[9]		Co-op's proponents identify benefits for students (including motivation, career clarity, enhanced employability, vocational maturity) and employers (labor force flexibility, recruitment/retention of trained workers, input into curricula) as well as educational institutions and society (ibid.). Beyond informal and anecdotal evidence, however, a familiar refrain in the literature is the lack of well-done research that empirically demonstrates these benefits (Barton 1996; Wilson, Stull & Vinsonhaler 1996). Barton (1996) identifies some of the research problems for secondary co-op as follows: federal data collection on high school co-op enrollments and completions ceased in the 1980s; some studies use data in which co-op was not isolated from other work experience programs. Ricks et al. (1993) describe other problems: due to lack of a clear or consistent definition of cooperative education, researchers cannot accurately identify variables and findings cannot be compared; theory is not well developed; theory, research, and practice are not integrated; and co-op research does not adhere to established standards.		Another set of problems involves perceptions of the field and its marginalization. Because of its "vocational" association, co-op is not regarded as academically legitimate; rather, it is viewed as taking time away from the classroom (Crow 1997). Experiential activities are necessarily rewarded in post-secondary promotion and tenure systems (except in certain extenuating situations), and co-op faculty may be isolated from other faculty (Crow 1997; Schaafsma 1996). Despite the current emphasis on contextual learning, work is not recognized as a vehicle for learning (Ricks et al. 1993). Schaafsma (1996) and Van Gyn (1996) agree that the field places too much emphasis on placements rather than learning. Wilson, Stull & Vinsonhaler (1996) also decry the focus on administration, logistics, placements, and procedures.		Some institutions are fully dedicated to the co-op ideal (such as Northeastern University, Drexel University, Georgia Institute of Technology, RIT, Kettering University, LaGuardia Community College, and Purdue University). In others, the co-op program may be viewed as an add-on and therefore is vulnerable to cost cutting (Wilson, Stull & Vinsonhaler 1996). Even where co-op programs are strong they can be threatened, as at Cincinnati Technical College when it became a comprehensive community college (Grubb & Villeneuve 1995) or LaGuardia during a budget crisis (Grubb & Badway 1998). For students, costs and time to degree completion may be deterrents to co-op participation (Grubb & Villeneuve 1995). Other deterrents may include financial barriers, aversion to moving frequently due to family obligations or other pressures as well as difficulty managing the job search during a school semester.		Despite these problems, there is optimism about the future of co-op education; "Social, economic, and historic forces are making cooperative education more relevant than ever" (Grubb & Villeneuve 1995, p. 17), including emphasis on university-industry-government cooperation, a fluid and demanding workplace, new technology, the need for continuous on-the-job learning, globalization, and demands for accountability (John, Doherty & Nichols 1998). Federal investments in school-to-work and community service have resulted in a number of initiatives designed to provide "learning opportunities beyond the classroom walls" (Furco 1996, p. 9). Because this has always been a principle of co-op, the field is in a position to capitalize on its strengths and the ways it complements other experiential methods in the effort to provide meaningful learning opportunities for students. To do this, however, cooperative education must be redesigned.		For Wilson, Stull & Vinsonhaler (1996), a new vision involves conceiving, defining, and presenting co-op "as a curriculum model that links work and academics - a model that is based on sound learning theory" (p. 158). Ricks (1996) suggests affirming the work-based learning principles upon which co-op is based. These principles assert that cooperative education fosters self-directed learning, reflective practice, and transformative learning; and integrates school and work learning experiences that are grounded in adult learning theories.		Fleming (2013)[10] suggests that a new practical and research focus should be on the relationship between educational institutions and employers - institutions should take more initiative when it comes to training supervisors to be effective mentors. This would maximize the success of the work term and the amount the student learns, while also increasing the quality and quantity of the students' work. Drewery and Pretti (2015) echo this as they call for greater attention on the relationship between the student and the supervisor, explaining that this relationship can greatly impact the students' satisfaction with the co-op term and the benefits they gain from it.[11]		Schaafsma (1996) also focuses on learning, seeing a need for a paradigm shift from content learning to greater understanding of learning processes, including reflection and critical thinking. Co-op is an experiential method, but learning from experience is not automatic. Therefore, Van Gyn (1996) recommends strengthening the reflective component that is already a part of some co-op models. "If co-op is only a vehicle for experience to gain information about the workplace and to link technical knowledge with workplace application, then its effectiveness is not fully developed" (Van Gyn 1996, p. 125). A Higher Education Council of Ontario paper reviewing the University of Waterloo's PD programs states that the reflective element of the program is one of the main strengths, as it encourages students to review their own experiences and learn from their work terms.[12] Maureen Drysdale suggests in a 2012 paper that the reflective elements of co-op allow students to increase their career and personal clarity relative to non-co-op students.[13]		The Bergen County Academies, a public magnet high school in New Jersey, utilizes co-op education in a program called Senior Experience. This program allows all 12th grade students to participate in cooperative education or an internship opportunity for the full business day each Wednesday. Students explore a wide range of career possibilities. This new approach was recognized as an educational best practice and has been adopted as a state educational initiative for 12th grade students.		Although there are many benefits to the co-operative education programme, there are some downsides. The negative implications do not fully compromise the number of students undertaking the study, but rather how the programme will affect the government's future funding for education.[2] A huge burden that co-operative education brings to the education institution is financial struggles. The financial struggles come from the schools and universities who put pressure on the departments of education for funding to keep the programme going.[2]		Implications directly to the students who participate in co-operative education is mainly based on direct learning at their institution, whether it is school or university. The co-operative education programme takes students away from school or university. As a student misses a consecutive number of school days, they can start to fall behind in school work and will eventually be unable to cope with their workload.[14] For students who attend school and also participate in the co-operative education programme, commonly called Work Placement or VET courses, they are no longer eligible to be granted direct entry into university. This then gives the student an option of TAFE entry, a university certified bridging course or go on to full-time work after completion of graduation.		School-to-work and service learning have also been promoted as ways to link theory and practice through meaningful experiential learning experiences. Furco (1996) outlines the similarities between school-to-work and service learning. Although school-to-work, service learning, and co-op have different goals, each of his points also applies to cooperative education:		The Community Service Scholarship Program at California State University-Fresno combines cooperative education with service learning. Students receive co-op/internship credit and scholarships for completing a placement at a community service site (Derousi & Sherwood 1997). As in traditional co-op work placements, students get real-world training, opportunities to explore career options, and enhanced employability skills such as communication, problem solving, and leadership as well as awareness of community and social problems. Combining co-op and service learning thus prepares students for roles as workers and citizens.		Research on highly successful co-op programs in Cincinnati (Grubb & Villeneuve 1995) and at LaGuardia Community College (Grubb & Badway 1998) shows that they share the basic philosophy and fundamental characteristics of the educational strategy of school-to-work. The reconceptualization of co-op should recognize and build upon this connection. At the same time, lessons from successful co-op programs can benefit the broader STW movement.		There is a need for broader definition of acceptable models for integrating work and learning. Barton (1996) and Wilson, Stull & Vinsonhaler (1996) identify a variety of work-based learning activities taking different names: co-op, internships, externships, apprenticeship, career academies, etc. Work-based learning programs should look for connections and develop collaborative relationships. The alternating and parallel co-op models may not meet the needs of returning adult students and dislocated workers needing retraining (Varty 1994). Alternatives such as extended-day programs emphasizing mentoring should be considered.		Connecting activities to integrate school- and work-based learning are an essential part of STW. At LaGuardia, the required co-op seminar helps students make connections by giving them a structure within which to reinforce employability skills, examine larger issues about work and society, and undertake the crucial activities of critical reflection (Grubb & Badway 1998).		Grubb & Badway (1998) and Grubb & Villeneuve (1995) found that the value of cooperative education is embedded in the culture of the institution (LaGuardia) and the region (Cincinnati). In this supportive culture, employer support does not have to be repeatedly obtained and there are clearly understood long-term expectations on all sides (schools, employers, students). This "informal culture of expectations around work-based learning may be more powerful in the long run than a complex set of regulations and bureaucratic requirements" (Grubb & Villeneuve 1995, p. 27).		However, even LaGuardia has found it difficult to sustain co-op culture over time (Grubb & Badway 1998). "The only way in which STW programs can find a permanent place in schools and colleges is for the work-based component to become so central to the educational purposes of the institutions that it becomes as unthinkable to give it up as it would be to abandon math, English, or science" (Grubb & Badway 1998, p. 28).		Finn (1997) believes that the answer lies in going beyond reconceiving co-op as an "educational strategy, pedagogy, model, methodology, or curriculum" (Finn 1997, p. 41). She asserts that it is time for cooperative education to develop and define its body of knowledge, investigate its unique phenomena-e.g., the concept of learning from experience, and clarify and strengthen the qualifications of co-op practitioners. For Ricks (1996), cooperative education is inherently committed to improving the economy, people's working lives, and lifelong learning abilities. It can thus position itself to serve the experiential learning needs of students into the 21st century.		Cates and Cedercreutz (2008) demonstrate that the assessment of student work performance as pursued by co-op employers, can be used for continuous improvement of curricula. The methodology, funded by the Fund for Postsecondary Education (FIPSE) has been developed to a level allowing institutionalization. The methodology could, when implemented over a larger front, provide a substantial competitive advantage for the entire field.		This article incorporates text from the ERIC Digests article "New Directions for Cooperative Education" by Sandra Kerka, a publication in the public domain.		
Further education (often abbreviated FE) in the United Kingdom and Ireland, similar to continuing education in the United States, is education (in addition to that received at secondary school), that is distinct from the higher education offered in universities and also some FE colleges. It may be at any level in compulsory secondary education, from entry to higher level qualifications such as awards, certificates, diplomas and other vocational, competency-based qualifications (including those previously known as NVQ/SVQs) through awarding organisations including City and Guilds, Edexcel (BTEC) and OCR. In addition, HE qualifications are often provided such as HNC, HND, Foundation Degree or PGCE. FE colleges are also a large provider of apprenticeships, where most of the training takes place in the apprentices' workplace with some day release into college.		A distinction is usually made between FE and higher education (HE), an education at a higher level than secondary school, usually provided in distinct institutions such as universities but also provided in some FE colleges. FE in the United Kingdom is usually a means to attain an intermediate, advanced or follow-up qualification necessary to progress into HE, or begin a specific career path, e.g. accountant, engineer or veterinary surgeon, for anyone over 16, primarily available at colleges of Further Education, work-based learning, or adult and community learning institutions.						Colleges in England are incorporated under the Further and Higher Education Act 1992. These include:		Colleges are primarily covered by the Department for Education (DfE). Until July 2016, colleges were also covered by the Department for Business, Innovation and Skills (BIS). With the abolition of BIS and formation of the Department for Business, Energy and Industrial Strategy (BEIS) on 14 July 2016, responsibility for FE colleges moved to DfE.[1] The regulatory body for sixth form colleges was already DfE prior to the 2016 changes.		Funding for colleges is provided through the Education Funding Agency (EFA) (for students up to 18 years old, and those with learning difficulties/disabilities up to 24 years old[2]) and the Skills Funding Agency (SFA) (for students aged 19 and over).		All colleges and FE providers are subject to inspection by Ofsted which monitors the quality of provision in publicly funded institutions in England and Wales.		Colleges in England are represented by the Association of Colleges.		Further education in Northern Ireland is provided through six multi-campus colleges [1]. Northern Ireland's Department for Employment and Learning has the responsibility for providing FE in the province.		Most secondary schools also provide a Sixth Form scheme whereby a student can choose to attend said school for 2 additional years to complete their AS and A-levels.		Scotland's further education colleges provide education for those young people who follow a vocational route after the end of compulsory education at age 16. They offer a wide range of vocational qualifications to young people and older adults, including vocational, competency-based qualifications (previously known as SVQs), Higher National Certificates and Higher National Diplomas. Frequently, the first two years of higher education, usually in the form of an HND can be taken in an FE college, followed by attendance at university.		Further education in Wales is provided through:		Further education in Wales comes under the remit of the Welsh Assembly Government and was formerly funded by ELWa before its merger with the Assembly.		The FE education in the Republic of Ireland is similar to that offered in the UK. Typical areas include apprenticeships and other vocational qualifications in many disciplines, such as childcare, farming, retail, and tourism. There are many different types of further education awards, known as Post Leaving Certificates.		Further education has expanded immensely in recent years helped by the institutions and their relationships with their communities. Quality and Qualifications Ireland (QQI), which was established on November 6, 2012,[3] is the regulatory for FE qualifications.		
The cursus honorum (Latin: "course of offices") was the sequential order of public offices held by aspiring politicians in both the Roman Republic and the early Roman Empire. It was designed for men of senatorial rank. The cursus honorum comprised a mixture of military and political administration posts. Each office had a minimum age for election. There were minimum intervals between holding successive offices and laws forbade repeating an office.[citation needed]		These rules were altered and flagrantly ignored in the course of the last century of the Republic. For example, Gaius Marius held consulships for five years in a row between 104 BC and 100 BC. Officially presented as opportunities for public service, the offices often became mere opportunities for self-aggrandizement. The reforms of Lucius Cornelius Sulla required a ten-year period between holding another term in the same office.[1]		To have held each office at the youngest possible age (suo anno, "in his year") was considered a great political success, since to miss out on a praetorship at 39 meant that one could not become consul at 42. Cicero expressed extreme pride not only in being a novus homo ("new man"; comparable to a "self-made man") who became consul even though none of his ancestors had ever served as a consul, but also in having become consul "in his year".[2]						The cursus honorum began with ten years of military duty in the Roman cavalry (the equites) or in the staff of a general who was a relative or a friend of the family. The ten years of service were intended to be mandatory in order to qualify for political office, but in practice, the rule was not always rigidly applied.		A more prestigious position was that of a military tribune. In the early Roman Republic, 24 men at the age of around 20 were elected by the Tribal Assembly to serve as a commander in the legions, with six tribunes to each and command rotating among them. Tribunes could also be appointed by the consuls or by military commanders in the field as necessary. After the reforms of Gaius Marius in 107 BC, the six tribunes acted as staff officers for the legionary Legatus and were appointed tasks and command of units of troops whenever the need arose.[citation needed]		The following steps of the cursus honorum were achieved by direct election every year.[citation needed]		The first official post was that of quaestor. Candidates had to be at least 30 years old. However, men of patrician rank could subtract two years from this and other minimum age requirements.[citation needed]		Twenty quaestors served in the financial administration at Rome or as second-in-command to a governor in the provinces. They could also serve as the paymaster for a legion. A young man who obtained this job was expected to become a very important official. An additional task of all quaestors was the supervision of public games. As a quaestor, an official was allowed to wear the toga praetexta, but was not escorted by lictors, nor did he possess imperium.[citation needed]		At 36 years of age, former quaestors could stand for election to one of the aedile positions. Of these aediles, two were plebeian and two were patrician, with the patrician aediles called Curule Aediles. The plebeian aediles were elected by the Plebeian Council and the curule aediles were either elected by the Tribal Assembly or appointed by the reigning consul. The aediles had administrative responsibilities in Rome. They had to take care of the temples (whence their title, from the Latin aedes, "temple"), organize games, and be responsible for the maintenance of the public buildings in Rome. Moreover, they took charge of Rome's water and food supplies; in their capacity as market superintendents, they served sometimes as judges in mercantile affairs.[citation needed]		The Aedile was the supervisor of public works; the words "edifice" and "edification" stem from the same root. He oversaw the public works, temples and markets. Therefore, the Aediles would have been in some cooperation with the current Censors, who had similar or related duties. Also they oversaw the organization of festivals and games (ludi), which made this a very sought-after office for a career minded politician of the late republic, as it was a good means of gaining popularity by staging spectacles.[citation needed]		Curule Aediles were added at a later date in the 4th century BC, and their duties do not differ substantially from plebeian aediles. However, unlike plebeian aediles, curule aediles were allowed certain symbols of rank—the sella curulis or 'curule chair,' for example—and only patricians could stand for election to curule aedile. This later changed, and both Plebeians and Patricians could stand for Curule Aedileship.[citation needed]		The elections for Curule Aedile were at first alternated between Patricians and Plebeians, until late in the 2nd century BC, when the practice was abandoned and both classes became free to run during all years.[citation needed]		While part of the cursus honorum, this step was optional and not required to hold future offices. Though the office was usually held after the quaestorship and before the praetorship, there are some cases with former praetors serving as aediles.[citation needed]		After holding either the office of quaestor or aedile, a man of 39 years could run for praetor. The number of Praetors elected varied through history, generally increasing with time. During the republic, six or eight were generally elected each year to serve judicial functions throughout Rome and other governmental responsibilities. In the absence of the Consuls, a Praetor would be given command of the garrison in Rome or in Italy. Also, a Praetor could exercise the functions of the Consuls throughout Rome, but their main function was that of a judge. They would preside over trials involving criminal acts as well as grant court orders or validate "illegal" acts as acts of administering justice. As a Praetor, a magistrate was escorted by six lictors, and wielded imperium. After a term as Praetor, the magistrate would serve as a provincial governor in the office of Propraetor, wielding Propraetor imperium, commanding the province’s legions, and possessing ultimate authority within his province(s).[3]		Of all the Praetors, two were more prestigious than the others. The first was the Praetor Peregrinus, who was the chief judge in trials involving one or more foreigners. The other was the Praetor Urbanus, the chief judicial office in Rome. He had the power to overturn any verdict by any other courts, and served as judge in cases involving criminal charges against provincial governors. The Praetor Urbanus was not allowed to leave the city for more than ten days. If one of these two Praetors was absent from Rome, the other would perform the duties of both.[citation needed]		The office of consul was the most prestigious of all, and represented the summit of a successful career. The minimum age was 42 for plebeians and 40 for patricians. Years were identified by the names of the two consuls elected[4] for a particular year; for instance, M. Messalla et M. Pisone consulibus, "in the consulship of Messalla and Piso,"[5] dates an event to 61 BC. Consuls were responsible for the city's political agenda, commanded large-scale armies and controlled important provinces. The consuls served for only a year (a restriction intended to limit the amassing of power by individuals) and could only rule when they agreed, because each consul could veto the other's decision.[citation needed]		The consuls would alternate monthly as the chairman of the Senate. They also were the supreme commanders in the Roman army, with each being granted two legions during their consular year. Consuls also exercised the highest juridical power in the Republic, being the only office with the power to override the decisions of the Praetor Urbanus. Only laws and the decrees of the Senate or the People's assembly limited their powers, and only the veto of a fellow consul or a tribune of the plebs could supersede their decisions.[citation needed]		A consul was escorted by twelve lictors, owned imperium and wore the toga praetexta. Because the consul was the highest executive office within the Republic, they had the power to veto any action or proposal by any other magistrate, save that of the Tribune of the Plebs. After a consulship, a consul was assigned one of the more important provinces and acted as the governor in the same way that a Propraetor did, only owning Proconsular imperium. A second consulship could only be attempted after an interval of 10 years to prevent one man holding too much power.[citation needed]		Although not part of the Cursus Honorum, upon completing a term as either Praetor or Consul, an officer was required to serve a term as Propraetor and Proconsul, respectively, in one of Rome's many provinces. These Propraetors and Proconsuls held near autocratic authority within their selected province or provinces. Because each governor held equal imperium to the equivalent magistrate, they were escorted by the same number of lictors (12) and could only be vetoed by a reigning Consul or Praetor. Their abilities to govern were only limited by the decrees of the Senate or the people's assemblies, and the Tribune of the Plebs was unable to veto their acts as long as the governor remained at least a mile outside of Rome.		After a term as consul, the final step in the Cursus Honorum was the office of censor. This was the only office in the Roman Republic whose term was a period of eighteen months instead of the usual twelve. Censors were elected every five years and although the office held no military imperium, it was considered a great honour. The censors took a regular census of the people and then apportioned the citizens into voting classes on the basis of income and tribal affiliation. The censors enrolled new citizens in tribes and voting classes as well. The censors were also in charge of the membership roll of the Senate, every five years adding new senators who had been elected to the requisite offices. Censors could also remove unworthy members from the Senate. This ability was lost during the dictatorship of Sulla. Censors were also responsible for construction of public buildings and the moral status of the city.[citation needed]		Censors also had financial duties, in that they had to put out to tender projects that were to be financed by the state. Also, the censors were in charge of the leasing out of conquered land for public use and auction. Though this office owned no imperium, meaning no lictors for protection, they were allowed to wear the toga praetexta.[citation needed]		The office of Tribune of the Plebs was an important step in the political career of plebeians. Patricians could not hold the office. The Tribune was an office first created to protect the right of the common man in Roman politics and served as the head of the Plebeian Council. In the mid-to-late Republic, however, plebeians were often just as, and sometimes more, wealthy and powerful than patricians. Those who held the office were granted sacrosanctity (the right to be legally protected from any physical harm), the power to rescue any plebeian from the hands of a patrician magistrate, and the right to veto any act or proposal of any magistrate, including another tribune of the people and the consuls. The tribune also had the power to exercise capital punishment against any person who interfered in the performance of his duties. The tribunes could even convene a Senate meeting and lay legislation before it and arrest magistrates. Their houses had to remain open for visitors even during the night, and they were not allowed to be more than a day's journey from Rome. Due to their unique power of sacrosanctity, the Tribune had no need for lictors for protection and owned no imperium, nor could they wear the toga praetexta.[citation needed] After Sulla's reforms, a person who had held the office of Tribune of the Plebs could no longer qualify for any other office, and the powers of the tribunes were more limited.		Another office not officially a step in the cursus honorum was the princeps senatus, an extremely prestigious office for a patrician. The princeps senatus served as the leader of the Senate and was chosen to serve a five-year term by each pair of Censors every five years. Censors could, however, confirm a princeps senatus for a period of another five years. The princeps senatus was chosen from all Patricians who had served as a Consul, with former Censors usually holding the office. The office originally granted the holder the ability to speak first at session on the topic presented by the presiding magistrate, but eventually gained the power to open and close the senate sessions, decide the agenda, decide where the session should take place, impose order and other rules of the session, meet in the name of the senate with embassies of foreign countries, and write in the name of the senate letters and dispatches. This office, like the Tribune, did not own imperium, was not escorted by lictors, and could not wear the toga praetexta.[citation needed]		Of all the offices within the Roman Republic, none granted as much power and authority as the position of dictator, known as the Master of the People. In times of emergency, the Senate would declare that a dictator was required, and the current consuls would appoint a dictator. This was the only decision that could not be vetoed by the Tribune of the Plebs. The dictator was the sole exception to the Roman legal principles of having multiple magistrates in the same office and being legally able to be held to answer for actions in office. Essentially by definition, only one dictator could serve at a time, and no dictator could ever be held legally responsible for any action during his time in office for any reason.[citation needed]		The dictator was the highest magistrate in degree of imperium and was attended by twenty-four lictors (as were the former Kings of Rome). Although his term lasted only six months instead of twelve (except for the Dictatorships of Sulla and Caesar), all other magistrates reported to the dictator (except for the tribunes of the plebs - although they could not veto any of the dictator's acts), granting the dictator absolute authority in both civil and military matters throughout the Republic. The Dictator was free from the control of the Senate in all that he did, could execute anyone without a trial for any reason, and could ignore any law in the performance of his duties. The Dictator was the sole magistrate under the Republic that was truly independent in discharging his duties. All of the other offices were extensions of the Senate's executive authority and thus answerable to the Senate. Since the Dictator exercised his own authority, he did not suffer this limitation, which was the cornerstone of the office's power.[citation needed]		When a Dictator entered office, he appointed to serve as his second-in-command a magister equitum, the Master of the Horse, whose office ceased to exist once the Dictator left office. The magister equitum held Praetorian imperium, was attended by six lictors, and was charged with assisting the Dictator in managing the State. When the Dictator was away from Rome, the magister equitum usually remained behind to administer the city. The magister equitum, like the Dictator, had unchallengeable authority in all civil and military affairs, with his decisions only being overturned by the Dictator himself.[citation needed]		The Dictatorship was definitively abolished in 44 BC after the assassination of Gaius Julius Caesar (Lex Antonia).		
In the workplace, an evaluation is a tool employers use to review the performance of an employee.		Usually, the employee's supervisor (and frequently, a more senior manager) is responsible for evaluating the employee. A private conference is often scheduled to discuss the evaluation.		The process of an evaluation may include one or more of these things:		The frequency of an evaluation, and policies concerning them, varies widely from workplace to workplace. Sometimes, an evaluation will be given to a new employee after a probationary period lapses, after which they may be conducted on a regular basis (such as every year). According to the 2014 Performance Management survey, 96% of employers perform annual performance evaluations and 44% of employers perform a 90-day performance review for new employees.[2]		
Career counseling and career guidance are similar in nature to other types of counseling, e.g. marriage or psychological counseling. What unites all types of professional counseling is the role of practitioners, who combine giving advice on their topic of expertise with counseling techniques that support clients in making complex decisions and facing difficult situations. The focus of career counseling is generally on issues such as career exploration, career change, personal career development and other career related issues.		There is no agreed definition of career counseling worldwide, mainly due to conceptual, cultural and linguistic differences.[1] This even affects the most central term counseling (or: counselling in British English) which is often substituted with the word guidance as in career guidance. For example, in the UK, career counseling would usually be referred to as careers advice or guidance. Due to the widespread reference to both career guidance and career counseling among policy-makers, academics and practitioners around the world, references to career guidance and counselling are becoming common.[2]						Career counseling or career guidance includes a wide variety of professional activities which help people deal with career-related challenges. Career counselors work with adolescents seeking to explore career options, experienced professionals contemplating a career change, parents who want to return to the world of work after taking time to raise their child, or people seeking employment. Career counselling is also offered in various settings, including in groups and individually, in person or by means of digital communication.		Several approaches have been undertaken to systemize the variety of professional activities related to career guidance and counseling. In the most recent attempt, the Network for Innovation in Career Guidance and Counselling in Europe (NICE) – a consortium of 45 European institutions of higher education in the field of career counseling – has agreed on a system of professional roles for guidance counselors. Each of these five roles is seen as an important facet of the career guidance and counselling profession. Career counselors performing in any of these roles are expected to behave professionally, e.g. by following ethical standards in their practice. The NICE Professional Roles (NPR) are:[3]		The description of the NICE Professional Roles (NPR) draws on a variety of prior models to define the central activities and competences of guidance counselors.[4] The NPR can, therefore, be understood as a state-of-the-art framework which includes all relevant aspects of career counselling. For this reason, other models haven't been included here so far. Models which are reflected in the NPR include:		Professional career counselors can support people with career-related challenges. Through their expertise in career development and labor markets, they can put a person's qualifications, experience, strengths and weakness in a broad perspective while also considering their desired salary, personal hobbies and interests, location, job market and educational possibilities. Through their counseling and teaching abilities, career counselors can additionally support people in gaining a better understanding of what really matters for them personally, how they can plan their careers autonomously, or help them in making tough decisions and getting through times of crisis. Finally, career counselors are often capable of supporting their clients in finding suitable placements/ jobs, in working out conflicts with their employers, or finding the support of other helpful services.		It is due to these various benefits of career counseling that policy makers in many countries publicly fund guidance services. For example, the European Union understands career guidance and counseling as an instrument to effectively combat social exclusion and increase citizens' employability.[8]		Frank Parson's Choosing a Vocation (1909) was perhaps the first major work which is concerned with careers guidance. While until the 1970s a strongly normative approach was characteristic for theories (e.g. of Donald E. Super's life-span approach[9]) and practice of career counseling (e.g. concept of matching), new models have their starting point in the individual needs and transferable skills of the clients while managing biographical breaks and discontinuities. Career development is no longer viewed as a linear process. More consideration is now placed on nonlinear, chance and unplanned influences.		There is no standardized qualification for professional career counselors, although various certificates are offered nationally and internationally (e.g. by professional associations). The number of degree programs in career guidance and/or career counseling is growing worldwide. The title "career counselor" is unregulated, unlike engineers or psychologists whose professional titles are legally protected. At the same time, policy makers agree that the competence of career counselors is one of the most important factors in ensuring that people receive high quality support in dealing with their career questions.[10] Depending on the country of their education, career counselors may have a variety of academic backgrounds: In Europe, for instance, degrees in (vocational/ industrial/ organization) psychology and educational sciences are among the most common, but backgrounds in sociology, public administration and other sciences are also frequent.[11] At the same time, many training programs for career counselors are becoming increasingly multidisciplinary.		In the United States, the designation, "career counselor" is not legally protected; that is, anyone can call themselves a career counselor. However, CACREP, the accrediting body for counselor education programs requires that these programmes include one course in career counseling as a part of the coursework for a masters in counseling.		The National Career Development Association (NCDA), the credentialing body for career counselors, provides various certifications for qualified career counselors. For those university-trained counselors or psychologists who have devoted a certain number of years to career counseling and taken specific coursework, it offers a Master Career Counselor (MCC) credential. The National Career Development Association is the only professional association of career counselors in the United States that provides certification in career counseling.		There are many career guidance and counseling centres all over the world. They give services of guidance and counseling on higher studies, possibilities, chances and nature of courses and institutes. Also that these services are offered either fixing up a meeting with the Experts or having telephonic conversations with the guide or even the online guidance which is very common these days with the people getting services on click of their mouse. There are many such service providers all over the world providing online counseling to people about their career or conducting a psychometric test to know the persons aptitude as well as interests.		Tests are often used in career counseling to help clients make realistic career decisions. Tests used in career counseling generally fall into three categories: interest inventories, personality inventories, and aptitude tests.		Interest inventories are not technically tests at all, because there are no right or wrong answers. The theory is that if you have similar interests to people in an occupation who like their job, you will probably like that occupation also. Thus, interest inventories may suggest occupations that the client has not thought of and which have a good chance of being something that the client will be happy with.		Aptitude tests can predict with good odds whether a particular person will be able to be successful in a particular occupation. For example, a student who wants to be a physicist is unlikely to succeed if he cannot do the math. An aptitude test will tell him if he is likely to do will in advanced math, which is necessary for physics. There are also aptitude tests which can predict success or failure in many different occupations.		Personality inventories are sometimes used to help people with career choice. The use of these inventories for this purpose is questionable, because in any occupation there are people with many different personalities. A popular personality inventory is the Myers-Briggs Type Indicator. It is based on Carl Jung’s theory of personality, but Jung never approved it. According to Jung most people fall in the middle of each scale, but the MBTI ignores this and puts everyone in a type category. For example, according to the MBTI, everyone is either an extrovert or an introvert. According to Jung, most people are somewhere in between, and people at the extremes are rare. The validity of the MBTI for career choice is highly questionable.[12]		One of the major challenges associated with career counseling is encouraging participants to engage in the process. For example, in the UK 70% of people under 14 say they have had no careers advice while 45% of people over 14 have had no or very poor/limited advice.		In a related issue some client groups tend to reject the interventions made by professional career counselors preferring to rely on the advice of peers or superiors within their own profession. Jackson et al. found that 44% of doctors in training felt that senior members of their own profession were best placed to give careers advice.[13] Furthermore, it is recognised that the giving of career advice is something that is widely spread through a range of formal and informal roles. In addition to career counselors it is also common for psychologists, teachers, managers, trainers and Human Resources (HR) specialists to give formal support in career choices. Similarly it is also common for people to seek informal support from friends and family around their career choices and to bypass career professionals altogether. Today increasingly people rely on career web portals to seek advice on resume writing and handling interviews; as also to research on various professions and companies. It has even become possible to take vocational assessments online.		A Diploma of Counselling is offered at a variety of TAFE colleges and other registered training organisations throughout Western Australia. Most universities in Western Australia offer relevant undergraduate degree courses. Postgraduate courses in career development are also offered at interstate universities, through distance education. The Career Industry Council of Australia (CICA) endorses career development programs in Australia.[14] The Certificate IV in Career Development is offered at TAFE colleges and other registered training organisations throughout Western Australia.		
A vocational university, sometimes called professional university, applied technological university, college of higher vocational studies or university of applied sciences, is an institution of higher education and sometimes research, which provides both tertiary and sometimes quaternary education and grants academic degrees at all levels (bachelor, master, and sometimes doctorate) in a variety of subjects.		In some countries, a vocational university more precisely grants professional degrees like professional bachelor's degree, Professional master's degree and Professional doctorates. The term is not officially used in many countries and an assignment to a certain type of university in a certain country's educational system is therefore difficult. The UK once had a very extensive vocational university sector with its polytechnic system dating back to the mid 19th century. Vocational universities are often regulated and funded differently (for example, by the local government rather than the state) from research-focused universities, and the degrees granted are not necessarily interchangeable.						The education at vocational universities combines teaching of both practical skills and theoretical expertise. It can be contrasted with education in a usually broader scientific field, which may concentrate on theory and abstract conceptual knowledge. There is also the historical background that an educational institution was called a university in the Middle Ages only if a certain classical canon of subjects was taught (typically including philosophy, medicine and theology). In modern times, other subjects, namely natural and engineering sciences, became more important, but institutions of tertiary education focusing on these subjects and not offering the classical canon have until recently or are still denied the prestigious denomination "university" in all countries. They had to use other, more general terms (which in many languages are false friends of the English term "high school", sometimes with modifiers), including Fachhochschule in German, Haute Ecole in French (Belgium and Switzerland), Hogeschool in Dutch, Høyskole in Norwegian, Scuola universitaria professionale in Italian, etc.		There are different varieties, including vocational universities of applied sciences (also named polytechnics or institutes of technology), vocational universities of liberal arts, etc. In recent years, many vocational universities have received full university status, such as the University of Music and Performing Arts, Vienna, Austria (Universität für Musik und darstellende Kunst Wien, formerly Hochschule für Musik und Darstellende Kunst Wien), or the Örebro University, Sweden (formerly Örebro Högskola). There are also some establishments which now have full university status but continue to use their former names, such as the Royal Institute of Technology in Stockholm, Sweden.		Notice: certain universities are called korkeakoulu because they effectively have only one faculty, e.g. Teatterikorkeakoulu, the Theatre Academy, whereas universities with several faculties are called yliopisto. The term ammattikorkeakoulu (AMK) creates some confusion with korkeakoulu, because traditionally AMK's were not considered universities. A graduate of university of applied sciences (ammattikorkeakoulu) is generally not eligible for doctoral studies in Finnish universities without formally completing a master's degree from a university (yliopisto).		The term vocational university is not used. In contrast to traditional German universities, a Fachhochschule (translated University of Applied Sciences) has a more practical profile. Universities of Applied Sciences grant Bachelor degrees and Master degrees. Some Universities of Applied Sciences run doctoral programs where the degree itself is awarded by a partner institution. Furthermore, Berufsakademie is a college type strongly inspired by the dual education system. A Berufsakademie is called a university of cooperative education in English and only grants bachelor's degrees. This type of institution was first created in the German state of Baden-Württemberg and now exists in Hamburg, Hesse, Lower Saxony, Saarland, Saxony, Schleswig-Holstein, and Thuringia, but not in the other German states. In 2009, Baden-Württemberg transformed its Berufsakademie into a new type of institution, which until now only exists in that state, a "Duale Hochschule". In English, this type of institution is also called university of cooperative education, but a Duale Hochschule also offers master's degrees.		In Greece, comparable institutions to the Vocational Universities (or perhaps better to the Universities of Applied Sciences) are the Technological Educational Institutes (TEIs). These constitute part and parcel of the (otherwise exclusively academic) Higher Education in Greece and offer in their own capacity [4 years] bachelor's and master's degrees, but not Doctorate degrees.[1] In Greece, "Higher Education" entails also the distinction between "Higher" and "Highest", where Universities fall under the latter category/distinction, being able to issue titles up to Doctorate degrees.		On the other hand, the term College in Greece may refer, among others, to the institutions that are officially titled Centres of Post-lyceum(secondary) Education. These have a solely professional, i.e. non-academic, orientation according to existing Greek law, and are so far only private. However, they run in collaboration with foreign authorities, such as universities and accreditation organisations, that may recognise them academically. They may offer [professional] bachelor degrees of minimum three years, as well as master and doctorate degrees.		As of January 29, 2008, a Dutch hogeschool (hbo) may call itself a "University Of Applied Sciences" in English.[2]		Universities of Applied Sciences offer bachelor's (BA) and master's degrees (MA), but no doctorates.[3]		Since 1 January 2014 there is no longer a distinction between the bachelor's and master's degrees awarded by the Dutch research universities (Dutch "universiteiten") and the Dutch Universities of Applied Sciences (Dutch "hogescholen"), therefore graduates from Universities of Applied Sciences in the Netherlands will obtain a Bachelor of Arts (BA), Master of Science (MSc) or an equivalent academic title corresponding to their specific field of study.[4][5]		Hogescholen in the Netherlands have been provided with the right to conduct research by the revised Higher Education and Research Act (WHOO)2010.[6]		The main difference between universities (universitet) and vocational universities (högskola, official translation university college) is that only the former ones have the right to award doctorate degrees in all subjects they offer. Some vocational universities have been given such rights within limited areas of research.		Iran Technical and Vocational University (TVU) is of one Higher Education under the Ministry of Science, Research and Technology of the task management and technical school and vocational schools across the country is responsible. The university has more than 176 schools and colleges across the country and more than 220 thousand students is one of the largest universities in Iran Technical and Vocational University is of one Higher Education under the Ministry of Science, Research and Technology of the task management and technical school and vocational schools across the country is responsible.		While the terms vocational university and professional university do not have a clear legal definition in India, the University Grants Commission (UGC), which is the body that recognises universities in India, drawing its power from the University Grants Commission Act, 1956,[7] shares power with 15 professional councils.[8] These councils are "responsible for recognition of courses, promotion of professional institutions and providing grants to undergraduate programmes and various awards" in the relevant areas. The bodies relevant for professional education are:		There are five public vocational universities in Malaysia:		In 2009, the first University of Vocational Technology was established under the purview of the Ministry of Vocational and Technical Training. There are also nine College of Technologys in Sri Lanka.[9]		
Grey-collar refers to the balance of employed people not classified as white- or blue collar. It is used to refer to occupations that incorporate some of the elements of both blue- and white-collar, and generally are in between the two categories in terms of income-earning capability.		Examples of grey-collar industries:		Grey-collar workers often have licenses, associate degrees or diplomas from a trade or technical school in a particular field. They are unlike blue-collar workers in that blue-collar workers can often be trained on the job within several weeks whereas grey-collar workers already have a specific skill set and require more specialized knowledge than their blue-collar counterparts.		The field which most recognizes the diversity between these two groups is that of human resources and the insurance industry. These different groups must be insured differently for liability as the potential for injury is different.						The Pittsburgh Post-Gazette wrote that another definition for grey collar could be the underemployed white collar worker.[2]		Charle Brecher of the Citizens Budget Commission and the Partnership for New York City defined it sub-blue-collar jobs: "maintenance and custodial".[3]		
Corporate behaviour is the actions of a company or group who are acting as a single body. It defines the company's ethical strategies and describes the image of the company.[1]						Not only does corporate behaviour play various roles within different areas of a business, it also enables businesses to overcome any problems they may face. For example, due to an increase in globalisation, language barriers are likely to increase for organisations creating major problems as day-to-day business may be disrupted. Corporate behaviour enables managers to overcome this problem by improving flexibility. Also, many businesses are struggling to remain competitive in terms of quality and productivity due to intense competition within markets. However, corporate behaviour is able to fix this issue by allowing managers to empower their employees as they are the ones who are able to make a change. Positive corporate behaviour can result in employees feeling happy and content at work providing their best outcome. This is beneficial for management as it could lead to effective teams being created thus resulting in innovative ideas which is beneficial for the business. It also helps to decrease labour turnover enabling the organisation to retain its most valuable employees.[2][3]		Corporate behaviour is important in strengthening relationships within organisations between individuals, teams, and in the organisation as a whole. It is important as it reflects the values of the business and the extent to which it is ethical.[4] Corporate behavior refers to the company values that defines it and makes it different and better than other companies. Portraying positive corporate behavior within a company facilitates strong brand image creation; consequently branding then strengthens the importance associated with corporate behavior.[5]		PESTLE factors influence corporate behaviour in many ways. They cause organisations to change the way they operate, however the size and nature of change is dependent upon which factor is causing the change; (political, economic, social, technological, legal, or environmental).		Examples of political factors could be changes in government legislation. This could affect an organisations Corporate behaviour as they would have to change the way they operate in order to implement these changes; some employees may not like the new changes made.[6][7][8]		Recession is an example of an economic factor. If the economy were to be in a recession, businesses may find they have to reduce jobs. This would affect Corporate behaviour as business teams would be short of skills and ideas in order to operate effectively.[6][7][8]		Changes in trends and the market is a social factor which affects Corporate behaviour. Organisations may have to change their products or services in order to keep up to date with new trends. In order to do this, employees may be required to learn new skills within a short amount of time to make these changes; relationships between employees and management could be at risk due to these changes.[6][7][8]		Implementing technology within organisations could mean more virtual meetings and fewer face to face meetings. As a result, relationships between management and employees could weaken as a result of less face to face conversations.[6][7][8]		Legislative rules such as tax may increase which would increase an organisations costs. Changes such as, changing the way the organisation operates may have to be made in order to cover these extra costs.[7][8]		Environmental factors could be any factors which prevent damage to the environment. For example, more employees may be required to telework to reduce the number of employees physically travelling to offices thus reducing carbon dioxide emissions. However this may lead to isolation as communication is reduced, weakening Corporate behaviour within firms.[7][8]		Businesses have many stakeholders who influence corporate behaviour. However, businesses who adopt the stakeholder theory are likely to appeal more to their stakeholders as they are showing their care and commitment towards them. This helps to strengthen the Corporate behaviour within a firm and reduces the need for stakeholders to demand change.[9][10]		
A corporate collapse typically involves the insolvency or bankruptcy of a major business enterprise. A corporate scandal involves alleged or actual unethical behavior by people acting within or on behalf of a corporation. Many recent corporate collapses and scandals have involved false or inappropriate accounting of some sort (see list at accounting scandals).						The following list of corporations involved major collapses, through the risk of job losses or size of the business, and meant entering into insolvency or bankruptcy, or being nationalised or requiring a non-market loan by a government.		Investors were paid returns out of their own money or that of other investors rather than from profits.		Madoff told his sons about his scheme and they reported him to the SEC. He was arrested the next day.		
In English-speaking countries, a blue-collar worker is a working class person who performs non-agricultural manual labour. Blue-collar work may involve skilled or unskilled manufacturing, mining, sanitation, custodial work, oil field work, construction, mechanic, maintenance, warehousing, firefighting, technical installation and many other types of physical work. Often something is physically being built or maintained.		In contrast, the white-collar worker typically performs work in an office environment and may involve sitting at a computer or desk. A third type of work is a service worker (pink collar) whose labour is related to customer interaction, entertainment, sales or other service-oriented work. Many occupations blend blue, white or pink industry categorizations.		Blue-collar work is often paid hourly wage-labor, although some professionals may be paid by the project or salaried. There is a wide range of payscales for such work depending upon field of specialty and experience.						Industrial and manual workers often wear durable canvas or cotton clothing that may be soiled during the course of their work. Navy and light blue colors conceal potential dirt or grease on the worker's clothing, helping him or her to appear cleaner. For the same reason, blue is a popular color for boilersuits which protect a worker's clothing. Some blue collar workers have uniforms with the name of the business and/or the individual's name embroidered or printed on it.		Historically the popularity of the color blue among manual labourers contrasts with the popularity of white dress shirts worn by people in office environments. The blue collar/white collar color scheme has socio-economic class connotations. However, this distinction has become blurred with the increasing importance of skilled labour, and the relative increase in low-paying white-collar jobs.		The term blue collar was first used in reference to trades jobs in 1924, Alden, Iowa newspaper.[1]		A higher level academic education is often not required for many blue-collar jobs. However, certain fields may require specialized training, licensing or certification as well as a high school diploma or GED.		With the information revolution, Western nations have moved towards a service and white collar economy. Many manufacturing jobs have been offshored to developing nations which pay their workers lower wages. This offshoring has pushed formerly agrarian nations to industrialized economies and concurrently decreased the number of blue-collar jobs in developed countries.		In the United States, blue collar and service occupations generally refer to jobs in precision production, craft, and repair occupations; machine operators and inspectors; transportation and moving occupations; handlers, equipment cleaners, helpers, and laborers.[2]		In the United States an area known as the Rust Belt comprising the Northeast and Midwest, including Western New York and Western Pennsylvania, has seen its once large manufacturing base shrink significantly. With the de-industrialization of these areas starting in the mid-1960s cities like Cleveland, Ohio; Detroit, Michigan; Buffalo, New York; Pittsburgh, Pennsylvania; Erie, Pennsylvania; Youngstown, Ohio; Toledo, Ohio, Rochester, New York, and Saint Louis, Missouri, have experienced a steady decline of the blue-collar workforce and subsequent population decreases. Due to this economic osmosis, the rust belt has experienced high unemployment, poverty, and urban blight.		"Blue-collar" can be used as an adjective to describe the environment of the blue-collar worker such as a "blue-collar" neighborhood, restaurant, or bar.[3]		
The Wayback Machine is a digital archive of the World Wide Web and other information on the Internet created by the Internet Archive, a nonprofit organization, based in San Francisco, California, United States.		The Internet Archive launched the Wayback Machine in October 2001.[4][5] It was set up by Brewster Kahle and Bruce Gilliat, and is maintained with content from Alexa Internet.[citation needed] The service enables users to see archived versions of web pages across time, which the archive calls a "three dimensional index".[citation needed]		Since 1996, the Wayback Machine has been archiving cached pages of websites onto its large cluster of Linux nodes.[citation needed] It revisits sites every few weeks or months and archives a new version.[citation needed] Sites can also be captured on the fly by visitors who enter the site's URL into a search box.[citation needed] The intent is to capture and archive content that otherwise would be lost whenever a site is changed or closed down.[citation needed] The overall vision of the machine's creators is to archive the entire Internet.[citation needed]		The name Wayback Machine was chosen as a reference to the "WABAC machine" (pronounced way-back), a time-traveling device used by the characters Mr. Peabody and Sherman in The Rocky and Bullwinkle Show, an animated cartoon.[6][7] In one of the animated cartoon's component segments, Peabody's Improbable History, the characters routinely used the machine to witness, participate in, and, more often than not, alter famous events in history.[citation needed]		Software has been developed to "crawl" the web and download all publicly accessible World Wide Web pages, the Gopher hierarchy, the Netnews (Usenet) bulletin board system, and downloadable software.[8] The information collected by these "crawlers" does not include all the information available on the Internet, since much of the data is restricted by the publisher or stored in databases that are not accessible. To overcome inconsistencies in partially cached websites, Archive-It.org was developed in 2005 by the Internet Archive as a means of allowing institutions and content creators to voluntarily harvest and preserve collections of digital content, and create digital archives.[citation needed]		Information had been kept on digital tape for five years, with Kahle occasionally allowing researchers and scientists to tap into the clunky database.[9] When the archive reached its fifth anniversary, it was unveiled and opened to the public in a ceremony at the University of California, Berkeley.[citation needed]		Snapshots usually become available more than six months after they are archived or, in some cases, even later; it can take twenty-four months or longer.[10] The frequency of snapshots is variable, so not all tracked website updates are recorded. Sometimes there are intervals of several weeks or years between snapshots.[citation needed]		After August 2008 sites had to be listed on the Open Directory in order to be included.[11] According to Jeff Kaplan of the Internet Archive in November 2010, other sites were still being archived,[12] but more recent captures would become visible only after the next major indexing, an infrequent operation.[citation needed]		As of 2009[update], the Wayback Machine contained approximately three petabytes of data and was growing at a rate of 100 terabytes each month;[13] the growth rate reported in 2003 was 12 terabytes/month. The data is stored on PetaBox rack systems manufactured by Capricorn Technologies.[14]		In 2009, the Internet Archive migrated its customized storage architecture to Sun Open Storage, and hosts a new data center in a Sun Modular Datacenter on Sun Microsystems' California campus.[15]		In 2011 a new, improved version of the Wayback Machine, with an updated interface and fresher index of archived content, was made available for public testing.[16]		In March 2011, it was said on the Wayback Machine forum that "The Beta of the new Wayback Machine has a more complete and up-to-date index of all crawled materials into 2010, and will continue to be updated regularly. The index driving the classic Wayback Machine only has a little bit of material past 2008, and no further index updates are planned, as it will be phased out this year".[17]		In January 2013, the company announced a ground-breaking milestone of 240 billion URLs.[18]		In October 2013, the company announced the "Save a Page" feature[19] which allows any Internet user to archive the contents of a URL. This became a threat of abuse by the service for hosting malicious binaries.[20][21]		As of December 2014[update], the Wayback Machine contained almost nine petabytes of data and was growing at a rate of about 20 terabytes each week.[22]		As of July 2016[update], the Wayback Machine reportedly contained around 15 petabytes of data.[23]		Between October 2013 and March 2015 the website's global Alexa rank changed from 162[24] to 208.[25]		Historically, Wayback Machine respected the robots exclusion standard (robots.txt) in determining if a website would be crawled or not; or if already crawled, if its archives would be publicly viewable. Website owners had the option to opt-out of Wayback Machine through the use of robots.txt. It applied robots.txt rules retroactively; if a site blocked the Internet Archive, any previously archived pages from the domain were immediately rendered unavailable as well. In addition the Internet Archive stated, "Sometimes a website owner will contact us directly and ask us to stop crawling or archiving a site. We comply with these requests."[37] In addition, the website says: "The Internet Archive is not interested in preserving or offering access to Web sites or other Internet documents of persons who do not want their materials in the collection."[38]		This policy began to relax in 2017, when it stopped honoring robots.txt on U.S. government and military web sites for both crawling and displaying web pages. As of April 2017, Wayback is exploring ignoring robots.txt more broadly, not just for U.S. government websites.[39][40][41][42]		The site is frequently used by journalists and citizens to review dead websites, dated news reports or changes to website contents. Its content has been used to hold politicians accountable and expose battlefield lies.[43]		In 2014 an archived social media page of separatist rebel leader in Ukraine Igor Girkin showed him boasting about his troops having shot down a suspected Ukrainian military airplane before it became known that the plane actually was a civilian Malaysian Airlines jet after which he deleted the post and blamed Ukraine's military.[43][44]		In 2017 the March for Science originated from a discussion on reddit that indicated someone had visited Archive.org and discovered that all references to climate change had been deleted from the White House website. In response, a user commented, "There needs to be a Scientists' March on Washington".[45][46][47]		Furthermore, the site is used heavily for verification, providing access to references and content creation by Wikipedia editors.[citation needed]		In a 2009 case, Netbula, LLC v. Chordiant Software Inc., defendant Chordiant filed a motion to compel Netbula to disable the robots.txt file on its website that was causing the Wayback Machine to retroactively remove access to previous versions of pages it had archived from Netbula's site, pages that Chordiant believed would support its case.[48]		Netbula objected to the motion on the ground that defendants were asking to alter Netbula's website and that they should have subpoenaed Internet Archive for the pages directly.[49] An employee of Internet Archive filed a sworn statement supporting Chordiant's motion, however, stating that it could not produce the web pages by any other means "without considerable burden, expense and disruption to its operations."[48]		Magistrate Judge Howard Lloyd in the Northern District of California, San Jose Division, rejected Netbula's arguments and ordered them to disable the robots.txt blockage temporarily in order to allow Chordiant to retrieve the archived pages that they sought.[48]		In an October 2004 case, Telewizja Polska USA, Inc. v. Echostar Satellite, No. 02 C 3293, 65 Fed. R. Evid. Serv. 673 (N.D. Ill. Oct. 15, 2004), a litigant attempted to use the Wayback Machine archives as a source of admissible evidence, perhaps for the first time. Telewizja Polska is the provider of TVP Polonia and EchoStar operates the Dish Network. Prior to the trial proceedings, EchoStar indicated that it intended to offer Wayback Machine snapshots as proof of the past content of Telewizja Polska's website. Telewizja Polska brought a motion in limine to suppress the snapshots on the grounds of hearsay and unauthenticated source, but Magistrate Judge Arlander Keys rejected Telewizja Polska's assertion of hearsay and denied TVP's motion in limine to exclude the evidence at trial.[50][51] At the trial, however, district Court Judge Ronald Guzman, the trial judge, overruled Magistrate Keys' findings,[citation needed] and held that neither the affidavit of the Internet Archive employee nor the underlying pages (i.e., the Telewizja Polska website) were admissible as evidence. Judge Guzman reasoned that the employee's affidavit contained both hearsay and inconclusive supporting statements, and the purported web page printouts were not self-authenticating.[citation needed]		Provided some additional requirements are met (e.g., providing an authoritative statement of the archivist), the United States patent office and the European Patent Office will accept date stamps from the Internet Archive as evidence of when a given Web page was accessible to the public. These dates are used to determine if a Web page is available as prior art for instance in examining a patent application.[52]		There are technical limitations to archiving a website, and as a consequence, it is possible for opposing parties in litigation to misuse the results provided by website archives. This problem can be exacerbated by the practice of submitting screen shots of web pages in complaints, answers, or expert witness reports, when the underlying links are not exposed and therefore, can contain errors. For example, archives such as the Wayback Machine do not fill out forms and therefore, do not include the contents of non-RESTful e-commerce databases in their archives.[53]		In Europe the Wayback Machine could be interpreted as violating copyright laws. Only the content creator can decide where their content is published or duplicated, so the Archive would have to delete pages from its system upon request of the creator.[54] The exclusion policies for the Wayback Machine may be found in the FAQ section of the site.[citation needed]		A number of cases have been brought against the Internet Archive specifically for its Wayback Machine archiving efforts.		In late 2002, the Internet Archive removed various sites that were critical of Scientology from the Wayback Machine.[55] An error message stated that this was in response to a "request by the site owner".[56] Later, it was clarified that lawyers from the Church of Scientology had demanded the removal and that the site owners did not want their material removed.[57]		In 2003, Harding Earley Follmer & Frailey defended a client from a trademark dispute using the Archive's Wayback Machine. The attorneys were able to demonstrate that the claims made by the plaintiff were invalid, based on the content of their website from several years prior. The plaintiff, Healthcare Advocates, then amended their complaint to include the Internet Archive, accusing the organization of copyright infringement as well as violations of the DMCA and the Computer Fraud and Abuse Act. Healthcare Advocates claimed that, since they had installed a robots.txt file on their website, even if after the initial lawsuit was filed, the Archive should have removed all previous copies of the plaintiff website from the Wayback Machine.[58] The lawsuit was settled out of court.[59]		In December 2005, activist Suzanne Shell filed suit demanding Internet Archive pay her US $100,000 for archiving her website profane-justice.org between 1999 and 2004.[60][61] Internet Archive filed a declaratory judgment action in the United States District Court for the Northern District of California on January 20, 2006, seeking a judicial determination that Internet Archive did not violate Shell's copyright. Shell responded and brought a countersuit against Internet Archive for archiving her site, which she alleges is in violation of her terms of service.[62] On February 13, 2007, a judge for the United States District Court for the District of Colorado dismissed all counterclaims except breach of contract.[61] The Internet Archive did not move to dismiss copyright infringement claims Shell asserted arising out of its copying activities, which would also go forward.[63]		On April 25, 2007, Internet Archive and Suzanne Shell jointly announced the settlement of their lawsuit.[60] The Internet Archive said it "...has no interest in including materials in the Wayback Machine of persons who do not wish to have their Web content archived. We recognize that Ms. Shell has a valid and enforceable copyright in her Web site and we regret that the inclusion of her Web site in the Wayback Machine resulted in this litigation." Shell said, "I respect the historical value of Internet Archive's goal. I never intended to interfere with that goal nor cause it any harm."[64]		In 2013–14, a pornographic actor tried to remove archived images of himself from the WayBack Machine's archive, first by sending multiple DMCA requests to the archive, and then by appealing to the Federal Court of Canada.[65][66]		In 2005, Yahoo! Search began to provide links to other versions of pages archived on the Wayback Machine.[67]		Archive.org is currently blocked in China.[68][69] After the site enabled the encrypted HTTPS protocol, the Internet Archive was blocked in its entirety in Russia in 2015.[70][71][43][needs update?]		Alison Macrina, director of the Library Freedom Project, notes that "while librarians deeply value individual privacy, we also strongly oppose censorship".[43]		There are known rare cases where online access to content which "for nothing" has put people in danger was disabled.[43]		In April 2017, emails of French presidential candidate Emmanuel Macron were leaked to the site and elsewhere.[72][73] As archive.org is not a website for publishing original leaks, uploading this data to the site first may have been intended to or may effectively cause harm to the site.[citation needed]		Other threats include natural disasters,[74] destruction (remote or physical),[citation needed] manipulation of the archive's contents (see also: cyberattack, backup), problematic copyright laws[75] and surveillance of the site's users.[76]		Kevin Vaughan suspects that in the long-term of multiple generations "next to nothing" will survive in a useful way besides "if we have continuity in our technological civilization" by which "a lot of the bare data will remain findable and searchable".[77]		Some find the Internet Archive, which describes itself to be built for the long-term,.[78] to be working furiously to capture data before it disappears without any long-term infrastructure to speak of.[79][copyright violation?]		
Telecommuting, also called remote work,[1] telework, or teleworking,[2] is a work arrangement in which employees do not commute or travel (e.g. by bus or car) to a central place of work, such as an office building, warehouse or store. Teleworkers in the 2000s often use mobile telecommunications technology such as Wi-Fi-equipped laptop or tablet computers and smartphones to work from coffee shops; others may use a desktop computer and a landline phone at their home. According to a Reuters poll, approximately "one in five workers around the globe, particularly employees in the Middle East, Latin America and Asia, telecommute frequently and nearly 10 percent work from home every day."[3] In the 2000s, annual leave or vacation in some organizations is seen as absence from the workplace rather than ceasing work, and some office employees use telework to continue to check work e-mails while on vacation.[citation needed]		Although the concepts of "telecommuting" and "telework" are closely related, there is a difference between the two. All types of technology-assisted work conducted outside of a centrally located work space (including work undertaken in the home, outside calls, etc.) are regarded as telework. Telecommuters often maintain a traditional office and usually work from an alternative work site from 1 to 3 days a week.[4] Telecommuting refers more specifically to work undertaken at a location that reduces commuting time. These locations can be inside the home or at some other remote workplace, which is facilitated through a broadband connection, computer or phone lines,[5] or any other electronic media used to interact and communicate.[6] As a broader concept than telecommuting, telework has four dimensions in its definitional framework: work location, that can be anywhere outside of a centralized organizational work place; usage of ICTs (information and communication technologies) as technical support for telework; time distribution, referring to the amount of time replaced in the traditional workplace; and the diversity of employment relationships between employer and employee, ranging from contract work to traditional full-time employment.[7]		In the 1990s, telecommuting became the subject of pop culture attention. In 1995, the motto that "work is something you do, not something you travel to" was coined.[8] Variations of this motto include: "Work is something we DO, not a place that we GO"[9] and "Work is what we do, not where we are."[10] Telecommuting has been adopted by a range of businesses, governments and not-for-profit organizations. Organizations may use telecommuting to reduce costs (telecommuting employees do not require an office or cubicle, a space which has to be rented or purchased, provided with lighting and climate control, etc.). Some organizations adopt telecommuting to improve workers' quality of life, as teleworking typically reduces commuting time and time stuck in traffic jams. As well, teleworking may make it easier for workers to balance their work responsibilities with family roles (e.g., caring for children or elderly parents). Some organizations adopt teleworking for environmental reasons, as telework can reduce congestion and air pollution, as it can reduce the number of cars on the roads.						Telecommuting is also called "remote work",[1] "telework", or "teleworking".[2] A person who telecommutes is known as a "telecommuter", "teleworker", and sometimes as a "home-sourced", or "work-at-home" employee. A telecommuter is also called the "telecommuting specialist", as a designation and in a professional context. Many telecommuters work from home, while others, sometimes called "nomad workers" work at coffee shops or other locations. The terms "telecommuting" and "telework" were coined by Jack Nilles in 1973.[11]		As of 2012[update], estimates suggest that over fifty million U.S. workers (about 40% of the working population) could work from home at least part of the time,[12] but in 2008 only 2.5 million employees, excluding the self-employed, considered their home to be their primary place of business.[13] The number of employees reported to have worked from their home "on their primary job" in 2010 has been reported as 9.4 million (6.6% of the workforce), though, this number might include the self-employed.[14] Very few companies employ large numbers of home-based full-time staff.[citation needed] The call center industry is one notable exception: several U.S. call centers employ thousands of home-based workers. For many employees, the option to work from home is available as an employee benefit but most participants only do so a fraction of the time.[15] Top paid among work-from-home sectors are home-based physicians and radiologists in which it is suspected that they earn near the $1,975 median weekly income of physicians, as reported by the U.S. Bureau of Labor Statistics, making it a six-figure job. Studies show that at-home workers are willing to earn up to 30% less and experience heightened productivity. [16]		In 2009, the United States Office of Personnel Management reported that approximately 103,000 federal employees were teleworking. However, less than 14,000 were teleworking three or more days per week.[17] In January 2012, Reuters, drawing from an Ipsos/Reuters poll, predicted that telecommuting was "a trend that has grown and one which looks like it will continue with 34% of connected workers saying they would be very likely to telecommute on a full-time basis if they could."[3] On December 9, 2010, the U.S. Federal Government passed the Telework Enhancement Act of 2010[18] in order to improve Continuity of Operations and ensure essential Federal functions are maintained during emergencies; to promote management effectiveness when telework is used to achieve reductions in organizational and transit costs and environmental impacts; and to enhance the work-life balance of workers. For example, telework allows employees to better manage their work and family obligations and thus helps retain a more resilient Federal workforce better able to meet agency goals.[19]		Study results from the 2013 Regus Global Economic Indicator were published in September 2013 and showed that 48% of business managers worldwide work remotely for at least half their working week. The study engaged over 26,000 business managers across 90 countries, with 55% of respondents stating that the effective management of remote workers is an attainable goal. Following the release of the results, Regus CEO Mark Dixon stated: "The business people we speak with tell us that trust and freedom play a key role in remote management, and once these are in place the benefits are clear for all to see: greater productivity, improved staff retention and lower operating costs."[20] A living list of fully distributed companies can be found here. Forrester Research’s US Telecommuting Forecast reporting that 34 million Americans work from home and the number is expected to reach a staggering 63 million – or 43% of the U.S. workforce – by 2016. Cisco reports that the company has generated an estimated annual savings of $277 million in productivity by allowing employees to telecommute and telework. And Intuit reports that by 2020, more than 40% of the American workforce, or 60 million people, will be freelancers, contractors and temp workers. In the UK between 2007 and 2012, the number of employees who usually work from home increased by 13% - an increase of almost half a million people, taking the total to over 4 million employees out of a UK workforce of 30 million.[21]		The roots of telecommuting are found in early 1970s technology that linked satellite offices to downtown mainframes through dumb terminals using telephone lines as a network bridge. The ongoing and exponential decreases in cost along with the increases in performance and usability of personal computers, forged the way for moving the office to the home. By the early 1980s, branch offices and home workers were able to connect to organizational mainframes using personal computers and terminal emulation. Telework is facilitated by tools such as groupware, virtual private networks, conference calling, videoconferencing, virtual call centre and Voice over IP (VOIP) and by the decreasing cost of good quality laptop computers. It can be efficient and useful for companies since it allows workers to communicate over long distances, saving significant amounts of travel time and cost. As broadband Internet connections become more commonplace, more and more workers have adequate bandwidth at home to use these tools to link their home to their corporate intranet and internal phone networks.		The adoption of local area networks promoted the sharing of resources, and client–server computing allowed for even greater decentralization. Today, telecommuters can carry laptops which they can use both at the office, at home, and nearly anywhere else. The rise of cloud computing technology and Wi-Fi availability have enabled access to remote servers via a combination of portable hardware and software.[22] Furthermore, with their improving technology and increasing popularity, smartphones are becoming widely used in telework. They substantially increase the mobility of the worker and the degree of coordination with their organization. The technology of mobile phones and personal digital assistant (PDA) devices allows instant communication through text messages, camera photos, and video clips from anywhere and at any time.[23]		The technology to communicate is not advanced enough to replicate face-to-face office interactions. Room for mistakes and miscommunication can increase. According to media richness theory, face-to-face interactions provide the capacity to process rich information: ambiguous issues can be clarified, immediate feedback can be provided, and there is personalized communication (e.g. body language, tone of voice).[24] Telecommuting requires the use of various types of media to communicate, such as the telephone and email. Emails have a time lag that does not allow for immediate feedback; telephone conversations make it harder to decipher the emotions of the person or team on the phone; and both of these forms of communication do not allow one to see the other person.[25] Typical organization communication patterns are thus altered in telecommuting. For instance, teams using computer-mediated communication with computer conferencing take longer to make group decisions than face-to-face groups.[26] Workers tend to be satisfied with face-to-face interactions, phone conversations, and in-person departmental meetings to receive communications, but email and the Internet do not add to their communication satisfaction.[27] This suggests that teleworking may not have the components for “rich communication” compared to face-to-face interactions, although one study found that virtual workers in a team were more satisfied with their technology-mediated communication than their in-person office communication.[28]		Some of the potential benefits and drawbacks of telecommuting can be explained by job characteristic theory, which proposes that the traits and tasks of the job itself affect employees’ work attitudes and behavior.[29] If five characteristics of a job are present (skill variety, task identity, task significance, autonomy, and feedback), then the employee in that job will experience more internal work motivation, satisfaction with personal growth opportunities, general job satisfaction, higher job performance, and lower absenteeism and turnover.[29][30] Many studies have provided evidence that job characteristics influence employees’ behaviors and attitudes.[31] Additionally, job characteristics can interact with individual differences to impact employee attitudes and behavior.[30][32] Of these five job characteristics, telework specifically changes autonomy and feedback compared to face-to-face work and can thus influence employees’ behaviors and attitudes. According to Job Characteristics Theory, changes in autonomy and feedback influence work behaviors and attitudes more than a change in skill variety, task identity, or task significance.[29]		Autonomy influences experienced responsibility such that if the job provides freedom, independence, and scheduling flexibility, the individual should feel responsible for his or her work outcomes. Telework provides flexibility in scheduling and freedom because being outside the office gives the worker more choices. Teleworkers do not have to stick to office routines and can shift work to different times of day.[33] Telework allows employees the freedom to choose where they work, when they work and even what they wear to work to allow their best work.[34] Teleworkers may experience more responsibility to the extent that they feel in control and accountable for their work.[35] The autonomy of telework allows for lower work-family conflict.[36] Teleworking provides the freedom to arrange work to avoid family conflicts. Increased control over life demands[37] is one of its main attractions. The level of autonomy in telework felt by the employee depends on a variety of factors, including scheduling flexibility and the household size.[37] In addition to reducing work-family conflict, conflicts with activities are also reduced. Increased and fewer time restrictions freedom allow workers to participate more in recreational activities, whether social or physical.[34]		The job characteristic dimension, feedback, increases employees' knowledge of results. Feedback refers to the degree that an individual receives direct and clear information about his or her performance related to work activities.[30] Feedback is particularly important so that the employees continuously learn about how they are performing.[32] Electronic communication provides fewer cues for teleworkers and thus, they may have more difficulties interpreting and gaining information, and subsequently, receiving feedback.[33] When a worker is not in the office, there is limited information and greater ambiguity, such as in assignments and expectations.[38] Role ambiguity, when situations have unclear expectations as to what the worker is to do,[39] may result in greater conflict, frustration, and exhaustion.[33]		Communication personalized for individual needs is important for feedback interactions.[40] People differ in their need for communication and their level of social connectedness to their environment, partially because of personality and temperament differences.[41] Although the level of communication may decrease for teleworkers, satisfaction with this level of communication can be higher in some samples, like those who are more tenured and have functional instead of social relationships.[28] Feedback and communication can also be affected by a manager’s location. The clarity, speed of response, richness of the communication, frequency, and quality of the feedback are often reduced when managers telework.[38]		Three of the five job attributes: skill variety, task identity, and task significance, influence how much employees think their jobs are meaningful.[32] Skill variety is the degree that a job requires a variety of activities and skills to complete the task. An increase in skill variety is thought to increase the challenge of the job. Increasing the challenge of the job increases the individual’s experienced meaningfulness, how much the individual cares about work, and finds it worthwhile.[29][32] Telework may not directly affect skill variety and task meaningfulness for the individual compared to when he or she worked in an office; however, skill variety and meaningfulness of individual tasks can increase when working in a group. If the work done at home is focused on the individual rather than the team, there may be fewer opportunities to use a variety of skills.[42] Task identity is the degree that the individual sees work from beginning to end or completes an identifiable or whole piece of work rather than only a small piece. Task significance is the degree that the individual feels his or her work has a substantial impact on the lives or work of other people within the organization or outside of the organization.[32][42] Telework may not change the job characteristics of skill variety, task identity, and task significance compared to working in an office; however, the presence of these characteristics will influence teleworkers’ work outcomes and attitudes.		Individuals may differ in their reactions to the job characteristics in telecommuting. According to job characteristics theory, the personal need for accomplishment and development (growth need strength)[30] influences how much an individual will react to the job dimensions of telecommuting. For instance, those individuals high in growth need strength will have a more positive reaction to increased autonomy and a more negative reaction to decreased feedback in telecommuting than those individuals low in growth need strength.		Telecommuting is a new work situation with a flexible structure that makes it different from traditional work environments[43] Various job design theories, in addition to job characteristics theory, can help explain the differences between telecommuting and traditional job settings.		Motivator-hygiene theory[44] differentiates between motivating factors (motivators) and dissatisfying factors (hygienes). Factors that are motivators such as recognition and career advancement may be lessened with telework. When teleworkers are not physically present, they may be “out of sight, out of mind” to other workers in the office.[45] Additionally, telework may not always be seen positively by management due to fear of loss of managerial control.[46] A 2008 study found that more time spent telecommuting decreased the perception of productivity of the teleworker in the eyes of management.[47] Hygiene factors, such as work conditions, may improve when teleworking such that teleworkers have the flexibility to work in a variety of locations.[43][45] Thus, telework has different work motivating factors and dissatisfying factors than office work.[45]		Social information processing suggests that individuals give meaning to job characteristics.[48] Individuals have the ability to construct their own perception of the environment by interpreting social cues.[49] This social information comes from overt statements from coworkers, cognitive evaluations of the job or task dimensions, and previous behaviors. This social context can affect individuals’ beliefs about the nature of the job, the expectations for individual behavior, and the potential consequences of behavior, especially in uncertain situations.[49] In telework, there are fewer social cues because social exchange and personalized communication takes longer to process in computer-mediated communication than face-to-face interactions.[50]		Sociotechnical systems (STS) theory explains the interaction between social and technological factors. STS examines the relationships between people, technology, and the work environment, in order to design work in a way that enhances job satisfaction and increases productivity.[43] Originally developed to explain the paradox of improved technology but decreased productivity,[51] the theory can be applied to the design of telework. One of the principles of STS is minimal critical specification.[52] This principle states that, unless absolutely essential, there should be minimal specification of objectives and how to do tasks in order to avoid closing options or inhibiting effective actions. Telecommuting provides teleworkers with the freedom to decide how and when to do their tasks.[34] Similarly, teleworkers have the responsibility to use their equipment and resources to carry out their responsibilities. This increase in responsibility for their work also increases their power,[52] supporting the idea that teleworking is a privilege and in some companies, considered a promotion.[45]		Adaptive structuration theory studies variations in organizations as new technologies are introduced[53] Adaptive structural theory proposes that structures (general rules and resources offered by the technology) can differ from structuration (how people actually use these rules and resources).[43] There is an interplay between the intended use of technology and the way that people use the technology. Telecommuting provides a social structure that enables and constrains certain interactions.[54] For instance, in office settings, the norm may be to interact with others face-to-face. To accomplish interpersonal exchange in telecommuting, other forms of interaction need to be used. AST suggests that when technologies are used over time, the rules and resources for social interactions will change.[53] Teleworking may alter traditional work practices,[43] such as switching from primarily face-to-face communication to electronic communication.		In general, telecommuting benefits society in economic, environmental, and personal ways. The wide application of ICTs provides increasing benefits for employees, especially ones with physical disabilities. It also leads to a more energy-saving society without adversely impacting economic growth.[55] Telecommuting offers benefits to communities, employers, and employees. For communities, telecommuting may offer fuller employment (by increasing the employability of circumstantially marginalized groups such as work at home parents and caregivers, the disabled, retirees, and people living in remote areas), reducing traffic congestion and traffic accidents, relieving pressure on transportation infrastructure, reducing greenhouse gases, reducing energy use, and improving disaster preparedness.[56]		For companies, telecommuting expands the talent pool, reduces the spread of illness, reduces costs including real-estate footprint, increases productivity, reduces their carbon footprint and energy usage, offers a means of complying with the Americans with Disabilities Act of 1990 (ADA) and possibly earning a tax credit, if they're American, reduces turnover and absenteeism, improves employee morale, enhances continuity-of-operations strategies, improves their ability to handle business across multiple time zones, and augments their cultural adaptability. Some estimates suggest that full-time telework can save companies approximately $20,000 per employee.[57]		Telecommuting individuals, or more specifically those in "work from home" arrangements, may find that it improves work-life balance, reduces their carbon footprint and fuel usage, frees up the equivalent of 15 to 25 workdays a year (time they would have otherwise spent commuting), and saves thousands of dollars per year in travel and work-related costs.[58][59] Half-time telecommuting by those with compatible jobs (40%) and a desire to do so (79%) would save companies, communities, and employees over $650 billion a year; the result of increased productivity, reduced office expense, lower absenteeism and turnover, reduced travel, less road repairs, less gas consumption, and other savings.[60]		Telecommuting gained ground in the United States in 1996 after "Clean Air Act amendments were adopted with the expectation of reducing carbon dioxide and ground-level ozone levels by 25 percent."[61] The act required companies with over 100 employees to encourage car pools, public transportation, shortened work weeks, and telecommuting. In 2004, an appropriations bill was enacted by Congress to encourage telecommuting for certain Federal agencies. The bill threatened to withhold money from agencies that failed to provide telecommuting options to all eligible employees.		If the 40% of the U.S. population that holds telework-compatible jobs and wants to work from home did so half of the time:		In the UK, it has been estimated that increasing the numbers of employees working from home could save over 3 million tonnes of carbon pollution each year, in addition to the economic benefits of cutting costs by GBP 3 billion a year for UK employers and employees.[63]		According to the job characteristic theory, the relationship between characteristics of the job and job satisfaction was moderately strong.[64] Of the five task characteristics, autonomy has a strong relationship with job satisfaction such that greater autonomy leads to greater job satisfaction.[64] Teleworkers may have increased satisfaction due to the flexibility and autonomy their jobs provide. Teleworkers were found to have higher satisfaction than office based workers.[65][66] It was found that autonomy increased teleworkers' satisfaction by reducing work-family conflicts,[37][67] especially when workers were allowed to work outside of traditional work hours and be more flexible for family purposes.[38] Additionally, autonomy explained an increase in employee engagement when the amount of time spent teleworking increased.[33] Furthermore, a study from FlexJobs that surveyed over 3000 people found that 81 percent of respondents also said they would be more loyal to their employers if they had flexible work options.[68]		Telecommuting has long been promoted as a way to substantially increase employee productivity. A working-from-home-related experiment conducted using 242 employees of a large Chinese travel agency by professors at Stanford and Beijing University found that employees randomly assigned to work at home for 9 months increased their output by 13.5% versus the office-based control group. This improvement in output arose from working 9% more hours from saved commuting time and from 3.5% improved efficiency from quieter working conditions. The study also found that home-workers reported significantly higher job-satisfaction scores and their quit rates fell by almost 50%. However, home workers' promotion rates dropped by half due to apparent performance declines, indicating a potential career cost of home-working.[69]		Telework flexibility is a desirable prerequisite for employees. A 2008 Robert Half International Financial Hiring Index, a survey of 1,400 CFOs by recruitment firm Robert Half International, indicated that 13% consider telework the best recruiting incentive today for accounting professionals.[70] In earlier surveys, 33% considered telework the best recruiting incentive, and half considered it second best.[71]		Since work hours are less regulated in telework, employee effort and dedication are far more likely to be measured purely in terms of output or results. Fewer, if any, traces of non-productive work activities (research, self-training, dealing with technical problems or equipment failures) and time lost on unsuccessful attempts (early drafts, fruitless endeavors, abortive innovations) are visible to employers. Piece rate, commissions, or other performance-based compensation also become more likely for telecommuters. Furthermore, major chunks of per-employee expenses are absorbed by the telecommuter himself - from simple coffee, water, electricity, and telecommunications services, to huge capital expenses like office equipment or software licenses. Thus, hours spent on the job tend to be underestimated and expenses under-reported, creating overly optimistic figures of productivity gains and savings, some or all of those in fact coming out of the telecommuter's time and pocket.[citation needed]		International evidence and experience shows that telework can deliver a broad range of benefits to individuals, employers and society as a whole. Telework is a shift in the way business is accomplished which can make a difference overtime. As an example, a recent Australian study revealed that telework enabled by the National Broadband Network is expected to add $8.3 billion to Gross Domestic Product by 2020, creating the equivalent of an additional 25,000 full-time jobs. Around 10,000 of these jobs will be in regional Australia. When it comes to environment, it has been estimated that if 10 per cent of Australian employees were to telework 50 percent of the time, it would save 120 million litres of fuel and 320,000 tonnes of carbon emissions. That rate of telework would also deliver a productivity benefit of between $1.4 billion and $1.9 billion a year.[72]		Turnover intention, or the desire to leave the organization, is lower for teleworkers.[34] Those teleworkers who experienced greater professional isolation actually had lower turnover intent.[73] One study found that by increasing feedback and task identity through clear communication of goals, objectives, and expectations, turnover intent decreased in teleworkers and quality of work output increased.[74]		A meta-analysis of 46 studies of telecommuting involving 12,833 employees conducted by Ravi Gajendran and David A. Harrison in the Journal of Applied Psychology, published by the American Psychological Association (APA), found that telecommuting has largely positive consequences for employees and employers.[75][76] In their meta-analytic study, Gajendran and Harrison found that telecommuting had modest but beneficial effects on employees' job satisfaction, perceived autonomy, stress levels, manager-rated job performance, and (lower) work-family conflict. Telecommuting also reduces turnover intent, or the intention to quit one’s job. Increased job satisfaction, decreased turnover intent and role stress related to telecommuting partly because of a decrease in work-family conflict. Additionally, the increase in autonomy from teleworking in turn increases job satisfaction.[citation needed] Although a number of scholars and managers[77] had previously expressed fears that employee careers might suffer and workplace relationships might be damaged because of telecommuting, the meta-analysis found that there are no generally detrimental effects on the quality of workplace relationships and career outcomes. Telecommuting actually was found to positively affect employee-supervisor relations and the relationship between job satisfaction and turnover intent was in part due to supervisor relationship quality. Only high-intensity telecommuting (where employees work from home for more than 2.5 days a week) harmed employee relationships with co-workers, even though it did reduce work-family conflict.		Skill variety has the strongest relationship with internal work motivation.[31] Jobs that allow workers to use a variety of skills increase workers’ internal work motivation. If teleworkers are limited in teamwork opportunities and have fewer opportunities to use a variety of skills,[42] they may have lower internal motivation towards their work. Also, perceived social isolation can lead to less motivation.[78] It can be argued that without a work climate or manager nearby, the ability to motivate oneself is even more important when telecommuting than when working in an office. Though working in an office has its distractions, it is often argued that telecommuting involves even greater distractions. According to one study, children are ranked as the number one distractions, followed by spouses, pets, neighbors, and solicitors. The lack of proper tools and facilities also serves as a major distraction,[79] though this can be mitigated by using short-term coworking rental facilities.		Face-to-face interactions increase interpersonal contact, connectedness, and trust[47] Therefore, 54% of teleworkers thought they lost out on social interaction and 52.5% felt they lost out on professional interaction in a 2012 study.[78] Teleworking can hurt working relationships between the teleworker and their coworkers, especially if their coworkers do not telework. Coworkers who do not telework can feel resentful and jealous because they may consider it unfair if they are not allowed to telework as well.[34][45] However, despite fewer interpersonal actions and professional isolation,[47] a meta-analysis of telecommuting did not find support for negative telecommuter-coworker relationships or telecommuter-supervisor relationships.[34] Employers' largest concerns about telecommuting are fear of loss of control; 75% of managers say they trust their employees, but a third say they'd like to be able to see them, "just to be sure".[80]		Employees who telework may feel pressure to produce more output in order to be seen as valuable, and reduce the idea that they are doing less work than others. This pressure to produce output, as well as a lack of social support from limited coworker relationships and feelings of isolation, leads to lower job engagement in teleworkers.[33] Additionally, higher-quality relationships with teammates decreased job satisfaction of teleworkers, potentially because of frustrations with exchanging interactions via technology.[81] However, coworker support and virtual social groups for team building had a direct influence on increasing job satisfaction,[65][82] perhaps due to an increase in skill variety from teamwork and an increase in task significance from more working relationships.		The inconsistent findings regarding telework and satisfaction may be explained by a more complicated relationship. Presumably because of the effects of autonomy, initial job satisfaction increases as the amount of telecommuting increases; however, as the individual telecommutes more, declines in feedback and task significance lead job satisfaction to level off and decrease slightly.[83] Thus, the amount of time teleworking influences the relationship between telework and job satisfaction. Barriers to continued growth of telecommuting include distrust from employers and personal disconnectedness for employees.[84] In the telework circumstance, employees and supervisors have to work harder to maintain relationships with co-workers.[85] An isolation from daily activities arise of the company and may be less aware of other things going on to the company and a possible hatred from other employees arises from other employees who do not telecommute.[86] Telecommuting has come to be viewed by some as more of a "complement rather than a substitute for work in the workplace".[87]		Security must be addressed for teleworkers and non-teleworkers as well. In 2006, a United States Department of Veterans Affairs employee's stolen laptop represented what was described as "potentially the largest loss of Social Security numbers to date".[88] While he was not a telecommuter, this incident brought attention to the risks inherent in working off-site. Ninety percent of executives charged with security in large organizations feel that telework is not a security concern. They are more concerned with the occasional work that's taken out of the office by non-teleworkers because they lack the training, tools, and technologies that teleworkers receive.[89] In other studies regarding Job Characteristics Theory, job feedback seemed to have the strongest relationship with overall job satisfaction compared to other job characteristics.[31] While teleworking, communication is not as immediate or rich as face-to-face interactions.[24] Less feedback when teleworking is associated with lower job engagement.[33] Thus, when perceived supervisor support and relationship quality between leaders and teleworkers decreases, job satisfaction of the teleworker decreases.[37][90] The importance of manager communication with teleworkers is made clear in a study that found that individuals have lower job satisfaction when their managers telework.[38]		Managers may view the teleworker as experiencing a drop in productivity during the first few months. This drop occurs as "the employee, his peers, and the manager adjust to the new work regimen".[91] The drop could also be due to inadequate office setup. Additionally, a 1999 study claimed that "70 minutes of each day in a regular office are wasted by interruptions, yakking around the photocopier, and other distractions".[92] Over the long term, though, surveys found that productivity of the teleworker will climb; over two-thirds of employers report increased productivity among telecommuters, according to a 2008 survey.[93] Traditional line managers are accustomed to managing by observation and not necessarily by results. This causes a serious obstacle in organizations attempting to adopt telecommuting. Liability and workers' compensation can become serious issues as well.[94] Weaker relationships between job dimensions and job outcomes, such as job performance and absenteeism,[32] may explain why the results regarding performance and telework are conflicting. Some studies have found that telework increases productivity in workers[95] and leads to higher supervisor ratings of performance and higher performance appraisals.[34] However, another study found that professional isolation in teleworkers led to a decrease in job performance, especially for those who spent more time teleworking and engaged in fewer face-to-face interactions.[73] Thus, similar to job attitudes, the amount of time spent teleworking may also influence the relationship between telework and job performance.		Teleworking can negatively affect a person's career. A recent survey of 1,300 executives from 71 countries indicated that respondents believe that people who telework were less likely to get promoted. Companies rarely promote people into leadership roles who haven't been consistently seen and measured.[96] A decrease in productivity due to continual procrastination with a lack of supervision will result to a poor performance in the quality of work of the employee. These factors are part of the negative influence that telework may have on a person's career.[97]		Work-at-home and telecommuting scams are very common; many of these job offers are scams claiming that people can "get rich quick" while working from home. In fact, these scams require an investment up front with no pay-off at the end.[98] The problem is so pervasive that in 2006 the US Federal Trade Commission (FTC) established 'Project False Hopes', a Federal and state law enforcement sweep that targeted bogus business opportunity and work-at-home scams. The crackdown involved more than 100 law enforcement actions by the FTC, the Department of Justice, the United States Postal Inspection Service, and law enforcement agencies in 11 states. In four of the new FTC cases alone, consumers lost more than $30 million.[citation needed] "Bogus business opportunities trample on Americans’ dreams of financial independence", said FTC Chairman Deborah Platt Majoras. "If a business opportunity promises no risk, little effort, and big profits, it almost certainly is a scam. These scams offer only a money pit, where no matter how much time and money is invested, consumers never achieve the riches or financial freedom that they were promised."[99] The FBI warned of such scams on February 2009, as well.		Of the more than three million web entries resulting from a search on the phrase "work at home", more than 95% of the results were scams, links to scams, or other dead ends. Work at home scams earn their perpetrators more than $500 million per year, and home business scams account for another $250 million per year. Even the sites that claim to be scam-free often feature ads that link to scams.[100] According to Christine Durst, CEO of Staffcentrix, there is a 48-to-1 ratio of scams to legitimate offerings among work-at-home job leads on the Internet.[101]		Businesses often provide teleworkers access to corporate in-house applications, accessible by a remote device such as a tablet or laptop. These devices are gaining popularity in the workforce but come with different underlying operating systems and therefore a variety compatibility issues. However, with the use of desktop virtualization, specifically remote desktop virtualization, any legacy application or operating system can be accessed from a mobile device, as this device is primary used as a display unit while the processing is performed on the company's internal server.		If all Federal employees who are eligible to telework full-time were to do so, the Federal Government could realize $13.9 billion savings in commuting costs annually and eliminate 21.5 billion pounds (9,800,000 t; 9,600,000 long tons) of pollutants from the environment each year.[102] Events in 2007 have pushed telework to the forefront as a critical measurement for the U.S. federal government. Telework relates to continuity of operations (COOP) and national pandemic preparedness planning, reducing dependence on foreign oil and the burden of rising gas prices, the Defense Base Closure and Realignment Commission (BRAC), and a focus on recruitment and retention. During a keynote address at the September 12, 2007 Telework Exchange Town Hall Meeting, Lurita Doan, at that time the Administrator for the General Services Administration, announced an aggressive commitment goal to increase agency telework participation. Her challenge would enable 50 percent of eligible agency employees to telework one or more days per week by 2010. As of 2007[update], 10 percent of eligible GSA employees telework, compared to 4.2 percent for the overall Federal workforce. Her goals were to increase participation to 20 percent by the end of 2008, 40 percent by the end of 2009, and finally 50 percent by 2010.[103]		A 2007 study of National Science Foundation employees indicated that approximately one-third participated in telework regularly, characterized staff satisfaction with the program, and noted savings in employee time and greenhouse-gas emissions as a result of telework.[104][105] Rep. Sarbanes (D-MD) introduced the Telework Improvements Act of 2009 in March 2009. Co-sponsors of the bill included Reps. Connolly (D-VA), Wolf (R-VA), and Capito (R-WV). The bill requires each executive agency to establish a policy under which employees may be authorized to telework to the maximum extent possible without diminishing employee performance or agency operations. At the same time in the U.S. Senate, Sen. Akaka (D-HI) introduced the companion bill, along with Sens. Landrieu (D-LA) and Voinovich (R-OH).[106]		On May 24, 2010, the Senate passed the Telework Enhancement Act (S. 707) sponsored by Sens. Daniel Akaka (D-Hawaii) and George Voinovich (R-Ohio). The bill grants Federal employees eligibility to telework and requires Federal agencies to establish telework policies and identify telework managers.[107] On July 14, 2010, the House passed the Telework Improvements Act of 2010 (H.R. 1722) with a vote of 290-131. The U.S. Senate passed the final version of the legislation by unanimous consent on September 29, 2010 and the House passed it with a bipartisan vote of 254-152 on November 18, 2010.[108] On December 9, 2010 President Obama signed H.R. 1722, the Telework Enhancement Act of 2010, into law.[109] The Telework Enhancement Act of 2012 provided a framework for U.S. agencies to offer teleworking as a viable option to employees. By increasing the number of employees who telework, the Telework Enhancement Act has three main objectives. (1) Improve continuity of operations, (2) Promote management Effectiveness and (3) Enhance work-life balance.[110]		The 2012 Status Telework in the Federal Government features teleworking highlights from the past 18 months as well as goals for improving teleworking in the future. Reports finding that all 87 agencies participating in the Data Cell had established telework policies and 73 percent of the policies met the Telework Act Requirements. More than 684,000 federal employees were deemed eligible to telework, this represents approximately 32 percent of all federal employees. More than 144,000 federal employees had written teleworking agreements with their agencies. 27 percent of teleworkers worked remotely three or more days per week.[111] In addition to the findings, the reports examine teleworking at the Department of Defense. According to the report, there are more than 793,000 employees in the DoD and of those employees, 134,477 were deemed eligible for teleworking. Overall, the federal government seems to have embraced teleworking and is attempting to create more remote working opportunities for employees. In closing, the report listed several ways that the government could make more jobs available through telework. Suggestions include using telework as a tool to retain employees at or near retirement age and using telework to expand hiring of highly trained disabled veterans.[111]		Telework centers are offices that are generally set up close to a majority of people who might otherwise drive or take public transit. They usually feature the full complement of office equipment and a high-speed Internet connection for maximum productivity. Some feature support staff, including receptionists or administrators. For example, a number of telework centers have been set up around the Washington Metropolitan Area: 7 in Maryland, 8 in Virginia, 3 in Washington, D.C. and 1 in West Virginia. Telework centers allow people to reduce their commute yet still work in a traditional office setting. Some telework centers are set up by individual companies while others are established by independent organizations for use by many organizations. Telework centers are attractive to people who do not have the space or inclination to work from home. They offer employers the ability to maintain a more formal structure for their workforce.		These work arrangements are more likely to become more popular with current trends towards greater customization of services and virtual organizing. Distributed work offers great potential for firms to reduce costs, enhance competitive advantage and agility, access a greater variety of scarce talents, and improve employee flexibility, effectiveness and productivity.[112][113][114][115] It has gained in popularity in the West, particularly in Europe. While increasing in importance, distributed work has not yet gained widespread acceptance in Asia.[116]		Remote office centers (ROCs) are distributed centers for leasing offices to individuals from multiple companies. A remote office center provides professional grade network access, phone system, security system, mail stop and optional services for additional costs. ROCs are generally located in areas near to where people live throughout population centers, so that workers do not have to commute more than a couple of miles. The telecommuter works in a real office but accesses the company network across the internet using a VPN just as in traditional telecommuting.		This type of arrangement does not share fully in the benefits of home-based telecommuting, but can address the needs of employees who are unable or unwilling to work from home.		"To hackers who make a living stealing information from unsecured computers and network connections, the teleworker could be an open the door to the organization’s most sensitive data. Security and privacy have become increasingly rare commodities these days thanks to the ability of hackers to stay one step ahead of just about every security measure that technicians can create. Security breaches are a significant enough threat in a standard office environment; however, when an organization has employees working from home or on the go, these risks become even greater.		It is vital for organizations to convey to teleworkers that data protection and information security are important to an organization, and employees’ actions make a difference in achieving the overall goal of protection of sensitive data. Despite increased awareness and training on security issues, many employees do not take the necessary precautions for deterring security risks.		Real security begins with security policy. The Information Security professional must ensure that the security policy covers telecommuting/teleworking and who may telework, services available to teleworkers, information restrictions, identification/authentication/authorization, equipment and software specifications, integrity and confidentiality, maintenance guidelines, and robust user education."[117]				According to an article from New York Times, telecommuting now takes about 2.6 percent of the American workforce not including remote works like drivers. The article also mentions an experiment done by Nicholas Bloom. Nicholas Bloom is an economics professor from Stanford University. During this experiment, 250 workers were picked randomly from Ctrip to work either at home or at an office. Ctrip is a large China travel agency. The result showed that those who telecommuted worked longer hours than those who worked at an office. The telecommuters were also more productive and happier. Ctrip saved around 2K from telecommuting. Although the quitting rate decreased for telecommuters, the promotion rate also decreased. Many telecommuters asked to be back in the office at the end with reasoning like loneliness and desire for promotion. Kate Lister, president of Global Workplace Analytics, came to the conclusion that most workers prefer telecommuting and office work combined. Telecommuting increases efficiency and workers’ flexibility.[118] America has an increasing trend of using teleworking due to its strong economics and multimedia services. Among the top 10 telecommuter countries, U.S is ranked number one;[119] however, developing countries like China is also catching up to the trend.An article from money.163.com states that the number of telecommuters in the Asia pacific region exceeds region like America, Europe, Middle East and Africa. Asia Pacific region has about 37% telecommuters while the others have about 23-4%. Chinese citizens also favor the combination of telecommuting and office work due to reason like disturbance at work and increase in flexibility.[120] Not all workers have the chance to telecommute. One of the ethical issues behind telecommuting is who should have the chance to telecommute? One may have more chance to work at home because he/she has young children. The other one may argue he/she also has personal problems. It is favored by most workers to combine telecommuting and office work. Many think that telecommuting once or twice a week is a reasonable schedule. Businesses also favor this suggestion because workers are more satisfied and companies save money from it.		Some companies, particularly those where employees spend a great deal of time on the road and at remote locations, offer a hotdesking or hoteling arrangement where employees can reserve the use of a temporary traditional office, cubicle or meeting room at the company headquarters, a remote office center, or other shared office facility.		Coworking is a social gathering of a group of people who are still working independently, but who share a common working area as well as the synergy that can happen from working with people in the same space. Coworking facilities can range from shared space in formal offices to social areas such as a coffee shop. Entrepreneurs and social entrepreneurs often cowork in shared office and workshop facilities provided by business incubators and business accelerator organizations. In entrepreneurship, coworking allows creative start-up founders, researchers and knowledge workers to meet and share ideas, collaborate, share new research, and find potential partners.		Distributed work entails the conduct of organizational tasks in places that extend beyond the confines of traditional offices or workspaces. It can refer to organizational arrangements that permit or require workers to perform work more effectively at any appropriate location—such as their homes or customers' sites—through the application of information and communication technology. An example is financial planners who meet clients during the client's lunchtime at the client's workplace; even though this is an out-of-the-office, meeting, the Internet enables the planner to present financial planning tools and presentations on their mobile computers. Another example is a publishing executives who recommends books and places orders for the latest book offerings to libraries and university professors from the executive's home using e-mail or an online system. If this type of distributed work replaces the worker's commute, it would be considered telecommuting. If not, it would be telework (see §1. Definition).		
Slow living is a lifestyle emphasizing slower approaches to aspects of everyday life.[1] The concept of "slow" lifestyles started with the slow food movement, which emphasizes more "traditional" food production processes as a reaction to fast food that emerged in Italy during the 1980s and 1990s. Slow food and slow living are frequently, but not always, proposed as solutions to what the green movement describes as problems in materialistic and industrial lifestyles.		People every day are constantly living at a fast pace which is making them feel like their lives are chaotic but with slow living they end up taking a step back and start enjoying life being conscious of sensory profusion. Slow living also incorporates slow food, slow money, and slow cities. The term slow is a movement or action at a relaxed or leisurely pace.[2]		This approach to life lived slowly does not refuse the usage of things such as mobile phones, internet, and access to goods and services.[3]		The term slow is used as an acronym to show different issues:		S = Sustainable – not having an impact		L = Local – not someone else’s patch		O = Organic – not mass-produced		W = Whole – not processed[4]						Slow food movement was originally known as Arcigola but was renamed Slow Food in 1989 in Italy. In 85 countries it has over 78,000 members that include Japan, Australia and the US. This movement continues to grow in reputation and in membership. Slow food is a concept, were the target is in the taste, comfort, and quality of the food that is natural using locally sourced ingredients.[5]		
In English peon (/ˈpiːɒn/), from the Spanish peón [peˈon]) usually refers to a person subject to peonage: any form of unfree labor or wage labor in which a laborer (peon) has little control over employment conditions. In this sense, peon and peonage are, strictly speaking, used accurately only of the colonial period in Latin America and other countries colonized by Spain. However, the word peon has a variety of related, less formal uses.						In English, peon and peonage have meanings related to their Spanish etymology, as well as a variety of other usages.[1] In addition to the meaning of forced laborer, a peon may also be a person with little authority, often assigned unskilled tasks; an underling or any person subjected to capricious or unreasonable oversight. In this sense, peon can be used in either a derogatory or self-effacing context.		However, the term has a historical basis and usage related to much more severe conditions of forced labor.		There are other usages in contemporary cultures:		The origin of peonage goes back to the Spanish conquest of Mexico when conquistadors forced natives to work for Spanish planters and mine operators. Peonage was prevalent in Latin America especially in the countries of Mexico, Guatemala, Ecuador, and Peru. It remains an important part of social life, as among the Urarina of the Peruvian Amazon.[3]		After the American Civil War of 1861–1865, peonage developed in the Southern United States. Poor white farmers and formerly enslaved African Americans known as freedmen who could not afford their own land would farm another person's land, exchanging labor for a share of the crops. This was called sharecropping and initially the benefits were mutual. The land owner would pay for the seeds and tools in exchange for a percentage of the money earned from the crop and a portion of the crop. As time passed, many landowners began to abuse this system. The landowner would force the tenant farmer or sharecropper to buy seeds and tools from the land owner’s store, which often had inflated prices. As sharecroppers were often illiterate, they had to depend on the books and accounting by the landowner and his staff. Other tactics included debiting expenses against the sharecropper's profits after the crop was harvested and "miscalculating" the net profit from the harvest, thereby keeping the sharecropper in perpetual debt to the landowner. Since the tenant farmers could not offset the costs, they were forced into involuntary labor due to the debts they owed the land owner.		After the U.S. Civil War, the South passed "Black Codes", laws that tried to control freed black slaves. Vagrancy laws were included in these Black Codes. Homeless or even unemployed African Americans who were between jobs, most of whom were former slaves were arrested and fined as vagrants. Usually lacking the resources to pay the fine, the "vagrant" was sent to county labor or hired out to a private employer. The authorities also tried to restrict the movement of freedmen between rural areas and cities, to between towns. Under such laws, local officials arbitrarily arrested tens of thousands of freedmen and charged them with fines and court costs of their cases. White merchants, farmers, and business owners could pay their debts and the prisoner had to work off the debt. Prisoners were leased as laborers to owners and operators of coal mines, lumber camps, brickyards, railroads, quarries, and farm plantations, with the revenues for their labor going to the states. Government officials leased imprisoned blacks and whites to small town entrepreneurs, provincial farmers, and dozens of corporations looking for cheap labor. Their labor was repeatedly bought and sold for decades after the official abolition of American slavery.[4]		Southern states and private businesses boomed with this unpaid labor. It is estimated that up to 40% of blacks in the South were trapped in peonage in the beginning of the 20th century. Overseers and owners often used severe deprivation, beatings, whippings, and other abuse as "discipline" against the workers.[5]		After the Civil War, the Thirteenth Amendment prohibited involuntary servitude such as peonage for all but convicted criminals. Congress also passed various laws to protect the constitutional rights of Southern blacks, making those who violated such rights by conspiracy, by trespass, or in disguise, guilty of an offence punishable by ten years in prison and civil disability. Unlawful use of state law to subvert rights under the Federal Constitution was made punishable by fine or a year's imprisonment. Until the involuntary servitude was abolished by president Lyndon B. Johnson in 1966 (exact date unknown), sharecroppers in Southern states were forced to continue working to pay off old debts or to pay taxes. Southern states allowed this in order to preserve sharecropping.		In October 1910 Florida sugar cane plantation planter Edgar Watson was shot and killed by his own neighbors; according to legend he would use peonage Native Americans and Negros and then would "pay" his workers by killing them.[6] His story was fictionalized by writer Peter Matthiessen in his Lost River Trilogy later remade into Shadow Country		The following reported Court cases involving Peonage:		Because of the Spanish tradition, peonage was still widespread in New Mexico Territory after the American Civil War. Because New Mexico laws supported peonage, the US Congress passed the Peonage Act of 1867 on March 2, 1867 as follows: "Sec 1990. The holding of any person to service or labor under the system known as peonage is abolished and forever prohibited in the territory of New Mexico, or in any other territory or state of the United States; and all acts, laws, … made to establish, maintain, or enforce, directly or indirectly, the voluntary or involuntary service or labour of any persons as peons, in liquidation of any debt or obligation, or otherwise, are declared null and void."[20] The current version of this statute is codified at Chapter 21-I of 42 U.S.C. § 1994 and makes no specific mention of New Mexico.		
A job, or occupation, is a person's role in society. More specifically, a job is an activity, often regular and performed in exchange for payment ("for a living"). Many people have multiple jobs (e.g., parent, homemaker, and employee). A person can begin a job by becoming an employee, volunteering, starting a business, or becoming a parent. The duration of a job may range from temporary (e.g., hourly odd jobs) to a lifetime (e.g., judges).		An activity that requires a person's mental or physical effort is work (as in "a day's work"). If a person is trained for a certain type of job, they may have a profession. Typically, a job would be a subset of someone's career. The two may differ in that one usually retires from their career, versus resignation or termination from a job.						Most people spend up to forty or more hours each week in paid employment. Some exceptions are children, retirees, and people with disabilities; However, within these groups, many will work part-time, volunteer, or work as a homemaker. From the age of 5 or so, many children's primary role in society(and therefore their 'job') is to learn and study as a student.		Jobs can be categorized, by the hours per week, into full time or part time. They can be categorized as temporary, odd jobs, seasonal, self-employment, consulting, or contract employment.		Jobs can be categorized as paid or unpaid. Examples of unpaid jobs include volunteer, homemaker, mentor, student, and sometimes intern.		Jobs can be categorized by the level of experience required: entry level, intern, and co-op.		Some jobs require specific training or an academic degree.		Those without paid full-time employment may be categorized as unemployed or underemployed if they are seeking a full-time paid job.		Moonlighting is the practice of holding an additional job or jobs, often at night, in addition to one's main job, usually to earn extra income. A person who moonlights may have little time left for sleep or leisure activities.		The Office for National Statistics in the United Kingdom lists 27,966 different job titles, within a website published 2015.[1]		The expression day job is often used for a job one works in to make ends meet while performing low-paying (or non-paying) work in their preferred vocation. Archetypal examples of this are the woman who works as a waitress (her day job) while she tries to become an actress, and the professional athlete who works as a laborer in the off season because he is currently only able to make the roster of a semi-professional team.		While many people do hold a full-time occupation, "day job" specifically refers to those who hold the position solely to pay living expenses so they can pursue, through low paying entry work, the job they really want (which may also be during the day). The phrase strongly implies that the day job would be quit, if only the real vocation paid a living wage.		The phrase "don't quit your day job" is a humorous response to a poor or mediocre performance not up to professional caliber. The phrase implies that the performer is not talented enough in that activity to be able to make a career out of it.		Getting a first job is an important rite of passage in many cultures.[2] The youth may start by doing household work, odd jobs, or working for a family business. In many countries, school children get summer jobs during the longer summer vacation. Students enrolled in higher education can apply for internships or coops to further enhance the probability of securing an entry level job upon graduation.		Résumés summarize a person's education and job experience for potential employers. Employers read job candidate résumés to decide whom to interview for an open position.		Workers often talk of "getting a job", or "having a job". This conceptual metaphor of a "job" as a possession has led to its use in slogans such as "money for jobs, not bombs". Similar conceptions are that of "land" as a possession (real estate) or intellectual rights as a possession (intellectual property).		Manual work seems to shorten one's lifespan.[3] High rank[4] (a higher position at the pecking order) has a positive effect. Professions that cause anxiety have a direct negative impact on health and lifespan.[5] Some data are more complex to interpret due to the various reasons of long life expectancy; thus skilled professionals, employees with secure jobs and low anxiety occupants may live a long life for variant reasons.[6] The more positive characteristics one's job is, the more likely he or she will have a longer lifespan.[7][8] Gender, country,[9] and actual (what statistics reveal, not what people believe) danger are also notable parameters.[10][11]		
In public policy, a living wage is the minimum income necessary for a worker to meet their basic needs.[1] This is not necessarily the same as subsistence, which refers to a biological minimum, though the two terms are commonly confused. Some have defined needs to include shelter, food, and other incidentals such as clothing. Due to the flexible nature of the term 'needs', there is not one universally accepted measure of what a living wage is and as such it varies by location and household type.[2] Some definitions include:		The living wage differs from the minimum wage in that the latter is set by national law and can fail to meet the requirements to have a basic quality of life which leaves the family to rely on government programs for additional income.[1] Living wages, on the other hand, have typically only been adopted in municipalities. In economic terms, the living wage is similar to the minimum wage as it is a price floor for labor. It differs somewhat from basic needs in that the basic needs model usually measures a minimum level of consumption, without regard for the source of the income.		In the 1990s, the first living wage campaigns were launched by community initiatives in the US addressing increasing poverty faced by workers and their families. They argued that employee, employer, and the community win with a living wage. Employees would be more willing to work, helping the employer reduce worker turnover, and it would help the community when the citizens have enough to have a decent life.[5] These campaigns came about partially as a response to Reaganomics and Thatcherism in the US and UK respectively which shifted macroeconomic policy towards neoliberalism.[6]		A living wage, in increasing the purchasing power of low income workers, is supported by Keynesian and post-Keynesian economics which focuses on stimulating demand in order to improve the state of the economy.[6]		A related concept is that of a family wage – one sufficient to not only support oneself, but also to raise a family.						The concept of a living wage, though it was not defined as such, can be traced back to the works of ancient Greek philosophers such as Plato and Aristotle. Both argued for an income that considers needs, particularly those that ensure the communal good.[6] Aristotle saw self-sufficiency as a requirement for happiness which he defines as, ‘that which on its own makes life worthy of choice and lacking in nothing’.[7] As he placed the responsibility in ensuring that the poor could earn a sustainable living in the state, his ideas can be seen an early example of vying for a living wage.		The evolution of the concept can be seen later on in medieval scholars such as Aquinas who argued for a 'just wage'.[6] The concept of a just wage was related to that of just prices, which were those that allowed everyone access to necessities. Prices and wages that prevented access to necessities were considered unjust as they would imperil the virtue of those without access.[8]		Activists argue that a wage is more than just compensation for labor. It is a means of securing a living and it leads to public policies that address both the level of the wage and its decency.[9] In his Wealth of Nations, Adam Smith recognized that rising real wages lead to the "improvement in the circumstances of the lower ranks of people" and are therefore an advantage to society.[10] Growth and a system of liberty were the means by which the laboring poor were able to secure high wages and an acceptable standard of living. Rising real wages are secured by growth through increasing productivity against stable price levels, i.e. prices not affected by inflation. A system of liberty, secured through political institutions whereupon even the "lower ranks of people" could secure the opportunity for higher wages and an acceptable standard of living.		Servants, labourers and workmen of different kinds, make up the far greater part of every great political society. But what improves the circumstances of the greater part can never be regarded as an inconvenience to the whole. No society can surely be ﬂourishing and happy, of which the far greater part of the members are poor and miserable. It is but equity, besides, that they who feed, clothe, and lodge the whole body of the people, should have such a share of the produce of their own labour as to be themselves tolerably well fed, clothed and lodged.		According to living wage advocates Smith advocated that labor should receive an equitable share of what labor produces. For Smith, this equitable share amounts to more than subsistence. Smith equated the interests of labor and the interests of land with overarching societal interests. He reasoned that as wages and rents rise, as a result of higher productivity, societal growth will occur thus increasing the quality of life for the greater part of its members.[9]		Activists argue that the greater good for society is achieved through justice. They argue that government should in turn attempt to align the interests of those pursuing profits with the interests of the labor in order to produce societal advantages for the majority of society. Smith argued that higher productivity and overall growth led to higher wages that in turn led to greater benefits for society. Based on his writings, one can infer that Smith would support a living wage commensurate with the overall growth of the economy. This, in turn, would lead to more happiness and joy for people, while helping to keep families and people out of poverty. Political institutions can create a system of liberty for individuals to ensure opportunity for higher wages through higher production and thus stable growth for society.		In 1891, Pope Leo XIII issued a papal bull entitled Rerum novarum, which is considered the Catholic Church's first expression of a view supportive of a living wage. The Church recognized that wages should be sufficient to support a family. This position has been widely supported by the church since that time, and has been reaffirmed by the papacy on multiple occasions, such as by Pope Pius XI in 1931 Quadragesimo anno and again in 1961, by Pope John XXIII writing in the encyclical Mater et magistra. More recently, Pope John Paul II wrote, "Hence in every case a just wage is the concrete means of verifying the whole socioeconomic system and, in any case, of checking that it is functioning justly."[12]		Contemporary ideas on living wages are put forth through the various campaigns that push for localities to adopt them. Today, one of major supporting groups for the Living Wage is the Universal Living Wage group. The group currently has over 1,500 followers and continues to grow.[13] Contemporary research by Andrea Werner and Ming Lim have analyzed the works of John Ryan, Jerold Waltman and Donald Stabile for their philosophical and ethical insights around living wages.[6]		John Ryan argues for a living wage from a rights perspective. He considers a living wage to be a right that all laborers are entitled to from the ‘common bounty of nature’.[14] His argument is that private ownership of resources precludes access to them by others who would need them to maintain themselves. As such, the obligation to fulfill the right of a living wage rests on the owners and employers of private resources. His argument goes beyond that a wage that should provide mere sustenance but that it should provide humans with the capabilities to ‘develop within reasonable limits all [their] faculties, physical, intellectual, moral and spiritual.’[14]		Jerold Waltman, in A Case for the Living Wage (2004), argues for a living wage not based on individual rights but from a communal, or ‘civic republicanism’, perspective. He sees the need for citizens to be connected to their community and thus sees individual and communal interests as inseparably bound. Two major problems that are antithetical to civic republicanism are poverty and inequality. A living wage is meant to address these by providing the material basis that allows individuals a degree of autonomy and prevents disproportionate income and wealth that would inevitably lead to a societal fissure between the rich and poor. A living wage further allows for political participation by all classes of people which is required to prevent the political interests of the rich from undermining the needs of the poor. These arguments for a living wage, taken together, can be construed seen as necessary elements for ‘social sustainability and cohesion’.[6]		Donald Stabile argues for a living wage based on moral economic thought and its related themes of sustainability, capability and externality.		Broadly speaking, Stabile indicates that sustainability in the economy may require that people have the means for ‘decent accommodation, transport, clothing and personal care'.[6] He qualifies the statement as he sees individual necessities as contextual and therefore able to change over time, between cultures and under difference macroeconomic circumstances.[6] This suggests that the concept and definition of a living wage cannot be made objective over all places and in all times.		Stabile’s thoughts on capabilities make direct reference to Amartya Sen's work on the same subject.[6] The tie-in with a living wage is the idea that income is an important, though not exclusive, means for capabilities. The enhancement of people’s capabilities allows them to better function both in society and as workers. These capabilities are further passed down from parents to children. For more information, see capability approach.		Finally, Stabile analyzes the lack of a living wage as the imposition of negative externalities on others. These externalities take the form of depleting the stock of workers by ‘exploiting and exhausting the workforce’.[6] This leads to economic inefficiency as businesses end up overproducing their products due to not paying the full cost of labor.[6]		Other contemporary accounts have taken up the theme of externalities arising due to a lack of living wage. Muilenburg and Singh see welfare programs, such as housing and school meals, as being a subsidy for employers that allow them to pay low wages.[15] This subsidy, taking the form of an externality, is of course paid for by society in the form of taxes. This thought is reiterated by Grimshaw who argues that employers offset the social costs of maintaining their workforce through tax credits, housing, benefits and other wage subsidies.[16] The issue has been brought up during the Democratic party primary election of 2016 in the United States, when Bernie Sanders released a statement that implicated Wallmart in not paying fair wages and being subsidized by tax payers. The press release states, "Struggling working families should not have to subsidize the wealthiest family in the country".[17]		In 2013 the University of Manchester published a report suggesting that the competition amongst buying organizations has implications for poor wages in countries such as Bangladesh.[18]		In Australia, the 1907 Harvester Judgment ruled that an employer was obliged to pay his employees a wage that guaranteed them a standard of living which was reasonable for "a human being in a civilised community" to live in "frugal comfort estimated by current... standards,"[19] regardless of the employer's capacity to pay. Justice Higgins established a wage of 7/- (7 shillings) per day or 42/- per week as a 'fair and reasonable' minimum wage for unskilled workers. The judgment was later overturned but remains influential. From the Harvester Judgement arose the Australian industrial concept of the "basic wage". For most skilled workers, in addition to the basic wage they received a margin on top of the basic wage, in proportion to a court or commission's judgement of a group of worker's skill levels. In 1913, to compensate for the rising cost of living, the basic wage was increased to 8/- per day, the first increase since the minimum was set. The first Retail Price Index in Australia was published late in 1912, the A Series Index. From 1934, the basic wage was indexed against the C Series Index of household prices. The concept of a basic wage was repeatedly challenged by employer groups through the Basic wage cases and Metal Trades Award cases where the employers argued that the basic wage and margin ought to be replaced by a "total wage". The basic wage system remained in place in Australia until 1967. It was also adopted by some state tribunals and was in use in some states during the 1980s.		In Bangladesh salaries are among the lowest in the world. During 2012 wages hovered around US$38 per month depending on the exchange rate. Studies by Professor Doug Miller during 2010 to 2012, has highlighted the evolving global trade practices in Towards Sustainable Labour Costing in UK Fashion Retail.[20] This white paper published in 2013 by University of Manchester, appears to suggest that the competition among buying organizations has implications to low wages in countries such as Bangladesh. It has laid down a road map to achieve sustainable wages.		Municipal regulation of wage levels began in some towns in the United Kingdom in 1524. National minimum wage law began with the Trade Boards Act 1909, and the Wages Councils Act 1945 set minimum wage standards in many sectors of the economy. Wages Councils were abolished in 1993 and subsequently replaced with a single statutory national minimum wage by the National Minimum Wage Act 1998, which is still in force. The rates are reviewed each year by the country's Low Pay Commission. From 1 April 2016 the minimum wage has been paid as a mandatory National Living Wage for workers over 25. It is being phased in between 2016 and 2020 and is set at a significantly higher level than previous minimum wage rates. By 2020 it is expected to have risen to at least £9 per hour and represent a full-time annual pay equivalent to 60% of the median UK earnings.[21] The National Living Wage is nevertheless lower than the value of the Living Wage calculated by the Living Wage Foundation.[22] Some organisations voluntarily pay a living wage to their staff, at a level somewhat higher than the statutory level.		From September 2014 all NHS Wales staff have been paid a minimum of the "living wage" recommended by the Living Wage Commission. About 2,400 employees received an initial salary increase of up to £470 above the UK-wide Agenda for Change rates.[23]		In the United States, the state of Maryland and several municipalities and local governments have enacted ordinances which set a minimum wage higher than the federal minimum that requires all jobs to meet the living wage for that region. This usually works out to be $3 to $7 above the federal minimum wage. However, San Francisco, California and Santa Fe, New Mexico have notably passed very wide-reaching living wage ordinances.[citation needed] U.S. cities with living wage laws include Santa Fe and Albuquerque in New Mexico; San Francisco, California; and Washington, D.C.[24] The city of Chicago, Illinois also passed a living wage ordinance in 2006, but it was vetoed by Mayor Richard M. Daley.[25] Living wage laws typically cover only businesses that receive state assistance or have contracts with the government.[26]		This effort began in 1994 when an alliance between a labor union and religious leaders in Baltimore launched a successful campaign requiring city service contractors to pay a living wage.[27] Subsequent to this effort, community advocates have won similar ordinances in cities such as Boston, Los Angeles, San Francisco, and St. Louis. In 2007, there were at least 140 living wage ordinances in cities throughout the United States and more than 100 living wage campaigns underway in cities, counties, states, and college campuses.[28] In 2014, Wisconsin Service Employees International Union teamed up with public officials against legislation to eliminate local living wages. According to U.S. Department of Labor data, Wisconsin Jobs Now - a non-profit organization fighting inequality through higher wages - has received at least $2.5 million from SEIU organizations from 2011 to 2013.[29]		Although these ordinances are recent, a number of studies have attempted to measure the impact of these policies on wages and employment. Researchers have had difficulty measuring the impact of these policies because it is difficult to isolate a control group for comparison. A notable study defined the control group as the subset of cities that attempted to pass a living wage law but were unsuccessful.[30] This comparison indicates that living wages raise the average wage level in cities, however, it reduces the likelihood of employment for individuals in the bottom percentile of wage distribution.[citation needed]		Research shows that minimum wage laws and living wage legislation impact poverty differently: evidence demonstrates that living wage legislation reduces poverty.[31] The parties impacted by minimum wage laws and living wage laws differ as living wage legislation generally applies to a more limited sector of the population. It is estimated that workers who qualify for the living wage legislation are currently between 1-2% of the bottom quartile of wage distribution.[31] One must consider that the impact of living wage laws depends heavily on the degree to which these ordinances are enforced.[citation needed]		Neumark and Adams, in their paper, "Do living wage ordinances reduce urban poverty?", state, "There is evidence that living wage ordinances modestly reduce the poverty rates in locations in which these ordinances are enacted. However, there is no evidence that state minimum wage laws do so."[32]		A study carried out in Hamilton, Canada by Zeng and Honig indicated that living wage workers have higher affective commitment and lower turnover intention.[33] Workers paid a living wage were more likely to support the organization they work for in various ways including: "protecting the organizations public image, helping colleagues solve problems, improving their skills and techniques, providing suggestions or advice to a management team, and caring about the organization."[33] The authors interpret these finding through social exchange theory, which points out the mutual obligation employers and employees feel towards each other when employees perceive they are provided favorable treatment.[33]		As of 2003, there are 122 living wage ordinances in American cities and an additional 75 under discussion.[34] Article 23 of the United Nations Universal Declaration of Human Rights states that "Everyone who works has the right to just and favourable remuneration ensuring for himself and for his family an existence worthy of human dignity."		In addition to legislative acts, many corporations have adopted voluntary codes of conduct. The Sullivan Principles in South Africa are an example of a voluntary code of conduct which state that firms should compensate workers to at least cover their basic needs.		In the table below, cross national comparable living wages were estimated for twelve countries and reported in local currencies and purchasing power parity (PPP). Living wage estimates for the year 2000 range from US $1.7 PPP per hour, in low-income examples, to approximately US$11.6 PPP per hour, in high-income examples.[34]		The Living Wage Campaign in the United Kingdom originated in London, where it was launched in 2001 by members of the community organisation London Citizens (now Citizens UK). It engaged in a series of Living Wage campaigns and in 2005 the Greater London Authority established the Living Wage Unit to calculate the London Living Wage, although the authority had no power to enforce it. The London Living Wage was developed in 2008 when Trust for London awarded a grant of over £1 million for campaigning, research and an employer accreditation scheme. The Living Wage campaign subsequently grew into a national movement with local campaigns across the UK. The Joseph Rowntree Foundation funded the Centre for Research in Social Policy (CRSP) at Loughborough University[35] to calculate a UK-wide Minimum Income Standard (MIS) figure, an average across the whole of the UK independent of the higher living costs in London.		In 2011 the CRSP used the MIS as the basis for developing a standard model for setting the UK Living Wage outside of London. Citizens UK, a nationwide community organising institution developed out of London Citizens, launched the Living Wage Foundation and Living Wage Employer mark.[36] Since 2011, the Living Wage Foundation has accredited over 1,800 employers that pay its proposed living wage. The living wage in London is calculated by GLA Economics and the CRSP calculates the out-of-London Living Wage. Their recommended rates for 2015 are £9.40 for London and £8.25 for the rest of the UK.[37] These rates are updated annually in November. In January 2016 the Living Wage Foundation set up a new Living Wage Commission to oversee the calculation of the Living Wage rates in the UK.[38]		In 2012, research into the costs and benefits of a living wage in London was funded by the Trust for London and carried out by Queen Mary University of London.[39] Further research was published in 2014 in a number of reports on the potential impact of raising the UK's statutory national minimum wage to the same level as the Living Wage Foundation's living wage recommendation. This included two reports funded by the Trust for London[40] and carried out by the Institute for Public Policy Research (IPPR) and Resolution Foundation: "Beyond the Bottom Line"[41] and "What Price a Living Wage?"[42] Additionally, Landman Economics published "The Economic Impact of Extending the Living Wage to all Employees in the UK".[43]		A 2014 report by the Living Wage Commission, chaired by Doctor John Sentamu, the Archbishop of York, recommended that the UK government should pay its own workers a "living wage", but that it should be voluntary for the private sector.[44] Data published in late 2014 by New Policy Institute and Trust for London found 20% of employees in London were paid below the Living Wage Foundation's recommended living wage between 2011 and 2013. The proportion of residents paid less than this rate was highest in Newham (37%) and Brent (32%).[45] Research by the Office for National Statistics in 2014 indicated that at that time the proportion of jobs outside London paying less than the living wage was 23%. The equivalent figure within London was 19%.[46] Research by Loughborough University, commissioned by Trust for London, shows 4 in 10 Londoners cannot afford a decent standard of living - that is one that allows them to meet their basic needs and participate in society at a minimum level. This is significantly higher than the 30% that fall below the standard in the UK as a whole. This represents 3.5 million Londoners, an increase of 400,000 since 2010/11. The research highlights the need to improve incomes through better wages, mainly, the London Living Wage, to ensure more Londoners reach a decent standard of living. [47]		Ed Miliband, the leader of the Labour Party in opposition from 2010 until 2015, supported a living wage[48] and proposed tax breaks for employers who adopted it.[49] The Labour Party has implemented a living wage in some local councils which it controls, such as in Birmingham[50] and Cardiff[51] councils. The Green Party also supports the introduction of a living wage, believing that the national minimum wage should be 60% of net national average earnings.[52] Sinn Féin also supports the introduction of a living wage for Northern Ireland. Other supporters include The Guardian newspaper columnist Polly Toynbee, Church Action on Poverty,[53] the Scottish Low Pay Unit, and Bloomsbury Fightback!.[54]		In New Zealand a new social movement, Living Wage Movement Aotearoa New Zealand, was formed in April 2013. It emerged from a loose network that launched a Living Wage Campaign in May 2012. In 2015 there were over 50 faith, union and community member organisations.		In February 2013, independent research by the Family Centre Social Policy Research Unit identified the New Zealand Living Wage as $18.40 per hour. This was increased in 2014 to $18.80 per hour and in 2015 to $19.25 per hour.[55] On July 1, 2016, the Living Wage rose to be 19.80 per hour,[56] while minimum wage rose to 15.25 in April.[57]		On July 1, 2014 the first accredited NZ Living Wage Employers were announced. The twenty businesses for 2014-15 included food manufacturing, social service agencies, community organisations and a café. This number has now increased to 30 businesses.		The proposed law will inform tax-payers of where their investment dollars go and will hold developers to more stringent employment standards. The proposed act will require developers who receive substantial tax-payer funded subsidies to pay employees a minimum living wage. The law is designed to raise quality of life and stimulate local economy. Specifically the proposed act will guarantee that workers in large developmental projects will receive a wage of at least $10.00 an hour. The living wage will get indexed so that it keeps up with cost of living increases. Furthermore, the act will require that employees who do not receive health insurance from their employer will receive an additional $1.50 an hour to subsidize their healthcare expenses. Workers employed at a subsidized development will also be entitled to the living wage guarantee.[58]		Many city officials have opposed living wage requirements because they believe that they restrict business climate thus making cities less appealing to potential industries. Logistically cities must hire employees to administer the ordinance. Conversely advocates for the legislation have acknowledged that when wages aren't sufficient, low-wage workers are often forced to rely on public assistance in the form of food stamps or Medicaid.[58]		James Parrott of the Fiscal Policy Institute testified during a May 2011 New York City Council meeting that real wages for low-wage workers in the city have declined substantially over the last 20 years, despite dramatic increases in average education levels. A report by the Fiscal Policy Institute indicated that business tax subsidies have grown two and a half times faster than overall New York City tax collections and asks why these public resources are invested in poverty-level jobs. Mr. Parrott testified that income inequality in New York City exceeds that of other large cities, with the highest-earning 1 percent receiving 44 percent of all income.		Harvard University students began organizing a campaign to combat the issue of low living wages for Harvard workers beginning in 1998. After failed attempts to get a meeting with Harvard president Neil Rudenstine, The Living Wage Campaign began to take action. As the movement gained momentum, The Living Wage Campaign held rallies with the support of students, alumni, faculty, staff, community members and organizations. Most importantly, the rallies gained the support of the Harvard workers, strengthening the campaign's demands for a higher wage. After various measures trying to provoke change among the administration, the movement took its most drastic measure. Approximately fifty students occupied the office of the president and university administrators in 2001 for a three-week sit-in. While students were in the office of the president, supporters would sleep outside the building to show solidarity. At the end of the sit-in, dining hall workers were able to agree on a contract to raise the pay of workers. After the sit-in, The Living Wage Campaign sparked unions, contract and service workers to begin negotiating for fair wages.[59]		The Miami University Living Wage Campaign began after it became known that Miami University wage was 18-19% below the market value. In 2003 the members of the Miami University Fair Labor Coalition began marching for university staff wages. After negotiations failed between the university and the American Federation of State and County Municipal Employees (AFSCME), workers went on strike. For two weeks workers protested and students created a tent city as a way of showing support for the strikers. Eventually more students, faculty and community members came out to show support. Even the union president at the time also went on a hunger strike as another means of protesting wages. In late 2003 the union was able to make an agreement with the university for gradual raises totaling about 10.25%. There was still an ongoing push for Miami University to adopt a living wage policy.[60]		The Student Labor Action Committee (SLAC) of Johns Hopkins University took action by conducting a sit-in until the administration listen to their demands. In 1999, after a petition with thousands of signatures, Johns Hopkins University president, William R. Brody raised the hourly wage (to only $7.75) but did not include healthcare benefits nor would the wage adjust for inflation. The sit-in began in early 2000 to meet the demands of students for the university to adopt a living wage. A few weeks later, a settlement was made with the administration. SLAC now just ensures that the living wage policy is implemented.[61]		Starting in 2000, the Living Wage and Democracy Campaign of Swarthmore College began as small meetings between students and staff to voice concerns about their wages. Over the next two years the Living Wage and Democracy Campaign voiced concerns to the university administration. As a response in 2002, the wage was increased from $6.66 to $9 an hour. While the campaigners were pleased with this first result, they believed the college still had a long way to go. The college president, Al Bloom created the Ad Hoc Committee to help learn what the living wage was and released a committee report. In the report suggested an hourly wage, childcare benefit, health coverage for employees and families.[62]		Launched in 2009, Asia Floor Wage is a loose coalition of labour and other groups seeking to implement a Living Wage throughout Asia, with a particular focus on textile manufacturing. There are member associations in Bangladesh, Cambodia, Hong Kong S.A.R., India, Indonesia, Malaysia, Pakistan, the Philippines, Sri Lanka, Thailand and Turkey as well as supporters in Europe and North America. The campaign targets multinational employers who do not pay their developing world workers a living wage.[63]		The Living Wage Campaign at the University of Virginia in Charlottesville, Virginia, composed of University students, faculty, staff, and community members, began in 1995 during the administration of University President John Casteen and continues under the administration of President Teresa Sullivan.[64] The campaign has demanded that the university raise wages to meet basic standards of cost-of-living in the Charlottesville area, as calculated by the nonpartisan Economic Policy Institute.[64]		In 2000, the campaign succeeded in persuading university administrators to raise the wage floor from $6.10 to $8.19; however, this wage only applied to direct employees, not contracted workers.[64] In the spring of 2006, the campaign garnered national media attention when 17 students staged a sit-in in the university president's office in Madison Hall. A professor was arrested on the first day of the protest. The 17 students were arrested after 4 days of protest and later acquitted at trial.[65][66]		Beginning in 2010, the campaign has staged a series of rallies and other events to draw attention to the necessity of the living wage for UVA employees. They have also met with members of the administration numerous times, including with the president.[64] In making the argument for a living wage, the campaign has claimed that continuing to pay low wages is inconsistent with the University's values of the "Community of Trust."[64] They have also noted that University President Sullivan's 2011 co-written textbook, The Social Organization of Work, states that, "Being paid a living wage for one's work is a necessary condition for self-actualization."[67] After rallies and meetings in the spring of 2011, President Sullivan posted a "Commitment to Lowest-Paid Employees" on the University President's website including a letter addressed to the Campaign.[68]		On February 8, 2012, the Campaign released a series of demands to University administrators calling for a living wage policy at the University. These demands included a requirement that the University "explicitly address" the issue by Feb. 17. Although University President Teresa Sullivan did respond to the demands in a mass email sent to the University community shortly before the end of the day on February 17, the Campaign criticized her response as "intentionally misleading" and vowed to take action.[69]		On February 18, the campaign announced that 12 students would begin a hunger strike to publicize the plight of low-paid workers.[70]		Criticisms against the implementation living wage laws have taken similar forms to those against minimum wage. Economically, both can be analyzed as a price floor for labor. A price floor, if above the equilibrium price and thus effective, necessarily leads to a “surplus”. In the context of a labor market, this means that unemployment goes up as the number of employers willing to hire people at a “living wage” is below the number they would be willing to hire at the equilibrium wage price. As such, setting the minimum wage at a living wage has been criticized for possibly destroying jobs.[71][72] For more information, see price floor.		Critics have warned of not just an increase in unemployment but also price increases and a lack of entry level jobs due to ‘labor substitutions effects’.[6] The voluntary undertaking of a living wage is criticized as impossible due to the competitive advantage other businesses in the same market would have over the one adopting a living wage.[73] The economic argument would be that, ceteris paribus (all other things being equal), a company that paid its workers more than required by the market would be unable to compete with those that pay according to market rates. See competitive advantage for more information. This criticism ignores possible benefits that come out of higher wages that include: higher efficiency and production gains due to reduced absenteeism and a reduction in recruitment, training and supervision costs.[6]		Another issue that has emerged is that living wages may be a less effective anti-poverty tool than other measures. Authors point to living wages as being only a limited way of addressing the problems of rising economic inequality, the increase of long-term low-wage jobs, and a decline of unions and legal protection for workers.[6] Since living wage ordinances attempt to address the issue of a living wage, defined by some of its proponents as a family wage, rather than as an individual wage, many of the beneficiaries may already be in families that make substantially more than that necessary to provide an adequate standard of living. According to a survey of labor economists by the Employment Policies Institute in 2000, only 31% viewed living wages as a very or somewhat effective anti-poverty tool, while 98% viewed policies like the US earned income tax credit and general welfare grants in a similar vein.[74] On the other hand, according to Zagros Madjd-Sadjadi, an economist with the State of California's Division of Labor Statistics and Research, the living wage may be seen by the public as preferable to other methods because it reinforces the "work ethic" and ensures that there is something of value produced, unlike welfare, that is often believed to be a pure cash "gift" from the public coffers."[75]		The concept of a living wage based on its definition as a family wage has been criticized by feminists for emphasizing the role of men as breadwinners.[6] However, occupations that would most benefit from a living wage, including cleaning, catering, childcare, care for the elderly and the sick, and routine office jobs, are disproportionatly affected by low pay and disproportionatly staffed by women.[76]		
In professional sports, a salary cap (or wage cap) is an agreement or rule that places a limit on the amount of money that a team can spend on players' salaries. It exists as a per-player limit or a total limit for the team's roster, or both. Several sports leagues have implemented salary caps, using it to keep overall costs down, and also to maintain a competitive balance by restricting richer clubs from entrenching dominance by signing many more top players than their rivals. Salary caps can be a major issue in negotiations between league management and players' unions, and have been the focus point of several strikes by players and lockouts by owners and administrators.[1][2]		Salary caps are used by the following major sports leagues around the world:		Recently, several European association football leagues have also discussed introducing salary caps. The Union of European Football Associations introduced a set of Financial Fair Play Regulations in 2011, which limits football clubs' spending relative to their income.		In theory, there are two main benefits derived from salary caps – promotion of parity between teams, and control of costs.[5][6][7]		Primarily, an effective salary cap prevents wealthy teams from certain destructive behaviours, such as signing a multitude of high-paid star players to prevent their rivals from accessing talented players and ensuring victory through superior economic power. With a salary cap, each club has roughly the same economic power to attract players, which contributes to parity by producing roughly equal playing talent in each team in the league, and in turn brings economic benefits, both to the league and to its individual teams.		Leagues need to ensure a degree of parity between teams so that games are exciting for the fans and not a foregone conclusion. The leagues that have adopted salary caps generally do so because they believe letting richer teams accumulate talent affects the quality of the sporting product they want to sell. If only a handful of dominant teams are able to win consistently and challenge for the championship, many of the contests will be blowouts by the superior team, reducing the sport's attractiveness for fans at the stadium and viewers on television. Television revenue is an important part of the income of many sports around the world, and the more evenly matched and exciting the contests, the more interesting the television product, meaning the value of the television broadcast rights is higher. An unbalanced league also threatens the financial viability of the weaker teams, because if there is no long-term hope of their team winning, fans of the weaker clubs will gravitate to other sports and leagues.		One famous example of this occurring was in the Union Association, which operated in 1884. The league was dominated by St. Louis Maroons, whose owner, Henry Lucas, was also league president in an obvious conflict-of-interest situation which is now banned. Lucas bought all the best players for his own franchise, and the Maroons easily won the pennant with a record of 94–19 (.832 winning percentage), 21 games ahead of their nearest rivals. The league folded at the end of the season.		Another famous example is the All-America Football Conference, which operated between 1946 and 1949. Despite starring many top players and innovative coaches, the AAFC was dominated by one team, the Cleveland Browns, who lost only three games in four years and won every championship in the league's four-year existence, being unbeaten in 1948, and winning three of the championships in blowouts. Attendances and profits consistently fell after the league's second season, for the Browns as well as their rivals, and the league folded at the end of 1949.		The need for parity is more pronounced in leagues that use the franchise system rather than the promotion and relegation system, which is used in European football. The structure of a promotion and relegation system means weaker teams struggle against the threat of relegation, adding importance and excitement to the matches of weaker teams. International club competitions such as the UEFA Champions League also means that the top clubs always have something to play for, even in the most unbalanced of national leagues.		A salary cap can also help to control the costs of teams and prevent situations in which a club will sign high-cost contracts for star players in order to reap the benefits of immediate popularity and success, only to later find themselves in financial difficulty because of these costs. Without caps, there is a risk that teams will overspend in order to win now at the expense of long term stability, and team owners who use the same risk-benefit analysis used in business may risk not just the fortunes of their own team but the reputation and viability of the whole league.		Sports fans are generally looking to support a team for life, not just a product to purchase for the short term. If teams regularly go bankrupt or change markets the same way businesses do, then the whole sport looks unstable to the fans, who may lose interest and switch their support to a more stable sport where their team and their rivals are more likely to be playing in the long term.		A salary cap can be defined as a hard cap or a soft cap.		A hard cap represents a maximum amount that may not be exceeded for any reason. Contracts which cause a team to violate a hard cap are subject to major sanctions, including the voiding of violating contracts, and the stripping of championships won while breaching salary cap rules, which happened to the Melbourne Storm in the NRL. Hard caps are designed so that penalties deter breaking the cap, but there are numerous examples of clubs who have occasionally and/or systematically cheated the cap.		A soft cap represents an amount which may be exceeded in limited circumstances, but otherwise exceeding the cap will trigger a penalty which is known in advance. Typically these penalties are financial in nature; fines or a luxury tax are common penalties used by leagues.		A salary floor is a minimum amount that must be spent on the team as a whole, and this is separate from the minimum player salary that is agreed to by the league. Some leagues, in particular the NFL, have a hard salary floor that requires teams to meet the salary floor every year, which helps prevent teams from using the salary cap to minimize costs.		When the salary cap and floor are the same, the result is a standard form contract model of payment, in which each player is paid the same amount, sometimes varying by position. The standard form contract model is used extensively in North American minor leagues.		Before the implementation of salary caps, the economic influence of clubs on player markets was controlled by the reserve clause, which was long a standard clause in professional sports player contracts in the United States. The clause forbade a player from negotiations with another team without the permission of the team holding that player's rights even after the contract's term was completed. This system began to unravel in the 1970s due largely to the activism of players' unions, and the threat of anti-trust legal actions. St. Louis Cardinal star Curt Flood started the ball rolling when he refused a trade to the Philadelphia Phillies. He had been a player for 12 years and felt he should have a voice in where he played baseball. Even though he lost his case in the Supreme Court he was the first baseball player to try to end the reserve clause. Although anti-trust actions were not a threat to baseball, which has long been exempt from anti-trust laws, that sport's reserve clause was struck down by a United States arbitrator as a violation of labor laws.		By the 1990s most players with several years' professional experience became free agents upon the expiry of their contracts and were free to negotiate a new contract with their previous team or with any other team. This situation, called Restricted Free Agency, led to "bidding wars" for the best players—a situation which inherently gave an advantage in landing such players to more affluent teams in larger media markets.		The new Collective Bargaining Agreement (CBA) formulated in 2011 had an initial salary cap of $120 million. While the previous CBA had a salary floor, the new CBA did not have one until 2013. Starting with that season, each team is required to spend a minimum of 88.8% of the cap in cash on player compensation,[8] and 90% in future years. However, the floor is based on total cash spent over each of two four-year periods, the first running from 2013–2016 and the second from 2017–2020. A team can be under the floor in one or more seasons in a cycle without violating the CBA, as long as its total spending during the four-year period reaches the required percentage of the cap.[8] This allows for unforeseen circumstances such as career ending injuries or unexpected player retirements leading to immediate penalty. As a result, teams are not forced to immediately take on a replacement for missing players which allows them to use more organic approaches such as a trade, free agency acquisition or the draft.		The NFL's cap is a hard cap that the teams have to stay under at all times, and the salary floor is also a hard floor; penalties for violating or circumventing the cap and floor regulations include fines of up to $5 million for each violation, cancellation of contracts and/or loss of draft picks.		The cap was first introduced for the 1994 season and was initially $34.6 million. Both the cap and the floor are adjusted annually based on the league's revenues, and they have increased each year. In 2009, the final capped year under that agreement, the cap was $128 million per team, while the floor was 87.6% of the cap. Using the formula provided in the league's collective bargaining agreement, the floor in 2009 was $112.1 million. Under the NFL's agreement with the NFLPA, the effects on the salary cap of guaranteed payments (such as signing bonuses) are, with a few rare exceptions, prorated evenly over the term of the contract.		In transitions, if a player retires, is traded, or is cut before June 1, all remaining bonus is applied to the salary cap for the current season. If the payroll change occurs after June 1, the current season's bonus proration is unchanged, and the next year's cap must absorb the entire remaining bonus.		Because of this setup, NFL contracts almost always include the right to cut a player before the beginning of a season. If a player is cut, his salary for the remainder of his contract is neither paid nor counted against the salary cap for that team. A highly sought-after player signing a long-term contract will usually receive a signing bonus, thus providing him with financial security even if he is cut before the end of his contract.		Incentive bonuses require a team to pay a player additional money if he achieves a certain goal. For the purposes of the salary cap, bonuses are classified as either "likely to be earned", which requires the amount of the bonus to count against the team's salary cap, or "not likely to be earned", which is not counted. A team's salary cap is adjusted downward for NLTBE bonuses that were earned in the previous year but not counted against that year's cap. It is adjusted upward for LTBE bonuses that were not earned in the previous year but were counted against that year's cap.		One effect of the salary cap was the release of many higher-salaried veteran players to other teams once their production started to decline from the elite level. On the other hand, many teams have made a practice of using free agents to restock with better personnel more suited to the team. The salary cap prevented teams with superior finances from engaging in the formerly widespread practice of stocking as much talent on the roster as possible by placing younger players on reserve lists with false injuries while they develop into NFL-capable players. In this respect, the cap functions as a supplement to the 55-man roster limit and practice squad limits.		Generally, the practice of retaining veteran players who had contributed to the team in the past, but whose abilities have declined, became less common in the era of the salary cap.[9] A veteran's minimum salary was required to be higher than a player with lesser experience, which means teams tended to favor cheaper, less experienced prospects with growth potential, with an aim to having a group of players who quickly develop into their prime while still being on cheaper contracts than their peers. To offset this tendency which pushed out veteran players, including those who became fan favorites, the players' association accepted an arrangement where a veteran player who receives no bonuses in his contract may be paid the veteran minimum of up to $810,000, while accounting for only $425,000 in salary-cap space (a 47.5% discount).		The salary cap also served to limit the rate of increase of the cost of operating a team. This has accrued to the owners' benefit, and while the initial cap of $34.6 million has increased to $123 million (maximum in 2009), this is due to large growths of revenue, including merchandising revenues and web enterprises, which ownership is sharing with players as well.		The owners opted out of the CBA in 2008, leading to an uncapped season in 2010.[10] During the season, most NFL teams spent as if there was a cap in place anyway, with the league warning against teams front-loading contracts during the season. The Dallas Cowboys, New Orleans Saints, Oakland Raiders, and Washington Redskins chose to spend money in the spirit of an uncapped year, and in 2012 the Cowboys and Redskins (the top two NFL teams by revenue in 2011)[11] were docked $10 million and $36 million respectively from their salary caps, to be spread over the next two seasons. This $46 million was subsequently divided up among the remaining 26 NFL teams ($1.77 million each) as added cap space (this excludes the Raiders and Saints, the latter of which was also dealing with their ongoing bounty scandal, as both teams were over the cap, though to a lesser degree than the Cowboys and Redskins).[12]		The actions of the league to punish those teams that were acting within their legal bounds during the uncapped year led to a lawsuit against them by the NFLPA. The case argued that the rest of the league colluded to keep average player salaries from rising in a year they expected them to skyrocket and unfairly punished teams that did not collude. The NFL settled the lawsuit with the NFLPA.[13]		Year by Year Salary Cap		A salary cap existed in the early days of the National Hockey League (NHL). During the Great Depression, the league was under financial pressure to lower its salary cap to $62,500 per team and $7,000 per player, forcing some teams to trade away well-paid star players in order to fit the cap.[19]		Prior to the resolution of the 2004–05 lockout, the NHL was the only major North American professional sports league that had no luxury tax, very limited revenue sharing and no salary cap.		During the Original Six era through to the early years of the expansion era, the NHL's strict reserve clause negated the need for a salary cap.		Player salaries first became an issue in the 1970s, when Alan Eagleson founded the NHL Players' Association (NHLPA) and the upstart World Hockey Association began competing with the NHL for players. Not all NHL owners were willing to engage in a bidding war, in particular, Harold Ballard of the Toronto Maple Leafs spent as close to the league minimum on rosters as he could. Since Maple Leaf Gardens was consistently sold out no matter how poorly the Maple Leafs played, Ballard's team was by far the most profitable.		The 1994–95 NHL lockout was fought over the issue of the salary cap. The 1994–95 season was only partially cancelled, with 48 games and the playoffs eventually being played.		Eight NHL franchises were based in Canada at the time of the lockout. Until the 1990s, the Canadian teams usually paid player salaries in Canadian dollars, but with the rise of free agency and a decline in the value of Canadian dollar, players and their agents increasingly demanded to be paid in U.S. dollars. Canadian teams' revenues were, then as now, mostly in Canadian dollars, and the effects of the discrepancy were particularly acute for the small market franchises. The financial difficulties and uncertainties of competing in smaller Canadian markets led to two clubs moving to the U.S.; the Quebec Nordiques to Denver, and the Winnipeg Jets to Phoenix. NHL Commissioner Gary Bettman successfully persuaded the US-based teams to donate towards a pool to mitigate the adverse effects of the exchange rate.		The negotiations for the 2004–05 NHL Collective Bargaining Agreement revolved primarily around players' salaries. The league contended that its clubs spent about 75% of revenues on salaries, a percentage far higher than existed in other North American sports; NHL Commissioner Gary Bettman demanded "cost certainty" and presented the NHLPA with several concepts that the Players' Association considered nothing more than euphemisms for a salary cap, which it had vowed it would never accept. The previous CBA had expired on September 15, 2004, and a lockout ensued, leading to the cancellation of the entire 2004–05 NHL season, the first time a major sports league in North America had lost an entire season to a labor dispute.		The lockout was resolved when the NHLPA agreed to a hard salary cap based on league revenues, with the NHL implementing revenue sharing to allow for a higher cap figure. The NHL salary cap is formally titled the "Upper Limit of the Payroll Range" in the new CBA. For the 2005–06 NHL season, the salary cap was set at US$39 million per team, with a maximum of $7.8 million (20% of the team's cap) for a player. The CBA also mandated the payment of salaries in U.S. dollars, codifying what had been a universal practice for more than a decade.[20]		Revenues for the six Canadian teams that were in the league at the time of the lockout have all increased significantly since then, and because the US dollar fell to relative parity with its Canadian counterpart in the early 2010s, league-wide revenues measured in U.S. dollars were inflated accordingly.		As a result of these factors, the cap was raised each year of the 2005–12 CBA to $64.3 million for the 2011–12 season, with a cap of $12.86 million for a player. The CBA also contains a salary floor which is formally titled the "Lower Limit of the Payroll Range", the minimum that each team must pay in player salaries. The lower limit was originally set at 55% of the cap, but is now defined to be $16 million below the cap, therefore the 2011–12 minimum was $48.3 million.		Since the current CBA was approved after a later lockout in 2012–13, league revenues have stagnated due to a significant fall in the value of the Canadian dollar against the U.S. dollar. The cap was $69 million for the 2014–15 season and will be $74 million for 2016–17.[21]		The difference between the salary cap and a team's actual payroll is referred to as the team's "payroll room" or "cap room".		Each year of an NHL player contract, the salary earned contributes to the team's "cap hit". The basic cap hit of a contract for each year it is effective is the total money a player will earn in regular salary over the life of the contract divided by the number of years it is effective. This, in theory, prevents a team from paying a player different amounts each year in order to load his cap hit in years in which the team has more cap room. Teams still use this practice, however, for other reasons. Performance bonuses also count towards the cap, but there is a percentage a team is allowed to go over the cap in order to pay bonuses. A team must still factor in possible bonus payments, however, which could go over that percentage.		Salaries for players sent to the minors, under most circumstances, do not count towards the cap while they are there. If a player has a legitimate long-term injury, his cap hit is still counted; however, the team is permitted to replace him with one or more players whose combined salary is equal to (or less than) that of the injured player, even if the additional players would put the team over the salary cap. If the team's cap room is larger than the injured player's cap hit, they may take on as much as their cap room; however, the injured player may not return to play until the team is again compliant with the original cap.		The NHL has become the first of the major North American leagues to implement a hard cap while retaining guaranteed player contracts. Guaranteed player contracts in the NHL differ from other sports, notably the NFL, where teams may opt out of a contract by waiving or cutting a player. NHL teams may buy out players' contracts, but must still pay a portion of the money still owed which is spread out over twice the remaining duration of the contract. This does not apply for players over 35 at the time of signing; in this case a team cannot buy out the player's contract to reduce salary. Any other player can be bought out for ⅓ of the remaining salary if the player is younger than 28 at the time of termination, or ⅔ of the remaining salary if the player is 28 or older. Trading cash for players or paying a player's remaining salary after trading him have been banned outright in order to prevent wealthier teams from evading the restrictions of the cap.		Players, agents or employees found to have violated the cap face fines of $250,000 – $1 million and/or suspension. Teams found to have violated the cap face fines of up to $5 million, cancellation of contracts, forfeiture of draft picks, deduction of points and/or forfeiture of game(s) determined to have been affected by the violation of the cap.		The NBA had salary cap in the mid-1940s, but it was abolished after only one season. The league continued to operate without such a cap until 1984–85 season, when one was instituted in an attempt to level the playing field among all of the NBA's teams and ensure competitive balance for the Association in the future. Before the cap was reinstated, teams could spend whatever amount of money they wanted on players, but in the first season under the new cap, they were each limited to $3.6 million in total payroll. Under the 2005 CBA, salaries were capped at 57 percent of basketball-related income (BRI) and lasted for six years until June 30, 2011. The current 2011 CBA set the cap at 51.2 percent of BRI in 2011–12, with a 49-to-51 band in subsequent years. The salary cap for 2016–17 season will be set at $94.14 million, with the salary floor at 84.73 million and the luxury tax limit at $113.29 million.[22]		The NBA uses "soft" cap, meaning that teams were allowed to exceed the cap in order to retain the rights to a player who was already on the team. This provision was known as the "Larry Bird" exception, named after the former Boston Celtics great who was retained by that team until his retirement under the provisions of this rule. The purpose of this rule was to address fan unease over the frequent changing of teams by players under the free agency system, as fans became displeased over their favorite player on their favorite team suddenly bolting to another team. The "Larry Bird" provision of the salary cap gave the player's current team an advantage over other teams in free agent negotiations, thus increasing the chances that a player would stay with his current team. The provision tended to result in most teams being over the cap at any given time. Teams that violated the cap rules faced fines of up to $5 million, cancellation of contracts and/or loss of draft picks, and are prohibited from signing free agents for more than the league minimum. The NBA also has a salary floor, but teams are not penalized as long as their total payroll exceeds the floor at the end of the season.		The NBA also uses "luxury tax" which is triggered if the average team payroll exceeds a certain amount higher than the cap. In this case, the teams with payrolls exceeding a certain threshold had to pay a tax to the league which is divided amongst the teams with lower payrolls. However, this penalty was levied against teams in violation only if the league average also breached a separate threshold.		The NBA implemented a maximum salary for individual players. This was done following a dramatic increase in player salaries, in spite of the salary cap, in the mid-1990s. Under the CBA, a player's maximum possible salary increased along with his time of service in the league. For a player of five years' experience, the maximum salary threshold began at 25% of the salary cap, with annual increases of up to 10.5% possible beyond that for players re-signed by their original team, or 8% annual increases for free agents that signed with new teams. For players of greater experience, the salary limit was higher – but the 10.5% limit on annual increases remained the same.		The 2011 CBA resulted in several major changes to the salary cap scheme.		The cap remains a soft cap; the Bird exception remains in place, but teams have less financial room to retain a player with Bird rights than under the previous agreement.[23]		The new CBA also reduced the maximum length of a contract by a year, and reduced allowable annual raises. Bird free agents are entitled to 5-year contracts with 7.5% raises; all other players (including sign-and-trade acquisitions) are limited to 4-year deals with 4.5% raises. Maximum salaries remain at 25, 30, or 35% of the cap, depending on years of service. A player coming off his rookie scale contract, who would normally be eligible to receive a salary of 25% of the cap, is eligible to receive 30% if he is named MVP, makes an All-NBA Team twice, or is selected as a starter in two All-Star Games.[23]		Substantial changes were made to the luxury tax regime. The dollar-for-dollar tax provisions of the previous CBA remained in effect through the 2012–13 season. Starting in 2013–14, the tax changed to an incremental system. Tax is now assessed at different levels based on the amount that a team is over the tax threshold, which remains at a level above the actual cap. The scheme is not cumulative—each level of tax applies only to amounts over that level's threshold. For example, a team that is $8 million over the tax threshold pays $1.50 for each of its first $5 million over the tax threshold, and $1.75 per dollar for the remaining $3 million. In addition, "repeat offenders", subject to additional tax penalties, are defined as teams that paid tax in four of the five previous seasons. As in the previous CBA, the tax revenue is divided among teams with lower payrolls.[24] However, under the new scheme, no more than 50% of the total tax revenue can go exclusively to teams that did not go over the cap; the use of the remaining 50% was not specified in the new agreement.[23]		Taxpaying teams have additional spending limits under the new agreement. They have a smaller "midlevel exception" (another cap provision that allows teams to go over the cap to sign at least one player per season), and can acquire less salary in a trade. Also, since 2013–14, teams that exceed the tax threshold by $4 million or more cannot receive a player in a sign-and-trade deal.[23]		The midlevel exception itself also changed with the new CBA. The maximum duration of midlevel contracts was reduced from 5 years to 4 for non-taxpaying teams and 3 for taxpaying teams, and maximum allowable raises were also reduced. In addition, the midlevel exception was extended to teams under the salary cap for the first time; these teams received a 2-year exception.[23]		Under the current CBA, teams are allowed to "amnesty" one player before the start of any season, as long as his current contract was signed during the 2005 CBA. The amnestied player is waived from the team; although the player's former team remains obligated to pay his salary under the old contract (with a credit for any salary paid by a future team), that salary is no longer counted for purposes of the cap or luxury tax calculations. This provision can be used only once per team during the duration of the CBA, which lasts for 10 years with either side able to opt out in 2017.[23]		The salary floor, previously 75% of the cap, increased to 85% in 2011–12 and 2012–13, and 90% in future years.[23]		Instead of a salary cap, Major League Baseball implements a luxury tax (also called a competitive balance tax), an arrangement in which teams whose total payroll exceeds a certain figure (determined annually) are taxed on the excess amount in order to discourage large market teams from having a substantially higher payroll than the rest of the league. The tax is paid to the league, which then puts the money into its industry-growth fund.[25][26]		A team that goes over the luxury tax threshold for the first time in a five-year period pays a penalty of 22.5% of the amount they were over the threshold, second-time violators pay a 30% penalty, and teams that exceed the limit three or more times pay a 50% penalty from 2013 onwards. There is also an incentive to lower payroll; if in any year a team goes under the threshold, the penalty rate decreases to 17.5%, 25% or 40% (depending on prior record over the previous five years) for the next time the tax is paid, which will apply from 2013.		The threshold for 2017 is $195 million.		The following teams have been subject to luxury tax as of 2015:		The New York Yankees have paid 73.78% of all luxury tax collected by MLB.		Money collected under the MLB luxury tax are apportioned as follows: The first $2,375,400 and 50% of the remaining total are used to fund player benefits, 25% goes to the Industry Growth Fund, and the remaining 25% is used to defray team's funding obligations from player benefits.[36]		Measuring the success of the luxury tax in bringing the benefits of parity has brought mixed results.		A team with a $100 million plus payroll has won the World Series 10 times (the 2009 Yankees, the 2004, 2007 and 2013 Red Sox, the 2011 St. Louis Cardinals, the 2010, 2012 and 2014 San Francisco Giants, the 2015 Kansas City Royals, and the 2016 Chicago Cubs); however, while $100 million plus payrolls have only existed since 2001, the last team to win the World Series with a payroll less than $100 million was the 2008 Philadelphia Phillies (payroll $98.26 million). However, this can be explained by the fact that the majority of elite players require high salaries when they hit free agency (unless their team extends them beforehand) and teams with those players generally perform better.		While a top tier payroll increases a team's chances of making the playoffs, it does not guarantee they will consistently win championships. On the other hand, the New York Yankees have consistently had the highest total payroll in MLB, and they have appeared in 40 of the 113 World Series for 27 wins as of 2017 (35.5% of all World Series for a 23.9% success rate).		In the past 30 years, 19 different teams have won the World Series. In comparison, only 14 different teams won the NFL Super Bowl, 13 won the NHL Stanley Cup and 8 won the NBA championship in that same time frame.		Other pundits, such as Michael Lewis, the author of the bestseller Moneyball, have argued that using World Series championships as an example of parity may be misleading, and playoff appearances may be a better indicator of relative team strength. The playoff system used in baseball comprises a small number of games compared to success over a long season, and has been described as a "crapshoot" by Oakland A's General Manager Billy Beane (the central figure of Moneyball). In fact, teams with consistently high payrolls, including the New York Yankees and Boston Red Sox, have secured high numbers of playoff berths (the two teams have combined to win the AL East 17 out of 21 seasons from 1994–2014). In contrast, teams with low payrolls are far less likely to make the playoffs: for example, the Pittsburgh Pirates went 20 years without a winning season before making the 2013 playoffs.		A number of the small market teams, notably the Milwaukee Brewers, have called for the introduction of a salary cap, but any introduction is opposed by the MLB players' union and the Yankees' ownership group; the latter have threatened legal action if such a cap is implemented.		Although some saw the success of NHL owners in their 2004–05 lockout as an opportunity for MLB to reform its collective bargaining agreement, baseball owners agreed to a new five-year deal in October 2006 that did not include a salary cap. Unlike the other three major North American sports, MLB also has no team salary floor: the only minimum limits for team payrolls are based on the minimum salaries for individual players of various levels of experience that are written into MLB's collective bargaining agreement.		Here are some major points of the MLS rules and regulations for the 2017 season.[37]		Since the 2012 season, the cap number for designated players has depended on the players' ages. Since the 2013 season, players 20 or younger have counted $150,000 against the cap and those age 21 to 23 have counted $200,000, with older players remaining at the standard cap number ($368,750 for 2013, $387,500 for 2014, $436,250 for 2015, $457,500 for 2016, and $480,625 for 2017). For the purpose of determining a cap number, the player's age is determined solely by his year of birth.[38]		On June 13, 2006, a proposed salary management system featuring a Maximum Salary Expenditure Cap (SEC) was ratified at the Canadian Football League board of governors meeting in Winnipeg, Manitoba.[39] The CFL began enforcing strict salary cap regulation for the 2007 season, which was set at $4.05 million with a salary floor of $3,746,250. The cap will be set at $5.15 million for the 2017 season or an average salary of $111,956 per active roster player.[40] However, most clubs spend between $7,000,000 to $8,000,000 per season on salaries due to injury exemptions allowed under the cap.		Penalties for teams found to have breached the salary cap or salary floor regulations are:		[41]		The following breaches of the salary cap have occurred (no team has yet been penalized for violating salary floor regulations):[42]		Salary caps are common in other leagues.		The salary cap of the first Arena Football League was $1.82 million per team in its final season in 2008. In 2005, the Tampa Bay Storm were fined $125,000 for salary cap violations and their head coach Tim Marcum was suspended for four games (last two of the 2005 season and first two of the 2006 season) and fined $25,250; Marcum was suspended for a fifth game the next day for criticizing the decision at a press conference.		When the Arena Football League returned in 2010, it instituted a standard salary of $400 per game and a salary cap of $1.5 million, considerably lower than that paid by teams in the previous AFL; given that the new AFL had a 16-game season in 2010, this effectively means that its players are semi-professional.		The National Women's Soccer League, launched in 2013, was initially planned to have a team cap of $500,000, but that was later lowered to $200,000.[43] However, the sport's three North American national federations—the United States Soccer Federation, which runs the league; the Canadian Soccer Association; and the Mexican Football Federation—committed to paying the league salaries of many national team players. For the league's first season, 23 US players, plus 16 players each from Canada and Mexico, had their salaries paid by their respective federations; these players' salaries do not count against the team cap.[44] In a player allocation held before the inaugural season, each of the eight charter teams received two Canadian and two Mexican internationals; seven of the eight teams received three US internationals and the Western New York Flash received two.[44]		Salary caps are rarely used in Europe. However, several European rugby competitions, as well as ice hockey leagues have successfully instituted salary caps. Rugby league's Super League, mainly in England with a team also in France (and formerly one in Wales), is capped. The league has used promotion and relegation for most of its history, though from 2009 through 2014 it operated on a licensing system with some similarities to the North American franchising model. Promotion and relegation returned to Super League in the 2015 season. In rugby union, two of the continent's three main domestic/regional leagues—the English Premiership and the French Top 14—instituted caps despite both being at the top of extensive pyramid structures with promotion and relegation throughout. The most notable European ice hockey league with a salary cap is the Kontinental Hockey League (which uses the franchising model), and that league implemented a cap despite currency issues.		The Premiership's salary cap has been in place since the late 1990s.[45] By 2007–08, the cap reached £2.2 million. In the following season, it nearly doubled to £4 million,[45] and remained at that amount through the 2011–12 season.[46] A provision applicable only in seasons that run up against the quadrennial Rugby World Cup, such as 2015–16, gives teams a credit for each player in the squad participating in the competition, helping them to manage their reduced squads in the season's early weeks. This credit was £30,000 in the 2011–12 season,[46] and rose to £35,000 for 2015–16.[47] In addition, each club has a separate salary cap for its academy players (£200,000 prior to 2015–16, reduced to £100,000 thereafter, but with home-grown players no longer counting under this cap),[47][48] and is allowed to provide an unlimited educational fund to enable its players to pursue university or vocational training.[47] Finally, each club has a separate cap of £400,000 for use in signing replacements for players lost to long-term injuries (12 weeks or more).[47]		Through 2011–12, the cap remained at £4 million. However, academy credits were introduced that season. Teams received a £30,000 credit for each home-grown player in their senior squads who was under age 24 at the start of the season and earned more than £30,000,[48] with a maximum of eight such credits. This increased the effective cap to a maximum of £4.24 million (not counting World Cup roster credits).[46]		Two substantial changes took effect for 2012–13. First, the cap increased to £4.26 million before academy credits and up to £4.5 million with credits. The most significant change was that each team could now sign one player whose salary did not count against the cap, similar to the Designated Player Rule in MLS.[46] The player so designated, which the Premiership calls an "excluded player", had to meet one of the following three criteria:[48]		For the 2014–15 season, the cap increased to £4.76 million before academy credits and up to £5 million with credits. Other features of the cap remained unchanged.[49]		Several significant changes were introduced for the 2015–16 season:[47][50]		The cap has since increased to a base of £6.5 million with maximum academy credits of £600,000 for the 2016–17 season, and will further increase to a base of £7 million with maximum academy credits of £800,000 for 2017–18.[47]		In December 2009, Ligue Nationale de Rugby (LNR), operator of the Top 14, announced it would impose a cap of €8 million, effective with the 2010–11 season.[51] Previously, the only restrictions on team salaries were that wage bills were limited to 50% of turnover[52] and that 10% of the salary budget had to be held in reserve.[51] Along with the announcement of the cap, LNR also declared that the reserve requirement would be raised to 20%,[51] with the previous limitation of 50% of turnover remaining in effect.[52]		The new cap was slightly higher than the highest official wage bill in the 2009–10 season. Also, due to the complex nature of French club administration, clubs were seen as likely to find creative ways to skirt the cap.[52] This was publicly admitted in 2014 by Mourad Boudjellal, owner of current Top 14 power Toulon. Boudjellal found a loophole that allowed him to set up a separate company to supplement the salary of star Jonny Wilkinson by a six-figure amount while staying under the cap.[53]		The Top 14 salary cap was set at €9.5 million for 2012–13.[54] For 2013–14, the cap was increased to €10 million, and in addition youth players are excluded from the cap unless their salaries are more than €50,000. The €10 million total cap remained in place for three seasons (through 2015–16); the agreement allowed for the threshold for exclusion of youth players to be adjusted before any of those seasons,[55] but no such adjustment was made. The €10 million cap was later extended through the 2018–19 season.[56] Additionally, each club that has a member of the France national team on its roster (more specifically, one of the 30 players named by the French Rugby Federation to the so-called "elite squad") is allowed to exceed the cap by a set amount per national team member. This amount was fixed at €100,000 through the 2015–16 season,[57] and increased to €200,000 starting in 2016–17.[56]		The cap rules were further tweaked for the 2015–16 season. Player bonus payments that amount to more than 10% of a player's salary are now counted against the cap.[58]		On 20 December 2011, the four Welsh regional sides that participate in the Pro12 competition announced that they would impose a salary cap of £3.5 million, effective with the 2012–13 season. The cap covers only the registered squad for European competitions—at the time of announcement, the Heineken Cup and European Challenge Cup, and from 2014–15 the European Rugby Champions Cup and European Rugby Challenge Cup. It does not cover players in the regions' academies.[59]		This cap was unilaterally instituted only on the Welsh teams. The Pro12 league is uncapped, and none of the other three countries involved in the Pro12 (Ireland, Italy, and Scotland) are known to have formally instituted such a system.		When the Russian Superleague was dissolved to make way to the modern-day KHL, the Kontinental Hockey League Players' Trade Union (KHLPTU) agreed to the implementation of a salary cap. When first implemented there was a salary cap, as well as a salary floor. For the 2009-10 KHL season, the salary cap was 620 million rubles ($US18.3 million) and the salary floor was 200 million rubles ($US5.9 million).		The KHL's cap operates despite the KHL's multinational nature, with teams in Belarus, China, Finland, Kazakhstan, Latvia, and Slovakia, in addition to its primary base of Russia. The six non-Russian countries use four different currencies (three countries use the euro), with most floating against the ruble.		From 2011–12, each team can sign up to two "designated players" whose salaries are not counted against the cap. Up until 2011, the KHL salary cap was a soft cap, with a luxury tax amounting to 30% of the payroll that is over the cap paid to the special stabilization account, which helps KHL teams facing financial hardship. From the 2012-13 KHL season onward, the KHL uses a hard cap, set at 1.25 billion rubles ($US36.5 million).		Several European association football leagues have considered introducing salary caps in the early 21st century. In 2002, the BBC reported[60] that the G14 group of 18 leading European football teams would cap their payrolls at 70% of team's income, starting from the 2005/2006 season, however this did not occur. Serie A, the leading Italian football league and The Football League in England have also considered salary caps.		These measures would be implemented to ensure clubs spend responsibly rather than as a tool to create parity. Top executives in European football have acknowledged that a number of challenges not present in North America would confront anyone who tried to implement an effective cap across European football or even across a single league with a view to creating competitive balance:		The Australian Football League has implemented a salary cap on its clubs since 1987, when Brisbane and West Coast were admitted, as part of its equalization policy designed to neutralize the ability of the richest and most successful clubs, Carlton, Collingwood and Essendon, to perennially dominate the competition.		The cap was set at A$1.25 million for 1987–1989 as per VFL agreement, with the salary floor set at 90% of the cap or $1.125 million; the salary floor was increased to 92.5% of the cap in 2001, and to 95% of the cap for 2013 onwards due to increased revenues. The salary cap, known officially as Total Player Payments, is A$10,070,000 for the 2015 season with a salary floor of $9,566,500 except for Greater Western Sydney, whose salary cap is $10,500,000 with a floor of $9,996,500.		Both the salary cap and salary floor has increased substantially since the competition was re-branded as the AFL in 1990 to assist in stemming the dominance of other high membership clubs, such as Adelaide, Hawthorn and the West Coast Eagles.		Certain payments are excluded from the cap, and concessions are available for some players, in particular "veteran" players (those who have completed 10 seasons with their current club[61]) and "nominated" rookie list players, who are discounted by 30% or 50% for purposes of the cap, depending on the number of these players at each club.		The AFL Players Association negotiates for players with the AFL on the topic of average salary.		The breaches of the salary cap and salary floor regulations are exceeding the TPP, falling below the salary floor, not informing the AFL of payments, late or incorrect lodgement or loss of documents relating to player financial and contract details, or engaging in draft tampering. Trading cash for players and playing coaches, formerly common practices, are also prohibited to prevent wealthier clubs from circumventing the restrictions of the salary cap and salary floor.		Penalties for players, club officials or agents include fines of up to one and one half times the amount involved and/or suspension. Penalties for clubs include fines of up to triple the amount involved, forfeiture of draft picks and/or deduction of premiership points. As of 2015, no club has been penalised for breaches of the salary floor regulations, and no punishment has included the deduction of premiership points.		The VFL/AFL's salary cap has been quite successful in terms of parity: since the cap was introduced in 1987, each of the 17 teams [62] have reached the top four and played in a Preliminary Final, 15 teams have played in a Grand Final, and 12 teams have won the premiership.		Another major statistic in regards to the success of the VFL/AFL's cap is that three richest and most successful clubs, Carlton, Collingwood and Essendon, who won 42 of the premierships between 1897 and 1987, [63] have only won five of the premierships since.[64]		The AFL salary cap is occasionally controversial, as it is a soft salary cap and can sometimes be slightly different for each club. Clubs in poor financial circumstances have not always used their full cap, in some circumstances not even reaching the salary floor. The cap is only for the Total Player Payments of each club and not the club's football department. This has caused concern in recent years; for instance, three of the four top-spending clubs in played in the Preliminary Finals in 2012, and the last team to win the premiership outside the top eight spending teams was North Melbourne in 1999. There have been calls for a separate cap for the football department, or to reform the salary cap to include football department spending, but this has been opposed by the wealthier clubs. The AFL has also used the cap to pursue its policy of supporting clubs in non-traditional markets such as Sydney and Brisbane.		Apart from the AFL, several regional leagues also have salary caps which although widening between them and the AFL and overall less than national competitions, are substantial enough to dictate the movement of semi-professional and professional players between states and the overall playing quality and spectator attendance of the state leagues.		The National Rugby League has a salary cap of A$6.4 million in 2014, with a salary floor of A$5.92 million (92.5% of the cap). The salary cap keeps average annual player salaries at around A$256,000.		The National Rugby League adopted a hard salary cap model in its first season in 1998. The NRL's stated purposes for having a salary cap are "to assist in spreading the playing talent" and "ensure that clubs are not put into positions where they are forced to spend more money than they can afford in terms of player payments, just to be competitive." [65] Before the 2012 season, the NRL's then Chief executive David Gallop said "The cap's there to make sure that pure purchasing power cannot dominate the sport... It means we can genuinely say that all 16 teams ... have a chance. For the fan every week, every game is a contest. That's at the core of why rugby league is so successful."[66]		The breaches of the salary cap and salary floor regulations outlined by the NRL are exceeding the salary cap, falling below the salary floor, not informing the NRL of payments, late or incorrect lodgement or loss of documents relating to player financial and contract details or engaging in contract tampering. Trading cash for players is also prohibited to prevent wealthier clubs from evading the salary cap and salary floor regulations.		Penalties for players, club officials and agents include fines of the lesser of 10% of the amount involved or $100,000 and/or suspension. Penalties for clubs include fines of the lesser of half the amount involved or $500,000 ($2,500 for each document that is late or incorrectly lodged or lost) and/or deduction of premiership points.		The NRL is one of the few major leagues to implement a salary cap in a sport that has competing leagues in other countries where there is either no salary cap or a much higher cap per club. As a result, there has developed a tradition of players from Australia moving to Europe where salaries for the elite, and even for average players, were considerably higher. The NRL chooses to continue with the cap, believing that any reduction in quality of the sporting product due to the loss of these players is less than allowing richer clubs to dominate. In practice, the goal of parity has been quite successful, with twelve different clubs winning the premiership in the 20 seasons between 1998 and 2016.		The five Australian teams playing in rugby union's Super Rugby competition are subjected to a $5 million salary cap for a squad of 30 full-time players per Australian team.[67][68][69] The Australian Rugby Union decided in 2011 to introduce the salary cap because of financial pressures.[70] Originally starting in 2012 as a cap of A$4.1 million, it was later was raised to $4.5 million for the 2013 and 2014 seasons to take pressure off the teams' ability to recruit and retain players.[71] The salary cap is a key component of the negotiation between the ARU and the Rugby Union Players Association over the collective bargaining agreement.[72] The fact that the Australian teams in Super Rugby face a salary cap has been attributed as a factor that makes it more difficult for Australian teams to win the title.[73]		The cap regulations have some small concessions:[74]		The 14 teams participating in New Zealand's top domestic competition, now known as the Mitre 10 Cup, faced a salary cap in 2013 that was the lesser of $NZ 1.35 million or 36% of the union's commercial revenue.[75] Maximum player salaries are $55,000, and minimum salaries are $18,000.[76] In August 2013, it was announced that the cap would be further reduced, with the team cap for the 2015 season set to $1.025 million.[77]		New Zealand first implemented the salary cap in the 2006 season. The purpose of the salary cap was to ensure an even spread of players to produce competitive matches and higher television audiences for the new, fully professional competition.[78]		The salary cap had been as high as $2 million in 2008. However, the competition had generated losses of approximately $9.6m in 2007, and salary payments had increased by 75% in the previous four years.[79] Some teams were reported to be in dire financial position, with four teams having payrolls of $1.75 million or more.[80] The salary cap was cut in 2008, converting what was then known as the ITM Cup into a semi-professional competition, with players not under national team or Super Rugby contracts needing to find other part-time jobs.		The A-League national association football (soccer) competition has set a salary cap of A$2.55 million (excluding Marquee, guest and replacement players) for the 2014/2015 season, with a salary floor of $2.294 million. Each team can sign one "marquee player" and one "guest player", whose salaries are excluded from the team's salary cap. The A-League has also introduced a "junior marquee" for eligible under 23 year old players with the aim of keeping young talented players in Australia (or New Zealand for the Wellington Phoenix) for a longer period,[81] similar to the Designated Player Rule in Major League Soccer in North America.		The breaches of the salary cap and salary floor regulations outlined by the A-League are exceeding the salary cap, falling below the salary floor, not informing the A-League of payments, late or incorrect lodgement or loss of documents relating to player financial and contract details or engaging in contract tampering. Penalties for players, club officials or agents include fines of up to one and one half times the amount involved and/or suspension. Penalties for clubs include fines of up to triple the amount involved ($7,500 for each document that is late or incorrectly lodged or lost) and/or deduction of competition points. In the 2006–07 season, Sydney FC were fined $174,000 and deducted three competition points after it was found that they had exceeded the salary cap by $110,000 and failed to declare third-party payments during the 2005–06 season in which they were premiers.		The National Basketball League has a salary cap of A$1.1 million for each of its eight teams as of the 2016–17 season. In addition, since 2003–04, the NBL has used a "points cap" to encourage spread of talent: players are assigned points on a 1–10 basis each season "based on their performance in the NBL or based on the league they have participated in for the season just concluded", and each team's player roster (of between 10 and 12 players) must fall within a "Total Team Points" limit.[82]		On May 9, 2014, in order to help attract high-calibre imports or offer financial incentive for local stars considering overseas opportunities, the NBL introduced a marquee player rule. It originally allowed a team to nominate one player whose salary was paid outside the cap, with a 25% Marquee Player levy applied to any payment made above the salary cap.[83][84] The levy still applies to non-local marquee players (i.e., players who are neither Australians nor New Zealanders).		Effective with the 2016–17 season, two significant changes were made to the cap scheme. First, the cap was changed from a hard cap of A$1 million to a soft cap of $1.1 million. Teams exceeding the cap are required to pay "salary equalisation" (effectively a luxury tax) equal to the amount above the cap.[85] Second, the Marquee Player rule was modified with regard to local players; the cap charge for an Australian or New Zealander who fills a team's marquee player slot is now $150,000, regardless of his actual salary.[85]		In netball's ANZ Championship, each of the 10 franchises are each restricted to a NZ$380,000 salary cap (as of 2013) from which player salaries are paid. Salary amounts vary among players, but each player receives a retainer of at least NZ$12,000 per season; high-profile players are expected to earn up to NZ$50,000.[86][87]		Salary caps and currency conversions accurate as of August 2016[update].		Notes:		Salary caps and currency conversions accurate as of January 2015.		
An internship is a job training for white collar and professional careers.[1][2]		Internships for professional careers are similar in some ways but not as rigorous as apprenticeships for professions, trade and vocational jobs,[3] but the lack of standardisation and oversight leaves the term open to broad interpretation.[4][5] Interns may be college or university students, high school students, or post-graduate adults. These positions may be paid or unpaid and are usually temporary.		Generally, an internship consists of an exchange of services for experience between the student and an organization. Students can also use an internship to determine if they have an interest in a particular career, to create a network of contacts, to acquire a recommendation letter to add to their curriculum vitae, or to gain school credit. Some interns find permanent, paid employment with the organizations for which they worked upon completion of the internship. This can be a significant benefit to the employer as experienced interns often need little or no training when they begin regular employment. It also helps an employer in gauging a student's aptitude, since grade inflation has undermined the reliability of academic grades.[6] Unlike a trainee program, employment at the completion of an internship is not guaranteed.						Internships exist in a wide variety of industries and settings. An internship may be paid, unpaid, or partially paid (in the form of a stipend). Stipends are typically a fixed amount of money that is paid out on a regular basis. Usually, interns that are paid through stipends are paid on a monthly basis. Paid internships are common in professional fields including medicine, architecture, science, engineering, law, business (especially accounting and finance), technology, and advertising. Non-profit charities and think tanks often have unpaid, volunteer positions. Internships may be part-time or full-time. A typical internship lasts between one and four months,[7] but can be shorter or longer, depending on the organisation involved. The act of job shadowing may also constitute interning.		The two primary types of internships that exist in the United States are:		European internships are popular among non-Europeans in order to gain international exposure on one's résumé and for foreign language improvement, although they may be unpaid.		Another type of internship growing in popularity is the virtual internship, in which the intern works remotely, and is not physically present at the job location. It provides the capacity to gain job experience without the conventional requirement of being physically present in an office. The internship is conducted via virtual means, such as phone, email, and web communication. Virtual interns generally have the opportunity to work at their own pace.[8]		Some companies now find and place students in mostly unpaid internships for a fee.[9] These companies charge to assist with a search, promising to refund their fees if no internship is found.[10] These programs vary, but they claim to provide internship placements at reputable companies, provide controlled housing in a new city, mentorship and support throughout the summer, networking, weekend activities in some programs, and sometimes academic credit.[11]		Another form of paying for internships is through charity auctions. A company with an internship will select a charity who will obtain an internship position funded by the auction. In some cases, companies have created internships simply to help a charity.[9]		Some claim that fee-based programs and charity auctions restrict internship opportunities to students in wealthier families.[11] These companies respond that "the average student comes from the middle class," and that their parents "'dig deep' to pay for it". Some companies specifically fund scholarships and grants for low-income applicants.		Critics of internships decry the practice of requiring certain college credits to be obtained only through unpaid internships. Depending on the cost of the school, this is often seen as an unethical practice, as it requires students to exchange paid-for and often limited tuition credits in order to work an uncompensated job. Even if the school does not require credit for an internship, companies offering the internship often pressure colleges to give college credit so interns do not complain that they receive nothing for their efforts.		Paying for academic credits can also be seen as a way to ensure that students complete the duration of the internship, as they can be held accountable by their academic institution. For example, a student may be awarded academic credit only after their university receives a positive review from the intern's supervisor at the sponsoring organization. Some employers feel that this enables them to view the intern as having some "skin in the game".		Internship laws and practices vary widely from country to country, and region to region.		The potential that international markets have, particularly Asia, is often ignored. Asia boasts some of the world’s fastest growing economies, incredible innovation in a wide array of industries (particularly tech), and rapidly expanding companies that are in search for spots to fill. It currently makes up 60% of the world’s population, and Mandarin Chinese and Hindi/Urdu are two of the three most popular languages spoken in the world. With an internship in Asia, students are able to immerse themselves in the working/business culture there, which will not only be a catalyst for personal growth but will be a very valuable experience for future employment. Having a professional network in Asia will be useful anywhere student go, especially now that Asian countries are making huge economic impacts globally. [12]		Internships positions in China generally follow the academic calendar, starting in late summer and finishing in late spring. There is a growing trend for overseas students (particularly those from Australia, the UK, and the US) to compete for increasing internship opportunities available in China.[13][14][15] It is common for overseas students to use the services of (for-profit) China-based internship providers to secure their internship.[16][17][18]		Internship opportunities in India are career specific. College students often choose internships based on their branch of study. Students often perceive it as a way to develop their capabilities by practically applying their degree while learning in a professional work environment.		Most students apply for internships during their summer and winter breaks. In some universities, internships during the college breaks are compulsory and a part of the curriculum. It is common that previous interns become employees to the organization once they have acquired the necessary skills and experience. Moreover, many engineering students also term their training period in certain industrial organization as an internship.		Most universities and colleges in Malaysia require students to complete a minimum of one semester-long internship during their final year of studies before they are eligible to graduate from a diploma or bachelor program. This applies to both government and private universities and colleges. It also applies to most programs offered. Students can either find a suitable internship at a company in Malaysia or they can complete their internship overseas.Once a student completes their internship, it is estimated that they have around 20% more chances to land a job compared to students without prior internship.		Internships in Australia are often referred to as "work experience" when undertaken by high school students and "industry experience" when undertaken by university students. Some degree programs such as engineering require a minimum amount of industry experience (usually 12 weeks) to attain professional accreditation with industry bodies, such as Engineers Australia.		Unpaid internships are legal and allowed under the Fair Work Act 2009. There are a number of criteria used to determine if the engagement forms a legitimate internship, including:		In New Zealand, there are a number of colleges where students can undertake an internship while studying. Students studying adventure tourism or hospitality management must complete an internship in order to complete their course studies. Most of these internships are paid by the employer.		Students studying for careers in educational psychology in New Zealand must complete a one-year internship – the internship program at Massey University, for example, is a one-year post masters, post graduate diploma in educational psychology and the majority of internships are carried out with placement in the Ministry of Education. Some funding is available through the Ministry of Education Study Awards program to support interns in their applied practice. The funds are contestable each year. In 2013, there were 10 study awards each worth NZ$15,000.		Students studying for a bachelor of communication studies, majoring in journalism at Auckland University of Technology are required to undergo a two-week compulsory internship in their third year of study. This internship can be at a national or local newspaper, a magazine or a radio station. They are required to research and write stories for publication and broadcast and get experience in a fast-paced news environment.		Work without pay is inappropriate in Denmark. One way it can be done is as part of a work-trial where a person is tested by the authorities in conjunction with putting the individual back into the workplace.[20]		It is also common within most Danish universities to place students in "free work" jobs. The company is then compensated and the intern receives welfare during this period. This normally lasts about three months. The Danish trade unions monitor this type of work very closely so the hiring of an intern does not result in the loss of a paid job.[21][22]		In 2008, a new system established by the department of education may lower the motivation of students who take time to work for charities.[23]		Students and citizens of the EEA/EU area can freely move and reside in Denmark under EU rules. If their stay exceeds three months then an application for registration with the Regional State Administration has to be filed.[24] If the student comes from outside the European Union than the following rules to apply for residence and work permit for an internship apply:		Furthermore, there are special rules for agricultural, healthcare and architectural internships.[25]		The European Commission operates a sizeable traineeship programme.		At French universities, internships, known as "stages", are common. The duration of French internships usually varies from two to six months (which is the maximal legal length). French labor law requires that all internships lasting two months or longer include a minimum stipend (€3.60 per hour in 2016).[26]		Internships in France are also popular for international students. The primary reason international students intern in France is to learn to speak French fluently. French companies greatly appreciate employees who speak multiple languages and thus international opportunities are available.[27]		As in most other countries, most students take their internship (German: "praktikum") between the fourth or fifth semester of their degree at a university of applied sciences. In some fields of study it is common to write the final thesis in a company as part of an internship. Some degrees do not require practical training in order to graduate.		Another type of internship has emerged in recent years is the post graduation internship. The purpose of a post graduation internship is to equip the student with knowledge and tools to be successful in their future position. These post graduation internships should last between six and ten months.[28]		Since the Italian university system entered the Bologna process, an internship experience (commonly referred to by the French term stage) has been made compulsory for almost all those who are studying for a bachelors or a master's degree (especially in technical, economic or scientific faculties). The goal of this process is to reduce the gap between companies' demands and the often very theoretical learning offered by Italian universities. However, since the internship is usually completed at university as well and since only few companies who employ student interns rarely offer proper training, these internships are generally not considered real work experience. Almost all students therefore have to do a second or a third internship after they have completed their studies, hoping to receive appropriate professional training and possibly getting employed afterward in the same company or in another company in a close or related business.		Italian internships can last up to six months and can be extended for another six months for a potential year long internship. Internships in Italy can be either paid or unpaid. Student internships, especially the ones not involved with the development of a thesis, are usually not paid.		Almost all graduate internships are paid, but the remuneration is usually only around 600 euros gross per month (that is about ¼ of the gross monthly remuneration of an hired young graduate employee) and without benefits other than lunch and a few paid days for sickness or vacation[citation needed]. This poses a problem for graduates, considering as well that some companies use graduate interns just to save money, making them work for 6 to 12 months without giving them a decent remuneration, without offering them proper training or formation, and without hiring them after the internship even if they were shown to be productive, fast-learning and trustworthy.		In order to get an internship, graduates have to go to interviews, which might be held in cities far from the ones in which graduates have been studying.		In the Netherlands, it is also common to perform internships during college[citation needed] which, like in Belgium and France, is called a stage. Most student internships last between 3 and 10 months[citation needed]. Companies are not obligated to pay the student, so sometimes small companies do not pay anything. Unpaid internships are also the de facto standard in education. The normal internship compensation rate in the Netherlands is around €300 per month[citation needed], depending on education level and company generosity. Netherlands no.5		At Spanish universities, internship during the education period are uncommon. "Real" work experience for students begins only when they are done with their study.		Some Spanish companies are getting more used to having student internships—mostly these are international students from other European countries. Often, students want to learn Spanish. Placement organizations may be needed as Spanish companies are harder to contact directly. The normal stage compensation rate in Spain is around €500 per month. Retribution is regulated in many universities starting from €6 per hour. Given these rates, Spanish employers who do hire interns often may be taking advantage of unpaid interships in order to get free labor.[29]		In the United Kingdom, work experience is offered as part of the national curriculum to secondary school students in years 10 and 11 (14 to 16 years of age.) Generally, these placements are unpaid.		During their degree programme, students may apply for internships during the summer holidays. University staff give students access, and students apply direct to employers. Some students opt to apply for year-long placements, often referred to as "sandwich placements", between the penultimate and final year of their degree. This is done as part of a degree programme. Some universities and employers hold fairs and exhibitions to encourage students to consider the option and to enable students to meet potential employers.[30] In the modern labour market, graduates with internship work experience are deemed more desirable to employers.		The purpose of these placements is varied. Some university students see it as a way to develop their employability by utilising the academic elements of their degree in a practical setting. International students may also seek to get understanding about how work is conducted in the English-speaking world and to experience cultural diversity. Organisations such as the Trades Union Congress and Intern Aware have been lobbying for a change in British internships to make interns aware of their employment law rights, especially in relation to whether they are entitled to minimum wage and paid holidays.[31][32]		The legal status of volunteers and interns is not always clear. Various factors determine whether an individual is classed as an employee, a worker, a volunteer or self-employed and these often require further examination.[33]		In Canada, high school, college, and university student placements are typically referred to as "co-ops" (co-operative education) programs. University co-op programs are often highly competitive; students must apply to and compete for admission, as enrollment is limited. Partnering employers will post placement opportunities through the university. These positions typically span a four-month term taking place either during summer break or during the school year. Some summer internships require proof that the student is returning to school in the fall.		While some internships are unpaid (particularly in media, advertising, PR, and communications), there are a few Canadian organizations that do offer paid internships. Not all internships are entry-level positions; organizations may also offer internships for mid-level professionals. For example, in the province of Ontario, paid internships are available for immigrants who have extensive experience in other countries but lack relevant Canadian experience.		The nature and scope of unpaid internships in Canada is difficult to estimate. This is in part because there are no written regulations defining internships directly. Minimum wage for labour is covered by employment standards legislation and is governed at the provincial level. Ontario provides for a six-point test to be applied to determine if an employee-employer relationship does not exist, where all of the conditions must be met:[34]		Many internships in the United States are career specific. Students often choose internships based on their major at the university or college level. It is not uncommon for former interns to acquire full-time employment at an organization once they have enough necessary experience. The challenging job market has made it essential for college students to gain real world experience prior to graduation. Yet, only 37% of unpaid interns have job offers awaiting them at graduation compared to 60% of paid interns and 36% of students with no internship experience.[35] In the US, company internships are at the center of NIGMS funded biotechnology training programs[36] for science PhD students. The Office of Personnel Management of the US federal government operation operates a robust internship program for college students and recent graduates.[37]		Not all internships are paid. Many internships that are unpaid involve receiving college credit, especially if an internship is correlated with a specific class. The U.S. Department of Labor's Wage and Hour Division allows an employer not to pay a trainee if all of the following are true:[38]		An exception is allowed for individuals who volunteer their time, freely and without anticipation of compensation for religious, charitable, civic, or humanitarian purposes to non-profit organizations.[39] An exception is also allowed for work performed for a state or local government agency.[39]		Some states have their own laws on the subject.[40] Laws in the state of California, for example, require an employer to pay its interns working in California or offer the intern academic or college credit for their time and labor.[40] As another example, in the state of New York, for For-Profit companies, the activities of trainees or students cannot provide an immediate advantage to the employer. On the other hand, trainees or students do not receive employee benefits like health and dental insurance, pension or retirement credit, discounted or free goods and services from the employer, etc.[41]		Internships in Brazil are known as estágios and internship workers are known as estagiários. The term "estágio" applies to both company internships and academic internships.		Academic internships are usually arranged for college credit (research hours). Brazilian research projects do not usually allocate funds for paid interns, and as a result, academic internships are often unpaid.		Company and industry internships are regulated by the Lei do Estágio ("internship law"), which requires that companies provide a monthly salary and personal injury service. The Lei do Estágio further stipulates a limit of 30 worked hours per week.[44] Estagiários also have the right to 30 days of paid holidays for each year worked. In practice, legal interns in Big Law and others in professional services routinely work past that limit and receive no vacation time.		An internship in Nigeria is called a "Student Industrial Work Experience Scheme" (SIWES) or "Industrial Training". Students In universities usually spend six months in their third year for four-year courses or fourth year for five-year courses. Students in polytechnics for a two-year National Diploma undergo four months of SIWES which is in most case a 4 credit (unit) course, at the end of National Diploma program, they will go for another 1 year Industrial Training which is key to gaining admission into Higher National Diploma to complete their studies. Students usually use their semester break period for industrial training. While an SIWES is a type of internship in Nigeria, a more official form of internship is seen in medical and allied health professions. Professionals in radiography, medical laboratory sciences, pharmacy, physiotherapy, and medicine undergo a one-year government-paid internship after graduation from school. These professionals are actually called "interns" in Nigeria.		
Human factors and ergonomics (commonly referred to as HF&E), also known as comfort design, functional design, and systems,[1] is the practice of designing products, systems, or processes to take proper account of the interaction between them and the people who use them.		The field has seen some contributions from numerous disciplines, such as psychology, engineering, biomechanics, industrial design, physiology, and anthropometry. In essence, it is the study of designing equipment, devices and processes that fit the human body and its cognitive abilities. The two terms "human factors" and "ergonomics" are essentially synonymous.[2][3][4]		The International Ergonomics Association defines ergonomics or human factors as follows:[5]		Ergonomics (or human factors) is the scientific discipline concerned with the understanding of interactions among humans and other elements of a system, and the profession that applies theory, principles, data and methods to design in order to optimize human well-being and overall system performance.		HF&E is employed to fulfill the goals of occupational health and safety and productivity. It is relevant in the design of such things as safe furniture and easy-to-use interfaces to machines and equipment.		Proper ergonomic design is necessary to prevent repetitive strain injuries and other musculoskeletal disorders, which can develop over time and can lead to long-term disability.		Human factors and ergonomics is concerned with the "fit" between the user, equipment and their environments. It takes account of the user's capabilities and limitations in seeking to ensure that tasks, functions, information and the environment suit each user.		To assess the fit between a person and the used technology, human factors specialists or ergonomists consider the job (activity) being done and the demands on the user; the equipment used (its size, shape, and how appropriate it is for the task), and the information used (how it is presented, accessed, and changed). Ergonomics draws on many disciplines in its study of humans and their environments, including anthropometry, biomechanics, mechanical engineering, industrial engineering, industrial design, information design, kinesiology, physiology, cognitive psychology, industrial and organizational psychology, and space psychology.						The term ergonomics (from the Greek ἔργον, meaning "work", and νόμος, meaning "natural law") first entered the modern lexicon when Polish scientist Wojciech Jastrzębowski used the word in his 1857 article Rys ergonomji czyli nauki o pracy, opartej na prawdach poczerpniętych z Nauki Przyrody (The Outline of Ergonomics; i.e. Science of Work, Based on the Truths Taken from the Natural Science).[6] The introduction of the term to the English lexicon is widely attributed to British psychologist Hywel Murrell, at the 1949 meeting at the UK's Admiralty, which led to the foundation of The Ergonomics Society. He used it to encompass the studies in which he had been engaged during and after World War II.[7]		The expression human factors is a predominantly North American[8] term which has been adopted to emphasise the application of the same methods to non work-related situations. A "human factor" is a physical or cognitive property of an individual or social behavior specific to humans that may influence the functioning of technological systems. The terms "human factors" and "ergonomics" are essentially synonymous.[2]		Ergonomics comprise three main fields of research: Physical, cognitive and organisational ergonomics.		There are many specializations within these broad categories. Specialisations in the field of physical ergonomics may include visual ergonomics. Specialisations within the field of cognitive ergonomics may include usability, human–computer interaction, and user experience engineering.		Some specialisations may cut across these domains: Environmental ergonomics is concerned with human interaction with the environment as characterized by climate, temperature, pressure, vibration, light.[9] The emerging field of human factors in highway safety uses human factor principles to understand the actions and capabilities of road users – car and truck drivers, pedestrians, bicyclists, etc. – and use this knowledge to design roads and streets to reduce traffic collisions. Driver error is listed as a contributing factor in 44% of fatal collisions in the United States, so a topic of particular interest is how road users gather and process information about the road and its environment, and how to assist them to make the appropriate decision.[10]		New terms are being generated all the time. For instance, "user trial engineer" may refer to a human factors professional who specialises in user trials.[citation needed] Although the names change, human factors professionals apply an understanding of human factors to the design of equipment, systems and working methods in order to improve comfort, health, safety, and productivity.		According to the International Ergonomics Association, within the discipline of ergonomics there exist domains of specialization:		Physical ergonomics is concerned with human anatomy, and some of the anthropometric, physiological and bio mechanical characteristics as they relate to physical activity.[5] Physical ergonomic principles have been widely used in the design of both consumer and industrial products. Physical ergonomics is important in the medical field, particularly to those diagnosed with physiological ailments or disorders such as arthritis (both chronic and temporary) or carpal tunnel syndrome. Pressure that is insignificant or imperceptible to those unaffected by these disorders may be very painful, or render a device unusable, for those who are. Many ergonomically designed products are also used or recommended to treat or prevent such disorders, and to treat pressure-related chronic pain.[citation needed]		One of the most prevalent types of work-related injuries is musculoskeletal disorder. Work-related musculoskeletal disorders (WRMDs) result in persistent pain, loss of functional capacity and work disability, but their initial diagnosis is difficult because they are mainly based on complaints of pain and other symptoms.[11] Every year, 1.8 million U.S. workers experience WRMDs and nearly 600,000 of the injuries are serious enough to cause workers to miss work.[12] Certain jobs or work conditions cause a higher rate of worker complaints of undue strain, localized fatigue, discomfort, or pain that does not go away after overnight rest. These types of jobs are often those involving activities such as repetitive and forceful exertions; frequent, heavy, or overhead lifts; awkward work positions; or use of vibrating equipment.[13] The Occupational Safety and Health Administration (OSHA) has found substantial evidence that ergonomics programs can cut workers' compensation costs, increase productivity and decrease employee turnover.[14] Therefore, it is important to gather data to identify jobs or work conditions that are most problematic, using sources such as injury and illness logs, medical records, and job analyses.[13]		Cognitive ergonomics is concerned with mental processes, such as perception, memory, reasoning, and motor response, as they affect interactions among humans and other elements of a system.[5] (Relevant topics include mental workload, decision-making, skilled performance, human reliability, work stress and training as these may relate to human-system and Human-Computer Interaction design.)		Organizational ergonomics is concerned with the optimization of socio-technical systems, including their organizational structures, policies, and processes.[5] (Relevant topics include communication, crew resource management, work design, work systems, design of working times, teamwork, participatory design, community ergonomics, cooperative work, new work programs, virtual organizations, telework, and quality management.)		The foundations of the science of ergonomics appear to have been laid within the context of the culture of Ancient Greece. A good deal of evidence indicates that Greek civilization in the 5th century BC used ergonomic principles in the design of their tools, jobs, and workplaces. One outstanding example of this can be found in the description Hippocrates gave of how a surgeon's workplace should be designed and how the tools he uses should be arranged.[15] The archaeological record also shows that the early Egyptian dynasties made tools and household equipment that illustrated ergonomic principles.		In the 19th century, Frederick Winslow Taylor pioneered the "scientific management" method, which proposed a way to find the optimum method of carrying out a given task. Taylor found that he could, for example, triple the amount of coal that workers were shoveling by incrementally reducing the size and weight of coal shovels until the fastest shoveling rate was reached.[16] Frank and Lillian Gilbreth expanded Taylor's methods in the early 1900s to develop the "time and motion study". They aimed to improve efficiency by eliminating unnecessary steps and actions. By applying this approach, the Gilbreths reduced the number of motions in bricklaying from 18 to 4.5, allowing bricklayers to increase their productivity from 120 to 350 bricks per hour.[16]		However, this approach was rejected by Russian researchers who focused on the well being of the worker. At the First Conference on Scientific Organization of Labour (1921) Vladimir Bekhterev and Vladimir Nikolayevich Myasishchev criticised Taylorism. Bekhterev argued that "The ultimate ideal of the labour problem is not in it [Taylorism], but is in such organisation of the labour process that would yield a maximum of efficiency coupled with a minimum of health hazards, absence of fatigue and a guarantee of the sound health and all round personal development of the working people."[17] Myasishchev rejected Frederick Taylor's proposal to turn man into a machine. Dull monotonous work was a temporary necessity until a corresponding machine can be developed. He also went on to suggest a new discipline of "ergology" to study work as an integral part of the re-organisation of work. The concept was taken up by Myasishchev's mentor, Bekhterev, in his final report on the conference, merely changing the name to "ergonology"[17]		Prior to World War I, the focus of aviation psychology was on the aviator himself, but the war shifted the focus onto the aircraft, in particular, the design of controls and displays, and the effects of altitude and environmental factors on the pilot. The war saw the emergence of aeromedical research and the need for testing and measurement methods. Studies on driver behaviour started gaining momentum during this period, as Henry Ford started providing millions of Americans with automobiles. Another major development during this period was the performance of aeromedical research. By the end of World War I, two aeronautical labs were established, one at Brooks Air Force Base, Texas and the other at Wright-Patterson Air Force Base outside of Dayton, Ohio. Many tests were conducted to determine which characteristic differentiated the successful pilots from the unsuccessful ones. During the early 1930s, Edwin Link developed the first flight simulator. The trend continued and more sophisticated simulators and test equipment were developed. Another significant development was in the civilian sector, where the effects of illumination on worker productivity were examined. This led to the identification of the Hawthorne Effect, which suggested that motivational factors could significantly influence human performance.[16]		World War II marked the development of new and complex machines and weaponry, and these made new demands on operators' cognition. It was no longer possible to adopt the Tayloristic principle of matching individuals to preexisting jobs. Now the design of equipment had to take into account human limitations and take advantage of human capabilities. The decision-making, attention, situational awareness and hand-eye coordination of the machine's operator became key in the success or failure of a task. There was substantial research conducted to determine the human capabilities and limitations that had to be accomplished. A lot of this research took off where the aeromedical research between the wars had left off. An example of this is the study done by Fitts and Jones (1947), who studied the most effective configuration of control knobs to be used in aircraft cockpits.		Much of this research transcended into other equipment with the aim of making the controls and displays easier for the operators to use. The entry of the terms "human factors" and "ergonomics" into the modern lexicon date from this period. It was observed that fully functional aircraft flown by the best-trained pilots, still crashed. In 1943 Alphonse Chapanis, a lieutenant in the U.S. Army, showed that this so-called "pilot error" could be greatly reduced when more logical and differentiable controls replaced confusing designs in airplane cockpits. After the war, the Army Air Force published 19 volumes summarizing what had been established from research during the war.[16]		In the decades since World War II, HF&E has continued to flourish and diversify. Work by Elias Porter and others within the RAND Corporation after WWII extended the conception of HF&E. "As the thinking progressed, a new concept developed—that it was possible to view an organization such as an air-defense, man-machine system as a single organism and that it was possible to study the behavior of such an organism. It was the climate for a breakthrough."[18] In the initial 20 years after the World War II, most activities were done by the "founding fathers": Alphonse Chapanis, Paul Fitts, and Small.[citation needed]		The beginning of the Cold War it was very cold and led to a major expansion of Defense supported research laboratories. Also, many labs established during WWII started expanding. Most of the research following the war was military-sponsored. Large sums of money were granted to universities to conduct research. The scope of the research also broadened from small equipments to entire workstations and systems. Concurrently, a lot of opportunities started opening up in the civilian industry. The focus shifted from research to participation through advice to engineers in the design of equipment. After 1965, the period saw a maturation of the discipline. The field has expanded with the development of the computer and computer applications.[16]		The Space Age created new human factors issues such as weightlessness and extreme g-forces. Tolerance of the harsh environment of space and its effects on the mind and body were widely studied[citation needed]		The dawn of the Information Age has resulted in the related field of human–computer interaction (HCI). Likewise, the growing demand for and competition among consumer goods and electronics has resulted in more companies and industries including human factors in their product design. Using advanced technologies in human kinetics, body-mapping, movement patterns and heat zones, companies are able to manufacture purpose-specific garments, including full body suits, jerseys, shorts, shoes, and even underwear.		Formed in 1946 in the UK, the oldest professional body for human factors specialists and ergonomists is The Chartered Institute of Ergonomics and Human Factors, formally known as the Institute of Ergonomics and Human Factors and before that, The Ergonomics Society.		The Human Factors and Ergonomics Society (HFES) was founded in 1957. The Society's mission is to promote the discovery and exchange of knowledge concerning the characteristics of human beings that are applicable to the design of systems and devices of all kinds.		The International Ergonomics Association (IEA) is a federation of ergonomics and human factors societies from around the world. The mission of the IEA is to elaborate and advance ergonomics science and practice, and to improve the quality of life by expanding its scope of application and contribution to society. As of September 2008, the International Ergonomics Association has 46 federated societies and 2 affiliated societies.		The Institute of Occupational Medicine (IOM) was founded by the coal industry in 1969. From the outset the IOM employed an ergonomics staff to apply ergonomics principles to the design of mining machinery and environments. To this day, the IOM continues ergonomics activities, especially in the fields of musculoskeletal disorders; heat stress and the ergonomics of personal protective equipment (PPE). Like many in occupational ergonomics, the demands and requirements of an ageing UK workforce are a growing concern and interest to IOM ergonomists.		The International Society of Automotive Engineers (SAE) is a professional organization for mobility engineering professionals in the aerospace, automotive, and commercial vehicle industries. The Society is a standards development organization for the engineering of powered vehicles of all kinds, including cars, trucks, boats, aircraft, and others. The Society of Automotive Engineers has established a number of standards used in the automotive industry and elsewhere. It encourages the design of vehicles in accordance with established Human Factors principles. It is one of the most influential organizations with respect to Ergonomics work in Automotive design. This society regularly holds conferences which address topics spanning all aspects of Human Factors/Ergonomics.[citation needed]		Human factors practitioners come from a variety of backgrounds, though predominantly they are psychologists (from the various subfields of industrial and organizational psychology, engineering psychology, cognitive psychology, perceptual psychology, applied psychology, and experimental psychology) and physiologists. Designers (industrial, interaction, and graphic), anthropologists, technical communication scholars and computer scientists also contribute. Typically, an ergonomist will have an undergraduate degree in psychology, engineering, design or health sciences, and usually a masters degree or doctoral degree in a related discipline. Though some practitioners enter the field of human factors from other disciplines, both M.S. and PhD degrees in Human Factors Engineering are available from several universities worldwide.		Until recently, methods used to evaluate human factors and ergonomics ranged from simple questionnaires to more complex and expensive usability labs.[19] Some of the more common HF&E methods are listed below:		Problems related to measures of usability include the fact that measures of learning and retention of how to use an interface are rarely employed and some studies treat measures of how users interact with interfaces as synonymous with quality-in-use, despite an unclear relation.[27]		Although field methods can be extremely useful because they are conducted in the users' natural environment, they have some major limitations to consider. The limitations include:		Related subjects		Related fields		Related scientists – living		Related scientists – deceased		Books		Peer-reviewed Journals (numbers between brackets are the ISI impact factor, followed by the date)		
In a workplace setting, probation (or probationary period) is a status given to new employees of a company or business. It is widely termed as the Probation Period of an employee. This status allows a supervisor or other company manager to evaluate closely the progress and skills of the newly hired worker, determine appropriate assignments, and monitor other aspects of the employee such as honesty, reliability, and interactions with co-workers, supervisors or customers.		A probationary period varies widely depending on the business, but can last anywhere from 30 days to several years. In cases of several years, probationary levels may change as time goes on. If the new employee shows promise and does well during the probationary time, they are usually removed from probationary status, and may be given a raise or promotion as well (in addition to other privileges, as defined by the business). Probation is usually defined in a company's employee handbook, which is given to workers when they first begin a job.		The probationary period also allows an employer to terminate an employee who is not doing well at their job or is otherwise deemed not suitable for a particular position or any position. Whether or not this empowers employers to abuse their employees by, without warning, terminating their contract before the probation period has ended is open for debate. To avoid problems arising from the termination of a new employee, many companies are waiving the probationary period entirely, and instead conducting multiple interviews of the candidate, under a variety of conditions – before making the decision to hire.		The placement of an employee on probationary status is usually at the discretion of their manager.		
Personal protective equipment (PPE) refers to protective clothing, helmets, goggles, or other garments or equipment designed to protect the wearer's body from injury or infection. The hazards addressed by protective equipment include physical, electrical, heat, chemicals, biohazards, and airborne particulate matter. Protective equipment may be worn for job-related occupational safety and health purposes, as well as for sports and other recreational activities. "Protective clothing" is applied to traditional categories of clothing, and "protective gear" applies to items such as pads, guards, shields, or masks, and others.		The purpose of personal protective equipment is to reduce employee exposure to hazards when engineering controls and administrative controls are not feasible or effective to reduce these risks to acceptable levels. PPE is needed when there are hazards present. PPE has the serious limitation that it does not eliminate the hazard at source and may result in employees being exposed to the hazard if the equipment fails.[1]		Any item of PPE imposes a barrier between the wearer/user and the working environment. This can create additional strains on the wearer; impair their ability to carry out their work and create significant levels of discomfort. Any of these can discourage wearers from using PPE correctly, therefore placing them at risk of injury, ill-health or, under extreme circumstances, death. Good ergonomic design can help to minimise these barriers and can therefore help to ensure safe and healthy working conditions through the correct use of PPE.		Practices of occupational safety and health can use hazard controls and interventions to mitigate workplace hazards, which pose a threat to the safety and quality of life of workers. The hierarchy of hazard controls provides a policy framework which ranks the types of hazard controls in terms of absolute risk reduction. At the top of the hierarchy are elimination and substitution, which remove the hazard entirely or replace the hazard with a safer alternative. If elimination or substitution measures cannot apply, engineering controls and administrative controls, which seek to design safer mechanisms and coach safer human behavior, are implemented. Personal protective equipment ranks last on the hierarchy of controls, as the workers are regularly exposed to the hazard, with a barrier of protection. The hierarchy of controls is important in acknowledging that, while personal protective equipment has tremendous utility, it is not the desired mechanism of control in terms of worker safety.						Personal protective equipment can be categorized by the area of the body protected, by the types of hazard, and by the type of garment or accessory. A single item, for example boots, may provide multiple forms of protection: a steel toe cap and steel insoles for protection of the feet from crushing or puncture injuries, impervious rubber and lining for protection from water and chemicals, high reflectivity and heat resistance for protection from radiant heat, and high electrical resistivity for protection from electric shock. The protective attributes of each piece of equipment must be compared with the hazards expected to be found in the workplace. More breathable types of personal protective equipment may not lead to more contamination but do result in greater user satisfaction.[2]		Respirators serve to protect the user from breathing in contaminants in the air, thus preserving the health of one's respiratory tract. There are two main types of respirators. One type functions by filtering out chemicals and gases, or airborne particles, from the air breathed by the user.[3] The filtration may be either passive or active (powered). Gas masks and particulate respirators are examples of this type of respirator. A second type protects users by providing clean, respirable air from another source. This type includes airline respirators and self-contained breathing apparatus (SCBA).[3] In work environments, respirators are relied upon when adequate ventilation is not available or other engineering control systems are not feasible or inadequate.[3]		In the United Kingdom, an organization that has extensive expertise in respiratory protective equipment is the Institute of Occupational Medicine. This expertise has been built on a long-standing and varied research programme that has included the setting of workplace protection factors to the assessment of efficacy of masks available through high street retail outlets.[citation needed]		The Health and Safety Executive (HSE), NHS Health Scotland and Healthy Working Lives (HWL) have jointly developed the RPE (Respiratory Protective Equipment) Selector Tool, which is web-based. This interactive tool provides descriptions of different types of respirators and breathing apparatuses, as well as "dos and don'ts" for each type.[4]		In the United States, The National Institute for Occupational Safety and Health (NIOSH) provides recommendations on respirator use, in accordance to NIOSH federal respiratory regulations 42 CFR Part 84.[3] The National Personal Protective Technology Laboratory (NPPTL) of NIOSH is tasked towards actively conducting studies on respirators and providing recommendations.[5]		Occupational skin diseases such as contact dermatitis, skin cancers, and other skin injuries and infections are the second-most common type of occupational disease and can be very costly.[6] Skin hazards, which lead to occupational skin disease, can be classified into four groups. Chemical agents can come into contact with the skin through direct contact with contaminated surfaces, deposition of aerosols, immersion or splashes.[6] Physical agents such as extreme temperatures and ultraviolet or solar radiation can be damaging to the skin over prolonged exposure.[6] Mechanical trauma occurs in the form of friction, pressure, abrasions, lacerations and contusions.[6] Biological agents such as parasites, microorganisms, plants and animals can have varied effects when exposed to the skin.[6]		Any form of PPE that acts as a barrier between the skin and the agent of exposure can be considered skin protection. Because much work is done with the hands, gloves are an essential item in providing skin protection. Some examples of gloves commonly used as PPE include rubber gloves, cut-resistant gloves, chainsaw gloves and heat-resistant gloves. For sports and other recreational activities, many different gloves are used for protection, generally against mechanical trauma.		Other than gloves, any other article of clothing or protection worn for a purpose serve to protect the skin. Lab coats for example, are worn to protect against potential splashes of chemicals. Face shields serve to protect one's face from potential impact hazards, chemical splashes or possible infectious fluid.		Each day, about 2000 US workers have a job-related eye injury that requires medical attention.[7] Eye injuries can happen through a variety of means. Most eye injuries occur when solid particles such as metal slivers, wood chips, sand or cement chips get into the eye.[7] Smaller particles in smokes and larger particles such as broken glass also account for particulate matter-causing eye injuries. Blunt force trauma can occur to the eye when excessive force comes into contact with the eye. Chemical burns, biological agents, and thermal agents, from sources such as welding torches and UV light, also contribute to occupational eye injury.[8]		While the required eye protection varies by occupation, the safety provided can be generalized. Safety glasses provide protection from external debris, and should provide side protection via a wrap-around design or side shields.[8]		Industrial noise is often overlooked as an occupational hazard, as it is not visible to the eye. Overall, about 22 million workers in the United States are exposed to potentially damaging noise levels each year.[9] Occupational hearing loss accounted for 14% of all occupational illnesses in 2007, with about 23,000 cases significant enough to cause permanent hearing impairment.[9] About 82% of occupational hearing loss cases occurred to workers in the manufacturing sector.[9] The Occupational Safety and Health Administration establishes occupational noise exposure standards.[10] NIOSH recommends that worker exposures to noise be reduced to a level equivalent to 85 dBA for eight hours to reduce occupational noise-induced hearing loss.[11]		PPE for hearing protection consists of earplugs and earmuffs. Workers who are regularly exposed to noise levels above the NIOSH recommendation should be furnished hearing protection by the employers, as they are a low-cost intervention.		This form of PPE is all-encompassing and refers to the various suits and uniforms worn to protect the user from harm. Lab coats worn by scientists and ballistic vests worn by law enforcement officials, which are worn on a regular basis, would fall into this category. Entire sets of PPE, worn together in a combined suit, are also in this category.		Below are some examples of ensembles of personal protective equipment, worn together for a specific occupation or task, to provide maximum protection for the user.		Participants in sports often wear protective equipment. Studies performed on the injuries of professional athletes, such as that on NFL players,[12][13] question the effectiveness of existing personal protective equipment.		The definition of what constitutes as personal protective equipment varies by country. In the United States, the laws regarding PPE also vary by state. In 2011, workplace safety complaints were brought against Hustler and other adult film production companies by the AIDS Healthcare Foundation, leading to several citations brought by Cal/OSHA.[14] The failure to use condoms by adult film stars was a violation of Cal/OSHA's Blood borne Pathogens Program, Personal Protective Equipment.[14] This example shows that personal protective equipment can cover a variety of occupations in the United States, and has a wide-ranging definition.		At the European Union level, personal protective equipment is governed by Directive 89/686/EEC on personal protective equipment (PPE). The Directive is designed to ensure that PPE meets common quality and safety standards by setting out basic safety requirements for personal protective equipment, as well as conditions for its placement on the market and free movement within the EU single market. It covers ‘any device or appliance designed to be worn or held by an individual for protection against one or more health and safety hazards’.[15] The directive was adopted on 21 January 1989 and came into force on 1 July 1992. The European Commission additionally allowed for a transition period until 30 June 1995 to give companies sufficient time to adapt to the legislation. After this date, all PPE placed on the market in EU Member States was required to comply with the requirements of Directive 89/686/EEC and carry the CE Marking.		Article 1 of Directive 89/686/EEC defines personal protective equipment as any device or appliance designed to be worn or held by an individual for protection against one or more health and safety hazards. PPE which falls under the scope of the Directive is divided into three categories:		Directive 89/686/EEC on personal protective equipment does not distinguish between PPE for professional use and PPE for leisure purposes.		Personal protective equipment falling within the scope of the Directive must comply with the basic health and safety requirements set out in Annex II of the Directive. To facilitate conformity with these requirements, harmonized standards are developed at the European or international level by the European Committee for Standardization (CEN, CENELEC) and the International Organization for Standardization in relation to the design and manufacture of the product. Usage of the harmonized standards is voluntary and provides presumption of conformity. However, manufacturers may choose an alternative method of complying with the requirements of the Directive.		Personal protective equipment excluded from the scope of the Directive includes:		The European Commission is currently working to revise Directive 89/686/EEC. The revision will look at the scope of the Directive, the conformity assessment procedures and technical requirements regarding market surveillance. It will also align the Directive with the New Legalislative Framework. The European Commission is likely to publish its proposal in 2013. It will then be discussed by the European Parliament and Council of the European Union under the ordinary legislative procedure before being published in the Official Journal of the European Union and becoming law.		NHS medics practise using PPE to treat Ebola patients.		Members of FEMA wear PPE to protect against radioactive hazards.		Worker wearing a face shield, a helmet, Tyvek coveralls, gloves, and earplugs while decontaminating a containment boom.		Rescue worker wearing a half-mask respirator.		Chilean miners wearing PPE.		Riot officer dressed in armor to protect against impacts.		National Hockey League goaltender wearing the required PPE to play.		Trawl fishermen wearing brightly colored personal flotation devices to protect against drowning.		
Paid time off or personal time off (PTO) is a policy in some employee handbooks that provides a bank of hours in which the employer pools sick days, vacation days, and personal days that allows employees to use as the need or desire arises. This policy pertains mainly to the United States, where there are no federal legal requirements for a minimum number of paid vacation days (see also the list of statutory minimum employment leave by country). Instead, U.S. companies determine the amount of paid time off that will be allotted to employees, while keeping in mind the payoff in recruiting and retaining employees. In the United States, paid vacation is typically two weeks or less per year for the first few years of employment in addition to roughly 10 paid federal holidays.[1]		Generally PTO hours cover everything from planned vacations to sick days, and are becoming more prevalent in the field of human resource management. Unlike more traditional leave plans, PTO plans don't distinguish employee absences from personal days, vacation days, or sick days. Upon employment, the company determines how many PTO hours will be allotted per year and a "rollover" policy. Some companies let PTO hours accumulate for only a year, and unused hours disappear at year-end.[1] Some PTO plans may also accommodate unexpected or unforeseeable circumstances such as jury duty, military duty, and bereavement leave.[2] PTO bank plans typically do not include short-term or long-term disability leave, workers compensation, family and medical leave, sabbatical, or community service leave.[3]		It is unclear as to when PTO bank-type plans were first being utilized in the workforce. In a 2010 study conducted by WorldatWork, 44% of 387 companies surveyed said they started using PTO bank-type plans prior to year 2000.[4]						An early instance of paid time off, in the late 19th century in Australia, was by Alfred Edments who gave every employee a fortnight's holiday on full pay, and when ill, Edments continued to pay their salaries.[5]		A longitudinal study conducted by WorldatWork of over 1,000 organizations of different sizes concluded that over recent years, PTO plans have become more actively utilized by the general workforce. In 2002, about 71% of organizations were using traditional distinguished paid time off system, and about 28% were utilizing the PTO bank-type system. As of 2010, the use of the traditional paid time off system decreased to 54%, while the use of the PTO bank system increased to around 40% of all organizations.		Recent information may indicate that PTO bank-type plans are difficult to implement in very large organizations. In 2010, only 32% of organizations with 20,000+ employees had a PTO bank-type system. However, 51% of organizations with 10,000-19,999 employees had PTO bank-type plans. In organizations with less than 100 employees, 48% had PTO bank-type plans.[4]		In the 2010 study performed by WorldatWork, industrial differences were also found. 97% of organizations in the Education industry use traditional paid time off plans with only 3% utilizing a PTO bank-type system. On the other hand, 80% of organizations in the Health-care and Social Assistance industry utilize PTO bank-type systems.[4]		As of 2012, nearly one in five employees in the United States receive leave in the form of a PTO bank plan, but the contours of such policies are often little understood—especially outside of the human resources community.		Among employees with paid leave, lower-wage employees are less likely to have access to a PTO bank than a traditional paid vacation system. 51% of employees in the lowest average wage quartile have access to any vacation time, and only 9 percent of the lowest wage employees have access to a PTO bank. 89% of employees in the highest wage quartile have access to vacation time and 28% have access to a PTO bank.		There is also a difference in PTO among employment status. 9% of Part-time employees have access to a PTO bank, whereas about 23% of full-time employees do.		14% of unionized organizations have access to PTO bank-type plans.[3]		Paid time off usually increases with years of service to an organization. This provides a greater benefit for employees who have been with the organization longer.		Source: Society for Human Resource Management, 2004 SHRM Benefits Survey.[2]		Because there are no federal requirements in the United States, the states must each determine respective regulations for paid time off in the state labor law. Because of this, employers not only need to be aware, but also need to establish and follow a formal written policy for paid time off. Failing to formally establish paid time off policies may result in violating the state's code and the policy not being legally enforceable.		Vacation is legally vested per formal language in the California Labor Code. Vacation cannot be forfeited once earned, and unused balances must be paid out upon termination.		There is no Pennsylvania labor law which requires an employer to pay an employee not to work. Benefits like sick leave, vacation pay, and severance pay are payments to an employee not to be at work. Therefore, an employer only has to pay these benefits if the employer has a policy to pay such benefits or a contract with you to pay these benefits. An employer must follow its own rules for these kinds of payments.		Most states, in fact, do not require unused vacation balances to be paid out upon termination, and very few states have formal rules protecting employees from changes in the vacation policy; however, all states must comply with federal labor laws such as the Family Medical Leave Act.[7]		In January 2014, 17 days after taking office, Mayor Bill de Blasio put forward paid sick leave legislation to expand this right to more New Yorkers, including 200,000 of whom do not currently have any paid sick days. The law took effect on April 1 and apply to all workers at businesses with five or more employees, encompassing those excluded under the previous legislation that applied to businesses with 15 or more workers.[8]		
Unfree labour is a generic or collective term for those work relations, especially in modern or early modern history, in which people are employed against their will with the threat of destitution, detention, violence (including death), compulsion,[1] or other forms of extreme hardship to themselves or members of their families.		Unfree labour includes all forms of slavery, and related institutions (e.g. debt slavery, serfdom, corvée and labour camps). Many of these forms of work may be covered by the term forced labour, which is defined by the International Labour Organization (ILO) as all involuntary work or service exacted under the menace of a penalty.[2]		However, under the ILO Forced Labour Convention of 1930, the term forced or compulsory labour shall not include:[3]						If payment occurs, it may be in one or more of the following forms:		Unfree labour is often more easily instituted and enforced on migrant workers, who have traveled far from their homelands and who are easily identified because of their physical, ethnic, linguistic, or cultural differences from the general population, since they are unable or unlikely to report their conditions to the authorities.		According to the Marxian economics, under capitalism, workers never keep all of the wealth they create, as some of it goes to the profit of capitalists. By contrast with modern subjective theory of value (as used by neoclassical economists), the wages offered necessarily represent the marginal utility of the labour, and any profit (or loss) is also due to other inputs provided, such as capital, time value of money, or risk.		Unfree labor re-emerged as an issue in the debate about rural development during the years following the end of the 1939–45 war, when a political concern of Keynesian theory was not just economic reconstruction (mainly in Europe and Asia) but also planning (in the Third World). A crucial aspect of the ensuing discussion concerned the extent to which different relational forms constituted obstacles to capitalist development, and why.		During the 1960s and 1970s unfree labor was regarded as incompatible with capitalist accumulation, and thus an obstacle to economic growth, an interpretation advanced by exponents of the then-dominant semi-feudal thesis. From the 1980s onwards, however, another and very different Marxist view emerged, arguing that evidence from Latin America and India suggested agribusiness enterprises, commercial farmers and rich peasants reproduced, introduced or reintroduced unfree relations.		However, recent contributions to this debate have attempted to exclude Marxism from the discussion. These contributions maintain that, because Marxist theory failed to understand the centrality of unfreedom to modern capitalism, a new explanation of this link is needed. This claim has been questioned by Tom Brass (2014), ‘Debating Capitalist Dynamics and Unfree Labour: A Missing Link?’, The Journal of Development Studies, 50:4, 570–82. He argues that many of these new characteristics are in fact no different from those identified earlier by Marxist theory and that the exclusion of the latter approach from the debate is thus unwarranted.		The International Labour Organization (ILO) estimates that at least 12.3 million people are victims of forced labour worldwide; of these, 9.8 million are exploited by private agents and more than 2.4 million are trafficked. Other 2.5 million are forced to work by the state or by rebel military groups.[4][5] From an international law perspective, countries that allow forced labor are violating international labour standards as set forth in the Abolition of Forced Labour Convention (C105), one of the fundamental conventions of the ILO.[6]		According to the ILO Special Action Programme to Combat Forced Labour (SAP-FL), global profits from forced trafficked labour exploited by private agents are estimated at US$44,3 billion per year. About 70% of this value (US$31.6 billion) come from trafficked victims. At least the half of this sum (more than US$15 billion) comes from industrialized countries.[7]		Trafficking is a term to define the recruiting, harbouring, obtaining and transportation of a person by use of force, fraud, or coercion for the purpose of subjecting them to involuntary acts, such as acts related to commercial sexual exploitation (including forced prostitution) or involuntary labour.[8]		The archetypal and best-known form of unfree labour is chattel slavery, in which individual workers are legally owned throughout their lives, and may be bought, sold or otherwise exchanged by owners, while never or rarely receiving any personal benefit from their labour. Slavery was common in many ancient societies, including ancient Greece, ancient Rome, ancient Israel, ancient China, classical Arab states, as well as many societies in Africa and the Americas. Being sold into slavery was a common fate of populations conquered in wars. Perhaps the most prominent example of chattel slavery was the enslavement of many millions of black people in Africa, as well as their forced transplantation to the Americas, Asia or Europe where their status as slaves was usually inherited by their descendants.		The term slavery is often applied to situations which do not meet the above definitions, but which are other, closely related forms of unfree labour, such as debt slavery or debt-bondage (although not all repayment of debts through labour constitutes unfree labour). Examples are the Repartimiento system in the Spanish Empire, or the work of Indigenous Australians in northern Australia on sheep or cattle stations (ranches), from the mid-19th to the mid-20th century. In the latter case, workers were rarely or never paid, and were restricted by regulations and/or police intervention to regions around their places of work.		In late 16th century Japan, "unfree labour" or slavery was officially banned; but forms of contract and indentured labour persisted alongside the period penal codes' forced labour. Somewhat later, the Edo period penal laws prescribed "non-free labour" for the immediate family of executed criminals in Article 17 of the Gotōke reijō (Tokugawa House Laws), but the practice never became common. The 1711 Gotōke reijō was compiled from over 600 statutes promulgated between 1597 and 1696.[9]		According to Kevin Bales, in Disposable People: New Slavery in the Global Economy (1999), there are now an estimated 27 million slaves in the world.[10][11]		A truck system, in the specific sense in which the term is used by labour historians, refers to an unpopular or even exploitative form of payment associated with small, isolated and/or rural communities, in which workers or self-employed small producers are paid in either: goods, a form of payment known as truck wages, or tokens, private currency ("scrip") or direct credit, to be used at a company store, owned by their employers. A specific kind of truck system, in which credit advances are made against future work, is known in the U.S. as debt bondage.		Many scholars have suggested that employers use such systems to exploit workers and/or indebt them. This could occur, for example, if employers were able to pay workers with goods which had a market value below the level of subsistence, or by selling items to workers at inflated prices. Others argue that truck wages, at least in some cases, were a convenient way for isolated communities to operate, when official currency was scarce.		By the early 20th century, truck systems were widely seen, in industrialised countries, as exploitative; perhaps the most well-known example of this view was a 1947 U.S. hit song "Sixteen Tons". Many countries have Truck Act legislation that outlaws truck systems and requires payment in cash.		Though most closely associated with Medieval Europe, governments throughout human history have imposed regular short stints of unpaid labor upon lower social classes. These might be annual obligations of a few weeks or something similarly regular that lasted for the laborer's entire working life. As the system developed in the Philippines and elsewhere, the laborer could pay an appropriate fee and be exempted from the obligation.[citation needed]		In Vetti-chakiri and begar lower castes have only had obligations or duties to render free services to the upper caste community also called as Vetti or Vetti chakiri.[12][clarification needed]		Some countries have implemented conscription for military, paramilitary or security forces, like internal troops, border guards or even police forces. While sometimes paid, conscripts are not free to decline enlistment and draft dodging or desertion are often met with severe punishment. Even in countries which prohibit other forms of unfree labour, conscription is generally justified as being necessary in the national interest.[citation needed]		Some countries also practice forms of conscription for public works. In Portugal[13] and, due to the severe economic crisis, in Greece,[14] a system of civil mobilization[15][16] (in Greece also called political mobilization[citation needed]) and civil conscription[citation needed] was implemented to provide public services as a national interest.		Discussions about implementing civil conscription for unemployed persons were held in Australia during an election campaign in the 1990s.[17]		In Switzerland in general, and in Austria and Germany, as rare exceptions, citizens have to join a Compulsory Fire Service.[18][19][20] In some German states in theory it is possible for communities to draft citizens for public services, called Hand and Tension Services.[21][22]		Another historically significant example of forced labour was that of political prisoners, people from conquered or occupied countries, members of persecuted minorities, and prisoners of war, especially during the 20th century. The best-known example of this are the concentration camp system run by Nazi Germany in Europe during World War II, the Gulag camps[23] run by the Soviet Union,[24] and the forced labour used by the military of the Empire of Japan, especially during the Pacific War (such as the Burma Railway). Roughly 4,000,000 German POWs were used as "reparations labour" by the Allies for several years after the German surrender; this was permitted under the Third Geneva Convention provided they were accorded proper treatment.[25] China's Laogai ("labour reform") system and North Korea's Kwalliso camps are current examples.		About 12 million forced labourers, most of whom were Poles and Soviet citizens (Ost-Arbeiter), were employed in the German war economy inside Nazi Germany.[26][27] More than 2000 German companies profited from slave labour during the Nazi era, including Daimler, Deutsche Bank, Siemens, Volkswagen, Hoechst, Dresdner Bank, Krupp, Allianz, BASF, Bayer, BMW, and Degussa.[28][29]		In Asia, according to a joint study of historians featuring Zhifen Ju, Mark Peattie, Toru Kubo, and Mitsuyoshi Himeta, more than 10 million Chinese were mobilized by the Japanese army and enslaved by the Kōa-in for slave labour in Manchukuo and north China.[30] The U.S. Library of Congress estimates that in Java, between 4 and 10 million romusha (Japanese: "manual laborer") were forced to work by the Japanese military. About 270,000 of these Javanese laborers were sent to other Japanese-held areas in South East Asia. Only 52,000 were repatriated to Java, meaning that there was a death rate of 80%.[31]		Kerja rodi (Heerensteinten), was the term for forced labor in Indonesia during Dutch colonisation.		The Khmer Rouge attempted to turn Cambodia into a classless society by depopulating cities and forcing the urban population ("New People") into agricultural communes. The entire population was forced to become farmers in labour camps.		Convict or prison labour is another classic form of unfree labour. The forced labour of convicts has often been regarded with lack of sympathy, because of the social stigma attached to people regarded as "common criminals". In some countries and historical periods, however, prison labour has been forced upon people who have been victims of prejudice, convicted of political crimes, convicted of "victimless crimes", or people who committed theft or related offences because they lacked any other means of subsistence—categories of people who typically call for compassion according to current ethical ideas.		Three British colonies in Australia—New South Wales, Van Diemen's Land (Tasmania) and Western Australia—are three examples of the state use of convict labour. Australia received thousands of convict labourers in the eighteenth and nineteenth centuries who were given sentences for crimes ranging from those now considered to be minor misdemeanors to such serious offences as murder, rape and incest. A considerable number of Irish convicts were sentenced to transportation for 'treason' while fighting for Irish independence from British rule.		More than 165,000 convicts were transported to Australian colonies from 1788 to 1868.[32] Most British or Irish convicts who were sentenced to transportation, however, completed their sentences in British jails and were not transported at all.		It is estimated that in the last 50 years more than 50 million people have been sent to Chinese laogai camps.[33]		A more common form in modern society is indenture, or bonded labour, under which workers sign contracts to work for a specific period of time, for which they are paid only with accommodation and sustenance, or these essentials in addition to limited benefits such as cancellation of a debt, or transportation to a desired country.		
The Great Depression was a severe worldwide economic depression that took place during the 1930s. The timing of the Great Depression varied across nations; in most countries it started in 1929 and lasted until 1941.[1] It was the longest, deepest, and most widespread depression of the 20th century.[2] In the 21st century, the Great Depression is commonly used as an example of how far the world's economy can decline.[3]		The depression originated in the United States, after a major fall in stock prices that began around September 4, 1929, and became worldwide news with the stock market crash of October 29, 1929 (known as Black Tuesday). Between 1929 and 1932, worldwide gross domestic product (GDP) fell by an estimated 15%. By comparison, worldwide GDP fell by less than 1% from 2008 to 2009 during the Great Recession.[4] Some economies started to recover by the mid-1930s. However, in many countries, the negative effects of the Great Depression lasted until the beginning of World War II.[5]		The Great Depression had devastating effects in countries both rich and poor. Personal income, tax revenue, profits and prices dropped, while international trade plunged by more than 50%. Unemployment in the U.S. rose to 25% and in some countries rose as high as 33%.[6]		Cities all around the world were hit hard, especially those dependent on heavy industry. Construction was virtually halted in many countries. Farming communities and rural areas suffered as crop prices fell by about 60%.[7][8][9] Facing plummeting demand with few alternative sources of jobs, areas dependent on primary sector industries such as mining and logging suffered the most.[10]						Economic historians usually attribute the start of the Great Depression to the sudden devastating collapse of U.S. stock market prices on October 29, 1929, known as Black Tuesday, However,[11] some dispute this conclusion and see the stock crash as a symptom, rather than a cause, of the Great Depression.[6][12]		Even after the Wall Street Crash of 1929 optimism persisted for some time. John D. Rockefeller said "These are days when many are discouraged. In the 93 years of my life, depressions have come and gone. Prosperity has always returned and will again."[13] The stock market turned upward in early 1930, returning to early 1929 levels by April. This was still almost 30% below the peak of September 1929.[14]		Together, government and business spent more in the first half of 1930 than in the corresponding period of the previous year. On the other hand, consumers, many of whom had suffered severe losses in the stock market the previous year, cut back their expenditures by 10%. In addition, beginning in the mid-1930s, a severe drought ravaged the agricultural heartland of the U.S.[15]		By mid-1930, interest rates had dropped to low levels, but expected deflation and the continuing reluctance of people to borrow meant that consumer spending and investment were depressed.[16] By May 1930, automobile sales had declined to below the levels of 1928. Prices in general began to decline, although wages held steady in 1930. Then a deflationary spiral started in 1931. Conditions were worse in farming areas, where commodity prices plunged and in mining and logging areas, where unemployment was high and there were few other jobs.[citation needed]		The decline in the U.S. economy was the factor that pulled down most other countries at first; then, internal weaknesses or strengths in each country made conditions worse or better. Frantic attempts to shore up the economies of individual nations through protectionist policies, such as the 1930 U.S. Smoot–Hawley Tariff Act and retaliatory tariffs in other countries, exacerbated the collapse in global trade.[17] By late 1930, a steady decline in the world economy had set in, which did not reach bottom until 1933.[citation needed]		Change in economic indicators 1929–32[18]		The two classical competing theories of the Great Depression are the Keynesian (demand-driven) and the monetarist explanation. There are also various heterodox theories that downplay or reject the explanations of the Keynesians and monetarists. The consensus among demand-driven theories is that a large-scale loss of confidence led to a sudden reduction in consumption and investment spending. Once panic and deflation set in, many people believed they could avoid further losses by keeping clear of the markets. Holding money became profitable as prices dropped lower and a given amount of money bought ever more goods, exacerbating the drop in demand. Monetarists believe that the Great Depression started as an ordinary recession, but the shrinking of the money supply greatly exacerbated the economic situation, causing a recession to descend into the Great Depression.		Economists and economic historians are almost evenly split as to whether the traditional monetary explanation that monetary forces were the primary cause of the Great Depression is right, or the traditional Keynesian explanation that a fall in autonomous spending, particularly investment, is the primary explanation for the onset of the Great Depression.[19] Today the controversy is of lesser importance since there is mainstream support for the debt deflation theory and the expectations hypothesis that building on the monetary explanation of Milton Friedman and Anna Schwartz add non-monetary explanations.		There is consensus that the Federal Reserve System should have cut short the process of monetary deflation and banking collapse. If the Fed had done that the economic downturn would have been far less severe and much shorter.[20]		British economist John Maynard Keynes argued in The General Theory of Employment, Interest and Money that lower aggregate expenditures in the economy contributed to a massive decline in income and to employment that was well below the average. In such a situation, the economy reached equilibrium at low levels of economic activity and high unemployment.		Keynes' basic idea was simple: to keep people fully employed, governments have to run deficits when the economy is slowing, as the private sector would not invest enough to keep production at the normal level and bring the economy out of recession. Keynesian economists called on governments during times of economic crisis to pick up the slack by increasing government spending and/or cutting taxes.		As the Depression wore on, Franklin D. Roosevelt tried public works, farm subsidies, and other devices to restart the U.S. economy, but never completely gave up trying to balance the budget. According to the Keynesians, this improved the economy, but Roosevelt never spent enough to bring the economy out of recession until the start of World War II.[21]		Monetarists follow the explanation given by Milton Friedman and Anna J. Schwartz. They argue that the Great Depression was caused by the banking crisis that caused one-third of all banks to vanish, a reduction of bank shareholder wealth and more importantly monetary contraction by 35%. This caused a price drop by 33% (deflation).[22] By not lowering interest rates, by not increasing the monetary base and by not injecting liquidity into the banking system to prevent it from crumbling the Federal Reserve passively watched the transforming of a normal recession into the Great Depression. Friedman argued that the downward turn in the economy, starting with the stock market crash, would have been just another garden variety recession if the Federal Reserve had taken aggressive action.[23][24][25]		The Federal Reserve allowed some large public bank failures – particularly that of the New York Bank of United States – which produced panic and widespread runs on local banks, and the Federal Reserve sat idly by while banks collapsed. He claimed that, if the Fed had provided emergency lending to these key banks, or simply bought government bonds on the open market to provide liquidity and increase the quantity of money after the key banks fell, all the rest of the banks would not have fallen after the large ones did, and the money supply would not have fallen as far and as fast as it did.[26]		With significantly less money to go around, businessmen could not get new loans and could not even get their old loans renewed, forcing many to stop investing. This interpretation blames the Federal Reserve for inaction, especially the New York Branch.[27]		One reason why the Federal Reserve did not act to limit the decline of the money supply was the gold standard. At that time, the amount of credit the Federal Reserve could issue was limited by the Federal Reserve Act, which required 40% gold backing of Federal Reserve Notes issued. By the late 1920s, the Federal Reserve had almost hit the limit of allowable credit that could be backed by the gold in its possession. This credit was in the form of Federal Reserve demand notes.[28] A "promise of gold" is not as good as "gold in the hand", particularly when they only had enough gold to cover 40% of the Federal Reserve Notes outstanding. During the bank panics a portion of those demand notes were redeemed for Federal Reserve gold. Since the Federal Reserve had hit its limit on allowable credit, any reduction in gold in its vaults had to be accompanied by a greater reduction in credit. On April 5, 1933, President Roosevelt signed Executive Order 6102 making the private ownership of gold certificates, coins and bullion illegal, reducing the pressure on Federal Reserve gold.[28]		From the point of view of today's mainstream schools of economic thought, government should strive to keep the interconnected macroeconomic aggregates money supply and/or aggregate demand on a stable growth path. When threatened by the forecast of a depression central banks should pour liquidity into the banking system and the government should cut taxes and accelerate spending in order to keep the nominal money stock and total nominal demand from collapsing.[29] At the beginning of the Great Depression most economists believed in Say's law and the self-equilibrating powers of the market and failed to explain the severity of the Depression. Outright leave-it-alone liquidationism was a position mainly held by the Austrian School.[30] The liquidationist position was that a depression is good medicine. The idea was the benefit of a depression was to liquidate failed investments and businesses that have been made obsolete by technological development in order to release factors of production (capital and labor) from unproductive uses so that these could be redeployed in other sectors of the technologically dynamic economy. They argued that even if self-adjustment of the economy took mass bankruptcies, then so be it.[30] An increasingly common view among economic historians is that the adherence of some Federal Reserve policymakers to the liquidationist thesis led to disastrous consequences.[31] Regarding the policies of President Hoover, economists like Barry Eichengreen and J. Bradford DeLong point out that President Hoover tried to keep the federal budget balanced until 1932, when he lost confidence in his Secretary of the Treasury Andrew Mellon and replaced him.[30][31][32] Despite liquidationist expectations, a large proportion of the capital stock was not redeployed but vanished during the first years of the Great Depression. According to a study by Olivier Blanchard and Lawrence Summers, the recession caused a drop of net capital accumulation to pre-1924 levels by 1933.[33] Milton Friedman called the leave-it-alone liquidationism "dangerous nonsense".[29] He wrote:		The monetary explanation has two weaknesses. First it is not able to explain why the demand for money was falling more rapidly than the supply during the initial downturn in 1930–31.[19] Second it is not able to explain why in March 1933 a recovery took place although short term interest rates remained close to zero and the Money supply was still falling. These questions are addressed by modern explanations that build on the monetary explanation of Milton Friedman and Anna Schwartz but add non-monetary explanations.		Irving Fisher argued that the predominant factor leading to the Great Depression was a vicious circle of deflation and growing over-indebtedness.[34] He outlined nine factors interacting with one another under conditions of debt and deflation to create the mechanics of boom to bust. The chain of events proceeded as follows:		During the Crash of 1929 preceding the Great Depression, margin requirements were only 10%.[35] Brokerage firms, in other words, would lend $9 for every $1 an investor had deposited. When the market fell, brokers called in these loans, which could not be paid back.[36] Banks began to fail as debtors defaulted on debt and depositors attempted to withdraw their deposits en masse, triggering multiple bank runs. Government guarantees and Federal Reserve banking regulations to prevent such panics were ineffective or not used. Bank failures led to the loss of billions of dollars in assets.[36]		Outstanding debts became heavier, because prices and incomes fell by 20–50% but the debts remained at the same dollar amount. After the panic of 1929, and during the first 10 months of 1930, 744 U.S. banks failed. (In all, 9,000 banks failed during the 1930s). By April 1933, around $7 billion in deposits had been frozen in failed banks or those left unlicensed after the March Bank Holiday.[37] Bank failures snowballed as desperate bankers called in loans which the borrowers did not have time or money to repay. With future profits looking poor, capital investment and construction slowed or completely ceased. In the face of bad loans and worsening future prospects, the surviving banks became even more conservative in their lending.[36] Banks built up their capital reserves and made fewer loans, which intensified deflationary pressures. A vicious cycle developed and the downward spiral accelerated.		The liquidation of debt could not keep up with the fall of prices which it caused. The mass effect of the stampede to liquidate increased the value of each dollar owed, relative to the value of declining asset holdings. The very effort of individuals to lessen their burden of debt effectively increased it. Paradoxically, the more the debtors paid, the more they owed.[34] This self-aggravating process turned a 1930 recession into a 1933 great depression.		Fisher's debt-deflation theory initially lacked mainstream influence because of the counter-argument that debt-deflation represented no more than a redistribution from one group (debtors) to another (creditors). Pure re-distributions should have no significant macroeconomic effects.		Building on both the monetary hypothesis of Milton Friedman and Anna Schwartz as well as the debt deflation hypothesis of Irving Fisher, Ben Bernanke developed an alternative way in which the financial crisis affected output. He builds on Fisher's argument that dramatic declines in the price level and nominal incomes lead to increasing real debt burdens which in turn leads to debtor insolvency and consequently leads to lowered aggregate demand, a further decline in the price level then results in a debt deflationary spiral. According to Bernanke, a small decline in the price level simply reallocates wealth from debtors to creditors without doing damage to the economy. But when the deflation is severe falling asset prices along with debtor bankruptcies lead to a decline in the nominal value of assets on bank balance sheets. Banks will react by tightening their credit conditions, that in turn leads to a credit crunch which does serious harm to the economy. A credit crunch lowers investment and consumption and results in declining aggregate demand which additionally contributes to the deflationary spiral.[38][39][40]		Since economic mainstream turned to the new neoclassical synthesis, expectations are a central element of macroeconomic models. According to Peter Temin, Barry Wigmore, Gauti B. Eggertsson and Christina Romer, the key to recovery and to ending the Great Depression was brought about by a successful management of public expectations. The thesis is based on the observation that after years of deflation and a very severe recession important economic indicators turned positive in March 1933 when Franklin D. Roosevelt took office. Consumer prices turned from deflation to a mild inflation, industrial production bottomed out in March 1933, and investment doubled in 1933 with a turnaround in March 1933. There were no monetary forces to explain that turn around. Money supply was still falling and short term interest rates remained close to zero. Before March 1933 people expected further deflation and a recession so that even interest rates at zero did not stimulate investment. But when Roosevelt announced major regime changes people began to expect inflation and an economic expansion. With these positive expectations, interest rates at zero began to stimulate investment just as they were expected to do. Roosevelt's fiscal and monetary policy regime change helped to make his policy objectives credible. The expectation of higher future income and higher future inflation stimulated demand and investments. The analysis suggests that the elimination of the policy dogmas of the gold standard, a balanced budget in times of crises and small government led endogenously to a large shift in expectation that accounts for about 70–80 percent of the recovery of output and prices from 1933 to 1937. If the regime change had not happened and the Hoover policy had continued, the economy would have continued its free fall in 1933, and output would have been 30% lower in 1937 than in 1933.[41][42][43]		The recession of 1937–38, which slowed down economic recovery from the Great Depression, is explained by fears of the population that the moderate tightening of the monetary and fiscal policy in 1937 would be first steps to a restoration of the pre-March 1933 policy regime.[44]		Theorists of the "Austrian School" who wrote about the Depression include Austrian economist Friedrich Hayek and American economist Murray Rothbard, who wrote America's Great Depression (1963). In their view and like the monetarists, the Federal Reserve, which was created in 1913, shoulders much of the blame; but in opposition to the monetarists, they argue that the key cause of the Depression was the expansion of the money supply in the 1920s that led to an unsustainable credit-driven boom.[45]		In the Austrian view it was this inflation of the money supply that led to an unsustainable boom in both asset prices (stocks and bonds) and capital goods. By the time the Fed belatedly tightened in 1928, it was far too late and, in the Austrian view, a significant economic contraction was inevitable.[45] In February 1929 Hayek published a paper predicting the Federal Reserve's actions would lead to a crisis starting in the stock and credit markets.[46]		According to Rothbard, government support for failed enterprises and keeping wages above their market values actually prolonged the Depression.[47] Hayek, unlike Rothbard, believed since the 1970s, along with the monetarists, that the Federal Reserve further contributed to the problems of the Depression by permitting the money supply to shrink during the earliest years of the Depression.[48] However, in 1932 and 1934 Hayek had criticised the FED and the Bank of England for not taking a more contractionary stance.[49]		Hans Sennholz argued that most boom and busts that plagued the American economy in 1819–20, 1839–43, 1857–60, 1873–78, 1893–97, and 1920–21, were generated by government creating a boom through easy money and credit, which was soon followed by the inevitable bust. The spectacular crash of 1929 followed five years of reckless credit expansion by the Federal Reserve System under the Coolidge Administration. The passing of the Sixteenth Amendment, the passage of The Federal Reserve Act, rising government deficits, the passage of the Hawley-Smoot Tariff Act, and the Revenue Act of 1932, exacerbated the crisis, prolonging it.[50]		Ludwig von Mises wrote in the 1930s: "Credit expansion cannot increase the supply of real goods. It merely brings about a rearrangement. It diverts capital investment away from the course prescribed by the state of economic wealth and market conditions. It causes production to pursue paths which it would not follow unless the economy were to acquire an increase in material goods. As a result, the upswing lacks a solid base. It is not a real prosperity. It is illusory prosperity. It did not develop from an increase in economic wealth, i.e. the accumulation of savings made available for productive investment. Rather, it arose because the credit expansion created the illusion of such an increase. Sooner or later, it must become apparent that this economic situation is built on sand."[51][52]		Karl Marx saw recession and depression as unavoidable under free-market capitalism as there are no restrictions on accumulations of capital other than the market itself. In the Marxist view, capitalism tends to create unbalanced accumulations of wealth, leading to over-accumulations of capital which inevitably lead to a crisis. This especially sharp bust is a regular feature of the boom and bust pattern of what Marxists term "chaotic" capitalist development. It is a tenet of many Marxist groupings that such crises are inevitable and will be increasingly severe until the contradictions inherent in the mismatch between the mode of production and the development of productive forces reach the final point of failure. At which point, the crisis period encourages intensified class conflict and forces societal change.[53]		Two economists of the 1920s, Waddill Catchings and William Trufant Foster, popularized a theory that influenced many policy makers, including Herbert Hoover, Henry A. Wallace, Paul Douglas, and Marriner Eccles. It held the economy produced more than it consumed, because the consumers did not have enough income. Thus the unequal distribution of wealth throughout the 1920s caused the Great Depression.[54][55]		According to this view, the root cause of the Great Depression was a global over-investment in heavy industry capacity compared to wages and earnings from independent businesses, such as farms. The proposed solution was for the government to pump money into the consumers' pockets. That is, it must redistribute purchasing power, maintaining the industrial base, and re-inflating prices and wages to force as much of the inflationary increase in purchasing power into consumer spending. The economy was overbuilt, and new factories were not needed. Foster and Catchings recommended[56] federal and state governments to start large construction projects, a program followed by Hoover and Roosevelt.		It cannot be emphasized too strongly that the [productivity, output and employment] trends we are describing are long-time trends and were thoroughly evident prior to 1929. These trends are in nowise the result of the present depression, nor are they the result of the World War. On the contrary, the present depression is a collapse resulting from these long-term trends. [57]		The first three decades of the 20th century saw economic output surge with electrification, mass production and motorized farm machinery, and because of the rapid growth in productivity there was a lot of excess production capacity and the work week was being reduced.[citation needed]		The dramatic rise in productivity of major industries in the U. S. and the effects of productivity on output, wages and the work week are discussed by Spurgeon Bell in his book Productivity, Wages, and National Income (1940).[58]		The gold standard was the primary transmission mechanism of the Great Depression. Even countries that did not face bank failures and a monetary contraction first hand were forced to join the deflationary policy since higher interest rates in countries that performed a deflationary policy led to a gold outflow in countries with lower interest rates. Under the gold standards price–specie flow mechanism countries that lost gold but nevertheless wanted to maintain the gold standard had to permit their money supply to decrease and the domestic price level to decline (deflation).[59][60]		There is also consensus that protectionist policies such as the Smoot-Hawley Tariff Act helped to worsen the depression.[61]		Some economic studies have indicated that just as the downturn was spread worldwide by the rigidities of the Gold Standard, it was suspending gold convertibility (or devaluing the currency in gold terms) that did the most to make recovery possible.[63]		Every major currency left the gold standard during the Great Depression. Great Britain was the first to do so. Facing speculative attacks on the pound and depleting gold reserves, in September 1931 the Bank of England ceased exchanging pound notes for gold and the pound was floated on foreign exchange markets.		Great Britain, Japan, and the Scandinavian countries left the gold standard in 1931. Other countries, such as Italy and the U.S., remained on the gold standard into 1932 or 1933, while a few countries in the so-called "gold bloc", led by France and including Poland, Belgium and Switzerland, stayed on the standard until 1935–36.		According to later analysis, the earliness with which a country left the gold standard reliably predicted its economic recovery. For example, Great Britain and Scandinavia, which left the gold standard in 1931, recovered much earlier than France and Belgium, which remained on gold much longer. Countries such as China, which had a silver standard, almost avoided the depression entirely. The connection between leaving the gold standard as a strong predictor of that country's severity of its depression and the length of time of its recovery has been shown to be consistent for dozens of countries, including developing countries. This partly explains why the experience and length of the depression differed between national economies.[64]		Many economists have argued that the sharp decline in international trade after 1930 helped to worsen the depression, especially for countries significantly dependent on foreign trade. In a 1995 survey of American economic historians, two-thirds agreed that the Smoot-Hawley tariff act at least worsened the Great Depression.[65] Most historians and economists partly blame the American Smoot-Hawley Tariff Act (enacted June 17, 1930) for worsening the depression by seriously reducing international trade and causing retaliatory tariffs in other countries. While foreign trade was a small part of overall economic activity in the U.S. and was concentrated in a few businesses like farming, it was a much larger factor in many other countries.[66] The average ad valorem rate of duties on dutiable imports for 1921–25 was 25.9% but under the new tariff it jumped to 50% during 1931–35. In dollar terms, American exports declined over the next four (4) years from about $5.2 billion in 1929 to $1.7 billion in 1933; so, not only did the physical volume of exports fall, but also the prices fell by about 1/3 as written. Hardest hit were farm commodities such as wheat, cotton, tobacco, and lumber.		Governments around the world took various steps into spending less money on foreign goods such as: “imposing tariffs, import quotas, and exchange controls”. These restrictions formed a lot of tension between trade nations, causing a major deduction during the depression. Not all countries enforced the same measures of protectionism. Some countries raised tariffs drastically and enforced severe restrictions on foreign exchange transactions, while other countries condensed “trade and exchange restrictions only marginally”:[67]		But many economist think that the Smoot-Hawley tariff act was not a major contribution to the great depression:		Economist Paul Krugman argues against the notion that protectionism caused the Great Depression."Where protectionism really mattered was in preventing a recovery in trade when production recovered". He cites a report by Barry Eichengreen and Douglas Irwin: Figure 1 in that report shows trade and production dropping together from 1929 to 1932, but production increasing faster than trade from 1932 to 1937. The authors argue that adherence to the gold standard forced many countries to resort to tariffs, when instead they should have devalued their currencies.[68]		Milton Friedman also said that Smoot-Hawley tariff of 1930 didn't cause the Great Depression. Douglas A. Irwin writes : "most economists, both liberal and conservative, doubt that Smoot Hawley played much of a role in the subsequent contraction."[69]		Peter Temin an economist at the Massachusetts Institute of Technology explains a tariff is an expansionary policy, like a devaluation as it diverts demand from foreign to home producers. He notes that exports were 7 percent of GNP in 1929, they fell by 1.5 percent of 1929 GNP in the next two years, real GNP fell over 15 percent in these same years and the fall was offset by the increase in domestic demand from tariff. He concludes that contrary the popular argument, contractionary effect of the tariff was small. (Temin, P. 1989. Lessons from the Great Depression, MIT Press, Cambridge, Mass)[70]		William Bernstein writes "most economic historians now believe that only a minuscule part of that huge loss of both world GDP and the United States’ GDP can be ascribed to the tariff wars"because trade was only nine percent of global output, not enough to account for the seventeen percent drop in GDP following the Crash. He thinks the damage done could not possibly have exceeded 2 percent of world GDP and tariff "didn't even significantly deepen the Great Depression."(A Splendid Exchange: How Trade Shaped the World)		Nobel laureate Maurice Allais, thinks that tariff was rather helpful in the face of deregulation of competition in the global labor market and excessively loose credit prior to the Crash, and believes the financial and banking crisis were the consequence of it. He notes and higher trade barriers were partly a means to protect domestic demand from deflation and external disturbances. He obserses domestic production in the major industrialized countries fell faster than international trade contracted; if contraction of foreign trade had been the cause of the Depression, he argues, the opposite should have occurred. So, the decline in trade between 1929 and 1933 was a consequence of the Depression, not a cause. Most of the trade contraction took place between January 1930 and July 1932, before the introduction of the majority of protectionist measures, excepting limited American measures applied in the summer of 1930. It was the collapse of international liquidity that caused of the contraction of trade.[71]		The financial crisis escalated out of control and mid-1931, starting with the collapse of the Credit Anstalt in Vienna in May.[72][73] This put heavy pressure on Germany, which was already in political turmoil. With the rise in violence of Nazi and communist movements, as well as investor nervousness at harsh government financial policies.[74] Investors withdrew their short-term money from Germany, as confidence spiraled downward. The Reichsbank lost 150 million marks in the first week of June, 540 million in the second, and 150 million in two days, June 19–20. Collapse was at hand. U.S. President Herbert Hoover called for a moratorium on Payment of war reparations. This angered Paris, which depended on a steady flow of German payments, but it slowed the crisis down and the moratorium, was agreed to in July 1931. International conference in London later in July produced no agreements but on August 19 a standstill agreement froze Germany's foreign liabilities for six months. Germany received emergency funding from private banks in New York as well as the Bank of International Settlements and the Bank of England. The funding only slowed the process; it's nothing. Industrial failures began in Germany, a major bank closed in July and a two-day holiday for all German banks was declared. Business failures more frequent in July, and spread to Romania and Hungary. The crisis continued to get worse in Germany, bringing political upheaval that finally led to the coming to power (through free elections) of Hitler's Nazi regime in January 1933.[75]		The world financial crisis now began to overwhelm Britain; investors across the world started withdrawing their gold from London at the rate of £2½ millions a day.[76] Credits of £25 millions each from the Bank of France and the Federal Reserve Bank of New York and an issue of £15 millions fiduciary note slowed, but did not reverse the British crisis. The financial crisis now caused a major political crisis in Britain in August 1931. With deficits mounting, the bankers demanded a balanced budget; the divided cabinet of Prime Minister Ramsay MacDonald's Labour government agreed; it proposed to raise taxes, cut spending and most controversially, to cut unemployment benefits 20%. The attack on welfare was totally unacceptable to the Labour movement. MacDonald wanted to resign, but King George V insisted he remain and form an all-party coalition "National government." The Conservative and Liberals parties signed on, along with a small cadre of Labour, but the vast majority of Labour leaders denounced MacDonald as a traitor for leading the new government. Britain went off the gold standard, and suffered relatively less than other major countries in the Grade Depression. In the 1931 British election the Labour Party was virtually destroyed, leaving MacDonald as Prime Minister for a largely Conservative coalition.[77][78]		In most countries of the world, recovery from the Great Depression began in 1933.[11] In the U.S., recovery began in early 1933,[11] but the U.S. did not return to 1929 GNP for over a decade and still had an unemployment rate of about 15% in 1940, albeit down from the high of 25% in 1933. The measurement of the unemployment rate in this time period was unsophisticated and complicated by the presence of massive underemployment, in which employers and workers engaged in rationing of jobs.[citation needed]		There is no consensus among economists regarding the motive force for the U.S. economic expansion that continued through most of the Roosevelt years (and the 1937 recession that interrupted it). The common view among most economists is that Roosevelt's New Deal policies either caused or accelerated the recovery, although his policies were never aggressive enough to bring the economy completely out of recession. Some economists have also called attention to the positive effects from expectations of reflation and rising nominal interest rates that Roosevelt's words and actions portended.[80][81] It was the rollback of those same reflationary policies that led to the interrupting recession of 1937.[82][83] One contributing policy that reversed reflation was the Banking Act of 1935, which effectively raised reserve requirements, causing a monetary contraction that helped to thwart the recovery.[84] GDP returned to its upward trend in 1938.[citation needed]		According to Christina Romer, the money supply growth caused by huge international gold inflows was a crucial source of the recovery of the United States economy, and that the economy showed little sign of self-correction. The gold inflows were partly due to devaluation of the U.S. dollar and partly due to deterioration of the political situation in Europe.[85] In their book, A Monetary History of the United States, Milton Friedman and Anna J. Schwartz also attributed the recovery to monetary factors, and contended that it was much slowed by poor management of money by the Federal Reserve System. Former Chairman of the Federal Reserve Ben Bernanke agreed that monetary factors played important roles both in the worldwide economic decline and eventual recovery.[86] Bernanke also saw a strong role for institutional factors, particularly the rebuilding and restructuring of the financial system,[87] and pointed out that the Depression should be examined in an international perspective.[88]		Women's primary role were as housewives; without a steady flow of family income, their work became much harder in dealing with food and clothing and medical care. Birthrates fell everywhere, as children were postponed until families could financially support them. The average birthrate for 14 major countries fell 12% from 19.3 births per thousand population in 1930, to 17.0 in 1935.[89] In Canada, half of Roman Catholic women defied Church teachings and used contraception to postpone births.[90]		Among the few women in the labor force, layoffs were less common in the white-collar jobs and they were typically found in light manufacturing work. However, there was a widespread demand to limit families to one paid job, so that wives might lose employment if their husband was employed.[91][92][93] Across Britain, there was a tendency for married women to join the labor force, competing for part-time jobs especially.[94]		In rural and small-town areas, women expanded their operation of vegetable gardens to include as much food production as possible. In the United States, agricultural organizations sponsored programs to teach housewives how to optimize their gardens and to raise poultry for meat and eggs.[95] In American cities, African American women quiltmakers enlarged their activities, promoted collaboration, and trained neophytes. Quilts were created for practical use from various inexpensive materials and increased social interaction for women and promoted camaraderie and personal fulfillment.[96]		Oral history provides evidence for how housewives in a modern industrial city handled shortages of money and resources. Often they updated strategies their mothers used when they were growing up in poor families. Cheap foods were used, such as soups, beans and noodles. They purchased the cheapest cuts of meat—sometimes even horse meat—and recycled the Sunday roast into sandwiches and soups. They sewed and patched clothing, traded with their neighbors for outgrown items, and made do with colder homes. New furniture and appliances were postponed until better days. Many women also worked outside the home, or took boarders, did laundry for trade or cash, and did sewing for neighbors in exchange for something they could offer. Extended families used mutual aid—extra food, spare rooms, repair-work, cash loans—to help cousins and in-laws.[97]		In Japan, official government policy was deflationary and the opposite of Keynesian spending. Consequently, the government launched a nationwide campaign to induce households to reduce their consumption, focusing attention on spending by housewives.[98]		In Germany, the government tried to reshape private household consumption under the Four-Year Plan of 1936 to achieve German economic self-sufficiency. The Nazi women's organizations, other propaganda agencies and the authorities all attempted to shape such consumption as economic self-sufficiency was needed to prepare for and to sustain the coming war. The organizations, propaganda agencies and authorities employed slogans that called up traditional values of thrift and healthy living. However, these efforts were only partly successful in changing the behavior of housewives.[99]		The common view among economic historians is that the Great Depression ended with the advent of World War II. Many economists believe that government spending on the war caused or at least accelerated recovery from the Great Depression, though some consider that it did not play a very large role in the recovery. It did help in reducing unemployment.[11][100][101][102]		The rearmament policies leading up to World War II helped stimulate the economies of Europe in 1937–39. By 1937, unemployment in Britain had fallen to 1.5 million. The mobilization of manpower following the outbreak of war in 1939 ended unemployment.[103]		When the United States entered into the war in 1941, it finally eliminated the last effects from the Great Depression and brought the U.S. unemployment rate down below 10%.[104] In the U.S., massive war spending doubled economic growth rates, either masking the effects of the Depression or essentially ending the Depression. Businessmen ignored the mounting national debt and heavy new taxes, redoubling their efforts for greater output to take advantage of generous government contracts.[citation needed]		The majority of countries set up relief programs and most underwent some sort of political upheaval, pushing them to the right. Many of the countries in Europe and Latin America that were democracies saw them overthrown by some form of dictatorship or authoritarian rule, most famously in Germany in 1933. The Dominion of Newfoundland gave up democracy voluntarily.		Australia's dependence on agricultural and industrial exports meant it was one of the hardest-hit developed countries.[105] Falling export demand and commodity prices placed massive downward pressures on wages. Unemployment reached a record high of 29% in 1932,[106] with incidents of civil unrest becoming common. After 1932, an increase in wool and meat prices led to a gradual recovery.[107]		Harshly affected by both the global economic downturn and the Dust Bowl, Canadian industrial production had fallen to only 58% of the 1929 level by 1932, the second lowest level in the world after the United States, and well behind nations such as Britain, which fell to only 83% of the 1929 level. Total national income fell to 56% of the 1929 level, again worse than any nation apart from the United States. Unemployment reached 27% at the depth of the Depression in 1933.[108]		The League of Nations labeled Chile the country hardest hit by the Great Depression because 80% of government revenue came from exports of copper and nitrates, which were in low demand. Chile initially felt the impact of the Great Depression in 1930, when GDP dropped 14%, mining income declined 27%, and export earnings fell 28%. By 1932, GDP had shrunk to less than half of what it had been in 1929, exacting a terrible toll in unemployment and business failures.		Influenced profoundly by the Great Depression, many national leaders promoted the development of local industry in an effort to insulate the economy from future external shocks. After six years of government austerity measures, which succeeded in reestablishing Chile's creditworthiness, Chileans elected to office during the 1938–58 period a succession of center and left-of-center governments interested in promoting economic growth by means of government intervention.		Prompted in part by the devastating 1939 Chillán earthquake, the Popular Front government of Pedro Aguirre Cerda created the Production Development Corporation (Corporación de Fomento de la Producción, CORFO) to encourage with subsidies and direct investments an ambitious program of import substitution industrialization. Consequently, as in other Latin American countries, protectionism became an entrenched aspect of the Chilean economy.		China was largely unaffected by the Depression, mainly by having stuck to the Silver standard. However, the U.S. silver purchase act of 1934 created an intolerable demand on China's silver coins, and so in the end the silver standard was officially abandoned in 1935 in favor of the four Chinese national banks' "legal note" issues. China and the British colony of Hong Kong, which followed suit in this regard in September 1935, would be the last to abandon the silver standard. In addition, the Nationalist Government also acted energetically to modernize the legal and penal systems, stabilize prices, amortize debts, reform the banking and currency systems, build railroads and highways, improve public health facilities, legislate against traffic in narcotics and augment industrial and agricultural production. On November 3, 1935, the government instituted the fiat currency (fapi) reform, immediately stabilizing prices and also raising revenues for the government.		The crisis affected France a bit later than other countries, hitting around 1931.[109] While the 1920s grew at the very strong rate of 4.43% per year, the 1930s rate fell to only 0.63%.[110]		The depression was relatively mild: unemployment peaked under 5%, the fall in production was at most 20% below the 1929 output; there was no banking crisis.[111]		However, the depression had drastic effects on the local economy, and partly explains the February 6, 1934 riots and even more the formation of the Popular Front, led by SFIO socialist leader Léon Blum, which won the elections in 1936. Ultra-nationalist groups also saw increased popularity, although democracy prevailed into World War II.		France's relatively high degree of self-sufficiency meant the damage was considerably less than in nations like Germany.		The Great Depression hit Germany hard. The impact of the Wall Street Crash forced American banks to end the new loans that had been funding the repayments under the Dawes Plan and the Young Plan. The financial crisis escalated out of control and mid-1931, starting with the collapse of the Credit Anstalt in Vienna in May.[73] This put heavy pressure on Germany, which was already in political turmoil. With the rise in violence of Nazi and communist movements, as well as investor nervousness at harsh government financial policies.[74] Investors withdrew their short-term money from Germany, as confidence spiraled downward. The Reichsbank lost 150 million marks in the first week of June, 540 million in the second, and 150 million in two days, June 19–20. Collapse was at hand. U.S. President Herbert Hoover called for a moratorium on Payment of war reparations. This angered Paris, which depended on a steady flow of German payments, but it slowed the crisis down and the moratorium, was agreed to in July 1931. International conference in London later in July produced no agreements but on August 19 a standstill agreement froze Germany's foreign liabilities for six months. Germany received emergency funding from private banks in New York as well as the Bank of International Settlements and the Bank of England. The funding only slowed the process; it's nothing. Industrial failures began in Germany, a major bank closed in July and a two-day holiday for all German banks was declared. Business failures more frequent in July, and spread to Romania and Hungary.[75]		In 1932, 90% of German reparation payments were cancelled. (In the 1950s, Germany repaid all its missed reparations debts.) Widespread unemployment reached 25% as every sector was hurt. The government did not increase government spending to deal with Germany's growing crisis, as they were afraid that a high-spending policy could lead to a return of the hyperinflation that had affected Germany in 1923. Germany's Weimar Republic was hit hard by the depression, as American loans to help rebuild the German economy now stopped.[112] The unemployment rate reached nearly 30% in 1932, bolstering support for the Nazi (NSDAP) and Communist (KPD) parties, causing the collapse of the politically centrist Social Democratic Party. Hitler ran for the Presidency in 1932, and while he lost to the incumbent Hindenburg in the election, it marked a point during which both Nazi Party and the Communist parties rose in the years following the crash to altogether possess a Reichstag majority following the general election in July 1932.[113][114]		Hitler followed an autarky economic policy, creating a network of client states and economic allies in central Europe and Latin America. By cutting wages and taking control of labor unions, plus public works spending, unemployment fell significantly by 1935. Large scale military spending played a major role in the recovery.[115]		The reverberations of the Great Depression hit Greece in 1932. The Bank of Greece tried to adopt deflationary policies to stave off the crises that were going on in other countries, but these largely failed. For a brief period the drachma was pegged to the U.S. dollar, but this was unsustainable given the country's large trade deficit and the only long-term effects of this were Greece's foreign exchange reserves being almost totally wiped out in 1932. Remittances from abroad declined sharply and the value of the drachma began to plummet from 77 drachmas to the dollar in March 1931 to 111 drachmas to the dollar in April, 1931. This was especially harmful to Greece as the country relied on imports from the UK, France and the Middle East for many necessities. Greece went off the gold standard in April, 1932 and declared a moratorium on all interest payments. The country also adopted protectionist policies such as import quotas, which a number of European countries did during the time period.		Protectionist policies coupled with a weak drachma, stifling imports, allowed Greek industry to expand during the Great Depression. In 1939 Greek Industrial output was 179% that of 1928. These industries were for the most part "built on sand" as one report of the Bank of Greece put it, as without massive protection they would not have been able to survive. Despite the global depression, Greece managed to suffer comparatively little, averaging an average growth rate of 3.5% from 1932 to 1939. The dictatorial regime of Ioannis Metaxas took over the Greek government in 1936, and economic growth was strong in the years leading up to the Second World War.		Icelandic post-World War I prosperity came to an end with the outbreak of the Great Depression. The Depression hit Iceland hard as the value of exports plummeted. The total value of Icelandic exports fell from 74 million kronur in 1929 to 48 million in 1932, and was not to rise again to the pre-1930 level until after 1939.[116] Government interference in the economy increased: "Imports were regulated, trade with foreign currency was monopolized by state-owned banks, and loan capital was largely distributed by state-regulated funds".[116] Due to the outbreak of the Spanish Civil War, which cut Iceland's exports of saltfish by half, the Depression lasted in Iceland until the outbreak of World War II (when prices for fish exports soared).[116]		Frank Barry and Mary E. Daly have argued that :		The Great Depression hit Italy very hard.[121] As industries came close to failure they were bought out by the banks in a largely illusionary bail-out—the assets used to fund the purchases were largely worthless. This led to a financial crisis peaking in 1932 and major government intervention. The Industrial Reconstruction Institute (IRI) was formed in January 1933 and took control of the bank-owned companies, suddenly giving Italy the largest state-owned industrial sector in Europe (excluding the USSR). IRI did rather well with its new responsibilities—restructuring, modernising and rationalising as much as it could. It was a significant factor in post-1945 development. But it took the Italian economy until 1935 to recover the manufacturing levels of 1930—a position that was only 60% better than that of 1913.[122][123]		The Great Depression did not strongly affect Japan. The Japanese economy shrank by 8% during 1929–31. Japan's Finance Minister Takahashi Korekiyo was the first to implement what have come to be identified as Keynesian economic policies: first, by large fiscal stimulus involving deficit spending; and second, by devaluing the currency. Takahashi used the Bank of Japan to sterilize the deficit spending and minimize resulting inflationary pressures. Econometric studies have identified the fiscal stimulus as especially effective.[124]		The devaluation of the currency had an immediate effect. Japanese textiles began to displace British textiles in export markets. The deficit spending proved to be most profound and went into the purchase of munitions for the armed forces. By 1933, Japan was already out of the depression. By 1934, Takahashi realized that the economy was in danger of overheating, and to avoid inflation, moved to reduce the deficit spending that went towards armaments and munitions.		This resulted in a strong and swift negative reaction from nationalists, especially those in the army, culminating in his assassination in the course of the February 26 Incident. This had a chilling effect on all civilian bureaucrats in the Japanese government. From 1934, the military's dominance of the government continued to grow. Instead of reducing deficit spending, the government introduced price controls and rationing schemes that reduced, but did not eliminate inflation, which remained a problem until the end of World War II.		The deficit spending had a transformative effect on Japan. Japan's industrial production doubled during the 1930s. Further, in 1929 the list of the largest firms in Japan was dominated by light industries, especially textile companies (many of Japan's automakers, such as Toyota, have their roots in the textile industry). By 1940 light industry had been displaced by heavy industry as the largest firms inside the Japanese economy.[125]		Because of high levels of U.S. investment in Latin American economies, they were severely damaged by the Depression. Within the region, Chile, Bolivia and Peru were particularly badly affected.[126]		Before the 1929 crisis, links between the world economy and Latin American economies had been established through American and British investment in Latin American exports to the world. As a result, Latin Americans export industries felt the depression quickly. World prices for commodities such as wheat, coffee and copper plunged. Exports from all of Latin America to the U.S. fell in value from $1.2 billion in 1929 to $335 million in 1933, rising to $660 million in 1940.		But on the other hand, the depression led the area governments to develop new local industries and expand consumption and production. Following the example of the New Deal, governments in the area approved regulations and created or improved welfare institutions that helped millions of new industrial workers to achieve a better standard of living.		From roughly 1931 to 1937, the Netherlands suffered a deep and exceptionally long depression. This depression was partly caused by the after-effects of the Stock Market Crash of 1929 in the U.S., and partly by internal factors in the Netherlands. Government policy, especially the very late dropping of the Gold Standard, played a role in prolonging the depression. The Great Depression in the Netherlands led to some political instability and riots, and can be linked to the rise of the Dutch national-socialist party NSB. The depression in the Netherlands eased off somewhat at the end of 1936, when the government finally dropped the Gold Standard, but real economic stability did not return until after World War II.[127]		New Zealand was especially vulnerable to worldwide depression, as it relied almost totally on agricultural exports to the United Kingdom for its economy. The drop in exports led to a lack of disposable income from the farmers, who were the mainstay of the local economy. Jobs disappeared and wages plummeted, leaving people desperate and charities unable to cope. Work relief schemes were the only government support available to the unemployed, the rate of which by the early 1930s was officially around 15%, but unofficially nearly twice that level (official figures excluded Māori and women). In 1932, riots occurred among the unemployed in three of the country's main cities (Auckland, Dunedin, and Wellington). Many were arrested or injured through the tough official handling of these riots by police and volunteer "special constables".[128]		Already under the rule of a dictatorial junta, the Ditadura Nacional, Portugal suffered no turbulent political effects of the Depression, although António de Oliveira Salazar, already appointed Minister of Finance in 1928 greatly expanded his powers and in 1932 rose to Prime Minister of Portugal to found the Estado Novo, an authoritarian corporatist dictatorship. With the budget balanced in 1929, the effects of the depression were relaxed through harsh measures towards budget balance and autarky, causing social discontent but stability and, eventually, an impressive economic growth.[129]		In the years immediately preceding the depression, negative developments in the island and world economies perpetuated an unsustainable cycle of subsistence for many Puerto Rican workers. The 1920s brought a dramatic drop in Puerto Rico’s two primary exports, raw sugar and coffee, due to a devastating hurricane in 1928 and the plummeting demand from global markets in the latter half of the decade. 1930 unemployment on the island was roughly 36% and by 1933 Puerto Rico’s per capita income dropped 30% (by comparison, unemployment in the United States in 1930 was approximately 8% reaching a height of 25% in 1933).[130] [131] To provide relief and economic reform, the United States government and Puerto Rican politicians such as Carlos Chardon and Luis Munoz Marin created and administered first the Puerto Rico Emergency Relief Administration (PRERA) 1933 and then in 1935, the Puerto Rico Reconstruction Administration (PRRA).[132]		As world trade slumped, demand for South African agricultural and mineral exports fell drastically. The Carnegie Commission on Poor Whites had concluded in 1931 that nearly one third of Afrikaners lived as paupers. The social discomfort caused by the depression was a contributing factor in the 1933 split between the "gesuiwerde" (purified) and "smelter" (fusionist) factions within the National Party and the National Party's subsequent fusion with the South African Party.[133][134]		The Soviet Union was the world's sole communist state with very little international trade. Its economy was not tied to the rest of the world and was only slightly affected by the Great Depression.[135] Its forced transformation from a rural to an industrial society succeeded in building up heavy industry, at the cost of millions of lives in rural Russia and Ukraine.[136]		At the time of the Depression, the Soviet economy was growing steadily, fuelled by intensive investment in heavy industry. The apparent economic success of the Soviet Union at a time when the capitalist world was in crisis led many Western intellectuals to view the Soviet system favorably. Jennifer Burns wrote:		As the Great Depression ground on and unemployment soared, intellectuals began unfavorably comparing their faltering capitalist economy to Russian Communism. ... More than ten years after the Revolution, Communism was finally reaching full flower, according to New York Times reporter Walter Duranty, a Stalin fan who vigorously debunked accounts of the Ukraine famine, a man-made disaster that would leave millions dead."[137]		Despite all of this, The Great Depression caused mass immigration to the Soviet Union, mostly from Finland and Germany. Soviet Russia was at first happy to help these immigrants settle, because they believed they were victims of capitalism who had come to help the Soviet cause. However, when the Soviet Union entered the war in 1941, most of these Germans and Finns were arrested and sent to Siberia, while their Russian-born children were placed in orphanages. Their fate is unknown.[138]		Spain had a relatively isolated economy, with high protective tariffs and was not one of the main countries affected by the Depression. The banking system held up well, as did agriculture.[139]		By far the most serious negative impact came after 1936 from the heavy destruction of infrastructure and manpower by the civil war, 1936–39. Many talented workers were forced into permanent exile. By staying neutral in the Second World War, and selling to both sides, the economy avoided further disasters.[140]		By the 1930s, Sweden had what America's Life magazine called in 1938 the "world's highest standard of living". Sweden was also the first country worldwide to recover completely from the Great Depression. Taking place in the midst of a short-lived government and a less-than-a-decade old Swedish democracy, events such as those surrounding Ivar Kreuger (who eventually committed suicide) remain infamous in Swedish history. Eventually, the Social Democrats under Per Albin Hansson would form their first long-lived government in 1932 based on strong interventionist and welfare state policies, monopolizing the office of Prime Minister until 1976 with the sole and short-lived exception of Axel Pehrsson-Bramstorp's "summer cabinet" in 1936. During forty years of hegemony, it was the most successful political party in the history of Western liberal democracy.[141]		In Thailand, then known as the Kingdom of Siam, the Great Depression contributed to the end of the absolute monarchy of King Rama VII in the Siamese revolution of 1932.		The World Depression broke at a time when the United Kingdom was still far from having recovered from the effects of the First World War more than a decade earlier. The country was driven off the gold standard in 1931.		The world financial crisis began to overwhelm Britain in 1931; investors across the world started withdrawing their gold from London at the rate of £2½ millions a day.[76] Credits of £25 millions each from the Bank of France and the Federal Reserve Bank of New York and an issue of £15 millions fiduciary note slowed, but did not reverse the British crisis. The financial crisis now caused a major political crisis in Britain in August 1931. With deficits mounting, the bankers demanded a balanced budget; the divided cabinet of Prime Minister Ramsay MacDonald's Labour government agreed; it proposed to raise taxes, cut spending and most controversially, to cut unemployment benefits 20%. The attack on welfare was totally unacceptable to the Labour movement. MacDonald wanted to resign, but King George V insisted he remain and form an all-party coalition "National Government". The Conservative and Liberals parties signed on, along with a small cadre of Labour, but the vast majority of Labour leaders denounced MacDonald as a traitor for leading the new government. Britain went off the gold standard, and suffered relatively less than other major countries in the Grade Depression. In the 1931 British election, the Labour Party was virtually destroyed, leaving MacDonald as Prime Minister for a largely Conservative coalition.[142][78]		The effects on the northern industrial areas of Britain were immediate and devastating, as demand for traditional industrial products collapsed. By the end of 1930 unemployment had more than doubled from 1 million to 2.5 million (20% of the insured workforce), and exports had fallen in value by 50%. In 1933, 30% of Glaswegians were unemployed due to the severe decline in heavy industry. In some towns and cities in the north east, unemployment reached as high as 70% as shipbuilding fell 90%.[143] The National Hunger March of September–October 1932 was the largest[144] of a series of hunger marches in Britain in the 1920s and 1930s. About 200,000 unemployed men were sent to the work camps, which continued in operation until 1939.[145]		In the less industrial Midlands and Southern England, the effects were short-lived and the later 1930s were a prosperous time. Growth in modern manufacture of electrical goods and a boom in the motor car industry was helped by a growing southern population and an expanding middle class. Agriculture also saw a boom during this period.[146]		Hoover's first measures to combat the depression were based on voluntarism by businesses not to reduce their workforce or cut wages. But businesses had little choice and wages were reduced, workers were laid off, and investments postponed.[147][148]		In June 1930 Congress approved the Smoot–Hawley Tariff Act which raised tariffs on thousands of imported items. The intent of the Act was to encourage the purchase of American-made products by increasing the cost of imported goods, while raising revenue for the federal government and protecting farmers. Other nations increased tariffs on American-made goods in retaliation, reducing international trade, and worsening the Depression.[149]		In 1931 Hoover urged bankers to set up the National Credit Corporation[150] so that big banks could help failing banks survive. But bankers were reluctant to invest in failing banks, and the National Credit Corporation did almost nothing to address the problem.[151]		By 1932, unemployment had reached 23.6%, peaking in early 1933 at 25%.[153] Drought persisted in the agricultural heartland, businesses and families defaulted on record numbers of loans, and more than 5,000 banks had failed.[154] Hundreds of thousands of Americans found themselves homeless, and began congregating in shanty towns – dubbed "Hoovervilles" – that began to appear across the country.[155] In response, President Hoover and Congress approved the Federal Home Loan Bank Act, to spur new home construction, and reduce foreclosures. The final attempt of the Hoover Administration to stimulate the economy was the passage of the Emergency Relief and Construction Act (ERA) which included funds for public works programs such as dams and the creation of the Reconstruction Finance Corporation (RFC) in 1932. The Reconstruction Finance Corporation was a Federal agency with the authority to lend up to $2 billion to rescue banks and restore confidence in financial institutions. But $2 billion was not enough to save all the banks, and bank runs and bank failures continued.[147] Quarter by quarter the economy went downhill, as prices, profits and employment fell, leading to the political realignment in 1932 that brought to power Franklin Delano Roosevelt. It is important to note, however, that after volunteerism failed, Hoover developed ideas that laid the framework for parts of the New Deal.		Shortly after President Franklin Delano Roosevelt was inaugurated in 1933, drought and erosion combined to cause the Dust Bowl, shifting hundreds of thousands of displaced persons off their farms in the Midwest. From his inauguration onward, Roosevelt argued that restructuring of the economy would be needed to prevent another depression or avoid prolonging the current one. New Deal programs sought to stimulate demand and provide work and relief for the impoverished through increased government spending and the institution of financial reforms.		During a "bank holiday" that lasted five days, the Emergency Banking Act was signed into law. It provided for a system of reopening sound banks under Treasury supervision, with federal loans available if needed. The Securities Act of 1933 comprehensively regulated the securities industry. This was followed by the Securities Exchange Act of 1934 which created the Securities and Exchange Commission. Though amended, key provisions of both Acts are still in force. Federal insurance of bank deposits was provided by the FDIC, and the Glass–Steagall Act.		The Agricultural Adjustment Act provided incentives to cut farm production in order to raise farming prices. The National Recovery Administration (NRA) made a number of sweeping changes to the American economy. It forced businesses to work with government to set price codes through the NRA to fight deflationary "cut-throat competition" by the setting of minimum prices and wages, labor standards, and competitive conditions in all industries. It encouraged unions that would raise wages, to increase the purchasing power of the working class. The NRA was deemed unconstitutional by the Supreme Court of the United States in 1935.		These reforms, together with several other relief and recovery measures, are called the First New Deal. Economic stimulus was attempted through a new alphabet soup of agencies set up in 1933 and 1934 and previously extant agencies such as the Reconstruction Finance Corporation. By 1935, the "Second New Deal" added Social Security (which was later considerably extended through the Fair Deal), a jobs program for the unemployed (the Works Progress Administration, WPA) and, through the National Labor Relations Board, a strong stimulus to the growth of labor unions. In 1929, federal expenditures constituted only 3% of the GDP. The national debt as a proportion of GNP rose under Hoover from 20% to 40%. Roosevelt kept it at 40% until the war began, when it soared to 128%.		By 1936, the main economic indicators had regained the levels of the late 1920s, except for unemployment, which remained high at 11%, although this was considerably lower than the 25% unemployment rate seen in 1933. In the spring of 1937, American industrial production exceeded that of 1929 and remained level until June 1937. In June 1937, the Roosevelt administration cut spending and increased taxation in an attempt to balance the federal budget.[158] The American economy then took a sharp downturn, lasting for 13 months through most of 1938. Industrial production fell almost 30 per cent within a few months and production of durable goods fell even faster. Unemployment jumped from 14.3% in 1937 to 19.0% in 1938, rising from 5 million to more than 12 million in early 1938.[159] Manufacturing output fell by 37% from the 1937 peak and was back to 1934 levels.[160]		Producers reduced their expenditures on durable goods, and inventories declined, but personal income was only 15% lower than it had been at the peak in 1937. As unemployment rose, consumers' expenditures declined, leading to further cutbacks in production. By May 1938 retail sales began to increase, employment improved, and industrial production turned up after June 1938.[161] After the recovery from the Recession of 1937–38, conservatives were able to form a bipartisan conservative coalition to stop further expansion of the New Deal and, when unemployment dropped to 2% in the early 1940s, they abolished WPA, CCC and the PWA relief programs. Social Security remained in place.		Between 1933 and 1939, federal expenditure tripled, and Roosevelt's critics charged that he was turning America into a socialist state.[162] The Great Depression was a main factor in the implementation of social democracy and planned economies in European countries after World War II (see Marshall Plan). Keynesianism generally remained the most influential economic school in the United States and in parts of Europe until the periods between the 1970s and the 1980s, when Milton Friedman and other neoliberal economists formulated and propagated the newly created theories of neoliberalism and incorporated them into the Chicago School of Economics as an alternative approach to the study of economics. Neoliberalism went on to challenge the dominance of the Keynesian school of Economics in the mainstream academia and policy-making in the United States, having reached its peak in popularity in the election of the presidency of Ronald Reagan in the United States, and Margaret Thatcher in the United Kingdom.[163]		The Great Depression has been the subject of much writing, as authors have sought to evaluate an era that caused both financial and emotional trauma. Perhaps the most noteworthy and famous novel written on the subject is The Grapes of Wrath, published in 1939 and written by John Steinbeck, who was awarded both the Nobel Prize for literature and the Pulitzer Prize for the work. The novel focuses on a poor family of sharecroppers who are forced from their home as drought, economic hardship, and changes in the agricultural industry occur during the Great Depression. Steinbeck's Of Mice and Men is another important novella about a journey during the Great Depression. Additionally, Harper Lee's To Kill a Mockingbird is set during the Great Depression. Margaret Atwood's Booker prize-winning The Blind Assassin is likewise set in the Great Depression, centering on a privileged socialite's love affair with a Marxist revolutionary. The era spurred the resurgence of social realism, practiced by many who started their writing careers on relief programs, especially the Federal Writers' Project in the U.S.[165][166][167][168]		A number of works for younger audiences are also set during the Great Depression, among them the Kit Kittredge series of American Girl books written by Valerie Tripp and illustrated by Walter Rane, released to tie in with the dolls and playsets sold by the company. The stories, which take place during the early to mid 1930s in Cincinnati, focuses on the changes brought by the Depression to the titular character's family and how the Kittredges dealt with it.[169] A theatrical adaptation of the series entitled Kit Kittredge: An American Girl was later released in 2008 to positive reviews.[170][171] Similarly, Christmas After All, part of the Dear America series of books for older girls, take place in 1930s Indianapolis; while Kit Kittredge is told in a third-person viewpoint, Christmas After All is in the form of a fictional journal as told by the protagonist Minnie Swift as she recounts her experiences during the era, especially when her family takes in an orphan cousin from Texas.[172]		The term "The Great Depression" is most frequently attributed to British economist Lionel Robbins, whose 1934 book The Great Depression is credited with formalizing the phrase,[173] though Hoover is widely credited with popularizing the term,[173][174] informally referring to the downturn as a depression, with such uses as "Economic depression cannot be cured by legislative action or executive pronouncement" (December 1930, Message to Congress), and "I need not recount to you that the world is passing through a great depression" (1931).		The term "depression" to refer to an economic downturn dates to the 19th century, when it was used by varied Americans and British politicians and economists. Indeed, the first major American economic crisis, the Panic of 1819, was described by then-president James Monroe as "a depression",[173] and the most recent economic crisis, the Depression of 1920–21, had been referred to as a "depression" by then-president Calvin Coolidge.		Financial crises were traditionally referred to as "panics", most recently the major Panic of 1907, and the minor Panic of 1910–11, though the 1929 crisis was called "The Crash", and the term "panic" has since fallen out of use. At the time of the Great Depression, the term "The Great Depression" was already used to referred to the period 1873–96 (in the United Kingdom), or more narrowly 1873–79 (in the United States), which has retroactively been renamed the Long Depression.[175]		Other economic downturns have been called a "great depression", but none had been as widespread, or lasted for so long. Various nations have experienced brief or extended periods of economic downturns, which were referred to as "depressions", but none have had such a widespread global impact.[citation needed]		The collapse of the Soviet Union, and the breakdown of economic ties which followed, led to a severe economic crisis and catastrophic fall in the standards of living in the 1990s in post-Soviet states and the former Eastern Bloc,[176] which was even worse than the Great Depression.[177][178] Even before Russia's financial crisis of 1998, Russia's GDP was half of what it had been in the early 1990s,[178] and some populations are still poorer as of 2009[update] than they were in 1989, including Moldova, Central Asia, and the Caucasus.[citation needed]		Some journalists and economists have taken to calling the late-2000s recession the "Great Recession" in allusion to the Great Depression.[179][180][181][182]		The causes of the Great Recession seem similar to the Great Depression, but significant differences exist. The previous chairman of the Federal Reserve, Ben Bernanke, had extensively studied the Great Depression as part of his doctoral work at MIT, and implemented policies to manipulate the money supply and interest rates in ways that were not done in the 1930s. Bernanke's policies will undoubtedly be analyzed and scrutinized in the years to come, as economists debate the wisdom of his choices. Generally speaking, the recovery of the world's financial systems tended to be quicker during the Great Depression of the 1930s as opposed to the late-2000s recession.		If we contrast the 1930s with the Crash of 2008 where gold went through the roof, it is clear that the U.S. dollar on the gold standard was a completely different animal in comparison to the fiat free-floating U.S. dollar currency we have today. Both currencies in 1929 and 2008 were the U.S. dollar, but in an analogous way it is as if one was a Saber-toothed tiger and the other is a Bengal tiger; they are two completely different animals. Where we have experienced inflation since the Crash of 2008, the situation was much different in the 1930s when deflation set in. Unlike the deflation of the early 1930s, the U.S. economy currently appears to be in a "liquidity trap," or a situation where monetary policy is unable to stimulate an economy back to health. In terms of the stock market, nearly three years after the 1929 crash, the DJIA dropped 8.4% on August 12, 1932. Where we have experienced great volatility with large intraday swings in the past two months, in 2011, we have not experienced any record-shattering daily percentage drops to the tune of the 1930s. Where many of us may have that '30s feeling, in light of the DJIA, the CPI, and the national unemployment rate, we are simply not living in the '30s. Some individuals may feel as if we are living in a depression, but for many others the current global financial crisis simply does not feel like a depression akin to the 1930s.[183]		1928 and 1929 were the times in the 20th century that the wealth gap reached such skewed extremes;[184] half the unemployed had been out of work for over six months, something that was not repeated until the late-2000s recession. 2007 and 2008 eventually saw the world reach new levels of wealth gap inequality that rivalled the years of 1928 and 1929.		General:		
Knowledge workers are workers whose main capital is knowledge. Examples include software engineers, physicians, pharmacists, architects, engineers, scientists, design thinkers, public accountants, lawyers, and academics, and any other white-collar worker, whose line of work requires the one to "think for a living".[1]						Knowledge work can be differentiated from other forms of work by its emphasis on "non-routine" problem solving that requires a combination of convergent, divergent, and creative thinking.[2] But despite the amount of research and literature on knowledge work, there is no succinct definition of the term.[3]		Mosco and McKercher (2007) outline various viewpoints on the matter. They first point to the most narrow and defined definition of knowledge work, such as Florida's view of it as specifically, "the direct manipulation of symbols to create an original knowledge product, or to add obvious value to an existing one", which limits the definition of knowledge work to mainly creative work. They then contrast this view of knowledge work with the notably broader view which includes the handling and distribution of information, arguing that workers who play a role in the handling and distribution of information add real value to the field, despite not necessarily contributing a creative element. Thirdly, one might consider a definition of knowledge work which includes, "all workers involved in the chain of producing and distributing knowledge products", which allows for a very broad and inclusive categorization of knowledge workers. It should thus be acknowledged that the term "knowledge worker" can be quite broad in its meaning, and is not always definitive in who it refers to.[4]		Knowledge workers spend 38% of their time searching for information.[5][dubious – discuss] They are also often displaced from their bosses, working in various departments and time zones or from remote sites such as home offices and airport lounges.[6] As businesses increase their dependence on information technology, the number of fields in which knowledge workers must operate has expanded dramatically.[citation needed]		Even though they sometimes are called "gold collars",[7] because of their high salaries, as well as because of their relative independence in controlling the process of their own work,[8] current research shows that they are also more prone to burnout, and very close normative control from organizations they work for, unlike regular workers.[9]		Managing knowledge workers can be a difficult task. Most knowledge workers prefer some level of autonomy, and do not like being overseen or managed. Those who manage knowledge workers are often knowledge workers themselves, or have been in the past. Projects must be carefully considered before assigning to a knowledge worker, as their interest and goals will affect the quality of the completed project. Knowledge workers must be treated as individuals.		Loo ([10] 2017) using empirical findings from knowledge workers of two sectors – advertising and IT software sectors – and from three developed countries – England, Japan and Singapore – investigated a specific type of knowledge workers – the creative knowledge workers - as opposed to the generic ones as indicated above. The findings from the analysed empirical data offer a complex picture of this type of work in the knowledge economy where workers use a combination of creativity, abilities, talents, skills, and knowledge towards the eventual production of products and services. This investigation (Loo, 2017) identified a definition of creative knowledge work from four specific roles of copywriting, creative directing, software programming, and systems programme managing in advertising and IT software. The manner in which each of the creative applications is applied is dependent on the role(s) of the creative workers. This type of work includes a complex combination of skill sets or ‘creative knowledge work (ckw) capacities.’ "Creative knowledge workers use a combination of creative applications to perform their functions/roles in the knowledge economy including anticipatory imagination, problem solving, problem seeking, and generating ideas and aesthetic sensibilities" (Loo, 2017, p. 138).		Taking aesthetic sensibility as an example, for a creative director, it is a visual imagery whether still or moving via a camera lens and for a software programmer, it is the innovative technical expertise in which the software is written.		Other sector-related creative applications include an emotional connection in the advertising sector and the power of expression and sensitivity in the IT software sector. Terms such as ‘general sponge,’ ‘social chameleon,’ and ‘in tune with the zeitgeist’ were identified which the creative knowledge workers used to identify emotionally with their potential audience in ad making. From the IT software perspective, creative knowledge workers used a ‘sensitivity’ creative application to ascertain business intelligence and as a measurement of information, the software worker might obtain from various parties (Loo, 2017).		Creative workers also require abilities and aptitudes. Passion for one's job was generic to the roles investigated in the two sectors and for copywriters, this passion was identified with fun, enjoyment, and happiness in carrying out the role alongside attributes such as honesty (regarding the product), confidence, and patience in finding the appropriate copy. As with the other roles, a creative worker in software programming requires team working and interpersonal skills in order to communicate effectively with those from other disciplinary backgrounds and training. As regards the managerial roles of creative directing and systems programme managing, the abilities to create a vision for the job in hand, to convince, strategize, execute, and plan towards the eventual completion of the given task (such as a campaign or a software) are necessary capacities (Loo, 2017).		Linking these abilities and capacities are collaborative ways of working, which the findings from this study have identified. The two modes of working ranged from individual to collaborative where a worker might be doing either or both depending on the specific activity. The abilities to traverse between these two work modes alongside the relevant creative application are part of the complexity of this style of working.		Creative workers also require an understanding of various forms of knowledge (Loo, 2017). These are related to disciplines such as those from the humanities (e.g., literature), and the creative arts such as painting and music (e.g., popular and classical varieties). Creative knowledge workers also require technical-related knowledge such as mathematics and computer sciences (e.g., software engineering) and physical sciences (e.g., physics) though there are distinctions in the two sectors. In the IT software sector, technical knowledge of software languages is especially significant for programmers as ascertained in the findings. However, the degree of technical expertise may be less for a programme manager, as only knowledge of the relevant software language is necessary to understand the issues for communicating with the team of developers and testers. The technical know-how for a creative director relates only to the understanding of the possibilities of technologies (such as graphics and typography) in order to capitalise on the technical wizardry. The technical specialists are then required to execute the creative director's vision.		The above types of disciplinary knowledge may appear in explicit formats, which can be learnt from formal programmes at teaching institutions such as higher education and professional institutions alongside other skills and abilities relating to presentation, communication, and team working. As ascertained in the findings, there was other non-disciplinary knowledge, which was not explicit but tacit in nature. Interviewees mentioned tacit experiences from their past work and life experiences, which they used to draw upon in performing their creative knowledge work. This form of knowledge was harnessed collectively as a team (of an advertising campaign or a software programme). This collaborative approach to working, especially with roles such as creative directing and software programme managing, requires tacit knowledge of the strengths and weaknesses and the needs and wants of the related team members (knowledge of psychology). This form of working may occur within the organisation, as a stand-alone group for a specific project in the organisation, or as a sub-contracted team outside the organisation. Within this role, creative knowledge workers may perform their activities individually and/or collectively as part of their contribution to the project. The findings also brought out some characteristics of collaborative working such as the varieties of stakeholders such as sub-contracted groups, and the indirect relationships between clients, workers (of an ad agency), and consumers (Loo, 2017).		The term was first coined by Peter Drucker (1957).[11] He suggested "the most valuable asset of a 21st-century institution, whether business or non-business, will be its knowledge workers and their productivity."[12]		Paul Alfred Weiss (1960)[13] said that "knowledge grows like organisms, with data serving as food to be assimilated rather than merely stored". Popper (1963)[full citation needed] stated there is always an increasing need for knowledge to grow and progress continually, whether tacit (Polanyi, 1976)[full citation needed] or explicit.		Toffler (1990)[full citation needed] observed that typical knowledge workers (especially R&D scientists and engineers) in the age of knowledge economy must have some system at their disposal to create, process and enhance their own knowledge. In some cases they would also need to manage the knowledge of their co-workers.		Nonaka (1991)[full citation needed] described knowledge as the fuel for innovation, but was concerned that many managers failed to understand how knowledge could be leveraged. Companies are more like living organisms than machines, he argued, and most viewed knowledge as a static input to the corporate machine. Nonaka advocated a view of knowledge as renewable and changing, and that knowledge workers were the agents for that change. Knowledge-creating companies, he believed, should be focused primarily on the task of innovation.		This laid the foundation for the new practice of knowledge management, or "KM", which evolved in the 1990s to support knowledge workers with standard tools and processes.		Savage (1995) describes a knowledge-focus as the third wave of human socio-economic development. The first wave was the Agricultural Age with wealth defined as ownership of land. In the second wave, the Industrial Age, wealth was based on ownership of Capital, i.e. factories. In the Knowledge Age, wealth is based upon the ownership of knowledge and the ability to use that knowledge to create or improve goods and services. Product improvements include cost, durability, suitability, timeliness of delivery, and security. Using data,[citation needed] in the Knowledge Age, 2% of the working population will work on the land, 10% will work in Industry and the rest will be knowledge workers.[14]		Davenport (2005) says that the rise of knowledge work has actually been foreseen for years.[1]:4 He points to the fact that Fritz Machlup did a lot of the early work on both knowledge as well as knowledge work roles and as early as 1958 stated that the sector was growing much faster than the rest of the economy with knowledge workers making up almost a third of the workforce in the United States.[1]:4 "According to the Organization for Economic Co-operation and Development (1981), by the beginning of the 1970s around 40 percent of the working population in the USA and Canada were classified to the information sector, whereas in most other OECD countries the figures were still considerably lower."[3]:118		Tapscott (2006) sees a strong, on-going linkage between knowledge workers and innovation, but the pace and manner of interaction have become more advanced. He describes social media tools on the internet that now drive more powerful forms of collaboration. Knowledge workers engage in ‘’peer-to-peer’’ knowledge sharing across organizational and company boundaries, forming networks of expertise. Some of these are open to the public. While he echoes concern over copyright and intellectual property law being challenged in the marketplace, he feels strongly that businesses must engage in collaboration to survive. He sees on-going alliance of public (government) and private (commercial) teams to solve problems, referencing the open source Linux operating system along with the Human Genome Project as examples where knowledge is being freely exchanged, with commercial value being realized.		Palmer (2014) [15] researched knowledge worker productivity and work patterns. Part of this research has involved the analysis of how an average knowledge worker spends their day. He notes that effective and efficient knowledge work relies on the smooth navigation of unstructured processes and the elaboration of custom and one-off procedures. "As we move to the 21st century business model, the focus must be on equipping knowledge workers with tools and infrastructure that enable communication and information sharing, such as networking, e-mail, content management and increasingly, social media." Palmer points to the emergence of Adaptive Case Management (also known as Dynamic or Advanced case management) representing the paradigm shift triggered by the appearance from adapting business practices to the design of IT systems, to building systems that reflect how work is actually performed.		Due to the rapid global expansion of information-based transactions and interactions being conducted via the Internet, there has been an ever-increasing demand for a workforce that is capable of performing these activities. Knowledge Workers are now estimated to outnumber all other workers in North America by at least a four to one margin.[16]:4		While knowledge worker roles overlap heavily with professions that require college degrees, the comprehensive nature of knowledge work in today's connected workplace requires virtually all workers to obtain these skills at some level. To that end, the public education and community college systems have become increasingly focused on lifelong learning to ensure students receive skills necessary to be productive knowledge workers in the 21st century.		Many of the knowledge workers currently entering the workforce are from the generation X demographic. These new knowledge workers value lifelong learning over lifelong employment.[17] "They seek employability over employment [and] value career over self-reliance" (Elsdon and Iyer, 1999)[full citation needed]. Where baby boomers are proficient in specified knowledge regarding a specific firm, generation X knowledge workers acquire knowledge from many firms and take that knowledge with them from company to company (2002).[17]		Knowledge workers bring benefits to organizations in a variety of ways. These include:		These knowledge worker contributions are in contrast with activities that they would typically not be asked to perform, including:		There is a set of transitional tasks which include roles that are seemingly routine, but that require deeper technology, product, or customer knowledge to fulfill the function. These include:		Generally, if the knowledge can be retained, knowledge worker contributions will serve to expand the knowledge assets of a company. While it can be difficult to measure, this increases the overall value of its intellectual capital. In cases where the knowledge assets have commercial or monetary value, companies may create patents around their assets, at which point the material becomes restricted intellectual property. In these knowledge-intensive situations, knowledge workers play a direct, vital role in increasing the financial value of a company. They can do this by finding solutions on how they can find new ways to make profits. This can also be related with market and research. Davenport (2005) says that even if knowledge workers are not a majority of all workers, they do have the most influence on their economies.[1] He adds that companies with a high volume of knowledge workers are the most successful and fastest growing in leading economies including the United States.		Reinhardt et al.'s (2011) review of current literature shows that the roles of knowledge workers across the workforce are incredibly diverse. In two empirical studies they have "proposed a new way of classifying the roles of knowledge workers and the knowledge actions they perform during their daily work."[2]:150 The typology of knowledge worker roles suggested by them are "controller, helper, learner, linker, networker, organizer, retriever, sharer, solver, and tracker":[2]:160		Drucker (1966) defines six factors for knowledge worker productivity:[19]		The theory of Human Interaction Management asserts that there are 5 principles characterizing effective knowledge work:		Another, more recent breakdown of knowledge work (author unknown) shows activity that ranges from tasks performed by individual knowledge workers to global social networks. This framework spans every class of knowledge work that is being or is likely to be undertaken. There are seven levels or scales of knowledge work, with references for each are cited.		The hierarchy ranges from the effort of individual specialists, through technical activity, professional projects, and management programs, to organizational strategy, knowledge markets, and global-scale networking.		This framework is useful for positioning the myriad types of knowledge work relative to each other and within the context of organizations, markets, and the global knowledge economy. It also provides a useful context for planning, developing, and implementing knowledge management projects.		Loo (2017) investigates how a particular group - creative knowledge workers – carries out their jobs and learns within it. Using empirical data from advertising and software development in England, Japan and Singapore, it develops a new conceptual framework to analyse the complexities of creative knowledge work. The framework draws from four disciplines of business and management, economics, sociology and psychology (Loo, 2017, p. 59). Focusing uniquely on the human element of working in the knowledge economy, Loo explores the real world of how people work in this emerging phenomenon and examines the relationships between knowledge and creative dimensions to provide new frameworks for learning and working. This research identified three levels of creative knowledge applications. They relate to intra-sectoral approaches, inter-sectoral approaches (where jobs require different styles of work depending on the sectors), and changes in culture/practices in the sectors. With the intra-sectoral work, they refer to the roles and functions of specific jobs in each of the two sectors of advertising (e.g. copywriting and creative directing) and software development (e.g. software developing and software programme managing). With the inter-sectoral work, it may include software programme managers having different functions when working in different organisations – e.g. a computer software company and a multinational financial organisation. With the last type of creative working, it may include aspects such as the culture of ‘good practice’ in technical problem-solving and the ‘power of expression’ in software programming. All the three types of micro-level of creative knowledge work offer a highly contextualized understanding of how these workers operate in the knowledge economy. This approach is different from that taken by Zuboff (1988), Drucker (1993), Nonaka and Takeuchi (1995) and Reich (2001) who sought to provide a more generic understanding (Loo, 2017).		Finally, complex creative knowledge work needs a supportive environment. One such environment relates to the supporting technical base. Based on the findings, information, communications and electronic technologies (ICET) are viewed as an organisational tool, a source of ideas (such as the Internet), and a way of modelling a concept. It may also be applied to inter-sectoral activities such as software for cross-disciplinary applications. This organisational tool enables creative knowledge workers to devote their energies to multi-faceted activities such as analysis of huge data sets and the enabling of new jobs such as webpage designing. ICET enables workers to spend more time on advanced activities, which leads to the intensification of creative applications. Lastly, it was noted from the findings that a supportive environment focused on training, work environment, and education (Loo, 2017 Loo, S. (2017) Creative Working in the Knowledge Economy. Abingdon: Routledge).		
Accounting scandals are business scandals which arise from intentional manipulation of financial statements with the disclosure of financial misdeeds by trusted executives of corporations or governments. Such misdeeds typically involve complex methods for misusing or misdirecting funds, overstating revenues, understating expenses, overstating[1] the value of corporate assets or underreporting the existence of liabilities. It involves an employee, account or corporation itself and is misleading to investor and shareholders.[2]		This type of "creative accounting" can amount to fraud, and investigations are typically launched by government oversight agencies, such as the Securities and Exchange Commission (SEC) in the United States. Employees who commit accounting fraud at the request of their employers are subject to personal criminal prosecution.[3]						Misappropriation of assets, often called defalcation or employee fraud, occurs when an employee steals company's asset, whether those assets are of monetary or physical nature. Typically, assets stolen are cash or cash equivalents and company data or intellectual property.[4] However, misappropriation of assets also includes taking inventory out of a facility or using company assets for personal purpose without authorization. Company assets include everything from office supplies, inventory to intellectual property.[5]		Fraudulent financial reporting, also known as earnings management fraud. In this context, management intentionally manipulates accounting policies or accounting estimates to improve financial statements. Public and private corporations commit fraudulent financial reporting to secure investor interest or obtain bank approvals for financing, as justifications for bonuses or increased salaries or to meet expectations of shareholders.[6] The Securities and Exchange Commission has brought enforcement actions against corporations for many types of fraudulent financial reporting, including improper revenue recognition, period-end stuffing, fraudulent post-closing entries, improper asset valuations, and misleading non-GAAP financial measures.[7]		The fraud triangle is a model for explaining the factors that cause someone to commit fraudulent behaviors in accounting. It consists of three components, which together, lead to fraudulent behavior:		Incentives/ Pressures: A common incentive for companies to manipulate financial statement is a decline in the company's financial prospects. Companies may also manipulate earnings to meet analysts' forecasts or benchmarks such as prior year earnings, to meet debt covenant restrictions, to achieve a bonus target based on earnings, or to artificially inflate stock prices. As for misappropriation of assets, financial pressures are a common incentive for employees. Employees with excessive financial obligations, or those with drug abuse or gambling problems may steal to meet their personal needs.[9]		Opportunities: Although the financial statements of all companies are potentially subject to manipulation, the risk is greater for companies in industries where significant judgments and accounting estimates are involved. Turnover in accounting personnel or other deficiencies in accounting and information processes can create an opportunity for misstatement. As for misappropriation of assets, opportunities are greater in companies with accessible cash or with inventory or other valuable assets, especially if the assets are small or easily removed. A lack of controls over payments to vendors or payroll systems, can allow employees to create fictitious vendors or employees and bill the company for services or time.[10]		Attitudes/ Rationalization: The attitude of top management toward financial reporting is a critical risk factor in assessing the likelihood of fraudulent financial statements. If the CEO or other top managers display a significant disregard for the financial reporting process, such as consistently issuing overly optimistic forecasts, or they are overly concerned about the meeting analysts' earnings forecast, fraudulent financial reporting is more likely. Similarly, for misappropriation of assets, if management cheats customers through overcharging for goods or engaging in high-pressure sales tactics, employees may feel that it is acceptable for them to behave in the same fashion.[11]		A weak internal control is an opportunity for fraudster.[12] Lack of transparency in financial transactions is an ideal method to hide a fraud. Poor management information where a company's management system does not produce results that are timely, accurate, sufficiently detailed and relevant. In such case, the warning signal of fraud such as ongoing theft from bank account can be obscured. Lack of an independent audit department within the company is also a sign of weak internal control.[13] Poor accounting practice is also part of a weak internal control. An example of poor accounting practice is failure to make monthly reconciliation of bank account.[14]		Top executive can reduce the price of his/her company's stock easily due to information asymmetry. The executive can accelerate accounting of expected expenses, delay accounting of expected revenue, engage in off balance sheet transactions to make the company's profitability appear temporarily poorer, or simply promote and report severely conservative (e.g. pessimistic) estimates of future earnings. Such seemingly adverse earnings news will be likely to (at least temporarily) reduce share price. (This is again due to information asymmetries since it is more common for top executives to do everything they can to window dress their company's earnings forecasts.		Top managers tend to share price to make a company an easier takeover target. When the company gets bought out (or taken private) – at a dramatically lower price – the takeover artist gains a windfall from the former top executive's actions to surreptitiously reduce share price. This can represent tens of billions of dollars (questionably) transferred from previous shareholders to the takeover artist. The former top executive is then rewarded with a golden handshake for presiding over the firesale that can sometimes be in the hundreds of millions of dollars for one or two years of work.[15]		Similar issues occur when a publicly held asset or non-profit organization undergoes privatization. Top executives often reap tremendous monetary benefits when a government-owned or non-profit entity is sold to private hands. Just as in the example above, they can facilitate this process by making the entity appear to be in financial crisis – this reduces the sale price (to the profit of the purchaser), and makes non-profits and governments more likely to sell. It can also contribute to a public perception that private entities are more efficiently run, thereby reinforcing the political will to sell off public assets. Again, due to asymmetric information, policy makers and the general public see a government-owned firm that was a financial 'disaster' – miraculously turned around by the private sector (and typically resold) within a few years.		Not all accounting scandals are caused by top executives. Often managers and employees are pressured or willingly alter financial statements for the personal benefit of the individuals over the company. Managerial opportunism plays a large role in these scandals. For example, managers who would be compensated more for short-term results would report inaccurate information, since short-term benefits outweigh the long-term ones such as pension obligations.[16]		The Enron scandal turned in the indictment and criminal conviction of one of the Big Five auditor Arthur Andersen on June 15, 2002. Although the conviction was overturned on May 31, 2005, by the Supreme Court of the United States, the firm ceased performing audits and is currently unwinding its business operations. The Enron scandal was defined as being one of the biggest audit failures. The scandal included utilizing loopholes that were found within the GAAP (General Accepted Accounting Principles). For auditing a big sized company such as Enron, the auditors were criticized for having brief meetings a few times a year that covered large amounts of material. By January 17, 2002, Enron decided to discontinue its business with Arthur Andersen claiming they had failed in accounting advice and related documents. Arthur Andersen was judged guilty of obstruction of justice for getting rid of many emails and documents that were related to auditing Enron. Since the SEC is not allowed to accept audits from convicted felons, the firm was forced to give up its CPA licenses later in 2002, costing over 113,000 employees their jobs. Although later the ruling was overturned by the U.S. Supreme Court, the once-proud firm's image was tarnished beyond repair, and it has not returned as a viable business even on a limited scale.		On July 9, 2002 George W. Bush gave a speech about recent accounting scandals that had been uncovered. In spite of its stern tone, the speech did not focus on establishing new policy, but instead focused on actually enforcing current laws, which include holding CEOs and directors personally responsible for accountancy fraud.		In July 2002, WorldCom filed for bankruptcy protection, in what was considered the largest corporate insolvency ever at the time.		These scandals reignited the debate over the relative merits of US GAAP, which takes a "rules-based" approach to accounting, versus International Accounting Standards and UK GAAP, which takes a "principles-based" approach. The Financial Accounting Standards Board announced that it intends to introduce more principles-based standards. More radical means of accounting reform have been proposed, but so far have very little support. The debate itself, however, overlooks the difficulties of classifying any system of knowledge, including accounting, as rules-based or principles-based. This also led to the establishment of Sarbanes-Oxley.		On a lighter note, the 2002 Ig Nobel Prize in Economics went to the CEOs of those companies involved in the corporate accounting scandals of that year for "adapting the mathematical concept of imaginary numbers for use in the business world".		In 2003, Nortel made a big contribution to this list of scandals by incorrectly reporting a one cent per share earnings directly after their massive layoff period. They used this money to pay the top 43 managers of the company. The SEC and the Ontario securities commission eventually settled civil action with Nortel. However, a separate civil action will be taken up against top Nortel executives including former CEO Frank A. Dunn, Douglas C. Beatty, Michael J. Gollogly and MaryAnne E. Pahapill and Hamilton. These proceedings have been postponed pending criminal proceedings in Canada, which opened in Toronto on January 12, 2012.[80] Crown lawyers at this fraud trial of three former Nortel Networks executives say the men defrauded the shareholders of Nortel of more than $5 million. According to the prosecutor this was accomplished by engineering a financial loss in 2002, and a profit in 2003 thereby triggering Return to Profit bonuses of $70 million for top executives.[81][82][83][84][85]		In 2005, after a scandal on insurance and mutual funds the year before, AIG was investigated for accounting fraud. The company already lost over 45 billion US dollars' worth of market capitalisation because of the scandal. Investigations also discovered over a billion US dollars' worth of errors in accounting transactions. The New York Attorney General's investigation led to a $1.6 billion fine for AIG and criminal charges for some of its executives.[86] CEO Maurice R. "Hank" Greenberg was forced to step down and is still fighting civil charges being pursued by New York state.[87][88]		Well before Bernard Madoff's massive Ponzi scheme came to light, observers doubted whether his listed accounting firm—an unknown two-person firm in a rural area north of New York City—was competent to service a multimillion-dollar operation, especially since it had only one active accountant.[89] Ultimately, Madoff's accountant, David G. Friehling, admitted to simply rubber-stamping 18 years' worth of Madoff's filings with the SEC. He also revealed that he continued to audit Madoff even though he had invested a substantial amount of money with him. Accountants aren't allowed to audit broker-dealers with whom they're investing. He agreed to forfeit $3.18 million in accounting fees and withdrawals from his account with Madoff. His involvement makes the Madoff scheme the largest accounting fraud in world history.[90]		
Job satisfaction or employee satisfaction has been defined in many different ways. Some believe it is simply how content an individual is with his or her job, in other words, whether or not they like the job or individual aspects or facets of jobs, such as nature of work or supervision.[1] Others believe it is not as simplistic as this definition suggests and instead that multidimensional psychological responses to one's job are involved.[2] Researchers have also noted that job satisfaction measures vary in the extent to which they measure feelings about the job (affective job satisfaction).[3] or cognitions about the job (cognitive job satisfaction).[4]						The concept of job satisfaction has been developed in many ways by many different researchers and practitioners. One of the most widely used definitions in organizational research is that of Locke (1976), who defines job satisfaction as "a pleasurable or positive emotional state resulting from the appraisal of one's job or job experiences" (p. 1304).[5] Others have defined it as simply how content an individual is with his or her job; whether he or she likes the job or not.[6] It is assessed at both the global level (whether or not the individual is satisfied with the job overall), or at the facet level (whether or not the individual is satisfied with different aspects of the job).[1] Spector (1997)[1] lists 14 common facets: Appreciation, Communication, Coworkers, Fringe benefits, Job conditions, Nature of the work, Organization, Personal growth, Policies and procedures, Promotion opportunities, Recognition, Security, and Supervision.		A more recent definition of the concept of job satisfaction is from Hulin and Judge (2003), who have noted that job satisfaction includes multidimensional psychological responses to an individual's job, and that these personal responses have cognitive (evaluative), affective (or emotional), and behavioral components.[2] Job satisfaction scales vary in the extent to which they assess the affective feelings about the job or the cognitive assessment of the job. Affective job satisfaction is a subjective construct representing an emotional feeling individuals have about their job.[1][3][4][7] Hence, affective job satisfaction for individuals reflects the degree of pleasure or happiness their job in general induces. Cognitive job satisfaction is a more objective and logical evaluation of various facets of a job. Cognitive job satisfaction can be unidimensional if it comprises evaluation of just one facet of a job, such as pay or maternity leave, or multidimensional if two or more facets of a job are simultaneously evaluated. Cognitive job satisfaction does not assess the degree of pleasure or happiness that arises from specific job facets, but rather gauges the extent to which those job facets are judged by the job holder to be satisfactory in comparison with objectives they themselves set or with other jobs. While cognitive job satisfaction might help to bring about affective job satisfaction, the two constructs are distinct, not necessarily directly related, and have different antecedents and consequences.[4]		Job satisfaction can also be seen within the broader context of the range of issues which affect an individual's experience of work, or their quality of working life. Job satisfaction can be understood in terms of its relationships with other key factors, such as general well-being, stress at work, control at work, home-work interface, and working conditions.[8]		A study title "Analysis of Factors Affecting Job Satisfaction of the Employees in Public and Private Sector", in India concluded that in India Employees tend to love their job if they get what they believe is an important attribute of a good job. Weightage factor of each such attribute based on exhaustive survey has been calculated. Region, sector and gender wise study of job satisfaction has provided consistent picture with respect to distribution of data set analyzed showed that most of the employees in Indian industry are not satisfied with their job except for a few like male in commerce sector and female in education sector. Total job satisfaction level of males is found to be higher than that of woman. Total job satisfaction level in manufacturing sector is found to be very low.[9]		The assessment of job satisfaction through employee anonymous surveys became commonplace in the 1930s.[10] Although prior to that time there was the beginning of interest in employee attitudes, there were only a handful of studies published.[11] Latham and Budworth[10] note that Uhrbrock[12] in 1934 was one of the first psychologists to use the newly developed attitude measurement techniques to assess factory worker attitudes. They also note that in 1935 Hoppock[13] conducted a study that focused explicitly on job satisfaction that is affected by both the nature of the job and relationships with coworkers and supervisors.		Edwin A. Locke’s Range of Affect Theory (1976) is arguably the most famous job satisfaction model. The main premise of this theory is that satisfaction is determined by a discrepancy between what one wants in a job and what one has in a job. Further, the theory states that how much one values a given facet of work (e.g. the degree of autonomy in a position) moderates how satisfied/dissatisfied one becomes when expectations are/aren’t met. When a person values a particular facet of a job, his satisfaction is more greatly impacted both positively (when expectations are met) and negatively (when expectations are not met), compared to one who doesn’t value that facet. To illustrate, if Employee A values autonomy in the workplace and Employee B is indifferent about autonomy, then Employee A would be more satisfied in a position that offers a high degree of autonomy and less satisfied in a position with little or no autonomy compared to Employee B. This theory also states that too much of a particular facet will produce stronger feelings of dissatisfaction the more a worker values that facet.		The dispositional approach suggests that individuals vary in their tendency to be satisfied with their jobs, in other words, job satisfaction is to some extent an individual trait.[14] This approach became a notable explanation of job satisfaction in light of evidence that job satisfaction tends to be stable over time and across careers and jobs.[15] Research also indicates that identical twins raised apart have similar levels of job satisfaction.[16]		A significant model that narrowed the scope of the dispositional approach was the Core Self-evaluations Model, proposed by Timothy A. Judge, Edwin A. Locke, and Cathy C. Durham in 1997.[17] Judge et al. argued that there are four Core Self-evaluations that determine one’s disposition towards job satisfaction: self-esteem, general self-efficacy, locus of control, and neuroticism. This model states that higher levels of self-esteem (the value one places on his/her self) and general self-efficacy (the belief in one’s own competence) lead to higher work satisfaction. Having an internal locus of control (believing one has control over her\his own life, as opposed to outside forces having control) leads to higher job satisfaction. Finally, lower levels of neuroticism lead to higher job satisfaction.[17]		Equity Theory shows how a person views fairness in regard to social relationships such as with an employer. A person identifies the amount of input (things gained) from a relationship compared to the output (things given) to produce an input/output ratio. They then compare this ratio to the ratio of other people in deciding whether or not they have an equitable relationship.[18][19] Equity Theory suggests that if an individual thinks there is an inequality between two social groups or individuals, the person is likely to be distressed because the ratio between the input and the output are not equal.[20]		For example, consider two employees who work the same job and receive the same pay and benefits. If one individual gets a pay raise for doing the same work as the other, then the less benefited individual will become distressed in his workplace. If, on the other hand, both individuals get pay raises and new responsibilities, then the feeling of equity will be maintained.[20]		Other psychologists have extended the equity theory, suggesting three behavioral response patterns to situations of perceived equity or inequity (Huseman, Hatfield, & Mile, 1987; O'Neil & Mone 1998). These three types are benevolent, equity sensitive, and entitled. The level by each type affects motivation, job satisfaction, and job performance.		The concept of discrepancy theory is to explain the ultimate source of anxiety and dejection.[22] An individual who has not fulfilled his responsibility feels the sense of anxiety and regret for not performing well. They will also feel dejection due to not being able to achieve their hopes and aspirations. According to this theory, all individuals will learn what their obligations and responsibilities are for a particular function, and if they fail to fulfill those obligations then they are punished. Over time, these duties and obligations consolidate to form an abstracted set of principles, designated as a self-guide.[23] Agitation and anxiety are the main responses when an individual fails to achieve the obligation or responsibility.[24] This theory also explains that if achievement of the obligations is obtained then the reward can be praise, approval, or love. These achievements and aspirations also form an abstracted set of principles, referred to as the ideal self guide.[23] When the individual fails to obtain these rewards, they begin to have feelings of dejection, disappointment, or even depression.[24]		Frederick Herzberg’s two-factor theory (also known as motivator-hygiene theory) attempts to explain satisfaction and motivation in the workplace.[25] This theory states that satisfaction and dissatisfaction are driven by different factors – motivation and hygiene factors, respectively. An employee’s motivation to work is continually related to job satisfaction of a subordinate. Motivation can be seen as an inner force that drives individuals to attain personal and organizational goals (Hoskinson, Porter, & Wrench, p. 133). Motivating factors are those aspects of the job that make people want to perform, and provide people with satisfaction, for example achievement in work, recognition, promotion opportunities.[26] These motivating factors are considered to be intrinsic to the job, or the work carried out.[25] Hygiene factors include aspects of the working environment such as pay, company policies, supervisory practices, and other working conditions.[25]		While Herzberg's model has stimulated much research, researchers have been unable to reliably empirically prove the model, with Hackman & Oldham suggesting that Herzberg's original formulation of the model may have been a methodological artifact.[25] Furthermore, the theory does not consider individual differences, conversely predicting all employees will react in an identical manner to changes in motivating/hygiene factors.[25] Finally, the model has been criticised in that it does not specify how motivating/hygiene factors are to be measured.[25]		Hackman & Oldham proposed the job characteristics model, which is widely used as a framework to study how particular job characteristics impact job outcomes, including job satisfaction. The five core job characteristics can be combined to form a motivating potential score (MPS) for a job, which can be used as an index of how likely a job is to affect an employee's attitudes and behaviors. Not everyone is equally affected by the MPS of a job. People who are high in growth need strength (the desire for autonomy, challenge and development of new skills on the job) are particularly affected by job characteristics.[27] A meta-analysis of studies that assess the framework of the model provides some support for the validity of the JCM.[28]		One of the most important aspects of an individual’s work in a modern organization concerns the management of communication demands that he or she encounters on the job.[29] Demands can be characterized as a communication load, which refers to “the rate and complexity of communication inputs an individual must process in a particular time frame.”[30] Individuals in an organization can experience communication over-load and communication under- load which can affect their level of job satisfaction. Communication overload can occur when “an individual receives too many messages in a short period of time which can result in unprocessed information or when an individual faces more complex messages that are more difficult to process.[30]” Due to this process, “given an individual’s style of work and motivation to complete a task, when more inputs exist than outputs, the individual perceives a condition of overload[29] which can be positively or negatively related to job satisfaction. In comparison, communication under load can occur when messages or inputs are sent below the individual’s ability to process them.”[30] According to the ideas of communication over-load and under-load, if an individual does not receive enough input on the job or is unsuccessful in processing these inputs, the individual is more likely to become dissatisfied, aggravated, and unhappy with their work which leads to a low level of job satisfaction.		Superior-subordinate communication is an important influence on job satisfaction in the workplace. The way in which subordinates perceive a supervisor's behavior can positively or negatively influence job satisfaction. Communication behavior such as facial expression, eye contact, vocal expression, and body movement is crucial to the superior-subordinate relationship (Teven, p. 156). Nonverbal messages play a central role in interpersonal interactions with respect to impression formation, deception, attraction, social influence, and emotional.[31] Nonverbal immediacy from the supervisor helps to increase interpersonal involvement with their subordinates impacting job satisfaction. The manner in which supervisors communicate with their subordinates non-verbally may be more important than the verbal content (Teven, p. 156). Individuals who dislike and think negatively about their supervisor are less willing to communicate or have motivation to work whereas individuals who like and think positively of their supervisor are more likely to communicate and are satisfied with their job and work environment. A supervisor who uses nonverbal immediacy, friendliness, and open communication lines is more likely to receive positive feedback and high job satisfaction from a subordinate. Conversely, a supervisor who is antisocial, unfriendly, and unwilling to communicate will naturally receive negative feedback and create low job satisfaction in their subordinates in the workplace.		A Watson Wyatt Worldwide study identified a positive outcome between a collegical and flexible work environment and an increase in shareholder value. Suggesting that employee satisfaction is directly related to financial gain. Over 40 percent of the companies listed in the top 100 of Fortune magazine’s, “America’s Best Companies to Work For” also appear on the Fortune 500. It is possible that successful workers enjoy working at successful companies, however, the Watson Wyatt Worldwide Human Capital Index study claims that effective human resources practices, such as employee recognition programs, lead to positive financial outcomes more often than positive financial outcomes lead to good practices.[32]		Employee recognition is not only about gifts and points. It's about changing the corporate culture in order to meet goals and initiatives and most importantly to connect employees to the company's core values and beliefs. Strategic employee recognition is seen as the most important program not only to improve employee retention and motivation but also to positively influence the financial situation.[33] The difference between the traditional approach (gifts and points) and strategic recognition is the ability to serve as a serious business influencer that can advance a company’s strategic objectives in a measurable way. "The vast majority of companies want to be innovative, coming up with new products, business models and better ways of doing things. However, innovation is not so easy to achieve. A CEO cannot just order it, and so it will be. You have to carefully manage an organization so that, over time, innovations will emerge."[34]		Mood and emotions at work are related to job satisfaction. Moods tend to be longer lasting but often weaker states of uncertain origin, while emotions are often more intense, short-lived and have a clear object or cause.[35]		Some research suggests moods are related to overall job satisfaction.[36][37] Positive and negative emotions were also found to be significantly related to overall job satisfaction.[38]		Frequency of experiencing net positive emotion will be a better predictor of overall job satisfaction than will intensity of positive emotion when it is experienced.[38]		Emotion work (or emotion management) refers to various types of efforts to manage emotional states and displays. Emotion management includes all of the conscious and unconscious efforts to increase, maintain, or decrease one or more components of an emotion. Although early studies of the consequences of emotional work emphasized its harmful effects on workers, studies of workers in a variety of occupations suggest that the consequences of emotional work are not uniformly negative.[39]		It was found that suppression of unpleasant emotions decreases job satisfaction and the amplification of pleasant emotions increases job satisfaction.[40]		The understanding of how emotion regulation relates to job satisfaction concerns two models:		It has been well documented that genetics influence a variety of individual differences.[45] Some research suggests genetics also play a role in the intrinsic, direct experiences of job satisfaction like challenge or achievement (as opposed to extrinsic, environmental factors like working conditions). One experiment used sets of monozygotic twins, reared apart, to test for the existence of genetic influence on job satisfaction. While the results indicate the majority of the variance in job satisfaction was due to environmental factors (70%), genetic influence is still a minor factor. Genetic heritability was also suggested for several of the job characteristics measured in the experiment, such as complexity level, motor skill requirements, and physical demands.[46]		Some research suggests an association between personality and job satisfaction.[47] Specifically, this research describes the role of negative affectivity and positive affectivity. Negative affectivity is related strongly to the personality trait of neuroticism. Individuals high in negative affectivity are more prone to experience less job satisfaction. Positive affectivity is related strongly to the personality trait of extraversion. Those high in positive affectivity are more prone to be satisfied in most dimensions of their life, including their job. Differences in affectivity likely impact how individuals will perceive objective job circumstances like pay and working conditions, thus affecting their satisfaction in that job.[48]		There are two personality factors related to job satisfaction, alienation and locus of control. Employees who have an internal locus of control and feel less alienated are more likely to experience job satisfaction, job involvement and organizational commitment. A meta-analysis of 187 studies of job satisfaction concluded that high satisfaction was positively associated with internal locus of control. The study also showed characteristics like high machiavellianism, narcissism, trait anger, type A personality dimensions of achievement striving and impatience/irritability, are also related to job satisfaction.[49]		Psychological well-being (PWB) is defined as “the overall effectiveness of an individual’s psychological functioning” as related to primary facets of one’s life: work, family, community, etc.[50] There are three defining characteristics of PWB. First, it is a phenomenological event, meaning that people are happy when they subjectively believe themselves to be so. Second, well-being involves some emotional conditions. Particularly, psychologically well people are more prone to experience positive emotions and less prone to experience negative emotions. Third, well-being refers to one's life as a whole. It is a global evaluation.[50] PWB is primarily measured using the eight-item Index of Psychological Well-Being developed by Berkman (IPWB). IPWB asks respondents to reply to a series a questions on how often they felt “pleased about accomplishing something,” “bored,” “depressed or unhappy,” etc.[50]		PWB in the workplace plays an important role in determining job satisfaction and has attracted much research attention in recent years.[51] These studies have focused on the effects of PWB on job satisfaction as well as job performance.[52] One study noted that because job satisfaction is specific to one’s job, the research that examined job satisfaction had not taken into account aspects of one’s life external to the job.[53] Prior studies had focused only on the work environment as the main determinant of job satisfaction. Ultimately, to better understand job satisfaction (and its close relative, job performance), it is important to take into account an individual’s PWB. Research published in 2000 showed a significant correlation between PWB and job satisfaction (r = .35, p < .01).[50] A follow-up study by the same authors in 2007 revealed similar results (r = .30, p < .01).[53] In addition, these studies show that PWB is a better predictor of job performance than job satisfaction alone.		The majority of job satisfaction measures are self-reports and based on multi-item scales. Several measures have been developed over the years, although they vary in terms of how carefully and distinctively they are conceptualized with respect to affective or cognitive job satisfaction. They also vary in terms of the extent and rigour of their psychometric validation.		The Brief Index of Affective Job Satisfaction (BIAJS) is a 4-item, overtly affective as opposed to cognitive, measure of overall affective job satisfaction. The BIAJS differs from other job satisfaction measures in being comprehensively validated not just for internal consistency reliability, temporal stability, convergent and criterion-related validities, but also for cross-population invariance by nationality, job level, and job type. Reported internal consistency reliabilities range between .81 and .87.[3]		The Job Descriptive Index (JDI),[54] is a specifically cognitive job satisfaction measure. It measures one’s satisfaction in five facets: pay, promotions and promotion opportunities, coworkers, supervision, and the work itself. The scale is simple, participants answer either yes, no, or can’t decide (indicated by ‘?’) in response to whether given statements accurately describe one’s job.		Other job satisfaction questionnaires include: the Minnesota Satisfaction Questionnaire (MSQ), the Job Satisfaction Survey (JSS), and the Faces Scale.[1] The MSQ measures job satisfaction in 20 facets and has a long form with 100 questions (five items from each facet) and a short form with 20 questions (one item from each facet). The JSS is a 36 item questionnaire that measures nine facets of job satisfaction. Finally, the Faces Scale of job satisfaction, one of the first scales used widely, measured overall job satisfaction with just one item which participants respond to by choosing a face.		Job satisfaction can be indicative of work behaviors such as organizational citizenship,[55] and withdrawal behaviors such as absenteeism,[56] and turnover.[57] Further, job satisfaction can partially mediate the relationship of personality variables and deviant work behaviors.[58]		One common research finding is that job satisfaction is correlated with life satisfaction.[59] This correlation is reciprocal, meaning people who are satisfied with life tend to be satisfied with their job and people who are satisfied with their job tend to be satisfied with life. In fact, a 2016 FlexJobs survey revealed 97% of respondents believe a job that offered flexibility would positively impact their lives, 87% think it would help lower stress and 79% think the flexibility would help them live healthier.[60] Additionally, a second survey of 650 working parents revealed that flexible work arrangements can positively affect people’s personal health, as well as improve their romantic relationships and 99% of respondents believe a flexible job would make them a happier person in general.[61] However, some research has found that job satisfaction is not significantly related to life satisfaction when other variables such as nonwork satisfaction and core self-evaluations are taken into account.[62]		An important finding for organizations to note is that job satisfaction has a rather tenuous correlation to productivity on the job. This is a vital piece of information to researchers and businesses, as the idea that satisfaction and job performance are directly related to one another is often cited in the media and in some non-academic management literature. A recent meta-analysis found surprisingly low correlations between job satisfaction and performance.[63] Further, the meta-analysis found that the relationship between satisfaction and performance can be moderated by job complexity, such that for high-complexity jobs the correlation between satisfaction and performance is higher than for jobs of low to moderate complexity. Additionally, one longitudinal study indicated that among work attitudes, job satisfaction is a strong predictor of absenteeism, suggesting that increasing job satisfaction and organizational commitment are potentially good strategies for reducing absenteeism and turnover intentions.[64] Recent research has also shown that intention to quit alone can have negative effects on performance, organizational deviance, and organizational citizenship behaviours.[65] In short, the relationship of satisfaction to productivity is not as straightforward as often assumed and can be influenced by a number of different work-related constructs, and the notion that "a happy worker is a productive worker" should not be the foundation of organizational decision-making. For example, employee personality may even be more important than job satisfaction in regards to performance.[66]		Numerous studies have been done to show the correlation of job satisfaction and absenteeism.[67] For example, Goldberg and Waldman looked at absenteeism in two dimensions as total time lost (number of missed days) and the frequency of time lost. Self-reported data and records-based data were collected and compared. Following absenteeism measures were evaluated according to absenteeism predictors.		Only three categories of predictors had a significant relationship ratio and were taken in account further:		This research results revealed that absenteeism cannot be predicted by job satisfaction, although other studies have found significant relationships.		
Retirement is the point where a person stops employment completely.[1][2] A person may also semi-retire by reducing work hours.		An increasing number of individuals are choosing to put off this point of total retirement, by selecting to exist in the emerging state of Pre-tirement.[3]		Many people choose to retire when they are eligible for private or public pension benefits, although some are forced to retire when physical conditions no longer allow the person to work any longer (by illness or accident) or as a result of legislation concerning their position.[4] In most countries, the idea of retirement is of recent origin, being introduced during the late 19th and early 20th centuries. Previously, low life expectancy and the absence of pension arrangements meant that most workers continued to work until death. Germany was the first country to introduce retirement, in 1889.[5]		Nowadays, most developed countries have systems to provide pensions on retirement in old age, which may be sponsored by employers and/or the state. In many poorer countries, support for the old is still mainly provided through the family. Today, retirement with a pension is considered a right of the worker in many societies, and hard ideological, social, cultural and political battles have been fought over whether this is a right. In many western countries this right is mentioned in national constitutions.						Retirement, or the practice of leaving one's job or ceasing to work after reaching a certain age, has been around since around the 18th century. Prior to the 18th century, the average life expectancy of people was between 26 and 40 years.[6][7][8][9][10][11] Due to this, only a small percentage of the population were reaching an age where physical impairments began to be obstacles to working. Retirement as a government policy began to be adopted by countries during the late 19th century and the 20th century, beginning in Germany under Otto Von Bismarck.[12]		A person may retire at whatever age they please. However, a country's tax laws and/or state old-age pension rules usually mean that in a given country a certain age is thought of as the "standard" retirement age.		The "standard" retirement age varies from country to country but it is generally between 50 and 70 (according to latest statistics, 2011). In some countries this age is different for males and females, although this has recently been challenged in some countries (e.g., Austria), and in some countries the ages are being brought into line.[13] The table below shows the variation in eligibility ages for public old-age benefits in the United States and many European countries, according to the OECD.		Notes: Parentheses indicate eligibility age for women when different. Sources: Cols. 1–2: OECD Pensions at a Glance (2005), Cols. 3–6: Tabulations from HRS, ELSA and SHARE. Square brackets indicate early retirement for some public employees.		* In France, the retirement age has been extended to 62 and 67 respectively, over the next eight years.[15]		** In Spain, the retirement age will be extended to 63 and 67 respectively, this increase will be progressively done from 2013 to 2027 at a rate of 1 month during the first 6 years and 2 months during the other 9.[16]		In the United States, while the normal retirement age for Social Security, or Old Age Survivors Insurance (OASI), historically has been age 65 to receive unreduced benefits, it is gradually increasing to age 67. For those turning 65 in 2008, full benefits will be payable beginning at age 66.[17] Public servants are often not covered by Social Security but have their own pension programs. Police officers in the United States are typically allowed to retire at half pay after only 20 years of service or three-quarter pay after 30 years, allowing people to retire in their early forties or fifties.[18] Military members of the US Armed Forces may elect to retire after 20 years of active duty. Their retirement pay (not a pension since they can be involuntarily called back to active duty at any time) is calculated on total number of years on active duty, their final pay grade and the retirement system in place when they entered service. Allowances such as housing and subsistence are not used to calculate a member's retired pay. Members awarded the Medal of Honor qualify for a separate stipend, regardless of the years of service. Military members in the reserve and US National Guard have their retirement based on a point system.[citation needed]		Recent advances in data collection have vastly improved our ability to understand important relationships between retirement and factors such as health, wealth, employment characteristics and family dynamics, among others. The most prominent study for examining retirement behavior in the United States is the ongoing Health and Retirement Study (HRS), first fielded in 1992. The HRS is a nationally representative longitudinal survey of adults in the U.S. ages 51+, conducted every two years, and contains a wealth of information on such topics as labor force participation (e.g., current employment, job history, retirement plans, industry/occupation, pensions, disability), health (e.g., health status and history, health and life insurance, cognition), financial variables (e.g., assets and income, housing, net worth, wills, consumption and savings), family characteristics (e.g., family structure, transfers, parent/child/grandchild/sibling information) and a host of other topics (e.g., expectations, expenses, internet use, risk taking, psychosocial, time use).[19]		2002 and 2004 saw the introductions of the English Longitudinal Study of Ageing (ELSA) and the Survey of Health, Ageing and Retirement in Europe (SHARE), which includes respondents from 14 continental European countries plus Israel. These surveys were closely modeled after the HRS in sample frame, design and content. A number of other countries (e.g., Japan, South Korea) also now field HRS-like surveys, and others (e.g., China, India) are currently fielding pilot studies. These data sets have expanded the ability of researchers to examine questions about retirement behavior by adding a cross-national perspective.		Notes: MHAS discontinued in 2003; ELSA numbers exclude institutionalized (nursing homes). Source: Borsch-Supan et al., eds. (November 2008). Health, Ageing and Retirement in Europe (2004–2007): Starting the Longitudinal Dimension.		Many factors affect people's retirement decisions. Retirement funding education is a big factor that affects the success of an individual’s retirement experience. Social Security clearly plays an important role because most individuals solely rely on Social Security as their only retirement option, when Social Security’s both trust funds are expected to be depleted by 2034.[20] Knowledge affects an individual’s retirement decisions by simply finding more reliable retirement options such as, Individual Retirement Accounts or Employer-Sponsored Plans. In countries around the world, people are much more likely to retire at the early and normal retirement ages of the public pension system (e.g., ages 62 and 65 in the U.S.).[21] This pattern cannot be explained by different financial incentives to retire at these ages since typically retirement benefits at these ages are approximately actuarially fair; that is, the present value of lifetime pension benefits (pension wealth) conditional on retiring at age a is approximately the same as pension wealth conditional on retiring one year later at age a+1.[22] Nevertheless, a large literature has found that individuals respond significantly to financial incentives relating to retirement (e.g., to discontinuities stemming from the Social Security earnings test or the tax system).[23][24][25]		Greater wealth tends to lead to earlier retirement, since wealthier individuals can essentially "purchase" additional leisure. Generally the effect of wealth on retirement is difficult to estimate empirically since observing greater wealth at older ages may be the result of increased saving over the working life in anticipation of earlier retirement. However, a number of economists have found creative ways to estimate wealth effects on retirement and typically find that they are small. For example, one paper exploits the receipt of an inheritance to measure the effect of wealth shocks on retirement using data from the HRS.[26] The authors find that receiving an inheritance increases the probability of retiring earlier than expected by 4.4 percentage points, or 12 percent relative to the baseline retirement rate, over an eight-year period.		A great deal of attention has surrounded how the financial crisis (2007 - ?) is affecting retirement decisions, with the conventional wisdom saying that fewer people will retire since their savings have been depleted; however recent research suggests that the opposite may happen. Using data from the HRS, researchers examined trends in defined benefit (DB) vs. defined contribution (DC) pension plans and found that those nearing retirement had only limited exposure to the recent stock market decline and thus are not likely to substantially delay their retirement.[27] At the same time, using data from the Current Population Survey (CPS), another study estimates that mass layoffs are likely to lead to an increase in retirement almost 50% larger than the decrease brought about by the stock market crash, so that on net retirements are likely to increase in response to the crisis.[28]		More information tells of how many who retire will continue to work, but not in the career they have had for the majority of their life. Job openings will increase in the next 5 years due to retirements of the baby boomer generation. The Over 50 population is actually the fastest growing labor groups in the US.		A great deal of research has examined the effects of health status and health shocks on retirement. It is widely found that individuals in poor health generally retire earlier than those in better health. This does not necessarily imply that poor health status leads people to retire earlier, since in surveys retirees may be more likely to exaggerate their poor health status to justify their earlier decision to retire. This justification bias, however, is likely to be small.[29] In general, declining health over time, as well as the onset of new health conditions, have been found to be positively related to earlier retirement.[30] Health conditions that can cause someone to retire include hypertension, diabetes mellitus, sleep apnea, joint diseases, and hyperlipidemia.[31]		Most people are married when they reach retirement age; thus, spouse's employment status may affect one's decision to retire. On average, husbands are three years older than their wives in the U.S., and spouses often coordinate their retirement decisions. Thus, men are more likely to retire if their wives are also retired than if they are still in the labor force, and vice versa.[32][33]		Researchers analyzed factors affecting retirement decisions in EU Member States:		Retired workers support themselves either through pensions or savings. In most cases the money is provided by the government, but sometimes granted only by private subscriptions to mutual funds. In this latter case, subscriptions might be compulsory or voluntary. In some countries an additional "bonus" is granted una tantum (once only) in proportion to the years of work and the average wages; this is usually provided by the employer.		The financial weight of provision of pensions on a government's budget is often heavy and is the reason for political debates about the retirement age. The state might be interested in a later retirement age for economic reasons.		The cost of health care in retirement is large because people tend to be ill more frequently in later life. Most countries provide universal health insurance coverage for seniors, although in the United States many people retire before they become eligible for Medicare at age 65. In 2006, Medicare Part D went into effect in the United States, expanding benefits to include prescription drug coverage.		A poll made in Washington said many people were unaware that "medicare doesn't pay for the most common types of long-term care" (Neergaard); 37 percent of Americans who took the survey believe that it does cover it. Medicaid is a federal-state program for the needy and the main source seniors use to pay their long-term care.[48]		Overall, income after retirement can come from state pensions, occupational pensions, private savings and investments (private pension funds, owned housing), donations (e.g., by children), and social benefits.[49] On a personal level, the rising cost of living during retirement is a serious concern to many older adults. Health care costs play an important role.		A useful and straightforward calculation can be done if we assume that interest, after expenses, taxes, and inflation is zero. Assume that in real (after-inflation) terms, your salary never changes during your w years of working life. During your p years of pension, you have a living standard that costs a replacement ratio R times as much as your living standard in your working life. Your working life living standard is your salary less the proportion of salary Z that you need to save. Calculations are per unit salary (e.g., assume salary = 1).		Then after w years work, retirement age accumulated savings = wZ. To pay for pension for p years, necessary savings at retirement = Rp(1-Z)		Equate these: wZ = Rp(1-Z) and solve to give Z = Rp / (w + Rp). For example, if w = 35, p = 30 and R = 0.65 we find that we need to save a proportion Z = 35.78% of our salary.		Retirement calculators generally accumulate a proportion of salary up to retirement age. This shows a straightforward case, which nonetheless could be practically useful for optimistic people hoping to work for only as long as they are likely to be retired. References relevant to the zero real interest assumption are listed here		For more complicated situations, there are several online retirement calculators on the Internet. Many retirement calculators project how much an investor needs to save, and for how long, to provide a certain level of retirement expenditures. Some retirement calculators, appropriate for safe investments, assume a constant, unvarying rate of return. Monte Carlo retirement calculators take volatility into account and project the probability that a particular plan of retirement savings, investments, and expenditures will outlast the retiree. Retirement calculators vary in the extent to which they take taxes, social security, pensions, and other sources of retirement income and expenditures into account.		The assumptions keyed into a retirement calculator are critical. One of the most important assumptions is the assumed rate of real (after inflation) investment return. A conservative return estimate could be based on the real yield of Inflation-indexed bonds offered by some governments, including the United States, Canada, and the United Kingdom. The TIP$TER retirement calculator projects the retirement expenditures that a portfolio of inflation-linked bonds, coupled with other income sources like Social Security, would be able to sustain. Current real yields on United States Treasury Inflation Protected Securities (TIPS) are available at the US Treasury site. Current real yields on Canadian 'Real Return Bonds' are available at the Bank of Canada's site. As of December 2011, US Treasury inflation-linked bonds (TIPS) were yielding about 0.8% real per annum for the 30-year maturity and a noteworthy slightly negative real return for the 7-year maturity.		Many individuals use "retirement calculators" on the Internet to determine the proportion of their pay they should be saving in a tax advantaged-plan (e.g., IRA or 401-K in the US, RRSP in Canada, personal pension in the UK, superannuation in Australia). After expenses and any taxes, a reasonable (though arguably pessimistic) long-term assumption for a safe real rate of return is zero. So in real terms, interest does not help the savings grow. Each year of work must pay its share of a year of retirement. For someone planning to work for 40 years and be retired for 20 years, each year of work pays for itself and for half a year of retirement. Hence, 33.33% of pay must be saved, and 66.67% can be spent when earned. After 40 years of saving 33.33% of pay, we have accumulated assets of 13.33 years of pay, as in the graph. In the graph to the right, the lines are straight, which is appropriate given the assumption of a zero real investment return.		The graph above can be compared with those generated by many retirement calculators. However, most retirement calculators use nominal (not "real" dollars) and therefore require a projection of both the expected inflation rate and the expected nominal rate of return. One way to work around this limitation is to, for example, enter "0% return, 0% inflation" inputs into the calculator. The Bloomberg retirement calculator gives the flexibility to specify, for example, zero inflation and zero investment return and to reproduce the graph above. The MSN retirement calculator in 2011 has as the defaults a realistic 3% per annum inflation rate and optimistic 8% return assumptions; consistency with the December 2011 US nominal bond and inflation-protected bond market rates requires a change to about 3% inflation and 4% investment return before and after retirement.		Ignoring tax, someone wishing to work for a year and then relax for a year on the same living standard needs to save 50% of pay. Similarly, someone wishing to work from age 25 to 55 and be retired for 30 years till 85 needs to save 50% of pay if government and employment pensions are not a factor and if it is considered appropriate to assume a zero real investment return. The problem that the lifespan is not known in advance can be reduced in some countries by the purchase at retirement of an inflation-indexed life annuity.		For most people, employer pensions, government pensions, and the tax situation in their country are important factors, typically taken account of in calculations by actuaries. Ignoring those significant nation-specific factors but not necessarily assuming zero real interest rates, a "not to be relied upon" calculation of required personal savings rate zprop can be made using a little mathematics.[50] It helps to have a dimly remembered acquaintance with geometric series, maybe in the form		You work for w years, saving a proportion zprop of pay at the end of each year. So the after-savings purchasing power is (1-zprop) of pay while you are working. You need a pension for p years. Let's say that at retirement you are earning S per year and require to replace a ratio Rrepl of your pre-retirement living standard. So you need a pension of (1 – zprop ) Rrepl S, indexed to price inflation.		Let's assume that the investments, after price inflation fprice, earn a real rate ireal in real terms where		(1+ ireal ) = ((1+inominal))/((1+fprice ) ) (Ret-01)		Let's assume that the investments, after wage inflation fpay, earn a real rate i rel to pay where		(1+ i rel to pay ) = ((1+inominal))/((1+fpay ) ) (Ret-02)		To pay for your pension, assumed for simplicity to be received at the end of each year, and taking discounted values in the manner of a net present value calculation, you need a lump sum available at retirement of:		(1 – zprop ) R repl S {(1+ ireal ) −1+(1+ ireal ) −2 +… ….+ (1+ ireal ) −p} = (1-zprop ) R repl S {(1 – (1+ireal)−p )/ireal}		Above we have used the standard mathematical formula for the sum of a geometric series. (Or if ireal =0 then the series in braces sums to p since it then has p equal terms). As an example, assume that S=60,000 per year and that it is desired to replace Rrepl=0.80, or 80%, of pre-retirement living standard for p=30 years. Assume for current purposes that a proportion z prop=0.25 (25%) of pay was being saved. Using ireal=0.02, or 2% per year real return on investments, the necessary lump sum is given by the formula as (1-0.25)*0.80*60,000*annuity-series-sum(30)=36,000*22.396=806,272 in the nation's currency in 2008–2010 terms. To allow for inflation in a straightforward way, it is best to talk of the 806,272 as being '13.43 years of retirement age salary'. It may be appropriate to regard this as being the necessary lump sum to fund 36,000 of annual supplements to any employer or government pensions that are available. It is common to not include any house value in the calculation of this necessary lump sum, so for a homeowner the lump sum pays primarily for non-housing living costs.		Will you have saved enough at retirement? Use our necessary but unrealistic assumption of a constant after-pay-rises rate of interest. At retirement you have accumulated		zprop S {(1+ i rel to pay )w-1+(1+ i rel to pay )w-2 +… ….+ (1+ i rel to pay )+ 1 }		 = zprop S ((1+i rel to pay)w- 1)/i rel to pay		To make the accumulation match with the lump sum needed to pay your pension:		zprop S (((1+i rel to pay )) w - 1)/i rel to pay = (1-zprop ) R repl S (1 – ((1+i real)) −p )/i real		Bring zprop to the left hand side to give our answer, under this rough and unguaranteed method, for the proportion of pay that we should be saving:		zprop = R repl (1 – ((1+i real )) −p )/i real / [(((1+i rel to pay )) w - 1)/i rel to pay + R repl (1 – ((1+i real )) −p )/i real ] (Ret-03)		Note that the special case i rel to pay =0 = i real means that we instead sum the geometric series by noting that we have p or w identical terms and hence z prop = p/(w+p). This corresponds to our graph above with the straight line real-terms accumulation.		The result for the necessary zprop given by (Ret-03) depends critically on the assumptions that you make. As an example, you might assume that price inflation will be 3.5% per year forever and that your pay will increase only at that same rate of 3.5%. If you assume a 4.5% per year nominal rate of interest, then (using 1.045/1.035 in real terms ) your pre-retirement and post-retirement net interest rates will remain the same, irel to pay = 0.966 percent per year and ireal = 0.966 percent per year. These assumptions may be reasonable in view of the market returns available on inflation-indexed bonds, after expenses and any tax. Equation (Ret-03) is readily coded in Excel and with these assumptions gives the required savings rates in the accompanying picture.		Finally, a newer method for determining the adequacy of a retirement plan is Monte Carlo simulation. This method has been gaining popularity and is now employed by many financial planners.[51] Monte Carlo retirement calculators[52][53] allow users to enter savings, income and expense information and run simulations of retirement scenarios. The simulation results show the probability that the retirement plan will be successful.		Early retirement can be at any age, but is generally before the age (or tenure) needed for eligibility for support and funds from government or employer-provided sources. Thus, early-retirees rely on their own savings and investments to be initially self-supporting, until they start receiving such external support. Early retirement is also a euphemistic term for accepting termination of employment before retirement age as part of the employer's labor force rationalization. In this case, a monetary inducement may be involved.[citation needed]		While conventional wisdom has it that one can retire and take 7% or more out of a portfolio year after year, this would not have worked very often in the past.[54][55] When making periodic inflation-adjusted withdrawals from retirement savings,[56] can make meaningless many assumptions that are based on long term average investment returns.		The chart at the right shows the year-to-year portfolio balances after taking $35,000 (and adjusting for inflation) from a $750,000 portfolio every year for 30 years, starting in 1973 (red line), 1974 (blue line), or 1975 (green line).[57] While the overall market conditions and inflation affected all three about the same (since all three experienced exactly the same conditions between 1975 and 2003), the chance of making the funds last for 30 years depended heavily on what happened to the stock market in the first few years.		Those contemplating early retirement will want to know if they have enough to survive possible bear markets such as the one that would cause the hypothetical 1973 retiree's fund to be exhausted after only 20 years.		The history of the US stock market shows that one would need to live on about 4% of the initial portfolio per year to ensure that the portfolio is not depleted before the end of the retirement;[58] this rule of thumb is a summary of one conclusion of the Trinity study, though the report is more nuanced and the conclusions and very approach have been heavily criticized (see Trinity study for details). This allows for increasing the withdrawals with inflation to maintain a consistent spending ability throughout the retirement, and to continue making withdrawals even in dramatic and prolonged bear markets.[59] (The 4% figure does not assume any pension or change in spending levels throughout the retirement.)		When retiring prior to age  59 1⁄2, there is a 10% IRS penalty on withdrawals from a retirement plan such as a 401(k) plan or a Traditional IRA. Exceptions apply under certain circumstances. At age 59 and six months, the penalty-free status is achieved and the 10% IRS penalty no longer applies.		To avoid the 10% penalty prior to age  59 1⁄2, a person should consult a lawyer about the use of IRS rule 72 T. This rule must be applied for with the IRS. It allows the distribution of an IRA account prior to age  59 1⁄2 in equal amounts of a period of either 5 years or until the age of  59 1⁄2, whichever is the longest time period, without a 10% penalty. Taxes still must be paid on the distributions.		Although the 4% initial portfolio withdrawal rate described above can be used as a rough gauge, it is often desirable to use a retirement planning tool that accepts detailed input and can render a result that has more precision. Some of these tools model only the retirement phase of the plan while others can model both the savings or accumulation phase as well as the retirement phase of the plan. For example, an analysis by Forbes reckoned that in 90% of historical markets, a 4% rate would have lasted for at least 30 years, while in 50% of the historical markets, a 4% rate would have been sustained for more than 40 years.[60]		The effects of making inflation-adjusted withdrawals from a given starting portfolio can be modeled with a downloadable spreadsheet[61] that uses historical stock market data to estimate likely portfolio returns. Another approach is to employ a retirement calculator[62] that also uses historical stock market modeling, but adds provisions for incorporating pensions, other retirement income, and changes in spending that may occur during the course of the retirement.		Retirement might coincide with important life changes; a retired worker might move to a new location, for example a retirement community, thereby having less frequent contact with their previous social context and adopting a new lifestyle. Often retirees volunteer for charities and other community organizations. Tourism is a common marker of retirement and for some becomes a way of life, such as for so-called grey nomads. Some retired people even choose to go and live in warmer climates in what is known as retirement migration.		It has been found that Americans have six lifestyle choices as they age: continuing to work full-time, continuing to work part-time, retiring from work and becoming engaged in a variety of leisure activities, retiring from work and becoming involved in a variety of recreational and leisure activities, retiring from work and later returning to work part-time, and retiring from work and later returning to work full-time.[63] An important note to make from these lifestyle definitions are that four of the six involve working. America is facing an important demographic change in that the Baby Boomer generation is now reaching retirement age. This poses two challenges: whether there will be a sufficient number of skilled workers in the work force, and whether the current pension programs will be sufficient to support the growing number of retired people.[64] The reasons that some people choose to never retire, or to return to work after retiring include not only the difficulty of planning for retirement but also wages and fringe benefits, expenditure of physical and mental energy, production of goods and services, social interaction, and social status may interact to influence an individual’s work force participation decision.[63]		Often retirees are called upon to care for grandchildren and occasionally aged parents. For many it gives them more time to devote to a hobby or sport such as golf or sailing. On the other hand, many retirees feel restless and suffer from depression as a result of their new situation. Although it is not scientifically possible to directly show that retirement either causes or contributes to depression, the newly retired are one of the most vulnerable societal groups when it comes to depression most likely due to confluence of increasing age and deteriorating health status.[65] Retirement coincides with deterioration of one's health that correlates with increasing age and this likely plays a major role in increased rates of depression in retirees. Longitudinal and cross-sectional studies have shown that healthy elderly and retired people are as happy or happier and have an equal quality of life as they age as compared to younger employed adults, therefore retirement in and of itself is not likely to contribute to development of depression.		Many people in the later years of their lives, due to failing health, require assistance, sometimes in extremely expensive treatments – in some countries – being provided in a nursing home. Those who need care, but are not in need of constant assistance, may choose to live in a retirement home.		
A person is a being, such as a human, that has certain capacities or attributes such as reason, morality, consciousness or self-consciousness, and being a part of a culturally established form of social relations such as kinship, ownership of property, or legal responsibility.[1][2][3][4] The defining features of personhood and consequently what makes a person count as a person differ widely among cultures and contexts.		In addition to the question of personhood, of what makes a being count as a person to begin with, there are further questions about personal identity and self: both about what makes any particular person that particular person instead of another, and about what makes a person at one time the same person as they were or will be at another time despite any intervening changes.		The common plural of "person", "people", is often used to refer to an entire nation or ethnic group (as in "a people"). The plural "persons" is often used in philosophical and legal writing.						In ancient Rome, the word persona (Latin) or prosopon (πρόσωπον; Greek) originally referred to the masks worn by actors on stage. The various masks represented the various "personae" in the stage play.[5]		The concept of person was further developed during the Trinitarian and Christological debates of the 4th and 5th centuries in contrast to the word nature.[6] During the theological debates, some philosophical tools (concepts) were needed so that the debates could be held on common basis to all theological schools. The purpose of the debate was to establish the relation, similarities and differences between the Λóγος/Verbum and God. The philosophical concept of person arose, taking the word "prosopon" (πρόσωπον) from the Greek theatre. Therefore, Christus (the Λóγος/Verbum) and God were defined as different "persons". This concept was applied later to the Holy Ghost, the angels and to all human beings.		Since then, a number of important changes to the word's meaning and use have taken place, and attempts have been made to redefine the word with varying degrees of adoption and influence.		The criteria for being a person... are designed to capture those attributes which are the subject of our most humane concern with ourselves and the source of what we regard as most important and most problematical in our lives.		Personhood is the status of being a person. Defining personhood is a controversial topic in philosophy and law, and is closely tied to legal and political concepts of citizenship, equality, and liberty. According to law[specify], only a natural person or legal personality has rights, protections, privileges, responsibilities, and legal liability. Personhood continues to be a topic of international debate, and has been questioned during the abolition of slavery and the fight for women's rights, in debates about abortion, fetal rights, and in animal rights advocacy.[7]		Various debates have focused on questions about the personhood of different classes of entities. Historically, the personhood of animals, women, and slaves has been a catalyst of social upheaval. In most societies today, living adult humans are usually considered persons, but depending on the context, theory or definition, the category of "person" may be taken to include or not children or such non-human entities as animals, artificial intelligences, or extraterrestrial life, as well as legal entities such as corporations, sovereign states and other polities, or estates in probate.[8]		The category may exclude some human entities in prenatal development, and those with extreme mental impairment.		Personal identity is the unique identity of persons through time. That is to say, the necessary and sufficient conditions under which a person at one time and a person at another time can be said to be the same person, persisting through time. In the modern philosophy of mind, this concept of personal identity is sometimes referred to as the diachronic problem of personal identity. The synchronic problem is grounded in the question of what features or traits characterize a given person at one time.		Identity is an issue for both continental philosophy[citation needed] and analytic philosophy.[citation needed] A key question in continental philosophy is in what sense we can maintain the modern conception of identity, while realizing many of our prior assumptions about the world are incorrect.[citation needed]		Proposed solutions to the problem of personal identity include continuity of the physical body, continuity of an immaterial mind or soul, continuity of consciousness or memory,[9] the bundle theory of self,[citation needed] continuity of personality after the death of the physical body,[10] and proposals that there are actually no persons or selves who persist over time at all.[citation needed]		
A green-collar worker is a worker who is employed in the environmental sectors of the economy.[1] Environmental green-collar workers (or green jobs) satisfy the demand for green development. Generally, they implement environmentally conscious design, policy, and technology to improve conservation and sustainability. Formal environmental regulations as well as informal social expectations are pushing many firms to seek professionals with expertise with environmental, energy efficiency, and clean renewable energy issues. They often seek to make their output more sustainable, and thus more favorable to public opinion, governmental regulation, and the Earth's ecology.		Green collar workers include professionals such as conservation movement workers, environmental consultants, council environmental services/waste management/recycling managers/officers, environmental or biological systems engineers, green building architects, landscape architects, holistic passive solar building designers, solar energy and wind energy engineers and installers, nuclear engineers,[2][3][4][5][6][7] green vehicle engineers, "green business" owners,[8] green vehicle, organic farmers, environmental lawyers, ecology educators, and ecotechnology workers, and sales staff working with these services or products. Green collar workers also include vocational or trade-level workers: electricians who install solar panels, plumbers who install solar water heaters, recycling centre/MRF attendants, process managers and collectors, construction workers who build energy-efficient green buildings and wind power farms, construction workers who weatherize buildings to make them more energy efficient, or other workers involved in clean, renewable, sustainable future energy development.		There is a growing movement to incorporate social responsibility within the green industries. A sustainable green economy simultaneously values the importance of natural resources and inclusive, equitable, and healthy opportunities for all communities.[9]		In the context of the current world economic crisis, many experts now argue that a massive push to develop renewable sources of energy could create millions of new jobs and help the economy recover while simultaneously improving the environment, increasing labour conditions in poor economies, and strengthening energy and food security.[citation needed]						Al Gore states that economists across the spectrum — including Martin Feldstein and Lawrence Summers — agree that large and rapid investments in a jobs-intensive infrastructure initiative is the best way to revive the economy in a quick and sustainable way.[17]		A report from the Center for American Progress concludes that a $100 billion federal investment in clean energy technologies over 2009 and 2010 would yield 2 million new U.S. jobs, cutting the unemployment rate by 1.3% and put the nation on a path toward a low-carbon economy. The report, prepared by the Political Economy Research Institute at the University of Massachusetts Amherst, proposes $50 billion in tax credits for energy efficiency retrofits and renewable energy systems; $46 billion in direct government spending for public building retrofits, mass transit, freight rail, smart electrical grid systems, and renewable energy systems; and $4 billion for federal loan guarantees to help finance building retrofits and renewable energy projects. The Center believes that clean energy investments would yield about 300,000 more jobs than if the same funds were distributed among U.S. taxpayers. The clean energy investments would also have the added benefits of lower home energy bills and reduced prices for non-renewable energy sources, due to the reduced consumption of those energy sources.[18]		Global efforts to tackle climate change could result in millions of "green" jobs over the coming decades, according to a 2008 study prepared by the Worldwatch Institute with funding from the United Nations Environment Programme (UNEP). The study found that the global market for environmental products and services is projected to double from $1.37 trillion per year at present to $2.74 trillion by 2020, with half of that market in efficient energy use. In terms of energy supply, the renewable energy industry will be particularly important. Some 2.3 million people have found renewable energy jobs in recent years, and projected investments of $630 billion by 2030 would translate into at least 20 million additional jobs.[19]		Also in 2008, the U.S. Conference of Mayors released a report that finds the U.S. economy currently generates more than 750,000 green jobs, while over the next 30 years, an emphasis on clean energy could result in a five-fold increase, to more than 4.2 million jobs. Engineering, legal, research, and consulting jobs currently dominate the green jobs in the United States and could grow by 1.4 million by 2038, while renewable electricity production will create 1.23 million jobs, alternative transportation fuels will add 1.5 million jobs, and building retrofits will create another 81,000 jobs. The report notes that most of today's jobs are in metropolitan areas, led by New York City; Washington, D.C.; Houston, Texas; and Los Angeles, California.[20]		The Swedish utility Vattenfall did a study of full life cycle emissions of Nuclear, Hydro, Coal, Gas, Solar Cell, Peat and Wind which the utility uses to produce electricity. The net result of the study was that nuclear power produced 3.3 grams of carbon dioxide per KW-Hr of produced power. This compares to 400 for natural gas and 700 for coal (according to this study). The study also concluded that nuclear power produced the smallest amount of CO2 of any of their electricity sources.[21]		Claims exist that the problems of nuclear waste do not come anywhere close to approaching the problems of fossil fuel waste.[22][23] A 2004 article from the BBC states: "The World Health Organization (WHO) says 3 million people are killed worldwide by outdoor air pollution annually from vehicles and industrial emissions, and 1.6 million indoors through using solid fuel."[24] In the U.S. alone, fossil fuel waste kills 20,000 people each year.[25] A coal power plant releases 100 times as much radiation as a nuclear power plant of the same wattage.[26] It is estimated that during 1982, US coal burning released 155 times as much radioactivity into the atmosphere as the Three Mile Island incident.[27] In addition, fossil fuel waste causes global warming, which leads to increased deaths from hurricanes, flooding, and other weather events. The World Nuclear Association provides a comparison of deaths due to accidents among different forms of energy production. In their comparison, deaths per TW-yr of electricity produced from 1970 to 1992 are quoted as 885 for hydropower, 342 for coal, 85 for natural gas, and 8 for nuclear.[28]		"Green collar" is used in the Metal Gear franchise to refer to members of the arms industry, mercenaries, and other individuals in the private sector involved in war and military activity, notably for profit.[29]		
In economics, a depression is a sustained, long-term downturn in economic activity in one or more economies. It is a more severe economic downturn than a recession, which is a slowdown in economic activity over the course of a normal business cycle.		A depression is an unusual and extreme form of recession. Depressions are characterized by their length, by abnormally large increases in unemployment, falls in the availability of credit (often due to some form of banking or financial crisis), shrinking output as buyers dry up and suppliers cut back on production and investment, large number of bankruptcies including sovereign debt defaults, significantly reduced amounts of trade and commerce (especially international trade), as well as highly volatile relative currency value fluctuations (often due to currency devaluations). Price deflation, financial crises and bank failures are also common elements of a depression that do not normally occur during a recession.						In the United States the National Bureau of Economic Research determines contractions and expansions in the business cycle, but does not declare depressions.[1] Generally, periods labeled depressions are marked by a substantial and sustained shortfall of the ability to purchase goods relative to the amount that could be produced using current resources and technology (potential output).[2] Another proposed definition of depression includes two general rules:[3][4]		There are also differences in the duration of depression across definitions. Some economists refer only to the period when economic activity is declining. The more common use, however, also encompasses the time until economic activity has returned close to normal levels.[1]		A recession is briefly defined as a period of declining economic activity spread across the economy (according to NBER). Under the first definition, each depression will always coincide with a recession, since the difference between a depression and a recession is the severity of the fall in economic activity. In other words, each depression is always a recession, sharing the same starting and ending dates and having the same duration.		Under the second definition, depressions and recessions will always be distinct events however, having the same starting dates. This definition of depression implies that a recession and a depression will have different ending dates and thus distinct durations. Under this definition, the length of a depression will always be longer than that of the recession starting the same date.		A useful example is the difference in the chronology of the Great Depression in the U.S. under the view of alternative definitions. Using the second definition of depression, most economists refer to the Great Depression, as the period between 1929 and 1941. On the other hand, using the first definition, the depression that started in August 1929 lasted until March 1933. Note that NBER, which publishes the recession (instead of depression) dates for the U.S. economy, has identified two recessions during that period. The first between August 1929 and March 1933 and the second starting in May 1937 and ending in June 1938.[5]		Today the term "depression" is most often associated with the Great Depression of the 1930s, but the term had been in use long before then. Indeed, an early major American economic crisis, the Panic of 1819, was described by then-president James Monroe as "a depression",[6] and the economic crisis immediately preceding the 1930s depression, the Depression of 1920–21, was referred to as a "depression" by president Calvin Coolidge.		However, in the 19th and early 20th centuries, financial crises were traditionally referred to as "panics", e.g., the 'major' Panic of 1907, and the 'minor' Panic of 1910–1911, though the 1929 crisis was more commonly called "The Crash", and the term "panic" has since fallen out of use. At the time of the Great Depression (of the 1930s), the phrase "The Great Depression" had already been used to refer to the period 1873–96 (in the United Kingdom), or more narrowly 1873–79 (in the United States), which has since been renamed the Long Depression.		Common use of the phrase "The Great Depression" for the 1930s crisis is most frequently attributed to British economist Lionel Robbins, whose 1934 book The Great Depression is credited with 'formalizing' the phrase,[6] though US president Herbert Hoover is widely credited with having 'popularized' the term/phrase,[6][7] informally referring to the downturn as a "depression", with such uses as "Economic depression cannot be cured by legislative action or executive pronouncement", (December 1930, Message to Congress) and "I need not recount to you that the world is passing through a great depression" (1931).		Due to the lack of an agreed definition and the strong negative associations, the characterization of any period as a "depression" is contentious. The term was frequently used for regional crises from the early 19th century until the 1930s, and for the more widespread crises of the 1870s and 1930s, but economic crises since 1945 have generally been referred to as "recessions", with the 1970s global crisis referred to as "stagflation", but not a depression. The only two eras commonly referred to at the current time as "depressions" are the 1870s and 1930s.[8]		To some degree this is simply a stylistic change, similar to the decline in the use of "panic" to refer to financial crises, but it does also reflect that the economic cycle – both in the United States and in most OECD countries – though not in all – has been more moderate since 1945.		There have been many periods of prolonged economic underperformance in particular countries/regions since 1945, detailed below, but terming these as "depressions" is controversial. The current economic cycle, which has comprised the most significant global crisis since the Great Depression, has at times been termed a depression,[8] but this terminology is not widely used, with the episode instead being referred to by other terms, such as the "Great Recession".		The best-known depression was the Great Depression, which affected most national economies in the world throughout the 1930s. This depression is generally considered to have begun with the Wall Street Crash of 1929, and the crisis quickly spread to other national economies.[9] Between 1929 and 1933, the gross national product of the United States decreased by 33% while the rate of unemployment increased to 25% (with industrial unemployment alone rising to approximately 35% – U.S. employment was still over 25% agricultural).[citation needed]		A long-term effect of the Great Depression was the departure of every major currency from the gold standard, although the initial impetus for this was World War II (see Bretton Woods Accord). In any case, the world economy has simply outgrown the capacity of additions to the world gold supply to accommodate the increase in world population and increased trade without periodic, painful revaluations of any currencies tied to gold.		Starting with the adoption of the gold standard in Britain and the United States, the Long Depression (1873–1896) was indeed longer than what is now referred to as the Great Depression, but shallower. However, it was known as "the Great Depression" until the 1930s.		The Panic of 1837 was an American financial crisis, built on a speculative real estate market.[10] The bubble burst on May 10, 1837 in New York City, when every bank stopped payment in gold and silver coinage. The Panic was followed by a five-year depression,[10] with the failure of banks and record high unemployment levels.[11]		Beginning in 2009, Greece sank into a recession that, after two years, became a depression. The country saw an almost 20% drop in economic output, and unemployment soared to near 25%.[12] Greece's high amounts of sovereign debt precipitated the crisis, and the poor performance of its economy since the introduction of severe austerity measures has slowed the entire eurozone's recovery. Greece's continuing troubles have led to discussions about its departure from the eurozone.		The late 1910s and early 1920s were marked by an economic depression that unraveled in particularly catastrophic circumstances: The Great War and its aftermath led to a global nosedive in commodities that ruined many developing nations, while servicemen returning from the trenches found themselves with high unemployment as businesses failed, unable to transition into a peacetime economy. Also, the Spanish flu pandemic of 1918-20 brought economic activity to a standstill as even more people became incapacitated. Most developed countries had mostly recovered by 1921-22, however Germany saw its economy crippled until 1923-24 because of the hyperinflation crisis.		The 1973 oil crisis, coupled with the rising costs of maintenance of welfare state in most countries led to a recession between 1973 and 1975, followed by a period of almost minimal growth and rising inflation and unemployment. The 1980-82 recession marked the end of the period.		The savings & loans and the leveraged buyout crises led to a severe depression in mid-to-late 1989, causing a recession in 1990-91 (also fueled by the oil price crisis), whose effects lasted as late as 1994. This downturn is more remembered for its political effects: British Prime Minister Margaret Thatcher had to resign in November 1990 as a result of the socioeconomic debacle caused by her later policies; and while his approval ratings were above 60%, U.S. President George H. W. Bush lost the 1992 election to Bill Clinton because of the domestic malady marked by the depression and increasing urban decay.		In 2005, the persistent oil price rises and economic overheating caused by deregulation led to a gradual deterioration of the world economy with inflation and unemployment rising as growth slowed down: The housing bubble in the U.S. burst in 2007, and the American economy slipped into a recession. This in turn provoked the failure of many prominent financial institutions throughout 2008, most notably Lehman Brothers, leading to the loss of millions of jobs.		Several Latin American countries had severe downturns in the 1980s: by the Kehoe and Prescott definition of a great depression as at least one year with output 20% below trend, Argentina, Brazil, Chile, and Mexico experienced great depressions in the 1980s, and Argentina experienced another in 1998–2002. South American countries fell once again into this in the early-to-mid 2010s as a result of falling prices of commodities such as in the 1980s, with the economies of Argentina, Brazil and Venezuela falling into recession as massive government spending further strained their financial situation.		This definition also includes the economic performance of New Zealand from 1974–1992 and Switzerland from 1973 to the present, although this designation for Switzerland has been controversial.[13][14]		Over the period 1980–2000, Sub-Saharan Africa broadly suffered a fall in absolute income levels.[15]		The economic crisis in the 1990s that struck former members of the Soviet Union was almost twice as intense as the Great Depression in the countries of Western Europe and the United States in the 1930s.[16][17][18] Average standards of living registered a catastrophic fall in the early 1990s in many parts of the former Eastern Bloc - most notably, in post-Soviet states.[19] Even before Russia's financial crisis of 1998, Russia's GDP was half of what it had been in the early 1990s.[18] Some populations are still poorer today than they were in 1989 (e.g. Ukraine, Moldova, Serbia, Central Asia, Caucasus).[citation needed] The collapse of the Soviet planned economy and the transition to market economy resulted in catastrophic declines in GDP of about 45% during the 1990–1996 period[20] and poverty in the region had increased more than tenfold.[21]		Finnish economists refer to the Finnish economic decline around the breakup of the Soviet Union (1989–1994) as a great depression; this is partly attributed to the breakup of the Soviet Union, and partly to the Scandinavian banking crisis, which was also suffered, to a lesser degree, by Sweden and Norway.		
Employability can be defined as “doing value creating work, getting paid for it and learning at the same time, enhancing the ability to get work in the future” [1]						Employability is a management philosophy, developed Sumantra Goshal in 1997, which recognises that employment and market performance stem from the initiative, creativity and competencies of all employees, and not just from the wisdom of senior management.		For employers, it involves creating a working environment that can provide opportunities for personal and professional growth, within a management environment where it is understood that talented, growing people mean talented, growing organisations.[2]		For many employees, the new contract would involve movement towards a greater commitment to continuous learning and development, and towards an acceptance that, in a climate of constant change and uncertainty, the will to develop is the only hedge against a changing job market.[3]		There are several options for and many aspects of employability:		Traditional employment does not include employability. Review of the literature regarding traditional employment and employability suggests that employability is related to work and the ability to be employed, such as:		Lee Harvey defines employability as the ability of a graduate to get a satisfying job, stating that job acquisition should not be prioritized over preparedness for employment to avoid pseudo measure of individual employability. Lee argues that employability is not a set of skills but a range of experiences and attributes developed through higher-level learning, thus employability is not a “product‘ but a process of learning.		Employability continues to develop because the graduate, once employed, does not stop learning (i.e. continuous learning). Thus employability by this definition is about learning, not least learning how to learn, and it is about empowering learners as critical reflective citizens[6] definition is important for it emphasizes employability of graduates, which is similar to our context, hence, able to provide insight about how to measure graduates‘ employability and what are the differences between graduates and experienced individuals in labor market.		Berntson (2008) argues that employability refers to an individual‘s perception of his or her possibilities of getting new, equal, or better employment. Berntson‘s study differentiates employability into two main categories – actual employability (objective employability) and perceived employability (subjective employability).		Several employability definitions have been developed based on, or including input from business and industry. In the United States, an Employability Skills Framework was developed through a collaboration of employers, educators, human resources associations, and labour market associations. This framework states, “Employability skills are general skills that are necessary for success in the labor market at all employment levels and in all sectors”. After conducting research with employers across Canada, the Conference Board of Canada released Employability Skills 2000+, which defines employability as “the skills you need to enter, stay in, and progress in the world of work”. Saunders & Zuzel (2010) found that employers valued personal qualities such as dependability and enthusiasm over subject knowledge and ability to negotiate.[7]		In the future fewer will be employed and more people work as free lancers or ad hoc on projects. Robin Chase, co-founder of Zip Car, argues that in the future more work will be done as freelancers or ad hoc works. Collaborative economy and other similar platforms are reinventing capitalism, for example platforms like Freelancer.com, a new way of organizing demand and supply.[8] Freelancer is also an example of how employability can be developed even for people who are not employed – Freelancers offers exposure of certification and in the future similar platforms will also offer continuous upgrade of competencies for the people associated.		INSEAD, and other organization institutions are experimenting with “pro-active development of employability”. INSEAD works for example with future competency profiles developed by SanderMap CEO, Sandeep Sander . The idea is to translate future strategies into competencies needed – and tailor programs to cover competency gaps for the individual.		A similar approach is also used in leading corporations like Novo-Nordisk, a pharma company with 40,000 employees.[9]		Employability creates organizational issues, because future competency needs may require re-organization in many ways. The increasing automation and use of technology also makes it relevant to discuss not only change but also transformation is tasks for people. The issues are relevant at government level, at corporate level and for individuals, as highlighted in a recent manifest from though leaders like Steve Jurvetson		Although the intention behind employability from employers might be to retain the best talent, it will happen that others offer opportunities that are more attractive or fit better. In these cases it is relevant to discuss how to end the employment contract including “employability” or “competence upgrade paid by the corporation”. A model used with MBA students might become more common; the company pays for “employability development” but if the employees decides to leave before xx months of employment then the invested amount is due, fully or partly.		In the past, government had institutions to handle unemployment and employment. In the future this will be extended to include employability.		Singapore, has created a “Institute for Employability”[10] that works on competency upgrades, to reduce risk of unemployment and increase the competitiveness of the nation, the corporations and the employability for the individual.		
A career break is a period of time out from employment. Traditionally, this is for women to raise children[citation needed], but it is sometimes used for people taking time out of their career for personal development and/or professional development.						A career break is usually between one month and two years long. Six months to two years is the most common period of time for a career break.[1] It is also possible to take a mini career break[2] of less than one month, which enables people to try out career break activities without committing to longer periods of time. Shorter career breaks are most popular with the people over 45 years of age.[1]		It can take the form of a sabbatical, which can be paid or unpaid; unpaid sabbaticals are much more common.[1] Sabbaticals were originally only offered to academics and clerics but are now being increasingly offered by companies.[3]		A career break is not simply a period of unemployment. Career breakers usually do one or more of the following:		The career break has grown in popularity over the last five years, with 75% of the British workforce currently considering a career break.[4] Every year, around 90,000 professionals are estimated to take a career break.[5] It is most common in the UK, where it grew out of the gap year concept. The career break is sometimes referred to as an 'adult gap year', which reflects the commitment towards developing skills and gaining experience while out of the workforce. This was talked about by Stefan Sagmeister in his TED talk "The power of time off".[6]		There is currently no law in the UK requiring an employer to offer or grant career breaks.[7]		
The Long Depression was a worldwide price and economic recession, beginning in 1873 and running either through the spring of 1879, or 1896, depending on the metrics used.[1] It was the most severe in Europe and the United States, which had been experiencing strong economic growth fueled by the Second Industrial Revolution in the decade following the American Civil War. The episode was labeled the "Great Depression" at the time, and it held that designation until the Great Depression of the 1930s. Though a period of general deflation and a general contraction, it did not have the severe economic retrogression of the Great Depression.[2]		It was most notable in Western Europe and North America, at least in part because reliable data from the period is most readily available in those parts of the world. The United Kingdom is often considered to have been the hardest hit; during this period it lost some of its large industrial lead over the economies of Continental Europe.[3] While it was occurring, the view was prominent that the economy of the United Kingdom had been in continuous depression from 1873 to as late as 1896 and some texts refer to the period as the Great Depression of 1873–96.[4]		In the United States, economists typically refer to the Long Depression as the Depression of 1873–79, kicked off by the Panic of 1873, and followed by the Panic of 1893, book-ending the entire period of the wider Long Depression.[5] The National Bureau of Economic Research dates the contraction following the panic as lasting from October 1873 to March 1879. At 65 months, it is the longest-lasting contraction identified by the NBER, eclipsing the Great Depression's 43 months of contraction.[6][7] In the US, from 1873 to 1879, 18,000 businesses went bankrupt, including 89 railroads.[8] Ten states and hundreds of banks went bankrupt.[citation needed] Unemployment peaked in 1878, long after the panic ended. Different sources peg the peak U.S. unemployment rate anywhere from 8.25%[9] to 14%.[10]						The period preceding the depression was dominated by several major military conflicts and a period of economic expansion. In Europe, the end of the Franco-Prussian War yielded a new political order in Germany, and the £200 million reparations imposed on France led to an inflationary investment boom in Germany and Central Europe.[11] New technologies in industry such as the Bessemer converter were being rapidly applied; railroads were booming.[11] In the United States, the end of the Civil War and a brief post-war recession (1865–1867) gave way to an investment boom, focused especially on railroads on public lands in the Western United States - an expansion funded greatly by foreign investors.[11]		In 1873, during a decline in the value of silver—exacerbated by the end of the German Empire's production of the thaler coins from which the dollar got its name—the US government passed the Coinage Act of 1873 in April of that year. This essentially ended the bimetallic standard of the US, forcing it for the first time onto a pure gold standard. This measure, referred to by its opponents as "the Crime of 1873" and the topic of William Jennings Bryan's Cross of Gold speech in 1896, forced a contraction of the money supply in the US. It also drove down silver prices further, even as new silver mines were being established in Nevada, which stimulated mining investment but increased supply as demand was falling.[12] Silver miners arrived at US mints, unaware of the ban on production of silver coins, only to find their product no longer welcome there. By September, the US economy was in a crisis, deflation causing banking panics and destabilizing business investment, climaxing in the Panic of 1873.		The Panic of 1873 has been described as "the first truly international crisis".[11] The optimism that had been driving booming stock prices in central Europe had reached a fever pitch, and fears of a bubble culminated in a panic in Vienna beginning in April 1873. The collapse of the Vienna Stock Exchange began on May 8, 1873 and continued until May 10, when the exchange was closed; when it was reopened three days later, the panic seemed to have faded, and appeared confined to Austria-Hungary.[11] Financial panic arrived in the Americas only months later on Black Thursday, September 18, 1873 after the failure of the banking house of Jay Cooke and Company over the Northern Pacific Railway.[13] The Northern Pacific railway had been given 40 million acres (160,000 km2) of public land in the Western United States and Cooke sought $100,000,000 in capital for the company; the bank failed when the bond issue proved unsalable, and was shortly followed by several other major banks. The New York Stock Exchange closed for ten days on September 20.[11]		The financial contagion then returned to Europe, provoking a second panic in Vienna and further failures in continental Europe before receding. France, which had been experiencing deflation in the years preceding the crash, was spared financial calamity for the moment, as was Britain.[11]		Some have argued the depression was rooted in the 1870 Franco-Prussian War that hurt the French economy and, under the Treaty of Frankfurt, forced that country to make large war reparations payments to Germany. The primary cause of the price depression in the United States was the tight monetary policy that the US followed to get back to the gold standard after the Civil War. The US government was taking money out of circulation to achieve this goal, therefore there was less available money to facilitate trade. Because of this monetary policy the price of silver started to fall causing considerable losses of asset values; by most accounts, after 1879 production was growing, thus further putting downward pressure on prices due to increased industrial productivity, trade and competition.		In the US the speculative nature of financing due to both the greenback, which was paper currency issued to pay for the Civil War and rampant fraud in the building of the Union Pacific Railway up to 1869 culminated in the Credit Mobilier panic. Railway overbuilding and weak markets collapsed the bubble in 1873. Both the Union Pacific and the Northern Pacific lines were central to the collapse; another railway bubble was the railway mania in the United Kingdom.		Because of the Panic of 1873, governments depegged their currencies, to save money. The demonetization of silver by European and North American governments in the early 1870s was certainly a contributing factor. The US Coinage Act of 1873 was met with great opposition by farmers and miners, as silver was seen as more of a monetary benefit to rural areas than to banks in big cities. In addition, there were US citizens who advocated the continuance of government-issued fiat money (United States Notes) to avoid deflation and promote exports. The western US states were outraged—Nevada, Colorado, and Idaho were huge silver producers with productive mines, and for a few years mining abated. Resumption of silver dollar coinage was authorized by the Bland-Allison Act of 1878. The resumption of the US government buying silver was enacted in 1890 with the Sherman Silver Purchase Act.		Monetarists believe that the 1873 depression was caused by shortages of gold that undermined the gold standard, and that the 1848 California Gold Rush, 1886 Witwatersrand Gold Rush in South Africa and the 1896–99 Klondike Gold Rush helped alleviate such crises. Other analyses have pointed to developmental surges (see Kondratiev wave), theorizing that the Second Industrial Revolution was causing large shifts in the economies of many states, imposing transition costs, which may also have played a role in causing the depression.		Like the later Great Depression, the Long Depression affected different countries at different times, at different rates, and some countries accomplished rapid growth over certain periods. Globally, however, the 1870s, 1880s, and 1890s were a period of falling price levels and rates of economic growth significantly below the periods preceding and following.		Between 1870 and 1890, iron production in the five largest producing countries more than doubled, from 11 million tons to 23 million tons, steel production increased twentyfold (half a million tons to 11 million tons), and railroad development boomed.[14] But at the same time, prices in several markets collapsed - the price of grain in 1894 was only a third what it had been in 1867,[15] and the price of cotton fell by nearly 50 percent in just the five years from 1872 to 1877,[16] imposing great hardship on farmers and planters. This collapse provoked protectionism in many countries, such as France, Germany, and the United States,[15] while triggering mass emigration from other countries such as Italy, Spain, Austria-Hungary, and Russia.[17] Similarly, while the production of iron doubled between the 1870s and 1890s,[14] the price of iron halved.[15]		Many countries experienced significantly lower growth rates relative to what they had experienced earlier in the 19th century and to what they experienced afterwards:		The global economic crisis first erupted in Austria-Hungary, where in May 1873 the Vienna Stock Exchange crashed.[11] In Hungary, the panic of 1873 terminated a mania of railroad-building.[20]		In the late 1870s the economic situation in Chile deteriorated. Chilean wheat exports were outcompeted by production in Canada, Russia and Argentina and Chilean copper was largely replaced in international markets by copper from the United States and Spain.[21] Income from silver mining in Chile also dropped.[21] Anibal Pinto president of Chile by 1878 expressed his concerns the following way:[21]		This "mining discovery" came, according to historians Gabriel Salazar and Julio Pinto, into existence through the conquest of Bolivian and Peruvian lands in the War of the Pacific.[21] It has been argued that economic situation and the view of new wealth in the nitrate was the true reason for the Chilean elite to go into war with its neighbors.[21]		Another response to the economic crisis, according to Jorge Pinto Rodríguez, was the new pulse of conquest of indigenous lands that took place in Araucanía in the 1870s.[22][23]		France's experience was somewhat unusual. Having been defeated in the Franco-Prussian War, the country was required to pay £200 million in reparations to the Germans and was already reeling when the 1873 crash occurred.[11] The French adopted a policy of deliberate deflation while paying off the reparations.[11]		While the United States resumed growth for a time in the 1880s, the Paris Bourse crash of 1882 sent France careening into depression, one which "lasted longer and probably cost France more than any other in the 19th century".[24] The Union Générale, a French bank, failed in 1882, prompting the French to withdraw three million pounds from the Bank of England and triggering a collapse in French stock prices.[25]		The financial crisis was compounded by diseases impacting the wine and silk industries[24] French capital accumulation and foreign investment plummeted to the lowest levels experienced by France in the latter half of the 19th century.[26] After a boom in new investment banks after the end of the Franco-Prussian War, the destruction of the French banking industry wrought by the crash cast a pall over the financial sector that lasted until the dawn of the 20th century.[24] French finances were further sunk by failing investments abroad, principally in railroads and buildings.[20] The French net national product declined over the ten years from 1882 to 1892.[27]		A ten-year tariff war broke out between France and Italy after 1887, damaging Franco-Italian relations which had prospered during Italian Unification. As France was Italy's biggest investor, the liquidation of French assets in the country was especially damaging.[27]		The Russian experience was similar to the US experience - three separate recessions, concentrated in manufacturing, occurred in the period (1874–1877, 1881–1886, and 1891–1892), separated by periods of recovery.[28]		The United Kingdom, which had previously experienced crises every decade since the 1820s, was unusually insulated from the effects of this financial crisis, even though the Bank of England kept interest rates as high as 9 percent in the 1870s.[11]		Building on an 1870 reform, and the 1879 famine, thousands of Irish tenant farmers affected by depressed producer prices and high rents launched the Land War in 1879, which resulted in the reforming Irish Land Acts.		In the United States, the Long Depression began with the Panic of 1873. The National Bureau of Economic Research dates the contraction following the panic as lasting from October 1873 to March 1879. At 65 months, it is the longest-lasting contraction identified by the NBER, eclipsing the Great Depression's 43 months of contraction.[6][30] Figures from Milton Friedman and Anna Schwartz show net national product increased 3 percent per year from 1869 to 1879 and real national product grew at 6.8 percent per year during that time frame.[31] However, since between 1869 and 1879 the population of the United States increased by over 17.5 percent,[32] per capita NNP growth was lower. Following the end of the episode in 1879, the U.S. economy would remain unstable, experiencing recessions for 114 of the 253 months until January 1901.[33]		The dramatic shift in prices mauled nominal wages - in the United States, nominal wages declined by one-quarter during the 1870s,[13] and as much as one-half in some places, such as Pennsylvania.[34] Although real wages had enjoyed robust growth in the aftermath of the American Civil War, increasing by nearly a quarter between 1865 and 1873, they stagnated until the 1880s, posting no real growth, before resuming their robust rate of expansion in the later 1880s.[35] The collapse of cotton prices devastated the already war-ravaged economy of the southern United States.[16] Although farm prices fell dramatically, American agriculture continued to expand production.[29]		Thousands of American businesses failed, defaulting on more than a billion dollars of debt.[34] One in four laborers in New York were out of work in the winter of 1873–1874[34] and, nationally, a million became unemployed.[34]		The sectors which experienced the most severe declines in output were manufacturing, construction, and railroads.[29] The railroads had been a tremendous engine of growth in the years before the crisis, yielding a 50% increase in railroad mileage from 1867 to 1873.[29] After absorbing as much as 20% of US capital investment in the years preceding the crash, this expansion came to a dramatic end in 1873; between 1873 and 1878, the total amount of railroad mileage in the United States barely increased at all.[29]		The Freedman's Savings Bank was a typical casualty of the financial crisis. Chartered in 1865 in the aftermath of the American Civil War, the bank had been established to advance the economic welfare of America's newly emancipated freedmen.[36] In the early 1870s, the bank had joined in the speculative fever, investing in real estate and unsecured loans to railroads; its collapse in 1874 was a severe blow to African-Americans.[36]		The recession exacted a harsh political toll on President Ulysses S. Grant. Historian Allan Nevins says of the end of Grant's presidency:[37]		Various administrations have closed in gloom and weakness ... but no other has closed in such paralysis and discredit as (in all domestic fields) did Grant's. The President was without policies or popular support. He was compelled to remake his Cabinet under a grueling fire from reformers and investigators; half its members were utterly inexperienced, several others discredited, one was even disgraced. The personnel of the departments was largely demoralized. The party that autumn appealed for votes on the implicit ground that the next Administration would be totally unlike the one in office. In its centennial year, a year of deepest economic depression, the nation drifted almost rudderless.[37]		Recovery began in 1878. The mileage of railroad track laid down increased from 2,665 mi (4,289 km) in 1878 to 11,568 in 1882.[29] Construction began recovery by 1879; the value of building permits increased two and a half times between 1878 and 1883, and unemployment fell to 2.5% in spite of (or perhaps facilitated by) high immigration.[25]		The recovery, however, proved short-lived. Business profits declined steeply between 1882 and 1884.[25] The recovery in railroad construction reversed itself, falling from 11,569 mi (18,619 km) of track laid in 1882 to 2,866 mi (4,612 km) of track laid in 1885; the price of steel rails collapsed from $71/ton in 1880 to $20/ton in 1884.[25] Manufacturing again collapsed - durable goods output fell by a quarter again.[25] The decline became another financial crisis in 1884, when multiple New York banks collapsed; simultaneously, in 1883–1884, tens of millions of dollars of foreign-owned American securities were sold out of fears that the United States was preparing to abandon the gold standard.[25] This financial panic destroyed eleven New York banks, more than a hundred state banks, and led to defaults on at least $32 million worth of debt.[25] Unemployment, which had stood at 2.5% between recessions, surged to 7.5% in 1884–1885, and 13% in the northeastern United States, even as immigration plunged in response to deteriorating labor markets.[25]		This second recession led to further deterioration of farm prices. Kansas farmers burned their own corn in 1885 because it was worth less than other fuels such as coal or wood.[25] The country began to recover in 1885.[25]		The period preceding the Long Depression had been one of increasing economic internationalism, championed by efforts such as the Latin Monetary Union, many of which then were derailed or stunted by the impacts of economic uncertainty.[38] The extraordinary collapse of farm prices[15] provoked a protectionist response in many nations. Rejecting the free trade policies of the Second Empire, French president Adolphe Thiers led the new Third Republic to protectionism, which led ultimately to the stringent Méline tariff in 1892.[39] Germany's agrarian Junker aristocracy, under attack by cheap, imported grain, successfully agitated for a protective tariff in 1879 in Otto von Bismarck's Germany over the protests of his National Liberal Party allies.[39] In 1887, Italy and France embarked on a bitter tariff war.[40] In the United States, Benjamin Harrison won the 1888 US presidential election on a protectionist pledge.[41]		As a result of the protectionist policies enacted by the world's major trading nations, the global merchant marine fleet posted no significant growth from 1870 to 1890 before it nearly doubled in tonnage in the prewar economic boom that followed.[42] Only the United Kingdom and the Netherlands remained committed to low tariffs.[40]		In 1874, a year after the 1873 crash, the United States Congress passed legislation called the Inflation Bill of 1874 designed to confront the issue of falling prices by injecting fresh greenbacks into the money supply.[43] Under pressure from business interests, President Ulysses S. Grant vetoed the measure.[43] In 1878, Congress overrode President Rutherford B. Hayes's veto to pass the Silver Purchase Act, a similar but more successful attempt to promote "easy money."[29]		The United States endured its first nationwide strike in 1877, the Great Railroad Strike of 1877.[29] This led to widespread unrest and often violence in many major cities and industrial hubs including Baltimore, Philadelphia, Pittsburgh, Reading, Saint Louis, Scranton, and Shamokin.[44]		The Long Depression contributed to the revival of colonialism leading to the New Imperialism period, symbolized by the scramble for Africa, as the western powers sought new markets for their goods.[45] According to Hannah Arendt's The Origins of Totalitarianism (1951), the "unlimited expansion of power" followed the "unlimited expansion of capital".[46]		In the United States, beginning in 1878, the rebuilding, extending, and refinancing of the western railways, commensurate with the wholesale giveaway of water, timber, fish, minerals in what had previously been Indian territory, characterized a rising market. This led to the expansion of markets and industry, together with the robber barons of railroad owners, which culminated in the genteel 1880s and 1890s. The Gilded Age was the outcome for the few rich. The cycle repeated itself with the Panic of 1893, another huge market crash.		In the United States, the National Bureau of Economic Analysis dates the recession through March 1879. In January 1879, the United States returned to the gold standard which it had abandoned during the Civil War; according to economist Rendigs Fels, the gold standard put a floor to the deflation, and this was further boosted by especially good agricultural production in 1879.[47] The view that a single recession lasted from 1873 to 1896 or 1897 is not supported by most modern reviews of the period. It has even been suggested that the trough of this business cycle may have occurred as early as 1875.[48] In fact, from 1869 to 1879, the US economy grew at a rate of 6.8% for real net national product (NNP) and 4.5% for real NNP per capita.[49] Real wages were flat from 1869 to 1879, while from 1879 to 1896, nominal wages rose 23% and prices fell 4.2%.[50]		Irving Fisher believed that the Panic of 1873 and the severity of the contractions which followed it could be explained by debt and deflation and that a financial panic would trigger catastrophic deleveraging in an attempt to sell assets and increase capital reserves; that selloff would trigger a collapse in asset prices and deflation, which would in turn prompt financial institutions to sell off more assets, only to further deflation and strain capital ratios. Fisher believed that had governments or private enterprise embarked on efforts to reflate financial markets, the crisis would have been less severe.[51]		David Ames Wells (1890) wrote of the technological advancements during the period 1870–90, which included the Long Depression. Wells gives an account of the changes in the world economy transitioning into the Second Industrial Revolution in which he documents changes in trade, such as triple expansion steam shipping, railroads, the effect of the international telegraph network and the opening of the Suez Canal.[52] Wells gives numerous examples of productivity increases in various industries and discusses the problems of excess capacity and market saturation.		Wells' opening sentence:		The economic changes that have occurred during the last quarter of a century - or during the present generation of living men - have unquestionably been more important and more varied than during any period of the world's history.		Other changes Wells mentions are reductions in warehousing and inventories, elimination of middlemen, economies of scale, the decline of craftsmen, and the displacement of agricultural workers. About the whole 1870–90 period Wells said:		Some of these changes have been destructive, and all of them have inevitably occasioned, and for a long time yet will continue to occasion, great disturbances in old methods, and entail losses of capital and changes in occupation on the part of individuals. And yet the world wonders, and commissions of great states inquire, without coming to definite conclusions, why trade and industry in recent years has been universally and abnormally disturbed and depressed.		Wells notes that many of the government inquires on the "depression of prices" (deflation) found various reasons such as the scarcity of gold and silver. Wells showed that the US money supply actually grew over the period of the deflation. Wells noted that deflation lowered the cost of only goods that benefited from improved methods of manufacturing and transportation. Goods produced by craftsmen and many services did not decrease in value, and the cost of labor actually increased. Also, deflation did not occur in countries that did not have modern manufacturing, transportation, and communications.		Nobel laureate economist Milton Friedman, author of A Monetary History of the United States, on the other hand, blamed this prolonged economic crisis on the imposition of a new gold standard, part of which he referred to by its traditional name, The Crime of 1873.[53] This forced shift into a currency whose supply was limited by nature, unable to expand with demand, caused a series of economic and monetary contractions that plagued the entire period of the Long Depression. Murray Rothbard, in his book History of Money and Banking of the United States, argues that the long depression was only a misunderstood recession since real wages and production were actually increasing throughout the period. Like Friedman, he attributes falling prices to the resumption of a deflationary gold standard in the U.S. after the Civil War.		Most economic historians see this period as negative for the most industrial nations.[citation needed] Many argue that most of the stagnation was caused by a monetary contraction caused by abandonment of the bimetallic standard, for a new fiat gold standard, starting with the Coinage Act of 1873.[citation needed]		Other economic historians have complained about the characterization of this period as a "depression" because of conflicting economic statistics that cast doubt on the interpretation of this period as a depression. They note that this period saw a relatively large expansion of industry, of railroads, of physical output, of net national product and real per-capita income.		As economists Friedman and Schwartz have noted, the decade from 1869 to 1879 saw a growth of 3 percent per year in money national product, an outstanding real national product growth of 6.8 percent per year in this period and a rise of 4.5 percent per year in real product per capita. Even the alleged "monetary contraction" never took place, the money supply increasing by 2.7 percent per year in this period. From 1873 through 1878, before another spurt of monetary expansion, the total supply of bank money rose from $1.964 billion to $2.221 billion, a rise of 13.1 percent or 2.6 percent per year. In short, a modest but definite rise, and scarcely a contraction.[54] Although per-capita nominal income declined very gradually from 1873 to 1879, that decline was more than offset by a gradual increase over the course of the next 17 years.		Furthermore, real per-capita income either stayed approximately constant (1873–1880; 1883–1885) or rose (1881–1882; 1886–1896) so the average consumer appears to have been considerably better off at the end of the "depression" than before. Studies of other countries where prices also tumbled, including the US, Germany, France, and Italy, reported more markedly positive trends in both nominal and real per-capita income figures. Profits generally were also not adversely affected by deflation although they declined (particularly in Britain) in industries that were struggling against superior, foreign competition. Furthermore, some economists argue that a falling general price level is not inherently harmful to an economy and cite the economic growth of the period as evidence.[55] As economist Murray Rothbard has stated:		Unfortunately, most historians and economists are conditioned to believe that steadily and sharply falling prices must result in depression: hence their amazement at the obvious prosperity and economic growth during this era. For they have overlooked the fact that in the natural course of events, when government and the banking system do not increase the money supply very rapidly, freemarket capitalism will result in an increase of production and economic growth so great as to swamp the increase of money supply. Prices will fall, and the consequences will be not depression or stagnation, but prosperity (since costs are falling, too), economic growth, and the spread of the increased living standard to all the consumers.[55]		Accompanying the overall growth in real prosperity was a marked shift in consumption from necessities to luxuries: by 1885, "more houses were being built, twice as much tea was being consumed, and even the working classes were eating imported meat, oranges, and dairy produce in quantities unprecedented". The change in working class incomes and tastes was symbolized by "the spectacular development of the department store and the chain store".		Prices certainly fell, but almost every other index of economic activity - output of coal and pig iron, tonnage of ships built, consumption of raw wool and cotton, import and export figures, shipping entries and clearances, railway freight clearances, joint-stock company formations, trading profits, consumption per head of wheat, meat, tea, beer, and tobacco - all of these showed an upward trend.[56]		A large part at least of the deflation commencing in the 1870s was a reflection of unprecedented advances in factor productivity. Real unit production costs for most final goods dropped steadily throughout the 19th century and especially from 1873 to 1896. At no previous time had there been an equivalent "harvest of technological advances... so general in their application and so radical in their implications". That is why, notwithstanding the dire predictions of many eminent economists, Britain did not end up paralyzed by strikes and lockouts. Falling prices did not mean falling money wages. Instead of inspiring large numbers of workers to go on strike, falling prices were inspiring them to go shopping.[57]		
The Phillips curve is a single-equation empirical model, named after William Phillips, describing a historical inverse relationship between rates of unemployment and corresponding rates of inflation that result within an economy. Stated simply, decreased unemployment, (i.e., increased levels of employment) in an economy will correlate with higher rates of inflation.		While there is a short run tradeoff between unemployment and inflation, it has not been observed in the long run.[1] In 1968, Milton Friedman asserted that the Phillips curve was only applicable in the short-run and that in the long-run, inflationary policies will not decrease unemployment.[2][3] Friedman then correctly predicted that in the 1973–75 recession, both inflation and unemployment would increase.[3] The long-run Phillips curve is now seen as a vertical line at the natural rate of unemployment, where the rate of inflation has no effect on unemployment.[4] Accordingly, the Phillips curve is now seen as too simplistic, with the unemployment rate supplanted by more accurate predictors of inflation based on velocity of money supply measures such as the MZM ("money zero maturity") velocity,[5] which is affected by unemployment in the short but not the long term.[6]						William Phillips, a New Zealand born economist, wrote a paper in 1958 titled The Relation between Unemployment and the Rate of Change of Money Wage Rates in the United Kingdom, 1861-1957, which was published in the quarterly journal Economica.[7] In the paper Phillips describes how he observed an inverse relationship between money wage changes and unemployment in the British economy over the period examined. Similar patterns were found in other countries and in 1960 Paul Samuelson and Robert Solow took Phillips' work and made explicit the link between inflation and unemployment: when inflation was high, unemployment was low, and vice versa.[8]		In the 1920s, an American economist Irving Fisher noted this kind of Phillips curve relationship. However, Phillips' original curve described the behavior of money wages.[9]		In the years following Phillips' 1958 paper, many economists in the advanced industrial countries believed that his results showed that there was a permanently stable relationship between inflation and unemployment.[citation needed] One implication of this for government policy was that governments could control unemployment and inflation with a Keynesian policy. They could tolerate a reasonably high rate of inflation as this would lead to lower unemployment – there would be a trade-off between inflation and unemployment. For example, monetary policy and/or fiscal policy could be used to stimulate the economy, raising gross domestic product and lowering the unemployment rate. Moving along the Phillips curve, this would lead to a higher inflation rate, the cost of enjoying lower unemployment rates.[citation needed] Economist James Forder argues that this view is historically false and that neither economists nor governments took that view and that the 'Phillips curve myth' was an invention of the 1970s.[10]		Since 1974, seven Nobel Prizes have been given to economists for, among other things, work critical of some variations of the Phillips curve. Some of this criticism is based on the United States' experience during the 1970s, which had periods of high unemployment and high inflation at the same time. The authors receiving those prizes include Thomas Sargent, Christopher Sims, Edmund Phelps, Edward Prescott, Robert A. Mundell, Robert E. Lucas, Milton Friedman, and F.A. Hayek.[11]		In the 1970s, many countries experienced high levels of both inflation and unemployment also known as stagflation. Theories based on the Phillips curve suggested that this could not happen, and the curve came under a concerted attack from a group of economists headed by Milton Friedman.[citation needed] Friedman argued that the Phillips curve relationship was only a short-run phenomenon. In this he followed eight years after Samuelson and Solow [1960] who wrote " All of our discussion has been phrased in short-run terms, dealing with what might happen in the next few years. It would be wrong, though, to think that our Figure 2 menu that related obtainable price and unemployment behavior will maintain its same shape in the longer run. What we do in a policy way during the next few years might cause it to shift in a definite way."[8] As Samuelson and Solow had argued 8 years earlier, he argued that in the long run, workers and employers will take inflation into account, resulting in employment contracts that increase pay at rates near anticipated inflation. Unemployment would then begin to rise back to its previous level, but now with higher inflation rates. This result implies that over the longer-run there is no trade-off between inflation and unemployment. This implication is significant for practical reasons because it implies that central banks should not set unemployment targets below the natural rate.[1]		More recent research has shown that there is a moderate trade-off between low-levels of inflation and unemployment. Work by George Akerlof, William Dickens, and George Perry,[12] implies that if inflation is reduced from two to zero percent, unemployment will be permanently increased by 1.5 percent. This is because workers generally have a higher tolerance for real wage cuts than nominal ones. For example, a worker will more likely accept a wage increase of two percent when inflation is three percent, than a wage cut of one percent when the inflation rate is zero.		Most economists no longer use the Phillips curve in its original form because it was shown to be too simplistic.[6] This can be seen in a cursory analysis of US inflation and unemployment data from 1953–92. There is no single curve that will fit the data, but there are three rough aggregations—1955–71, 1974–84, and 1985–92—each of which shows a general, downwards slope, but at three very different levels with the shifts occurring abruptly. The data for 1953–54 and 1972–73 do not group easily, and a more formal analysis posits up to five groups/curves over the period.[1]		But still today, modified forms of the Phillips Curve that take inflationary expectations into account remain influential. The theory goes under several names, with some variation in its details, but all modern versions distinguish between short-run and long-run effects on unemployment. Modern Phillips curve models include both a short-run Phillips Curve and a long-run Phillips Curve. This is because in the short run, there is generally an inverse relationship between inflation and the unemployment rate; as illustrated in the downward sloping short-run Phillips curve. In the long run, that relationship breaks down and the economy eventually returns to the natural rate of unemployment regardless of the inflation rate.[13]		The "short-run Phillips curve" is also called the "expectations-augmented Phillips curve", since it shifts up when inflationary expectations rise, Edmund Phelps and Milton Friedman argued. In the long run, this implies that monetary policy cannot affect unemployment, which adjusts back to its "natural rate", also called the "NAIRU" or "long-run Phillips curve". However, this long-run "neutrality" of monetary policy does allow for short run fluctuations and the ability of the monetary authority to temporarily decrease unemployment by increasing permanent inflation, and vice versa. The popular textbook of Blanchard gives a textbook presentation of the expectations-augmented Phillips curve.[14]		An equation like the expectations-augmented Phillips curve also appears in many recent New Keynesian dynamic stochastic general equilibrium models. In these macroeconomic models with sticky prices, there is a positive relation between the rate of inflation and the level of demand, and therefore a negative relation between the rate of inflation and the rate of unemployment. This relationship is often called the "New Keynesian Phillips curve". Like the expectations-augmented Phillips curve, the New Keynesian Phillips curve implies that increased inflation can lower unemployment temporarily, but cannot lower it permanently. Two influential papers that incorporate a New Keynesian Phillips curve are Clarida, Galí, and Gertler (1999),[15] and Blanchard and Galí (2007).[16]		There are at least two different mathematical derivations of the Phillips curve. First, there is the traditional or Keynesian version. Then, there is the new Classical version associated with Robert E. Lucas, Jr.		The original Phillips curve literature was not based on the unaided application of economic theory. Instead, it was based on empirical generalizations. After that, economists tried to develop theories that fit the data.		The traditional Phillips curve story starts with a wage Phillips Curve, of the sort described by Phillips himself. This describes the rate of growth of money wages (gW). Here and below, the operator g is the equivalent of "the percentage rate of growth of" the variable that follows.		The "money wage rate" (W) is shorthand for total money wage costs per production employee, including benefits and payroll taxes. The focus is on only production workers' money wages, because (as discussed below) these costs are crucial to pricing decisions by the firms.		This equation tells us that the growth of money wages rises with the trend rate of growth of money wages (indicated by the superscript "T") and falls with the unemployment rate (U). The function f() is assumed to be monotonically increasing with U so that the dampening of money-wage increases by unemployment is shown by the negative sign in the equation above.		There are several possible stories behind this equation. A major one is that money wages are set by bilateral negotiations under partial bilateral monopoly: as the unemployment rate rises, all else constant worker bargaining power falls, so that workers are less able to increase their wages in the face of employer resistance.		During the 1970s, this story had to be modified, because (as the late Abba Lerner had suggested in the 1940s) workers try to keep up with inflation. Since the 1970s, the equation has been changed to introduce the role of inflationary expectations (or the expected inflation rate, gPex). This produces the expectations-augmented wage Phillips curve:		The introduction of inflationary expectations into the equation implies that actual inflation can feed back into inflationary expectations and thus cause further inflation. The late economist James Tobin dubbed the last term "inflationary inertia," because in the current period, inflation exists which represents an inflationary impulse left over from the past.		It also involved much more than expectations, including the price-wage spiral. In this spiral, employers try to protect profits by raising their prices and employees try to keep up with inflation to protect their real wages. This process can feed on itself, becoming a self-fulfilling prophecy.		The parameter λ (which is presumed constant during any time period) represents the degree to which employees can gain money wage increases to keep up with expected inflation, preventing a fall in expected real wages. It is usually assumed that this parameter equals unity in the long run.		In addition, the function f() was modified to introduce the idea of the non-accelerating inflation rate of unemployment (NAIRU) or what's sometimes called the "natural" rate of unemployment or the inflation-threshold unemployment rate:		Here, U* is the NAIRU. As discussed below, if U < U*, inflation tends to accelerate. Similarly, if U > U*, inflation tends to slow. It is assumed that f(0) = 0, so that when U = U*, the f term drops out of the equation.		In equation [1], the roles of gWT and gPex seem to be redundant, playing much the same role. However, assuming that λ is equal to unity, it can be seen that they are not. If the trend rate of growth of money wages equals zero, then the case where U equals U* implies that gW equals expected inflation. That is, expected real wages are constant.		In any reasonable economy, however, having constant expected real wages could only be consistent with actual real wages that are constant over the long haul. This does not fit with economic experience in the U.S. or any other major industrial country. Even though real wages have not risen much in recent years, there have been important increases over the decades.		An alternative is to assume that the trend rate of growth of money wages equals the trend rate of growth of average labor productivity (Z). That is:		Under assumption [2], when U equals U* and λ equals unity, expected real wages would increase with labor productivity. This would be consistent with an economy in which actual real wages increase with labor productivity. Deviations of real-wage trends from those of labor productivity might be explained by reference to other variables in the model.		Next, there is price behavior. The standard assumption is that markets are imperfectly competitive, where most businesses have some power to set prices. So the model assumes that the average business sets a unit price (P) as a mark-up (M) over the unit labor cost in production measured at a standard rate of capacity utilization (say, at 90 percent use of plant and equipment) and then adds in the unit materials cost.		The standardization involves later ignoring deviations from the trend in labor productivity. For example, assume that the growth of labor productivity is the same as that in the trend and that current productivity equals its trend value:		The markup reflects both the firm's degree of market power and the extent to which overhead costs have to be paid. Put another way, all else equal, M rises with the firm's power to set prices or with a rise of overhead costs relative to total costs.		So pricing follows this equation:		UMC is unit raw materials cost (total raw materials costs divided by total output). So the equation can be restated as:		This equation can again be stated as:		Now, assume that both the average price/cost mark-up (M) and UMC are constant. On the other hand, labor productivity grows, as before. Thus, an equation determining the price inflation rate (gP) is:		Then, combined with the wage Phillips curve [equation 1] and the assumption made above about the trend behavior of money wages [equation 2], this price-inflation equation gives us a simple expectations-augmented price Phillips curve:		Some assume that we can simply add in gUMC, the rate of growth of UMC, in order to represent the role of supply shocks (of the sort that plagued the U.S. during the 1970s). This produces a standard short-term Phillips curve:		Economist Robert J. Gordon has called this the "Triangle Model" because it explains short-run inflationary behavior by three factors: demand inflation (due to low unemployment), supply-shock inflation (gUMC), and inflationary expectations or inertial inflation.		In the long run, it is assumed, inflationary expectations catch up with and equal actual inflation so that gP = gPex. This represents the long-term equilibrium of expectations adjustment. Part of this adjustment may involve the adaptation of expectations to the experience with actual inflation. Another might involve guesses made by people in the economy based on other evidence. (The latter idea gave us the notion of so-called rational expectations.)		Expectational equilibrium gives us the long-term Phillips curve. First, with λ less than unity:		This is nothing but a steeper version of the short-run Phillips curve above. Inflation rises as unemployment falls, while this connection is stronger. That is, a low unemployment rate (less than U*) will be associated with a higher inflation rate in the long run than in the short run. This occurs because the actual higher-inflation situation seen in the short run feeds back to raise inflationary expectations, which in turn raises the inflation rate further. Similarly, at high unemployment rates (greater than U*) lead to low inflation rates. These in turn encourage lower inflationary expectations, so that inflation itself drops again.		This logic goes further if λ is equal to unity, i.e., if workers are able to protect their wages completely from expected inflation, even in the short run. Now, the Triangle Model equation becomes:		If we further assume (as seems reasonable) that there are no long-term supply shocks, this can be simplified to become:		All of the assumptions imply that in the long run, there is only one possible unemployment rate, U* at any one time. This uniqueness explains why some call this unemployment rate "natural."		To truly understand and criticize the uniqueness of U*, a more sophisticated and realistic model is needed. For example, we might introduce the idea that workers in different sectors push for money wage increases that are similar to those in other sectors. Or we might make the model even more realistic. One important place to look is at the determination of the mark-up, M.		The Phillips curve equation can be derived from the (short-run) Lucas aggregate supply function. The Lucas approach is very different from that the traditional view. Instead of starting with empirical data, he started with a classical economic model following very simple economic principles.		Start with the aggregate supply function:		where Y is log value of the actual output, Yn is log value of the "natural" level of output, a is a positive constant, P is log value of the actual price level, and Pe is log value of the expected price level. Lucas assumes that Yn has a unique value.		Note that this equation indicates that when expectations of future inflation (or, more correctly, the future price level) are totally accurate, the last term drops out, so that actual output equals the so-called "natural" level of real GDP. This means that in the Lucas aggregate supply curve, the only reason why actual real GDP should deviate from potential—and the actual unemployment rate should deviate from the "natural" rate—is because of incorrect expectations of what is going to happen with prices in the future. (The idea has been expressed first by Keynes, General Theory, Chapter 20 section III paragraph 4).		This differs from other views of the Phillips curve, in which the failure to attain the "natural" level of output can be due to the imperfection or incompleteness of markets, the stickiness of prices, and the like. In the non-Lucas view, incorrect expectations can contribute to aggregate demand failure, but they are not the only cause. To the "new Classical" followers of Lucas, markets are presumed to be perfect and always attain equilibrium (given inflationary expectations).		We re-arrange the equation into:		Next we add unexpected exogenous shocks to the world supply v:		Subtracting last year's price levels P−1 will give us inflation rates, because		and		where π and πe are the inflation and expected inflation respectively.		There is also a negative relationship between output and unemployment (as expressed by Okun's law). Therefore, using		where b is a positive constant, U is unemployment, and Un is the natural rate of unemployment or NAIRU, we arrive at the final form of the short-run Phillips curve:		This equation, plotting inflation rate π against unemployment U gives the downward-sloping curve in the diagram that characterises the Phillips curve.		The New Keynesian Phillips curve was originally derived by Roberts in 1995,[17] and since been used in most state-of-the-art New Keynesian DSGE models like the one of Clarida, Galí, and Gertler (2000).[18][19]		where κ = α [ 1 − ( 1 − α ) β ] ϕ 1 − α {\displaystyle \kappa ={\frac {\alpha [1-(1-\alpha )\beta ]\phi }{1-\alpha }}} . The current expectations of next period's inflation are incorporated as β E t [ π t + 1 ] {\displaystyle \beta E_{t}[\pi _{t+1}]}		In the 1970s, new theories, such as rational expectations and the NAIRU (non-accelerating inflation rate of unemployment) arose to explain how stagflation could occur. The latter theory, also known as the "natural rate of unemployment", distinguished between the "short-term" Phillips curve and the "long-term" one. The short-term Phillips Curve looked like a normal Phillips Curve, but shifted in the long run as expectations changed. In the long run, only a single rate of unemployment (the NAIRU or "natural" rate) was consistent with a stable inflation rate. The long-run Phillips Curve was thus vertical, so there was no trade-off between inflation and unemployment. Edmund Phelps won the Nobel Prize in Economics in 2006 in part for this work. However, the expectations argument was in fact very widely understood (albeit not formally) before Phelps' work on it.[20]		In the diagram, the long-run Phillips curve is the vertical red line. The NAIRU theory says that when unemployment is at the rate defined by this line, inflation will be stable. However, in the short-run policymakers will face an inflation-unemployment rate tradeoff marked by the "Initial Short-Run Phillips Curve" in the graph. Policymakers can therefore reduce the unemployment rate temporarily, moving from point A to point B through expansionary policy. However, according to the NAIRU, exploiting this short-run tradeoff will raise inflation expectations, shifting the short-run curve rightward to the "New Short-Run Phillips Curve" and moving the point of equilibrium from B to C. Thus the reduction in unemployment below the "Natural Rate" will be temporary, and lead only to higher inflation in the long run.		Since the short-run curve shifts outward due to the attempt to reduce unemployment, the expansionary policy ultimately worsens the exploitable tradeoff between unemployment and inflation. That is, it results in more inflation at each short-run unemployment rate. The name "NAIRU" arises because with actual unemployment below it, inflation accelerates, while with unemployment above it, inflation decelerates. With the actual rate equal to it, inflation is stable, neither accelerating nor decelerating. One practical use of this model was to provide an explanation for stagflation, which confounded the traditional Phillips curve.		The rational expectations theory said that expectations of inflation were equal to what actually happened, with some minor and temporary errors. This in turn suggested that the short-run period was so short that it was non-existent: any effort to reduce unemployment below the NAIRU, for example, would immediately cause inflationary expectations to rise and thus imply that the policy would fail. Unemployment would never deviate from the NAIRU except due to random and transitory mistakes in developing expectations about future inflation rates. In this perspective, any deviation of the actual unemployment rate from the NAIRU was an illusion.		However, in the 1990s in the U.S., it became increasingly clear that the NAIRU did not have a unique equilibrium and could change in unpredictable ways. In the late 1990s, the actual unemployment rate fell below 4% of the labor force, much lower than almost all estimates of the NAIRU. But inflation stayed very moderate rather than accelerating. So, just as the Phillips curve had become a subject of debate, so did the NAIRU.		Furthermore, the concept of rational expectations had become subject to much doubt when it became clear that the main assumption of models based on it was that there exists a single (unique) equilibrium in the economy that is set ahead of time, determined independently of demand conditions. The experience of the 1990s suggests that this assumption cannot be sustained.		The Phillips curve started as an empirical observation in search of a theoretical explanation.[citation needed] Specifically, the Phillips curve tried to determine whether the inflation-unemployment link was causal or simply correlational. There are several major explanations of the short-term Phillips curve regularity.		To Milton Friedman there is a short-term correlation between inflation shocks and employment. When an inflationary surprise occurs, workers are fooled into accepting lower pay because they do not see the fall in real wages right away. Firms hire them because they see the inflation as allowing higher profits for given nominal wages. This is a movement along the Phillips curve as with change A. Eventually, workers discover that real wages have fallen, so they push for higher money wages. This causes the Phillips curve to shift upward and to the right, as with B. Some research underlines that some implicit and serious assumptions are actually in the background of the Friedmanian Phillips curve. This information asymmetry and a special pattern of flexibility of prices and wages are both necessary if one wants to maintain the mechanism told by Friedman. However, as it is argued, these presumptions remain completely unrevealed and theoretically ungrounded by Friedman.[21]		Economists such as Edmund Phelps reject this theory because it implies that workers suffer from money illusion. According to them, rational workers would only react to real wages, that is, inflation adjusted wages. However, one of the characteristics of a modern industrial economy is that workers do not encounter their employers in an atomized and perfect market. They operate in a complex combination of imperfect markets, monopolies, monopsonies, labor unions, and other institutions. In many cases, they may lack the bargaining power to act on their expectations, no matter how rational they are, or their perceptions, no matter how free of money illusion they are. It is not that high inflation causes low unemployment (as in Milton Friedman's theory) as much as vice versa: Low unemployment raises worker bargaining power, allowing them to successfully push for higher nominal wages. To protect profits, employers raise prices.		Similarly, built-in inflation is not simply a matter of subjective "inflationary expectations" but also reflects the fact that high inflation can gather momentum and continue beyond the time when it was started, due to the objective price/wage spiral.		However, other economists, like Jeffrey Herbener, argue that price is market-determined and competitive firms cannot simply raise prices.[citation needed] They reject the Phillips curve entirely, concluding that unemployment's influence is only a small portion of a much larger inflation picture that includes prices of raw materials, intermediate goods, cost of raising capital, worker productivity, land, and other factors.		Robert J. Gordon of Northwestern University has analyzed the Phillips curve to produce what he calls the triangle model, in which the actual inflation rate is determined by the sum of		The last reflects inflationary expectations and the price/wage spiral. Supply shocks and changes in built-in inflation are the main factors shifting the short-run Phillips Curve and changing the trade-off. In this theory, it is not only inflationary expectations that can cause stagflation. For example, the steep climb of oil prices during the 1970s could have this result.		Changes in built-in inflation follow the partial-adjustment logic behind most theories of the NAIRU:		In between these two lies the NAIRU, where the Phillips curve does not have any inherent tendency to shift, so that the inflation rate is stable. However, there seems to be a range in the middle between "high" and "low" where built-in inflation stays stable. The ends of this "non-accelerating inflation range of unemployment rates" change over time.		
Casual employment is an employment classification under employment law.						In Australian workplace law whereby an employee is paid at a higher hourly rate (at least 20%) in lieu of having their employment guaranteed, and lacking other usual full-time employment conditions such as sick leave.[1] 28% of all Australian workers were employed on a casual basis in 2003.[2]		Casual employees are often contacted regularly by their employers to arrange working times from week to week to get the work done which is more than their normal workforce can handle. As there is no expectation in a casual work contract between employee and employer of ongoing work, employees can legally refuse a specific work opportunity at any time. The government defines Casual employees as those from whom regular work is not expected, they are not bounded by any legal body and can at their convenience switch places they work at.[3][4][5]		Under various workplace awards, employment classification can change if a certain number of hours is worked in a particular time frame.[3]		In New Zealand, casual employees are guaranteed either annual leave pro-rata, or 8% holiday pay on top of earnings. Casual employment contracts lack sick leave and guaranteed work hours.		In Jinkinson v Oceana Gold (NZ) Ltd, the Employment Court of New Zealand ruled that:		The distinction between casual employment and ongoing employment lies in the extent to which the parties have mutual employment related obligations between periods of work. If those obligations only exist during periods of work, the employment will be regarded as casual. If there are mutual obligations which continue between periods of work, there will be an ongoing employment relationship"[6]		Under Lee v Minor Developments Ltd t/a Before Six Childcare Centre (2008), the Employment Court outlined the following characteristics as being those which the courts have used to assess whether employment is casual:[7]		In 2008, the Fourth Labour Government proposed the strengthening of casual employment rights.[8] However, they were voted out of office later during the year.		The UK Government defines casual employment as the following:[9]		
The International Standard Book Number (ISBN) is a unique[a][b] numeric commercial book identifier.		An ISBN is assigned to each edition and variation (except reprintings) of a book. For example, an e-book, a paperback and a hardcover edition of the same book would each have a different ISBN. The ISBN is 13 digits long if assigned on or after 1 January 2007, and 10 digits long if assigned before 2007. The method of assigning an ISBN is nation-based and varies from country to country, often depending on how large the publishing industry is within a country.		The initial ISBN configuration of recognition was generated in 1967 based upon the 9-digit Standard Book Numbering (SBN) created in 1966. The 10-digit ISBN format was developed by the International Organization for Standardization (ISO) and was published in 1970 as international standard ISO 2108 (the SBN code can be converted to a ten digit ISBN by prefixing it with a zero).		Occasionally, a book may appear without a printed ISBN if it is printed privately or the author does not follow the usual ISBN procedure; however, this can be rectified later.[1]		Another identifier, the International Standard Serial Number (ISSN), identifies periodical publications such as magazines; and the International Standard Music Number (ISMN) covers for musical scores.						The Standard Book Numbering (SBN) code is a 9-digit commercial book identifier system created by Gordon Foster, Emeritus Professor of Statistics at Trinity College, Dublin,[2] for the booksellers and stationers WHSmith and others in 1965.[3] The ISBN configuration of recognition was generated in 1967 in the United Kingdom by David Whitaker[4] (regarded as the "Father of the ISBN"[5]) and in 1968 in the US by Emery Koltay[4] (who later became director of the U.S. ISBN agency R.R. Bowker).[5][6][7]		The 10-digit ISBN format was developed by the International Organization for Standardization (ISO) and was published in 1970 as international standard ISO 2108.[3][4] The United Kingdom continued to use the 9-digit SBN code until 1974. ISO has appointed the International ISBN Agency as the registration authority for ISBN worldwide and the ISBN Standard is developed under the control of ISO Technical Committee 46/Subcommittee 9 TC 46/SC 9. The ISO on-line facility only refers back to 1978.[8]		An SBN may be converted to an ISBN by prefixing the digit "0". For example, the second edition of Mr. J. G. Reeder Returns, published by Hodder in 1965, has "SBN 340 01381 8" – 340 indicating the publisher, 01381 their serial number, and 8 being the check digit. This can be converted to ISBN 0-340-01381-8; the check digit does not need to be re-calculated.		Since 1 January 2007, ISBNs have contained 13 digits, a format that is compatible with "Bookland" European Article Number EAN-13s.[9]		An ISBN is assigned to each edition and variation (except reprintings) of a book. For example, an ebook, a paperback, and a hardcover edition of the same book would each have a different ISBN.[10] The ISBN is 13 digits long if assigned on or after 1 January 2007, and 10 digits long if assigned before 2007. An International Standard Book Number consists of 4 parts (if it is a 10 digit ISBN) or 5 parts (for a 13 digit ISBN):		A 13-digit ISBN can be separated into its parts (prefix element, registration group, registrant, publication and check digit), and when this is done it is customary to separate the parts with hyphens or spaces. Separating the parts (registration group, registrant, publication and check digit) of a 10-digit ISBN is also done with either hyphens or spaces. Figuring out how to correctly separate a given ISBN number is complicated, because most of the parts do not use a fixed number of digits.[13]		ISBN issuance is country-specific, in that ISBNs are issued by the ISBN registration agency that is responsible for that country or territory regardless of the publication language. The ranges of ISBNs assigned to any particular country are based on the publishing profile of the country concerned, and so the ranges will vary depending on the number of books and the number, type, and size of publishers that are active. Some ISBN registration agencies are based in national libraries or within ministries of culture and thus may receive direct funding from government to support their services. In other cases, the ISBN registration service is provided by organisations such as bibliographic data providers that are not government funded. In Canada, ISBNs are issued at no cost with the stated purpose of encouraging Canadian culture.[14] In the United Kingdom, United States, and some other countries, where the service is provided by non-government-funded organisations, the issuing of ISBNs requires payment of a fee.		Australia: ISBNs are issued by the commercial library services agency Thorpe-Bowker,[15] and prices range from $42 for a single ISBN (plus a $55 registration fee for new publishers) to $2,890 for a block of 1,000 ISBNs. Access is immediate when requested via their website.[16]		Brazil: National Library of Brazil, a government agency, is responsible for issuing ISBNs, and there is a cost of R$16 [17]		Canada: Library and Archives Canada, a government agency, is responsible for issuing ISBNs, and there is no cost. Works in French are issued an ISBN by the Bibliothèque et Archives nationales du Québec.		Colombia: Cámara Colombiana del Libro, a NGO, is responsible for issuing ISBNs. Cost of issuing an ISBN is about USD 20.		Hong Kong: The Books Registration Office (BRO), under the Hong Kong Public Libraries, issues ISBNs in Hong Kong. There is no fee.[18]		India: The Raja Rammohun Roy National Agency for ISBN (Book Promotion and Copyright Division), under Department of Higher Education, a constituent of the Ministry of Human Resource Development, is responsible for registration of Indian publishers, authors, universities, institutions, and government departments that are responsible for publishing books.[19] There is no fee associated in getting ISBN in India.[20]		Italy: The privately held company EDISER srl, owned by Associazione Italiana Editori (Italian Publishers Association) is responsible for issuing ISBNs.[21] The original national prefix 978-88 is reserved for publishing companies, starting at €49 for a ten-codes block[22] while a new prefix 979-12 is dedicated to self-publishing authors, at a fixed price of €25 for a single code.		Maldives: The National Bureau of Classification (NBC) is responsible for ISBN registrations for publishers who are publishing in the Maldives.[citation needed]		Malta: The National Book Council (Maltese: Il-Kunsill Nazzjonali tal-Ktieb) issues ISBN registrations in Malta.[23][24][25]		Morocco: The National Library of Morocco is responsible for ISBN registrations for publishing in Morocco and Moroccan-occupied portion of Western Sahara.		New Zealand: The National Library of New Zealand is responsible for ISBN registrations for publishers who are publishing in New Zealand.[26]		Pakistan: The National Library of Pakistan is responsible for ISBN registrations for Pakistani publishers, authors, universities, institutions, and government departments that are responsible for publishing books.		South Africa: The National Library of South Africa is responsible for ISBN issuance for South African publishing institutions and authors.		United Kingdom and Republic of Ireland: The privately held company Nielsen Book Services Ltd, part of Nielsen Holdings N.V., is responsible for issuing ISBNs in blocks of 10, 100 or 1000. Prices start from £120 (plus VAT) for the smallest block on a standard turnaround of ten days.[27]		United States: In the United States, the privately held company R.R. Bowker issues ISBNs.[4] There is a charge that varies depending upon the number of ISBNs purchased, with prices starting at $125.00 for a single number. Access is immediate when requested via their website.[28]		Publishers and authors in other countries obtain ISBNs from their respective national ISBN registration agency. A directory of ISBN agencies is available on the International ISBN Agency website.		The registration group identifier is a 1- to 5-digit number that is valid within a single prefix element (i.e. one of 978 or 979).[11] Registration group identifiers have primarily been allocated within the 978 prefix element.[29] The single-digit group identifiers within the 978 prefix element are: 0 or 1 for English-speaking countries; 2 for French-speaking countries; 3 for German-speaking countries; 4 for Japan; 5 for Russian-speaking countries; and 7 for People's Republic of China. An example 5-digit group identifier is 99936, for Bhutan. The allocated group IDs are: 0–5, 600–621, 7, 80–94, 950–989, 9926–9989, and 99901–99976.[30] Books published in rare languages typically have longer group identifiers.[31]		Within the 979 prefix element, the registration group identifier 0 is reserved for compatibility with International Standard Music Numbers (ISMNs), but such material is not actually assigned an ISBN.[11] The registration group identifiers within prefix element 979 that have been assigned are 10 for France, 11 for the Republic of Korea, and 12 for Italy.[32]		The original 9-digit standard book number (SBN) had no registration group identifier, but prefixing a zero (0) to a 9-digit SBN creates a valid 10-digit ISBN.		The national ISBN agency assigns the registrant element (cf. Category:ISBN agencies) and an accompanying series of ISBNs within that registrant element to the publisher; the publisher then allocates one of the ISBNs to each of its books. In most countries, a book publisher is not required by law to assign an ISBN; however, most bookstores only handle ISBN bearing publications.[citation needed]		A listing of more than 900,000 assigned publisher codes is published, and can be ordered in book form (€1399, US$1959). The web site of the ISBN agency does not offer any free method of looking up publisher codes.[33] Partial lists have been compiled (from library catalogs) for the English-language groups: identifier 0 and identifier 1.		Publishers receive blocks of ISBNs, with larger blocks allotted to publishers expecting to need them; a small publisher may receive ISBNs of one or more digits for the registration group identifier, several digits for the registrant, and a single digit for the publication element. Once that block of ISBNs is used, the publisher may receive another block of ISBNs, with a different registrant element. Consequently, a publisher may have different allotted registrant elements. There also may be more than one registration group identifier used in a country. This might occur once all the registrant elements from a particular registration group have been allocated to publishers.		By using variable block lengths, registration agencies are able to customise the allocations of ISBNs that they make to publishers. For example, a large publisher may be given a block of ISBNs where fewer digits are allocated for the registrant element and many digits are allocated for the publication element; likewise, countries publishing many titles have few allocated digits for the registration group identifier and many for the registrant and publication elements.[34] Here are some sample ISBN-10 codes, illustrating block length variations.		English-language registration group elements are 0 and 1 (2 of more than 220 registration group elements). These two registration group elements are divided into registrant elements in a systematic pattern, which allows their length to be determined, as follows:[35]		A check digit is a form of redundancy check used for error detection, the decimal equivalent of a binary check bit. It consists of a single digit computed from the other digits in the number. The method for the ten digit code is an extension of that for SBNs, the two systems are compatible, and SBN prefixed with "0" will give the same check-digit as without – the digit is base eleven, and can be 0-9 or X. The system for thirteen digit codes is not compatible and will, in general, give a different check digit from the corresponding 10 digit ISBN, and does not provide the same protection against transposition. This is because the thirteen digit code was required to be compatible with the EAN format, and hence could not contain an "X".		The 2001 edition of the official manual of the International ISBN Agency says that the ISBN-10 check digit[36] – which is the last digit of the ten-digit ISBN – must range from 0 to 10 (the symbol X is used for 10), and must be such that the sum of all the ten digits, each multiplied by its (integer) weight, descending from 10 to 1, is a multiple of 11.		For example, for an ISBN-10 of 0-306-40615-2:		Formally, using modular arithmetic, we can say:		It is also true for ISBN-10's that the sum of all the ten digits, each multiplied by its weight in ascending order from 1 to 10, is a multiple of 11. For this example:		Formally, we can say:		The two most common errors in handling an ISBN (e.g., typing or writing it) are a single altered digit or the transposition of adjacent digits. It can be proved that all possible valid ISBN-10's have at least two digits different from each other. It can also be proved that there are no pairs of valid ISBN-10's with eight identical digits and two transposed digits. (These are true only because the ISBN is less than 11 digits long, and because 11 is a prime number.) The ISBN check digit method therefore ensures that it will always be possible to detect these two most common types of error, i.e. if either of these types of error has occurred, the result will never be a valid ISBN – the sum of the digits multiplied by their weights will never be a multiple of 11. However, if the error occurs in the publishing house and goes undetected, the book will be issued with an invalid ISBN.[37]		In contrast, it is possible for other types of error, such as two altered non-transposed digits, or three altered digits, to result in a valid ISBN number (although it is still unlikely).		Modular arithmetic is convenient for calculating the check digit using modulus 11. Each of the first nine digits of the ten-digit ISBN—excluding the check digit itself—is multiplied by a number in a sequence from 10 to 2, and the remainder of the sum, with respect to 11, is computed. The resulting remainder, plus the check digit, must equal a multiple of 11 (either 0 or 11). Therefore, the check digit is (11 minus the remainder of the sum of the products modulo 11) modulo 11. Taking the remainder modulo 11 a second time accounts for the possibility that the first remainder is 0. Without the second modulo operation the calculation could end up with 11 – 0 = 11 which is invalid. (Strictly speaking the first "modulo 11" is unneeded, but it may be considered to simplify the calculation.)		For example, the check digit for an ISBN-10 of 0-306-40615-? is calculated as follows:		Thus the check digit is 2, and the complete sequence is ISBN 0-306-40615-2. The value x 10 {\displaystyle x_{10}} required to satisfy this condition might be 10; if so, an 'X' should be used.		It is possible to avoid the multiplications in a software implementation by using two accumulators. Repeatedly adding t into s computes the necessary multiples:		The modular reduction can be done once at the end, as shown above (in which case s could hold a value as large as 496, for the invalid ISBN 99999-999-9-X), or s and t could be reduced by a conditional subtract after each addition.		The 2005 edition of the International ISBN Agency's official manual[38] describes how the 13-digit ISBN check digit is calculated. The ISBN-13 check digit, which is the last digit of the ISBN, must range from 0 to 9 and must be such that the sum of all the thirteen digits, each multiplied by its (integer) weight, alternating between 1 and 3, is a multiple of 10.		Formally, using modular arithmetic, we can say:		The calculation of an ISBN-13 check digit begins with the first 12 digits of the thirteen-digit ISBN (thus excluding the check digit itself). Each digit, from left to right, is alternately multiplied by 1 or 3, then those products are summed modulo 10 to give a value ranging from 0 to 9. Subtracted from 10, that leaves a result from 1 to 10. A zero (0) replaces a ten (10), so, in all cases, a single check digit results.		For example, the ISBN-13 check digit of 978-0-306-40615-? is calculated as follows:		Thus, the check digit is 7, and the complete sequence is ISBN 978-0-306-40615-7.		In general, the ISBN-13 check digit is calculated as follows.		Let		Then		This check system – similar to the UPC check digit formula – does not catch all errors of adjacent digit transposition. Specifically, if the difference between two adjacent digits is 5, the check digit will not catch their transposition. For instance, the above example allows this situation with the 6 followed by a 1. The correct order contributes 3×6+1×1 = 19 to the sum; while, if the digits are transposed (1 followed by a 6), the contribution of those two digits will be 3×1+1×6 = 9. However, 19 and 9 are congruent modulo 10, and so produce the same, final result: both ISBNs will have a check digit of 7. The ISBN-10 formula uses the prime modulus 11 which avoids this blind spot, but requires more than the digits 0-9 to express the check digit.		Additionally, if the sum of the 2nd, 4th, 6th, 8th, 10th, and 12th digits is tripled then added to the remaining digits (1st, 3rd, 5th, 7th, 9th, 11th, and 13th), the total will always be divisible by 10 (i.e., end in 0).		The conversion is quite simple as one only needs to prefix "978" to the existing number and calculate the new checksum using the ISBN-13 algorithm.		Publishers and libraries have varied policies about the use of the ISBN check digit. Publishers sometimes fail to check the correspondence of a book title and its ISBN before publishing it; that failure causes book identification problems for libraries, booksellers, and readers.[39] For example, ISBN 0-590-76484-5 is shared by two books – Ninja gaiden® : a novel based on the best-selling game by Tecmo (1990) and Wacky Laws (1997), both published by Scholastic.		Most libraries and booksellers display the book record for an invalid ISBN issued by the publisher. The Library of Congress catalogue contains books published with invalid ISBNs, which it usually tags with the phrase "Cancelled ISBN".[40] However, book-ordering systems such as Amazon.com will not search for a book if an invalid ISBN is entered to its search engine.[citation needed] OCLC often indexes by invalid ISBNs, if the book is indexed in that way by a member library.		Only the term "ISBN" should be used; the terms "eISBN" and "e-ISBN" have historically been sources of confusion and should be avoided. If a book exists in one or more digital (e-book) formats, each of those formats must have its own ISBN. In other words, each of the three separate EPUB, Amazon Kindle, and PDF formats of a particular book will have its own specific ISBN. They should not share the ISBN of the paper version, and there is no generic "eISBN" which encompasses all the e-book formats for a title.[41]		Currently the barcodes on a book's back cover (or inside a mass-market paperback book's front cover) are EAN-13; they may have a separate barcode encoding five digits for the currency and the recommended retail price.[42] For 10 digit ISBNs, the number "978", the Bookland "country code", is prefixed to the ISBN in the barcode data, and the check digit is recalculated according to the EAN13 formula (modulo 10, 1x and 3x weighting on alternate digits).		Partly because of an expected shortage in certain ISBN categories, the International Organization for Standardization (ISO) decided to migrate to a thirteen-digit ISBN (ISBN-13). The process began 1 January 2005 and was planned to conclude 1 January 2007.[43] As of 2011, all the 13-digit ISBNs began with 978. As the 978 ISBN supply is exhausted, the 979 prefix was introduced. Part of the 979 prefix is reserved for use with the Musicland code for musical scores with an ISMN. 10 digit ISMN codes differed visually as they began with an "M" letter; the bar code represents the "M" as a zero (0), and for checksum purposes it counted as a 3. All ISMNs are now 13 digits commencing 979-0; 979-1 to 979-9 will be used by ISBN.		Publisher identification code numbers are unlikely to be the same in the 978 and 979 ISBNs, likewise, there is no guarantee that language area code numbers will be the same. Moreover, the ten-digit ISBN check digit generally is not the same as the thirteen-digit ISBN check digit. Because the GTIN-13 is part of the Global Trade Item Number (GTIN) system (that includes the GTIN-14, the GTIN-12, and the GTIN-8), the 13-digit ISBN falls within the 14-digit data field range.[44]		Barcode format compatibility is maintained, because (aside from the group breaks) the ISBN-13 barcode format is identical to the EAN barcode format of existing 10-digit ISBNs. So, migration to an EAN-based system allows booksellers the use of a single numbering system for both books and non-book products that is compatible with existing ISBN based data, with only minimal changes to information technology systems. Hence, many booksellers (e.g., Barnes & Noble) migrated to EAN barcodes as early as March 2005. Although many American and Canadian booksellers were able to read EAN-13 barcodes before 2005, most general retailers could not read them. The upgrading of the UPC barcode system to full EAN-13, in 2005, eased migration to the ISBN-13 in North America.		
Labour law (also known as labor law or employment law) mediates the relationship between workers, employing entities, trade unions and the government. Collective labour law relates to the tripartite relationship between employee, employer and union. Individual labour law concerns employees' rights at work and through the contract for work. Employment standards are social norms (in some cases also technical standards) for the minimum socially acceptable conditions under which employees or contractors are allowed to work. Government agencies (such as the former US Employment Standards Administration) enforce labour law (legislative, regulatory, or judicial).						Labour law arose in parallel with the Industrial Revolution as the relationship between worker and employer changed from small-scale production studios to large-scale factories. Workers sought better conditions and the right to join (or avoid joining) a labour union, while employers sought a more predictable, flexible and less costly workforce. The state of labour law at any one time is therefore both the product of, and a component of struggles between various social forces.		As England was the first country to industrialize, it was also the first to face the often appalling consequences of industrial revolution in a less regulated economic framework. Over the course of the late 18th and early to mid-19th century the foundation for modern labour law was slowly laid, as some of the more egregious aspects of working conditions were steadily ameliorated through legislation. This was largely achieved through the concerted pressure from social reformers, notably Anthony Ashley-Cooper, 7th Earl of Shaftesbury, and others.		A serious outbreak of fever in 1784 in cotton mills near Manchester drew widespread public opinion against the use of children in dangerous conditions. A local inquiry, presided over by Dr Thomas Percival, was instituted by the justices of the peace for Lancashire, and the resulting report recommended the limitation of children's working hours.[1] In 1802, the first major piece of labour legislation was passed − the Health and Morals of Apprentices Act. This was the first, albeit modest, step towards the protection of labour. The act limited working hours to twelve a day and abolished night work. It required the provision of a basic level of education for all apprentices, as well as adequate sleeping accommodation and clothing.		The rapid industrialization of manufacturing at the turn of the 19th century led to a rapid increase in child employment, and public opinion was steadily made aware of the terrible conditions these children were forced to endure. The Factory Act of 1819 was the outcome of the efforts of the industrialist Robert Owen and prohibited child labour under nine years of age and limited the working day to twelve. A great milestone in labour law was reached with the Factory Act of 1833, which limited the employment of children under eighteen years of age, prohibited all night work and, crucially, provided for inspectors to enforce the law. Pivotal in the campaigning for and the securing of this legislation were Michael Sadler and the Earl of Shaftesbury. This act was an important step forward, in that it mandated skilled inspection of workplaces and a rigorous enforcement of the law by an independent governmental body.		A lengthy campaign to limit the working day to ten hours was led by Shaftesbury, and included support from the Anglican Church.[2] Many committees were formed in support of the cause and some previously established groups lent their support as well.[3] The campaign finally led to the passage of the Factory Act of 1847, which restricted the working hours of women and children in British factories to effectively 10 hours per day.[4]		These early efforts were principally aimed at limiting child labour. From the mid-19th century, attention was first paid to the plight of working conditions for the workforce in general. In 1850, systematic reporting of fatal accidents was made compulsory, and basic safeguards for health, life and limb in the mines were put in place from 1855. Further regulations, relating to ventilation, fencing of disused shafts, signalling standards, and proper gauges and valves for steam-boilers and related machinery were also set down.		A series of further Acts, in 1860 and 1872 extended the legal provisions and strengthened safety provisions. Steady development of the coal industry, increasing association among miners, and increased scientific knowledge paved the way for the Coal Mines Act of 1872, which extended the legislation to similar industries. The same Act included the first comprehensive code of regulation to govern legal safeguards for health, life and limb. The presence of a more certified and competent management and increased levels of inspection were also provided for.		By the end of the century, a comprehensive set of regulations was in place in England that affected all industries. A similar system (with certain national differences) was implemented in other industrializing countries in the latter part of the 19th century and the early 20th century.		The basic feature of labour law in almost every country is that the rights and obligations of the worker and the employer are mediated through a contract of employment between the two. This has been the case since the collapse of feudalism. Many contract terms and conditions are covered by legislation or common law. In the US for example, the majority of state laws allow for employment to be "at will", meaning the employer can terminate an employee from a position for any reason, so long as the reason is not explicitly prohibited,[a] and, conversely, an employee may quit at any time, for any reason (or for no reason), and is not required to give notice.		One example of employment terms in many countries[5] is the duty to provide written particulars of employment with the essentialia negotii (Latin for "essential terms") to an employee. This aims to allow the employee to know concretely what to expect and what is expected. It covers items including compensation, holiday and illness rights, notice in the event of dismissal and job description.		The contract is subject to various legal provisions. An employer may not legally offer a contract that pays the worker less than a minimum wage. An employee may not agree to a contract that allows an employer to dismiss them for illegal reasons.[b]		Many jurisdictions define the minimum amount that a worker can be paid per hour. Algeria, Australia, Belgium, Brazil, Canada, China, France, Greece, Hungary, India, Ireland, Japan,South Korea, Luxembourg, the Netherlands, New Zealand, Paraguay, Portugal, Poland, Romania, Spain,Taiwan, the United Kingdom, the United States, Vietnam, Germany (in 2015[6]) and others have laws of this kind.[citation needed] The minimum wage is set usually higher than the lowest wage as determined by the forces of supply and demand in a free market and therefore acts as a price floor. Each country sets its own minimum wage laws and regulations, and while a majority of industrialized countries has a minimum wage, many developing countries do not.		Minimum wages are regulated and stipulated in some countries that lack explicit laws. In Sweden minimum wages are negotiated between the labour market parties (unions and employer organizations) through collective agreements that also cover non-union workers at workplaces with collective agreements. At workplaces without collective agreements there exist no minimum wages. Non-organized employers can sign substitute agreements directly with trade unions but far from all do. The Swedish case illustrates that in countries without statutory regulation will part of the labour market do not have regulated minimum wages, as self-regulation only applies to workplaces and employees covered by collective agreements (in Sweden about 90 per cent of employees).[7][8]		National minimum wage laws were first introduced in the United States in 1938,[9] Brazil in 1940[10] India in 1948,[11] France in 1950[12] and in the United Kingdom in 1998.[13] In the European Union, 18 out of 28 member states have national minimum wages as of 2011.[14]		The living wage is higher than the minimum wage and is designed that a full-time worker would be able to support themselves and a small family at that wage.[15]		The maximum number of hours worked per day or other time interval are set by law in many countries. Such laws also control whether workers who work longer hours must be paid additional compensation.		Before the Industrial Revolution, the workday varied between 11 and 14 hours. With the growth of industrialism and the introduction of machinery, longer hours became far more common, reaching as high as 16 hours per day.		The eight-hour movement led to the first law on the length of a working day, passed in 1833 in England. It limited miners to 12 hours and children to 8 hours. The 10-hour day was established in 1848, and shorter hours with the same pay were gradually accepted thereafter. The 1802 Factory Act was the first labour law in the UK.		Germany was the next European country to pass labour laws; Chancellor Otto von Bismarck's main goal was to undermine the Social Democratic Party of Germany. In 1878, Bismarck instituted a variety of anti-socialist measures, but despite this, socialists continued gaining seats in the Reichstag. To appease the working class, he enacted a variety of paternalistic social reforms, which became the first type of social security. In 1883 the Health Insurance Act was passed, which entitled workers to health insurance; the worker paid two-thirds and the employer one-third of the premiums. Accident insurance was provided in 1884, while old age pensions and disability insurance followed in 1889. Other laws restricted the employment of women and children. These efforts, however, were not entirely successful; the working class largely remained unreconciled with Bismarck's conservative government.[citation needed]		In France, the first labour law was voted in 1841. It limited under-age miners' hours. In the Third Republic labour law was first effectively enforced, in particular after Waldeck-Rousseau 1884 law legalising trade unions. With the Matignon Accords, the Popular Front (1936–38) enacted the laws mandating 12 days each year of paid vacations for workers and the law limiting the standard workweek to 40 hours.		Other labour laws involve safety concerning workers. The earliest English factory law was passed in 1802 and dealt with the safety and health of child textile workers.		Such laws prohibited discrimination against employees as morally unacceptable and illegal, in particular racial discrimination or gender discrimination.		Convention no. 158 of the International Labour Organization states that an employee "can't be fired without any legitimate motive" and "before offering him the possibility to defend himself". Thus, on April 28, 2006, after the unofficial repeal of the French First Employment Contract, the Longjumeau (Essonne) conseil des prud'hommes (labour law court) judged the New Employment Contract contrary to international law and therefore "illegitimate" and "without any juridical value". The court considered that the two-years period of "fire at will" (without any legal motive) was "unreasonable", and contrary to convention.[16][17]		Child labour was not seen as a problem throughout most of history, only disputed with the beginning of universal schooling and the concepts of labourers' and children's rights. Use of child labour was commonplace, often in factories. In England and Scotland in 1788, about two-thirds of persons working in water-powered textile factories were children.[18] Child labour can be factory work, mining or quarrying, agriculture, helping in the parents' business, operating a small business (such as selling food), or doing odd jobs. Children work as guides for tourists, sometimes combined with bringing in business for shops and restaurants (where they may also work). Other children do jobs such as assembling boxes or polishing shoes. However, rather than in factories and sweatshops, most child labour in the twenty-first century occurs in the informal sector, "selling on the street, at work in agriculture or hidden away in houses — far from the reach of official inspectors and from media scrutiny."[19]		Collective labour law concerns the relationship between employer, employee and trade unions. Trade unions (also "labor unions" in the US) are organizations which generally aim to promote the interests of their members.		Trade unions are organized groups of workers who engage in collective bargaining with employers. Some countries require unions and/or employers to follow particular procedures in pursuit of their goals. For example, some countries require that unions poll the membership to approve a strike or to approve using members' dues for political projects. Laws may govern the circumstances and procedures under which unions are formed. They may guarantee the right to join a union (banning employer discrimination), or remain silent in this respect. Some legal codes allow unions to obligate their members, such as the requirement to comply with a majority decision in a strike vote. Some restrict this, such as "right to work" legislation in parts of the United States.		In the different organization in the different countries trade union discuses with the employee on behalf of employer. At that time trade union discussed or talk with the manpower of the organization. At that time trade union perform his role like a bridge between the employee and employer.		A legally binding right for workers as a group to participate in workplace management is acknowledged in some form in most developed countries. In a majority of EU member states (for example, Germany, Sweden, and France) the workforce has a right to elect directors on the board of large corporations. This is usually called "codetermination" and currently most countries allow for the election of one third of the board, though the workforce can have the right to elect anywhere from a single director, to just under a half in Germany. However, German company law uses a split board system, in which a "supervisory board" appoints an "executive board". Under the Mitbestimmunggesetz 1976, shareholders and employees elect the supervisory board in equal numbers, but the head of the supervisory board with a casting vote is a shareholder representative. The first statutes to introduce board level codetermination were in Britain, however most of these measures, except in universities, were removed in 1948 and 1979. The oldest surviving statute is found in the United States, in the Massachusetts Laws on manufacturing corporations, introduced in 1919, however this was always voluntary.		In the United Kingdom, similar proposals were drawn up, and a command paper produced named the Bullock Report (Industrial democracy) was released in 1977 by the James Callaghan Labour Party government. Unions would have directly elected half of the board. An "independent" element would also be added. However, the proposal was not enacted. The European Commission offered proposals for worker participation in the "fifth company law directive", which was also not implemented.		In Sweden, participation is regulated through the "Law on board representation". The law covers all private companies with 25 or more employees. In these companies, workers (usually through unions) have a right to appoint two board members and two substitutes. If the company has more than 1,000 employees, this rises to three members and three substitutes. It is common practice to allocate them among the major union coalitions.		Workplace statutes in many countries require that employers consult their workers on various issues.		Strike action is the worker tactic most associated with industrial disputes. In most countries, strikes are legal under a circumscribed set of conditions. Among them may be that:		A boycott is a refusal to buy, sell, or otherwise trade with an individual or business. Other tactics include go-slow, sabotage, work-to-rule, sit-in or en-masse not reporting to work.[20] Some labour law explicitly bans such activity, none explicitly allows it.		Picketing is often used by workers during strikes. They may congregate near the business they are striking against to make their presence felt, increase worker participation and dissuade (or prevent) strike breakers from entering the workplace. In many countries, this activity is restricted by law, by more general law restricting demonstrations, or by injunctions on particular pickets. For example, labour law may restrict secondary picketing (picketing a business connected with the company not directly with the dispute, such as a supplier), or flying pickets (mobile strikers who travel to join a picket). Laws may prohibit obstructing others from conducting lawful business; outlaw obstructive pickets allow court orders to restrict picketing locations or behaving in particular ways (shouting abuse, for example).		The labour movement has long been concerned that economic globalization would weaken worker bargaining power, as their employers could hire workers abroad to avoid domestic labour standards. Karl Marx said:		The extension of the principle of free trade, which induces between nations such a competition that the interest of the workman is liable to be lost sight of and sacrificed in the fierce international race between capitalists, demands that such organizations [unions] should be still further extended and made international.[21]		The International Labour Organization and the World Trade Organization have been a primary focus among international bodies for regulating labour markets. Conflicts arise when people work in more than one country. EU law has a growing body of workplace rules.		Following World War One, the Treaty of Versailles contained the first constitution of a new International Labour Organization (ILO) founded on the principle that "labour is not a commodity", and for the reason that "peace can be established only if it is based upon social justice".[22] ILO's primary role has been to coordinate international labour law by issuing Conventions. ILO members can voluntarily adopt and ratify the Conventions. For instance, the first Hours of Work (Industry) Convention, 1919 required a maximum of a 48-hour week, and has been ratified by 52 out of 185 member states. The UK ultimately refused to ratify the Convention, as did many current EU members, although the Working Time Directive adopts its principles, subject to individual opt-out.[c] ILO's constitution comes from the 1944 Declaration of Philadelphia and under the 1998 Declaration on Fundamental Principles and Rights at Work classified eight conventions[d] as core.		These require freedom to join a union, bargain collectively and take action (Conventions No. 87 and 98), abolition of forced labour (29 and 105), abolition of labour by children before the end of compulsory school (138 and 182), and no discrimination at work (No. 100 and 111). Member compliance with the core Conventions is obligatory, even if the country has not ratified the Convention in question. To ensure compliance, the ILO is limited to gathering evidence and reporting on member states' progress, relying on publicity to create pressure to reform. Global reports on core standards are produced yearly, while individual reports on countries who have ratified other Conventions are compiled on a bi-annual or less frequent basis.		Because the ILO's enforcement mechanisms are weak,[citation needed] incorporating labour standards in the World Trade Organization's (WTO) operation has been proposed. WTO oversees, primarily, the General Agreement on Tariffs and Trade treaty aimed at reducing customs, tariffs and other barriers to import and export of goods, services and capital between its 157 member countries. Unlike for the ILO, contravening WTO rules as recognized by the dispute settlement procedures opens a country to retaliation through trade sanctions. This could include reinstatement of targeted tariffs against the offender.		Proponents have called for a "social clause" to be inserted into the GATT agreements, for example, by amending Article XX, which provides an exception that allows imposition of sanctions for breaches of human rights. An explicit reference to core labour standards could allow comparable action where a WTO member state breaches ILO standards. Opponents argue that such an approach could undermine labour rights, because industries, and therefore workforces could be harmed with no guarantee of reform. Furthermore, it was argued in the 1996 Singapore Ministerial Declaration 1996 that "the comparative advantage of countries, particularly low-age developing countries, must in no way be put into question."[23] Some countries want to take advantage of low wages and fewer rules as a comparative advantage to boost their economies. Another contested point is whether business moves production from high wage to low wage countries, given potential differences in worker productivity.[24] Since GATT, most trade agreements have been bilateral. Some of these protect core labour standards.[citation needed][e] Moreover, in domestic tariff regulations, some countries give preference to countries that respect core labour rights, for example under the EC Tariff Preference Regulation, articles 7 and 8.[25]		Conflicts of laws (or private international law) issues arise where workers work in multiple jurisdictions. If a US worker performs part of her job in Brazil, China and Denmark (a "peripatetic" worker) an employer may seek to characterize the employment contract as governed by the law of the country where labour rights are least favourable to the worker, or seek to argue that the most favourable system of labour rights does not apply. For example, in a UK labour law case, Ravat v Halliburton Manufacturing and Services Ltd[26] Ravat was from the UK but was employed in Libya by a German company that was part of Halliburton. He was dismissed by a supervisor based in Egypt. He was told he would be hired under UK law terms and conditions, and this was arranged by a staffing department in Aberdeen. Under the UK Employment Rights Act 1996 he would have a right to claim unfair dismissal, but the Act left open the question of the statute's territorial scope. The UK Supreme Court held that the principle would be that an expatriate worker, would be subject to UK rules if the worker could show a "close connection" to the UK, which was found in Rabat's case.[f]		This fits within the general framework in the EU. Under EU Rome I Regulation article 8,[27] workers have employment rights of the country where they habitually work. They may have a claim in another country if they can establish a close connection to it. The Regulation emphasises that the rules should be applied with the purpose of protecting the worker.[28]		It is also necessary that a court has jurisdiction to hear a claim. Under the Brussels I Regulation article 19,[29] this requires the worker habitually works in the place where the claim is brought, or is engaged there.		The European Union has extensive labour laws that officially exclude (according to the Treaty on the Functioning of the European Union) matters around direct wage regulation (e.g. setting a minimum wage), fairness of dismissals and collective bargaining. A series of Directives regulate almost all other issues, for instance the Working Time Directive guarantees 28 days of paid holiday, the Equality Framework Directive prohibits all forms of discrimination and the Collective Redundancies Directive requires that proper notice is given and consultation takes place on decisions about economic dismissals.		However, the European Court of Justice has recently extended the Treaties provisions via case law. Trade unions have sought to organize across borders in the same way that multinational corporations have organized production globally. Unions have sought to take collective action and strikes internationally. However, this coordination was challenged in the European Union in two controversial decisions. In Laval Ltd v Swedish Builders Union[30] a group of Latvian workers were sent to a construction site in Sweden. The local union took industrial action to make Laval Ltd sign up to the local collective bargaining agreement. Under the Posted Workers Directive, article 3 lays down minimum standards for foreign workers so that workers receive at least the minimum rights that they would have in their home country in case their place of work has lower minimum rights. Article 3(7) says that this "shall not prevent application of terms and conditions of employment which are more favourable to workers". Most people thought this meant that more favourable conditions could be given than the minimum (e.g., in Latvian law) by the host state's legislation or a collective agreement. However the European Court of Justice (ECJ) said that only the local state could raise standards beyond its minimum for foreign workers. Any attempt by the host state, or a collective agreement (unless the collective agreement is declared universal under article 3(8)) would infringe the business' freedom under TFEU article 56. This decision was implicitly reversed by the European Union legislature in the Rome I Regulation, which makes clear in recital 34 that the host state may allow more favourable standards. However, in The Rosella, the ECJ held that a blockade by the International Transport Workers Federation against a business that was using an Estonian flag of convenience (i.e., saying it was operating under Estonian law to avoid labour standards of Finland) infringed the business' right of free establishment under TFEU article 49. The ECJ said that it recognized the workers' "right to strike" in accordance with ILO Convention 87, but said that its use must be proportionately to the right of the business' establishment.		The Fair Work Act of 2009 provides the regulations governing Australian workplaces and employers. Australia has a minimum wage and workplace conditions overseen by the Fair Work Commission.[31]		In Canadian law, "labour law" refers to matters connected with unionized workplaces, while "employment law" deals with non-unionized employees.		In 2017, Premier Brad Wall announced that Saskatchewan's government is to cut 3.5 percent from its workers and officers' wages in 2018. This salary cut includes MLA ministers and the Premier's office staff along all people employed by the government. Unpaid days off, will also be implemented as well as limiting overtime to assist the wage cut.[32][33]		In the People's Republic of China the basic labour laws are the Labour Law of People's Republic of China (promulgated on 5 July 1994) and the Law of the People's Republic of China on Employment Contracts (adopted at the 28th Session of the Standing Committee of the 10th National People's Congress on June 29, 2007, effective from January 1, 2008). The administrative regulations enacted by the State Council, the ministerial rules and the judicial explanations of the Supreme People's Court stipulate detailed rules concerning various aspects of employment. The government-controlled All China Federation of Trade Unions is the sole legal labour union. Strikes are formally legal, but in practice are discouraged.		In France, the first labour laws were Waldeck Rousseau's laws passed in 1884. Between 1936 and 1938 the Popular Front enacted a law mandating 12 days (2 weeks) each year of paid vacation for workers, and a law limited the work week to 40 hours, excluding overtime. The Grenelle accords negotiated on May 25 and 26th in the middle of the May 1968 crisis, reduced the working week to 44 hours and created trade union sections in each enterprise.[34] The minimum wage was increased by 25%.[35] In 2000, Lionel Jospin's government enacted the 35-hour workweek, reduced from 39 hours. Five years later, conservative prime minister Dominique de Villepin enacted the New Employment Contract (CNE). Addressing the demands of employers asking for more flexibility in French labour laws, the CNE sparked criticism from trade unions and opponents claiming it favoured contingent work. In 2006, he then attempted to pass the First Employment Contract (CPE) through a vote by emergency procedure, but that was met by students and unions' protests. President Jacques Chirac finally had no choice but to repeal it.		Over fifty national and many more state-level laws govern work in India. So for instance, a permanent worker can be terminated only for proven misconduct or habitual absence.[36] In the Uttam Nakate case, the Bombay High Court held that dismissing an employee for repeated sleeping on the factory floor was illegal – the decision was overturned by the Supreme Court of India two decades later. In 2008, the World Bank criticized the complexity, lack of modernization and flexibility in Indian regulations.[37][38]		Iran has not ratified the two basic Conventions of the International Labour Organization on freedom of association and collective bargaining and one abolishing child labour.[39]		Mexican labour law reflects the historic interrelation between the state and the Confederation of Mexican Workers. The confederation is officially aligned with the Institutional Revolutionary Party (the Institutional Revolutionary Party, or PRI). While the law promises workers the right to strike and to organize, in practice it is difficult or impossible for independent unions to organize.		In Sweden many workplace issues such as working hours, minimum wage and right to overtime compensation are regulated through collective bargaining agreements in accordance with the Swedish model of self-regulation, i.e. regulation by the labour market parties themselves in contrast to state regulation (labour laws).[40][7] A notable exception is the Employment Protection act which regulates employment contracts and extensive employees' rights to employment under certain conditions.[41]		The labour law of Switzerland covers all standards governing the employment of some kind. The regulation of the employment by private employers is largely harmonized at the federal level, while public-sector employment still prevails a variety of cantonal laws. In particular, the civil standardization is distributed to a variety of laws. Of greater importance, particularly the new Federal Constitution of 1999, the Code of Obligations , the Labour Code as well as in the public sector, the Federal Personnel Act.[42]		The Factory Acts (first one in 1802, then 1833) and the 1823 Master and Servant Act were the first laws regulating labour relations in the United Kingdom. Most employment law before 1960 was based upon the Law of Contract. Since then there has been a significant expansion primarily due to the "equality movement"[43] and the European Union.[citation needed] Laws are either Acts of Parliament called Statutes, Statutory Regulations (made by a Secretary of State under an Act of Parliament) or Case Law (developed by various courts).		The first significant expansion was the Equal Pay Act of 1970. This act was introduced to bring about pay equality for women in the workplace. Since 1997, changes in UK employment law include enhanced maternity and paternity rights,[44] the introduction of a National Minimum Wage[45] and the Working Time Regulations,[46] which covers working time, rest breaks and the right to paid annual leave. Discrimination law has been tightened, with protection from discrimination now available on the grounds of age, religion or belief and sexual orientation as well as gender, race and disability.		The Fair Labor Standards Act of 1938 set the maximum standard work week to 44 hours. In 1950 this was reduced to 40 hours. A green card entitles immigrants to work, without requirement a separate work permit. Despite the 40-hour standard maximum work week,[47] some lines of work require more than 40 hours. For example, farm workers may work over 72 hours a week, followed by at least 24 hours off.[citation needed] Exceptions to the break period exist for certain harvesting employees, such as those involved in harvesting grapes, tree fruits and cotton.		Professionals, clerical (administrative assistants), technical, and mechanical employees cannot be terminated for refusing to work more than 72 hours in a work week.[48] These ceilings, combined with a competitive job market, often motivate American workers to work more hours. American workers on average take the fewest days off of any developed country.[49]		The Fifth and Fourteenth Amendments of the United States Constitution limit the power of the federal and state governments to discriminate. The private sector is not directly constrained by the Constitution, but several laws, particularly the Civil Rights Act of 1964, limit the private sector discrimination against certain groups. The Fifth Amendment[50] has an explicit requirement that the Federal Government not deprive individuals of "life, liberty, or property", without due process of law and an implicit guarantee that each person receive equal protection of the law. The Fourteenth Amendment[50] explicitly prohibits states from violating an individual's rights of due process and equal protection. Equal protection limits the State and Federal governments' power to discriminate in their employment practices by treating employees, former employees, or job applicants unequally because of membership in a group, like a race, religion or sex. Due process protection requires that employees have a fair procedural process before they are terminated if the termination is related to a "liberty", like the right to free speech, or a property interest.		The National Labor Relations Act, enacted in 1935 as part of the New Deal legislation, guarantees workers the right to form unions and engage in collective bargaining.		The Age Discrimination in Employment Act of 1967 prohibits employment discrimination based on age with respect to employees 40 years of age or older.		Title VII of the Civil Rights Act is the principal federal statute with regard to employment discrimination, prohibiting unlawful employment discrimination by public and private employers, labour organizations, training programmes and employment agencies based on race or colour, religion, sex and national origin. Retaliation is also prohibited by Title VII against any person for opposing any practice forbidden by statute, or for making a charge, testifying, assisting, or participating in a proceeding under the statute. The Civil Rights Act of 1991 expanded the damages available to Title VII cases and granted Title VII plaintiffs the right to jury trial.[51]		The beginnings of halakhic labour law are in the Bible, in which two commandments refer to this subject: The law against delayed wages (Lev. 19:13; Deut. 24:14-15) and the worker's right to eat the employer's crops (Deut. 23:25-26). The Talmudic law - in which labour law is called "laws of worker hiring" - elaborates on many more aspects of employment relations, mainly in Tractate Baba Metzi'a. In some issues the Talamud, following the Tosefta, refers the parties to the customary law: "All is as the custom of the region [postulates]". Modern halakhic labour law developed very slowly. Rabbi Israel Meir Hacohen (the Hafetz Hayim) interprets the worker's right for timely payment in a tendency that clearly favours the employee over the employer, but does not refer to new questions of employment relations. Only in the 1920s we find the first halakhic authority to tackle the questions of trade unions (that could easily be anchored in Talmudic law) and the right of strike (which is quite problematic in terms of Talmudic law). Rabbis A.I Kook and B.M.H. Uziel tend to corporatist settling of labour conflicts, while Rabbi Moshe Feinstein clearly adopts the liberal democratic collective bargaining model. Since the 1940s the halakhic literature on labour law was enriched by books and articles that referred to growing range of questions and basically adopted the liberal democratic approach.		
An occupational injury is bodily damage resulting from working. The most common organs involved are the spine, hands, the head, lungs, eyes, skeleton, and skin. Occupational injuries can result from exposure to occupational hazards (physical, chemical, biological, or psychosocial), such as temperature, noise, insect or animal bites, blood-borne pathogens, aerosols, hazardous chemicals, radiation, and occupational burnout.[1]		While many prevention methods are set in place, injuries may still occur due to poor ergonomics, manual handling of heavy loads, misuse or failure of equipment, exposure to general hazards, and inadequate safety training.						It has been estimated that worldwide there are more than 350,000 workplace fatalities and more than 270 million workplace injuries annually.[2] In 2000 there were approximately 2.9 billion workers worldwide. Occupational injuries resulted in the loss of 3.5 years of healthy life for every 1,000 workers.[3] 300,000 of the occupational injuries resulted in a fatality.[4]		The most common occupations associated with these hazards vary throughout the world depending on the major industries in a particular country. Overall, the most hazardous occupations are in farming, fishing, and forestry.[5] In more developed countries, construction [6] and manufacturing [7] occupations are associated with high rates of spine, hand, and wrist injuries.		In the United States in 2012, 4,383 workers died from job injuries, 92% of which were men,[8] and nearly 3 million nonfatal workplace injuries & illness were reported which cost businesses a collective loss of $198.2 billion and 60 million workdays.[9] In 2007, 5,488 workers died from job injuries, 92% of which were men,[10] and 49,000 died from work-related injuries.[11] NIOSH estimates that 4 million workers in the U.S. in 2007 suffered from non-fatal work related injuries or illnesses.[12]		According to data from the National Institute for Occupational Safety and Health (NIOSH) and the Bureau of Labor Statistics, an average of 15 workers die from traumatic injuries each day in the United States, and an additional 200 workers are hospitalized.[13]		In a study in the state of Washington, injured workers were followed for 14 years to determine the long term effects of work injury on employment. The work injuries resulted in an average of 1.06 years of lost productivity for each of the 31,588 allowed claims.[14]		In the U.S. the Bureau of Labor Statistics makes available extensive statistics on workplace accidents and injuries.[15] For example:				As in the United Kingdom, slips, trips and falls are common and account for 20-40% of disabling occupational injuries.[16] Often these accidents result in a back injury that can persist to a permanent disability. In the United States, a high risk of back injuries occurs in the health care industry. 25% of reported injuries in health care workers in the state of Pennsylvania are for back pain.[17] Among nurses, the prevalence of lower back pain may be as high as 72% mostly as a result of transferring patients.[18] Fortunately, some of these injuries can be prevented with the availability of patient lifts, improved worker training, and allocation of more time to perform work procedures.[19] Another common type of injury is carpal tunnel syndrome associated with overuse of the hands and wrists. Studies on a cohort of newly hired workers have thus far identified forceful gripping, repetitive lifting of > 1 kg, and using vibrating power tools as high risk work activities.[20]		Additionally, noise exposure in the workplace can cause hearing loss, which accounted for 14% of reported occupational illnesses in 2007.[21] Many initiatives have been created to prevent this common workplace injury. For example, the Buy Quiet program encourages employers to purchase tools and machines that produce less noise and the Safe-In-Sound Award was created to recognize companies and program that excel in the area of hearing loss prevention.[22][23]		Accidental injection or needlestick injuries are a common injury that plague agriculture workers and veterinarians. The majority of these injuries are located to the hands or legs, and can result in mild to severe reactions, including possible hospitalization.[24] Due to the wide variety of biologics used in animal agriculture, needlestick injuries can result in bacterial or fungal infections, lacerations, local inflammation, vaccine/antibiotic reactions, amputations, miscarriage, and death.[25] Due to daily human-animal interactions, livestock related injuries are also a prevalent injury of agriculture workers, and are responsible for the majoriy of nonfatal worker injuries on dairy farms. Additionally, approximately 30 people die of cattle and horse-related deaths in the United States annually.[26]		Perhaps the most important personal factor that predisposes to an increased risk is age. In the United States in 1998 17 million workers were over age 55 and by 2018 this population is expected to more than double.[6] Workers in this age group are more likely to suffer from lower back pain that may be worsened by work conditions that normally do not affect a younger worker. Older workers are also more likely by be killed in a construction related fall.[6] They are also at higher risk for injury due to age-related hearing loss,[27] visual impairment,[28] and use of multiple prescription medications[29] that has been linked to higher rates of work injuries.[30] In addition to age, other personal risk factors for injury include obesity [31] particularly its associated risk with back injury, and depression.[32]		Lack of proper education or training can also predispose an individual to an occupational injury. For example, there is limited needlestick injury awareness among agriculture workers, and there is a need for comprehensive programs to prevent needlestick injuries on livestock operations.[25] Proper animal handling techniques and training, or stockmanship, can also decrease the risk of livestock injury. A handler's timing, positioning, speed, direction of movement, and sounds made will affect the behavior of an animal and consequently the safety of the handler.[26] The agriculture industry has begun to focus more on proper education and training, and has made a variety of resources available to producers. For example, organizations like the Upper Midwest Agriculture Safety and Health Center (UMASH) have a variety of informational fact sheets and training videos easily accessible online. Additionally, organizations like Beef Quality Assurance offers stockmanship training seminars and demonstrations.		In the United States, the Occupational Safety and Health Administration (OSHA) sets and enforces national standards for occupational safety across all sectors.[33]		In the United Kingdom in 2013/2014, 133 people were killed at work. Of those 133 people, 89 were employed, while 44 were self-employed. In 2013/2014, an estimated 629,000 injuries occurred at work. Of these injuries 629,000 injuries, 203,000 led to more than 3 days absence from work. Of these, over 148,000 resulted in the victim being absent from work for more than 7 days.[34]		Of all the workplace accidents that resulted in death, the most common were falls from height, contact with moving machinery and being struck by a vehicle. These types of accidents resulted in over half of all recorded deaths.[34]		Slips, trips and falls account for over a third of all injuries that happen at work. Incorrect handling of items was the most common cause of injuries that led to absences from work of more than 7 days.[34] Upper limb injuries represented 47.3% of workplace injuries in 2010-2011, the most common area injured.[35]		In all, over 1,900,000 working days were lost in 2013/2014 due to slips, trips and falls.[34]		The Health & Safety Executive (HSE) prosecuted 582 cases in 2013/2014, with at least one conviction secured in 547 cases (94%).[34]		Local authorities prosecuted a total of 92 cases during the same period, with at least one conviction achieved in 89 cases (97%).[34]		A total of 13,790 notices were issued by the HSE and local authorities, with over £16,700,000 issued in fines.[34]		Unsurprisingly, occupation is the biggest influence on the risk of workplace injuries. Workers new to the job are at a much higher risk of injury than more experienced staff, while shift workers and part-time staff also have a greater risk of being injured at work.[34]		In Taiwan, there were 14,261 occupational injuries recorded in 2010. 45% of these involved trauma to the upper limbs.[35]		There are many methods of preventing or reducing industrial injuries, including anticipation of problems by risk assessment, safety training, control banding, personal protective equipment safety guards, mechanisms on machinery, and safety barriers. In addition, past problems can be analyzed to find their root causes by using a technique called root cause analysis. A 2013 Cochrane review found low-quality evidence showing that inspections, especially focused inspections, can reduce work-related injuries in the long term.[36]		
Equal pay for equal work[1] is the concept of labor rights that individuals in the same workplace be given equal pay.[1] It is most commonly used in the context of sexual discrimination, in relation to the gender pay gap. Equal pay relates to the full range of payments and benefits, including basic pay, non-salary payments, bonuses and allowances. Some countries have moved faster than others in addressing the problem. Since President John F. Kennedy signed the Equal Pay Act of 1963, it has been illegal in the United States to pay men and women working in the same place different salaries for similar work.[2]						As wage-labor became increasingly formalized during the Industrial Revolution, women were often paid less than their male counterparts for the same labor, whether for the explicit reason that they were women or under another pretext. The principle of equal pay for equal work arose at the same time, as part of first-wave feminism, with early efforts for equal pay being associated with nineteenth-century Trade Union activism in industrialised countries: for example, a series of strikes by unionised women in the UK in the 1830s.[3] Pressure from Trade Unions has had varied effects, with trade unions sometimes promoting conservatism. However, following the Second World War, trade unions and the legislatures of industrialized countries gradually embraced the principle of equal pay for equal work; one example of this process is the UK's introduction of the Equal Pay Act 1970 in response both to the Treaty of Rome and the Ford sewing machinists strike of 1968. In recent years European trade unions have generally exerted pressure on states and employers for progress in this direction.[4]		In international human rights law, the statement on equal pay is the 1951 Equal Remuneration Convention, Convention 100 of the International Labour Organisation, a United Nations body. The Convention states that		Equal pay for equal work is also covered by Article 7 of the International Covenant on Economic, Social and Cultural Rights,[5] Article 4 of the European Social Charter,[6] and Article 15 of African Charter on Human and Peoples' Rights.[7] The Constitution of the International Labour Organization also proclaims "the principles of equal remuneration for equal value".[8]		The EEOC's four affirmative defenses allows unequal pay for equal work when the wages are set "pursuant to (i) a seniority system; (ii) a merit system; (iii) a system which measures earnings by quantity or quality of production; or (iv) ... any other factor other than sex."[9] A pay differential due to one of these factors is not in breach of the Convention.		Post-war Europe has seen a fairly consistent pattern in women's participation in the labour market and legislation to promote equal pay for equal work across eastern and western countries.[10]		Some countries now in the EU, including France, Germany, and Poland, had already enshrined the principle of equal pay for equal work in their constitutions before the foundation of the EU (see table below). When the European Economic Community, later the European Union (EU), was founded in 1957, the principle of equal pay for equal work was named as a key principle. Article 141 of the Treaty of Rome says 'each Member State shall ensure that the principle of equal pay for male and female workers for equal work or work of equal value is applied.'[11] While socially progressive, this decision does not necessarily indicate widespread progressive attitudes among the signatories to the treaty:		The EEC's legislation was clarified in 1975 by the binding and directly applicable equal pay directive 75/117/EEC.[13] This prohibited all discrimination on the grounds of sex in relation to pay; this and other directives were integrated into a single Directive in 2006 (2006/54/EC).[14]		At the national level the principle of equal pay is in general fully reflected in the legislation of the 28 EU member states and the additional countries of the European Economic Area (EEA), Iceland, Liechtenstein and Norway. The EU candidate countries of Macedonia and Turkey also adapted their legislation to EU standards.[15] The main national legislation concerning pay equity between men and women for different European countries is as follows.[16]		The first attempt at equal pay legislation in the United States, H.R. 5056, "Prohibiting Discrimination in Pay on Account of Sex," was introduced by Congresswoman Winifred C. Stanley of Buffalo, N.Y. on June 19, 1944.[21] Twenty years later, Legislation passed by the Federal Government of the United States in 1963 made it illegal to pay men and women different wage rates for equal work on jobs that require equal skill, effort, and responsibility and are performed under similar working conditions.[22] One year after passing the Equal Pay Act, Congress passed the 1964 Civil Rights Act. Title VII of this act makes it unlawful to discriminate based on a person's race, religion, color, or sex.[23] Title VII attacks sex discrimination more broadly than the Equal Pay Act extending not only to wages but to compensation, terms, conditions or privileges of employment. Thus with the Equal Pay Act and Title VII, an employer cannot deny women equal pay for equal work; deny women transfers, promotions, or wage increases; manipulate job evaluations to relegate women’s pay; or intentionally segregate men and women into jobs according to their gender.[24]		Since Congress was debating this bill at the same time that the Equal Pay Act was coming into effect, there was concern over how these two laws would interact, which led to the passage of Senator Bennett's Amendment. This Amendment states: "It Shall not be unlawful employment practice under this subchapter for any employer to differentiate upon the basis of sex ... if such differentiation is authorized by the provisions of the [Equal Pay Act]." There was confusion on the interpretation of this Amendment, which was left to the courts to resolve.[25] Thus US federal law now states that "employers may not pay unequal wages to men and women who perform jobs that require substantially equal skill, effort and responsibility, and that are performed under similar working conditions within the same establishment."[9]		In Washington, Governor Evans implemented a pay equity study in 1973 and then another in 1977.[26] The results clearly showed that when comparing male and female dominated jobs there was almost no overlap between the averages for similar jobs and in every sector, a twenty percent gap emerged. For example, a food service worker earned $472 per month, and a Delivery Truck Driver earned $792, though they were both given the same amount of "points" on the scale of comparable worth to the state.[26] Unfortunately for the state, and for the female state workers, his successor Governor Dixie Lee Ray failed to implement the recommendations of the study (which clearly stated women made 20 percent less than men).[27] Thus in 1981, AFSCME filed a sex discrimination complaint with the EEOC against the State of Washington. The District Court ruled that since the state had done a study of sex discrimination in the state, found that there was severe disparities in wages, and had not done anything to ameliorate these disparities, this constituted discrimination under Title VII that was "pervasive and intentional."[28] The Court then ordered the State to pay its over 15,500 women back pay from 1979 based on a 1983 study of comparable worth.[29] This amounted to over $800 million. However, the United States Court of Appeals for the Ninth Circuit overturned this decision, stating that Washington had always required their employees' salaries to reflect the free market, and discrimination was one cause of many for wage disparities. The court stated, "the State did not create the market disparity ... [and] neither law nor logic deems the free market system a suspect enterprise."[30] While the suit was ultimately unsuccessful, it led to state legislation bolstering state workers’ pay. The costs for implementing this equal pay policy was 2.6% of personnel costs for the state.[31]		In Minnesota, the state began considering a formal comparable worth policy in the late 1970s when the Minnesota Task Force of the Council on the Economic Status of Women commissioned Hay Associates to conduct a study. The results were staggering and similar to the results in Washington (there was a 20% gap between state male and female workers pay). Hay Associates proved that in the 19 years since the Equal Pay Act was passed, wage discrimination persisted and had even increased over from 1976 to 1981.[32] Using their point system, they noted that while delivery van drivers and clerk typists were both scaled with 117 points each of “worth” to the state, the delivery van driver (a male dominated profession) was paid $1,382 a month while the clerk typist (a female dominated profession) was paid $1,115 a month.[33] The study also noted that women were severely underrepresented in manager and professional positions; and that state jobs were often segregated by sex. The study finally recommended that the state take several courses of action: 1) establish comparable worth considerations for female- dominated jobs; 2) set aside money to ameliorate the pay inequity; 3) encourage affirmative action for women and minorities and 4) continue analyzing the situation to improve it. The Minnesota Legislature moved immediately in response. In 1983 the state appropriated 21.8 million dollars to begin amending the pay disparities for state employees.[34] From 1982 to 1993, women’s wages in the state increased 10%. According to the Star Tribune, in 2005 women in Minnesota state government made 97 cents to the dollar, ranking Minnesota as one of the most equal for female state workers in the country.		In 2009, President Obama signed the Lilly Ledbetter Fair Pay Act, permitting women to sue employers for unfair pay up to 180 days after receiving an unfair paycheck. On 29 January 2016, he signed an executive order obliging all companies with at least 100 employees to disclose the pay of all workers to the federal government, with breakdowns of pay by race, gender, and ethnicity. The goal is to encourage employers to give equal pay for equal work by increasing transparency.[35]		Under Australia's old centralised wage fixing system, "equal pay for work of equal value" by women was introduced in 1969. Anti-discrimination on the basis of sex was legislated in 1984.[36]		Figures released by the Australian Workplace Gender Equality Agency, an Australian national government agency, show that a woman’s wages, on average, reached a record figure gap of 18.8% less than a man's in November 2014. This is despite a law being passed in 1984 by the Australian legislature making sexual discrimination in the workplace illegal. In November 2011, Australian Prime Minister Julia Gillard announced efforts by the national government to improve salaries of the 150,000 lowest-paid workers in Australia, roughly 120,000 women, by contributing A$2 billion over the next 6 years.[37]		The figures change substantially when other factors are taken into account. Geographically, female workers living in the Australian Capital Territory show the lowest wage gap in the country at 11.7%. The highest wage gap is 25.7% in Western Australia.[38]		A survey by industry shows that the lowest wage gap is in the area of Public Administration and Safety, with a wage gap of 7.2%. The highest wage gap is in the financial and insurance services industry, with a wage gap of 29.6%.[39] The continuation of the wage gap in Australia is influenced by societal factors such as the choices that individual women take when pursuing a career, and in some instances, cultural pressures to pursue predetermined careers. Australian society also relies heavily on women to take on the unpaid caring roles, which reduces the rate of women who are able to participate in the labor market.		As of November 2014, the wage gap in the private sector stands at 22.4%, while in the public sector it is 12.3%. This is primarily due to the method at which salaries are negotiated. In the private sector, wages are decided by individual negotiation, whereas in the public sector, wages are usually arrived at through collective bargaining agreements.		In Canadian usage, the terms pay equity and pay equality are used somewhat differently from in other countries. The two terms refer to distinctly separate legal concepts.		Pay equality, or equal pay for equal work, refers to the requirement that men and women be paid the same if performing the same job in the same organization. For example, a female electrician must be paid the same as a male electrician in the same organization. Reasonable differences are permitted if due to seniority or merit.		Pay equality is required by law in each of Canada’s 14 legislative jurisdictions (ten provinces, three territories, and the federal government). Note that federal legislation applies only to those employers in certain federally regulated industries such as banks, broadcasters, and airlines, to name a few. For most employers, the relevant legislation is that of the respective province or territory.		For federally regulated employers, pay equality is guaranteed under the Canadian Human Rights Act.[40] In Ontario, pay equality is required under the Ontario Employment Standards Act.[41][not in citation given] Every Canadian jurisdiction has similar legislation, although the name of the law will vary.		In contrast, pay equity, in the Canadian context, means that male-dominated occupations and female-dominated occupations of comparable value must be paid the same if within the same employer. The Canadian term pay equity is referred to as “comparable worth” in the US. For example, if an organization’s nurses and electricians are deemed to have jobs of equal importance, they must be paid the same. One way of distinguishing the concepts is to note that pay equality addresses the rights of women employees as individuals, whereas pay equity addresses the rights of female-dominated occupations as groups.		Certain Canadian jurisdictions have pay equity legislation while others do not, hence the necessity of distinguishing between pay equity and pay equality in Canadian usage. For example, in Ontario, pay equality is guaranteed through the Ontario Employment Standards Act[41] while pay equity is guaranteed through the Ontario Pay Equity Act.[42] On the other hand, the three westernmost provinces (British Columbia, Alberta, and Saskatchewan) have pay equality legislation but no pay equity legislation. Some provinces (for example, Manitoba) have legislation that requires pay equity for public sector employers but not for private sector employers; meanwhile, pay equality legislation applies to everyone.		Taiwan legislates the Act of Gender Equality in Employment in 2002. It regulates that an employer must give the same salary to the workers who do the same work.The law prescribes that employers shall not discriminate against employees because of their gender or sexual orientation in the case of paying wages. Employees shall receive equal pay for equal work or equal value. However, if such differentials are the result of seniority systems, award and discipline systems, merit systems or other justifiable reasons of non-sexual or non-sexual-orientation factors, the above-mentioned restriction shall not apply.Employers may not adopt methods of reducing the wages of other employees in order to evade the stipulation of the preceding paragraph.		According to a report released by the American Association of University Women (AAUW), the gender pay gap—which had significantly narrowed since the 1970s—has slowly plateaued in recent years. Compiling data from the Census Bureau, the Department of Education and the Bureau of Labor Statistics, AAUW calculated the median salaries for full-time employment in all 50 states and the District of Columbia. In the U.S., the average woman is paid 23 percent less than the average man. Although down from a 2012 figure of 91 percent, Washington, D.C. maintains the smallest wage gap in the U.S., with the median woman earning 90 percent of the median man ($60,116 vs. $66,754). Also consistent with last year's results, Wyoming came in last with women taking home 64 percent of men's average earnings ($33,152 vs. $51,932). While it remains important to note that geography and local industry have a large influence on differing salaries, there are other major factors that come into play—namely education level, race/ethnicity and age. AAUW analyzed the pay gap by looking at full-time, year-round workers over the age of 15. Beyond comparing salaries of all men to salaries of all women, the report broke down wage imbalances between the sexes along three additional demographics: race/ethnicity, education level and age. Race/Ethnicity Asian-American women had the largest gender wage gap while Hispanic or Latina women's earnings were most comparable to their male counterparts. Education While greater education does increase women's overall earnings, it does not significantly close the gender wage gap. At every academic achievement level, women's median salaries are less than men's by at least 21 percent. The pay gap is significantly linked to factors such as college major and type of job pursued after graduation, although there is still a large part that cannot be explained by career choice. In a 2012 analysis, AAUW found that women are paid only 82 percent of what their male peers are earning just a year after college graduation. Ten years out of college, the gap widens with women earning a mere 69 percent of what men earn.[43]		The Nan Ya PCB corporation's female employee accused that the female employees' salary were less than male's. The employee said that the company existed sex discrimination for long time and it made females' right damage. The trade union bargained with the employer for many years. In order to resolve the dispute, the employer add one thousand allowance to female employees per month. But the employees found it still can't eliminate the gender pay gap.[44]		Criticisms of the principle of equal pay for equal work include criticism of the mechanisms used to achieve it and the methodology by which the gap is measured.[45] Some[who?] believe that government actions to correct gender pay disparity serve to interfere with the system of voluntary exchange. They argue the fundamental issue is that the employer is the owner of the job, not the government or the employee. The employer negotiates the job and pays according to performance, not according to job duties. A private business would not want to lose its best performers by compensating them less and can ill afford paying its lower performers higher because the overall productivity will decline.[46][47] However, the Independent Women's Forum cites another study that prognosticates the wage gap possibly disappearing "when controlled for experience, education, and number of years on the job".[48]		The problem which exists in comparing jobs involving different skills with each other is that there are intangibles besides skills and experience which come into play in determining pay. For example, it may take the same level of skills to be an electrician as it does to be a nurse, but if the electrician is performing their job 200 feet above the base floor of an offshore oil rig, pay will tend to be higher because the attendant risks are likewise higher. Indeed, many argue that the unwillingness of women to work in jobs which are dangerous or otherwise undesirable such as plumbing and coal mining accounts for a significant percentage of the wage gap.[50]		
The following are lists of occupations grouped by category.						
Unemployment Convention, 1919 is an International Labour Organization Convention.		It was established in 1919:		Having decided upon the adoption of certain proposals with regard to the "question of preventing or providing against unemployment",...		As of 2013, the convention had been ratified by 57 states. Of the ratifying states, three have subsequently denounced the treaty.		
Vocational education is education that prepares people to work in a trade, a craft, as a technician, or in professional vocations such as engineering, accountancy, nursing, medicine, architecture, or law. Craft vocations are usually based on manual or practical activities and are traditionally non-academic but related to a specific trade or occupation. Vocational education is sometimes referred to as career education or technical education.[1]		Vocational education can take place at the secondary, post-secondary, further education, and higher education level; and can interact with the apprenticeship system. At the post-secondary level, vocational education is often provided by highly specialized trade, Technical schools, community colleges, colleges of further education UK, universities, Institutes of technology / Polytechnic Institutes.		Until recently, almost all vocational education took place in the classroom, or on the job site, with students learning trade skills and trade theory from accredited professors or established professionals. However, online vocational education has grown in popularity, and made it easier than ever for students to learn various trade skills and soft skills from established professionals in the industry.		Wilhelm von Humboldt's educational model goes beyond vocational training. In a letter to the Prussian king, he wrote: "There are undeniably certain kinds of knowledge that must be of a general nature and, more importantly, a certain cultivation of the mind and character that nobody can afford to be without. People obviously cannot be good craftworkers, merchants, soldiers or businessmen unless, regardless of their occupation, they are good, upstanding and – according to their condition – well-informed human beings and citizens. If this basis is laid through schooling, vocational skills are easily acquired later on, and a person is always free to move from one occupation to another, as so often happens in life."[2] The philosopher Julian Nida-Rümelin criticized discrepancies between Humboldt's ideals and the contemporary European education policy, which narrowly understands education as a preparation for the labor market, and argued that we need to decide between "McKinsey", to describe vocational training, and Humboldt.[3]						In Australia vocational education and training is mostly post-secondary and provided through the vocational education and training (VET) system by registered training organisations. However some senior schools do offer school-based apprenticeships and traineeships for students in years 10, 11 and 12. There were 24 Technical Colleges in Australia but now only 5 independent Trade Colleges remain with three in Queensland; one in Townsville (Tec-NQ), one in Brisbane (Australian Trade College) and one on the Gold Coast (Australian Industry Trade College) and one in Adelaide and Perth. This system encompasses both public, TAFE, and private providers in a national training framework consisting of the Australian Quality Training Framework, Australian Qualifications Framework and Industry Training Packages which define the assessment standards for the different vocational qualifications.		Australia’s apprenticeship system includes both apprenticeships in "traditional" trades and "traineeships" in other more service-oriented occupations. Both involve a legal contract between the employer and the apprentice or trainee and provide a combination of school-based and workplace training. Apprenticeships typically last three to four years, traineeships only one to two years. Apprentices and trainees receive a wage which increases as they progress through the training scheme.[4]		Since the states and territories are responsible for most public delivery and all regulation of providers, a central concept of the VET system is "national recognition", whereby the assessments and awards of any one registered training organisation must be recognised by all others, and the decisions of any state or territory training authority must be recognised by the other states and territories. This allows national portability of qualifications and units of competency.		A crucial feature of the training package (which accounts for about 60% of publicly funded training and almost all apprenticeship training) is that the content of the vocational qualifications is theoretically defined by industry and not by government or training providers. A Training Package is "owned" by one of 11 Industry Skills Councils which are responsible for developing and reviewing the qualifications.		The National Centre for Vocational Education Research or NCVER is a not-for-profit company owned by the federal, state and territory ministries responsible for training. It is responsible for collecting, managing, analysing, evaluating and communicating research and statistics about vocational education and training (VET).		The boundaries between vocational education and tertiary education are becoming more blurred. A number of vocational training providers such as Melbourne Polytechnic, BHI and WAI are now offering specialised bachelor's degrees in specific areas not being adequately provided by universities. Such applied courses include equine studies, winemaking and viticulture, aquaculture, information technology, music, illustration, culinary management and many more.[5]		The largest and the most unified system of vocational education was created in the Soviet Union with the professional`no-tehnicheskoye uchilische and Tehnikum. But it became less effective with the transition of the economies of post-Soviet countries to a market economy.		Education and training is the responsibility of member states, but the single European labour market makes some cooperation on education imperative, including on vocational education and training. The 'Copenhagen process', based on the open method of cooperation between Member States, was launched in 2002 in order to help make vocational education and training better and more attractive to learners throughout Europe. The process is based on mutually agreed priorities that are reviewed periodically. Much of the activity is monitored by Cedefop, the European Centre for the Development of Vocational Training.		There is strong support, particularly in northern Europe, for a shift of resources from university education to vocational training. This is due to the perception that an oversupply of university graduates in many fields of study has aggravated graduate unemployment and underemployment. At the same time, employers are experiencing a shortage of skilled tradespeople.[6]		In Finland, vocational education belongs to secondary education. After the nine-year comprehensive school, almost all students choose to go to either a lukio (high school), which is an institution preparing students for tertiary education, or to a vocational school. Both forms of secondary education last three years, and give a formal qualification to enter university or ammattikorkeakoulu, i.e., Finnish polytechnics. In certain fields (e.g., the police school, air traffic control personnel training), the entrance requirements of vocational schools include completion of the lukio, thus causing the students to complete their secondary education twice.		The education in vocational school is free, and students from low-income families are eligible for a state student grant. The curriculum is primarily vocational, and the academic part of the curriculum is adapted to the needs of a given course. The vocational schools are mostly maintained by municipalities.		After completing secondary education, one can enter higher vocational schools (ammattikorkeakoulu, or AMK) or universities.		It is also possible for a student to choose both lukio and vocational schooling. The education in such cases lasts usually from three to four years.		Vocational education is an important part of the education systems in Austria, Germany, Liechtenstein, Belgium (Deutsche Sprachgemeinschaft Ostbelgien) and Switzerland (including French- and Italian-speaking regions) and one element of the German model.		For example, in Germany a law (the Berufsausbildungsgesetz) was passed in 1969 which regulated and unified the vocational training system and codified the shared responsibility of the state, the unions, associations and Industrie- und Handelskammer (chambers of trade and industry). The system is very popular in modern Germany: in 2001, two-thirds of young people aged under 22 began an apprenticeship, and 78% of them completed it, meaning that approximately 51% of all young people under 22 have completed an apprenticeship. One in three companies offered apprenticeships in 2003; in 2004 the government signed a pledge with industrial unions that all companies except very small ones must take on apprentices.		The vocational education systems in the other German-speaking countries are very similar to the German system and a vocational qualification from one country is generally also recognized in the other states within this area.		In Hong Kong, vocational education is usually for post-secondary 6 students. The Hong Kong Institute of Vocational Education (IVE) provides training in nine different vocational fields, namely: applied science, business administration, child education and community services, construction, design, printing, textiles and clothing, hotel service and tourism studies, information technology, electrical and electronic engineering, and mechanical, manufacturing and industrial engineering.		Normally at the end of elementary school (at age 14) students are directed to one of three types of upper secondary education: one academic track (gymnasium) and two vocational tracks. Vocational secondary schools (szakközépiskola) provide four years of general education and also prepare students for the maturata (school leaving certificate). These schools combine general education with some specific subjects, referred to as pre-vocational education and career orientation. At that point many students enrol in a post-secondary VET programme often at the same institution, to obtain a vocational qualification, although they may also seek entry to tertiary education.		Vocational training schools (szakiskola) initially provide two years of general education, combined with some pre-vocational education and career orientation, they then choose an occupation, and then receive two or three years of vocational education and training focusing on that occupation—such as bricklayer. Students do not obtain the maturata but a vocational qualification at the end of a successfully completed programme. Demand for vocational training schools, both from the labour market and among students, has declined while it has increased for upper secondary schools delivering the maturata.[7]		Vocational training historically has been a subject handled by the Ministry of Labour, other central ministries and various state-level organizations. To harmonize the variations and multiplicity in terms of standards and costs, the National Skills Qualification Framework was launched in December 2013.		The National Skills Qualifications Framework (NSQF) is a competency-based framework that organizes all qualifications according to a series of levels of knowledge, skills and aptitude. These levels, graded from one to ten, are defined in terms of learning outcomes which the learner must possess regardless of whether they are obtained through formal, non-formal or informal learning. NSQF in India was notified on 27 December 2013. All other frameworks, including the NVEQF (National Vocational Educational Qualification Framework) released by the Ministry of HRD, stand superseded by the NSQF.		In November 2014 the new Government in India formed the Ministry of Skill Development & Entrepreneurship. Articulating the need for such a Ministry, the Prime Minister said, [1], "A separate Ministry, which will look after promoting entrepreneurship and skill development, would be created. Even developed countries have accorded priority to promoting skilled manpower".		As a continuation of its efforts to harmonize and consolidate skill development activities across the country, the Government launched the 1st Skill India Development Mission (NSDM) on 15 July 2015. Also launched on the day was the National Policy for Skill Development & Entrepreneurship.		Today all skill development efforts through the Government (Directorate General of Training) and through the Public Private Partnership arm (National Skill Development Corporation) are carried out under the Ministry, through the Skill India Mission.		The Ministry works with various central ministries and departments and the State government in implementing the NSQF across all Government funded projects, based on a five-year implementation schedule for complete convergence.		The involvement of the private sector in various aspects of skill development has enhanced access, quality, and innovative financing models leading to sustainable skill development organizations on the ground.[citation needed] The short-term skill development programs (largely offered by private organizations) combined with the long-term programs offered by the Indian technical institutes (ITIs) complement each other under the larger framework. Credit equivalency, transnational standards, quality assurance and standards are being managed by the Ministry through the National Skill Development Agency (an autonomous body under the Ministry) in close partnership with industry-led sector-specific bodies (Sector Skill Councils) and various line ministries.		India has bilateral collaboration with governments including those of the UK, Australia, Germany, Canada, and the UAE, with the intention of implementing globally acceptable standards and providing the Indian workforce with overseas job mobility.[citation needed]		Japanese vocational schools are known as senmon gakkō. They are part of Japan's higher education system. They are two-year schools that many students study at after finishing high school (although it is not always required that students graduate from high school). Some have a wide range of majors, others only a few majors. Some examples are computer technology, fashion, and English.		Vocational high schools offer programmes in five fields: agriculture, technology/engineering, commerce/business, maritime/fishery, and home economics. In principle, all students in the first year of high school (10th grade) follow a common national curriculum, In the second and third years (11th and 12th grades) students are offered courses relevant to their specialisation. In some programmes, students may participate in workplace training through co-operation between schools and local employers. The government is now piloting Vocational Meister Schools in which workplace training is an important part of the programme. Around half of all vocational high schools are private. Private and public schools operate according to similar rules; for example, they charge the same fees for high school education, with an exemption for poorer families.		The number of students in vocational high schools has decreased, from about half of students in 1995 down to about one-quarter today. To make vocational high schools more attractive, in April 2007 the Korean government changed the name of vocational high schools into professional high schools. With the change of the name the government also facilitated the entry of vocational high school graduates to colleges and universities.		Most vocational high school students continue into tertiary education; in 2007 43% transferred to junior colleges and 25% to university. At tertiary level, vocational education and training is provided in junior colleges (two- and three-year programmes) and at polytechnic colleges. Education at junior colleges and in two-year programmes in polytechnic colleges leads to an Industrial associate degree. Polytechnics also provide one-year programmes for craftsmen and master craftsmen and short programmes for employed workers. The requirements for admission to these institutions are in principle the same as those in the rest of tertiary sector (on the basis of the College Scholastic Aptitude Test) but candidates with vocational qualifications are given priority in the admission process. Junior colleges have expanded rapidly in response to demand and in 2006 enrolled around 27% of all tertiary students.		95% of junior college students are in private institutions. Fees charged by private colleges are approximately twice those of public institutions. Polytechnic colleges are state-run institutions under the responsibility of the Ministry of Labour; government funding keeps student fees much lower than those charged by other tertiary institutions. Around 5% of students are enrolled in polytechnic colleges.[8]		Skills training are no longer depicted as second-class education in Malaysia. There are numerous vocational education centres here including vocational schools (high schools to train skilled students), technic schools (high schools to train future engineers) and vocational colleges all of them under the Ministry of Education. Then there are 33 polytechnics and 86 community colleges under the Ministry of Higher Education; 10 MARA Advanced Skills Colleges, 13 MARA Skills Institutes, 286 GIATMARAs under Majlis Amanah Rakyat (MARA) and 15 National Youth Skills Institutes under Ministry of Youth and Sports. The first vocational institute in Malaysia is the Industrial Training Institute of Kuala Lumpur established in 1964 under the Manpower Department. Other institutes under the same department including 8 Advanced Technology Training Centres, one Centre for Instructor and Advanced Skill Training, one Japan-Malaysia Technical Institute and the other 21 ITIs.		In Mexico, both federal and state governments are responsible for the administration of vocational education. Federal schools are funded by the federal budget, in addition to their own funding sources. The state governments are responsible for the management of decentralised institutions, such as the State Centres for Scientific and Technological Studies (CECyTE) and Institutes of Training for Work (ICAT). These institutions are funded 50% from the federal budget and 50% from the state budget. The state governments also manage and fund "decentralised institutions of the federation", such as CONALEP schools.		Compulsory education (including primary and lower secondary education) finishes at the age of 15 and about half of those aged 15-to-19 are enrolled full-time or part-time in education. All programmes at upper secondary level require the payment of a tuition fee.		The upper secondary vocational education system in Mexico includes over a dozen subsystems (administrative units within the Upper Secondary Education Undersecretariat of the Ministry of Public Education, responsible for vocational programmes) which differ from each other to varying degrees in content, administration, and target group. The large number of school types and corresponding administrative units within the Ministry of Public Education makes the institutional landscape of vocational education and training complex by international standards.		Vocational education and training provided under the Upper Secondary Education Undersecretariat includes three main types of programme:		Nearly all of those leaving lower secondary school enter upper secondary education, and around 50% of them follow one of four vocational programmes; technology, economics, agricultural, personal/social services & health care. These programmes vary from 1 to 4 years (by level; only level 2, 3 and 4 diplomas are considered formal ‘start qualifications’ for successfully entering the labour market). The programmes can be attended in either of two pathways. One either involving a minimum of 20% of school time (apprenticeship pathway; BBL-BeroepsBegeleidende Leerweg) or the other, involving a maximum of 80% schooltime (BOL -BeroepsOpleidende Leerweg). The remaining time in both cases is apprenticeship/work in a company. So in effect, students have a choice out of 32 trajectories, leading to over 600 professional qualifications. BBL-Apprentices usually receive a wage negotiated in collective agreements. Employers taking on these apprentices receive a subsidy in the form of a tax reduction on the wages of the apprentice. (WVA-Wet vermindering afdracht). Level 4 graduates of senior secondary VET may go directly to institutes for Higher Profession Education and Training (HBO-Hoger beroepsonderwijs), after which entering university is a possibility. The social partners participate actively in the development of policy. As of January 1, 2012 they formed a foundation for Co operation Vocational Education and Entrepreneurship (St. SBB – stichting Samenwerking Beroepsonderwijs Bedrijfsleven; www.s-bb.nl). Its responsibility is to advise the Minister on the development of the national vocational education and training system, based on the full consensus of the constituent members (the representative organisations of schools and of entrepreneurship and their centres of expertise). Special topics are Qualification & Examination, Apprenticeships (BPV-Beroepspraktijkvorming) and (labourmarket) Efficiency of VET. The Centres of Expertices are linked to the four vocational education programmes provided in senior secondary VET on the content of VET programmes and on trends and future skill needs. The Local County Vocational Training (MBO Raad www.mboraad.nl) represents the VET schools in this foundation and advise on the quality, operations and provision of VET.[9]		New Zealand is served by 11 Industry Training Organisations (ITO). The unique element is that ITOs purchase training as well as set standards and aggregate industry opinion about skills in the labour market. Industry Training, as organised by ITOs, has expanded from apprenticeships to a more true lifelong learning situation with, for example, over 10% of trainees aged 50 or over. Moreover, much of the training is generic. This challenges the prevailing idea of vocational education and the standard layperson view that it focuses on apprenticeships.		One source for information in New Zealand is the Industry Training Federation.[10] Another is the Ministry of Education.[11]		Polytechnics, Private Training Establishments, Wananga and others also deliver vocational training, amongst other areas.		Nearly all those leaving lower secondary school enter upper secondary education, and around half follow one of nine vocational programmes. These programmes typically involve two years in school followed by two years of apprenticeship in a company. The first year provides general education alongside introductory knowledge of the vocational area. During the second year, courses become more trade-specific.		Apprentices receive a wage negotiated in collective agreements ranging between 30% and 80% of the wage of a qualified worker; the percentage increase over the apprenticeship period. Employers taking on apprentices receive a subsidy, equivalent to the cost of one year in school. After the two years vocational school programme some students opt for a third year in the ‘general’ programme as an alternative to an apprenticeship. Both apprenticeship and a third year of practical training in school lead to the same vocational qualifications. Upper secondary VET graduates may go directly to Vocational Technical Colleges, while those who wish to enter university need to take a supplementary year of education.		The social partners participate actively in the development of policy. The National Council for Vocational Education and Training advises the Minister on the development of the national vocational education and training system. The Advisory Councils for Vocational Education and Training are linked to the nine vocational education programmes provided in upper secondary education and advise on the content of VET programmes and on trends and future skill needs. The National Curriculum groups assist in deciding the contents of the vocational training within the specific occupations. The Local County Vocational Training Committees advise on the quality, provision of VET and career guidance.[12]		In Paraguay, vocational education is known as Bachillerato Técnico and is part of the secondary education system. These schools combine general education with some specific subjects, referred to as pre-vocational education and career orientation. After nine years of Educación Escolar Básica (Primary School), the student can choose to go to either a Bachillerato Técnico (Vocational School) or a Bachillerato Científico (High School). Both forms of secondary education last three years, and are usually located in the same campus called Colegio.		After completing secondary education, one can enter to the universities. It is also possible for a student to choose both Técnico and Científico schooling.		Vocational training from Agricultural subjects to ICT related subjects are available in Sri Lanka. In 2005 the Ministry of Vocational and Technical Training (MVTT) introduced the National Vocational Qualifications (NVQ) framework which was an important milestone for the education, economic and social development of Sri Lanka. The NVQ framework consists of seven levels of instruction. NVQ levels 1 to 4 are for craftsmen designation and successful candidates are issued with National certificates. NVQ levels 5 and 6 are Diploma level, whereas Level 7 is for degree equivalent qualification.		Training courses are provided by many institutions island wide. All training providers (public and private) must obtain institutional registration and course accreditation from the Tertiary and Vocational Education Commission (TVEC).In order to obtain registration institutions must satisfy specific criteria: infrastructure, basic services, tools and equipment, quality of instruction and staff, based on curriculum and syllabus, and quality of management and monitoring systems.		Government Ministries and Agencies involved in Vocational Training are The Ministry of Vocational and Technical Training (MVTT), The Tertiary and Vocational Education Commission (TVEC), The National Apprentice and Industrial Training Authority (NAITA), The Department of Technical Education and Training (DTET), The Vocational Training Authority (VTA) and the National Youth Services Council (NYSC).[13]		Nearly all of those leaving compulsory schooling immediately enter upper secondary schools, and most complete their upper secondary education in three years. Upper secondary education is divided into 13 vocationally oriented and 4 academic national programmes. Slightly more than half of all students follow vocational programmes. All programmes offer broad general education and basic eligibility to continue studies at the post-secondary level. In addition, there are local programmes specially designed to meet local needs and ‘individual’ programmes.		A 1992 school reform extended vocational upper secondary programmes by one year, aligning them with three years of general upper secondary education, increasing their general education content, and making core subjects compulsory in all programmes. The core subjects (which occupy around one-third of total teaching time in both vocational and academic programmes) include English, artistic activities, physical education and health, mathematics, natural science, social studies, Swedish or Swedish as a second language, and religious studies. In addition to the core subjects, students pursue optional courses, subjects which are specific to each programme and a special project.		Vocational programmes include 15 weeks of workplace training (Arbetsplatsförlagd utbildning – APU) over the three-year period. Schools are responsible for arranging workplace training and verifying its quality. Most municipalities have advisory bodies: programme councils (programmråd) and vocational councils (yrkesråd) composed of employers’ and employees’ representatives from the locality. The councils advise schools on matters such as provision of workplace training courses, equipment purchase and training of supervisors in APU.[8]		Nearly two thirds of those entering upper secondary education enter the vocational education and training system. At this level, vocational education and training is mainly provided through the ‘dual system’. Students spend some of their time in a vocational school; some of their time doing an apprenticeship at a host company; and for most programmes, students attend industry courses at an industry training centre to develop complementary practical skills relating to the occupation at hand. Common patterns are for students to spend one- two days per week at the vocational school and three-four days doing the apprenticeship at the host company; alternatively they alternate between some weeks attending classes at the vocational school and some weeks attending industry courses at an industry training centre. A different pattern is to begin the programme with most of the time devoted to in-school education and gradually diminishing the amount of in-school education in favour of more in-company training.		Switzerland draws a distinction between vocational education and training (VET) programmes at upper-secondary level, and professional education and training (PET) programmes, which take place at tertiary B level. In 2007, more than half of the population aged 25–64 had a VET or PET qualification as their highest level of education. In addition, universities of applied sciences (Fachhochschulen) offer vocational education at tertiary A level. Pathways enable people to shift from one part of the education system to another.[14]		Students in Turkey may choose vocational high schools after completing the 8-year-long compulsory primary and secondary education. Vocational high school graduates may pursue two year-long polytechnics or may continue with a related tertiary degree.		According to a survey by OECD, 38% of 15-year-old students attend vocational study programmes that are offered by Anatolian vocational, Anatolian technical, and technical high schools.[15]		Municipalities in Turkey also offer vocational training. The metropolitan municipality of Istanbul, the most populous city in Turkey, offers year long free vocational programs in a wide range of topics through ISMEK,[16] an umbrella organization formed under the municipality.		The first "Trades School" in the UK was Stanley Technical Trades School (now Harris Academy South Norwood) which was designed, built and set up by William Stanley. The initial idea was thought of in 1901, and the school opened in 1907.[17]		The system of vocational education in the UK initially developed independently of the state, with bodies such as the RSA and City & Guilds setting examinations for technical subjects. The Education Act 1944 made provision for a Tripartite System of grammar schools, secondary technical schools and secondary modern schools, but by 1975 only 0.5% of British senior pupils were in technical schools, compared to two-thirds of the equivalent German age group.[18]		Successive recent British Governments have made attempts to promote and expand vocational education. In the 1970s, the Business And Technology Education Council was founded to confer further and higher education awards, particularly to further education colleges in the United Kingdom. In the 1980s and 1990s, the Conservative Government promoted the Youth Training Scheme, National Vocational Qualifications and General National Vocational Qualifications. However, youth training was marginalised as the proportion of young people staying on in full-time education increased.[18]		In 1994, publicly funded Modern Apprenticeships were introduced to provide "quality training on a work-based (educational) route".[19] Numbers of apprentices have grown in recent years and the Department for Children, Schools and Families has stated its intention to make apprenticeships a "mainstream" part of England's education system.[20]		In the UK some higher engineering-technician positions that require 4–5 years' apprenticeship require academic study to HNC / HND or higher City & Guilds level. Apprenticeships are increasingly recognised as the gold standard for work-based training. There are four levels of apprenticeship available for those aged 16 and over:		Apprentices work towards work-based learning qualifications such as a Level 2 Competence Qualification, Functional Skills and, in most cases, a relevant knowledge-based qualification.		Apprentices work towards work-based learning such as a Level 3 Competence Qualification, Functional Skills and, in most cases, a relevant knowledgebased qualification. They can take four years to complete.		Apprentices work towards work-based learning qualifications such as a Level 4 and 5 Competence Qualification, Functional Skills and, in some cases, a knowledge-based qualification such as a Foundation Degree. They can take between four and five years to complete, depending on the level at which an apprentice enrolls.		They are similar to higher apprenticeships, but differ in that they provide an opportunity to gain a full bachelor’s (Level 6) or master's degree (Level 7). The courses are designed in partnership with employers, with part-time study taking place at a university. They can take between four and six years to complete, depending on the level of the course, and the level of entry.		Main article: TVET (Technical and Vocational Education and Training)		TVET (Technical and Vocational Education and Training) is education and training that provides the necessary knowledge and skills for employment.[21] It uses many forms of education including formal, non-formal and informal learning,[22] and is considered to be a crucial vehicle for social equity and inclusion, as well as for the sustainability of development. TVET, together with literacy and higher education, is one of three priority subsectors for UNESCO in its work to foster inclusive and equitable quality education and lifelong learning opportunities for all.[23]		The development and definition of TVET is one that parallels other types of education and training, such as Vocational Education; however, TVET was officiated on an international level as a better term to describe the field, and therefore is likewise used as an umbrella term to encompass education and training activities such as Vocational Education.[21]		To learn how to add open-license text to Wikipedia articles, please see Wikipedia:Adding open license text to Wikipedia.		
The Occupational Outlook Handbook is a publication of the United States Department of Labor's Bureau of Labor Statistics that includes information about the nature of work, working conditions, training and education, earnings and job outlook for hundreds of different occupations in the United States. It is released biennially with a companion publication, the Career Guide to Industries and is available free of charge from the Bureau of Labor Statistics' website. The 2012–13 edition was released in November 2012 and the 2014–15 edition in March 2014.		Because it is a work by the United States federal government, the Handbook is not under copyright and is reproduced in various forms by other publishers, often with additional information or features.[1]		The first edition was published in 1948.[2]				
A tradesman, tradesperson or skilled tradesman refers to a worker who specializes in a particular occupation that requires work experience, on-the-job training, and often formal vocational education, but often not a bachelor's degree.						In the Victorian era:		One study of Caversham, New Zealand at the turn of the century notes that a skilled trade was considered a trade that required an apprenticeship to entry.[2] Skilled tradesmen worked either in traditional handicraft workshops or newer factories that emerged during the Industrial Revolution.[2] Traditional handicraft roles included, for example: "sail-maker, candle-maker, cooper, jappaner, lapidary and taxidermist, canister-maker, furrier, cap-maker, dobbin-maker, french-polisher, baker, miller, brewer, confectioner, watch-maker, tinsmith, glazier, maltster, wood-turner, saddler, shipwright, scale-maker, engraver and cutler."[2]		Tradesmen are contrasted with unskilled workers (laborers), agricultural workers, and professionals (those in the learned professions).[3] Skilled tradesmen are distinguished:		There is no definitive list of modern skilled trades, as definitions vary, with some lists being broader than others.[7]		A June 2013 report by the Michigan Department of Technology, Management and Budget, however, generated the following list of trades (divided into industrial, construction, and service skilled trades), along with their Standard Occupational Classification System code:[7]		A British study found that, after taking student loan repayments into account, a higher apprenticeship (at level 5 on the Qualifications and Credit Framework, equivalent to a foundation degree) delivered higher lifetime median earnings than a degree from a university outside the Russell Group. Despite this, polling for the report found that apprenticeships have a lower perceived value than bachelor's degrees.[8]		According to data released from the United States Bureau of Labor Statistics, lists the wages and expected job openings of skilled trades with educational requirements ranging from an associate degree to a high school diploma.[9]		
Reflective practice is the ability to reflect on one's actions so as to engage in a process of continuous learning.[1] According to one definition it involves "paying critical attention to the practical values and theories which inform everyday actions, by examining practice reflectively and reflexively. This leads to developmental insight".[2] A key rationale for reflective practice is that experience alone does not necessarily lead to learning; deliberate reflection on experience is essential.[3][4]		Reflective practice can be an important tool in practice-based professional learning settings where people learn from their own professional experiences, rather than from formal learning or knowledge transfer. It may be the most important source of personal professional development and improvement. It is also an important way to bring together theory and practice; through reflection a person is able to see and label forms of thought and theory within the context of his or her work.[5] A person who reflects throughout his or her practice is not just looking back on past actions and events, but is taking a conscious look at emotions, experiences, actions, and responses, and using that information to add to his or her existing knowledge base and reach a higher level of understanding.[6]						Donald Schön's 1983 book The Reflective Practitioner introduced concepts such as reflection-on-action and reflection-in-action which explain how professionals meet the challenges of their work with a kind of improvisation that is improved through practice.[1] However, the concepts underlying reflective practice are much older. Earlier in the 20th century, John Dewey was among the first to write about reflective practice with his exploration of experience, interaction and reflection.[7] Soon thereafter, other researchers such as Kurt Lewin and Jean Piaget were developing relevant theories of human learning and development.[8] Some scholars have claimed to find precursors of reflective practice in ancient texts such as Buddhist teachings[9] and the Meditations of Stoic philosopher Marcus Aurelius.[10]		Central to the development of reflective theory was interest in the integration of theory and practice, the cyclic pattern of experience and the conscious application of lessons learned from experience. Since the 1970s, there has been a growing literature and focus around experiential learning and the development and application of reflective practice.		As adult education professor David Boud and his colleagues explained: "Reflection is an important human activity in which people recapture their experience, think about it, mull it over and evaluate it. It is this working with experience that is important in learning."[11] When a person is experiencing something, he or she may be implicitly learning; however, it can be difficult to put emotions, events, and thoughts into a coherent sequence of events. When a person rethinks or retells events, it is possible to categorize events, emotions, ideas, etc., and to compare the intended purpose of a past action with the results of the action. Stepping back from the action permits critical reflection on a sequence of events.[6]		The emergence in more recent years of blogging has been seen as another form of reflection on experience in a technological age.[12]		Many models of reflective practice have been created to guide reasoning about action.		Terry Borton's 1970 book Reach, Touch, and Teach popularized a simple learning cycle inspired by Gestalt therapy composed of three questions which ask the practitioner: What, So what, and Now what?[13] Through this analysis, a description of a situation is given which then leads into the scrutiny of the situation and the construction of knowledge that has been learnt through the experience. Subsequently, practitioners reflect on ways in which they can personally improve and the consequences of their response to the experience. Borton's model was later adapted by practitioners outside the field of education, such as the field of nursing and the helping professions.[14]		Learning theorist David A. Kolb was highly influenced by the earlier research conducted by John Dewey and Jean Piaget. Kolb's reflective model highlights the concept of experiential learning and is centered on the transformation of information into knowledge. This takes place after a situation has occurred, and entails a practitioner reflecting on the experience, gaining a general understanding of the concepts encountered during the experience, and then testing these general understandings in a new situation. In this way, the knowledge that is formed from a situation is continuously applied and reapplied, building on a practitioner's prior experiences and knowledge.[15]		Management researchers Chris Argyris and Donald Schön pioneered the idea of single-loop learning and double-loop learning in 1978. Their theory was built around the recognition and correction of a perceived fault or error.[16] Single-loop learning is when a practitioner or organisation, even after an error has occurred and a correction is made, continues to rely on current strategies, techniques or policies when a situation again comes to light. Double-loop learning involves the modification of objectives, strategies or policies so that when a similar situation arises a new framing system is employed.[17][page needed]		Schön claimed to derive the notions of "reflection-on-action, reflection-in-action, responding to problematic situations, problem framing, problem solving, and the priority of practical knowledge over abstract theory" from the writings of John Dewey, although education professor Harvey Shapiro has argued that Dewey's writings offer "more expansive, more integrated notions of professional growth" than do Schön's.[18]		Schon advocated 2 types of reflective practice. Firstly, reflection-on-action, which involves reflecting on an experience that you have already had, or an action that you have already taken, and considering what could have been done differently, as well as looking at the positives from that interaction. The other type of reflection Schon notes is reflection-in-action, or reflecting on your actions as you are doing them, and considering issues like best practice throughout the process.		For Schön, professional growth really begins when a person starts to view things with a critical lens, by doubting his or her actions. Doubt brings about a way of thinking that questions and frames situations as "problems". Through careful planning and systematic elimination of other possible problems, doubt is settled, and people are able to affirm their knowledge of the situation. Then people are able to think about possible situations and their outcomes, and deliberate about whether they carried out the right actions.[citation needed]		Learning researcher Graham Gibbs discussed the use of structured debriefing to facilitate the reflection involved in Kolb's experiential learning cycle. Gibbs presents the stages of a full structured debriefing as follows:[19]		Gibbs' suggestions are often cited as "Gibbs' reflective cycle" or "Gibbs' model of reflection", and simplified into the following six distinct stages to assist in structuring reflection on learning experiences:[20]		Professor of nursing Christopher Johns designed a structured mode of reflection that provides a practitioner with a guide to gain greater understanding of his or her practice.[21] It is designed to be carried out through the act of sharing with a colleague or mentor, which enables the experience to become learnt knowledge at a faster rate than reflection alone.[22]		Johns highlights the importance of experienced knowledge and the ability of a practitioner to access, understand and put into practice information that has been acquired through empirical means. Reflection occurs though "looking in" on one's thoughts and emotions and "looking out" at the situation experienced. Johns draws on the work of Barbara Carper to expand on the notion of "looking out" at a situation.[23] Five patterns of knowing are incorporated into the guided reflection: the aesthetic, personal, ethical, empirical and reflexive aspects of the situation. Johns' model is comprehensive and allows for reflection that touches on many important elements.[24]		Adult education scholar Stephen Brookfield proposed that critically reflective practitioners constantly research their assumptions by seeing practice through four complementary lenses: the lens of their autobiography as learners of reflective practice, the lens of other learners' eyes, the lens of colleagues' experiences, and the lens of theoretical, philosophical and research literature.[25] Reviewing practice through these lenses makes us more aware of the power dynamics that infuse all practice settings. It also helps us detect hegemonic assumptions—assumptions that we think are in our own best interests, but actually work against us in the long run.[25] Brookfield argued that these four lenses will reflect back to us starkly different pictures of who we are and what we do.		Reflective practice has been described as an unstructured or semi-structured approach directing learning, and a self-regulated process commonly used in health and teaching professions, though applicable to all professions.[1][11][26] Reflective practice is a learning process taught to professionals from a variety of disciplines, with the aim of enhancing abilities to communicate and making informed and balanced decisions. Professional associations such as the American Association of Nurse Practitioners are recognizing the importance of reflective practice and require practitioners to prepare reflective portfolios as a requirement to be licensed, and for yearly quality assurance purposes.[citation needed]		The concept of reflective practice has found wide application in the field of education, for learners, teachers and those who teach teachers.		Students can benefit from engaging in reflective practice as it can foster the critical thinking and decision making necessary for continuous learning and improvement.[27] When students are engaged in reflection, they are thinking about how their work meets established criteria; they analyze the effectiveness of their efforts, and plan for improvement.[27] Rolheiser and et al. (2000) assert that "Reflection is linked to elements that are fundamental to meaningful learning and cognitive development: the development of metacognition – the capacity for students to improve their ability to think about their thinking; the ability to self-evaluate - the capacity for students to judge the quality of their work based on evidence and explicit criteria for the purpose of doing better work; the development of critical thinking, problem-solving, and decision-making; and the enhancement of teacher understanding of the learner." (p 31-32)		When teachers teach metacognitive skills, it promotes student self-monitoring and self-regulation that can lead to intellectual growth, increase academic achievement, and support transfer of skills so that students are able to use any strategy at any time and for any purpose.[28] Guiding students in the habits of reflection requires teachers to approach their role as that of "facilitator of meaning-making" – they organize instruction and classroom practice so that students are the producers, not just the consumers, of knowledge.[29] Rolheiser and colleagues (2000) state that "When students develop their capacity to understand their own thinking processes, they are better equipped to employ the necessary cognitive skills to complete a task or achieve a goal. Students who have acquired metacognitive skills are better able to compensate for both low ability and insufficient information." (p. 34)		The Ontario Ministry of Education (2007)[30] describes many ways in which educators can help students acquire the skills required for effective reflection and self-assessment, including: modelling and/or intentionally teaching critical thinking skills necessary for reflection and self-assessment practices; addressing students' perceptions of self-assessment; engaging in discussion and dialogue about why self-assessment is important; allowing time to learn self-assessment and reflection skills; providing many opportunities to practice different aspects of the self-assessment and reflection process; and ensuring that parents/guardians understand that self-assessment is only one of a variety of assessment strategies that is utilized for student learning.		The concept of reflective practice is now widely employed in the field of teacher education and teacher professional development and many programmes of initial teacher education claim to espouse it.[3] In education, a minimalist understanding of reflective practice is that it refers to the process of the educator studying his or her own teaching methods and determining what works best for the students and the consideration of the ethical consequences of classroom procedures on students; a broader understanding would accept that it also involves questioning the organisational, social and political context in which the teaching takes place.[26] Education professor Hope Hartman has described reflective practice in education as teacher metacognition.[31]		There is broad consensus that teaching effectively requires a reflective approach.[32][33][34] However, reflective practice "is a term that carries diverse meaning"[3] and about which there is not complete consensus.[35][36] Teaching and learning are complex processes, and there is not one right approach. Reflecting on different approaches to teaching, and reshaping the understanding of past and current experiences, can lead to improvement in teaching practices.[35] Schön's reflection-in-action can help teachers explicitly incorporate into their decision-making the professional knowledge that they gain from their experience in the classroom.[37]		As professor of education Barbara Larrivee argues, reflective practice moves teachers from their knowledge base of distinct skills to a stage in their careers where they are able to modify their skills to suit specific contexts and situations, and eventually to invent new strategies.[26] In implementing a process of reflective practice teachers will be able to move themselves, and their schools, beyond existing theories in practice.[35] Larrivee concludes that teachers should "resist establishing a classroom culture of control and become a reflective practitioner, continuously engaging in a critical reflection, consequently remaining fluid in the dynamic environment of the classroom".[26]		Video recordings of classroom activities have been used to help trainee teachers / education interns develop more detailed reflective practice.[38]		According to physiotherapists Colin Paterson and Judith Chapman, reflection or learning from experience is key to staying accountable, and maintaining and developing aptitude throughout a teacher's practice.[6] Without reflection, teachers are not able to look objectively at their actions or take into account the emotions, experience, or consequences of actions to improve their practice. It is argued that, through the process of reflection, teachers are held accountable to the standards of practice for teaching, such as those in Ontario: commitment to students and student learning, professional knowledge, professional practice, leadership in learning communities, and ongoing professional learning.[39] Through reflective practice, teachers look back on their practice and reflect on how they have supported students by treating them "equitably and with respect and are sensitive to factors that influence individual student learning".[39] By doing this, teachers ask themselves: "Have I to the best of my abilities supported student learning, and provided all of my students with an entry point into learning?" Through reflection, and sharing their reflection, teachers show strong leadership because they show that they are willing to learn from their mistakes and improve their practice for everyone affected by it.[39]		For students to acquire necessary skills in reflection, their teachers need to be able to teach and model reflective practice (see above); similarly, teachers themselves need to have been taught reflective practice during their initial teacher education, and to continue to develop their reflective skills throughout their career.		However, Mary Ryan has noted that students are often asked to "reflect" without being taught how to do so,[40] or without being taught that different types of reflection are possible; they may not even receive a clear definition or rationale for reflective practice.[41] Many new teachers do not know how to transfer the reflection strategies they learned in college to their classroom teaching.[42]		Some writers have advocated that reflective practice needs to be taught explicitly to student teachers because it is not an intuitive act;[43][40] it is not enough for teacher educators to provide student teachers with "opportunities" to reflect: they must explicitly "teach reflection and types of reflection" and "need explicitly to facilitate the process of reflection and make transparent the metacognitive process it entails".[44] Larrivee notes that (student) teachers require "carefully constructed guidance" and "multifaceted and strategically constructed interventions" if they are to reflect effectively on their practice.[26]		Rod Lane and colleageues list strategies by which teacher educators can promote a habit of reflective practice in pre-service teachers, such as discussions of a teaching situation, reflective interviews or essays about one's teaching experiences, action research, or journaling or blogging.[45]		Neville Hatton and David Smith, in a brief literature review, conclude that teacher education programmes do use a wide range of strategies with the aim of encouraging students teachers to reflect (e.g. action research, case studies, video-recording or supervised practicum experiences), but that "there is little research evidence to show that this [aim] is actually being achieved".[46]		The implication of all this is that teacher educators must also be highly skilled in reflective practice. Andrea Gelfuso and Danielle Dennis, in a report on a formative experiment with student teachers, suggest that teaching how to reflect requires teacher educators to possess and deploy specific competences.[47] However, Janet Dyment and Timothy O'Connell, in a small-scale study of experienced teacher educators, noted that the teacher educators they studied had received no training in using reflection themselves, and that they in turn did not give such training to their students; all parties were expected to know how to reflect.[48]		Many writers advocate for teacher educators themselves to act as models of reflective practice.[49][50] This implies that the way that teacher educators teach their students needs to be congruent with the approaches they expect their students to adopt with pupils; teacher educators should not only model the way to teach, but should also explain why they have chosen a particular approach whilst doing so, by reference to theory; this implies that teacher educators need to be aware of their own tacit theories of teaching and able to connect them overtly to public theory.[51] However, some teacher educators do not always "teach as they preach";[52] they base their teaching decisions on "common sense" more than on public theory[53] and struggle with modelling reflective practice.[49]		Tom Russell, in a reflective article looking back on 35 years as teacher educator, concurs that teacher educators rarely model reflective practice, fail to link reflection clearly and directly to professional learning, and rarely explain what they mean by reflection, with the result that student teachers may complete their initial teacher education with "a muddled and negative view of what reflection is and how it might contribute to their professional learning".[54] For Russell, these problems result from the fact that teacher educators have not sufficiently explored how theories of reflective practice relate to their own teaching, and so have not made the necessary "paradigmatic changes" which they expect their students to make.[54]		Reflective practice is viewed as an important strategy for health professionals who embrace lifelong learning. Due to the ever-changing context of healthcare and the continual growth of medical knowledge, there is a high level of demand on healthcare professionals' expertise. Due to this complex and continually changing environment, healthcare professionals could benefit from a program of reflective practice.[55]		Adrienne Price explained that there are several reasons why a healthcare practitioner would engage in reflective practice: to further understand one's motives, perceptions, attitudes, values, and feelings associated with client care; to provide a fresh outlook to practice situations and to challenge existing thoughts, feelings, and actions; and to explore how the practice situation may be approached differently.[56] In the field of nursing there is concern that actions may run the risk of habitualisation, thus dehumanising patients and their needs.[57] In using reflective practice, nurses are able to plan their actions and consciously monitor the action to ensure it is beneficial to their patient.[57]		The act of reflection is seen as a way of promoting the development of autonomous, qualified and self-directed professionals, as well as a way of developing more effective healthcare teams.[58] Engaging in reflective practice is associated with improved quality of care, stimulating personal and professional growth and closing the gap between theory and practice.[59][page needed] Medical practitioners can combine reflective practice with checklists (when appropriate) to reduce diagnostic error.[60]		Activities to promote reflection are now being incorporated into undergraduate, postgraduate and continuing medical education across a variety of health professions.[61] Professor of medical education Karen Mann and her colleagues found through a 2009 literature review that in practising professionals the process of reflection appears to include a number of different aspects, and practicing professionals vary in their tendency and ability to reflect. They noted that the evidence to support curricular interventions and innovations promoting reflective practice remains largely theoretical.[61]		Samantha Davies identified benefits as well as limitations to reflective practice:[62]		Benefits to reflective practice include:		Limitations to reflective practice include:		The use of reflective practice in environmental management, combined with system monitoring, is often called adaptive management.[63] There is some criticism that traditional environmental management, which simply focuses on the problem at hand, fails to integrate into the decision making the wider systems within which an environment is situated.[64] While research and science must inform the process of environmental management, it is up to the practitioner to integrate those results within these wider systems.[65] In order to deal with this and to reaffirm the utility of environmental management, Bryant and Wilson propose that a "more reflective approach is required that seeks to rethink the basic premises of environmental management as a process".[64] This style of approach has been found to be successful in sustainable development projects where participants appreciated and enjoyed the educational aspect of utilising reflective practice throughout. However, the authors noted the challenges with melding the "circularity" of reflective practice theory with the "doing" of sustainability.[66]		Reflective practice provides a development opportunity for those in leadership positions. Managing a team of people requires a delicate balance between people skills and technical expertise, and success in this type of role does not come easily. Reflective practice provides leaders with an opportunity to critically review what has been successful in the past and where improvement can be made.		Reflective learning organizations have invested in coaching programs for their emerging and established leaders.[67] Leaders frequently engage in self-limiting behaviours because of their over-reliance on their preferred ways of reacting and responding.[68] Coaching can help support the establishment of new behaviours, as it encourages reflection, critical thinking and transformative learning. Adults have acquired a body of experience throughout their life, as well as habits of mind that define their world.[69] Coaching programs support the process of questioning and potentially rebuilding these pre-determined habits of mind. The goal is for leaders to maximize their professional potential, and in order to do this, there must be a process of critical reflection on current assumptions.[70]		Reflective practice can help any individual to develop personally, and is useful for professions other than those discussed above. It allows professionals to continually update their skills and knowledge and consider new ways to interact with their colleagues. David Somerville and June Keeling suggested eight simple ways that professionals can practice more reflectively:[71]		
Control fraud occurs when a trusted person in a high position of responsibility in a company, corporation, or state subverts the organization and engages in extensive fraud for personal gain. The term Control fraud was coined by William K. Black to refer both to the acts of fraud and to the individuals who commit them.						The concept of control fraud is based on the observation that the CEO of a company is uniquely placed to remove the checks and balances on fraud within a company such as through the use of selective hiring and firing. These tactics can position the executive in a way that allows him or her to engage in accountancy fraud and embezzle money, hide shortfalls or otherwise defraud investors, shareholders, or the public at large. A control fraud will often obtain "investments that have no readily ascertainable market value",[1] and then shop for appraisers that will assign unrealistically high values and auditing firms that will bless the fraudulent accounting statements.[2]		Some control frauds are reactive in the sense that they turn to fraud only after concluding that the business will fail.[3] Opportunistic control frauds, by contrast, are attracted to a criminogenic environment where it is harder to detect fraud, e.g., as a result of deregulation.[4]		An example would be when an insolvent company publishes accounts showing massive profits. This will cause the stock to rise beyond its actual value, and those exercising the control fraud will cash in their stocks before the reality is known by others.[5] Additionally, companies can lobby for changes to weaken the law or accompanying regulation. This can be particularly effective with large campaign contributors like Charles Keating, who with other control frauds in the United States League of Savings Institutions, was able to get his own people placed on the board of the primary regulatory agency, the Federal Home Loan Bank Board (FHLBB). With the assistance of people like Speaker of the House Jim Wright and the Keating Five, he was able to convert Lincoln Savings and Loan Association into a Ponzi scheme, making millions for himself, while suppressing the investigative and regulatory functions of the FHLBB. Eventually, the Ponzi collapsed, as all Ponzis must, but with a massive cost to the taxpayers and unsecured investors.		Control fraud can also occur in a political situation, for example by the leader of a country who can use their position to embezzle public funds and turn the country into a kleptocracy.		Examples of control fraud include Enron, the savings and loan crisis, and Ponzi schemes such as that of Bernard Madoff.		Black, William K. (2005). The Best Way to Rob a Bank Is to Own One. University of Texas Press. ISBN 0-292-72139-0. 		
Job losses caused by the Great Recession refers to jobs that have been lost worldwide within people since the start of the Great Recession. In the US, job losses have been going on since December 2007, and it accelerated drastically starting in September 2008 following the bankruptcy of Lehman Brothers.[1] By February 2010, the American economy was reported to be more shaky than the economy of Canada. Many service industries (particularly in countries that either have the same unemployment rate as the United States or greater) have reported dropping their prices in order to maximize profit margins (looking to make use of any price elasticity of demand in their market segments). This is an era in which employment is becoming unstable, and in which being either underemployed or unemployed is a common part of life for many people.						Note: Job losses in June and July 2010 are largely attributed to US census worker jobs lost. Private sector jobs have increased during those months.		Since the start of the recession, 8.8 million jobs have been lost, according to the Bureau of Labor Statistics.[11]		In the U.S., jobs paying between $14 and $21 per hour made up about 60% those lost during the recession, but such mid-wage jobs have comprised only about 27% of jobs gained during the recovery through mid-2012. In contrast, lower-paying jobs constituted about 58% of the jobs regained.[12]		Drastic job loss in Canada started later than in the US. Some months in 2008 had job growth, such as September, while others such as July had losses. Due to the collapse of the American car industry at the same time as a strong Canadian dollar achieved parity +10% against a poorly-performing US dollar, the cross-border manufacturing industry has been disproportionately affected throughout.[13]		(1) 37,000 jobs are gained in the self-employment category[25]		While job creation has increased in the past three months, most Canadians still complain about people getting laid off from work. However, a growing amount of these layoffs are considered for seasonal purposes and has little or no relation to the recession. Excluding Stelco employees, most laid off workers have six months to acquire a job while collecting dinosaur insurance. After that, they must go on welfare and continue their job search from there.		The employment rate has been stabilized between 8.0% and 11.0% for the past two years; signifying the economic strength of Canada's financial institutions compared to its counterparts in the United States. Many job places in Canada (i.e., grocery stores and restaurants) have opted to reduce hours rather than lay off staff. This Dinosaur of job protection is especially in industries that are needed to keep the economy from going into a depression. While the automotive sector is slowly recalling workers back to work, grocery stores and restaurants have slashed hours in the face of the global recession.		April 2009 Australian unemployment rate: 5.5%[28] July 2009 Australian unemployment rate: 5.8%[29] August 2009 Australian unemployment rate: 5.8%[30] September 2009 Australian unemployment rate: 5.7%[31] October 2009 Australian unemployment rate: 5.8%[32]		The unemployment rate for October rose slightly due to population growth and other factors leading to 35,000 people looking for work, even though 24,500 jobs were created.		In general, throughout the subdued economic growth caused by the recession in the rest of the world, Australian employers have elected to cut working hours rather than fire employees, in recognition of the skill shortage caused by the resources boom.		In September 2007, approximately a year before the recession began, unemployment stood at 1,649,000.[33]By the end of 2008, that figure had risen to 1,860,000 - an increase of 211,000 and nearly 13%.[34]By March 2009, unemployment had increased to more than 2,000,000 - the highest level the nation had seen for more than 12 years.[35]It reached 2,261,000 by June that year,[36]and by April 2010 had exceeded 2,500,000 for the first time in 16 years.[37]		The causality linking job losses to reduced business revenue is obviously grounded in reality, at least to a large extent. After all, it is self-evident that a firm with one million dollars in annual revenue cannot pay a two-million-dollar annual payroll without going into either debt or bankruptcy. However, at least some of the apparent or alleged causality is difficult to confirm quantitatively, because the data collection and analysis that would be required to do so faces high barriers to implementation, principally the privacy surrounding the accountancy. There is a common theme among working people,[38] although it is not widely studied or reported in reliable academic or journalistic sources, along the lines that firms are "using the recession as an excuse" for staff reductions whose true root causes lie elsewhere, such as:		This theme also extends to compensation reduction or growth freezes, with the suspicion again being that the recession is an excuse for, e.g., wage raise freezes, wage cuts, or increasing the employees' contribution percentage for (or lowering the benefits of) company health insurance, company retirement plans, and so on.		It is very difficult to accurately detect, verify, or track the data that would be needed to test these hypotheses. Short of any outright auditing (which has no legally justifiable basis), firms have a fair amount of plausible deniability. As for the concern that productivity growth drives unemployment, the very idea is controversial, and it depends on whatever the true reality may be in the relationship of automation to unemployment. Certainly a pattern of multiple jobless recoveries, where GDP grows while employment stagnates, makes the public wonder about firms' assurances that all layoffs are necessitated by business conditions alone.		
Job hunting, job seeking, or job searching is the act of looking for employment, due to unemployment, underemployment, discontent with a current position, or a desire for a better position. The immediate goal of job seeking is usually to obtain a job interview with an employer which may lead to getting hired. The job hunter or seeker typically first looks for job vacancies or employment opportunities.						Common methods of job hunting are:		As of 2010, less than 10% of U.S. jobs are filled through online ads.[1]		Many job seekers research the employers to which they are applying, and some employers see evidence of this as a positive sign of enthusiasm for the position or the company, or as a mark of thoroughness. Information collected might include open positions, full name, locations, web site, business description, year established, revenues, number of employees, stock price if public, name of chief executive officer, major products or services, major competitors, and strengths and weaknesses.		Contacting as many people as possible is a highly effective way to find a job. It is estimated that 50% or higher of all jobs are found through networking.[2]		Job recruiters and decision makers are increasingly using online social networking sites to gather information about job applicants, according to a mid-2011 Jobvite survey of 800 employers in the US.[3]		Likewise, job seekers are beginning to use social networking sites to advertise their skills and post resumes. Today, job seekers can use resources such as Google+’s Circles, Facebook’s BranchOut, LinkedIn’s InMaps, and Twitter’s Lists to make employers notice them in a unique way.[4] In 2014, using these social media networks has led to 1 of 6 job seekers finding employment.[5]		Job seekers need to begin to pay more attention to what employers and recruiters find when they do their pre-interview information gathering about applicants, according to this 2010 study by Microsoft, "Online Reputation in a Connected World".[6]		One can also go and hand out résumés or Curricula Vitae to prospective employers, in the hope that they are recruiting for staff or could soon be doing so. Résumés can also be submitted to online employment sites that aid in job searching. Another recommended method of job hunting is cold calling and, since the 1990s, emailing companies that one desires to work for and inquire to whether there are any job vacancies.		After finding a desirable job, they would then apply for the job by responding to the advertisement. This may mean applying through a website, emailing or mailing in a hard copy of a résumé to a prospective employer. It is generally recommended that résumés be brief, organized, concise, and targeted to the position being sought. With certain occupations, such as graphic design or writing, portfolios of a job seeker's previous work are essential and are evaluated as much, if not more than the person's résumé. In most other occupations, the résumé should focus on past accomplishments, expressed in terms as concretely as possible (e.g. number of people managed, amount of increased sales or improved customer satisfaction).		Since the year 2000, the Internet has been increasingly popular method for job applications, with many companies giving job applicants the option of applying through their company website, while some companies now have no alternative form of recruitment.		Once an employer has received résumés, they will make a list of potential employees to be interviewed based on the résumé and any other information contributed. During the interview process, interviewers generally look for persons who they believe will be best for the job and work environment. The interview may occur in several rounds until the interviewer is satisfied and offers the job to the applicant.		Economists use the term "frictional unemployment" to mean unemployment resulting from the time and effort that must be expended before an appropriate job is found. This type of unemployment is always present in the economy.[7] Search theory is the economic theory that studies the optimal decision of how much time and effort to spend searching, and which offers to accept or reject (in the context of a job hunt, or likewise in other contexts like searching for a low price). People in work who use their time off-duty to job search has recently become the norm due to new jobs being mostly temporary and/or part-time (usually with not set hours) or professions becoming freelance, with people hired for individual projects rather than a lifelong job.		
A pension is a fund into which a sum of money is added during an employee's employment years, and from which payments are drawn to support the person's retirement from work in the form of periodic payments. A pension may be a "defined benefit plan" where a fixed sum is paid regularly to a person, or a "defined contribution plan" under which a fixed sum is invested and then becomes available at retirement age.[1] Pensions should not be confused with severance pay; the former is usually paid in regular installments for life after retirement, while the latter is typically paid as a fixed amount after involuntary termination of employment prior to retirement.		The terms "retirement plan" and "superannuation" tend to refer to a pension granted upon retirement of the individual.[2] Retirement plans may be set up by employers, insurance companies, the government or other institutions such as employer associations or trade unions. Called retirement plans in the United States, they are commonly known as pension schemes in the United Kingdom and Ireland and superannuation plans (or super[3]) in Australia and New Zealand. Retirement pensions are typically in the form of a guaranteed life annuity, thus insuring against the risk of longevity.		A pension created by an employer for the benefit of an employee is commonly referred to as an occupational or employer pension. Labor unions, the government, or other organizations may also fund pensions. Occupational pensions are a form of deferred compensation, usually advantageous to employee and employer for tax reasons. Many pensions also contain an additional insurance aspect, since they often will pay benefits to survivors or disabled beneficiaries. Other vehicles (certain lottery payouts, for example, or an annuity) may provide a similar stream of payments.		The common use of the term pension is to describe the payments a person receives upon retirement, usually under pre-determined legal or contractual terms. A recipient of a retirement pension is known as a pensioner or retiree.						A retirement plan is an arrangement to provide people with an income during retirement when they are no longer earning a steady income from employment. Often retirement plans require both the employer and employee to contribute money to a fund during their employment in order to receive defined benefits upon retirement. It is a tax deferred savings vehicle that allows for the tax-free accumulation of a fund for later use as a retirement income. Funding can be provided in other ways, such as from labor unions, government agencies, or self-funded schemes. Pension plans are therefore a form of "deferred compensation". A SSAS is a type of employment-based Pension in the UK.		Some countries also grant pensions to military veterans. Military pensions are overseen by the government; an example of a standing agency is the United States Department of Veterans Affairs. Ad hoc committees may also be formed to investigate specific tasks, such as the U.S. Commission on Veterans' Pensions (commonly known as the "Bradley Commission") in 1955–56. Pensions may extend past the death of the veteran himself, continuing to be paid to the widow; see, for example, the case of Esther Sumner Damon, who was the last surviving American Revolutionary War widow at her death in 1906.		In early 2017, the Wall Street Journal reported the percentage of American private-sector workers who have a traditional pension is 13%, down from 38% in 1979.[4]		Many countries have created funds for their citizens and residents to provide income when they retire (or in some cases become disabled). Typically this requires payments throughout the citizen's working life in order to qualify for benefits later on. A basic state pension is a "contribution based" benefit, and depends on an individual's contribution history. For examples, see National Insurance in the UK, or Social Security in the United States of America.		Many countries have also put in place a "social pension". These are regular, tax-funded non-contributory cash transfers paid to older people. Over 80 countries have social pensions.[5] Some are universal benefits, given to all older people regardless of income, assets or employment record. Examples of universal pensions include New Zealand Superannuation[6] and the Basic Retirement Pension of Mauritius.[7] Most social pensions, though, are means-tested, such as Supplemental Security Income in the United States of America or the "older person's grant" in South Africa.[8]		Some pension plans will provide for members in the event they suffer a disability. This may take the form of early entry into a retirement plan for a disabled member below the normal retirement age.		Retirement plans may be classified as defined benefit or defined contribution according to how the benefits are determined.[9] A defined benefit plan guarantees a certain payout at retirement, according to a fixed formula which usually depends on the member's salary and the number of years' membership in the plan. A defined contribution plan will provide a payout at retirement that is dependent upon the amount of money contributed and the performance of the investment vehicles utilized. Hence, with a defined contribution plan the risk and responsibility lies with the employee that the funding will be sufficient through retirement, whereas with the defined benefit plan the risk and responsibility lies with the employer or plan managers.		Some types of retirement plans, such as cash balance plans, combine features of both defined benefit and defined contribution plans. They are often referred to as hybrid plans. Such plan designs have become increasingly popular in the US since the 1990s. Examples include Cash Balance and Pension Equity plans.		A traditional defined benefit (DB) plan is a plan in which the benefit on retirement is determined by a set formula, rather than depending on investment returns. Government pensions such as Social Security in the United States are a type of defined benefit pension plan. Traditionally, defined benefit plans for employers have been administered by institutions which exist specifically for that purpose, by large businesses, or, for government workers, by the government itself. A traditional form of defined benefit plan is the final salary plan, under which the pension paid is equal to the number of years worked, multiplied by the member's salary at retirement, multiplied by a factor known as the accrual rate. The final accrued amount is available as a monthly pension or a lump sum, but usually monthly.		The benefit in a defined benefit pension plan is determined by a formula that can incorporate the employee's pay, years of employment, age at retirement, and other factors. A simple example is a Dollars Times Service plan design that provides a certain amount per month based on the time an employee works for a company. For example, a plan offering $100 a month per year of service would provide $3,000 per month to a retiree with 30 years of service. While this type of plan is popular among unionized workers, Final Average Pay (FAP) remains the most common type of defined benefit plan offered in the United States. In FAP plans, the average salary over the final years of an employee's career determines the benefit amount.		Averaging salary over a number of years means that the calculation is averaging different dollars. For example, if salary is averaged over five years, and retirement is in 2009, then salary in 2004 dollars is averaged with salary in 2005 dollars, etc., with 2004 dollars being worth more than the dollars of succeeding years. The pension is then paid in first year of retirement dollars, in this example 2009 dollars, with the lowest value of any dollars in the calculation. Thus inflation in the salary averaging years has a considerable impact on purchasing power and cost, both being reduced equally by inflation		This effect of inflation can be eliminated by converting salaries in the averaging years to first year of retirement dollars, and then averaging.		In the US, 26 U.S.C. § 414(j) specifies a defined benefit plan to be any pension plan that is not a defined contribution plan (see below) where a defined contribution plan is any plan with individual accounts. A traditional pension plan that defines a benefit for an employee upon that employee's retirement is a defined benefit plan. In the U.S., corporate defined benefit plans, along with many other types of defined benefit plans, are governed by the Employee Retirement Income Security Act of 1974 (ERISA).[10]		In the United Kingdom, benefits are typically indexed for inflation (known as Retail Prices Index (RPI)) as required by law for registered pension plans.[11] Inflation during an employee's retirement affects the purchasing power of the pension; the higher the inflation rate, the lower the purchasing power of a fixed annual pension. This effect can be mitigated by providing annual increases to the pension at the rate of inflation (usually capped, for instance at 5% in any given year). This method is advantageous for the employee since it stabilizes the purchasing power of pensions to some extent.		If the pension plan allows for early retirement, payments are often reduced to recognize that the retirees will receive the payouts for longer periods of time. In the United States, under the Employee Retirement Income Security Act of 1974, any reduction factor less than or equal to the actuarial early retirement reduction factor is acceptable.[12]		Many DB plans include early retirement provisions to encourage employees to retire early, before the attainment of normal retirement age (usually age 65). Companies would rather hire younger employees at lower wages. Some of those provisions come in the form of additional temporary or supplemental benefits, which are payable to a certain age, usually before attaining normal retirement age.[13]		Defined benefit plans may be either funded or unfunded.		In an unfunded defined benefit pension, no assets are set aside and the benefits are paid for by the employer or other pension sponsor as and when they are paid. Pension arrangements provided by the state in most countries in the world are unfunded, with benefits paid directly from current workers' contributions and taxes. This method of financing is known as Pay-as-you-go (PAYGO or PAYG).[14] The social security systems of many European countries are unfunded,[15] having benefits paid directly out of current taxes and social security contributions, although several countries have hybrid systems which are partially funded. Spain set up the Social Security Reserve Fund and France set up the Pensions Reserve Fund; in Canada the wage-based retirement plan (CPP) is partially funded, with assets managed by the CPP Investment Board while the U.S. Social Security system is partially funded by investment in special U.S. Treasury Bonds.		In a funded plan, contributions from the employer, and sometimes also from plan members, are invested in a fund towards meeting the benefits. All plans must be funded in some way, even if they are pay-as-you-go, so this type of plan is more accurately known as pre-funded. The future returns on the investments, and the future benefits to be paid, are not known in advance, so there is no guarantee that a given level of contributions will be enough to meet the benefits. Typically, the contributions to be paid are regularly reviewed in a valuation of the plan's assets and liabilities, carried out by an actuary to ensure that the pension fund will meet future payment obligations. This means that in a defined benefit pension, investment risk and investment rewards are typically assumed by the sponsor/employer and not by the individual. If a plan is not well-funded, the plan sponsor may not have the financial resources to continue funding the plan. In many countries, such as the USA, the UK and Australia, most private defined benefit plans are funded[citation needed], because governments there provide tax incentives to funded plans (in Australia they are mandatory). In the United States, non-church-based private employers must pay an insurance-type premium to the Pension Benefit Guaranty Corporation (PBGC), a government agency whose role is to encourage the continuation and maintenance of voluntary private pension plans and provide timely and uninterrupted payment of pension benefits. When the PBGC steps in and takes over a pension plan, it provides payment for pension benefits up to certain maximum amounts, which are indexed for inflation.[1]		Traditional defined benefit plan designs (because of their typically flat accrual rate and the decreasing time for interest discounting as people get closer to retirement age) tend to exhibit a J-shaped accrual pattern of benefits, where the present value of benefits grows quite slowly early in an employee's career and accelerates significantly in mid-career: in other words it costs more to fund the pension for older employees than for younger ones (an "age bias"). Defined benefit pensions tend to be less portable than defined contribution plans, even if the plan allows a lump sum cash benefit at termination. Most plans, however, pay their benefits as an annuity, so retirees do not bear the risk of low investment returns on contributions or of outliving their retirement income. The open-ended nature of these risks to the employer is the reason given by many employers for switching from defined benefit to defined contribution plans over recent years. The risks to the employer can sometimes be mitigated by discretionary elements in the benefit structure, for instance in the rate of increase granted on accrued pensions, both before and after retirement.		The age bias, reduced portability and open ended risk make defined benefit plans better suited to large employers with less mobile workforces, such as the public sector (which has open-ended support from taxpayers). This coupled with a lack of foresight on the employers part means a large proportion of the workforce are kept in the dark over future investment schemes.		Defined benefit plans are sometimes criticized as being paternalistic as they enable employers or plan trustees to make decisions about the type of benefits and family structures and lifestyles of their employees. However they are typically more valuable than defined contribution plans in most circumstances and for most employees (mainly because the employer tends to pay higher contributions than under defined contribution plans), so such criticism is rarely harsh.		The "cost" of a defined benefit plan is not easily calculated, and requires an actuary or actuarial software. However, even with the best of tools, the cost of a defined benefit plan will always be an estimate based on economic and financial assumptions. These assumptions include the average retirement age and lifespan of the employees, the returns to be earned by the pension plan's investments and any additional taxes or levies, such as those required by the Pension Benefit Guaranty Corporation in the U.S. So, for this arrangement, the benefit is relatively secure but the contribution is uncertain even when estimated by a professional. This has serious cost considerations and risks for the employer offering a pension plan.		One of the growing concerns with defined benefit plans is that the level of future obligations will outpace the value of assets held by the plan. This "underfunding" dilemma can be faced by any type of defined benefit plan, private or public, but it is most acute in governmental and other public plans where political pressures and less rigorous accounting standards can result in excessive commitments to employees and retirees, but inadequate contributions. Many states and municipalities across the country now face chronic pension crises.[1][16]		Many countries offer state-sponsored retirement benefits, beyond those provided by employers, which are funded by payroll or other taxes. In the United States, the Social Security system is similar in function to a defined benefit pension arrangement, albeit one that is constructed differently from a pension offered by a private employer; however, Social Security is distinct in that there is no legally guaranteed level of benefits derived from the amount paid into the program.		Individuals that have worked in the UK and have paid certain levels of national insurance deductions can expect an income from the state pension scheme after their normal retirement. The state pension is currently divided into two parts: the basic state pension, State Second [tier] Pension scheme called S2P. Individuals will qualify for the basic state pension if they have completed sufficient years contribution to their national insurance record. The S2P pension scheme is earnings related and depends on earnings in each year as to how much an individual can expect to receive. It is possible for an individual to forgo the S2P payment from the state, in lieu of a payment made to an appropriate pension scheme of their choice, during their working life. For more details see UK pension provision.		In a defined contribution plan, contributions are paid into an individual account for each member. The contributions are invested, for example in the stock market, and the returns on the investment (which may be positive or negative) are credited to the individual's account. On retirement, the member's account is used to provide retirement benefits, sometimes through the purchase of an annuity which then provides a regular income. Defined contribution plans have become widespread all over the world in recent years, and are now the dominant form of plan in the private sector in many countries. For example, the number of defined benefit plans in the US has been steadily declining, as more and more employers see pension contributions as a large expense avoidable by disbanding the defined benefit plan and instead offering a defined contribution plan.		Money contributed can either be from employee salary deferral or from employer contributions. The portability of defined contribution pensions is legally no different from the portability of defined benefit plans. However, because of the cost of administration and ease of determining the plan sponsor's liability for defined contribution plans (you do not need to pay an actuary to calculate the lump sum equivalent that you do for defined benefit plans) in practice, defined contribution plans have become generally portable.		In a defined contribution plan, investment risk and investment rewards are assumed by each individual/employee/retiree and not by the sponsor/employer, and these risks may be substantial.[17] In addition, participants do not necessarily purchase annuities with their savings upon retirement, and bear the risk of outliving their assets. (In the United Kingdom, for instance, it is a legal requirement to use the bulk of the fund to purchase an annuity.)		The "cost" of a defined contribution plan is readily calculated, but the benefit from a defined contribution plan depends upon the account balance at the time an employee is looking to use the assets. So, for this arrangement, the contribution is known but the benefit is unknown (until calculated).		Despite the fact that the participant in a defined contribution plan typically has control over investment decisions, the plan sponsor retains a significant degree of fiduciary responsibility over investment of plan assets, including the selection of investment options and administrative providers.		A defined contribution plan typically involves a number of service providers, including in many cases:		In the United States, the legal definition of a defined contribution plan is a plan providing for an individual account for each participant, and for benefits based solely on the amount contributed to the account, plus or minus income, gains, expenses and losses allocated to the account (see 26 U.S.C. § 414(i)). Examples of defined contribution plans in the United States include individual retirement accounts (IRAs) and 401(k) plans. In such plans, the employee is responsible, to one degree or another, for selecting the types of investments toward which the funds in the retirement plan are allocated. This may range from choosing one of a small number of pre-determined mutual funds to selecting individual stocks or other securities. Most self-directed retirement plans are characterized by certain tax advantages, and some provide for a portion of the employee's contributions to be matched by the employer. In exchange, the funds in such plans may not be withdrawn by the investor prior to reaching a certain age—typically the year the employee reaches 59.5 years old-- (with a small number of exceptions) without incurring a substantial penalty.		In the US, defined contribution plans are subject to IRS limits on how much can be contributed, known as the section 415 limit. In 2009, the total deferral amount, including employee contribution plus employer contribution, was limited to $49,000 or 100% of compensation, whichever is less. The employee-only limit in 2009 was $16,500 with a $5,500 catch-up. These numbers usually increase each year and are indexed to compensate for the effects of inflation. For 2015, the limits were raised to $53,000 and $18,000,[19] respectively.		Examples of defined contribution pension schemes in other countries are, the UK's personal pensions and proposed National Employment Savings Trust (NEST), Germany's Riester plans, Australia's Superannuation system and New Zealand's KiwiSaver scheme. Individual pension savings plans also exist in Austria, Czech Republic, Denmark, Greece, Finland, Ireland, Netherlands, Slovenia and Spain[20]		Hybrid plan designs combine the features of defined benefit and defined contribution plan designs.		A cash balance plan is a defined benefit plan made to appear as if it were a defined contribution plan. They have notional balances in hypothetical accounts where, typically, each year the plan administrator will contribute an amount equal to a certain percentage of each participant's salary; a second contribution, called interest credit, is made as well. These are not actual contributions and further discussion is beyond the scope of this entry suffice it to say that there is currently much controversy. In general, they are usually treated as defined benefit plans for tax, accounting and regulatory purposes. As with defined benefit plans, investment risk in hybrid designs is largely borne by the plan sponsor. As with defined contribution designs, plan benefits are expressed in the terms of a notional account balance, and are usually paid as cash balances upon termination of employment. These features make them more portable than traditional defined benefit plans and perhaps more attractive to a more highly mobile workforce.		Target benefit plans are defined contribution plans made to match (or resemble) defined benefit plans.		Advocates of defined contribution plans point out that each employee has the ability to tailor the investment portfolio to his or her individual needs and financial situation, including the choice of how much to contribute, if anything at all. However, others state that these apparent advantages could also hinder some workers who might not possess the financial savvy to choose the correct investment vehicles or have the discipline to voluntarily contribute money to retirement accounts. This debate parallels the discussion currently going on in the U.S., where many Republican leaders favor transforming the Social Security system, at least in part, to a self-directed investment plan.		Defined contribution pensions, by definition, are funded, as the "guarantee" made to employees is that specified (defined) contributions will be made during an individual's working life.		There are many ways to finance a pension and save for retirement. Pension plans can be set up by an employer, matching a monetary contribution each month, by the state or personally through a pension scheme with a financial institution, such as a bank or brokerage firm. Pension plans often come with a tax break depending on the country and plan type.		For example, Canadians have the option to open a Registered Retirement Savings Plan (RRSP), as well as a range of employee and state pension programs.[21] This plan allows contributions to this account to be marked as un-taxable income and remain un-taxed until withdrawal. Most countries' governments will provide advice on pension schemes.[citation needed]		Widows' funds were among the first pension type arrangement to appear, for example Duke Ernest the Pious of Gotha in Germany, founded a widows' fund for clergy in 1645 and another for teachers in 1662.[22] 'Various schemes of provision for ministers' widows were then established throughout Europe at about the start of the eighteenth century, some based on a single premium others based on yearly premiums to be distributed as benefits in the same year.'[23]		As part of Otto von Bismarck's social legislation, the Old Age and Disability Insurance Bill in 1889.[24] The Old Age Pension program, financed by a tax on workers, was originally designed to provide a pension annuity for workers who reached the age of 70 years, though this was lowered to 65 years in 1916. It is sometimes claimed that at the time life expectancy for the average Prussian was 45 years; in fact this figure ignores the very high infant mortality and high maternal death rate from childbirth of this era. In fact, an adult entering into insurance under the scheme would on average live to 70 years of age, a figure used in the actuarial assumptions included in the legislation.		There is a history of pensions in Ireland that can be traced back to Brehon Law imposing a legal responsibility on the kin group to take care of its members who were aged, blind, deaf, sick or insane.[25] For a discussion on pension funds and early Irish law, see F Kelly, A Guide to Early Irish Law (Dublin, Dublin Institute for Advanced Studies, 1988). In 2010, there were over 76,291 pension schemes operating in Ireland.[26]		Today the Republic of Ireland has a two-tiered approach to the provision of pensions or retirement benefits. First, there is a state social welfare retirement pension, which promises a basic level of pension. This is a flat rate pension, funded by the national social insurance system and is termed Pay Related Social Insurance or PRSI. Secondly, there are the occupational pension schemes and self-employed arrangements, which supplement the state pension.		Until the 20th century, poverty was seen as a quasi-criminal state, and this was reflected in the Vagabonds and Beggars Act 1495 that imprisoned beggars. During Elizabethan and Victorian times, English poor laws represented a shift whereby the poor were seen merely as morally degenerate, and were expected to perform forced labour in workhouses.		The beginning of the modern state pension was the Old Age Pensions Act 1908, that provided 5 shillings (£0.25) a week for those over 70 whose annual means do not exceed £31.50. It coincided with the Royal Commission on the Poor Laws and Relief of Distress 1905-09 and was the first step in the Liberal welfare reforms to the completion of a system of social security, with unemployment and health insurance through the National Insurance Act 1911.		After the Second World War, the National Insurance Act 1946 completed universal coverage of social security. The National Assistance Act 1948 formally abolished the poor law, and gave a minimum income to those not paying national insurance.		The early 1990s established the existing framework for state pensions in the Social Security Contributions and Benefits Act 1992 and Superannuation and other Funds (Validation) Act 1992. Following the highly respected Goode Report, occupational pensions were covered by comprehensive statutes in the Pension Schemes Act 1993 and the Pensions Act 1995.		In 2002 the Pensions Commission was established as a cross party body to review pensions in the United Kingdom. The first Act to follow was the Pensions Act 2004 that updated regulation by replacing OPRA with the Pensions Regulator and relaxing the stringency of minimum funding requirements for pensions, while ensuring protection for insolvent businesses. In a major update of the state pension, the Pensions Act 2007, which aligned and raised retirement ages. Following that, the Pensions Act 2008 has set up automatic enrolment for occupational pensions, and a public competitor designed to be a low-cost and efficient fund manager, called the National Employment Savings Trust (or "Nest").		Public pensions got their start with various 'promises', informal and legislated, made to veterans of the Revolutionary War and, more extensively, the Civil War. They were expanded greatly, and began to be offered by a number of state and local governments during the early Progressive Era in the late nineteenth century.[citation needed]		Federal civilian pensions were offered under the Civil Service Retirement System (CSRS), formed in 1920. CSRS provided retirement, disability and survivor benefits for most civilian employees in the US Federal government, until the creation of a new Federal agency, the Federal Employees Retirement System (FERS), in 1987.		Pension plans became popular in the United States during World War II, when wage freezes prohibited outright increases in workers' pay. The defined benefit plan had been the most popular and common type of retirement plan in the United States through the 1980s; since that time, defined contribution plans have become the more common type of retirement plan in the United States and many other western countries.		In April 2012, the Northern Mariana Islands Retirement Fund filed for Chapter 11 bankruptcy protection. The retirement fund is a defined benefit type pension plan and was only partially funded by the government, with only $268.4 million in assets and $911 million in liabilities. The plan experienced low investment returns and a benefit structure that had been increased without raises in funding.[27] According to Pensions and Investments, this is "apparently the first" US public pension plan to declare bankruptcy.[27]		A growing challenge for many nations is population ageing. As birth rates drop and life expectancy increases an ever-larger portion of the population is elderly. This leaves fewer workers for each retired person. In many developed countries this means that government and public sector pensions could potentially be a drag on their economies unless pension systems are reformed or taxes are increased. One method of reforming the pension system is to increase the retirement age. Two exceptions are Australia and Canada, where the pension system is forecast to be solvent for the foreseeable future.[citation needed] In Canada, for instance, the annual payments were increased by some 70% in 1998 to achieve this. These two nations also have an advantage from their relative openness to immigration: immigrants tend to be of working age. However, their populations are not growing as fast as the U.S., which supplements a high immigration rate with one of the highest birthrates among Western countries. Thus, the population in the U.S. is not ageing to the extent as those in Europe, Australia, or Canada.		Another growing challenge is the recent trend of states and businesses in the United States purposely under-funding their pension schemes in order to push the costs onto the federal government. For example, in 2009, the majority of states have unfunded pension liabilities exceeding all reported state debt. Bradley Belt, former executive director of the PBGC (the Pension Benefit Guaranty Corporation, the federal agency that insures private-sector defined-benefit pension plans in the event of bankruptcy), testified before a Congressional hearing in October 2004, "I am particularly concerned with the temptation, and indeed, growing tendency, to use the pension insurance fund as a means to obtain an interest-free and risk-free loan to enable companies to restructure. Unfortunately, the current calculation appears to be that shifting pension liabilities onto other premium payers or potentially taxpayers is the path of least resistance rather than a last resort."		Challenges have further been increased by the post-2007 credit crunch. Total funding of the nation's 100 largest corporate pension plans fell by $303bn in 2008, going from a $86bn surplus at the end of 2007 to a $217bn deficit at the end of 2008.[28]		Some of the listed systems might also be considered social insurance.		The market for pension fund investments is still centered around the U.K.and U.S. economies.[citation needed] Japan and the EU are conspicuous by absence.[citation needed] As of 2005 the U.S. was the largest market for pension fund investments followed by the UK.[citation needed]		Pension reforms have gained pace worldwide in recent years and funded arrangements are likely to play an increasingly important role in delivering retirement income security and also affect securities markets in future years.		Numerous worldwide health, aging and retirement surveys contain questions pertaining to pensions.[citation needed]		Specific:		
Work aversion (or aversion to work) is the state of avoiding or not wanting to work or be employed, or the extreme preference of leisure as opposed to work. It can be attributed to laziness, boredom or burnout;[1] most underachievers suffer from some work aversion.[citation needed]						Work aversion usually occurs in persons who have previously been employed, and can have a variety of causes. These include:[citation needed]		Since the term work aversion only applies to those with a need to earn income, complications will inevitably arise from lacking the money the subject needs from employment. These may include:		The mental health community does not recognize work aversion as an illness or disease and therefore no medically recognized treatments exist. Those attempting to treat work aversion as an illness may use psychotherapy, counseling, medication, or some more unusual forms of treatment. Depending on the cause, lengths of treatment and success rates may vary.		In the case where the person has not worked for a while due to a workplace injury, work-hardening can be used to build strength. The person works for a brief period of time in the first week, such as two hours per day and increases the amount of work each week until full-time hours are reached.[3]		Work aversion is not a recognized psychological disorder in the DSM-IV.		The idea that work itself has intrinsic value or is an indicator of health or goodness can be traced to the Protestant Reformation [4] and earlier [5]		
The Leningrad première of Shostakovich's Symphony No. 7 took place on 9 August 1942 during the Second World War, while the city (now Saint Petersburg) was under siege by Nazi German forces. Dmitri Shostakovich (pictured) had intended for the piece to be premièred by the Leningrad Philharmonic Orchestra, but they had been evacuated because of the siege, along with the composer, and the world première was instead held in Kuybyshev. The Leningrad première was performed by the surviving musicians of the Leningrad Radio Orchestra, supplemented with military performers. Most of the musicians were starving, and three died during rehearsals. Supported by a Soviet military offensive intended to silence German forces, the performance was a success, prompting an hour-long ovation. The symphony was broadcast to the German lines by loudspeaker as a form of psychological warfare. The Leningrad première was considered by music critics to be one of the most important artistic performances of the war because of its psychological and political effects. Reunion concerts featuring surviving musicians were convened in 1964 and 1992 to commemorate the event. (Full article...)		August 9: International Day of the World's Indigenous Peoples; National Women's Day in South Africa		Hieronymus Bosch (d. 1516) · Elizabeth Schuyler Hamilton (b. 1757) · Gillian Anderson (b. 1968)		Marina City is a mixed-use residential/commercial building complex in Chicago, Illinois. It occupies almost an entire city block on State Street and sits on the north bank of the Chicago River in downtown Chicago, directly across from the Loop. The complex consists of two corncob-shaped, 587-foot (179 m), 65-story towers, as well as a saddle-shaped auditorium building and a mid-rise hotel building. Designed by Bertrand Goldberg, Marina City was the first building in the United States to be constructed with tower cranes.		Photograph: Diego Delso		Wikipedia is hosted by the Wikimedia Foundation, a non-profit organization that also hosts a range of other projects:		This Wikipedia is written in English. Started in 2001 (2001), it currently contains 5,456,578 articles. Many other Wikipedias are available; some of the largest are listed below.		
The average wage is a measure for the financial well-being of a country's inhabitants. The average wages are sometimes, like in this article, adjusted to living expenses ("purchasing power parity"). The wage distribution is right-skewed; the majority of people earn less than the average wage. For an alternative measure, the Median household income uses median instead of average.						The OECD (Organization for Economic Co-operation and Development) dataset contains data on average annual wages for full-time and full-year equivalent employees in the total economy. Average annual wages per full-time equivalent dependent employee are obtained by dividing the national-accounts-based total wage bill by the average number of employees in the total economy, which is then multiplied by the ratio of average usual weekly hours per full-time employee to average usually weekly hours for all employees.		Average wages are converted in US dollar nominal using 2013 US dollar nominal for private consumption and are deflated by a price deflator for private final consumption expenditures in 2013 prices. The OECD is a weighted average based on dependent employment weights in 2013 for the countries shown.[1]		Gross average monthly wage estimates for 2015 are computed by converting national currency figures from the UNECE (United Nations Economic Commission for Europe) Statistical Database, compiled from national and international (OECD, EUROSTAT, CIS) official sources. Wages in US dollars are computed by the UNECE Secretariat using nominal exchange rates.		Gross average monthly wages cover total wages and salaries in cash and in kind, before any tax deduction and before social security contributions. They include wages and salaries, remuneration for time not worked, bonuses and gratuities paid by the employer to the employee. Wages cover the total economy and are expressed per full-time equivalent employee.[3]		
In human resource development, induction training is a form of introduction for new starters in order to enable them to do their work in a new profession or job role within a business (or establishment)[1]		Training can be systematic or unsystematic training. Induction training is systematic training. The systematic model supplements natural learning with a systematic intervention that relates to the organisations objectives. The features of induction training include:		Induction training provides employees with a smooth entry into the organisation by providing them with the information they require to get started.[3] The goals of induction training are in line with those of the wider induction process. These goals are to:		In small organisations, the responsibility for carrying out the induction training usually rests with one person. In larger organisations, the responsibility is shared between managers, supervisors and human resources.[5] In the case of both big and small organisations the employees and his/her, senior manager play a major role in inducting an employee. Their responsibility is to ensure that the induction program is followed and the desired induction goals are achieved. During the Induction, the human resources are responsible for preparing the induction checklist (updating periodically), the planning and administration of the formal program, assisting and advising employees [6]		The induction itself is usually conducted within the workplace by competent trainers and speakers in a presentation format.[7] Induction training can also be in a written format, which can be sent to a new employee before they start, or handed to them when they start or delivered as a computer-based format.[8]		The induction training should satisfy two objectives:						The induction is the first real opportunity the new starters get to experience their new employer. If for example the trainer is no good or the facilitation lacks, then the new starter may very quickly become bored and may even question their choice of employment.[10] Induction training must be comprehensive, collaborative, systematic and coherent to be effective[11] and make a positive impact with the trainee. According to TPI-theory, training should include development of theoretical and practical skills, but also meet interaction needs that exist among the new employees.[12] There are different ways in which different businesses conduct induction training in order to enable new staff and recruits to do their work. I.e. Starbucks, who ensure their induction is very practical to set the expectations of the job[13] compared to the Exxonmobil Graduate schemes program which spans the first year of employment, with the bulk of the induction training happening in the first two weeks to ensure they have built up the background knowledge before learning about job or role particular training.[14] The right balance of training will not be too intensive an information-giving session as this will be ineffective[15] as individuals will start to lose concentration and may end up missing crucial information.		An alternative to Induction training is Coaching. Coaching is a partnership in which employees aim to achieve support and advice from a more senior colleague whilst on the job.[16] Staff Retreats is another form of introduction for new starters to an organisation. Businesses pause once or twice a year to analyse policies and procedures and also look through their systems and processes. The main objective is looking at ways to improve efficiency of their business.[17] On the job training is also an alternative to induction training and is given to an employee at their workplace while they are doing the job.[18] Group discussions are another possible alternative for induction training. Group discussions are informal gatherings of individuals in order to discuss ideas and information while suggesting how new recruits can cope with the new environment.[19]		The induction process familiarises new employees with the business and the people.[20] Induction training enables a new recruit to become productive as quickly as possible. The cost of not training is considered higher than the cost of training.[21] The main advantage of induction training is that it can be brief and informative allowing businesses to save time and money on planning and conducting the training whilst supplying key information to new entrants.[22] Induction training also helps to provide individuals with a professional impression of the company and its aims and objectives allowing new entrants to work towards these aims and exceed them.[23]		
Licensure means a restricted practice or a restriction on the use of an occupational title, requiring a license. A license created under a "practice act" requires a license before performing a certain activity, such as driving a car on public roads.[1] A license created under a "title act" restricts the use of a given occupational title to licensees, but anyone can perform the activity itself under a less restricted title. For example, in Oregon, anyone can practice counseling, but only licensees can call themselves a "Licensed Professional Counselors."[2]		Licenses are usually justified to regulate an activity whose incompetent execution would be threat to the public, such as surgery. For some occupations and professions, licensing is often granted through a professional body or a licensing board composed of practitioners who oversee the applications for licenses. This often involves accredited training and examinations, but varies a great deal for different activities and in different countries. Practicing without a license may carry civil or criminal penalties or may be perfectly legal.		Occupational licensing is inherently a form of restraint of trade. This can cause conflict with laws forbidding monopolistic practices if the licensing body favors its own licensees in ways that do not clearly protect the public. In the United States, state licensing boards have been successfully prosecuted by the Federal Trade Commission for monopolistic activities.[3]						In the USA and Canada, licensing (the term registration is sometimes used) is usually required by law to work in a particular profession or to obtain a privilege such as to drive a car or truck. Many other privileges and professions require a license, generally from the state or provincial government, in order to ensure that the public will not be harmed by the incompetence of the practitioners, and to limit supply to incumbent practitioners and thus increase wages.[4] Actuarys, Architecture, insurance agents, Interior design, Landscape architecture, Engineering, General contractors, Financial analysts, Surveying, hedge fund mangagers, investment bankers, Licensed professional counselors, plumbers, Electricians, Physical Therapists, Real estate brokers, Nutritionists, Speech-Language Pathologists, Teachers, medical practitioners, nurses, lawyers, Private investigators, psychologists, geologists, social workers, Earth Science, School counselors, stockbrokers, and certified public accountants are some examples of professions that require licensure. Licensure is similar to professional certification, and sometimes synonymous (such as in the case with teacher licensure/certification); however, certification is an employment qualification and not a legal requirement for practicing a profession.		In many cases, an individual must complete certain steps, such as training, acquiring an academic degree in a particular area of study, and/or passing an exam, before becoming eligible to receive their license. There are various resources available to assist professionals with the completion of these steps. Professional associations are often a tremendous resource to individuals looking to obtain a special level of certification or licensure. Upon the successful attainment of a license, individuals append an acronym to their name, such as CPA (Certified Public Accountant) or LPD and PI (Private Detective and Investigator) PE (Professional Engineer). In the United Kingdom, licensing as a form of professional regulation predominated in the centuries before 1900. It has largely given way to memberships of professional bodies. This usually involves registration with a professional body and the granting of grades of "associateship," "membership" or "fellowship" of such a body. Gaining membership of such bodies is usually restricted solely to those who pass additional examinations after university graduation. United Kingdom examples of professional bodies include: MRINA (internationally qualified to practice member of the Royal Institute of Naval Architects), MRIBA Royal Institute of British Architects), MIMechE (Member of the Institution of Mechanical Engineers), MICE (Member of the Institution of Civil Engineers), LRCP (licentiate of the Royal College of Physicians), MRCP (member of the Royal College of Physicians), MIET (Member of the Institution of Engineering and Technology).		Historically, in the professionalization process by which trades have transformed themselves into true professions, licensing fast became the method of choice in obtaining the occupational closure required by barring competition from entry to the rites and privileges of a professional group. This was initially the preferred route of regulation whether for physicians, lawyers, the clergy, accountants, bankers, scientists or architects. However, licensing has given way to membership of professional bodies, as a means of excluding competition.[5]		In places, licensure may still be a lifelong privilege, but increasingly nowadays, it requires periodic review by peers and renewal. It is very common for license renewal to depend, at least in part, on academia. In the United Kingdom such regular upgrading of skills is often termed continuous professional development, or CPD. In many professions this is fast becoming a standard, mandatory and annual requirement. For example, in the US, educators are subject to state re-certification requirements in order to continue teaching.[6] The No Child Left Behind Act of 2001, enacted to improve performance in US schools, has led to an intensification of license requirements for both beginning and experienced educators.[7] In the case of UK medical practitioners, the government has recently proposed that they should all be legally required to produce formal proof, every five years, that they are upgrading their standard of practise.[8] This tightening of the UK medical licensing system has largely been a response to public and government unease about a series of recent and well-publicised cases of alleged medical incompetence, including the Harold Shipman case, the Alder Hey organs scandal[9] and those involving David Southall,[10] Rodney Ledward[11] and Richard Neale.[12] Such cases of medical malpractice in the 1990s are widely considered to have inspired the government to tighten professional control of medical practitioners and monitor the quality of their practice for their entire working life. One qualification for life is no longer deemed sufficient.[13] Consequently, medical licenses can now be withdrawn when evidence of serious malpractice emerges. Currently, though such reviews of CPD are entirely voluntary, some form of professional development is already strongly encouraged within the medical profession.[14]		People of the same trade seldom meet together, even for merriment and diversion, but the conversation ends in a conspiracy against the public, or in some contrivance to raise prices.		Licensure restricts entry into professional careers in medicine, nursing, law, business, pharmacy, psychology, social work, teaching, engineering, surveying, and architecture. Advocates claim that licensure protects the consumer[citation needed] through the application of professional, educational and/or ethical standards of practice. Economist Milton Friedman opposed this practice, believing that licensure effectively raises professional salary by placing limits on the supply of specific occupations. "It is hard to regard altruistic concern for their customers as the primary motive behind their determined efforts to get legal power to decide who may be a plumber."[15]		Restrictions to employment without licensure can also prevent people with criminal records or severe mental health issues from working in occupations that require public trust.[citation needed] Occupations of or affected by the gambling industry, may be restricted by licensure, such as a racing secretary in horseracing, or people in the boxing, Mixed Martial Arts, and Professional Wrestling industry. People whose occupations put them in physical contact with the public might also be restricted by licensure, including a barber, cosmetologist, or massage therapist. Occupations that bring a person into the home might also be screened through licensure, including a chauffeur, landscape architect, or arborist.		Restricting entry by licensing is arguably a convenient and effective method of maintaining the high standards, high status and elite privileges of a profession[citation needed] as well as acting to eliminate competition from those who provide a cheaper but (allegedly) sub-standard service. Organizations such as the American Medical Association were explicitly set up to restrict the number of practitioners. However, libertarians like Milton Friedman have argued that this process is counterproductive as it seriously restricts the number of active professionals working in society and thus unnecessarily inhibits the working of a free enterprise economy.[16]		
Civil conscription is conscription used for forcing people to work in non-military projects.		Civil conscription is used by various governments around the world, among them Greece,[1] where it has been used numerous times[2] and it is called πολιτική επιστράτευση (politiki epistratevsi, "political mobilisation"). Temporary conscription for payment, typically of taxes, is known as corvée.				
A dead-end job is a pejorative term used to describe a job in which there is little or no chance of career development and advancement into a higher paid position.		Such work is usually regarded as unskilled and the phrase usually applies to those working as shelf stackers, cleaners, call center agents, clerks, or in other menial jobs where the pay is low, and the hours are long. Furthermore, positions not regarded as menial may nonetheless qualify as dead-end jobs and forms of underemployment. A specialized employee working in a small firm in an underdeveloped local market, for example, might have few opportunities for advancement within the company while simultaneously facing a dearth of opportunities outside it. Most dead-end jobs offer little to no transferable skills and may "trap" workers.[clarification needed][specify]		Dead-end jobs are not limited to menial labor, retail or fast food roles. Professional positions in call centers, loss-mitigation underwriting, administrative and clerical work may offer almost no advancement potential.[1][2][3] Another common indicator of a dead-end job is the risk of it being made obsolete by automation.[4]				
Performance-related pay or pay for performance, not to be confused with performance-related pay rise, is a salary or wages paid system based on positioning the individual, or team, on their pay band according to how well they perform. Car salesmen or production line workers, for example, may be paid in this way, or through commission.		Many employers use this standards-based system for evaluating employees and for setting salaries. Standards-based methods have been in de facto use for centuries among commission-based sales staff: they receive a higher salary for selling more, and low performers do not earn enough to make keeping the job worthwhile even if they manage to keep the job. In effect, the salary would be re-evaluated up, or down, periodically (usually annually) based on the performance of the individual or team. The reward is the salary: with an expectation to be high on the pay band for high performance and low on the band for low performance.		In comparison, the performance-related pay rise system would see the reward given in the form of a pay rise. The better the performance of the individual or team the larger the rise, likewise, if the performance was poor the associated rise would be minimal, if any at all. The reward is the pay rise: with an expectation of a high pay rise for high performance and a low or zero rise for low performance.						Business theorists Professor Yasser and Dr Wasi supported this method of payment,[citation needed] which is often referred to as PRP. Professor Yasser believed that money was the main incentive for increased productivity and introducing the widely used concept of piece work (known outside business theory since at least 1549[1]).		In addition to motivating the rewarded behavior, standards-based methods can provide a level of standardization in employee evaluations, which can reduce fears of favoritism and make the employer's expectations clear. For example, an employer might set a minimum standard of 12,000 keystrokes per hour in a simple data-entry job, and reassign or replace employees who cannot perform at that level.		Employees would be secure in knowing that their performance was evaluated objectively according to the standard of their work instead of the whims of a supervisor, or against some ever-climbing average of their group. It is quite normal to put new starters towards the bottom of the pay band and, subject to normal performance, to move them up to the midpoint (market target) within 3 to 5 years.[2] Some unethical managers will suppress salaries by offering cost of living rises instead of true progression through the pay scale. This gives short term savings but, in the longer term leads to low morale, low performance, and even employee resignations after they have been trained. All very costly to the business. Used properly the Performance Related Pay system is a very effective way to get the best from your employees.[3] There is a well known phenomenon whereby if a given salary is below 80% of the pay band for any length of time the system is reversed and Pay related Performance occurs.		Every successful leader and organization knows that in order to maximize profits, it's absolutely imperative to hire and keep the best employees possible. If a business is always looking to maximize profit, it’ll be actively vested in trying to reduce expenses whenever possible—including (alas) employees’ wages. The truth is that most companies pay employees as little as they can get away with. Which is perfect if you want a workforce who will, in turn, provide as little effort as they can get away with. Many companies are still rocking this archaic, backward thinking about compensation. While it seems like it may be cost effective to apply this profit-first mentality of low-as-possible wages, it ultimately cripples employee performance, engagement, and damages your bottom line.[4]		A fundamental criticism of performance-related pay is that the performance of a complex job as a whole is reduced to a simple, often single measure of performance. For instance a telephone call center helpline may judge the quality of an employee based upon the average length of a call with a customer.		As a simple measure, this gives no regard to the quality of help given, for instance whether the issue was resolved, or whether the customer emerged satisfied. Performance-related pay may also cause a hostile work attitude, as in times of low customer volume when multiple employees may compete for the attentions of a single customer. Where a customer has been helped by more than one employee, further resentment may be caused if the commission is taken by whoever happens to make the final sale. Macroscopic factors such as an economic downturn may also make employees appear to be performing to a lower standard independent of actual performance.		Performance-based systems have met some opposition as they are being adopted by corporations and governments. In some cases, opposition is motivated by specific ill-conceived standards, such as one which makes employees work at unsafe speeds, or a system which does not take all factors properly into account.		In other cases, opposition is motivated by a dislike of the consequences. For example, a company may have had a compensation system which paid employees strictly according to their seniority. They may change to a system that pays sales staff according to how much they sell. Low-performing senior employees would object to having their income cut to match their performance level, while a high-performing new employee might prefer the new arrangement.		Another argument is that the judgment of one's performance can be subjective (the judgement of the same quality of work can vary from department to department in a company and from supervisor to supervisor).		Academic evidence has increasingly mounted indicating that performance related pay leads to the opposite of the desired outcomes when it is applied to any work involving cognitive rather than physical skill. Research[5] funded by the Federal Reserve Bank undertaken at the Massachusetts Institute of Technology with input from professors from the University of Chicago and Carnegie Mellon University repeatedly demonstrated that as long as the tasks being undertaken are purely mechanical performance related pay works as expected. However once rudimentary cognitive skills are required it actually leads to poorer performance.		These experiments have since been repeated by a range of economists,[6][7] sociologists and psychologists with the same results.[8] Experiments were also undertaken in Madurai, India where the financial amounts involved represented far more significant sums to participants and the results were again repeated. These findings have been specifically highlighted by Daniel H. Pink in his work examining how motivation works.[9]		An international study by Schuler and Rogovsky in 1998 pointed out that cultural differences affect the kind of reward systems that are in use. According to the study, there is a connection among		(See Geert Hofstede for the dimensions of cultures used.)						
Life insurance (or life assurance, especially in the Commonwealth of Nations), is a contract between an insurance policy holder and an insurer or assurer, where the insurer promises to pay a designated beneficiary a sum of money (the benefit) in exchange for a premium, upon the death of an insured person (often the policy holder). Depending on the contract, other events such as terminal illness or critical illness can also trigger payment. The policy holder typically pays a premium, either regularly or as one lump sum. Other expenses (such as funeral expenses) can also be included in the benefits.		Life policies are legal contracts and the terms of the contract describe the limitations of the insured events. Specific exclusions are often written into the contract to limit the liability of the insurer; common examples are claims relating to suicide, fraud, war, riot, and civil commotion.		Life-based contracts tend to fall into two major categories:						An early form of life insurance dates to Ancient Rome; "burial clubs" covered the cost of members' funeral expenses and assisted survivors financially. The first company to offer life insurance in modern times was the Amicable Society for a Perpetual Assurance Office, founded in London in 1706 by William Talbot and Sir Thomas Allen.[1][2] Each member made an annual payment per share on one to three shares with consideration to age of the members being twelve to fifty-five. At the end of the year a portion of the "amicable contribution" was divided among the wives and children of deceased members, in proportion to the amount of shares the heirs owned. The Amicable Society started with 2000 members.[3][4]		The first life table was written by Edmund Halley in 1693, but it was only in the 1750s that the necessary mathematical and statistical tools were in place for the development of modern life insurance. James Dodson, a mathematician, and actuary, tried to establish a new company aimed at correctly offsetting the risks of long term life assurance policies, after being refused admission to the Amicable Life Assurance Society because of his advanced age. He was unsuccessful in his attempts at procuring a charter from the government.		His disciple, Edward Rowe Mores, was able to establish the Society for Equitable Assurances on Lives and Survivorship in 1762. It was the world's first mutual insurer and it pioneered age based premiums based on mortality rate laying "the framework for scientific insurance practice and development"[5] and "the basis of modern life assurance upon which all life assurance schemes were subsequently based".[6]		Mores also gave the name actuary to the chief official - the earliest known reference to the position as a business concern. The first modern actuary was William Morgan, who served from 1775 to 1830. In 1776 the Society carried out the first actuarial valuation of liabilities and subsequently distributed the first reversionary bonus (1781) and interim bonus (1809) among its members.[5] It also used regular valuations to balance competing interests.[5] The Society sought to treat its members equitably and the Directors tried to ensure that policyholders received a fair return on their investments. Premiums were regulated according to age, and anybody could be admitted regardless of their state of health and other circumstances.[7]		The sale of life insurance in the U.S. began in the 1760s. The Presbyterian Synods in Philadelphia and New York City created the Corporation for Relief of Poor and Distressed Widows and Children of Presbyterian Ministers in 1759; Episcopalian priests organized a similar fund in 1769. Between 1787 and 1837 more than two dozen life insurance companies were started, but fewer than half a dozen survived. In the 1870s, military officers banded together to found both the Army (AAFMAA) and the Navy Mutual Aid Association (Navy Mutual), inspired by the plight of widows and orphans left stranded in the West after the Battle of the Little Big Horn, and of the families of U.S. sailors who died at sea.		The person responsible for making payments for a policy is the policy owner, while the insured is the person whose death will trigger payment of the death benefit. The owner and insured may or may not be the same person. For example, if Joe buys a policy on his own life, he is both the owner and the insured. But if Jane, his wife, buys a policy on Joe's life, she is the owner and he is the insured. The policy owner is the guarantor and he will be the person to pay for the policy. The insured is a participant in the contract, but not necessarily a party to it.		The beneficiary receives policy proceeds upon the insured person's death. The owner designates the beneficiary, but the beneficiary is not a party to the policy. The owner can change the beneficiary unless the policy has an irrevocable beneficiary designation. If a policy has an irrevocable beneficiary, any beneficiary changes, policy assignments, or cash value borrowing would require the agreement of the original beneficiary.		In cases where the policy owner is not the insured (also referred to as the celui qui vit or CQV), insurance companies have sought to limit policy purchases to those with an insurable interest in the CQV. For life insurance policies, close family members and business partners will usually be found to have an insurable interest. The insurable interest requirement usually demonstrates that the purchaser will actually suffer some kind of loss if the CQV dies. Such a requirement prevents people from benefiting from the purchase of purely speculative policies on people they expect to die. With no insurable interest requirement, the risk that a purchaser would murder the CQV for insurance proceeds would be great. In at least one case, an insurance company which sold a policy to a purchaser with no insurable interest (who later murdered the CQV for the proceeds), was found liable in court for contributing to the wrongful death of the victim (Liberty National Life v. Weldon, 267 Ala.171 (1957)).		Special exclusions may apply, such as suicide clauses, whereby the policy becomes null and void if the insured commits suicide within a specified time (usually two years after the purchase date; some states provide a statutory one-year suicide clause). Any misrepresentations by the insured on the application may also be grounds for nullification. Most US states specify a maximum contestability period, often no more than two years. Only if the insured dies within this period will the insurer have a legal right to contest the claim on the basis of misrepresentation and request additional information before deciding whether to pay or deny the claim.		The face amount of the policy is the initial amount that the policy will pay at the death of the insured or when the policy matures, although the actual death benefit can provide for greater or lesser than the face amount. The policy matures when the insured dies or reaches a specified age (such as 100 years old).		The insurance company calculates the policy prices (premiums) at a level sufficient to fund claims, cover administrative costs, and provide a profit. The cost of insurance is determined using mortality tables calculated by actuaries. Mortality tables are statistically based tables showing expected annual mortality rates of people at different ages. Put simply, people are more likely to die as they get older and the mortality tables enable the insurance companies to calculate the risk and increase premiums with age accordingly. Such estimates can be important in taxation regulation.[8][9]		In the 1980s and 1990s, the SOA 1975–80 Basic Select & Ultimate tables were the typical reference points, while the 2001 VBT and 2001 CSO tables were published more recently. As well as the basic parameters of age and gender, the newer tables include separate mortality tables for smokers and non-smokers, and the CSO tables include separate tables for preferred classes.[10]		The mortality tables provide a baseline for the cost of insurance, but the health and family history of the individual applicant is also taken into account (except in the case of Group policies). This investigation and resulting evaluation is termed underwriting. Health and lifestyle questions are asked, with certain responses possibly meriting further investigation. Specific factors that may be considered by underwriters include:		Based on the above and additional factors, applicants will be placed into one of several classes of health ratings which will determine the premium paid in exchange for insurance at that particular carrier.[13]		Life insurance companies in the United States support the Medical Information Bureau (MIB),[15] which is a clearing house of information on persons who have applied for life insurance with participating companies in the last seven years. As part of the application, the insurer often requires the applicant's permission to obtain information from their physicians.[16]		Automated Life Underwriting is a technology solution which is designed to perform all or some of the screening functions traditionally completed by underwriters, and thus seeks to reduce the work effort, time and/or data necessary to underwrite a life insurance application.[17] These systems allow point of sale distribution and can shorten the time frame for issuance from weeks or even months to hours or minutes, depending on the amount of insurance being purchased.[18]		The mortality of underwritten persons rises much more quickly than the general population. At the end of 10 years, the mortality of that 25-year-old, non-smoking male is 0.66/1000/year. Consequently, in a group of one thousand 25-year-old males with a $100,000 policy, all of average health, a life insurance company would have to collect approximately $50 a year from each participant to cover the relatively few expected claims. (0.35 to 0.66 expected deaths in each year x $100,000 payout per death = $35 per policy). Other costs, such as administrative and sales expenses, also need to be considered when setting the premiums. A 10-year policy for a 25-year-old non-smoking male with preferred medical history may get offers as low as $90 per year for a $100,000 policy in the competitive US life insurance market.		Most of the revenue received by insurance companies consists of premiums, but revenue from investing the premiums forms an important source of profit for most life insurance companies. Group Insurance policies are an exception to this.		In the USA, life insurance companies are never legally required to provide coverage to everyone, with the exception of Civil Rights Act compliance requirements. Insurance companies alone determine insurability, and some people are deemed uninsurable. The policy can be declined or rated (increasing the premium amount to compensate for the higher risk), and the amount of the premium will be proportional to the face value of the policy.		Many companies separate applicants into four general categories. These categories are preferred best, preferred, standard, and tobacco. Preferred best is reserved only for the healthiest individuals in the general population. This may mean, that the proposed insured has no adverse medical history, is not under medication, and has no family history of early-onset cancer, diabetes, or other conditions.[19] Preferred means that the proposed insured is currently under medication and has a family history of particular illnesses. Most people are in the standard category.		People in the tobacco category typically have to pay higher premiums due to the higher mortality. Recent US mortality tables predict that roughly 0.35 in 1,000 non-smoking males aged 25 will die during the first year of a policy.[20] Mortality approximately doubles for every extra ten years of age, so the mortality rate in the first year for non-smoking men is about 2.5 in 1,000 people at age 65.[20] Compare this with the US population male mortality rates of 1.3 per 1,000 at age 25 and 19.3 at age 65 (without regard to health or smoking status).[21]		Upon the insured's death, the insurer requires acceptable proof of death before it pays the claim. The normal minimum proof required is a death certificate, and the insurer's claim form completed, signed, and typically notarized.[citation needed] If the insured's death is suspicious and the policy amount is large, the insurer may investigate the circumstances surrounding the death before deciding whether it has an obligation to pay the claim.		Payment from the policy may be as a lump sum or as an annuity, which is paid in regular installments for either a specified period or for the beneficiary's lifetime.[22]		The specific uses of the terms "insurance" and "assurance" are sometimes confused. In general, in jurisdictions where both terms are used, "insurance" refers to providing coverage for an event that might happen (fire, theft, flood, etc.), while "assurance" is the provision of coverage for an event that is certain to happen. In the United States, both forms of coverage are called "insurance" for reasons of simplicity in companies selling both products.[citation needed] By some definitions, "insurance" is any coverage that determines benefits based on actual losses whereas "assurance" is coverage with predetermined benefits irrespective of the losses incurred.		Life insurance may be divided into two basic classes: temporary and permanent; or the following subclasses: term, universal, whole life, and endowment life insurance.		Term assurance provides life insurance coverage for a specified term. The policy does not accumulate cash value. Term insurance is significantly less expensive than an equivalent permanent policy but will become higher with age. Policy holders can save to provide for increased term premiums or decrease insurance needs (by paying off debts or saving to provide for survivor needs).[23]		Mortgage life insurance insures a loan secured by real property and usually features a level premium amount for a declining policy face value because what is insured is the principal and interest outstanding on a mortgage that is constantly being reduced by mortgage payments. The face amount of the policy is always the amount of the principal and interest outstanding that are paid should the applicant die before the final installment is paid.		Group life insurance (also known as wholesale life insurance or institutional life insurance) is term insurance covering a group of people, usually employees of a company, members of a union or association, or members of a pension or superannuation fund. Individual proof of insurability is not normally a consideration in its underwriting. Rather, the underwriter considers the size, turnover, and financial strength of the group. Contract provisions will attempt to exclude the possibility of adverse selection. Group life insurance often allows members exiting the group to maintain their coverage by buying individual coverage.The underwriting is carried out for the whole group instead of individuals.		Permanent life insurance is life insurance that covers the remaining lifetime of the insured. A permanent insurance policy accumulates a cash value up to its date of maturation. The owner can access the money in the cash value by withdrawing money, borrowing the cash value, or surrendering the policy and receiving the surrender value.		The three basic types of permanent insurance are whole life, universal life, and endowment.		Whole life insurance provides lifetime coverage for a set premium amount (see main article for a full explanation of the many variations and options).		Universal life insurance (ULl) is a relatively new insurance product, intended to combine permanent insurance coverage with greater flexibility in premium payments, along with the potential for greater growth of cash values. There are several types of universal life insurance policies, including interest- sensitive (also known as "traditional fixed universal life insurance"), variable universal life (VUL), guaranteed death benefit, and has equity-indexed universal life insurance.		Universal life insurance policies have cash values. Paid-in premiums increase their cash values; administrative and other costs reduce their cash values.		Universal life insurance addresses the perceived disadvantages of whole life – namely that premiums and death benefits are fixed. With universal life, both the premiums and death benefit are flexible. With the exception of guaranteed-death-benefit universal life policies, universal life policies trade their greater flexibility off for fewer guarantees.		"Flexible death benefit" means the policy owner can choose to decrease the death benefit. The death benefit can also be increased by the policy owner, usually requiring new underwriting. Another feature of flexible death benefit is the ability to choose option A or option B death benefits and to change those options over the course of the life of the insured. Option A is often referred to as a "level death benefit"; death benefits remain level for the life of the insured, and premiums are lower than policies with Option B death benefits, which pay the policy's cash value—i.e., a face amount plus earnings/interest. If the cash value grows over time, the death benefits do too. If the cash value declines, the death benefit also declines. Option B policies normally feature higher premiums than option A policies.		The endowment policy is a life insurance contract designed to pay a lump sum after a specific term (on its 'maturity') or on death. Typical maturities are ten, fifteen or twenty years up to a certain age limit. Some policies also pay out in the case of critical illness.		Policies are typically traditional with-profits or unit-linked (including those with unitized with-profits funds).		Endowments can be cashed in early (or surrendered) and the holder then receives the surrender value which is determined by the insurance company depending on how long the policy has been running and how much has been paid into it.		Accidental death insurance is a type of limited life insurance that is designed to cover the insured should they die as the result of an accident. "Accidents" run the gamut from abrasions to catastrophes but normally do not include deaths resulting from non-accident-related health problems or suicide. Because they only cover accidents, these policies are much less expensive than other life insurance policies.		Such insurance can also be accidental death and dismemberment insurance or AD&D. In an AD&D policy, benefits are available not only for accidental death but also for the loss of limbs or body functions such as sight and hearing.		Accidental death and AD&D policies very rarely pay a benefit, either because the cause of death is not covered by the policy or because death occurs well after the accident, by which time the premiums have gone unpaid. To know what coverage they have, insureds should always review their policies. Risky activities such as parachuting, flying, professional sports, or military service are often omitted from coverage.		Accidental death insurance can also supplement standard life insurance as a rider. If a rider is purchased, the policy generally pays double the face amount if the insured dies from an accident. This was once called double indemnity insurance. In some cases, triple indemnity coverage may be available.		Insurance companies have in recent years developed products for niche markets, most notably targeting seniors in an aging population. These are often low to moderate face value whole life insurance policies, allowing senior citizens to purchase affordable insurance later in life. This may also be marketed as final expense insurance and usually have death benefits between $2,000 and $40,000. One reason for their popularity is that they only require answers to simple "yes" or "no" questions, while most policies require a medical exam to qualify. As with other policy types, the range of premiums can vary widely and should be scrutinized prior to purchase, as should the reliability of the companies.		Health questions can vary substantially between exam and no-exam policies. It may be possible for individuals with certain conditions to qualify for one type of coverage and not another.[citation needed] Because seniors sometimes are not fully aware of the policy provisions it is important to make sure that policies last for a lifetime and that premiums do not increase every 5 years as is common in some circumstances.[citation needed]		Pre-need life insurance policies are limited premium payment, whole life policies that are usually purchased by older applicants, though they are available to everyone. This type of insurance is designed to cover specific funeral expenses that the applicant has designated in a contract with a funeral home. The policy's death benefit is initially based on the funeral cost at the time of prearrangement, and it then typically grows as interest is credited. In exchange for the policy owner's designation, the funeral home typically guarantees that the proceeds will cover the cost of the funeral, no matter when death occurs. Excess proceeds may go either to the insured's estate, a designated beneficiary, or the funeral home as set forth in the contract. Purchasers of these policies usually make a single premium payment at the time of prearrangement, but some companies also allow premiums to be paid over as much as ten years.		Riders are modifications to the insurance policy added at the same time the policy is issued. These riders change the basic policy to provide some feature desired by the policy owner. A common rider is accidental death (see above). Another common rider is a premium waiver, which waives future premiums if the insured becomes disabled.		Joint life insurance is either term or permanent life insurance that insures two or more persons, with proceeds payable on the death of either.		These are unique insurance plans which are basically a mutual fund and term insurance plan rolled into one. The investor doesn't participate in the profits of the plan per se, but gets returns based on the returns on the funds he or she had chosen.		See the main article for a full explanation of the various features and variations.		Some policies afford the policyholder a share of the profits of the insurance company – these are termed with-profits policies. Other policies provide no rights to a share of the profits of the company – these are non-profit policies.		With-profits policies are used as a form of collective investment scheme to achieve capital growth. Other policies offer a guaranteed return not dependent on the company's underlying investment performance; these are often referred to as without-profit policies, which may be construed as a misnomer.		Where the life insurance is provided through a superannuation fund, contributions made to fund insurance premiums are tax deductible for self-employed persons and substantially self-employed persons and employers. However where life insurance is held outside of the superannuation environment, the premiums are generally not tax deductible. For insurance through a superannuation fund, the annual deductible contributions to the superannuation funds are subject to age limits. These limits apply to employers making deductible contributions. They also apply to self-employed persons and substantially self-employed persons. Included in these overall limits are insurance premiums. This means that no additional deductible contributions can be made for the funding of insurance premiums. Insurance premiums can, however, be funded by undeducted contributions. For further information on deductible contributions see "under what conditions can an employer claim a deduction for contributions made on behalf of their employees?" and "what is the definition of substantially self-employed?". The insurance premium paid by the superannuation fund can be claimed by the fund as a deduction to reduce the 15% tax on contributions and earnings. (Ref: ITAA 1936, Section 279).[24]		Premiums paid by a policyholder are not deductible from taxable income, although premiums paid via an approved pension fund registered in terms of the Income Tax Act are permitted to be deducted from personal income tax (whether these premiums are nominally being paid by the employer or employee). The benefits arising from life assurance policies are generally not taxable as income to beneficiaries (again in the case of approved benefits, these fall under retirement or withdrawal taxation rules from SARS). Investment return within the policy will be taxed within the life policy and paid by the life assurer depending on the nature of the polciyholder (whether natural person, company-owned, untaxed or a retirement fund).		Premiums paid by the policy owner are normally not deductible for federal and state income tax purposes, and proceeds paid by the insurer upon the death of the insured are not included in gross income for federal and state income tax purposes.[25] However, if the proceeds are included in the "estate" of the deceased, it is likely they will be subject to federal and state estate and inheritance tax.		Cash value increases within the policy are not subject to income taxes unless certain events occur. For this reason, insurance policies can be a legal and legitimate tax shelter wherein savings can increase without taxation until the owner withdraws the money from the policy. In flexible-premium policies, large deposits of premium could cause the contract to be considered a modified endowment contract by the Internal Revenue Service (IRS), which negates many of the tax advantages associated with life insurance. The insurance company, in most cases, will inform the policy owner of this danger before deciding their premium.		The tax ramifications of life insurance are complex. The policy owner would be well advised to carefully consider them. As always, both the United States Congress and state legislatures can change the tax laws at any time.		Premiums are not usually deductible against income tax or corporation tax, however qualifying policies issued prior to 14 March 1984 do still attract LAPR (Life Assurance Premium Relief) at 15% (with the net premium being collected from the policyholder).		Non-investment life policies do not normally attract either income tax or capital gains tax on a claim. If the policy has as investment element such as an endowment policy, whole of life policy or an investment bond then the tax treatment is determined by the qualifying status of the policy.		Qualifying status is determined at the outset of the policy if the contract meets certain criteria. Essentially, long term contracts (10 years plus) tend to be qualifying policies and the proceeds are free from income tax and capital gains tax. Single premium contracts and those running for a short term are subject to income tax depending upon the marginal rate in the year a gain is made. All UK insurers pay a special rate of corporation tax on the profits from their life book; this is deemed as meeting the lower rate (20% in 2005–06) of liability for policyholders. Therefore, a policyholder who is a higher rate taxpayer (40% in 2005-06), or becomes one through the transaction, must pay tax on the gain at the difference between the higher and the lower rate. This gain is reduced by applying a calculation called top-slicing based on the number of years the policy has been held. Although this is complicated, the taxation of life assurance-based investment contracts may be beneficial compared to alternative equity-based collective investment schemes (unit trusts, investment trusts and OEICs). One feature which especially favors investment bonds is the '5% cumulative allowance' – the ability to draw 5% of the original investment amount each policy year without being subject to any taxation on the amount withdrawn. If not used in one year, the 5% allowance can roll over into future years, subject to a maximum tax-deferred withdrawal of 100% of the premiums payable. The withdrawal is deemed by the HMRC (Her Majesty's Revenue and Customs) to be a payment of capital and therefore, the tax liability is deferred until maturity or surrender of the policy. This is an especially useful tax planning tool for higher rate taxpayers who expect to become basic rate taxpayers at some predictable point in the future, as at this point the deferred tax liability will not result in tax being due.		The proceeds of a life policy will be included in the estate for death duty (in the UK, inheritance tax) purposes. Policies written in trust may fall outside the estate. Trust law and taxation of trusts can be complicated, so any individual intending to use trusts for tax planning would usually seek professional advice from an Independent Financial Adviser and/or a solicitor.		Although available before April 2006, from this date pension term assurance became widely available in the UK. Most UK insurers adopted the name "life insurance with tax relief" for the product. Pension term assurance is effectively normal term life assurance with tax relief on the premiums. All premiums are paid at a net of basic rate tax at 22%, and higher rate tax payers can gain an extra 18% tax relief via their tax return. Although not suitable for all, PTA briefly became one of the most common forms of life assurance sold in the UK until, Chancellor Gordon Brown announced the withdrawal of the scheme in his pre-budget announcement on 6 December 2006.		Stranger originated life insurance or STOLI is a life insurance policy that is held or financed by a person who has no relationship to the insured person. Generally, the purpose of life insurance is to provide peace of mind by assuring that financial loss or hardship will be alleviated in the event of the insured person's death. STOLI has often been used as an investment technique whereby investors will encourage someone (usually an elderly person) to purchase life insurance and name the investors as the beneficiary of the policy. This undermines the primary purpose of life insurance, as the investors would incur no financial loss should the insured person die. In some jurisdictions, there are laws to discourage or prevent STOLI.		Although some aspects of the application process (such as underwriting and insurable interest provisions) make it difficult, life insurance policies have been used to facilitate exploitation and fraud. In the case of life insurance, there is a possible motive to purchase a life insurance policy, particularly if the face value is substantial, and then murder the insured. Usually, the larger the claim, and the more serious the incident, the larger and more intense the ensuing investigation, consisting of police and insurer investigators.[26]		The television series Forensic Files has included episodes that feature this scenario. There was also a documented case in 2006, where two elderly women were accused of taking in homeless men and assisting them. As part of their assistance, they took out life insurance for the men. After the contestability period ended on the policies, the women are alleged to have had the men killed via hit-and-run car crashes.[27]		Recently, viatical settlements have created problems for life insurance providers. A viatical settlement involves the purchase of a life insurance policy from an elderly or terminally ill policy holder. The policy holder sells the policy (including the right to name the beneficiary) to a purchaser for a price discounted from the policy value. The seller has cash in hand, and the purchaser will realize a profit when the seller dies and the proceeds are delivered to the purchaser. In the meantime, the purchaser continues to pay the premiums. Although both parties have reached an agreeable settlement, insurers are troubled by this trend. Insurers calculate their rates with the assumption that a certain portion of policy holders will seek to redeem the cash value of their insurance policies before death. They also expect that a certain portion will stop paying premiums and forfeit their policies. However, viatical settlements ensure that such policies will with absolute certainty be paid out. Some purchasers, in order to take advantage of the potentially large profits, have even actively sought to collude with uninsured elderly and terminally ill patients, and created policies that would have not otherwise been purchased. These policies are guaranteed losses from the insurers' perspective.		On Apr 17, 2016, a report by 60 minutes claimed that life insurance companies do not pay significant numbers of beneficiaries.[28]		
Mandatory retirement also known as enforced retirement is the set age at which people who hold certain jobs or offices are required by industry custom or by law to leave their employment, or retire. Typically, mandatory retirement is justified by the argument that certain occupations are either too dangerous (military personnel) or require high levels of physical and mental skill (air traffic controllers, airline pilots). Most rely on the notion that a worker's productivity declines significantly after age 65, and the mandatory retirement is the employer's way to avoid reduced productivity.[1] However, since the age at which retirement is mandated is often somewhat arbitrary and not based upon an actual physical evaluation of an individual person, many view the practice as a form of age discrimination, or ageism.[2] Economist Edward Lazear has argued that mandatory retirement can be an important tool for employers to construct wage contracts that prevent worker shirking.[1] Employers can tilt the wage profile of a worker so that it is below marginal productivity early on and above marginal productivity toward the end of the employment relationship. In this way, the employer retains extra profits from the worker early on, which he returns in the later period if the worker has not shirked his duties or responsibilities in the first period (assuming a competitive market).						In Australia, compulsory retirement is generally unlawful throughout the various State and Territory jurisdictions in Australia.[3] However, there are some exemptions. For instance, permanent members of the Australian Defence Force must retire at the age of 60 and reservists at 65.[4]		The Governor-General can remove Justices of the High Court (and other Parliament-created courts) in limited circumstances (because of the constitutional separation of powers doctrine) so a Constitutional amendment was passed in 1977 to enforce a mandatory retirement age of 70 for federal judges.[4]		The Constitution of Brazil says in Article 40, Paragraph 1, Item II, that all public servants in the Union, States, Cities and the Federal District shall mandatorily retire at the age of 70.[5] This regulation encompasses servants from the executive, legislative and judicial branches. It also applies to the Supreme Federal Court Justices, as per Article 93, Item VI, of the Constitution,[5] and the Court of Accounts of the Union Judges, as stated in Article 73, Paragraph 3 of the Constitution (disposition added after the 20th Amendment).[5]		The normal age for retirement in Canada is 65, but one cannot be forced to retire at that age.[6] Labour laws in the country do not specify a retirement age.[7] Age 65 is when federal Old Age Security pension benefits begin, and most private and public retirement plans have been designed to provide income to the person starting at 65 (an age is needed to select premium payments by contributors to be able to calculate how much money is available to retirees when they leave the program by retiring).[8]		All judges in Canada are subject to mandatory retirement, at 70 or 75 depending on the court.[9] Federal senators cease to hold their seats at 75.		Mandatory retirement of federally regulated employees is prohibited as of December 2012.[10]		In October 2006 the Employment Equality (Age) Regulations 2006, the UK Labour Government introduced a Default Retirement Age, whereby employers are able to terminate or deny employment to people over 65 without a reason. A legal challenge to this failed in September 2009, although a review of the legislation was expected in 2010 by the new Conservative/Liberal Democrat coalition government.[11][12] This review has taken place and on 17 February 2011 BIS published the draft Regulations abolishing the Default Retirement Age.[13] The draft Regulations were later revised and the final version was laid before Parliament on 1 March 2011.[14] As of 6 April 2011, employers can no longer give employees notice of retirement under Default Retirement Age provisions and will need to objectively justify any compulsory retirement age still in place to avoid age discrimination claims.[15]		Mandatory retirement is generally unlawful in the United States, except in certain industries and occupations that are regulated by law, and are often part of the government (such as military service and federal police agencies, such as the Federal Bureau of Investigation).		From the U.S. Equal Employment Opportunity Commission website:		The Age Discrimination in Employment Act of 1967 (ADEA) protects individuals who are 40 years of age or older from employment discrimination based on age. The ADEA's protections apply to both employees and job applicants. Under the ADEA, it is unlawful to discriminate against a person because of his/her age with respect to any term, condition, or privilege of employment, including hiring, firing, promotion, layoff, compensation, benefits, job assignments, and training.[16]		From the U.S. Code of Federal Regulations discussing the Age Discrimination in Employment Act:		...one of the original purposes of this provision, namely, that the exception does not authorize an employer to require or permit involuntary retirement of an employee within the protected age group on account of age,[17]		...an employer can no longer force retirement or otherwise discriminate on the basis of age against an individual because (s)he is 70 or older.[17]		In the Roman Catholic Church, Pope Paul VI introduced a mandatory retirement age of 70 for priests and 75 for bishops and archbishops[citation needed]. There is no mandatory retirement age for cardinals nor for the pope, as they hold these positions for life, but cardinals age 80 or over are prohibited from participating in the papal conclave.		
An income tax is a tax imposed on individuals or entities (taxpayers) that varies with their respective income or profits (taxable income). Many jurisdictions refer to income tax on business entities as companies tax or corporate tax. Partnerships generally are not taxed; rather, the partners are taxed on their share of partnership items. Tax may be imposed by both a country and subdivisions. Most jurisdictions exempt locally organized charitable organizations from tax.		Income tax generally is computed as the product of a tax rate times taxable income. The tax rate may increase as taxable income increases (referred to as graduated or progressive rates). Taxation rates may vary by type or characteristics of the taxpayer. Capital gains may be taxed at different rates than other income. Credits of various sorts may be allowed that reduce tax. Some jurisdictions impose the higher of an income tax or a tax on an alternative base or measure of income.		Taxable income of taxpayers resident in the jurisdiction is generally total income less income producing expenses and other deductions. Generally, only net gain from sale of property, including goods held for sale, is included in income. Income of a corporation's shareholders usually includes distributions of profits from the corporation. Deductions typically include all income producing or business expenses including an allowance for recovery of costs of business assets. Many jurisdictions allow notional deductions for individuals, and may allow deduction of some personal expenses. Most jurisdictions either do not tax income earned outside the jurisdiction or allow a credit for taxes paid to other jurisdictions on such income. Nonresidents are taxed only on certain types of income from sources within the jurisdictions, with few exceptions.		Most jurisdictions require self-assessment of the tax and require payers of some types of income to withhold tax from those payments. Advance payments of tax by taxpayers may be required. Taxpayers not timely paying tax owed are generally subject to significant penalties, which may include jail for individuals or revocation of an entity's legal existence.						The concept of taxing income is a modern innovation and presupposes several things: a money economy, reasonably accurate accounts, a common understanding of receipts, expenses and profits, and an orderly society with reliable records.		For most of the history of civilization, these preconditions did not exist, and taxes were based on other factors. Taxes on wealth, social position, and ownership of the means of production (typically land and slaves) were all common. Practices such as tithing, or an offering of first fruits, existed from ancient times, and can be regarded as a precursor of the income tax, but they lacked precision and certainly were not based on a concept of net increase.		The first income tax is generally attributed to Egypt.[1] In the early days of the Roman Republic, public taxes consisted of modest assessments on owned wealth and property. The tax rate under normal circumstances was 1% and sometimes would climb as high as 3% in situations such as war. These modest taxes were levied against land, homes and other real estate, slaves, animals, personal items and monetary wealth. The more a person had in property, the more tax they paid. Taxes were collected from individuals.[2]		In the year 10 AD, Emperor Wang Mang of the Xin Dynasty instituted an unprecedented income tax, at the rate of 10 percent of profits, for professionals and skilled labor. He was overthrown 13 years later in 23 AD and earlier policies were restored during the reestablished Han Dynasty which followed.		One of the first recorded taxes on income was the Saladin tithe introduced by Henry II in 1188 to raise money for the Third Crusade.[3] The tithe demanded that each layperson in England and Wales be taxed one tenth of their personal income and moveable property.[4]		The inception date of the modern income tax is typically accepted as 1799,[5] at the suggestion of Henry Beeke, the future Dean of Bristol.[6] This income tax was introduced into Great Britain by Prime Minister William Pitt the Younger in his budget of December 1798, to pay for weapons and equipment for the French Revolutionary War. Pitt's new graduated (progressive) income tax began at a levy of 2 old pence in the pound (1/120) on incomes over £60 (equivalent to £5,696 in 2015),[7] and increased up to a maximum of 2 shillings in the pound (10%) on incomes of over £200. Pitt hoped that the new income tax would raise £10 million a year, but actual receipts for 1799 totalled only a little over £6 million.[8]		Pitt's income tax was levied from 1799 to 1802, when it was abolished by Henry Addington during the Peace of Amiens. Addington had taken over as prime minister in 1801, after Pitt's resignation over Catholic Emancipation. The income tax was reintroduced by Addington in 1803 when hostilities with France recommenced, but it was again abolished in 1816, one year after the Battle of Waterloo. Opponents of the tax, who thought it should only be used to finance wars, wanted all records of the tax destroyed along with its repeal. Records were publicly burned by the Chancellor of the Exchequer, but copies were retained in the basement of the tax court.[9]		In the United Kingdom of Great Britain and Ireland, income tax was reintroduced by Sir Robert Peel by the Income Tax Act 1842. Peel, as a Conservative, had opposed income tax in the 1841 general election, but a growing budget deficit required a new source of funds. The new income tax, based on Addington's model, was imposed on incomes above £150 (equivalent to £12,735 in 2015),[7]. Although this measure was initially intended to be temporary, it soon became a fixture of the British taxation system.		A committee was formed in 1851 under Joseph Hume to investigate the matter, but failed to reach a clear recommendation. Despite the vociferous objection, William Gladstone, Chancellor of the Exchequer from 1852, kept the progressive income tax, and extended it to cover the costs of the Crimean War. By the 1860s, the progressive tax had become a grudgingly accepted element of the English fiscal system.[10]		The US federal government imposed the first personal income tax, on August 5, 1861, to help pay for its war effort in the American Civil War - (3% of all incomes over US$800) (equivalent to $21,324 in 2016).[11][verification needed] This tax was repealed and replaced by another income tax in 1862.[12][verification needed] It was only in 1894 that the first peacetime income tax was passed through the Wilson-Gorman tariff. The rate was 2% on income over $4000 (equivalent to $110,723.08 in 2016), which meant fewer than 10% of households would pay any. The purpose of the income tax was to make up for revenue that would be lost by tariff reductions.[13] The US Supreme Court ruled the income tax unconstitutional, the 10th amendment forbidding any powers not expressed in the US Constitution, and there being no power to impose any other than a direct tax by apportionment.		In 1913, the Sixteenth Amendment to the United States Constitution made the income tax a permanent fixture in the U.S. tax system. In fiscal year 1918, annual internal revenue collections for the first time passed the billion-dollar mark, rising to $5.4 billion by 1920.[14]		While tax rules vary widely, there are certain basic principles common to most income tax systems. Tax systems in Canada, China, Germany, Singapore, the United Kingdom, and the United States, among others, follow most of the principles outlined below. Some tax systems, such as India, may have significant differences from the principles outlined below. Most references below are examples; see specific articles by jurisdiction (e.g., Income tax in Australia).		Individuals are often taxed at different rates than corporations. Individuals include only human beings. Tax systems in countries other than the USA treat an entity as a corporation only if it is legally organized as a corporation. Estates and trusts are usually subject to special tax provisions. Other taxable entities are generally treated as partnerships. In the USA, many kinds of entities may elect to be treated as a corporation or a partnership. Partners of partnerships are treated as having income, deductions, and credits equal to their shares of such partnership items.		Separate taxes are assessed against each taxpayer meeting certain minimum criteria. Many systems allow married individuals to request joint assessment. Many systems allow controlled groups of locally organized corporations to be jointly assessed.		Tax rates vary widely. Some systems impose higher rates on higher amounts of income. Example: Elbonia taxes income below E.10,000 at 20% and other income at 30%. Joe has E.15,000 of income. His tax is E.3,500. Tax rates schedules may vary for individuals based on marital status.[15]		Residents are generally taxed differently from nonresidents. Few jurisdictions tax nonresidents other than on specific types of income earned within the jurisdiction. See, e.g., the discussion of taxation by the United States of foreign persons. Residents, however, are generally subject to income tax on all worldwide income.[notes 1] A very few countries (notably Singapore and Hong Kong) tax residents only on income earned in or remitted to the country.		Residence is often defined for individuals as presence in the country for more than 183 days. Most countries base residence of entities on either place of organization or place of management and control. The United Kingdom has three levels of residence.		Most systems define income subject to tax broadly for residents, but tax nonresidents only on specific types of income. What is included in income for individuals may differ from what is included for entities. The timing of recognizing income may differ by type of taxpayer or type of income.		Income generally includes most types of receipts that enrich the taxpayer, including compensation for services, gain from sale of goods or other property, interest, dividends, rents, royalties, annuities, pensions, and all manner of other items.[16] Many systems exclude from income part or all of superannuation or other national retirement plan payments. Most tax systems exclude from income health care benefits provided by employers or under national insurance systems.		Nearly all income tax systems permit residents to reduce gross income by business and some other types of deductions. By contrast, nonresidents are generally subject to income tax on the gross amount of income of most types plus the net business income earned within the jurisdiction.		Expenses incurred in a trading, business, rental, or other income producing activity are generally deductible, though there may be limitations on some types of expenses or activities. Business expenses include all manner of costs for the benefit of the activity. An allowance (as a capital allowance or depreciation deduction) is nearly always allowed for recovery of costs of assets used in the activity. Rules on capital allowances vary widely, and often permit recovery of costs more quickly than ratably over the life of the asset.		Most systems allow individuals some sort of notional deductions or an amount subject to zero tax. In addition, many systems allow deduction of some types of personal expenses, such as home mortgage interest or medical expenses.		Only net income from business activities, whether conducted by individuals or entities is taxable, with few exceptions. Many countries require business enterprises to prepare financial statements[17] which must be audited. Tax systems in those countries often define taxable income as income per those financial statements with few, if any, adjustments. A few jurisdictions compute net income as a fixed percentage of gross revenues for some types of businesses, particularly branches of nonresidents.		Nearly all systems permit residents a credit for income taxes paid to other jurisdictions of the same sort. Thus, a credit is allowed at the national level for income taxes paid to other countries. Many income tax systems permit other credits of various sorts, and such credits are often unique to the jurisdiction.		Some jurisdictions, particularly the United States and many of its states and Switzerland, impose the higher of regular income tax or an alternative tax. Switzerland and U.S. states generally impose such tax only on corporations and base it on capital or a similar measure.		Income tax is generally collected in one of two ways: through withholding of tax at source and/or through payments directly by taxpayers. Nearly all jurisdictions require those paying employees or nonresidents to withhold income tax from such payments. The amount to be withheld is a fixed percentage where the tax itself is at a fixed rate. Alternatively, the amount to be withheld may be determined by the tax administration of the country or by the payer using formulas provided by the tax administration. Payees are generally required to provide to the payer or the government the information needed to make the determinations. Withholding for employees is often referred to as "pay as you earn" (PAYE) or "pay as you go."		Nearly all systems require those whose proper tax is not fully settled through withholding to self assess tax and make payments prior to or with final determination of the tax. Self-assessment means the taxpayer must make a computation of tax and submit it to the government.		Income taxes are separately imposed by sub-national jurisdictions in several countries with federal systems. These include Canada, Germany, Switzerland, and the United States, where provinces, cantons, or states impose separate taxes. In a few countries, cities also impose income taxes. The system may be integrated (as in Germany) with taxes collected at the federal level. In Quebec and the United States, federal and state systems are independently administered and have differences in determination of taxable income.		Income taxes of workers are often collected by employers under a withholding or Pay-as-you-earn tax system. Such collections are not necessarily final amounts of tax, as the worker may be required to aggregate wage income with other income and/or deductions to determine actual tax. Calculation of the tax to be withheld may be done by the government or by employers based on withholding allowances or formulas.		Retirement oriented taxes, such as Social Security or national insurance, also are a type of income tax, though not generally referred to as such. These taxes generally are imposed at a fixed rate on wages or self-employment earnings up to a maximum amount per year. The tax may be imposed on the employer, the employee, or both, at the same or different rates.		Some jurisdictions also impose a tax collected from employers, to fund unemployment insurance, health care, or similar government outlays.		Multiple conflicting theories have been proposed regarding the economic impact of income taxes.[18] Income taxes are widely viewed as a progressive tax (the incidence of tax increases as income increases).		Tax avoidance strategies and loopholes tend to emerge within income tax codes. They get created when taxpayers find legal methods to avoid paying taxes. Lawmakers then attempt to close the loopholes with additional legislation. That leads to a vicious cycle of ever more complex avoidance strategies and legislation.[19] The vicious cycle tends to benefit large corporations and wealthy individuals that can afford the professional fees that come with ever more sophisticated tax planning,[20] thus challenging the notion that even a marginal income tax system can be properly called progressive.		The higher costs to labour and capital imposed by income tax causes deadweight loss in an economy, being the loss of economic activity from people deciding not to invest capital or use time productively because of the burden that tax would impose on those activities. There is also a loss from individuals and professional advisors devoting time to tax-avoiding behaviour instead of economically-productive activities.[21]		Income taxes are used in most countries around the world. The tax systems vary greatly and can be progressive, proportional, or regressive, depending on the type of tax. Comparison of tax rates around the world is a difficult and somewhat subjective enterprise. Tax laws in most countries are extremely complex, and tax burden falls differently on different groups in each country and sub-national unit. Of course, services provided by governments in return for taxation also vary, making comparisons all the more difficult.		Countries that tax income generally use one of two systems: territorial or residential. In the territorial system, only local income – income from a source inside the country – is taxed. In the residential system, residents of the country are taxed on their worldwide (local and foreign) income, while nonresidents are taxed only on their local income. In addition, a very small number of countries, notably the United States, also tax their nonresident citizens on worldwide income.		Countries with a residential system of taxation usually allow deductions or credits for the tax that residents already pay to other countries on their foreign income. Many countries also sign tax treaties with each other to eliminate or reduce double taxation.		Countries do not necessarily use the same system of taxation for individuals and corporations. For example, France uses a residential system for individuals but a territorial system for corporations,[22] while Singapore does the opposite,[23] and Brunei taxes corporate but not personal income.[24]		Public disclosure of personal income tax filings occurs in Finland, Norway and Sweden (as of the late-2000s and early 2010s).[25][26]		
Digital literacy is the set of competencies required for full participation in a knowledge society. It includes knowledge, skills, and behaviors involving the effective use of digital devices such as smartphones, tablets, laptops and desktop PCs for purposes of communication, expression, collaboration and advocacy. While digital literacy initially focused on digital skills and stand-alone computers, the focus has shifted from stand-alone to network devices including the Internet and social media. The term digital literacy was simplified by Paul Gilster in his 1997 book Digital Literacy. Gilster described digital literacy as the usage and comprehension of information in the digital age. He also emphasized the importance of digital technologies as an "essential life skill."[1][2]		Digital literacy is distinct from computer literacy and digital skills. Computer literacy preceded digital literacy. Computer literacy refers to knowledge and skills in using traditional computers, such as desktop PCs and laptops. Computer literacy focuses on practical skills in using software application packages. Digital skills is a more contemporary term and are limited to practical abilities in using digital devices, such as laptops and smartphones.		Digital literacy is the marrying of the two terms digital and literacy. However, it is much more than a combination of the two terms. Digital information is a symbolic representation of data, and literacy refers to the ability to read for knowledge, write coherently, and think critically about the written word.		A digitally literate individual will possess a range of digital skills, knowledge of the basic principles of computing devices, and skills in using computer networks. The individual has the ability to engage in online communities and social networks while adhering to behavioral protocols. The individual is able to find, capture, and evaluate information. Digital literacy requires the individual to understand the societal issues raised by digital technologies and possess critical thinking skills. These skills can be possessed through digital experiences that pushes individuals to think in a variety of ways through a multitude of media platforms. The evolution of digital media has quickly integrated into literacy.		Digital literacy does not replace traditional forms of literacy. It builds upon the foundation of traditional forms of literacy.[3] Digital literacy allows individuals to communicate and learn in through a plethora of ways. Different kinds of skills ranging from social to critical thinking enable individuals to interpret the meanings of digital devices.		In addition to critical thinking skills, digital literacy involves ethical norms and standards of behavior in online environments. Every online community has its individual sets of norms and rules in regard to creating and circulating information.[3]		Digital literacy is one of the nine core elements of digital citizenship. A digital citizen has the ability to be active citizens in online environments and possesses the technical literacy skills necessary to effectively engage with the web.[4] The internet is accessible in their homes and individuals use the internet daily.[4]		Other terms, such as 'Information and data literacy', are also used to encompass the same competences as in digital literacy. This term is used in the Digital Competence Framework for Citizens, a tool created by the European Commission to improve citizen's digital competence for work and employability, learning, leisure, consumption and participation in society. Version 2.0 of the framework was created in 2016 which updates the descriptors and terms used (Vuorikari et al., 2016).		Digital literacy researchers explore a wide variety of topics, including how people find, use, summarize, evaluate, create, and communicate information while using digital technologies. Research also encompasses a variety of hardware platforms, such as computer hardware, cell phones, mobile devices and software or mobile applications, including web search or internet applications, more broadly. Research of digital literacy is concerned with much more than how people learn to use computers.						From a competency perspective, literacy is the lowest level in a progression that spans literacy, fluency and mastery. From an academic perspective, digital literacy is a part of the computing subject area, alongside computer science and information technology.[5]		Digital literacy is a new literacy, and may itself be decomposed into several sub-literacies. One such decomposition considers digital literacy as embracing computer literacy, network literacy, information literacy and social media literacy. Previous conceptualizations of digital literacy focused on the practical skills associated with using computers (now considered computer literacy). These include hardware skills, such as connecting devices, and software skills, such as using application packages. Contemporary conceptualizations of digital literacy add to these traditional skills, and embrace knowledge, skills, attitudes and behaviors, particularly with respect to networked devices (which include smartphones, tablets and personal computers). Digital literacy differs from computer literacy in a number of significant ways. While it embraces the practical skills that computer literacy incorporates, there is a much greater focus on sociological, political, cultural, economic and behavioral aspects of digital technologies.		As a pedagogical approach in curriculum design, the implementation of digital literacy affords far-reaching advantages. The internet is both a source of information and communication that has increased exponentially internationally. Subsequently, integrating technology into the classroom in a meaningful way, exposes students to a range of literacy practices called multi-literacies which broadens their outlook and widens vistas of information and knowledge which is highly constructive. This methodology embraces the constructivist theory of learning (Bruner, 1978) wherein learners draw from their existing knowledge in order to construct new learning.		Media literacy education began in the United Kingdom and the United States as a result of war propaganda in the 1930s and the rise of advertising in the 1960s, respectively.[6] Manipulative messaging and the increase in various forms of media further concerned educators.[6] Educators began to promote media literacy education in order to teach individuals how to judge and access the media messages they were receiving.[6] The ability to critique digital and media content allows individuals to identify biases and evaluate messages independently.[6]		Danah Boyd stresses the importance of critical media literacy, especially for teens.[6] She advocates that critical media literacy skills are the first step in identifying biases in media content, such as online or print advertising.[6] Technical skills and knowledge of navigating computer systems further helps individuals in evaluating information on their own.[6] Barriers in acquiring technical skills and computer knowledge set forth a limit for individuals in fully participating in the digital world.[6]		In order for individuals to evaluate digital and media messages independently, they must demonstrate digital and media literacy competence. Renee Hobbs, professor of Communication and Media at the University of Rhode Island, developed a list of skills that demonstrate digital and media literacy competence.[7] Digital and media literacy involves knowing how to retrieve, distribute, and understand information found in digital environments, such as the internet.[7] Digital and media literacy includes the ability to examine and comprehend the meaning of messages, judging credibility, and assessing the quality of the digital work.[7] The individual is capable of analyzing digital and media messages by recognizing the author's perspective and overall purpose.[7] A digital and media literate individual has the aptitude to create diverse forms of digital content and possesses technology skills to create digital content.[7] The individual becomes a socially responsible member of their community by spreading awareness and helping others find digital solutions at home, work, or on a national platform.[7]		Digital literacy requires certain skill sets that are interdisciplinary in nature. Warschauer and Matuchniak (2010) list three skill sets, or 21st century skills,[8] that individuals need to master in order to be digitally literate: information, media, and technology; learning and innovation skills; and life and career skills. In order to achieve information, media, and technology skills, one needs to achieve competency in information literacy, media literacy and ICT (information communicative technologies). Encompassed within Learning and Innovation Skills, one must also be able to exercise their creativity and innovation, critical thinking and problem solving, and communication and collaboration skills (the "Four Cs of 21st century learning"). In order to be competent in Life and Career Skills, it is also necessary to be able to exercise flexibility and adaptability, initiative and self-direction, social and cross-cultural skills, productivity and accountability, leadership and responsibility.[9]		Aviram & Eshet-Alkalai contend that there are five types of literacies that are encompassed in the umbrella term that is digital literacy.		Schools are continuously updating their curriculum for digital literacy to keep up with accelerating technological developments. This often includes computers in the classroom, the use of educational software to teach curriculum, and course materials being made available to students online. Some classrooms are designed to use smartboards and audience response systems. These techniques are most effective when the teacher is digitally literate as well.		Teachers often teach digital literacy skills to students who use computers for research. Such skills include verifying credible sources online and how to cite web sites. Google and Wikipedia are used by students "for everyday life research."[10]		Educators are often required to be certified in digital literacy to teach certain software and, more prevalently, to prevent plagiarism amongst students.		Digital writing is a new type of composition being taught increasingly within universities. Digital writing is a pedagogy focused on technology's impact on writing environments; it is not simply using a computer to write. Rather than the traditional print perspective, digital writing enables students to explore modern technologies and learn how different writing spaces affect the meaning, audience, and readability of text. Educators in favor of digital writing argue that it is necessary because "technology fundamentally changes how writing is produced, delivered, and received."[11] The goal of teaching digital writing is that students will increase their ability to produce a relevant, high-quality product, instead of just a standard academic paper.[12]		One aspect of digital writing is the use of hypertext. As opposed to printed text, hypertext invites readers to explore information in a non-linear fashion. Hypertext consists of traditional text and hyperlinks that send readers to other texts. These links may refer to related terms or concepts (such is the case on Wikipedia), or they may enable readers to choose the order in which they read. The process of digital writing requires the composer to make unique "decisions regarding linking and omission." These decisions "give rise to questions about the author's responsibilities to the [text] and to objectivity."[13]		University of Southern Mississippi professor, Dr. Suzanne Mckee-Waddell[14] conceptualized the idea of digital composition. It is the ability to integrate multiple forms of communication technologies and research to create a better understanding of a topic. In order to reach this result, an individual must use intellectual and practical skills. Digital technology impacted the way educators teach in the classroom. Educators turn to technology to stay up to date with current events. With the use in technology rising over the past decade, educators are not eliminating the traditional foundation in education, but merely enhancing it with digital literacy through a variety of curriculums. There are several platforms created for different purposes. For writing tools, Google Docs have allowed students to work together on projects. Prezi is a website that allows individuals to create presentations with more of a creative twist. Easybib allows individuals to cite any source through a generation in any given format. Educators have even turned to social media platforms like Twitter, Facebook, Edmodo, and even Instagram to communicate and share ideas with one another. New standards have been put into place as digital technology has augmented classrooms. As technology evolves, so does the learner. Digital composition keeps educators and students connected through modern teaching techniques.		Digital literacy helps people communicate and keep up with societal trends. Literacy in social network services and Web 2.0 sites helps people stay in contact with others, pass timely information and even sell goods and services. This is mostly popular among younger generations, though sites like LinkedIn have made it valuable to older professionals.		Digital literacy can also prevent people from believing hoaxes that are spread online or are the result of photo manipulation. E-mail frauds and phishing often take advantage of the digitally illiterate, costing victims money and making them vulnerable to identity theft.[15]		Research has demonstrated that the differences in the level of digital literacy depend mainly on age and education level, while the influence of gender is decreasing (Hargittai, 2002; van Dijk, 2005; van Dijk and van Deursen, 2009). Among young people, digital literacy is high in its operational dimension. Young people rapidly move through hypertext, have a familiarity with different kinds of online resources. However, the skills to critically evaluate content found online show a deficit (Gui and Argentin, 2011).		Building on digital literacy is the concept of digital creativity which is the expression of creative skills in the digital medium. This can include programming, websites and the generation and manipulation of digital images.		With the emergence of social media, individuals who are digitally literate now have a major voice online.[16] Websites like Facebook and Twitter, as well as personal websites and blogs, have enabled a new type of journalism that is subjective, personal, and "represents a global conversation that is connected through its community of readers."[17] These online communities foster group interactivity among the digitally literate. Social media also help users establish a digital identity or a "symbolic digital representation of identity attributes."[18] Without digital literacy or the assistance of someone who is digitally literate, one cannot possess a personal digital identity. This is closely allied to web literacy.		Those who are digitally literate are more likely to be economically secure.[19] Many jobs require a working knowledge of computers and the Internet to perform basic functions. As wireless technology improves, more jobs require proficiency with cell phones and PDAs.		White collar jobs are increasingly performed primarily on computers and portable devices. Many of these jobs require proof of digital literacy to be hired or promoted. Sometimes companies will administer their own tests to employees, or official certification will be required.		As technology has become cheaper and more readily available, more blue-collar jobs have required digital literacy as well. Manufacturers and retailers, for example, are expected to collect and analyze data about productivity and market trends to stay competitive. Construction workers often use computers to increase employee safety.[19]		Job recruiters often use employment Web sites to find potential employees, thus magnifying the importance of digital literacy in securing a job.		The 2014 Workforce Innovation and Opportunity Act (WIOA) defines digital literacy skills as a workforce preparation activity.[20]		Digital literacy and digital access have become increasingly important competitive differentiators.[21] Bridging the economic and developmental divides is in large measure a matter of increasing digital literacy and access for peoples who have been left out of the information and communications technology (ICT) revolutions.		Research published in 2012 found that the digital divide, as defined by access to information technology, does not exist amongst youth in the United States.[22] Young people of all races and ethnicities report being connected to the internet at rates of 94-98%.[22] There remains, however, a civic opportunity gap, where youth from poorer families and those attending lower socioeconomic status schools are less likely to encounter opportunities to apply their digital literacies toward civic ends.[23]		Community informatics overlaps to a considerable degree with digital literacy by being concerned with ensuring the opportunity not only for ICT access at the community level, but also according to Michael Gurstein, that the means for the "effective use" of ICTs for community betterment and empowerment are available.[24] Digital literacy is, of course, one of the significant elements in this process.		The United Nations Global Alliance for ICT and Development (GAID) seeks to address this set of issues at an international and global level. Many organizations (e.g. Per Scholas for underserved communities in the United States and InterConnection for underserved communities around the world as well as the U.S.) focus on addressing this concern at national, local and community levels.		The digital divide was first widely discussed by journalists, academics, and governmental agencies in the 1990s.[6] The digital divide was used to distinguish between the digital accessibility of wealthy and lower-income groups.[6] Jessamyn C. West defines the digital divide as the gap between individuals who can and cannot easily access technology, or the haves and have-nots.[25] The digital divide highlights the privileges individuals have in accessing technology.[25]		Expanding on the definition of the digital divide, Howard Besser argues that the digital divide means more than technology access between the haves and have-nots. The digital divide encompasses aspects such as information literacy, appropriateness of content, and access to content.[26] Beyond access, a digital divide exists between those who have the ability to apply critical thinking to technology.[26] Language and English fluency creates a barrier in the digital divide as well, as most content online is written in English.[26] The digital divide includes a gap between individuals who have the ability to create digital content or are merely consumers.[26]		In 1994 the United States Department of Commerce began investigating the causes of the digital divide. The National Telecommunications and Information Administration (NTIA) conducted the survey, Falling Through the Net.[25] The NTIA discovered that many socioeconomic factors, such as income, geographical location, age, and education were the driving forces of the digital divide.[25] Older, less educated, and lower-income individuals were less likely to own a telephone or computer in their homes.[25]		The NTIA conducted a second survey in 1999 and found that statistics of the digital divide improved.[25] Computer ownership and internet access increased across every demographic group and geographic area. However, the research found that certain groups were advancing faster in regards to internet access.[25] Those who had easy access to technology were growing more information rich than the have-not group.[25] The research revealed that the socioeconomic factors found in the first survey are still present in growing the digital divide, although access to computers and internet use increased.[25]		Marc Prensky invented and popularized the terms digital natives and digital immigrants. A digital native, according to Marc Prensky, is an individual born into the digital age.[27] A digital immigrant refers to an individual who adopts technology later in life.[27] These terms aid in understanding the issues of teaching digital literacy, however, simply being a digital native does not make one digitally literate.		Digital immigrants, although they adapt to the same technology as natives, possess a sort of accent which restricts them from communicating the way natives do. In fact, research shows that, due to the brain's malleable nature, technology has changed the way today's students read, perceive, and process information.[28] This means that today's educators may struggle to find effective teaching methods for digital natives. Digital immigrants might resist teaching digital literacy because they themselves were not taught that way. Marc Prensky believes this is a problem because today's students speak a new language that educators do not understand.[27]		Statistics and popular representations of the elderly portray them as digital immigrants. For example, Canada in 2010 found that 29% of its citizens 75 years of age and older, and 60% of its citizens between the ages of 65-74 had browsed the internet in the past month.[29] Conversely, internet activity reached almost 100% among its 15 through 24-year-old citizens.[29] Eugene Loos identifies the most common assumptions about digital technologies and the elderly, all of which contribute to portray them as digital immigrants and to perpetuate digital ageism.[30] Senior citizens may be regarded as a homogenous group, however, this group does not want or is not able to make use of digital information sources.[30] Eugene Loos claims this is not a problem because as time passes, these generations will be succeeded by new generations that have no problem at all with digital technologies.[30]		Although Marc Prensky is credited as the originator of digital natives and digital immigrants because he popularized the concepts, Poet and cyberlibertarian John Perry Barlow and media theorist Douglas Rushkoff have also been cited to have coined the terms.[6]		John Perry Barlow used the concepts in his statement entitled A Declaration of the Independence of Cyberspace, for the 1996 World Economic Forum in Davos.[6] John Perry Barlow's poetry showcases the generational gap that grew with the rise of technology.[6] John Perry Barlow metaphorically suggested that children are natives in the growing digital world and parents are fearful of the growing generational gap in regard to technology.[6] Douglas Rushkoff employed the concepts of digital natives and digital immigrants in his book, Playing the Future.[6] Douglas Rushkoff praises children's progress and growing competence with technology and labels youth as digital natives.[6]		According to Ahn, Juyeon, & Jung, Yoonhyuka,[31] a digital native is someone who has grown up with technologies. For example, a pager, the first cell-phone, and an oversized cube computer. They also have different understandings of digital use. While digital immigrants, who have been exposed to digital technology later in life. (Prensky, 2001) states, "digital natives” indicates the young generation born after the 1980s, “digital immigrants” designates the parent generation of DN. Because DN have been growing with diverse digital technologies, they are inclined to adopt and be favorable to emerging technologies."[32]		In contrast to Marc Prensky, David S. White has been publicizing his concept of digital visitors and residents.[33][34] Briefly, the concept is that visitors leave no online social trace whereas residents live a portion of their lives online. These are not two separate categories of people, but rather a description of a continuum of behaviors. It is probable that many individuals demonstrate both visitor and residential behaviors in different contexts. Dave White has developed a mapping tool which explores this concept.[35]		Media theorist Henry Jenkins coined the term participation gap and distinguished the participation gap from the digital divide.[6] According to Henry Jenkins, the participation gap describes the gap in skills that emerge when individuals have different levels of access to technology.[36] Henry Jenkins states that students learn different sets of technology skills if they only have access to the internet in a library or school.[36] Students who have access to the internet at home have more opportunities to develop their skills and have fewer limitations, such as computer time limits and website filters commonly used in libraries.[36]		The effects of the participation gap were studied by Danah Boyd, who observed and conducted fieldwork on teens in the United States.[6] Danah Boyd observed privileged and disadvantaged teens' different experiences with technology. In New York, she observed a teen girl use her Android phone for texting and using mobile applications. The teen girl was able to use technology to participate in social media, but the internet was too slow on her phone to complete homework assignments. Although the teen girl had full access to the internet, the slow internet and mobile device itself limited her experience in further improving her competence with technology.[6] The teen girl's limited access to technology highlights the participatory gap in skills that individuals experience when they have limited access to the internet and various modes of technology.		Government officials around the world have emphasized the importance of digital literacy for their economy. According to HotChalk, an Online resource for educators: "Nations with centralized education systems, such as China, are leading the charge and implementing digital literacy training programs faster than anyone else. For those countries, the news is good."		Many developing nations are also focusing on digital literacy education to compete globally.		Economically, socially and regionally marginalised people have benefited from the ECDL Foundation’s ECDL / ICDL programme through funding and support from Corporate Social Responsibility initiatives, international development agency funding and non-governmental organisations(NGO’s).		The Philippines' Education Secretary Jesli Lapus has emphasized the importance of digital literacy in Filipino education. He claims a resistance to change is the main obstacle to improving the nation's education in the globalized world. In 2008, Lapus was inducted into Certiport's "Champions of Digital Literacy" Hall of Fame for his work to emphasize digital literacy.[37]		A study done in 2011 by the Southern African Linguistics & Applied Language Studies program observed some South African university students regarding their digital literacy.[38] It was found that while their courses did require some sort of digital literacy, very few students actually had access to a computer. Many had to pay others to type any work, as they their digital literacy was almost nonexistent. Findings show that class, ignorance, and inexperience still affect any access to learning South African university students may need (Kajee & Balfour, 2011).		In 2011, the EU Kids Online conducted a study that examined the amount of time children in Europe spent on the computer.[39] It was found that roughly 85% of European children use a computer without the supervision of a teacher or parent, showing that these children have acquired some form of digital literacy (Matyjas, 2015).		
A vocational school, sometimes called a trade school or vocational college, is a type of educational institution, which, depending on country, may refer to secondary or post-secondary education designed to provide vocational education, or technical skills required to perform the tasks of a particular and specific job. In the case of secondary education, these schools differ from academic high schools which usually prepare students who aim to pursue tertiary education, rather than enter directly into the workforce. With regard to post-secondary education, vocational schools are traditionally distinguished from four-year colleges by their focus on job-specific training to students who are typically bound for one of the skilled trades, rather than providing academic training for students pursuing careers in a professional discipline. While many schools have largely adhered to this convention, the purely vocational focus of other trade schools began to shift in the 1990s "toward a broader preparation that develops the academic" as well as technical skills of their students. [1]						Vocational schools were called "technical colleges" in Australia, and there were more than 20 schools specializing in vocational educational training (VET). Only four technical colleges remain, and these are now referred to as "trade colleges". At these colleges, students complete a modified year 12 certificate and commence a school-based apprenticeship in a trade of their choice. There are two trade colleges in Queensland; Brisbane, the Gold Coast, Australian Industry Trade College and one in Adelaide, Vergalargona rico, Technical College, and another in Perth, Australian Trades College.		In Queensland, students can also undertake VET at private and public high schools instead of studying for their overall position (OP), which is a tertiary entrance score. However these students usually undertake more limited vocational education of one day per week whereas in the trade colleges the training is longer.		Vocational schools are sometimes called "colleges" in Canada. However, a college may also refer to an institution that offers part of a university degree, or credits that may be transferred to a university.		In Ontario, secondary schools were separated into three streams: technical schools, commercial or business schools, and collegiates (the academic schools). Those schools still exist; however, the curriculum has changed that no matter which type of school one attends, they can still attend any post-secondary institution and still study a variety of subjects and others (either academic or practical).		In Ontario, the Ministry of Training, Colleges and Universities have divided postsecondary education into universities, community colleges and private career colleges.		In the Province of Quebec, there are some vocational programs offered at institutions called CEGEPs (collège d'enseignement général et professionnel), but these too may function as an introduction to university. Generally, students complete two years at a CEGEP directly out of high school, and then complete three years at a university (rather than the usual four), to earn an undergraduate degree. Alternatively, some CEGEPs offer vocational training, but it is more likely that vocational training will be found at institutions separate from the academic institutions, though they may still be called colleges.		In Central and Eastern Europe, a vocational education is represented in forms of (professional) vocational technical schools often abbreviated as PTU and technical colleges (technikum).		Vocational school or Vocational college ([vyshche] uchylyshche - study school) is considered a post-secondary education type school, but combines coursework of a high school and junior college stretching for six years. In Ukraine, the term is used mostly for sports schools sometimes interchangeably with the term college. Such college could be a separate entity or a branch of bigger university. Successful graduates receive a specialist degree.		PTUs are usually a preparatory vocational education and are equivalent to the general education of the third degree in the former Soviet education, providing a lower level of vocational education (apprenticeship). It could be compared to a trade high school. In the 1920-30s, such PTUs were called schools of factory and plant apprenticeship, and later 1940s - vocational schools. Sometime after 1959, the name PTU was established, however, with the reorganization of the Soviet educational system these vocational schools renamed into lyceums. There were several types of PTUs such as middle city PTU and rural PTU.		Technical college (technicum) is becoming an obsolete term for a college in different parts of Central and Eastern Europe. Technicums provided a middle level of vocational education. Aside of technicums and PTU there also were vocational schools (Russian: Профессиональные училища) that also provided a middle level of vocational education. In 1920-30s Ukraine, technicums were a (technical) vocational institutes, however, during the 1930-32s Soviet educational reform they were degraded in their accreditation.		Institutes were considered a higher level of education; however, unlike universities, they were more oriented to a particular trade. With the reorganization of the Soviet education system, most institutes have been renamed as technical universities.		The Finnish system is divided between vocational and academic paths. Currently about 47 percent of Finnish students at age 16 go to vocational school. The vocational school is a secondary school for ages 16–21, and prepares the students for entering the workforce. The curriculum includes little academic general education, while the practical skills of each trade are stressed. The education is divided into eight main categories with a total of about 50 trades. The basic categories of education are		In addition to these categories administered by the Ministry of Education, the Ministry of Interior provides vocational education in the security and rescue branch for policemen, prison guards and firefighters.		The vocational schools are usually owned by the municipalities, but in special cases, private or state vocational schools exist. The state grants aid to all vocational schools on the same basis, regardless of the owner. On the other hand, the vocational schools are not allowed to operate for profit. The Ministry of Education issues licences to provide vocational education. In the licence, the municipality or a private entity is given permission to train a yearly quota of students for specific trades. The licence also specifies the area where the school must be located and the languages used in the education.		The vocational school students are selected by the schools on the basis of criteria set by the Ministry of Education. The basic qualification for the study is completed nine-year comprehensive school. Anyone may seek admission in any vocational school regardless of their domicile. In certain trades, bad health or invalidity may be acceptable grounds for refusing admission. The students do not pay tuition and they must be provided with health care and a free daily school lunch. However, the students must pay for the books, although the tools and practice material are provided to the students for free.		In tertiary education, there are higher vocational schools (ammattikorkeakoulu which is translated to "polytechnic" or "university of applied sciences"), which give three- to four-year degrees in more involved fields, like engineering (see insinööri (amk)) and nursing.		In contrast to the vocational school, an academically orientated upper secondary school, or senior high school (Finnish: lukio) teaches no vocational skills. It prepares students for entering the university or a higher vocational school.		A vocational school in Ireland is a type of secondary education school which places a large emphasis on vocational and technical education; this led to some conflict in the 1960s when the Regional Technical College system was in development. Since 2013 the schools have been managed by Education and Training Boards, which replaced Vocational Education Committees which were largely based on city or county boundaries. Establishment of the schools is largely provided by the state; funding is through block grant system providing about 90% of necessary funding requirements.		Vocational schools typically have further education courses in addition to the traditional courses at secondary level. For instance, post leaving certificate courses which are intended for school leavers and pre-third level education students.		Until the 1970s the vocational schools were seen as inferior to the other schools then available in Ireland. This was mainly because traditional courses such as the leaving certificate were not available at the schools, however this changed with the Investment in Education (1962) report which resulted in an upgrade in their status. Currently about 25% of secondary education students attend these schools.		In Japan vocational schools are known as senmon gakkō (専門学校). They are a part of Japan's higher education system. There are two-year schools that many students study at after finishing high school (although it is not always required that students graduate from high school). Some have a wide range of majors, others only a few majors. Some examples are computer technology, fashion and English.		In the Middle Ages boys learned a vocation through an apprenticeship. They were usually 10 years old when they entered service, and were first called leerling (apprentice), then gezel (journeyman) and after an exam - sometimes with an example of workmanship called a meesterproef (masterpiece) - they were called meester (master craftsman). In 1795, all of the guilds in the Netherlands were disbanded by Napoleon, and with them the guild vocational schooling system. After the French occupation, in the 1820s, the need for quality education caused more and more cities to form day and evening schools for various trades. In 1854, the society Maatschappij tot verbetering van den werkenden stand (society to improve the working class) was founded in Amsterdam, that changed its name in 1861 to the Maatschappij voor de Werkende Stand (Society for the working class). This society started the first public vocational school (De Ambachtsschool) in Amsterdam, and many cities followed. At first only for boys, later the Huishoudschool (housekeeping) was introduced as vocational schooling for girls. Housekeeping education began in 1888 with the Haagsche Kookschool in The Hague.		In 1968 the law called the Mammoetwet changed all of this, effectively dissolving the Ambachtsschool and the Huishoudschool. The name was changed to LTS (lagere technische school, lower technical school), where mainly boys went because of its technical nature, and the other option, where most girls went, was LBO (lager beroepsonderwijs, lower vocational education). In 1992 both LTS and LBO changed to VBO (voorbereidend beroepsonderwijs, preparatory vocational education) and since 1999 VBO together with MAVO (middelbaar algemeen voortgezet onderwijs, intermediate general secondary education) changed to the current VMBO (voorbereidend middelbaar beroepsonderwijs, preparatory intermediate vocational education).		In the United States, there is a very large difference between career college and vocational college. The term career college is generally reserved for post-secondary for-profit institutions. Conversely, vocational schools are government-owned or at least government-supported institutions, occupy two full years of study, and their credits are by and large accepted elsewhere in the academic world and in some instances such as charter academies or magnet schools may take the place of the final years of high school.		Career colleges on the other hand are generally not government supported in any capacity, occupy periods of study less than a year, and their training and certifications are rarely recognized by the larger academic world. In addition, as most career colleges are private schools, this group may be further subdivided into non-profit schools and proprietary schools, operated for the sole economic benefit of their owners.		As a result of this emphasis on the commercialization of education, a widespread poor reputation for quality was retained by a great number of career colleges for over promising what the job prospects for their graduates would actually be in their field of study upon completion of their program, and for emphasizing the number of careers from which a student could choose.		Even though the popularity of career colleges has exploded in recent years, the number of government-sponsored vocational schools in the United States has decreased significantly.[citation needed].		The Association for Career and Technical Education (ACTE) is the largest American national education association dedicated to the advancement of career and technical education or vocational education that prepares youth and adults for careers.		Earlier vocational schools such as California Institute of Technology[2] and Carnegie Mellon University have gone on to become full degree-granting institutions.		
Social capital is a form of economic and cultural capital in which social networks are central, transactions are marked by reciprocity, trust, and cooperation, and market agents produce goods and services not mainly for themselves, but for a common good.		The term generally refers to (a) resources, and the value of these resources, both tangible (public spaces, private property) and intangible ("actors", "human capital", people), (b) the relationships among these resources, and (c) the impact that these relationships have on the resources involved in each relationship, and on larger groups. It is generally seen as a form of capital that produces public goods for a common good.		Social capital has been used to explain the improved performance of diverse groups, the growth of entrepreneurial firms, superior managerial performance, enhanced supply chain relations, the value derived from strategic alliances, and the evolution of communities.		During the 1990s and 2000s the concept has become increasingly popular in a wide range of social science disciplines and also in politics.[1][2]						The term social capital was in intermittent use from about 1890, before becoming widely used in the late 1990s.[3]		In the first half of the 19th century, Alexis de Tocqueville had observations about American life that seemed to outline and define social capital. He observed that Americans were prone to meeting at as many gatherings as possible to discuss all possible issues of state, economics, or the world that could be witnessed. The high levels of transparency caused greater participation from the people and thus allowed for democracy to work better. The French writer highlighted also that the level of social participation (social capital) in American society was directly linked to the equality of conditions (Ferragina, 2010; 2012; 2013).		L. J. Hanifan's 1916 article regarding local support for rural schools is one of the first occurrences of the term social capital in reference to social cohesion and personal investment in the community.[4] In defining the concept, Hanifan contrasts social capital with material goods by defining it as:		I do not refer to real estate, or to personal property or to cold cash, but rather to that in life which tends to make these tangible substances count for most in the daily lives of people, namely, goodwill, fellowship, mutual sympathy and social intercourse among a group of individuals and families who make up a social unit… If he may come into contact with his neighbour, and they with other neighbours, there will be an accumulation of social capital, which may immediately satisfy his social needs and which may bear a social potentiality sufficient to the substantial improvement of living conditions in the whole community. The community as a whole will benefit by the cooperation of all its parts, while the individual will find in his associations the advantages of the help, the sympathy, and the fellowship of his neighbours (pp. 130-131).		John Dewey used the term in his monograph entitled "School and Society" in 1900, but he offered no definition of it.		Jane Jacobs used the term early in the 1960s. Although she did not explicitly define the term social capital, her usage referred to the value of networks.[5] Political scientist Robert Salisbury advanced the term as a critical component of interest group formation in his 1969 article "An Exchange Theory of Interest Groups" in the Midwest Journal of Political Science. Sociologist Pierre Bourdieu used the term in 1972 in his Outline of a Theory of Practice,[6] and clarified the term some years later in contrast to cultural, economic, and symbolic capital. Sociologists James Coleman, and Barry Wellman & Scot Wortley adopted Glenn Loury's 1977 definition in developing and popularising the concept.[7] In the late 1990s the concept gained popularity, serving as the focus of a World Bank research programme and the subject of several mainstream books, including Robert Putnam's Bowling Alone[8] and Putnam and Lewis Feldstein's Better Together.		The concept that underlies social capital has a much longer history; thinkers exploring the relation between associational life and democracy were using similar concepts regularly by the 19th century, drawing on the work of earlier writers such as James Madison (The Federalist Papers) and Alexis de Tocqueville (Democracy in America) to integrate concepts of social cohesion and connectedness into the pluralist tradition in American political science. John Dewey may have made the first direct mainstream use of social capital in The School and Society in 1899, though he did not offer a definition.		The power of community governance has been stressed by many philosophers from antiquity to the 18th century, from Aristotle to Thomas Aquinas and Edmund Burke (Bowles and Gintis, 2002).[9] This vision was strongly criticised at the end of the 18th century, with the development of the idea of Homo Economicus and subsequently with rational choice theory. Such a set of theories became dominant in the last centuries, but many thinkers questioned the complicated relationship between modern society and the importance of old institutions, in particular family and traditional communities (Ferragina, 2010:75).[10] The debate of community versus modernization of society and individualism has been the most discussed topic among the fathers of sociology (Tönnies, 1887;[11] Durkheim, 1893;[12] Simmel, 1905;[13] Weber, 1946).[14] They were convinced that industrialisation and urbanization were transforming social relationship in an irreversible way. They observed a breakdown of traditional bonds and the progressive development of anomie and alienation in society (Wilmott, 1986).[15]		After Tönnies' and Weber's works, reflection on social links in modern society continued with interesting contributions in the 1950s and in the 1960s, in particular mass society theory (Bell, 1962;[16] Nisbet, 1969;[17] Stein, 1960;[18] Whyte, 1956).[19] They proposed themes similar to those of the founding fathers, with a more pessimistic emphasis on the development of society (Ferragina, 2010: 76). In the words of Stein (1960:1): "The price for maintaining a society that encourages cultural differentiation and experimentation is unquestionably the acceptance of a certain amount of disorganization on both the individual and social level." All these reflections contributed remarkably to the development of the social capital concept in the following decades.		The appearance of the modern social capital conceptualization is a new way to look at this debate, keeping together the importance of community to build generalized trust and the same time, the importance of individual free choice, in order to create a more cohesive society (Ferragina, 2010;[20] Ferragina, 2012[21] It is for this reason that social capital generated so much interest in the academic and political world (Rose, 2000).[22]		Pierre Bourdieu's work tends to show how social capital can be used practically to produce or reproduce inequality, demonstrating for instance how people gain access to powerful positions through the direct and indirect employment of social connections. Robert Putnam has used the concept in a much more positive light: though he was at first careful to argue that social capital was a neutral term, stating "whether or not [the] shared are praiseworthy is, of course, entirely another matter",[23] his work on American society tends to frame social capital as a producer of "civic engagement" and also a broad societal measure of communal health.[24] He also transforms social capital from a resource possessed by individuals to an attribute of collectives, focusing on norms and trust as producers of social capital to the exclusion of networks.		Mahyar Arefi[25] identifies consensus building as a direct positive indicator of social capital. Consensus implies "shared interest" and agreement among various actors and stakeholders to induce collective action. Collective action is thus an indicator of increased social capital.		Edwards and Foley, as editors of a special edition of the American Behavioural Scientist on "Social Capital, Civil Society and Contemporary Democracy", raised two key issues in the study of social capital. First, social capital is not equally available to all, in much the same way that other forms of capital are differently available. Geographic and social isolation limit access to this resource. Second, not all social capital is created equally. The value of a specific source of social capital depends in no small part on the socio-economic position of the source with society. On top of this, Portes has identified four negative consequences of social capital: exclusion of outsiders; excess claims on group members; restrictions on individual freedom; and downward levelling norms.[26]		An interesting distinction of social organization is that between bonding and bridging ties, which complicates the neo-Tocquevillean view of social capital.[citation needed]		Varshney[27] studied the correlation between the presence of interethnic networks (bridging) versus intra-ethnic ones (bonding) on ethnic violence in India.[28] He argues that interethnic networks are agents of peace because they build bridges and manage tensions, by noting that if communities are organized only along intra-ethnic lines and the interconnections with other communities are very weak or even nonexistent, then ethnic violence is quite likely. Three main implications of intercommunal ties explain their worth:		This is a useful distinction; nevertheless its implication on social capital can only be accepted if one espouses the functionalist understanding of the latter concept. Indeed, it can be argued that interethnic, as well as intra-ethnic networks can serve various purposes, either increasing or diminishing social capital. In fact, Varshney himself notes that intraethnic policing (equivalent to the "self-policing" mechanism proposed by Fearon and Laitin)[29] may lead to the same result as interethnic engagement.		Social capital is often linked to the success of democracy and political involvement. Robert D. Putnam, in his book Bowling Alone makes the argument that social capital is linked to the recent decline in American political participation.[30] Putnam's theoretical framework has been firstly applied to the South of Italy (Putnam, 1993). This framework has been rediscussed by considering simultaneously the condition of European regions and specifically Southern Italy (Ferragina, 2012; Ferragina, 2013).[31]		Social capital has multiple definitions, interpretations, and uses. Thomas Sander[32] defines it as "the collective value of all social networks (who people know), and the inclinations that arise from these networks to do things for each other (norms of reciprocity)." Social capital, in this view, emphasizes "specific benefits that flow from the trust, reciprocity, information, and cooperation associated with social networks". It "creates value for the people who are connected, and for bystanders as well."[33] Meanwhile, negative norms of reciprocity serve as disincentives for detrimental and violent behaviors.[34][35]		David Halpern argues that the popularity of social capital for policymakers is linked to the concept's duality, coming because "it has a hard nosed economic feel while restating the importance of the social." For researchers, the term is popular partly due to the broad range of outcomes it can explain;[36] the multiplicity of uses for social capital has led to a multiplicity of definitions. Social capital has been used at various times to explain superior managerial performance,[37] the growth of entrepreneurial firms,[38] improved performance of functionally diverse groups,[39] the value derived from strategic alliances,[40] and enhanced supply chain relations.[41] 'A resource that actors derive from specific social structures and then use to pursue their interests; it is created by changes in the relationship among actors'; (Baker 1990, p. 619).		Early attempts to define social capital focused on the degree to which social capital as a resource should be used for public good or for the benefit of individuals. Putnam[42] suggested that social capital would facilitate co-operation and mutually supportive relations in communities and nations and would therefore be a valuable means of combating many of the social disorders inherent in modern societies, for example crime. In contrast to those focusing on the individual benefit derived from the web of social relationships and ties individual actors find themselves in, attribute social capital to increased personal access to information and skill sets and enhanced power.[43] According to this view, individuals could use social capital to further their own career prospects, rather than for the good of organisations.		In The Forms of Capital[44] Pierre Bourdieu distinguishes between three forms of capital: economic capital, cultural capital and social capital. He defines social capital as "the aggregate of the actual or potential resources which are linked to possession of a durable network of more or less institutionalized relationships of mutual acquaintance and recognition."[45] His treatment of the concept is instrumental, focusing on the advantages to possessors of social capital and the "deliberate construction of sociability for the purpose of creating this resource."[26] Quite contrary to Putnam's positive view of social capital, Bourdieu employs the concept to demonstrate a mechanism for the generational reproduction of inequality. Bourdieu thus points out that the wealthy and powerful use their "old boys network" or other social capital to maintain advantages for themselves, their social class, and their children.		James Coleman defined social capital functionally as "a variety of entities with two elements in common: they all consist of some aspect of social structure, and they facilitate certain actions of actors...within the structure"[26]—that is, social capital is anything that facilitates individual or collective action, generated by networks of relationships, reciprocity, trust, and social norms. In Coleman's conception, social capital is a neutral resource that facilitates any manner of action, but whether society is better off as a result depends entirely on the individual uses to which it is put.[23]		According to Robert Putnam, social capital "connections among individuals - social networks and the norms of reciprocity and trustworthiness that arise from them."[46] According to Putnam and his followers, social capital is a key component to building and maintaining democracy. Putnam says that social capital is declining in the United States. This is seen in lower levels of trust in government and lower levels of civic participation. Putnam also says that television and urban sprawl have had a significant role in making America far less 'connected'. Putnam believes that social capital can be measured by the amount of trust and "reciprocity" in a community or between individuals.[citation needed]		Putnam also suggests that a root cause of the decline in social capital is women's entry the workforce, which could correlate with time restraints that inhibit civic organizational involvement like parent-teacher associations.[47] Technological transformation of leisure (e.g., television) is another cause of declining social capital, as stated by Putnam. This offered a reference point from which several studies assessed social capital measurements by how media is engaged strategically to build social capital ([48]		Nan Lin's concept of social capital has a more individualistic approach: "Investment in social relations with expected returns in the marketplace." This may subsume the concepts of some others such as Bourdieu, Flap and Eriksson.[49]		Newton (1997)[citation needed] considered social capital as subjective phenomenon formed by values and attitudes which influence interactions.		In “Social capital, civil society, and development,” political economist Frances Fukuyama defines social capital as generally understood rules than enable people to cooperate such as the norm of reciprocity or religious doctrine like Christianity. Social capital is formed by repeated interactions over time and he argues is critical for development and difficult to generate through public policy. The importance of social capital for economic development is that these norms of behavior reduce transaction cost of exchange such as legal contracts and government regulations. Fukuyama suggests that while social capital is beneficial for development, it also imposes cost on non-group members with unintended consequences for general welfare. Referencing Alexis de Tocqueville in Democracy in America, and what he described as the ‘art of association’ of Americans’ propensity for civil association, Fukuyama argues social capital is what produces a civil society. While civic engagement is an important part of democracy and development, Fukuyama states that, “one person’s civic engagement is another’s rent-seeking.” Therefore, while social capital can facilitate economic development by reducing transaction cost and increasing productivity, social capital can also distort democracy if civic association enables special interest to gain special favors. However, Fukuyama argues despite the risk of society having too much social capital, it is nonetheless worse to have too little and be unable to organize for public goods and welfare enhancing activity.		Nahapiet and Ghoshal in their examination of the role of social capital in the creation of intellectual capital, suggest that social capital should be considered in terms of three clusters: structural, relational, and cognitive.[50] Carlos García Timón describes that the structural dimensions of social capital relate to an individual ability to make weak and strong ties to others within a system. This dimension focuses on the advantages derived from the configuration of an actor's, either individual or collective, network.[citation needed] The differences between weak and strong ties are explained by Granovetter.[51] The relational dimension focuses on the character of the connection between individuals. This is best characterized through trust of others and their cooperation and the identification an individual has within a network. Hazleton and Kennan[52] added a third angle, that of communication. Communication is needed to access and use social capital through exchanging information, identifying problems and solutions, and managing conflict. According to Boisot[53] and Boland and Tenkasi,[54] meaningful communication requires at least some sharing context between the parties to such exchange. The cognitive dimension focuses on the shared meaning and understanding that individuals or groups have with one another.[citation needed]		A number of scholars have raised concerns about lack of precise definition of social capital. Portes, for example, noted that the term has become so widely used, including in mainstream media, that "the point is approaching at which social capital comes to be applied to so many events and in so many different contexts as to lose any distinct meaning."[55] Robison, Schmid, and Siles[56] reviewed various definitions of social capital and concluded that many did not satisfy the formal requirement of a definition. They noted that definitions must be of the form A=B while many definition of social capital described what it can be used to achieve, where it resides, how it can be created, and what it can transform. In addition, they argue that many proposed definition of social capital fail to satisfy the requirements of capital. They propose that social capital be defined as "sympathy". The object of another's sympathy has social capital. Those who have sympathy for others provide social capital. One of the main advantages of having social capital is that it provides access to resources on preferential terms. Their definition of sympathy follows that used by Adam Smith, the title of his first chapter in the "Theory of Moral Sentiments."		A network-based conception can also be used for characterizing the social capital of collectivities (such as organizations or business clusters).[57]		The modern emergence of social capital concept renewed the academic interest for an old debate in social science: the relationship between trust, social networks and the development of modern industrial society. Social Capital Theory gained importance through the integration of classical sociological theory with the description of an intangible form of capital. In this way the classical definition of capital has been overcome allowing researchers to tackle issues in a new manner (Ferragina, 2010:73). Through the social capital concept researchers have tried to propose a synthesis between the value contained in the communitarian approaches and individualism professed by the 'rational choice theory.' Social capital can only be generated collectively thanks to the presence of communities and social networks, but individuals and groups can use it at the same time. Individuals can exploit social capital of their networks to achieve private objectives and groups can use it to enforce a certain set of norms or behaviors. In this sense, social capital is generated collectively but it can also be used individually, bridging the dichotomized approach 'communitarianism' versus 'individualism' (Ferragina, 2010:75).[58]		The term capital is used by analogy with other forms of economic capital, as social capital is argued to have similar (although less measurable) benefits. However, the analogy with capital is misleading to the extent that, unlike traditional forms of capital, social capital is not depleted by use;[59] in fact it is depleted by non-use (use it or lose it). In this respect, it is similar to the now well-established economic concept of human capital.		Social capital is also distinguished from the economic theory social capitalism. Social capitalism as a theory challenges the idea that socialism and capitalism are mutually exclusive. Social capitalism posits that a strong social support network for the poor enhances capital output. By decreasing poverty, capital market participation is enlarged.		In Bowling Alone: The Collapse and Revival of American Community (Simon & Schuster, 2000), Harvard political scientist Robert D. Putnam wrote: "Henry Ward Beecher's advice a century ago to 'multiply picnics' is not entirely ridiculous today. We should do this, ironically, not because it will be good for America — though it will be — but because it will be good for us."[8] This quote is illustrative of the use of social capital within neo-liberal discourse to divert attention away from economic inequality as the source of social problems[citation needed].		Daniel P. Aldrich, Associate Professor at Purdue University, describes three mechanisms of social capital. Aldrich defines the three differences as bonding, bridging, and linking social capital. Bonding capital are the relationships a person has with friends and family, making it also the strongest form of social capital. Bridging capital is the relationship between friends of friends, making its strength secondary to bonding capital. Linking capital is the relationship between a person and a government official or other elected leader. Aldrich also applies the ideas of social capital to the fundamental principles of disaster recovery, and discusses factors that either aid or impede recovery, such as extent of damage, population density, quality of government and aid. He primarily examines Japanese recovery following the 2011 Fukishima nuclear meltdown in his book "Building Resilience: Social Capital in Post-Disaster Recovery."[citation needed]		Putnam speaks of two main components of the concept: bonding social capital and bridging social capital, the creation of which Putnam credits to Ross Gittell and Avis Vidal. Bonding refers to the value assigned to social networks between homogeneous groups of people and Bridging refers to that of social networks between socially heterogeneous groups. Typical examples are that criminal gangs create bonding social capital, while choirs and bowling clubs (hence the title, as Putnam lamented their decline) create bridging social capital.[60]		The distinction is useful in highlighting how social capital may not always be beneficial for society as a whole (though it is always an asset for those individuals and groups involved). Horizontal networks of individual citizens and groups that enhance community productivity and cohesion are said to be positive social capital assets whereas self-serving exclusive gangs and hierarchical patronage systems that operate at cross purposes to societal interests can be thought of as negative social capital burdens on society.		Social capital development on the internet via social networking websites such as Facebook or Myspace tends to be bridging capital according to one study, though "virtual" social capital is a new area of research.[61]		There are two other sub-sources of social capital. These are consummatory, or a behavior that is made up of actions that fulfill a basis of doing what is inherent, and instrumental, or behavior that is taught through ones surroundings over time.[62]		Two examples of consummatory social capital are value interjection and solidarity. Value interjection pertains to a person or community that fulfills obligations such as paying bills on time, philanthropy, and following the rules of society. People that live their life this way feel that these are norms of society and are able to live their lives free of worry for their credit, children, and receive charity if needed. Coleman goes on to say that when people live in this way and benefit from this type of social capital, individuals in the society are able to rest assured that their belongings and family will be safe.[63] This understanding of solidarity may be traced to 19th century socialist thinkers. The main focus of these thinkers was the urban working class of the Industrial Revolution. They analyzed the reasons these workers supported each other for the benefit of the group and held that this support was an adaptation to the immediate social environment, as opposed to a trait that had been taught to the workers in their youth.[62] As another example, Coleman states that possessing this type of social capital individuals to stand up for what they believe in, and even die for it, in the face of adversity.[64] (While the notion of solidarity as social capital is sometimes attributed to Karl Marx, in particular, the term "social capital" had a quite different meaning for Marx. All forms of "capital" were, for Marx, possessed only by capitalists and he emphasied the basis of labour in capitalist society, as a class constituted by individuals obliged to sell their labour power, because they lacked sufficient capital, in any sense of the word, to do otherwise. Marx saw "social capital" as a theoretical total amount of capital, purely in the sense of accumulated wealth or property, that existed within in a particular society. He thereby contrasted it with specific and discrete "individual capital".[65])		The second of these two other sub-sources of social capital is that of instrumental social capital. The basis of the category of social capital is that an individual who donates his or her resources not because he is seeking direct repayment from the recipient, but because they are part of the same social structure. By his or her donation, the individual might not see a direct repayment, but, most commonly, they will be held by the society in greater honor.[64] The best example of this, and the one that Portes mentions, is the donation of a scholarship to a member of the same ethnic group. The donor is not freely giving up his resources to be directly repaid by the recipient, but, as stated above, the honor of the community. With this in mind, the recipient might not know the benefactor personally, but he or she prospers on the sole factor that he or she is a member of the same social group.[66]		There is no widely held consensus on how to measure social capital, which has become a debate in itself.[67] Why refer to this phenomenon as 'capital' if there is no true way to measure it? While one can usually intuitively sense the level/amount of social capital present in a given relationship[citation needed](regardless of type or scale), quantitative measuring has proven somewhat complicated. This has resulted in different metrics for different functions.		One type of quantitative social capital measure uses name generators to construct social networks and to measure the level of social capital. These networks are constructed by asking participants to name people that they interact with, such as "Name all the people you've discussed important matters within the past six months." [68] Name generators are often useful to construct core discussion networks of close ties, rather than weaker ties.		Many studies measure social capital by asking the question: "do you trust the others?" Other researches analyse the participation in voluntary associations or civic activities.		To expand upon the methodological potential of measuring online and offline social bonding, as it relates to social capital,[69] offers a matrix of social capital measures that distinguishes social bridging as a form of less emotionally tethered relationships compared to bonding. Bonding and bridging sub-scales are proposed, which have been adopted by over 300 scholarly articles.[70] Lin, Peng, Kim, Kim & LaRose (2012) offer a noteworthy application of the scale by measuring international residents originating from locations outside of the United States. The study found that social media platforms like Facebook provide an opportunity for increased social capital, but mostly for extroverts. However, less introverted social media users could engage social media and build social capital by connecting with Americans before arriving and then maintaining old relationships from home upon arriving to the states. The ultimate outcome of the study indicates that social capital is measurable and is a concept that may be operationalized to understand strategies for coping with cross-cultural immersion through online engagement.		The level of cohesion of a group also affects its social capital.[71] However, there is no one quantitative way of determining the level of cohesiveness, but rather a collection of social network models that researchers have used over the decades to operationalize social capital. One of the dominant methods is Ronald Burt's constraint measure, which taps into the role of tie strength and group cohesion. Another network-based model is network transitivity.		In measuring political social capital, it is common to take the sum of society's membership of its groups. Groups with higher membership (such as political parties) contribute more to the amount of capital than groups with lower membership, although many groups with low membership (such as communities) still add up to be significant. While it may seem that this is limited by population, this need not be the case as people join multiple groups. In a study done by Yankee City,[72] a community of 17,000 people was found to have over 22,000 different groups.		Knack and Keefer (1996) measured econometrically correlations between confidence and civic cooperation norms, with economic growth in a big group of countries. They found that confidence and civic cooperation have a great impact in economic growth, and that in less polarized societies in terms of inequality and ethnic differences, social capital is bigger.		Narayan and Pritchet (1997) researched the associativity degree and economic performance in rural homes of Tanzania. They saw that even in high poverty indexes, families with higher levels of incomes had more participation in collective organizations. The social capital they accumulated because of this participation had individual benefits for them, and created collective benefits through different routes, for example: their agricultural practices were better than those of the families without participation (they had more information about agrochemicals, fertilizers and seeds); they had more information about the market; they were prepared to take more risks, because being part of a social network made them feel more protected; they had an influence on the improvement of public services, showing a bigger level of participation in schools; they cooperated more in the municipality level.		How a group relates to the rest of society also affects social capital, but in a different manner. Strong internal ties can in some cases weaken the group's perceived capital in the eyes of the general public, as in cases where the group is geared towards crime, distrust, intolerance, violence or hatred towards others. The Ku Klux Klan is an example of this kind of organizations.		Sociologists Carl L. Bankston and Min Zhou have argued that one of the reasons social capital is so difficult to measure is that it is neither an individual-level nor a group-level phenomenon, but one that emerges across levels of analysis as individuals participate in groups. They argue that the metaphor of "capital" may be misleading because unlike financial capital, which is a resource held by an individual, the benefits of forms of social organization are not held by actors, but are results of the participation of actors in advantageously organized groups.[73]		Recently, Foschi and Lauriola presented a measure of sociability as a proxy of social capital. The authors demonstrated that facets of sociability can mediate between general personality traits and measures of civic involvement and political participation, as predictors of social capital, in a holistic model of political behavior.[74]		Robert Putnam's work contributed to shape the discussion of the importance of social capital. His conclusions have been praised but also criticized. Criticism has mainly focused on:		Ferragina (2012;[21] 2013) integrated the insights of these two criticisms and proposed a cross-regional analysis of 85 European regions, linking together the socio-economic and the historic- institutional analyses to explore the determinants of social capital. He argued that to investigate the determinants of social capital, one has to integrate the synchronic and the diachronic perspectives under the guidance of a methodological framework able to put these two approaches in continuity.		Putnam's work, nourished by doctrines like the end of history (Fukuyama 1992)[86] was largely deterministic, and proposed the dismissal of more articulated historical interpretations. This determinism has reduced Southern Italian history as being a negative path to modernity; only the Italian regions that experienced the development of medieval towns during the twelfth and thirteenth centuries have got high levels of social capital today, the others 'are condemned' by the prevalence of the authoritarian rule of the Normans more than 800 years ago.[87]		However, from a purely historical perspective, the medieval town is not unanimously considered to be a symbol of freedom, creation of horizontal ties and embryo of democratic life. In Making Democracy Work, Putnam disregarded the division within municipal towns and their dearth of civic participation and considered only the experience of few areas in North Central Italy, ignoring the existence of important towns in the South.[88]		To this more complicated historical picture, Ferragina (2012)[21] added the result of a regression model, which indicated that social capital in the South of Italy and Wallonia should be much lower than currently detected according to their socio-economic condition. He unfolded Putnam's theory by undertaking a comparative analysis between these two deviant cases and two regular cases located in the same country, namely Flanders and the North of Italy. The historical legacy does not have a negative effect on the present lack of social capital in Wallonia and the South of Italy, but the potentially positive effect of the historical legacy is currently curtailed by the poor socio-economic conditions, notably by the high level of income inequality and the low level of labour market participation. This historical interpretation is driven by the comparison with Flanders and the North East of Italy.		The value of the historical legacy for present socio-economic development is similar to the 'appropriable social capital' theorized by Coleman (1990)[89] at the individual level.[90] Using the example of the Korean students, Coleman argued that the construction of a secret network of people (at a time in which the appreciation for the authoritarian government was rapidly declining among the population) as a means of organizing the democratic revolt was the result of a process of socialization that took place during their childhood (with the involvement in the local churches).		The relation between historical evolutions and the socio-economic variables has similar characteristics at the macro level.[90] Only after reaching a sufficient level of labour market activity and income redistribution (this is comparable to the growing unpopularity of the authoritarian government) can the memory of historical events of social engagement become fully appropriable by the population (this is comparable to the participation in the local churches during childhood), leading to the development of innovative forms of social participation (this is comparable to the construction of the secret circles that enhanced the democratic revolt). This process increases social capital even further if socio-economic development is matched by the revival of the unique historical legacy of the area.[91] The reconstruction of this unique past can rapidly become a source of pride for the entire area, contributing in turn to an increasing intra-regional solidarity, and with it enhancement of social networks and social trust.		The Flemish case (and also to a lesser extent that of the North East of Italy) illustrates this process well. The socio-economic improvements that took place in the nineteenth century were matched by the revival of the glorious Flemish traditions of the thirteenth and fourteenth century. The increase of social capital generated by the reduction of income inequality and the increasing participation in the labour market due to the economic development was multiplied by the reconstruction of Flemish identity and pride. This pride and self-confidence has, in turn, increased the feeling of solidarity within the region and contributed to generate a level of social capital, which is hardly explicable by the single socio-economic predictors.[90]		Ferragina suggests that, in the divergent cases, the value of the historical legacy is affected by the poor present socio-economic conditions. Social capital sleeps, not because of the absence of certain clearly defined historical steps as suggested by Putnam, but because socio-economic underdevelopment profoundly depressed the self-pride of Southern Italians and Walloons.		The biased and simplistic interpretations of Southern Italian and Walloon history will be discarded only when their socio-economic conditions reach a sufficient level, enacting a cycle similar to Flanders and the North East of Italy. Stronger redistribution, an increase of labour market participation accompanied by a simultaneous process of 'reinvention of the past' could enhance a positive cycle of social capital increase in both areas. The historical legacy in these two areas should not be seen as the root of the present lack of social capital but as a potential element for improvement. Important moments of social engagement also existed in the history of these two areas; the imagery of Walloons and Southern Italians should be nourished by these almost forgotten examples of collective history (i.e. the Fasci Siciliani in the south of Italy) rather than the prevailing idea that the historical legacy of these areas is simply an original sin, a burden to carry through the process of modernization.[90]		Robison and colleagues measured the relative importance of selfishness and four social capital motives using resource allocation data collected in hypothetical surveys and non-hypothetical experiments. The selfishness motive assumes that an agent's allocation of a scarce resource is independent of his relationships with others. This motive is sometimes referred to as the selfishness of preference assumption in neoclassical economics. Social capital motives assume that agents' allocation of a scarce resource may be influenced by their social capital or sympathetic relationships with others which may produce socio-emotional goods that satisfy socio-emotional needs for validation and belonging. The first social capital motive seeks for validation by acting consistently with the values of one's ideal self. The second social capital motive seeks to be validated by others by winning their approval. The third social capital motive seeks to belong. Recognizing that one may not be able to influence the sympathy of others, persons seeking to belong may act to increase their own sympathy for others and the organizations or institutions they represent. The fourth social capital motive recognizes that our sympathy or social capital for another person will motivate us to act in their interest. In doing so we satisfy our own needs for validation and belonging. Empirical results reject the hypothesis often implied in economics that we are 95% selfish.[92]		The social capital concept has influenced academic literature and public debate through the specter of social disintegration: would anybody disagree with the fact that we need healthy communities and civic engagement to protect our democracies? Ferragina and Arrigoni have argued that the popularity of this theory is rooted in the connection made with neoliberalism by James Coleman (1990) and Robert Putnam (1993). They contend that social capital theory has become an analytical tool to avoid the debate on the effects of neoliberal policies on civic engagement (Ferragina and Arrigoni 2016: 9[93]).		More specifically, by elaborating the most popular version of social capital theory, Putnam (1993) revitalised Tocqueville's seminal work on American democracy, showing that 'the health of liberal democracy' depends upon social engagement. However, in linking social capital, neoliberalism, and rational choice theory, Putnam did not consider that the intensity of social engagement in a society tends to be strictly related to the level of economic inequality (Ferragina, 2010, 2012) and other structural factors (Costa and Kahn, 2003), such as the universal nature of the welfare state (Rothstein, 2008). Hence, by arguing that the disadvantaged need more social capital to insure themselves against the odds of a competitive world, Putnam implicitly suggests that being powerless is a result of not having enough capital rather than a structural problem of society (Ferragina and Arrigoni 2016).		However, in a period during which neoliberal governance is showing many drawbacks and the marked incapacity to deliver economic growth (Piketty, 2014), it is possible that to strengthen secondary groups and social engagement, more equality and greater levels of solidarity are needed (as classically argued by Tocqueville, see Ferragina, 2010).		There is a tension between the individualisation of social risks pursued by several political parties and the call to create social capital: it is becoming harder to blame the individual for collective problems. Prior to the start of the economic crisis in 2008, the tension between rising economic inequality and the demand to strengthen civic engagement was undermined by neoliberalism's capacity to sustain a certain level of economic growth. One might claim this capacity contributed to a transposition of social capital theory within public discourse. The limitations of finance as the central engine of economic growth, the material hardships fostered by the crisis, and the austerity measures implemented by governments in response to these challenges are critically undermining the legitimacy of neoliberal policies (Ferragina and Arrigoni 2016: 10).		A number of authors[24][94][95][96] give definitions of civil society that refer to voluntary associations and organisations outside the market and state. This definition is very close to that of the third sector, which consists of "private organisations that are formed and sustained by groups of people acting voluntarily and without seeking personal profit to provide benefits for themselves or for others".[citation needed] According to such authors as Walzer, Alessandrini, Newtown, Stolle and Rochon, Foley and Edwards, and Walters, it is through civil society, or more accurately, the third sector, that individuals are able to establish and maintain relational networks. These voluntary associations also connect people with each other, build trust and reciprocity through informal, loosely structured associations, and consolidate society through altruism without obligation. It is "this range of activities, services and associations produced by... civil society"[24] that constitutes the sources of social capital.		If civil society, then, is taken to be synonymous with the third sector then the question it seems is not 'how important is social capital to the production of a civil society?' but 'how important is civil society to the production of social capital?'.[original research?] Not only have the authors above documented how civil society produces sources of social capital, but in Lyons work Third Sector,[97] social capital does not appear in any guise under either the factors that enable or those that stimulate the growth of the third sector, and Onyx[98] describes how social capital depends on an already functioning community.		The idea that creating social capital (i.e., creating networks) will strengthen civil society underlies current Australian social policy aimed at bridging deepening social divisions. The goal is to reintegrate those marginalised from the rewards of the economic system into "the community". However, according to Onyx (2000), while the explicit aim of this policy is inclusion, its effects are exclusionary.		Foley and Edwards[99] believe that "political systems... are important determinants of both the character of civil society and of the uses to which whatever social capital exists might be put".[23] Alessandrini agrees, saying, "in Australia in particular, neo-liberalism has been recast as economic rationalism and identified by several theorists and commentators as a danger to society at large because of the use to which they are putting social capital to work".[24]		The resurgence of interest in social capital as a remedy for the cause of today's social problems draws directly on the assumption that these problems lie in the weakening of civil society. However this ignores the arguments of many theorists who believe that social capital leads to exclusion[citation needed] rather than to a stronger civil society. In international development, Ben Fine and John Harriss have been heavily critical of the inappropriate adoption of social capital as a supposed panacea (promoting civil society organisations and NGOs, for example, as agents of development) for the inequalities generated by neo liberal economic development.[100][101] This leads to controversy as to the role of state institutions in the promotion of social capital. An abundance of social capital is seen as being almost a necessary condition for modern liberal democracy. A low level of social capital leads to an excessively rigid and unresponsive political system and high levels of corruption, in the political system and in the region as a whole. Formal public institutions require social capital in order to function properly, and while it is possible to have too much social capital (resulting in rapid changes and excessive regulation), it is decidedly worse to have too little.		Kathleen Dowley and Brian Silver published an article entitled "Social Capital, Ethnicity and Support for Democracy in the Post-Communist States". This article found that in post-communist states, higher levels of social capital did not equate to higher levels of democracy. However, higher levels of social capital led to higher support for democracy.[102]		A number of intellectuals in developing countries have argued that the idea of social capital, particularly when connected to certain ideas about civil society, is deeply implicated in contemporary modes of donor and NGO driven imperialism and that it functions, primarily, to blame the poor for their condition.[103]		The concept of social capital in a Chinese social context has been closely linked with the concept of guanxi.		An interesting attempt to measure social capital spearheaded by Corporate Alliance[104] in the English speaking market segment of the United States of America and Xentrum[105] through the Latin American Chamber of Commerce[106] in Utah on the Spanish speaking population of the same country, involves the quantity, quality and strength of an individual social capital. With the assistance of software applications and web-based relationship-oriented systems such as LinkedIn, these kinds of organizations are expected to provide its members with a way to keep track of the number of their relationships, meetings designed to boost the strength of each relationship using group dynamics, executive retreats and networking events as well as training in how to reach out to higher circles of influential people.		There are many factors that drive volume towards the ballot box, including education, employment, civil skills, and time. Careful evaluation of these fundamental factors often suggests that women do not vote at similar levels as men. However the gap between women and men voter turnout is diminishing and in some cases women are becoming more prevalent at the ballot box than their male counterparts. Recent research[107] on social capital is now serving as an explanation for this change.		Social capital offers a wealth of resources and networks that facilitate political engagement. Since social capital is readily available no matter the type of community, it is able to override more traditional queues for political engagement; e.g.: education, employment, civil skills, etc.		There are unique ways in which women organize. These differences from men make social capital more personable and impressionable to women audiences thus creating a stronger presence in regards to political engagement. A few examples of these characteristics are:		The often informal nature of female social capital allows women to politicize apolitical environments without conforming to masculine standards, thus keeping this activity at a low public profile. These differences are hard to recognize within the discourse of political engagement and may explain why social capital has not been considered as a tool for female political engagement until as of late.[107]		A growing body of research has found that the presence of social capital through social networks and communities has a protective quality on health. Social capital affects health risk behavior in the sense that individuals who are embedded in a network or community rich in support, social trust, information, and norms, have resources that help achieve health goals.[110] For example, a person who is sick with cancer may receive information, money, or moral support he or she needs to endure treatment and recover. Social capital also encourages social trust and membership. These factors can discourage individuals from engaging in risky health behaviors such as smoking and binge drinking.[111] Furthermore, neighbourhood social capital may also aid in buffering health inequities amongst children and adolescents.[112]		Inversely, a lack of social capital can impair health. For example, results from a survey given to 13- to 18-year-old students in Sweden showed that low social capital and low social trust are associated with higher rates of psychosomatic symptoms, musculoskeletal pain, and depression.[113] Additionally, negative social capital can detract from health. Although there are only a few studies that assess social capital in criminalized populations, there is information that suggests that social capital does have a negative effect in broken communities. Deviant behavior is encouraged by deviant peers via favorable definitions and learning opportunities provided by network-based norms.[114] However, in these same communities, an adjustment of norms (i.e. deviant peers being replaced by positive role models) can pose a positive effect.		Similar to watching the news and keeping abreast of current events, the use of the Internet can relate to an individual's level of social capital. In one study, informational uses of the Internet correlated positively with an individual's production of social capital, and social-recreational uses were negatively correlated (higher levels of these uses correlated with lower levels of social capital).[115] An example supporting the former argument is the contribution of Peter Maranci's blog (Charlie on the Commuter Line) to address the train problems in Massachusetts. He created it after an incident where a lady passed out during a train ride due to the congestion in the train and help was delayed because of the congestion in the train and the inefficiency of the train conductor. His blog exposed the poor conditions of train stations, overcrowding train rides and inefficiency of the train conductor which eventually influenced changes within the transit system.[116] Another perspective holds that the rapid growth of social networking sites such as Facebook and Myspace suggests that individuals are creating a virtual-network consisting of both bonding and bridging social capital. Unlike face to face interaction, people can instantly connect with others in a targeted fashion by placing specific parameters with internet use. This means that individuals can selectively connect with others based on ascertained interests, and backgrounds. Facebook is currently the most popular social networking site and touts many advantages to its users including serving as a social lubricant for individuals who otherwise have difficulties forming and maintaining both strong and weak ties with others.[117]		This argument continues, although the preponderance of evidence shows a positive association between social capital and the internet. Critics of virtual communities believe that the Internet replaces our strong bonds with online "weak-ties"[118] or with socially empty interactions with the technology itself.[119] Others fear that the Internet can create a world of "narcissism of similarity," where sociability is reduced to interactions between those that are similar in terms of ideology, race, or gender.[120] A few articles suggest that technologically based interactions has a negative relationship with social capital by displacing time spent engaging in geographical/ in-person social activities.[118] However, the consensus of research shows that the more time people spend online the more in-person contact they have, thus positively enhancing social capital.[121][122][123]		Recent research, conducted 2006, also shows that Internet users often have wider networks than those who uses internet irregularly or not at all. When not considering family and work contacts, Internet users actually tend to have contact with a higher number of friends and relatives.[124] This is supported by another study that shows that internet users and non-internet users do feel equally close to the same number of people; also the internet users maintain relationships with 20% more people that they "feel somewhat close" to.[125]		Other research shows that younger people use the Internet as a supplemental medium for communication, rather than letting the Internet communication replace face-to-face contact.[126] This supports the view that Internet communication does not hinder development of social capital and does not make people feel lonelier than before.		Ellison, Steinfield & Lampe (2007) suggest social capital exercised online is a result of relationships formed offline; whereby, bridging capital is enabled through a "maintenance" of relationships. Among respondents of this study, social capital built exclusively online creates weaker ties.[127] A distinction of social bonding is offered by Ellison et al., 2007, suggesting bonds, or strong ties, are possible through social media, but less likely.		Coleman and Hoffer collected quantitative data of 28,000 students in total 1,015 public, Catholic and other private high schools in America from the 7 years' period from 1980 to 1987.[128] It was found from this longitudinal research that social capital in students' families and communities attributed to the much lower dropout rates in Catholic schools compared with the higher rates in public.		Teachman et al.[129] further develop the family structure indicator suggested by Coleman. They criticise Coleman, who used only the number of parents present in the family, neglected the unseen effect of more discrete dimensions such as stepparents' and different types of single-parent families. They take into account of a detailed counting of family structure, not only with two biological parents or stepparent families, but also with types of single-parent families with each other (mother-only, father-only, never-married, and other). They also contribute to the literature by measuring parent-child interaction by the indicators of how often parents and children discuss school-related activities.		Morgan and Sorensen[130] directly challenge Coleman for his lacking of an explicit mechanism to explain why Catholic schools students perform better than public school students on standardised tests of achievement.[131] Researching students in Catholic schools and public schools again, they propose two comparable models of social capital effect on mathematic learning. One is on Catholic schools as norm-enforcing schools whereas another is on public schools as horizon-expanding schools. It is found that while social capital can bring about positive effect of maintaining an encompassing functional community in norm-enforcing schools, it also brings about the negative consequence of excessive monitoring. Creativity and exceptional achievement would be repressed as a result. Whereas in horizon expanding school, social closure is found to be negative for student's mathematic achievement. These schools explore a different type of social capital, such as information about opportunities in the extended social networks of parents and other adults. The consequence is that more learning is fostered than norm-enforcing Catholic school students. In sum, Morgan and Sorensen's (1999) study implies that social capital is contextualised, one kind of social capital may be positive in this setting but is not necessarily still positive in another setting.[130]		In the setting of education through Kilpatrick et al., (2010)[132] state, '... social capital is a useful lens for analysing lifelong learning and its relationship to community development'. Social capital is particularly important in terms of education. Also the importance of education with '...schools being designed to create "functioning community"- forging tighter links between parents and the school' (Coleman & Hoffer, 1987) linking that without this interaction, the social capital in this area is disadvantaged and demonstrates that social capital plays a major role in education.		Without social capital in the area of education, teachers and parents that play a responsibility in a students learning, the significant impacts on their child's academic learning can rely on these factors. With focus on parents contributing to their child's academic progress as well as being influenced by social capital in education. Without the contribution by the parent in their child's education, gives parents less opportunity and participation in the student's life. As Tedin et al. (2010)[133] state '...one of the most important factors in promoting student success is the active involvement of parents in a child's education. With parents also involved in activities and meetings the school conducts, the more involved parents are with other parents and the staff members. Thus parent involvement contributes to social capital with becoming more involved in the school community and participating makes the school a sustainable and easy to run community.		In their journal article "Beyond social capital: Spatial dynamics of collective efficacy for children", Sampson et al.[134] stress the normative or goal-directed dimension of social capital. They claim, "resources or networks alone (e.g. voluntary associations, friendship ties, organisational density) are neutral--- they may or may not be effective mechanism for achieving intended effect"[135]		Marjoribanks and Kwok[136] conducted a survey in Hong Kong secondary schools with 387 fourteen-year-old students with an aim to analyse female and male adolescents differential educational achievement by using social capital as the main analytic tool. In that research, social capital is approved of its different effects upon different genders. In his thesis "New Arrival Students in Hong Kong: Adaptation and School Performance", Hei Hang Hayes Tang argues that adaptation is a process of activation and accumulation of (cultural and social) capitals. The research findings show that supportive networks is the key determinant differentiating the divergent adaptation pathways. Supportive networks, as a form of social capital, is necessary for activating the cultural capital the newly arrived students possessed. The amount of accumulated capital is also relevant to further advancement in the ongoing adaptation process.[137]		Min Zhou and Carl L. Bankston[138] in their study of a Vietnamese community in New Orleans find that preserving traditional ethnic values enable immigrants to integrate socially and to maintain solidarity in an ethnic community. Ethnic solidarity is especially important in the context where immigrants just arrive in the host society. In her article "Social Capital in Chinatown", Zhou examines how the process of adaptation of young Chinese Americans is affected by tangible forms of social relations between the community, immigrant families, and the younger generations.[139] Chinatown serves as the basis of social capital that facilitates the accommodation of immigrant children in the expected directions. Ethnic support provides impetus to academic success. Furthermore, maintenance of literacy in native language also provides a form of social capital that contributes positively to academic achievement. Stanton-Salazar and Dornbusch[140] found that bilingual students were more likely to obtain the necessary forms of institutional support to advance their school performance and their life chances.		Putnam (2000) mentions in his book Bowling Alone, "Child development is powerfully shaped by social capital" and continues "presence of social capital has been linked to various positive outcomes, particularly in education".[141] According to his book, these positive outcomes are the result of parents' social capital in a community. In states where there is a high social capital, there is also a high education performance.[142] The similarity of these states is that parents were more associated with their children's education. Teachers have reported that when the parents participate more in their children's education and school life, it lowers levels of misbehavior, such as bringing weapons to school, engaging in physical violence, unauthorized absence, and being generally apathetic about education.[143] Borrowing Coleman's quotation from Putnam's book, Coleman once mentioned we cannot understate "the importance of the embeddedness of young persons in the enclaves of adults most proximate to them, first and most prominent the family and second, a surrounding community of adults".[144]		In order to understand social capital as a subject in geography, one must look at it in a sense of space, place, and territory. In its relationship, the tenets[who?] of geography relate to the ideas of social capital in the family, community, and in the use of social networks. The biggest advocate for seeing social capital as a geographical subject was American economist and political scientist Robert Putnam. His main argument for classifying social capital as a geographical concept is that the relationships of people is shaped and molded by the areas in which they live.[145]		Putnam (1993) argued that the lack of social capital in the South of Italy was more the product of a peculiar historical and geographical development than the consequence of a set of contemporary socio-economic conditions. This idea has sparked a lengthy debate and received fierce criticism (Ferragina, 2010; Ferragina 2012: 3).[146][147] There are many areas in which social capital can be defined by the theories and practices. Anthony Giddens developed a theory in 1984 in which he relates social structures and the actions that they produce. In his studies, he does not look at the individual participants of these structures, but how the structures and the social connections that stem from them are diffused over space.[148] If this is the case, the continuous change in social structures could bring about a change in social capital, which can cause changes in community atmosphere. If an area is plagued by social organizations whose goals are to revolt against social norms, such as gangs, it can cause a negative social capital for the area causing those who disagreed with said organizations to relocate thus taking their positive social capital to a different space than the negative.		Another area where social capital can be seen as an area of study in geography is through the analysis of participation in volunteerism and its support of different governments. One area to look into with this is through those who participate in social organizations. People that participate are of different races, ages, and economic status.[149] With these in mind, variances of the space in which these different demographics may vary, causing a difference in involvement among areas. Secondly, there are different social programs for different areas based on economic situation.[149] A governmental organization would not place a welfare center in a wealthier neighborhood where it would have very limited support to the community, as it is not needed. Thirdly, social capital can be affected by the participation of individuals of a certain area based on the type of institutions that are placed there.[149] Mohan supports this with the argument of J. Fox in his paper "Decentralization and Rural Development in Mexico", which states "structures of local governance in turn influence the capacity of grassroots communities to influence social investments."[150] With this theory, if the involvement of a government in specific areas raises the involvement of individuals in social organizations and/or communities, this will in turn raise the social capital for that area. Since every area is different, the government takes that into consideration and will provide different areas with different institutions to fit their needs thus there will be different changes in social capital in different areas.		In the context of leisure studies, social capital is seen as the consequence of investment in and cultivation of social relationships allowing an individual access to resources that would otherwise be unavailable to him or her.[151] The concept of social capital in relation to leisure is grounded in a perspective that emphasizes the interconnectedness rather than the separateness of human activity and human goals. There is a significant connection between leisure and democratic social capital.[152] Specific forms of leisure activity contribute to the development of the social capital central to democracy and democratic citizenship. The more an individual participates in social activities, the more autonomy the individual experiences, which will help her or his individual abilities and skills to develop. The greater the accumulation of social capital a person experiences, may transfer to other leisure activities as well as personal social roles, relationships and in other roles within a social structure.[152]		It has been noted that social capital may not always be used for positive ends.[153] An example of the complexities of the effects of social capital is violent or criminal gang activity that is encouraged through the strengthening of intra-group relationships (bonding social capital). The negative consequences of social capital are more often associated with bonding vis-à-vis bridging.[154]		Without "bridging" social capital, "bonding" groups can become isolated and disenfranchised from the rest of society and, most importantly, from groups with which bridging must occur in order to denote an "increase" in social capital. Bonding social capital is a necessary antecedent for the development of the more powerful form of bridging social capital.[155] Bonding and bridging social capital can work together productively if in balance, or they may work against each other. As social capital bonds and stronger homogeneous groups form, the likelihood of bridging social capital is attenuated. Bonding social capital can also perpetuate sentiments of a certain group, allowing for the bonding of certain individuals together upon a common radical ideal. The strengthening of insular ties can lead to a variety of effects such as ethnic marginalization or social isolation. In extreme cases ethnic cleansing may result if the relationship between different groups is so strongly negative. In mild cases, it just isolates certain communities such as suburbs of cities because of the bonding social capital and the fact that people in these communities spend so much time away from places that build bridging social capital.		Social capital (in the institutional Robert Putnam sense) may also lead to bad outcomes if the political institution and democracy in a specific country is not strong enough and is therefore overpowered by the social capital groups. "Civil society and the collapse of the Weimar Republic" suggests that "it was weak political institutionalization rather than a weak civil society that was Germany's main problem during the Wihelmine and Weimar eras."[156] Because the political institutions were so weak people looked to other outlets. "Germans threw themselves into their clubs, voluntary associations, and professional organizations out of frustration with the failures of the national government and political parties, thereby helping to undermine the Weimar Republic and facilitate Hitler's rise to power." In this article about the fall of the Weimar Republic, the author makes the claim that Hitler rose to power so quickly because he was able to mobilize the groups towards one common goal. Even though German society was, at the time, a "joining" society these groups were fragmented and their members did not use the skills they learned in their club associations to better their society. They were very introverted in the Weimar Republic. Hitler was able to capitalize on this by uniting these highly bonded groups under the common cause of bringing Germany to the top of world politics. The former world order had been destroyed during World War I, and Hitler believed that Germany had the right and the will to become a dominant global power. Additionally, in his essay "A Criticism of Putnam's Theory of Social Capital",[157] Michael Shindler expands upon Berman's argument that Wiemar social clubs and similar associations in countries that did not develop democracy, were organized in such a way that they fostered a "we" instead of an "I" mentality among their members, by arguing that groups which possess cultures that stress solidarity over individuality, even ones that are "horizontally" structured and which were also common to pre-soviet eastern europe, will not engender democracy if they are politically aligned with non-democratic ideologies.[158]		Later work by Putnam also suggests that social capital, and the associated growth of public trust are inhibited by immigration and rising racial diversity in communities.[159] Putnam's study regarding the issue argued that in American areas with a lack of homogeneity, some individuals neither participated in bonding nor bridging social capital. In societies where immigration is high (USA) or where ethnic heterogeneity is high (Eastern Europe), it was found that citizens lacked in both kinds of social capital and were overall far less trusting of others than members of homogenous communities were found to be. Lack of homogeneity led to people withdrawing from even their closest groups and relationships, creating an atomized society as opposed to a cohesive community. These findings challenge previous beliefs that exposure to diversity strengthens social capital, either through bridging social gaps between ethnicities or strengthening in-group bonds. It is very important for policy makers to monitor the level of perceived socio-economic threat from immigrants because negative attitudes towards immigrants make integration difficult and affect social capital.[160]		James Coleman has indicated that social capital eventually led to the creation of human capital for the future generation.[161] Human capital, a private resource, could be accessed through what the previous generation accumulated through social capital. Field suggested that such a process could lead to the very inequality social capital attempts to resolve.[162] While Coleman viewed social capital as a relatively neutral resource, he did not deny the class reproduction that could result from accessing such capital, given that individuals worked toward their own benefit. Even though Coleman never truly addresses Bourdieu in his discussion, this coincides with Bourdieu's argument set forth in Reproduction in Education, Society and Culture. Bourdieu and Coleman were fundamentally different at the theoretical level (as Bourdieu believed the actions of individuals were rarely ever conscious, but more so only a result of their habitus (see below) being enacted within a particular field, but this realization by both seems to undeniably connect their understanding of the more latent aspects of social capital.		According to Bourdieu, habitus refers to the social context within which a social actor is socialized. Thus, it is the social platform, itself, that equips one with the social reality they become accustomed to. Out of habitus comes field, the manner in which one integrates and displays his or her habitus. To this end, it is the social exchange and interaction between two or more social actors. To illustrate this, we assume that an individual wishes to better his place in society. He therefore accumulates social capital by involving himself in a social network, adhering to the norms of that group, allowing him to later access the resources (e.g. social relationships) gained over time. If, in the case of education, he uses these resources to better his educational outcomes, thereby enabling him to become socially mobile, he effectively has worked to reiterate and reproduce the stratification of society, as social capital has done little to alleviate the system as a whole. This may be one negative aspect of social capital, but seems to be an inevitable one in and of itself, as are all forms of capital.[citation needed]		[1]		
Professional development is learning to earn or maintain professional credentials such as academic degrees to formal coursework, conferences and informal learning opportunities situated in practice. It has been described as intensive and collaborative, ideally incorporating an evaluative stage.[1] There are a variety of approaches to professional development, including consultation, coaching, communities of practice, lesson study, mentoring, reflective supervision and technical assistance.[2]						The University of Management and Technology notes the use of the phrase "professional development" from 1857 onwards.[citation needed]		In the training of school staff in the United States, "[t]he need for professional development [...] came to the forefront in the 1960's".[3]		A wide variety of people, such as teachers, military officers and non-commissioned officers, health care professionals, lawyers, accountants and engineers engage in professional development. Individuals may participate in professional development because of an interest in lifelong learning, a sense of moral obligation, to maintain and improve professional competence, to enhance career progression, to keep abreast of new technology and practices, or to comply with professional regulatory requirements.[4][5] Many American states have professional development requirements for school teachers. For example, Arkansas teachers must complete 60 hours of documented professional development activities annually.[6] Professional development credits are named differently from state to state. For example, teachers: in Indiana are required to earn 90 Continuing Renewal Units (CRUs) per year;[7] in Massachusetts, teachers need 150 Professional Development Points (PDPs);[8] and in Georgia, must earn 10 Professional Learning Units (PLUs).[9] American and Canadian nurses, as well as those in the United Kingdom, have to participate in formal and informal professional development (earning Continuing education units, or CEUs) in order to maintain professional registration.[10][11][12]		In a broad sense, professional development may include formal types of vocational education, typically post-secondary or poly-technical training leading to qualification or credential required to obtain or retain employment. Professional development may also come in the form of pre-service or in-service professional development programs. These programs may be formal, or informal, group or individualized. Individuals may pursue professional development independently, or programs may be offered by human resource departments. Professional development on the job may develop or enhance process skills, sometimes referred to as leadership skills, as well as task skills. Some examples for process skills are 'effectiveness skills', 'team functioning skills', and 'systems thinking skills'.[13][14]		Professional development opportunities can range from a single workshop to a semester-long academic course, to services offered by a medley of different professional development providers and varying widely with respect to the philosophy, content, and format of the learning experiences. Some examples of approaches to professional development include:[2]		Initial professional development (IPD) is defined as "a period of development during which an individual acquires a level of competence necessary in order to operate as an autonomous professional".[15] Professional associations may recognise the successful completion of IPD by the award of chartered or similar status. Examples of professional bodies that require IPD prior to the award of professional status are the Institute of Mathematics and its Applications,[16] the Institution of Structural Engineers,[17] and the Institution of Occupational Safety and Health.[18]		Continuing professional development (CPD) or continuing professional education (CPE) is continuing education to maintain knowledge and skills. Most professions have CPD obligations. Examples are the Royal Institution of Chartered Surveyors,[19] American Academy of Financial Management,[20] safety professionals with the International Institute of Risk & Safety Management (IIRSM)[21] or the Institution of Occupational Safety and Health (IOSH),[22] and medical and legal professionals, who are subject to continuing medical education or continuing legal education requirements, which vary by jurisdiction.		
Dismissal (referred to informally as firing or sacking) is the termination of employment by an employer against the will of the employee. Though such a decision can be made by an employer for a variety of reasons, ranging from an economic downturn to performance-related problems on the part of the employee, being fired has a strong stigma in many cultures. To be dismissed, as opposed to quitting voluntarily (or being laid off), is often perceived as being the employee's fault.[1] Finding new employment may often be difficult after being fired, particularly if there is a history of being fired from previous jobs, if the reason for firing is for some serious infraction, or the employee did not hold the job very long. Job seekers will often not mention jobs that they were fired from on their resumes; accordingly, unexplained gaps in employment are often regarded as a red flag.						"Firing" is a common colloquial term in the English language (particularly used in the U.S. and Australia) for termination. The term "firing" may have been initiated[2] in the 1910s at the National Cash Register Company. Other terms for dismissal are being "sacked", "canned", "let go", "ran-off", "axed", "given walking papers", "given the pink slip" or "boned". Other terms, more often used in Commonwealth countries, include "to get the boot" and "to get the sack".[3][4]		Most US states have adopted the at-will employment contract that allows the employer to dismiss employees without having to provide a justified reason for firing, although the variety of court cases that have come out of "at-will" dismissals have made such at-will contracts ambiguous. Often, an at-will termination is handled as a "layoff". Sometimes, an employee will be dismissed if an employer can find better employees than the incumbent, even if the fired employee has not technically broken any rules. This is common with probationary employees who were recently hired, but who cannot adjust to the environment of the workplace, or those who have been around for a long time, but can be replaced with a less experienced employee who can be paid a lower salary. On the contrary, a dismissal in France is subjected to a just cause and a formal procedure.[5]		Some examples include conflict of interest, where the employee has done nothing wrong, but the presence of the employee on the employer's payroll may be harmful to the employer. For example:		More common reasons for firing include attendance problems[7], poor work performance, problematic conduct[8], insubordination (talking back to a manager or supervisor), drinking or doing illegal drugs at work (or consuming the same substances before work and showing up to work while intoxicated or "high" (an especially serious problem in jobs where the worker drives a vehicle, boat or aircraft or operates heavy machinery) or off job-site conduct.[9] Attendance problems include frequent absenteeism or tardiness, or even worse, the "no call, no show" in which an employee does not come to work and fails to notify the employer. Other attendance problems involve improper taking of breaks, such as taking extended or unauthorized breaks, failure to return from breaks in a timely manner, or walking off the premises or job site without approval from the supervisor.		Work performance problems can lead to termination even with good attendance at a job. An employee may be fired if their work performance does not meet the employer's standards. Some of these issues may be lack of necessary skills required to perform duties, incompetence, failure to learn the required skills or processes, neglect of maintenance or safety procedures, refusal to perform duties, laziness, or negligence. Conduct problems can lead to firing if they continue over a long period. Behavioral issues may include unprofessional manners (especially in customer service jobs), constant or gross insubordination, inability to properly relate (i.e., get along) with co-workers, customers or both, arguing with supervisor, co-workers or customers, use of foul language while at work, and sleeping while on duty. With these conduct problems, the firing is frequently (but not always) part of a "progressive step" process, meaning the employee will have been warned and given an opportunity to improve before more severe measures are taken.		Gross misconduct offenses can lead to immediate firing without any further warning. Gross misconduct includes damaging work equipment through negligence; discovery of false information on the job application (such as résumé fraud), fighting or brawling at work; harassment of other employees, such as sexual or racial harassment; use of employer's equipment (e.g. vehicles and computers) to engage in non-work-related activity or other violations of employer policies, illegal activity, or to view pornography; testing positive for illegal drug usage; failure to submit to a mandatory drug test (especially for transportation or heavy equipment-related jobs such as machine operators); engaging in illegal activities on the job (such as embezzlement or illegal subordinate harassment); or cheating the employer out of wages by "padding" a time sheet.		In some cases, an employee's off-the-job behavior could result in job loss. A common example is drunk driving, especially if the employee's principal responsibilities require driving. Often, an employee getting charged with a crime will affect the employer's ability to trust the employee. Whether off-the-job criminal charges will result in termination relates to several factors, including the nature of the offense, the nature of the job, and the values of the employer. In some types of jobs, minor convictions that are not related to the job activities may not lead to termination; a ditch digger who works with a shovel who is convicted of drunk driving may not lose their job, while a food delivery driver who is convicted of drunk driving will lose their job. However, in some jobs, the perception of trust is very important, so even a non-job-related conviction, however minor, will result in termination, as in the case of banks, security firms, and schools. Another factor is the values of the employer; while some employers may believe that an employee should have a "second chance", other employers may have no tolerance for convicted individuals in their company, even if the employee has very little responsibility, as in the case of a manual labourer.		Some fired employees may face additional consequences besides their dismissal. This may occur when the reason for the termination is a violation of criminal law, or if serious damages are caused to the employer as a result of the employee's actions. Such ex-employees may face criminal prosecution, a civil lawsuit, or a reporting to a database of those who have engaged in serious misconduct in such a position, so that the chances of ever obtaining a similar position with another employer are less likely(blacklisting). Some examples are a caregiver who engages in abuse, a bank teller who has stolen money from the cash drawer, or a member of law enforcement who has committed police brutality.		For the most serious violations, especially when the employer's security may be at risk from the employee in question, a guard or officer may escort a fired employee from the workplace to the parking lot upon their dismissal. Such actions are often taken by government offices or large corporations that contain sensitive materials, and where the risk exists that the terminated employee may remove some of these materials or otherwise steal trade secrets in order to retaliate against the employer or use it to the advantage of a competing enterprise.		Though many employers would like to get rid of their "problem employees", some employers are reluctant to fire those who one would expect would be deserving of termination. There are many reasons for this, which may include:[10] Some positions may be hard to fill. This may be the case with a rare skilled position, or with certain low-wage jobs that are generally unattractive, where finding applicants is difficult. A person who has unusual skills, or who is doing a job that is considered undesirable, such as cleaning sewage from pipes may be hard to replace. As such, a person in this position may be retained even if they have absenteeism or conduct problems.		Another reason that bosses do not fire some problem employees is from a fear of retaliation. Sometimes the employer must be concerned about various types of action the ex-employee may take against the company, such as filing a wrongful termination suit: The terminated employee may take legal action in response to the firing. While laws vary in each country and jurisdiction, many employers keep extensive documentation of disciplinary action, evaluations, attendance records, and correspondence from supervisors, co-workers and customers in order to defend themselves in the event of such a suit. Additionally, the at-will employment contract, where the law permits, allows the employer to dismiss employees without having to provide a reason, though this has sometimes been challenged in court. As well, the employer may be concerned that the employee could make a negative report to public: by divulging things about the company to others, thereby hurting their business; or that the employee may disclose trade secrets: An ex-employee may remove materials or divulge confidential information from the former employer and use it with another employer or in an independent business.		Some employers may be afraid that a worker may make a report to law enforcement, in the event that the employer's practices are illegal to the law. Other fears include the risk of sabotage by damaging machinery, erasing computer files, or shredding documents; or even violence; In some most extreme cases, fired employees have attacked or even killed their former employers or the staff at their old company or organization, sometimes known in slang as "going postal." In some cases, this has occurred several months or years after termination. Some employers terminate their staff off-site to avoid these issues.		Other reasons include:		Employers have several methods of countering some of these potential threats. A common method is forced resignation, and it allows the employee to resign as if by choice, thereby freeing the employer of the burden of a firing. Other methods used by employers include:		A forced resignation is when an employee is required to depart from their position due to some issue caused by their continued employment. A forced resignation may be due to the employer's wishes to dismiss the employee, but the employer may be offering a softened firing. Or, in a high profile position, the employee may want to leave before the press learns more negative information about one's controversial nature. To avoid this, and to allow the dismissed employee to "save face" in a more "graceful" exit, the employer will often ask the employee to resign "voluntarily" from their position. If the employee chooses not to resign, the processes necessary to fire them may be pursued, and the employee will usually be fired. The resignation thus makes it unclear whether the resignation was forced or voluntary, and this opaqueness may benefit both parties.		In some cases, the firing of an employee is a discriminatory act. Although an employer may often claim the dismissal was for "just cause", these discriminatory acts are often because of the employee's physical or mental disability, or perhaps their age, race, religion, gender, HIV status or sexual orientation. Other unjust firings may result from a workplace manager or supervisor wanting to retaliate against an employee. Often, this is because the worker reported wrongdoing (often, but not always sexual harassment or other misconduct) on the part of the supervisor. Such terminations are often illegal. Many successful lawsuits have resulted from discriminatory or retaliatory termination.		Discriminatory or retaliatory termination by a supervisor can take the form of administrative process. In this form the rules of the institution are used as the basis for termination. For example, if a place of employment has a rule that prohibits personal phone calls, receiving or making personal calls can be the grounds for termination even though it may be a common practice within the organization.		Employers who wish for an employee to exit of their own accord, but do not wish to pursue firing or forced resignation, may degrade the employee's working conditions, hoping that he or she will leave "voluntarily". The employee may be moved to a different geographical location, assigned to an undesirable shift, given too few hours if part-time, demoted (or relegated to a menial task), or assigned to work in uncomfortable conditions. Other forms of manipulation may be used, such as being unfairly hostile to the employee, and punishing them for things that are deliberately overlooked with other employees. Such tactics may amount to constructive dismissal, which is illegal in some jurisdictions.		Depending on the circumstances, one whose employment has been terminated may or may not be eligible for being rehired by the same employer. If the decision to terminate was the employee's, the willingness of the employer to rehire is often contingent upon the relationship the employee had with the employer, the amount of notice given by the employee prior to departure, and the needs of the employer. In some cases, when an employee departed on good terms, they may be given special priority by the employer when seeking rehire.		An employee may be terminated without prejudice, meaning the fired employee may be rehired readily for the same or a similar job in the future. This is usually true in the case of layoff. Conversely, a person can be terminated with prejudice, meaning an employer will not rehire the former employee to a similar job in the future. This can be for many reasons: incompetence, misconduct (such as dishonesty or "zero tolerance" violations), insubordination or "attitude" (personality clashes with peers or bosses). Termination forms ("pink slips"in the U.S.) routinely include a set of check boxes where a supervisor can indicate "with prejudice" or "without prejudice". During the Vietnam War, the CIA used this terminology with regard to its locally hired operatives. In the case of severe misconduct, it is alleged that the CIA would assassinate them or "terminate with extreme prejudice".[11]		High-profile firing refers to the termination of an employee who is in the public spotlight. In such cases, though terminology may be similar, the rules may differ from the typical firing. For example, in professional sports, a coach's dismissal is generally referred to as a firing. However, though the employment will cease, the team may be required to pay the coach the remainder of his contract. A player on a team who faces the same action is generally not considered to be "fired", but rather "released" or "waived".		In many countries and smaller jurisdictions, the chief executive reserves the right to dismiss certain officials who have been appointed to their positions, with the termination effective immediately and with no obligation for any further pay. In some cases, however, a severance package may be paid. There are times when the President of the United States will ask his entire Cabinet to submit their resignations.[citation needed] He can accept some of them and file the rest away. This is often done by a new President[citation needed] who has inherited his predecessor's Cabinet, as a way to reorganize with reduced hard feelings.		
Wage slavery is a pejorative term used to draw an analogy between slavery and wage labor by focusing on similarities between owning and renting a person. It is usually used to refer to a situation where a person's livelihood depends on wages or a salary, especially when the dependence is total and immediate.[1][2]		The term wage slavery has been used to criticize exploitation of labour and social stratification, with the former seen primarily as unequal bargaining power between labor and capital (particularly when workers are paid comparatively low wages, e.g. in sweatshops),[3] and the latter as a lack of workers' self-management, fulfilling job choices, and leisure in an economy.[4][5][6] The criticism of social stratification covers a wider range of employment choices bound by the pressures of a hierarchical society to perform otherwise unfulfilling work that deprives humans of their "species character"[7] not only under threat of starvation or poverty, but also of social stigma and status diminution.[8][9][10]		Similarities between wage labor and slavery were noted as early as Cicero in Ancient Rome.[11] With the advent of the industrial revolution, thinkers such as Proudhon and Marx elaborated the comparison between wage labor and slavery,[12][13] while Luddites emphasized the dehumanization brought about by machines. Before the American Civil War, Southern defenders of African American slavery invoked the concept of wage slavery to favorably compare the condition of their slaves to workers in the North.[14][15] The United States abolished slavery after the Civil War, but labor union activists found the metaphor useful. According to Lawrence Glickman, in the Gilded Age, "References abounded in the labor press, and it is hard to find a speech by a labor leader without the phrase."[16]		The introduction of wage labor in 18th century Britain was met with resistance, giving rise to the principles of syndicalism.[17][18][19][20] Historically, some labor organizations and individual social activists have espoused workers' self-management or worker cooperatives as possible alternatives to wage labor.[5][19]						The view that working for wages is akin to slavery dates back to the ancient world.[22] In ancient Rome, Cicero wrote that "whoever gives his labor for money sells himself and puts himself in the rank of slaves."[23]		In 1763, the French journalist Simon Linguet published an influential description of wage slavery:[13]		The slave was precious to his master because of the money he had cost him ... They were worth at least as much as they could be sold for in the market ... It is the impossibility of living by any other means that compels our farm labourers to till the soil whose fruits they will not eat and our masons to construct buildings in which they will not live ... It is want that compels them to go down on their knees to the rich man in order to get from him permission to enrich him ... what effective gain [has] the suppression of slavery brought [him ?] He is free, you say. Ah! That is his misfortune ... These men ... [have] the most terrible, the most imperious of masters, that is, need. ... They must therefore find someone to hire them, or die of hunger. Is that to be free?		The view that wage work has substantial similarities with chattel slavery was actively put forward in the late 18th and 19th centuries by defenders of chattel slavery (most notably in the Southern states of the US), and by opponents of capitalism (who were also critics of chattel slavery).[9][24] Some defenders of slavery, mainly from the Southern slave states argued that Northern workers were "free but in name – the slaves of endless toil," and that their slaves were better off.[25][26] This contention has been partly corroborated by some modern studies that indicate slaves' material conditions in the 19th century were "better than what was typically available to free urban laborers at the time."[27][28] In this period, Henry David Thoreau wrote that "[i]t is hard to have a Southern overseer; it is worse to have a Northern one; but worst of all when you are the slave-driver of yourself."[29]		Some abolitionists in the United States regarded the analogy as spurious.[30] They believed that wage workers were "neither wronged nor oppressed".[31] Abraham Lincoln and the Republicans argued that the condition of wage workers was different from slavery, as laborers were likely to have the opportunity to work for themselves in the future, achieving self-employment.[32] The abolitionist and former slave Frederick Douglass initially declared, "now I am my own master", upon taking a paying job.[33] But later in life, he concluded to the contrary, "experience demonstrates that there may be a slavery of wages only a little less galling and crushing in its effects than chattel slavery, and that this slavery of wages must go down with the other".[34][35] Douglass went on to speak about these conditions as arising from the unequal bargaining power between the ownership/capitalist class and the non-ownership/laborer class within a compulsory monetary market. "No more crafty and effective devise for defrauding the southern laborers could be adopted than the one that substitutes orders upon shopkeepers for currency in payment of wages. It has the merit of a show of honesty, while it puts the laborer completely at the mercy of the land-owner and the shopkeeper.".[36]		Self-employment became less common as the artisan tradition slowly disappeared in the later part of the 19th century.[5] In 1869 The New York Times described the system of wage labor as "a system of slavery as absolute if not as degrading as that which lately prevailed at the South".[32] E. P. Thompson notes that for British workers at the end of the 18th and beginning of the 19th centuries, the "gap in status between a 'servant,' a hired wage-laborer subject to the orders and discipline of the master, and an artisan, who might 'come and go' as he pleased, was wide enough for men to shed blood rather than allow themselves to be pushed from one side to the other. And, in the value system of the community, those who resisted degradation were in the right."[17] A "Member of the Builders' Union" in the 1830s argued that the trade unions "will not only strike for less work, and more wages, but will ultimately abolish wages, become their own masters and work for each other; labor and capital will no longer be separate but will be indissolubly joined together in the hands of workmen and work-women."[18] This perspective inspired the Grand National Consolidated Trades Union of 1834 which had the "two-fold purpose of syndicalist unions – the protection of the workers under the existing system and the formation of the nuclei of the future society" when the unions "take over the whole industry of the country."[19] "Research has shown", summarises William Lazonick, "that the 'free-born Englishman' of the eighteenth century – even those who, by force of circumstance, had to submit to agricultural wage labour – tenaciously resisted entry into the capitalist workshop."[20]		The use of the term wage slave by labor organizations may originate from the labor protests of the Lowell Mill Girls in 1836.[37] The imagery of wage slavery was widely used by labor organizations during the mid-19th century to object to the lack of workers' self-management. However, it was gradually replaced by the more neutral term "wage work" towards the end of the 19th century, as labor organizations shifted their focus to raising wages.[5]		Karl Marx described capitalist society as infringing on individual autonomy because it is based on a materialistic and commodified concept of the body and its liberty (i.e. as something that is sold, rented, or alienated in a class society). According to Friedrich Engels:[38][39]		The slave is sold once and for all; the proletarian must sell himself daily and hourly. The individual slave, property of one master, is assured an existence, however miserable it may be, because of the master's interest. The individual proletarian, property as it were of the entire bourgeois class which buys his labor only when someone has need of it, has no secure existence.		Critics of wage work have drawn several similarities between wage work and slavery:		According to American anarcho-syndicalist philosopher Noam Chomsky, the similarities between chattel and wage slavery were noticed by the workers themselves. He noted that the 19th century Lowell Mill Girls, who, without any reported knowledge of European Marxism or anarchism, condemned the "degradation and subordination" of the newly emerging industrial system, and the "new spirit of the age: gain wealth, forgetting all but self", maintaining that "those who work in the mills should own them."[45][46] They expressed their concerns in a protest song during their 1836 strike:		Oh! isn't it a pity, such a pretty girl as I Should be sent to the factory to pine away and die? Oh! I cannot be a slave, I will not be a slave, For I'm so fond of liberty, That I cannot be a slave.[47]		Defenses of wage labor and chattel slavery in the literature have linked the subjection of man to man with the subjection of man to nature; arguing that hierarchy and a social system's particular relations of production represent human nature and are no more coercive than the reality of life itself. According to this narrative, any well-intentioned attempt to fundamentally change the status quo is naively utopian and will result in more oppressive conditions.[48] Bosses in both of these long-lasting systems argued that their system created a lot of wealth and prosperity. In some sense, both did create jobs and their investment entailed risk. For example, slave owners risked losing money by buying chattel slaves who later became ill or died; while bosses risked losing money by hiring workers (wage slaves) to make products that didn't sell well on the market. Marginally, both chattel and wage slaves may become bosses; sometimes by working hard. It may be the "rags to riches" story which occasionally occurs in capitalism, or the "slave to master" story that occurred in places like colonial Brazil, where slaves could buy their own freedom and become business owners, self-employed, or slave owners themselves.[49] Social mobility, or the hard work and risk that it may entail, are thus not considered to be a redeeming factor by critics of the concept of wage slavery.[50]		Anthropologist David Graeber has noted that, historically, the first wage labor contracts we know about – whether in ancient Greece or Rome, or in the Malay or Swahili city states in the Indian Ocean – were in fact contracts for the rental of chattel slaves (usually the owner would receive a share of the money, and the slave, another, with which to maintain his or her living expenses.) Such arrangements, according to Graeber, were quite common in New World slavery as well, whether in the United States or Brazil. C. L. R. James argued that most of the techniques of human organization employed on factory workers during the industrial revolution were first developed on slave plantations.[51]		The usage of the term "wage slavery" shifted to "wage work" at the end of the 19th century as groups like the Knights of Labor and American Federation of Labor shifted to a more reformist, trade union ideology instead of worker's self-management. Much of the decline was caused by the rapid increase in manufacturing after the industrial revolution and the subsequent dominance of wage labor as a result. Another factor was immigration and demographic changes that led to ethnic tension between the workers.[5]		As Hallgrimsdottir and Benoit point out:		increased centralization of production ... declining wages ... [an] expanding ... labor pool ... intensifying competition, and ... [t]he loss of competence and independence experienced by skilled labor" meant that "a critique that referred to all [wage] work as slavery and avoided demands for wage concessions in favor of supporting the creation of the producerist republic (by diverting strike funds towards funding ... co-operatives, for example) was far less compelling than one that identified the specific conditions of slavery as low wages ...[5]		Some anti-capitalist thinkers claim that the elite maintain wage slavery and a divided working class through their influence over the media and entertainment industry,[52][53] educational institutions, unjust laws, nationalist and corporate propaganda, pressures and incentives to internalize values serviceable to the power structure, state violence, fear of unemployment[54] and a historical legacy of exploitation and profit accumulation/transfer under prior systems, which shaped the development of economic theory:		Adam Smith noted that employers often conspire together to keep wages low, and have the upper hand in conflicts between workers and employers:[55]		The interest of the dealers ... in any particular branch of trade or manufactures, is always in some respects different from, and even opposite to, that of the public... [They] have generally an interest to deceive and even to oppress the public ... We rarely hear, it has been said, of the combinations of masters, though frequently of those of workmen. But whoever imagines, upon this account, that masters rarely combine, is as ignorant of the world as of the subject. Masters are always and everywhere in a sort of tacit, but constant and uniform combination, not to raise the wages of labor above their actual rate ... It is not, however, difficult to foresee which of the two parties must, upon all ordinary occasions, have the advantage in the dispute, and force the other into a compliance with their terms.		The concept of wage slavery could conceivably be traced back to pre-capitalist figures like Gerrard Winstanley from the radical Christian Diggers movement in England, who wrote in his 1649 pamphlet, The New Law of Righteousness, that there "shall be no buying or selling, no fairs nor markets, but the whole earth shall be a common treasury for every man," and "there shall be none Lord over others, but every one shall be a Lord of himself."[56]		Aristotle stated that "the citizens must not live a mechanic or a mercantile life (for such a life is ignoble and inimical to virtue), nor yet must those who are to be citizens in the best state be tillers of the soil (for leisure is needed both for the development of virtue and for active participation in politics)",[57] often paraphrased as "all paid jobs absorb and degrade the mind."[58] Cicero wrote in 44 BC that "vulgar are the means of livelihood of all hired workmen whom we pay for mere manual labour, not for artistic skill; for in their case the very wage they receive is a pledge of their slavery."[59] Somewhat similar criticisms have also been expressed by some proponents of liberalism, like Silvio Gesell, and Thomas Paine,[60] Henry George, who inspired the economic philosophy known as Georgism,[9] and the Distributist school of thought within the Catholic Church.		To Marx and anarchist thinkers like Bakunin and Kropotkin, wage slavery was a class condition in place due to the existence of private property and the state. This class situation rested primarily on:		and secondarily on:		Fascism was more hostile against independent trade unions than modern economies in Europe or the United States.[62] Fascist economic policies were widely accepted in the 1920s and 1930s and foreign (especially US) corporate investment in Italy and Germany increased after the fascist take over.[63][64]		Fascism has been perceived by some notable critics, like Buenaventura Durruti, to be a last resort weapon of the privileged to ensure the maintenance of wage slavery:		No government fights fascism to destroy it. When the bourgeoisie sees that power is slipping out of its hands, it brings up fascism to hold onto their privileges.[65]		According to Noam Chomsky, analysis of the psychological implications of wage slavery goes back to the Enlightenment era. In his 1791 book On the Limits of State Action, classical liberal thinker Wilhelm von Humboldt explained how "whatever does not spring from a man's free choice, or is only the result of instruction and guidance, does not enter into his very nature; he does not perform it with truly human energies, but merely with mechanical exactness" and so when the laborer works under external control, "we may admire what he does, but we despise what he is."[66] Both the Milgram and Stanford experiments have been found useful in the psychological study of wage-based workplace relations.[67]		According to research, modern work provides people with a sense of personal and social identity that is tied to		Thus job loss entails the loss of this identity.[68]		Erich Fromm argued that if a person perceives himself as being what he owns, then when that person loses (or even thinks of losing) what he "owns" (e.g. the good looks or sharp mind that allow him to sell his labor for high wages), then, a fear of loss may create anxiety and authoritarian tendencies because that person's sense of identity is threatened. In contrast, when a person's sense of self is based on what he experiences in a state of being (creativity, love, sadness, taste, sight etc.) with a less materialistic regard for what he once had and lost, or may lose, then less authoritarian tendencies prevail. The state of being, in his view, flourishes under a worker-managed workplace and economy, whereas self-ownership entails a materialistic notion of self, created to rationalize the lack of worker control that would allow for a state of being.[69]		Investigative journalist Robert Kuttner analyzed the work of public-health scholars Jeffrey Johnson and Ellen Hall about modern conditions of work, and concludes that "to be in a life situation where one experiences relentless demands by others, over which one has relatively little control, is to be at risk of poor health, physically as well as mentally." Under wage labor, "a relatively small elite demands and gets empowerment, self-actualization, autonomy, and other work satisfaction that partially compensate for long hours" while "epidemiological data confirm that lower-paid, lower-status workers are more likely to experience the most clinically damaging forms of stress, in part because they have less control over their work."[70]		Wage slavery, and the educational system that precedes it "implies power held by the leader. Without power the leader is inept. The possession of power inevitably leads to corruption ... in spite of ... good intentions ... [Leadership means] power of initiative, this sense of responsibility, the self-respect which comes from expressed manhood, is taken from the men, and consolidated in the leader. The sum of their initiative, their responsibility, their self-respect becomes his ... [and the] order and system he maintains is based upon the suppression of the men, from being independent thinkers into being 'the men' ... In a word, he is compelled to become an autocrat and a foe to democracy." For the "leader", such marginalisation can be beneficial, for a leader "sees no need for any high level of intelligence in the rank and file, except to applaud his actions. Indeed such intelligence from his point of view, by breeding criticism and opposition, is an obstacle and causes confusion."[71] Wage slavery "implies erosion of the human personality ... [because] some men submit to the will of others, arousing in these instincts which predispose them to cruelty and indifference in the face of the suffering of their fellows."[72]		In 19th-century discussions of labor relations, it was normally assumed that the threat of starvation forced those without property to work for wages. Proponents of the view that modern forms of employment constitute wage slavery, even when workers appear to have a range of available alternatives, have attributed its perpetuation to a variety of social factors that maintain the hegemony of the employer class.[44][73]		Harriet Hanson Robinson in an account of the Lowell Mill Girls wrote that generously high wages were offered to overcome the degrading nature of the work:		At the time the Lowell cotton mills were started the caste of the factory girl was the lowest among the employments of women. ... She was represented as subjected to influences that must destroy her purity and selfrespect. In the eyes of her overseer she was but a brute, a slave, to be beaten, pinched and pushed about. It was to overcome this prejudice that such high wages had been offered to women that they might be induced to become millgirls, in spite of the opprobrium that still clung to this degrading occupation.[74]		In his book Disciplined Minds, Jeff Schmidt points out that professionals are trusted to run organizations in the interests of their employers. Because employers cannot be on hand to manage every decision, professionals are trained to "ensure that each and every detail of their work favors the right interests–or skewers the disfavored ones" in the absence of overt control:		The resulting professional is an obedient thinker, an intellectual property whom employers can trust to experiment, theorize, innovate and create safely within the confines of an assigned ideology.[75]		Parecon (participatory economics) theory posits a social class "between labor and capital" of higher paid professionals such as "doctors, lawyers, engineers, managers and others" who monopolize empowering labor and constitute a class above wage laborers who do mostly "obedient, rote work".[76]		The terms "employee" or "worker" have often been replaced by "associate". This plays up the allegedly voluntary nature of the interaction, while playing down the subordinate status of the wage laborer, as well as the worker-boss class distinction emphasized by labor movements. Billboards, as well as TV, Internet and newspaper advertisements, consistently show low-wage workers with smiles on their faces, appearing happy.[77]		Job interviews and other data on requirements for lower skilled workers in developed countries – particularly in the growing service sector – indicate that the more workers depend on low wages, and the less skilled or desirable their job is, the more employers screen for workers without better employment options and expect them to feign unremunerative motivation.[78] Such screening and feigning may not only contribute to the positive self-image of the employer as someone granting desirable employment, but also signal wage-dependence by indicating the employee's willingness to feign, which in turn may discourage the dissatisfaction normally associated with job-switching or union activity.[78]		At the same time, employers in the service industry have justified unstable, part-time employment and low wages by playing down the importance of service jobs for the lives of the wage laborers (e.g. just temporary before finding something better, student summer jobs etc.).[79][80]		In the early 20th century, "scientific methods of strikebreaking"[81] were devised – employing a variety of tactics that emphasized how strikes undermined "harmony" and "Americanism".[82]		Some social activists objecting to the market system or price system of wage working, historically have considered syndicalism, worker cooperatives, workers' self-management and workers' control as possible alternatives to the current wage system.[4][5][6][19]		The American philosopher John Dewey believed that until "industrial feudalism" is replaced by "industrial democracy," politics will be "the shadow cast on society by big business".[83] Thomas Ferguson has postulated in his investment theory of party competition that the undemocratic nature of economic institutions under capitalism causes elections to become occasions when blocs of investors coalesce and compete to control the state.[84]		Noam Chomsky has argued that political theory tends to blur the 'elite' function of government:		Modern political theory stresses Madison's belief that "in a just and a free government the rights both of property and of persons ought to be effectually guarded." But in this case too it is useful to look at the doctrine more carefully. There are no rights of property, only rights to property that is, rights of persons with property,...		[In] representative democracy, as in, say, the United States or Great Britain […] there is a monopoly of power centralized in the state, and secondly – and critically – […] the representative democracy is limited to the political sphere and in no serious way encroaches on the economic sphere […] That is, as long as individuals are compelled to rent themselves on the market to those who are willing to hire them, as long as their role in production is simply that of ancillary tools, then there are striking elements of coercion and oppression that make talk of democracy very limited, if even meaningful.[85]		In this regard Chomsky has used Bakunin's theories about an "instinct for freedom",[86] the militant history of labor movements, Kropotkin's mutual aid evolutionary principle of survival and Marc Hauser's theories supporting an innate and universal moral faculty,[87] to explain the incompatibility of oppression with certain aspects of human nature.[88][89]		Loyola University philosophy professor John Clark and libertarian socialist philosopher Murray Bookchin have criticized the system of wage labor for encouraging environmental destruction, arguing that a self-managed industrial society would better manage the environment. They, like other anarchists,[90] attribute much of the industrial revolution's pollution to the "hierarchical" and "competitive" economic relations accompanying it.[91]		Some criticize wage slavery on strictly contractual grounds, e.g. David Ellerman and Carole Pateman, arguing that the employment contract is a legal fiction in that it treats human beings juridically as mere tools or inputs by abdicating responsibility and self-determination, which the critics argue are inalienable. As Ellerman points out, "[t]he employee is legally transformed from being a co-responsible partner to being only an input supplier sharing no legal responsibility for either the input liabilities [costs] or the produced outputs [revenue, profits] of the employer's business."[92] Such contracts are inherently invalid "since the person remain[s] a de facto fully capacitated adult person with only the contractual role of a non-person" as it is impossible to physically transfer self-determination.[93] As Pateman argues:		The contractarian argument is unassailable all the time it is accepted that abilities can 'acquire' an external relation to an individual, and can be treated as if they were property. To treat abilities in this manner is also implicitly to accept that the 'exchange' between employer and worker is like any other exchange of material property . . . The answer to the question of how property in the person can be contracted out is that no such procedure is possible. Labour power, capacities or services, cannot be separated from the person of the worker like pieces of property.[94]		In a modern liberal-capitalist society, the employment contract is enforced while the enslavement contract is not; the former being considered valid because of its consensual/non-coercive nature, and the latter being considered inherently invalid, consensual or not. The noted economist Paul Samuelson described this discrepancy.		Since slavery was abolished, human earning power is forbidden by law to be capitalized. A man is not even free to sell himself; he must rent himself at a wage.[95]		Some advocates of right-libertarianism, among them philosopher Robert Nozick, address this inconsistency in modern societies, arguing that a consistently libertarian society would allow and regard as valid consensual/non-coercive enslavement contracts, rejecting the notion of inalienable rights.		The comparable question about an individual is whether a free system will allow him to sell himself into slavery. I believe that it would.[96]		Others like Murray Rothbard allow for the possibility of debt slavery, asserting that a lifetime labour contract can be broken so long as the slave pays appropriate damages:		[I]f A has agreed to work for life for B in exchange for 10,000 grams of gold, he will have to return the proportionate amount of property if he terminates the arrangement and ceases to work.[97]		In the philosophy of mainstream, neoclassical economics, wage labor is seen as the voluntary sale of one's own time and efforts, just like a carpenter would sell a chair, or a farmer would sell wheat. It is considered neither an antagonistic nor abusive relationship, and carries no particular moral implications.[98]		Austrian economics argues that a person is not "free" unless they can sell their labor, because otherwise that person has no self-ownership and will be owned by a "third party" of individuals.[99]		Post-Keynesian economics perceives wage slavery as resulting from inequality of bargaining power between labor and capital, which exists when the economy does not "allow labor to organize and form a strong countervailing force".[100]		The two main forms of socialist economics perceive wage slavery differently:		
A letter of resignation is written to announce the author's intent to leave a position currently held, such as an office, employment or commission.		Such a letter will often take legal effect to terminate an appointment or employment, as notice under the relevant terms of the position; many appointments and contractual employments are terminable by unilateral notice, or advance notice of a specified period of time, with or without further conditions. Even where an oral notice would be effective, the effective date or time of termination may be directly or indirectly fixed on delivery of a written letter or email, for the sake of clarity and record. In response, different arrangements may be made or agreed, such as an earlier effective date, or improved terms and conditions of appointment upon withdrawal of the letter.		It should normally be delivered in advance to the appropriate supervisor or superior, and contain such information as the intended last day at work. A period of notice may be required expressly by contract, impliedly by the pay interval, or otherwise. Nevertheless, in practice, some resignations can be effective immediately.		For courtesy's sake, a letter of resignation may thank the employer for the pleasure of working under them and the opportunities and experience gained thereby, and also offer to assist with the transition by, for example, training the replacement. A more hostile letter may assert other sentiments or claims, particularly that the contract or terms of employment have been broken. In any case, the terms of the letter and its consequences may often be negotiated, either before or after delivery.		A formal letter with minimal expression of courtesy is then-President of the United States Richard Nixon's letter of resignation, under the terms of the United States Constitution. Delivered to then-Secretary of State Henry Kissinger on 9 August 1974, it read simply, "I hereby resign the Office of President of the United States." It was simply dated, but the recipient also recorded upon it the time of receipt, at which it took effect with important consequences under the Constitution.		It is advisable to write a resignation letter in order to leave a good impression on one's employer or boss. The reason must be mentioned in a concise manner so that it catches the hiring managers attention. Such a resignation letter paves way for a smooth exit interview later.		
A leave of absence (LOA) is a period of time that one must be away from one's primary job, while maintaining the status of employee. This contrasts with normal periods away from the workplace, such as vacations, holidays, hiatuses, sabbaticals, and "working from home" programs, in that they are considered exceptional circumstances, rather than benefits. Generally such an arrangement has a predefined termination at a particular date or after a certain event has occurred.						Generally, paid leaves of absence are given at the request of the employer, or per some statutory or contractual requirement. Some examples of generally paid LOA include employee injury on the job, bereavement, jury duty, or if the employer is performing repairs or other activities in the building where the employee normally works which prevents them from performing their duties.		Unpaid LOAs are generally at the request of the employee or as a result of suspected misconduct on the part of the employee. A leave of absence may be obtained for a variety of employee-requested reasons, including active duty call-ups for reserve military personnel, or to attend to the health needs of the employee or of a family member of the employee.		In many jurisdictions, it is up to the employer's discretion as to whether or not an employee's request for a leave of absence is approved. In the United States, the Family and Medical Leave Act of 1993 defines certain circumstances under which approval of a leave of absence is compulsory. Additionally, the Uniformed Services Employment and Reemployment Rights Act (USERRA) dictates certain circumstances under which a LOA must be granted.		During periods of time where the employer's market is sluggish, some employers offer certain classes of employees an opportunity to take an unpaid leave of absence as extra vacation time, in an effort to temporarily reduce operating expenses without the complications of performing a layoff, and potentially losing critical employees permanently. Such a period is referred to as a leave of absence in lieu of layoff.[1]		In academia, "unpaid leave of absence" has no negative connotation. It is used to give the opportunity to a professor or staff member to assume a non-conflicting position (e.g., political position) without losing his/her original affiliation.		Generally, continuation of certain benefits, such as medical insurance, is maintained. Other benefits such as Life Insurance normally require the employee to pay the premium in order to be continued during the LOA.[2]		For those benefits that are based on an employee's time in his/ her job, the period of the absence may be included in the tallies of consecutive service for certain benefits. If the time is not included, it is simply omitted from the tally, but not considered a break in service.		In India, a Government service holder under the Union Government or any Provincial (State) Government can avail the following types of leave of absence during the service period:[3]		
The Great Recession was a period of general economic decline observed in world markets during the late 2000s and early 2010s. The scale and timing of the recession varied from country to country.[1][2] In terms of overall impact, the International Monetary Fund concluded that it was the worst global recession since the 1930s.[3][4] The causes of the recession largely originated in the United States, particularly related to the real-estate market, though choices made by other nations contributed as well. According to the U.S. National Bureau of Economic Research (the official arbiter of U.S. recessions) the recession, as experienced in that country, began in December 2007 and ended in June 2009, thus extending over 19 months.[5] The Great Recession was related to the financial crisis of 2007–08 and U.S. subprime mortgage crisis of 2007–09. The Great Recession resulted in the scarcity of valuable assets in the market economy and the collapse of the financial sector in the world economy.[6][7]		The recession was not felt evenly around the world. Whereas most of the world's developed economies, particularly in North America and Europe (including Russia), fell into a definitive recession, many of the newer developed economies suffered far less impact, particularly China and India whose economies grew substantially during this period.						Two senses of the word "recession" exist: a less precise sense, referring broadly to "a period of reduced economic activity";[8] and the academic sense used most often in economics, which is defined operationally, referring specifically to the contraction phase of a business cycle, with two or more consecutive quarters of GDP contraction. Under the academic definition, the recession ended in the United States in June or July 2009.[9][10] In the broader, lay sense of the word however, many people use the term to refer to ongoing hardship (in the same way that the term "Great Depression" is also popularly used).[11][12][13][14][15][16]		The Great Recession met the IMF criteria for being a global recession, requiring a decline in annual real world GDP per‑capita (Purchasing Power weighted), only in the single calendar year 2009.[3][4] Despite the fact that quarterly data are being used as recession definition criteria by all G20 members, representing 85% of the world GDP,[17] the International Monetary Fund (IMF) has decided—in the absence of a complete data set—not to declare/measure global recessions according to quarterly GDP data. The seasonally adjusted PPP‑weighted real GDP for the G20‑zone, however, is a good indicator for the world GDP, and it was measured to have suffered a direct quarter on quarter decline during the three quarters from Q3‑2008 until Q1‑2009, which more accurately mark when the recession took place at the global level.[18]		According to the U.S. National Bureau of Economic Research (the official arbiter of U.S. recessions) the recession began in December 2007 and ended in June 2009, and thus extended over eighteen months.[5][19]		The years leading up to the crisis were characterized by an exorbitant rise in asset prices and associated boom in economic demand.[20] Further, the U.S. shadow banking system (i.e., non-depository financial institutions such as investment banks) had grown to rival the depository system yet was not subject to the same regulatory oversight, making it vulnerable to a bank run.[21]		US mortgage-backed securities, which had risks that were hard to assess, were marketed around the world, as they offered higher yields than U.S. government bonds. Many of these securities were backed by subprime mortgages, which collapsed in value when the U.S. housing bubble burst during 2006 and homeowners began to default on their mortgage payments in large numbers starting in 2007.[22]		The emergence of sub-prime loan losses in 2007 began the crisis and exposed other risky loans and over-inflated asset prices. With loan losses mounting and the fall of Lehman Brothers on 15 September 2008, a major panic broke out on the inter-bank loan market. There was the equivalent of a bank run on the shadow banking system, resulting in many large and well established investment and commercial banks in the United States and Europe suffering huge losses and even facing bankruptcy, resulting in massive public financial assistance (government bailouts).[23]		The global recession that followed resulted in a sharp drop in international trade, rising unemployment and slumping commodity prices.[24] Several economists predicted that recovery might not appear until 2011 and that the recession would be the worst since the Great Depression of the 1930s.[25][26] Economist Paul Krugman once commented on this as seemingly the beginning of "a second Great Depression."[27]		Governments and central banks responded with fiscal and monetary policies to stimulate national economies and reduce financial system risks. The recession has renewed interest in Keynesian economic ideas on how to combat recessionary conditions. Economists advise that the stimulus should be withdrawn as soon as the economies recover enough to "chart a path to sustainable growth".[28][29][30]		The distribution of household incomes in the United States has become more unequal during the post-2008 economic recovery, a first for the US but in line with the trend over the last ten economic recoveries since 1949.[31] Income inequality in the United States has grown from 2005 to 2012 in more than 2 out of 3 metropolitan areas.[32] Median household wealth fell 35% in the US, from $106,591 to $68,839 between 2005 and 2011.[33]		The majority report provided by U.S. Financial Crisis Inquiry Commission, composed of six Democratic and four Republican appointees, reported its findings in January 2011. It concluded that "the crisis was avoidable and was caused by:		There were two Republican dissenting FCIC reports. One of them, signed by three Republican appointees, concluded that there were multiple causes. In his separate dissent to the majority and minority opinions of the FCIC, Commissioner Peter J. Wallison of the American Enterprise Institute (AEI) primarily blamed U.S. housing policy, including the actions of Fannie & Freddie, for the crisis. He wrote: "When the bubble began to deflate in mid-2007, the low quality and high risk loans engendered by government policies failed in unprecedented numbers.[36]		In its "Declaration of the Summit on Financial Markets and the World Economy," dated 15 November 2008, leaders of the Group of 20 cited the following causes:		During a period of strong global growth, growing capital flows, and prolonged stability earlier this decade, market participants sought higher yields without an adequate appreciation of the risks and failed to exercise proper due diligence. At the same time, weak underwriting standards, unsound risk management practices, increasingly complex and opaque financial products, and consequent excessive leverage combined to create vulnerabilities in the system. Policy-makers, regulators and supervisors, in some advanced countries, did not adequately appreciate and address the risks building up in financial markets, keep pace with financial innovation, or take into account the systemic ramifications of domestic regulatory actions.[37]		There are several "narratives" attempting to place the causes of the recession into context, with overlapping elements. Four such narratives include:		Underlying narratives #1-3 is a hypothesis that growing income inequality and wage stagnation encouraged families to increase their household debt to maintain their desired living standard, fueling the bubble. Further, this greater share of income flowing to the top increased the political power of business interests, who used that power to deregulate or limit regulation of the shadow banking system.[43][44][45]		The Economist wrote in July 2012 that the inflow of investment dollars required to fund the U.S. trade deficit was a major cause of the housing bubble and financial crisis: "The trade deficit, less than 1% of GDP in the early 1990s, hit 6% in 2006. That deficit was financed by inflows of foreign savings, in particular from East Asia and the Middle East. Much of that money went into dodgy mortgages to buy overvalued houses, and the financial crisis was the result."[46]		In May 2008, NPR explained in their Peabody Award winning program "The Giant Pool of Money" that a vast inflow of savings from developing nations flowed into the mortgage market, driving the U.S. housing bubble. This pool of fixed income savings increased from around $35 trillion in 2000 to about $70 trillion by 2008. NPR explained this money came from various sources, "[b]ut the main headline is that all sorts of poor countries became kind of rich, making things like TVs and selling us oil. China, India, Abu Dhabi, Saudi Arabia made a lot of money and banked it."[47]		Describing the crisis in Europe, Paul Krugman wrote in February 2012 that: "What we’re basically looking at, then, is a balance of payments problem, in which capital flooded south after the creation of the euro, leading to overvaluation in southern Europe."[48]		Another narrative about the origin has been focused on the respective parts played by the public monetary policy (in the US notably) and by the practices of private financial institutions. In the U.S., mortgage funding was unusually decentralised, opaque, and competitive, and it is believed that competition between lenders for revenue and market share contributed to declining underwriting standards and risky lending.[49]		While Alan Greenspan's role as Chairman of the Federal Reserve has been widely discussed (the main point of controversy remains the lowering of the Federal funds rate to 1% for more than a year, which, according to Austrian theorists, injected huge amounts of "easy" credit-based money into the financial system and created an unsustainable economic boom),[50] there is also the argument that Greenspan's actions in the years 2002–2004 were actually motivated by the need to take the U.S. economy out of the early 2000s recession caused by the bursting of the dot-com bubble—although by doing so he did not help avert the crisis, but only postpone it.[51][52]		Another narrative focuses on high levels of private debt in the US economy. USA household debt as a percentage of annual disposable personal income was 127% at the end of 2007, versus 77% in 1990.[53][54] Faced with increasing mortgage payments as their adjustable rate mortgage payments increased, households began to default in record numbers, rendering mortgage-backed securities worthless. High private debt levels also impact growth by making recessions deeper and the following recovery weaker.[55][56] Robert Reich claims the amount of debt in the US economy can be traced to economic inequality, assuming that middle-class wages remained stagnant while wealth concentrated at the top, and households "pull equity from their homes and overload on debt to maintain living standards."[57]		The IMF reported in April 2012: "Household debt soared in the years leading up to the downturn. In advanced economies, during the five years preceding 2007, the ratio of household debt to income rose by an average of 39 percentage points, to 138 percent. In Denmark, Iceland, Ireland, the Netherlands, and Norway, debt peaked at more than 200 percent of household income. A surge in household debt to historic highs also occurred in emerging economies such as Estonia, Hungary, Latvia, and Lithuania. The concurrent boom in both house prices and the stock market meant that household debt relative to assets held broadly stable, which masked households’ growing exposure to a sharp fall in asset prices. When house prices declined, ushering in the global financial crisis, many households saw their wealth shrink relative to their debt, and, with less income and more unemployment, found it harder to meet mortgage payments. By the end of 2011, real house prices had fallen from their peak by about 41% in Ireland, 29% in Iceland, 23% in Spain and the United States, and 21% in Denmark. Household defaults, underwater mortgages (where the loan balance exceeds the house value), foreclosures, and fire sales are now endemic to a number of economies. Household deleveraging by paying off debts or defaulting on them has begun in some countries. It has been most pronounced in the United States, where about two-thirds of the debt reduction reflects defaults."[58][59]		The onset of the economic crisis took most people by surprise. A 2009 paper identifies twelve economists and commentators who, between 2000 and 2006, predicted a recession based on the collapse of the then-booming housing market in the United States:[60] Dean Baker, Wynne Godley, Fred Harrison, Michael Hudson, Eric Janszen, Steve Keen, Jakob Brøchner Madsen, Jens Kjaer Sørensen, Kurt Richebächer, Nouriel Roubini, Peter Schiff, and Robert Shiller.[60]		By 2007, real estate bubbles were still under way in many parts of the world,[61] especially in the United States,[49] France, United Kingdom, Spain, The Netherlands, Australia, United Arab Emirates, New Zealand, Ireland, Poland,[62] South Africa, Israel, Greece, Bulgaria, Croatia,[63] Norway, Singapore, South Korea, Sweden, Finland, Argentina,[64] Baltic states, India, Romania, Ukraine, and China.[65] U.S. Federal Reserve Chairman Alan Greenspan said in mid-2005 that "at a minimum, there's a little 'froth' [in the U.S. housing market]...it's hard not to see that there are a lot of local bubbles".[66]		The Economist newspaper, writing at the same time, went further, saying "the worldwide rise in house prices is the biggest bubble in history".[67] Real estate bubbles are (by definition of the word "bubble") followed by a price decrease (also known as a housing price crash) that can result in many owners holding negative equity (a mortgage debt higher than the current value of the property).		Increases in uncertainty can depress investment, or consumption. The 2007–14 recession represents the most striking episode of heightened uncertainty since 1960.[68][69]		Several analysts, such as Peter Wallison and Edward Pinto of the American Enterprise Institute, have asserted that private lenders were encouraged to relax lending standards by government affordable housing policies.[70][71] They cite The Housing and Community Development Act of 1992, which initially required that 30 percent or more of Fannie’s and Freddie’s loan purchases be related to affordable housing. The legislation gave HUD the power to set future requirements, and eventually (under the Bush Administration) a 56 percent minimum was established.[72] To fulfil the requirements, Fannie Mae and Freddie Mac established programs to purchase $5 trillion in affordable housing loans,[73] and encouraged lenders to relax underwriting standards to produce those loans.[72]		These critics also cite, as inappropriate regulation, “The National Homeownership Strategy: Partners in the American Dream (“Strategy”), which was compiled in 1995 by Henry Cisneros, President Clinton’s HUD Secretary. In 2001, the independent research company, Graham Fisher & Company, stated: “While the underlying initiatives of the [Strategy] were broad in content, the main theme ... was the relaxation of credit standards.”[74]		The Community Reinvestment Act (CRA) is also identified as one of the causes of the recession, by some critics. They contend that lenders relaxed lending standards in an effort to meet CRA commitments, and they note that publicly announced CRA loan commitments were massive, totaling $4.5 trillion in the years between 1994 and 2007.[75]		However, the Financial Crisis Inquiry Commission (FCIC) concluded that Fannie & Freddie "were not a primary cause" of the crisis and that CRA was not a factor in the crisis.[35] Further, since housing bubbles appeared in multiple countries in Europe as well, the FCIC Republican minority dissenting report also concluded that U.S. housing policies were not a robust explanation for a wider global housing bubble.[35] The view that U.S. government housing policy was a primary cause of the crisis has been widely disputed,[76] with Paul Krugman referring to it as "imaginary history."[77]		Author Michael Lewis wrote that a type of derivative called a credit default swap (CDS) enabled speculators to stack bets on the same mortgage securities. This is analogous to allowing many persons to buy insurance on the same house. Speculators that bought CDS protection were betting that significant mortgage security defaults would occur, while the sellers (such as AIG) bet they would not. An unlimited amount could be wagered on the same housing-related securities, provided buyers and sellers of the CDS could be found.[78] When massive defaults occurred on underlying mortgage securities, companies like AIG that were selling CDS were unable to perform their side of the obligation and defaulted; U.S. taxpayers paid over $100 billion to global financial institutions to honor AIG obligations, generating considerable outrage.[79]		Derivatives such as CDS were unregulated or barely regulated. Several sources have noted the failure of the US government to supervise or even require transparency of the financial instruments known as derivatives.[80][81][82] A 2008 investigative article in the Washington Post found that leading government officials at the time (Federal Reserve Board Chairman Alan Greenspan, Treasury Secretary Robert Rubin, and SEC Chairman Arthur Levitt) vehemently opposed any regulation of derivatives. In 1998 Brooksley E. Born, head of the Commodity Futures Trading Commission, put forth a policy paper asking for feedback from regulators, lobbyists, legislators on the question of whether derivatives should be reported, sold through a central facility, or whether capital requirements should be required of their buyers. Greenspan, Rubin, and Levitt pressured her to withdraw the paper and Greenspan persuaded Congress to pass a resolution preventing CFTC from regulating derivatives for another six months — when Born's term of office would expire.[81] Ultimately, it was the collapse of a specific kind of derivative, the mortgage-backed security, that triggered the economic crisis of 2008.[82]		Paul Krugman wrote in 2009 that the run on the shadow banking system was the "core of what happened" to cause the crisis. "As the shadow banking system expanded to rival or even surpass conventional banking in importance, politicians and government officials should have realised that they were re-creating the kind of financial vulnerability that made the Great Depression possible – and they should have responded by extending regulations and the financial safety net to cover these new institutions. Influential figures should have proclaimed a simple rule: anything that does what a bank does, anything that has to be rescued in crises the way banks are, should be regulated like a bank." He referred to this lack of controls as "malign neglect."[83][84]		During 2008, three of the largest U.S. investment banks either went bankrupt (Lehman Brothers) or were sold at fire sale prices to other banks (Bear Stearns and Merrill Lynch). The investment banks were not subject to the more stringent regulations applied to depository banks. These failures exacerbated the instability in the global financial system. The remaining two investment banks, Morgan Stanley and Goldman Sachs, potentially facing failure, opted to become commercial banks, thereby subjecting themselves to more stringent regulation but receiving access to credit via the Federal Reserve.[85][86] Further, American International Group (AIG) had insured mortgage-backed and other securities but was not required to maintain sufficient reserves to pay its obligations when debtors defaulted on these securities. AIG was contractually required to post additional collateral with many creditors and counter-parties, touching off controversy when over $100 billion of U.S. taxpayer money was paid out to major global financial institutions on behalf of AIG. While this money was legally owed to the banks by AIG (under agreements made via credit default swaps purchased from AIG by the institutions), a number of Congressmen and media members expressed outrage that taxpayer money was used to bail out banks.[79]		Economist Gary Gorton wrote in May 2009: "Unlike the historical banking panics of the 19th and early 20th centuries, the current banking panic is a wholesale panic, not a retail panic. In the earlier episodes, depositors ran to their banks and demanded cash in exchange for their checking accounts. Unable to meet those demands, the banking system became insolvent. The current panic involved financial firms “running” on other financial firms by not renewing sale and repurchase agreements (repo) or increasing the repo margin (“haircut”), forcing massive deleveraging, and resulting in the banking system being insolvent."[87]		The Financial Crisis Inquiry Commission reported in January 2011: "In the early part of the 20th century, we erected a series of protections – the Federal Reserve as a lender of last resort, federal deposit insurance, ample regulations – to provide a bulwark against the panics that had regularly plagued America’s banking system in the 20th century. Yet, over the past 30-plus years, we permitted the growth of a shadow banking system – opaque and laden with short term debt – that rivaled the size of the traditional banking system. Key components of the market – for example, the multitrillion-dollar repo lending market, off-balance-sheet entities, and the use of over-the-counter derivatives – were hidden from view, without the protections we had constructed to prevent financial meltdowns. We had a 21st-century financial system with 19th-century safeguards."[35]		The financial crisis and the recession have been described as a symptom of another, deeper crisis by a number of economists. For example, Ravi Batra argues that growing inequality of financial capitalism produces speculative bubbles that burst and result in depression and major political changes.[88][89] Feminist economists Ailsa McKay and Margunn Bjørnholt argue that the financial crisis and the response to it revealed a crisis of ideas in mainstream economics and within the economics profession, and call for a reshaping of both the economy, economic theory and the economics profession. They argue that such a reshaping should include new advances within feminist economics and ecological economics that take as their starting point the socially responsible, sensible and accountable subject in creating an economy and economic theories that fully acknowledge care for each other as well as the planet.[90]		In the U.S., persistent high unemployment remained as of December 2012, along with low consumer confidence, the continuing decline in home values and increase in foreclosures and personal bankruptcies, an increasing federal debt, inflation, and rising petroleum and food prices. A 2011 poll found that more than half of all Americans thought that the U.S. was still in recession or even depression, although economic data showed a historically modest recovery.[91] This could have been because both private and public levels of debt were at historic highs in the U.S. and in many other countries.[92][93][94][95]		A 2011 poll found that more than half of all Americans think the U.S. is still in recession or even depression, despite official data that shows a historically modest recovery.[115] In 2013 the Census Bureau defined poverty rate decreased to roughly 14.5% of the population.[116] As late as 2014, and early 2015, a majority of Americans still believed that the nation remained in a recession.[117]		The crisis in Europe generally progressed from banking system crises to sovereign debt crises, as many countries elected to bailout their banking systems using taxpayer money.[citation needed] Greece was different in that it faced large public debts rather than problems within its banking system. Several countries received bailout packages from the troika (European Commission, European Central Bank, International Monetary Fund), which also implemented a series of emergency measures.		Many European countries embarked on austerity programs, reducing their budget deficits relative to GDP from 2010 to 2011. For example, according to the CIA World Factbook Greece improved its budget deficit from 10.4% GDP in 2010 to 9.6% in 2011. Iceland, Italy, Ireland, Portugal, France, and Spain also improved their budget deficits from 2010 to 2011 relative to GDP.[119][120]		However, with the exception of Germany, each of these countries had public-debt-to-GDP ratios that increased (i.e., worsened) from 2010 to 2011, as indicated in the chart at right. Greece's public-debt-to-GDP ratio increased from 143% in 2010 to 165% in 2011[119] to 185% in 2014. This indicates that despite improving budget deficits, GDP growth was not sufficient to support a decline (improvement) in the debt-to-GDP ratio for these countries during this period. Eurostat reported that the debt to GDP ratio for the 17 Euro area countries together was 70.1% in 2008, 79.9% in 2009, 85.3% in 2010, and 87.2% in 2011.[120][121]		According to the CIA World Factbook, from 2010 to 2011, the unemployment rates in Spain, Greece, Italy, Ireland, Portugal, and the UK increased. France had no significant changes, while in Germany and Iceland the unemployment rate declined.[119] Eurostat reported that Eurozone unemployment reached record levels in September 2012 at 11.6%, up from 10.3% the prior year. Unemployment varied significantly by country.[122]		Economist Martin Wolf analysed the relationship between cumulative GDP growth from 2008-2012 and total reduction in budget deficits due to austerity policies (see chart at right) in several European countries during April 2012. He concluded that: "In all, there is no evidence here that large fiscal contractions [budget deficit reductions] bring benefits to confidence and growth that offset the direct effects of the contractions. They bring exactly what one would expect: small contractions bring recessions and big contractions bring depressions." Changes in budget balances (deficits or surpluses) explained approximately 53% of the change in GDP, according to the equation derived from the IMF data used in his analysis.[118]		Economist Paul Krugman analysed the relationship between GDP and reduction in budget deficits for several European countries in April 2012 and concluded that austerity was slowing growth, similar to Martin Wolf. He also wrote: "... this also implies that 1 euro of austerity yields only about 0.4 euros of reduced deficit, even in the short run. No wonder, then, that the whole austerity enterprise is spiraling into disaster."[123]		Poland and Slovakia are the only two members of the European Union to have avoided a GDP recession during the years affected by the Great Recession. As of December 2009, the Polish economy had not entered recession nor even contracted, while its IMF 2010 GDP growth forecast of 1.9 percent was expected to be upgraded.[124][125][126] Analysts have identified several causes for the positive economic development in Poland: Extremely low levels of bank lending and a relatively very small mortgage market; the relatively recent dismantling of EU trade barriers and the resulting surge in demand for Polish goods since 2004; Poland being the recipient of direct EU funding since 2004; lack of over-dependence on a single export sector; a tradition of government fiscal responsibility; a relatively large internal market; the free-floating Polish zloty; low labour costs attracting continued foreign direct investment; economic difficulties at the start of the decade, which prompted austerity measures in advance of the world crisis.[citation needed]		While India, Uzbekistan, China, and Iran experienced slowing growth, they did not enter recessions.		South Korea narrowly avoided technical recession in the first quarter of 2009.[127] The International Energy Agency stated in mid September that South Korea could be the only large OECD country to avoid recession for the whole of 2009.[128] It was the only developed economy to expand in the first half of 2009.		Australia avoided a technical recession after experiencing only one quarter of negative growth in the fourth quarter of 2008, with GDP returning to positive in the first quarter of 2009.[129][130]		The financial crisis did not affect developing countries to a great extent. Experts see several reasons: Africa was not affected because it is not fully integrated in the world market. Latin America and Asia seemed better prepared, since they have experienced crises before. In Latin America, for example, banking laws and regulations are very stringent. Bruno Wenn of the German DEG suggests that Western countries could learn from these countries when it comes to regulations of financial markets.[131]		The table below displays all national recessions appearing in 2006-2013 (for the 71 countries with available data), according to the common recession definition, saying that a recession occurred whenever seasonally adjusted real GDP contracts quarter on quarter, through minimum two consecutive quarters. Only 11 out of the 71 listed countries with quarterly GDP data (Poland, Slovakia, Moldova, India, China, South Korea, Indonesia, Australia, Uruguay, Colombia and Bolivia) escaped a recession in this time period.		The few recessions appearing early in 2006-07 are commonly never associated to be part of the Great Recession, which is illustrated by the fact that only two countries (Iceland and Jamaica) were in recession in Q4-2007.		One year before the maximum, in Q1-2008, only six countries were in recession (Iceland, Sweden, Finland, Ireland, Portugal and New Zealand). The number of countries in recession was 25 in Q2‑2008, 39 in Q3‑2008 and 53 in Q4‑2008. At the steepest part of the Great Recession in Q1‑2009, a total of 59 out of 71 countries were simultaneously in recession. The number of countries in recession was 37 in Q2‑2009, 13 in Q3‑2009 and 11 in Q4‑2009. One year after the maximum, in Q1‑2010, only seven countries were in recession (Greece, Croatia, Romania, Iceland, Jamaica, Venezuela and Belize).		The recession data for the overall G20-zone (representing 85% of all GWP), depict that the Great Recession existed as a global recession throughout Q3‑2008 until Q1‑2009.		Subsequent follow-up recessions in 2010‑2013 were confined to Belize, El Salvador, Paraguay, Jamaica, Japan, Taiwan, New Zealand and 24 out of 50 European countries (including Greece). As of October 2014, only five out of the 71 countries with available quarterly data (Cyprus, Italy, Croatia, Belize and El Salvador), were still in ongoing recessions.[18][132] The many follow-up recessions hitting the European countries, are commonly referred to as being direct repercussions of the European sovereign‑debt crisis.		Iceland fell into an economic depression in 2008 following the collapse of its banking system (see 2008–2011 Icelandic financial crisis). By mid-2012 Iceland is regarded as one of Europe's recovery success stories largely as a result of a currency devaluation that has effectively reduced wages by 50%--making exports more competitive.[172]		The following countries had a recession already starting in the first quarter of 2008: Latvia,[173] Ireland,[174] New Zealand,[175] and Sweden.[18]		The following countries/territories had a recession starting in the second quarter of 2008: Japan,[176] Hong Kong,[177] Singapore,[178] Italy,[179] Turkey,[18] Germany,[180] United Kingdom,[18] the Eurozone,[181] the European Union,[18] and OECD.[18]		The following countries/territories had a recession starting in the third quarter of 2008: United States,[18] Spain,[182] and Taiwan.[183]		The following countries/territories had a recession starting in the fourth quarter of 2008: Switzerland.[184]		South Korea "miraculously" avoided recession with GDP returning positive at a 0.1% expansion in the first quarter of 2009.[185]		Of the seven largest economies in the world by GDP, only China avoided a recession in 2008. In the year to the third quarter of 2008 China grew by 9%. Until recently Chinese officials considered 8% GDP growth to be required simply to create enough jobs for rural people moving to urban centres.[186] This figure may more accurately be considered to be 5–7% now that the main growth in working population is receding.[citation needed]		Ukraine went into technical depression in January 2009 with a nominal annualised GDP growth of −20%, when comparing on a monthly basis with the GDP level in January 2008.[187] Overall the Ukrainian real GDP fell 14.8% when comparing the entire part of 2009 with 2008.[188] When measured quarter-on-quarter by changes of seasonally adjusted real GDP, Ukraine was more precisely in recession/depression throughout the four quarters from Q2-2008 until Q1-2009 (with respective qoq-changes of: -0.1%, -0.5%, -9.3%, -10.3%), and the two quarters from Q3-2012 until Q4-2012 (with respective qoq-changes of: -1.5% and -0.8%).[189]		Japan was in recovery in the middle of the decade 2000s but slipped back into recession and deflation in 2008.[190] The recession in Japan intensified in the fourth quarter of 2008 with a nominal annualized GDP growth of −12.7% (being equal to the seasonally adjusted real GDP having a quarter-on-quarter change of -3.2%),[191] and deepened further in the first quarter of 2009 with a nominal annualised GDP growth of −15.2% (being equal to the seasonally adjusted real GDP having a quarter-on-quarter change of -4.0%).[192]		On February 26, 2009, an Economic Intelligence Briefing was added to the daily intelligence briefings prepared for the President of the United States. This addition reflects the assessment of U.S. intelligence agencies that the global financial crisis presents a serious threat to international stability.[193]		Business Week stated in March 2009 that global political instability is rising fast because of the global financial crisis and is creating new challenges that need managing.[194] The Associated Press reported in March 2009 that: United States "Director of National Intelligence Dennis Blair has said the economic weakness could lead to political instability in many developing nations."[195] Even some developed countries are seeing political instability.[196] NPR reports that David Gordon, a former intelligence officer who now leads research at the Eurasia Group, said: "Many, if not most, of the big countries out there have room to accommodate economic downturns without having large-scale political instability if we're in a recession of normal length. If you're in a much longer-run downturn, then all bets are off."[197]		Political scientists have argued that the economic stasis triggered social churning that got expressed through protests on a variety of issues across the developing world. In Brazil, disaffected youth rallied against a minor bus-fare hike;[198] in Turkey, they agitated against the conversion of a park to a mall[199] and in Israel, they protested against high rents in Tel Aviv. In all these cases, the ostensible immediate cause of the protest was amplified by the underlying social suffering induced by the great recession.		In January 2009, the government leaders of Iceland were forced to call elections two years early after the people of Iceland staged mass protests and clashed with the police because of the government's handling of the economy.[196] Hundreds of thousands protested in France against President Sarkozy's economic policies.[200] Prompted by the financial crisis in Latvia, the opposition and trade unions there organised a rally against the cabinet of premier Ivars Godmanis. The rally gathered some 10–20 thousand people. In the evening the rally turned into a Riot. The crowd moved to the building of the parliament and attempted to force their way into it, but were repelled by the state's police. In late February many Greeks took part in a massive general strike because of the economic situation and they shut down schools, airports, and many other services in Greece.[201] Police and protesters clashed in Lithuania where people protesting the economic conditions were shot with rubber bullets.[202] Communists and others rallied in Moscow to protest the Russian government's economic plans.[203]		In addition to various levels of unrest in Europe, Asian countries have also seen various degrees of protest.[204] Protests have also occurred in China as demands from the west for exports have been dramatically reduced and unemployment has increased. Beyond these initial protests, the protest movement has grown and continued in 2011. In late 2011, the Occupy Wall Street protest took place in the United States, spawning several offshoots that came to be known as the Occupy movement.		In 2012 the economic difficulties in Spain increased support for secession movements. In Catalonia, support for the secession movement exceeded. On September 11, a pro-independence march drew a crowd that police estimated at 1.5 million.[205]		The financial phase of the crisis led to emergency interventions in many national financial systems. As the crisis developed into genuine recession in many major economies, economic stimulus meant to revive economic growth became the most common policy tool. After having implemented rescue plans for the banking system, major developed and emerging countries announced plans to relieve their economies. In particular, economic stimulus plans were announced in China, the United States, and the European Union.[206] In the final quarter of 2008, the financial crisis saw the G-20 group of major economies assume a new significance as a focus of economic and financial crisis management.		The Federal Reserve, Treasury, and Securities and Exchange Commission took several steps on September 19 to intervene in the crisis. To stop the potential run on money market mutual funds, the Treasury also announced on September 19 a new $50 billion program to insure the investments, similar to the Federal Deposit Insurance Corporation (FDIC) program.[207][208] Part of the announcements included temporary exceptions to section 23A and 23B (Regulation W), allowing financial groups to more easily share funds within their group. The exceptions would expire on January 30, 2009, unless extended by the Federal Reserve Board.[209] The Securities and Exchange Commission announced termination of short-selling of 799 financial stocks, as well as action against naked short selling, as part of its reaction to the mortgage crisis.[210] In May 2013 as the stock market was hitting record highs and the housing and employment markets were improving slightly[211] the prospect of the Federal Reserve beginning to decrease its economic stimulus activities began to enter the projections of investment analysts and affected global markets.[212]		On September 15, 2008, China cut its interest rate for the first time since 2002. Indonesia reduced its overnight repo rate, at which commercial banks can borrow overnight funds from the central bank, by two percentage points to 10.25 percent. The Reserve Bank of Australia injected nearly $1.5 billion into the banking system, nearly three times as much as the market's estimated requirement. The Reserve Bank of India added almost $1.32 billion, through a refinance operation, its biggest in at least a month.[213]		On November 9, 2008, the Chinese economic stimulus program, a RMB¥ 4 trillion ($586 billion) stimulus package, was announced by the central government of the People's Republic of China in its biggest move to stop the global financial crisis from hitting the world's second largest economy. A statement on the government's website said the State Council had approved a plan to invest 4 trillion yuan ($586 billion) in infrastructure and social welfare by the end of 2010. The stimulus package was invested in key areas such as housing, rural infrastructure, transportation, health and education, environment, industry, disaster rebuilding, income-building, tax cuts, and finance.		China's export driven economy is starting to feel the impact of the economic slowdown in the United States and Europe, and the government has already cut key interest rates three times in less than two months in a bid to spur economic expansion. On November 28, 2008, the Ministry of Finance of the People's Republic of China and the State Administration of Taxation jointly announced a rise in export tax rebate rates on some labour-intensive goods. These additional tax rebates will take place on December 1, 2008.[214]		The stimulus package was welcomed by world leaders and analysts as larger than expected and a sign that by boosting its own economy, China is helping to stabilise the global economy. News of the announcement of the stimulus package sent markets up across the world. However, Marc Faber claimed that he thought China was still in recession on January 16.		In Taiwan, the central bank on September 16, 2008, said it would cut its required reserve ratios for the first time in eight years. The central bank added $3.59 billion into the foreign-currency interbank market the same day. Bank of Japan pumped $29.3 billion into the financial system on September 17, 2008, and the Reserve Bank of Australia added $3.45 billion the same day.[215]		In developing and emerging economies, responses to the global crisis mainly consisted in low-rates monetary policy (Asia and the Middle East mainly) coupled with the depreciation of the currency against the dollar. There were also stimulus plans in some Asian countries, in the Middle East and in Argentina. In Asia, plans generally amounted to 1 to 3% of GDP, with the notable exception of China, which announced a plan accounting for 16% of GDP (6% of GDP per year).		Until September 2008, European policy measures were limited to a small number of countries (Spain and Italy). In both countries, the measures were dedicated to households (tax rebates) reform of the taxation system to support specific sectors such as housing. The European Commission proposed a €200 billion stimulus plan to be implemented at the European level by the countries. At the beginning of 2009, the UK and Spain completed their initial plans, while Germany announced a new plan.		On September 29, 2008, the Belgian, Luxembourg and Dutch authorities partially nationalised Fortis. The German government bailed out Hypo Real Estate.		On 8 October 2008 the British Government announced a bank rescue package of around £500 billion[216] ($850 billion at the time). The plan comprises three parts. The first £200 billion would be made in regard to the banks in liquidity stack. The second part will consist of the state government increasing the capital market within the banks. Along with this, £50 billion will be made available if the banks needed it, finally the government will write away any eligible lending between the British banks with a limit to £250 billion.		In early December German Finance Minister Peer Steinbrück indicated a lack of belief in a "Great Rescue Plan" and reluctance to spend more money addressing the crisis.[217] In March 2009, The European Union Presidency confirmed that the EU was at the time strongly resisting the US pressure to increase European budget deficits.[218]		From 2010, the United Kingdom began a fiscal consolidation program to reduce debt and deficit levels while at the same time stimulating economic recovery.[219] Other European countries also began fiscal consolidation with similar aims.[220]		Most political responses to the economic and financial crisis has been taken, as seen above, by individual nations. Some coordination took place at the European level, but the need to cooperate at the global level has led leaders to activate the G-20 major economies entity. A first summit dedicated to the crisis took place, at the Heads of state level in November 2008 (2008 G-20 Washington summit).		The G-20 countries met in a summit held on November 2008 in Washington to address the economic crisis. Apart from proposals on international financial regulation, they pledged to take measures to support their economy and to coordinate them, and refused any resort to protectionism.		Another G-20 summit was held in London on April 2009. Finance ministers and central banks leaders of the G-20 met in Horsham, England, on March to prepare the summit, and pledged to restore global growth as soon as possible. They decided to coordinate their actions and to stimulate demand and employment. They also pledged to fight against all forms of protectionism and to maintain trade and foreign investments. These actions will cost $1.1tn.[221]		They also committed to maintain the supply of credit by providing more liquidity and recapitalising the banking system, and to implement rapidly the stimulus plans. As for central bankers, they pledged to maintain low-rates policies as long as necessary. Finally, the leaders decided to help emerging and developing countries, through a strengthening of the IMF.		The IMF stated in September 2010 that the financial crisis would not end without a major decrease in unemployment as hundreds of millions of people were unemployed worldwide. The IMF urged governments to expand social safety nets and to generate job creation even as they are under pressure to cut spending. The IMF also encouraged governments to invest in skills training for the unemployed and even governments of countries, similar to that of Greece, with major debt risk to first focus on long-term economic recovery by creating jobs.[222]		The Bank of Israel was the first to raise interest rates after the global recession began.[223] It increased rates in August 2009.[223]		On October 6, 2009, Australia became the first G20 country to raise its main interest rate, with the Reserve Bank of Australia moving rates up from 3.00% to 3.25%.[224]		The Norges Bank of Norway and the Reserve Bank of India raised interest rates in March 2010.[225]		On April 17, 2009, the then head of the IMF Dominique Strauss-Kahn said that there was a chance that certain countries may not implement the proper policies to avoid feedback mechanisms that could eventually turn the recession into a depression. "The free-fall in the global economy may be starting to abate, with a recovery emerging in 2010, but this depends crucially on the right policies being adopted today." The IMF pointed out that unlike the Great Depression, this recession was synchronised by global integration of markets. Such synchronized recessions were explained to last longer than typical economic downturns and have slower recoveries.[226]		Olivier Blanchard, IMF Chief Economist, stated that the percentage of workers laid off for long stints has been rising with each downturn for decades but the figures have surged this time. "Long-term unemployment is alarmingly high: in the United States, half the unemployed have been out of work for over six months, something we have not seen since the Great Depression." The IMF also stated that a link between rising inequality within Western economies and deflating demand may exist. The last time that the wealth gap reached such skewed extremes was in 1928–1929.[227]		
Employment is a relationship between two parties, usually based on a contract where work is paid for, where one party, which may be a corporation, for profit, not-for-profit organization, co-operative or other entity is the employer and the other is the employee.[1] Employees work in return for payment, which may be in the form of an hourly wage, by piecework or an annual salary, depending on the type of work an employee does or which sector she or he is working in. Employees in some fields or sectors may receive gratuities, bonus payment or stock options. In some types of employment, employees may receive benefits in addition to payment. Benefits can include health insurance, housing, disability insurance or use of a gym. Employment is typically governed by employment laws or regulations or legal contracts.						An employee contributes labor and expertise to an endeavor of an employer or of a person conducting a business or undertaking (PCBU)[2] and is usually hired to perform specific duties which are packaged into a job. In a corporate context, an employee is a person who is hired to provide services to a company on a regular basis in exchange for compensation and who does not provide these services as part of an independent business.[3]		Employer and managerial control within an organization rests at many levels and has important implications for staff and productivity alike, with control forming the fundamental link between desired outcomes and actual processes. Employers must balance interests such as decreasing wage constraints with a maximization of labor productivity in order to achieve a profitable and productive employment relationship.		The main ways for employers to find workers and for people to find employers are via jobs listings in newspapers (via classified advertising) and online, also called job boards. Employers and job seekers also often find each other via professional recruitment consultants which receive a commission from the employer to find, screen and select suitable candidates. However, a study has shown that such consultants may not be reliable when they fail to use established principles in selecting employees.[1] A more traditional approach is with a "Help Wanted" sign in the establishment (usually hung on a window or door[4] or placed on a store counter).[3] Evaluating different employees can be quite laborious but setting up different techniques to analyze their skill to measure their talents within the field can be best through assessments.[5] Employer and potential employee commonly take the additional step of getting to know each other through the process of job interview.		Training and development refers to the employer's effort to equip a newly hired employee with necessary skills to perform at the job, and to help the employee grow within the organization. An appropriate level of training and development helps to improve employee's job satisfaction.[6]		There are many ways that employees are paid, including by hourly wages, by piecework, by yearly salary, or by gratuities (with the latter often being combined with another form of payment). In sales jobs and real estate positions, the employee may be paid a commission, a percentage of the value of the goods or services that they have sold. In some fields and professions (e.g., executive jobs), employees may be eligible for a bonus if they meet certain targets. Some executives and employees may be paid in stocks or stock options, a compensation approach that has the added benefit, from the company's point of view, of helping to align the interests of the compensated individual with the performance of the company.		Employee benefits are various non-wage compensation provided to employee in addition to their wages or salaries. The benefits can include: housing (employer-provided or employer-paid), group insurance (health, dental, life etc.), disability income protection, retirement benefits, daycare, tuition reimbursement, sick leave, vacation (paid and non-paid), social security, profit sharing, funding of education, and other specialized benefits. In some cases, such as with workers employed in remote or isolated regions, the benefits may include meals. Employee benefits can improve the relationship between employee and employer and lowers staff turnover.[7]		Organizational justice is an employee's perception and judgement of employer's treatment in the context of fairness or justice. The resulting actions to influence the employee-employer relationship is also a part of organizational justice.[7]		Employees can organize into trade or labor unions, which represent the work force to collectively bargain with the management of organizations about working, and contractual conditions and services.[8]		Usually, either an employee or employer may end the relationship at any time, often subject to a certain notice period. This is referred to as at-will employment. The contract between the two parties specifies the responsibilities of each when ending the relationship and may include requirements such as notice periods, severance pay, and security measures.[8] In some professions, notably teaching, civil servants, university professors, and some orchestra jobs, some employees may have tenure, which means that they cannot be dismissed at will. Another type of termination is a layoff.		Wage labor is the socioeconomic relationship between a worker and an employer, where the worker sells their labor under a formal or informal employment contract. These transactions usually occur in a labor market where wages are market determined.[6][7] In exchange for the wages paid, the work product generally becomes the undifferentiated property of the employer, except for special cases such as the vesting of intellectual property patents in the United States where patent rights are usually vested in the original personal inventor. A wage laborer is a person whose primary means of income is from the selling of his or her labor in this way.[8]		In modern mixed economies such as that of the OECD countries, it is currently the dominant form of work arrangement. Although most work occurs following this structure, the wage work arrangements of CEOs, professional employees, and professional contract workers are sometimes conflated with class assignments, so that "wage labor" is considered to apply only to unskilled, semi-skilled or manual labor.[9]		Wage labor, as institutionalized under today's market economic systems, has been criticized,[8] especially by both mainstream socialists and anarcho-syndicalists,[9][10][11][12] using the pejorative term wage slavery.[13][14] Socialists draw parallels between the trade of labor as a commodity and slavery. Cicero is also known to have suggested such parallels.[15]		The American philosopher John Dewey posited that until "industrial feudalism" is replaced by "industrial democracy", politics will be "the shadow cast on society by big business".[16] Thomas Ferguson has postulated in his investment theory of party competition that the undemocratic nature of economic institutions under capitalism causes elections to become occasions when blocs of investors coalesce and compete to control the state.[17]		Australian employment has been governed by the Fair Work Act since 2009.[18]		Bangladesh Association of International Recruiting Agencies (BAIRA) is an association of national level with its international reputation of co-operation and welfare of the migrant workforce as well as its approximately 1200 members agencies in collaboration with and support from the Government of Bangladesh.[9]		In the Canadian province of Ontario, formal complaints can be brought to the Ministry of Labour. In the province of Quebec, grievances can be filed with the Commission des normes du travail.[12]		Pakistan has Contract Labor, Minimum Wage and Provident Funds Acts. Contract labor in Pakistan must be paid minimum wage and certain facilities are to be provided to labor. However, the Acts are not yet fully implemented.[9]		India has Contract Labor, Minimum Wage, Provident Funds Act and various other acts to comply with. Contract labor in India must be paid minimum wage and certain facilities are to be provided to labor. However, there is still a large amount of work that remains to be done to fully implement the Act.[12]		In the Philippines, private employment is regulated under the Labor Code of the Philippines by the Department of Labor and Employment.[19]		In the United Kingdom, employment contracts are categorized by the government into the following types:[20]		For purposes of U.S. federal income tax withholding, 26 U.S.C. § 3401(c) provides a definition for the term "employee" specific to chapter 24 of the Internal Revenue Code:		"For purposes of this chapter, the term “employee” includes an officer, employee, or elected official of the United States, a State, or any political subdivision thereof, or the District of Columbia, or any agency or instrumentality of any one or more of the foregoing. The term “employee” also includes an officer of a corporation."[21] This definition does not exclude all those who are commonly known as 'employees'. “Similarly, Latham’s instruction which indicated that under 26 U.S.C. § 3401(c) the category of ‘employee’ does not include privately employed wage earners is a preposterous reading of the statute. It is obvious that within the context of both statutes the word ‘includes’ is a term of enlargement not of limitation, and the reference to certain entities or categories is not intended to exclude all others.”[22]		Employees are often contrasted with independent contractors, especially when there is dispute as to the worker's entitlement to have matching taxes paid, workers compensation, and unemployment insurance benefits. However, in September 2009, the court case of Brown v. J. Kaz, Inc. ruled that independent contractors are regarded as employees for the purpose of discrimination laws if they work for the employer on a regular basis, and said employer directs the time, place, and manner of employment.[19]		In non-union work environments, in the United States, unjust termination complaints can be brought to the United States Department of Labor.[23]		Labor unions are legally recognized as representatives of workers in many industries in the United States. Their activity today centers on collective bargaining over wages, benefits, and working conditions for their membership, and on representing their members in disputes with management over violations of contract provisions. Larger unions also typically engage in lobbying activities and electioneering at the state and federal level.[19]		Most unions in America are aligned with one of two larger umbrella organizations: the AFL-CIO created in 1955, and the Change to Win Federation which split from the AFL-CIO in 2005. Both advocate policies and legislation on behalf of workers in the United States and Canada, and take an active role in politics. The AFL-CIO is especially concerned with global trade issues.[17]		According to Swedish law,[24] there are three types of employment.		There are no laws about minimum salary in Sweden. Instead there are agreements between employer organizations and trade unions about minimum salaries, and other employment conditions.		There is a type of employment contract which is common but not regulated in law, and that is Hour employment (swe: Timanställning), which can be Normal employment (unlimited), but the work time is unregulated and decided per immediate need basis. The employee is expected to be answering the phone and come to work when needed, e.g. when someone is ill and absent from work. They will receive salary only for actual work time and can in reality be fired for no reason by not being called anymore. This type of contract is common in the public sector.[25]		Young workers are at higher risk for occupational injury and face certain occupational hazards at a higher rate; this is generally due to their employment in high-risk industries. For example, in the United States, young people are injured at work at twice the rate of their older counterparts.[26] These workers are also at higher risk for motor vehicle accidents at work, due to less work experience, a lower use of seatbelts, and higher rates of distracted driving.[27][28] To mitigate this risk, those under the age of 17 are restricted from certain types of driving, including transporting people and goods under certain circumstances.[27]		High-risk industries for young workers include agriculture, restaurants, waste management, and mining.[26][27] In the United States, those under the age of 18 are restricted from certain jobs that are deemed dangerous under the Fair Labor Standards Act.[27]		Youth employment programs are most effective when they include both theoretical classroom training and hands-on training with work placements.[29]		Those older than the statutory defined retirement age may continue to work, either out of enjoyment or necessity. However, depending on the nature of the job, older workers may need to transition into less-physical forms of work to avoid injury. Working past retirement age also has positive effects, because it gives a sense of purpose and allows people to maintain social networks and activity levels.[30] Older workers are often found to be discriminated against by employers.[31]		Employment is no guarantee of escaping poverty, the International Labour Organization (ILO) estimates that as many as 40% of workers are poor, not earning enough to keep their families above the $2 a day poverty line.[25] For instance, in India most of the chronically poor are wage earners in formal employment, because their jobs are insecure and low paid and offer no chance to accumulate wealth to avoid risks.[25] According to the UNRISD, increasing labor productivity appears to have a negative impact on job creation: in the 1960s, a 1% increase in output per worker was associated with a reduction in employment growth of 0.07%, by the first decade of this century the same productivity increase implies reduced employment growth by 0.54%.[25] Both increased employment opportunities and increased labor productivity (as long as it also translates into higher wages) are needed to tackle poverty. Increases in employment without increases in productivity leads to a rise in the number of "working poor", which is why some experts are now promoting the creation of "quality" and not "quantity" in labor market policies.[25] This approach does highlight how higher productivity has helped reduce poverty in East Asia, but the negative impact is beginning to show.[25] In Vietnam, for example, employment growth has slowed while productivity growth has continued.[25] Furthermore, productivity increases do not always lead to increased wages, as can be seen in the United States, where the gap between productivity and wages has been rising since the 1980s.[25]		Researchers at the Overseas Development Institute argue that there are differences across economic sectors in creating employment that reduces poverty.[25] 24 instances of growth were examined, in which 18 reduced poverty. This study showed that other sectors were just as important in reducing unemployment, such as manufacturing.[25] The services sector is most effective at translating productivity growth into employment growth. Agriculture provides a safety net for jobs and economic buffer when other sectors are struggling.[25]		Scholars conceptualize the employment relationship in various ways.[32] A key assumption is the extent to which the employment relationship necessarily includes conflicts of interests between employers and employees, and the form of such conflicts.[33] In economic theorizing, the labor market mediates all such conflicts such that employers and employees who enter into an employment relationship are assumed to find this arrangement in their own self-interest. In human resource management theorizing, employers and employees are assumed to have shared interests (or a unity of interests, hence the label “unitarism”). Any conflicts that exist are seen as a manifestation of poor human resource management policies or interpersonal clashes such as personality conflicts, both of which can and should be managed away. From the perspective of pluralist industrial relations, the employment relationship is characterized by a plurality of stakeholders with legitimate interests (hence the label “pluralism), and some conflicts of interests are seen as inherent in the employment relationship (e.g., wages v. profits). Lastly, the critical paradigm emphasizes antagonistic conflicts of interests between various groups (e.g., the competing capitalist and working classes in a Marxist framework) that are part of a deeper social conflict of unequal power relations. As a result, there are four common models of employment:[34]		These models are important because they help reveal why individuals hold differing perspectives on human resource management policies, labor unions, and employment regulation.[35] For example, human resource management policies are seen as dictated by the market in the first view, as essential mechanisms for aligning the interests of employees and employers and thereby creating profitable companies in the second view, as insufficient for looking out for workers’ interests in the third view, and as manipulative managerial tools for shaping the ideology and structure of the workplace in the fourth view.[36]		Literature on the employment impact of economic growth and on how growth is associated with employment at a macro, sector and industry level was aggregated in 2013.[37]		Researchers found evidence to suggest growth in manufacturing and services have good impact on employment. They found GDP growth on employment in agriculture to be limited, but that value-added growth had a relatively larger impact.[25] The impact on job creation by industries/economic activities as well as the extent of the body of evidence and the key studies. For extractives, they again found extensive evidence suggesting growth in the sector has limited impact on employment. In textiles however, although evidence was low, studies suggest growth there positively contributed to job creation. In agri-business and food processing, they found impact growth to be positive.[37]		They found that most available literature focuses on OECD and middle-income countries somewhat, where economic growth impact has been shown to be positive on employment. The researchers didn't find sufficient evidence to conclude any impact of growth on employment in LDCs despite some pointing to the positive impact, others point to limitations. They recommended that complementary policies are necessary to ensure economic growth's positive impact on LDC employment. With trade, industry and investment, they only found limited evidence of positive impact on employment from industrial and investment policies and for others, while large bodies of evidence does exist, the exact impact remains contested.[37]		Researchers have also explored the relationship between employment and illicit activities. Using evidence from Africa, a research team found that a program for Liberian ex-fighters reduced work hours on illicit activities. The employment program also reduced interest in mercenary work in nearby wars. The study concludes that while the use of capital inputs or cash payments for peaceful work created a reduction in illicit activities, the impact of training alone is rather low.[38]		The balance of economic efficiency and social equity is the ultimate debate in the field of employment relations.[39] By meeting the needs of the employer; generating profits to establish and maintain economic efficiency; whilst maintaining a balance with the employee and creating social equity that benefits the worker so that he/she can fund and enjoy healthy living; proves to be a continuous revolving issue in westernized societies.[39]		Globalization has effected these issues by creating certain economic factors that disallow or allow various employment issues. Economist Edward Lee (1996) studies the effects of globalization and summarizes the four major points of concern that affect employment relations:		What also results from Lee’s (1996) findings is that in industrialized countries an average of almost 70 per cent of workers are employed in the service sector, most of which consists of non-tradable activities. As a result, workers are forced to become more skilled and develop sought after trades, or find other means of survival. Ultimately this is a result of changes and trends of employment, an evolving workforce, and globalization that is represented by a more skilled and increasing highly diverse labor force, that are growing in non standard forms of employment (Markey, R. et al. 2006).[39]		Various youth subcultures have been associated with not working, such as the hippie subculture in the 1960s and 1970s (which endorsed the idea of "dropping out" of society) and the punk subculture, in which some members live in anarchist squats (illegal housing).		One of the alternatives to work is engaging in postsecondary education at a college, university or professional school. One of the major costs of obtaining a postsecondary education is the opportunity cost of forgone wages due to not working. At times when jobs are hard to find, such as during recessions, unemployed individuals may decide to get postsecondary education, because there is less of an opportunity cost.		Workplace democracy is the application of democracy in all its forms (including voting systems, debates, democratic structuring, due process, adversarial process, systems of appeal) to the workplace.[40][41]		When an individual entirely owns the business for which they labor, this is known as self-employment. Self-employment often leads to incorporation. Incorporation offers certain protections of one's personal assets.[39] Individuals who are self-employed may own a small business. They may also be considered to be an entrepreneur.		In some countries, individuals who are not working can receive social assistance support (e.g., welfare or food stamps) to enable them to rent housing, buy food, repair or replace household goods, maintenance of children and observe social customs that require financial expenditure.		Workers who are not paid wages, such as volunteers who perform tasks for charities, hospitals or not-for-profit organizations, are generally not considered employed. One exception to this is an internship, an employment situation in which the worker receives training or experience (and possibly college credit) as the chief form of compensation.[40]		Those who work under obligation for the purpose of fulfilling a debt, such as an indentured servant, or as property of the person or entity they work for, such as a slave, do not receive pay for their services and are not considered employed. Some historians suggest that slavery is older than employment, but both arrangements have existed for all recorded history. Indentured servitude and slavery are not considered compatible with human rights and democracy.[40]		
A recommendation letter or letter of recommendation, also known as a letter of reference, reference letter or simply reference, is a document in which the writer assesses the qualities, characteristics, and capabilities of the person being recommended in terms of that individual's ability to perform a particular task or function. Letters of recommendation are typically related to employment (such a letter may also be called an employment reference or job reference), admission to institutions of higher education, or scholarship eligibility. Recommendation letters are usually specifically requested to be written about someone, and are therefore addressed to a particular requester (such as a new employer, university admissions officer, etc.), although they may also be issued to the person being recommended without specifying an addressee.		References may also be required of companies seeking to win contracts, particularly in the fields of engineering, consultancy, industry and construction, and with regard to public procurement and tenders. Reference letters for organizations are used to assess its ability to deliver the required level of service.		Some applications, such as professional schools give applicants the choice to waive their right to view their letters or not. Usually, applicants are encouraged to waive their rights because if they do not, it is a sign they are not confident in their recommenders.						The person providing a reference is called a referee. An employment reference letter is usually written by a former employer or manager, but references can also be requested from co-workers, customers and vendors.[1] Teachers and professors often supply references for students who have taken their classes.[2] Reference letters for organizations are usually supplied by parties to which the company has provided similar services in the past.		The employment reference letter can cover topics such as:[3]		In some countries, elements of performance are evaluated using established forms of expression, sometimes euphemistic. For example, in the German-language Arbeitszeugnis, the following terms are frequently used:[4]		This language established itself as an unwritten code in the employment world. Its purpose was to give even weakly performing employees a letter of recommendation that does not sound negative. However, the euphemistically glazed-over descriptions are now codified and generally known, so that the original cryptic intent is no longer served. [5] Nonetheless, it is still standard to use this codified language.		Most potential employers will contact referees to obtain references before offering a job to a new employee. A survey by the Society for Human Resource Management (SHRM) found that eight out of ten resource professionals said they regularly conduct reference checks for professional (89%), executive (85%), administrative (84%) and technical (81%) positions.[6] Candidates are advised to ensure that they provide a suitable list of referees to their new prospective employer or institution, and to contact those referees to ensure that they are able and willing to provide a suitable reference. In some cases employers will contact a candidate's former company for a reference even if no contact is supplied by the candidate.		Some employers may not be willing to provide reference letters because they may be worried about potential lawsuits. In this case, the employer may only provide the job title, dates of employment and salary history for the employee.[1] Germany, Austria, Switzerland and Bulgaria are the only countries in Europe where employees can legally claim an employment reference, including the right to a correct, unambiguous and benevolent appraisal.[7]		While there is no common law duty to provide a reference,[8] the Supreme Court of Canada has held that a refusal to do so may constitute "conduct that is unfair or is in bad faith" with respect to a wrongful dismissal, and thus "indicative of the type of conduct that ought to merit compensation by way of an addition to the notice period."[9] There is a duty of care to ensure that, where one is provided, it is accurate and fair and not give a misleading impression,[10] as held by the House of Lords in Spring v Guardian Assurance plc.[11] If an employer goes beyond what a reference should contain, or if it gives inaccurate or misleading information, liability may arise in the areas of breach of statutory duty, negligent misstatement, deceit, defamation or malicious falsehood.[10] It does not matter what form the reference might take.[12]		In the United Kingdom, references received by an employer from another person or organization can be disclosed to the person about whom they are written under the subject access provisions of the Data Protection Act 1998, but certain confidentiality considerations apply as to the identity of the person giving the reference.[13] As a result, together with the duty of care under Spring, many organizations have issued guidance as to best practice to be undertaken by reference providers.[14][15]		The duty of care has also been held to apply in non-reference situations, as noted in 2011 in McKie v Swindon College.[16] In another case, the Court of Appeal of England and Wales has held that "a reference must not give an unfair or misleading impression overall, even if its discrete components are factually correct."[17] However, while a reference must be accurate and fair, it is not necessary to report all material facts concerning an individual,[18] but it can be argued that, if an agreed reference arising from a settlement agreement is misleadingly incomplete, the employer can be sued by a subsequent employer for breaching its duty of care.[19] The Employment Appeal Tribunal, in an unfair dismissal case, ruled that, in preparing a reference, it was not reasonable to provide details of complaints against an employee of which the employee was not aware.[20]		The Court of Appeal has further held that, if an employee leaves when an investigation is ongoing but has not been concluded, or where issues arise after an employee leaves that have not been investigated, employers can disclose this information but should do so in a measured and fair way, which will be particularly important if to omit this information would mean providing a misleading reference.[21]		In 2016. the Financial Conduct Authority and the Prudential Regulation Authority are issuing rules that will require the furnishing of references, before any approval or certification may be given by them, as well as specifying the information that they must contain.[10][22][23]		
Working time is the period of time that a person spends at paid labor. Unpaid labor such as personal housework or caring for children or pets is not considered part of the working week.		Many countries regulate the work week by law, such as stipulating minimum daily rest periods, annual holidays, and a maximum number of working hours per week. Working time may vary from person to person, often depending on location, culture, lifestyle choice, and the profitability of the individual's livelihood. For example, someone who is supporting children and paying a large mortgage will need to work more hours to meet basic costs of living than someone of the same earning power without children. Because fewer people than ever are having children,[1] choosing part time work is becoming more popular.[2]		Standard working hours (or normal working hours) refers to the legislation to limit the working hours per day, per week, per month or per year. If an employee needs to work overtime, the employer will need to pay overtime payments to employees as required in the law. Generally speaking, standard working hours of countries worldwide are around 40 to 44 hours per week (but not everywhere: from 35 hours per week in France[3] to up to 112 hours per week in North Korean labor camps [4]), and the additional overtime payments are around 25% to 50% above the normal hourly payments. Maximum working hours refers to the maximum working hours of an employee. The employee cannot work more than the level specified in the maximum working hours law.[5]		Since the 1960s, the consensus among anthropologists, historians, and sociologists has been that early hunter-gatherer societies enjoyed more leisure time than is permitted by capitalist and agrarian societies;[6][7] for instance, one camp of !Kung Bushmen was estimated to work two-and-a-half days per week, at around 6 hours a day.[8] Aggregated comparisons show that on average the working day was less than five hours.[6]		Subsequent studies in the 1970s examined the Machiguenga of the Upper Amazon and the Kayapo of northern Brazil. These studies expanded the definition of work beyond purely hunting-gathering activities, but the overall average across the hunter-gatherer societies he studied was still below 4.86 hours, while the maximum was below 8 hours.[6] Popular perception is still aligned with the old academic consensus that hunter-gatherers worked far in excess of modern humans' forty-hour week.[7]		The industrial revolution made it possible for a larger segment of the population to work year-round, because this labor was not tied to the season and artificial lighting made it possible to work longer each day. Peasants and farm laborers moved from rural areas to work in urban factories, and working time during the year increased significantly.[9] Before collective bargaining and worker protection laws, there was a financial incentive for a company to maximize the return on expensive machinery by having long hours. Records indicate that work schedules as long as twelve to sixteen hours per day, six to seven days per week were practiced in some industrial sites.[citation needed]		Over the 20th century, work hours shortened by almost half, mostly due to rising wages brought about by renewed economic growth, with a supporting role from trade unions, collective bargaining, and progressive legislation. The workweek, in most of the industrialized world, dropped steadily, to about 40 hours after World War II. The limitation of working hours is also proclaimed by the Universal Declaration of Human Rights,[10] International Covenant on Economic, Social and Cultural Rights,[11] and European Social Charter.[12] The decline continued at a faster pace in Europe: for example, France adopted a 35-hour workweek in 2000. In 1995, China adopted a 40-hour week, eliminating half-day work on Saturdays (though this is not widely practiced). Working hours in industrializing economies like South Korea, though still much higher than the leading industrial countries, are also declining steadily.		Technology has also continued to improve worker productivity, permitting standards of living to rise as hours decline.[13] In developed economies, as the time needed to manufacture goods has declined, more working hours have become available to provide services, resulting in a shift of much of the workforce between sectors.		Economic growth in monetary terms tends to be concentrated in health care, education, government, criminal justice, corrections, and other activities that are regarded as necessary for society rather than those that contribute directly to the production of material goods.[citation needed]		In the mid-2000s, the Netherlands was the first country in the industrialized world where the overall average working week dropped to less than 30 hours.[14]		Most countries in the developed world have seen average hours worked decrease significantly.[15][16] For example, in the U.S in the late 19th century it was estimated that the average work week was over 60 hours per week.[17] Today the average hours worked in the U.S is around 33,[18] with the average man employed full-time for 8.4 hours per work day, and the average woman employed full-time for 7.7 hours per work day.[19] The front runners for lowest average weekly work hours are the Netherlands with 27 hours,[20] and France with 30 hours.[21] At current rates the Netherlands is set to become the first country to reach an average work week under 21 hours.[22] In a 2011 report of 26 OECD countries, Germany had the lowest average working hours per week at 25.6 hours.[23]		The New Economics Foundation has recommended moving to a 21-hour standard work week to address problems with unemployment, high carbon emissions, low well-being, entrenched inequalities, overworking, family care, and the general lack of free time.[24][25][26] Actual work week lengths have been falling in the developed world.[27]		Factors that have contributed to lowering average work hours and increasing standard of living have been:		Recent articles[28][29] supporting a four-day week have argued that reduced work hours would increase consumption and invigorate the economy. However, other articles state that consumption would decrease.[30][31] Other arguments for the four-day week include improvements to workers' level of education (due to having extra time to take classes and courses) and improvements to workers' health (less work-related stress and extra time for exercise). Reduced hours also save money on day care costs and transportation, which in turn helps the environment with less carbon-related emissions. These benefits increase workforce productivity on a per-hour basis.		The structure of the work week varies considerably for different professions and cultures. Among salaried workers in the western world, the work week often consists of Monday to Friday or Saturday with the weekend set aside as a time of personal work and leisure. Sunday is set aside in the western world because it is the Christian sabbath.		The traditional American business hours are 9:00 a.m. to 5:00 p.m., Monday to Friday, representing a workweek of five eight-hour days comprising 40 hours in total. These are the origin of the phrase 9-to-5, used to describe a conventional and possibly tedious job.[32] Negatively used, it connotes a tedious or unremarkable occupation. The phrase also indicates that a person is an employee, usually in a large company, rather than self-employed. More neutrally, it connotes a job with stable hours and low career risk, but still a position of subordinate employment. The actual time at work often varies between 35 and 48 hours in practice due to the inclusion, or lack of inclusion, of breaks. In many traditional white collar positions, employees were required to be in the office during these hours to take orders from the bosses, hence the relationship between this phrase and subordination. Workplace hours have become more flexible, but even still the phrase is commonly used.		The meaning of the phrase in common usage has radically changed over the recent years or decades. Formerly the meaning of "someone working 9 to 5" was in the positive, that someone was working really hard to make his living in a full time job. Nowadays the usual meaning of a "9 to 5 guy" or a "9 to 5 mentality" is in a negative way, that someone has a less than desirable approach to work, being a "clock watcher" and refusing overtime. So the meaning changed from "working a lot/hard" to "could work more/harder".[citation needed]		Several countries have adopted a workweek from Monday morning until Friday noon, either due to religious rules (observation of shabbat in Israel whose workweek is Sunday to Friday afternoon) or the growing predominance of a 35–37.5 hour workweek in continental Europe. Several of the Muslim countries have a standard Sunday through Thursday or Saturday through Wednesday workweek leaving Friday for religious observance, and providing breaks for the daily prayer times.[citation needed]		South Korea has the fastest shortening working time in the OECD,[36] which is the result of the government's proactive move to lower working hours at all levels to increase leisure and relaxation time, which introduced the mandatory forty-hour, five-day working week in 2004 for companies with over 1,000 employees. Beyond regular working hours, it is legal to demand up to 12 hours of overtime during the week, plus another 16 hours on weekends.[citation needed] The 40-hour workweek expanded to companies with 300 employees or more in 2005, 100 employees or more in 2006, 50 or more in 2007, 20 or more in 2008 and a full inclusion to all workers nationwide in July 2011.[37] The government has continuously increased public holidays to 16 days in 2013, more than the 10 days of the United States and double that of the United Kingdom's 8 days.[38] Despite those efforts, South Korea's work hours are still relatively long, with an average 2,163 hours per year in 2012.[39]		Work hours in Japan are decreasing, but many Japanese still work long hours.[40] Recently, Japan's Ministry of Health, Labor and Welfare (MHLW) issued a draft report recommending major changes to the regulations that govern working hours. The centerpiece of the proposal is an exemption from overtime pay for white-collar workers.[citation needed]Japan has enacted an 8-hour work day and 40-hour work week (44 hours in specified workplaces). The overtime limits are: 15 hours a week, 27 hours over two weeks, 43 hours over four weeks, 45 hours a month, 81 hours over two months and 120 hours over three months; however, some workers get around these restrictions by working several hours a day without 'clocking in' whether physically or metaphorically.[41][citation needed] The overtime allowance should not be lower than 125% and not more than 150% of the normal hourly rate.[42]		In most European Union countries, working time is gradually decreasing.[43] The European Union's working time directive imposes a 48-hour maximum working week that applies to every member state except the United Kingdom and Malta (which have an opt-out, meaning that UK-based employees may work longer than 48 hours if they wish, but they cannot be forced to do so).[44] France has enacted a 35-hour workweek by law, and similar results have been produced in other countries through collective bargaining.[citation needed] A major reason for the low annual hours worked in Europe is a relatively high amount of paid annual leave.[45] Fixed employment comes with four to six weeks of holiday as standard. In the UK, for example, full-time employees are entitled to 28 days of paid leave a year.[46] It is commonly understood working hours in the UK are 09.00 to 17.00.[citation needed]		Mexican laws mandate a maximum of 48 hours of work per week, but they are rarely observed or enforced due to loopholes in the law, the volatility of labor rights in Mexico, and its underdevelopment relative to other members countries of the Organisation for Economic Co-operation and Development (OECD). Indeed, private sector employees often work overtime without receiving overtime compensation. Fear of unemployment and threats by employers explain in part why the 48-hour work week is disregarded.[citation needed].		Articles 161 to 167 of the Substantive Work Code in Colombia provide for a maximum of 48 hours of work a week.[47]		In Australia, between 1974 and 1997 no marked change took place in the average amount of time spent at work by Australians of "prime working age" (that is, between 25 and 54 years of age). Throughout this period, the average time spent at work by prime working-age Australians (including those who did not spend any time at work) remained stable at between 27 and 28 hours per week. This unchanging average, however, masks a significant redistribution of work from men to women. Between 1974 and 1997, the average time spent at work by prime working-age Australian men fell from 45 to 36 hours per week, while the average time spent at work by prime working-age Australian women rose from 12 to 19 hours per week. In the period leading up to 1997, the amount of time Australian workers spent at work outside the hours of 9 a.m. to 5 p.m. on weekdays also increased.[48]		In 2009, a rapid increase in the number of working hours was reported in a study by The Australia Institute. The study found the average Australian worked 1855 hours per year at work. According to Clive Hamilton of The Australia Institute, this surpasses even Japan. The Australia Institute believes that Australians work the highest number of hours in the developed world.[49]		From January 1, 2010, Australia enacted a 38-hour workweek in accordance with the Fair Work Act 2009, with an allowance for additional hours as overtime.[50]		The vast majority of full-time employees in Australia work additional overtime hours. A 2015 survey found that of Australia's 7.7 million full-time workers, 5 million put in more than 40 hours a week, including 1.4 million who worked more than 50 hours a week and 270,000 who put in more than 70 hours.[51]		In 2006, the average man employed full-time worked 8.4 hours per work day, and the average woman employed full-time worked 7.7 hours per work day.[19] There is no mandatory minimum amount of paid time off for sickness or holiday. The majority of jobs in America do not offer paid time off. [52] Because of the pressure of working, time is increasingly viewed as a commodity.[53]		By 1946 the United States government had inaugurated the 40-hour work week for all federal employees.[54] Beginning in 1950, under the Truman Administration, the United States became the first known industrialized nation to explicitly (albeit secretly) and permanently forswear a reduction of working time. Given the military-industrial requirements of the Cold War, the authors of the then secret National Security Council Report 68 (NSC-68)[55] proposed the US government undertake a massive permanent national economic expansion that would let it “siphon off” a part of the economic activity produced to support an ongoing military buildup to contain the Soviet Union. In his 1951 Annual Message to the Congress, President Truman stated:		In terms of manpower, our present defense targets will require an increase of nearly one million men and women in the armed forces within a few months, and probably not less than four million more in defense production by the end of the year. This means that an additional 8 percent of our labor force, and possibly much more, will be required by direct defense needs by the end of the year. These manpower needs will call both for increasing our labor force by reducing unemployment and drawing in women and older workers, and for lengthening hours of work in essential industries.[56]		According to the Bureau of Labor Statistics, the average non-farm private sector employee worked 34.5 hours per week as of June 2012.[57]		As President Truman’s 1951 message had predicted, the share of working women rose from 30 percent of the labor force in 1950 to 47 percent by 2000 – growing at a particularly rapid rate during the 1970s.[58] According to a Bureau of Labor Statistics report issued May 2002, "In 1950, the overall participation rate of women was 34 percent ... The rate rose to 38 percent in 1960, 43 percent in 1970, 52 percent in 1980, and 58 percent in 1990 and reached 60 percent by 2000. The overall labor force participation rate of women is projected to attain its highest level in 2010, at 62 percent.”[58] The inclusion of women in the work force can be seen as symbolic of social progress as well as of increasing American productivity and hours worked.		Between 1950 and 2007 official price inflation was measured to 861 percent. President Truman, in his 1951 message to Congress, predicted correctly that his military buildup “will cause intense and mounting inflationary pressures.” Using the data provided by the United State Bureau of Labor Statistics, Erik Rauch has estimated productivity to have increased by nearly 400%.[59] According to Rauch, “if productivity means anything at all, a worker should be able to earn the same standard of living as a 1950 worker in only 11 hours per week.”		In the United States, the working time for upper-income professionals has increased compared to 1965, while total annual working time for low-skill, low-income workers has decreased.[60] This effect is sometimes called the "leisure gap".		The average working time of married couples – of both spouses taken together – rose from 56 hours in 1969 to 67 hours in 2000.[61]		Many professional workers put in longer hours than the forty-hour standard.[62] In professional industries like investment banking and large law firms, a forty-hour workweek is considered inadequate and may result in job loss or failure to be promoted.[63][64] Medical residents in the United States routinely work long hours as part of their training.		Workweek policies are not uniform in the U.S. Many compensation arrangements are legal, and three of the most common are wage, commission, and salary payment schemes. Wage earners are compensated on a per-hour basis, whereas salaried workers are compensated on a per-week or per-job basis, and commission workers get paid according to how much they produce or sell.		Under most circumstances, wage earners and lower-level employees may be legally required by an employer to work more than forty hours in a week; however, they are paid extra for the additional work. Many salaried workers and commission-paid sales staff are not covered by overtime laws. These are generally called "exempt" positions, because they are exempt from federal and state laws that mandate extra pay for extra time worked.[65] The rules are complex, but generally exempt workers are executives, professionals, or sales staff.[66] For example, school teachers are not paid extra for working extra hours. Business owners and independent contractors are considered self-employed, and none of these laws apply to them.		Generally, workers are paid time-and-a-half, or 1.5 times the worker's base wage, for each hour of work past forty. California also applies this rule to work in excess of eight hours per day,[67] but exemptions[68] and exceptions[69] significantly limit the applicability of this law.		In some states, firms are required to pay double-time, or twice the base rate, for each hour of work past 60, or each hour of work past 12 in one day in California, also subject to numerous exemptions and exceptions.[67] This provides an incentive for companies to limit working time, but makes these additional hours more desirable for the worker. It is not uncommon for overtime hours to be accepted voluntarily by wage-earning workers. Unions often treat overtime as a desirable commodity when negotiating how these opportunities shall be partitioned among union members.		The work time in Brazil is 44 hours per week, usually 8 hours per day and 4 hours on Saturday or 8.8 hours per day. On duty/no meal break jobs are 6 hours per day. Public servants work 40 hours a week.		It is worth noting that in Brazil meal time is not usually counted as work. There is a 1-hour break for lunch and work schedule is typically 8h00 or 9h00-noon, 13h00 -18h00. In larger cities people have lunch meal on/near the work site, while in smaller cities a sizable fraction of the employees might go home for lunch.		A 30-day vacation is mandatory by law and there are about 13 to 15 holidays a year, depending on the municipality.		China adopted a 40-hour week, eliminating half-day work on Saturdays (though this was not widely practiced initially). [70]		Traditionally, Chinese have worked long hours, and this has led to many deaths from overwork, with the state media reporting in 2014 that 600,000 people were dying annually from overwork. Despite this, work hours have reportedly been falling for about three decades due to rising productivity, better labor laws, and the spread of the two-day weekend. The trend has affected both factories and white-collar companies that have been responding to growing demands for easier work schedules.[71][72]		Hong Kong has no legislation regarding maximum and normal working hours. The average weekly working hours of full-time employees in Hong Kong is 49 hours.[73] According to the Price and Earnings Report 2012 conducted by UBS, while the global and regional average were 1,915 and 2,154 hours per year respectively, the average working hours in Hong Kong is 2,296 hours per year, which ranked the fifth longest yearly working hours among 72 countries under study.[74] In addition, from the survey conducted by the Public Opinion Study Group of the University of Hong Kong, 79% of the respondents agree that the problem of overtime work in Hong Kong is “severe”, and 65% of the respondents support the legislation on the maximum working hours.[75] In Hong Kong, 70% of surveyed do not receive any overtime remuneration.[76] These show that people in Hong Kong concerns the working time issues. As Hong Kong implemented the minimum wage law in May 2011, the Chief Executive, Donald Tsang, of the Special Administrative Region pledged that the government will standardize working hours in Hong Kong.[77]		On 26 November 2012, the Labour Department of the HKSAR released the "Report of the policy study on standard working hours". The report covers three major areas, including: (1) the regimes and experience of other places in regulating working hours, (2) latest working time situations of employees in different sectors, and (3) estimation of the possible impact of introducing standard working hour in Hong Kong.[78] Under the selected parameters, from most loosen to most stringent, the estimated increase in labour cost vary from 1.1 billion to 55 billion HKD, and affect 957,100 (36.7% of total employees) to 2,378,900 (91.1%of total) employees.[73]		Various sectors of the community show concerns about the standard working hours in Hong Kong. The points are summarized as below:		Hong Kong Catholic Commission For Labour Affairs urges the government to legislate the standard working hours in Hong Kong, and suggests a 44 hours standard, 54 hours maximum working hours in a week. The organization thinks that long working time adversely affects the family and social life and health of employees; it also indicates that the current Employment Ordinance does not regulate overtime pays, working time limits nor rest day pays, which can protect employees rights.		Generally, business sector agrees that it is important to achieve work-life balance, but does not support a legislation to regulate working hours limit. They believe "standard working hours" is not the best way to achieve work-life balance and the root cause of the long working hours in Hong Kong is due to insufficient labor supply. The Managing Director of Century Environmental Services Group, Catherine Yan, said Employees may want to work more to obtain a higher salary due to financial reasons. If standard working hour legislation is passed, employers will need to pay a higher salary to employees, and hence the employers may choose to segment work tasks to employer more part time employees instead of providing overtime pay to employees. She thinks this will lead to a situation that the employees may need to find two part-time jobs to earn their living, making them wasting more time on transportation from one job to another.[79]		The Chairman of the Hong Kong General Chamber of Commerce, Chow Chung-kong believes that it is so difficult to implement standard working hours that apply “across-the-board”, specifically, to accountants and barristers.[80] In addition, he believes that standard working hours may decrease individual employees' working hours and would not increase their actual income. It may also lead to an increase of number of part-timers in the labor market.		According to a study conducted jointly by the Business, Economic and Public Affairs Research Centre and Enterprise and Social Development Research Centre of Hong Kong Shue Yan University, 16% surveyed companies believe that a standard working hours policy can be considered, and 55% surveyed think that it would be difficult to implement standard working hours in businesses.[81]		Employer representative in the Labour Advisory Board, Stanley Lau, said that standard working hours will completely alter the business environment of Hong Kong, affect small and medium enterprise and weaken competitiveness of businesses. He believes that the government can encourage employers to pay overtime salary, and there is no need to regulate standard working hours.[82]		On 17–18 October 2012, the Legislative Council members in Hong Kong debated on the motion "legislation for the regulation of working hours". Cheung Kwok-che proposed the motion "That is the Council urges the Government to introduce a bill on the regulation of working hours within this legislative session, the contents of which must include the number of standard weekly hours and overtime pay".[83] As the motion was not passed by both functional constituencies and geographical constituencies, it was negatived.[84]		The Hong Kong Federation of Trade Unions suggested a standard 44-hour work week with overtime pay of 1.5 times the usual pay. It believes the regulation of standard working hour can prevent the employers to force employees to work (overtime) without pay.[85]		Elizabeth Quat of Democratic Alliance for the Betterment and Progress of Hong Kong (DAB), believed that standard working hour was a labor policy and was not related to family-friendly policies. Vice President of Young DAB, Wai-hung Chan, stated that standard working hour would bring limitations to small and medium enterprises. He thought that the government should discuss the topic with public more before legislating the standard working hour.		The Democratic Party suggested a 44-hour standard work week and compulsory overtime pay to help achieve the balance between work, rest and entertainment of people in Hong Kong.[86]		The Labour Party believed regulating working hours such as standard working hour can help achieving the work-life balance.[87] It suggests an 8-hour work day, a 44-hour standard work week, a 60-hour maximum work week and an overtime pay of 1.5 times the usual pay.[76]		Poon Siu-ping of Federation of Hong Kong and Kowloon Labour Unions thought that it is possible to set work hour limit for all industries; and the regulation on working hours can ensure the overtime payment by employers to employees, and protect employees’ health.		The Civic party suggests "to actively study setting weekly standard working hours at 44 hours to align with family-friendly policies" in LegCo Election 2012.[88]		Member of Economic Synergy, Jeffery Lam, believes that standard working hour would adversely affect productivity, tense the employer-employee relationship, and increase the pressure faced by business who suffer from inadequate workers. He does not support the regulation on working hours at the current situation.[89]		Matthew Cheung Kin-chung, the Secretary for Labour and Welfare Bureau, said the Executive Council has already received the government report on working hours in June, and the Labour Advisory Board and the LegCo’s Manpower Panel will receive the report in late November and December respectively.[90] On 26 November 2012, the Labour Department released the report, and the report covered the regimes and experience of practicing standard working hours in selected regions, current work hour situations in different industries, and the impact assessment of standard working hours. Also, Matthew Cheung mentioned that the government will form a select committee by first quarter of 2013, which will include government officials, representative of labor unions and employers’ associations, academics and community leaders, to investigate the related issues. He also said that it would "perhaps be unrealistic" to put forward a bill for standard working hours in the next one to two years.[91]		Yip Siu-fai, Professor of the Department of Social Work and Social Administration of HKU, said that professions such as nurse, accountants, have a long work time, and this may affect their social life. He believes standard working hour can help making Hong Kong to become a family-friendly workplace and increase fertility rate. Randy Chiu, Professor of Department of Management of HKBU, said that standard working hour could avoid excessively long working hours of employees.[92] He also said that nowadays Hong Kong attains almost full employment, has a high rental price and severe inflation, recently implemented minimum wage, and affected by gloomy global economy; he also mentioned that comprehensive considerations on macroeconomic situations are needed, and pushed to the awareness that it is perhaps inappropriate to adopt working time regulation examples in other countries to Hong Kong.[93]		Lee Shu-Kam, Associate Professor of the Department of Economics and Finance of HKSYU, believed standard working hours cannot achieve ‘work-life balance’. He referenced the research to the US by the University of California, Los Angeles in 1999 and pointed out that in the industries and regions which the wage elasticity is low, the effects of standard working hours on lowering actual working time and increasing wage is limited: for regions where labor supply is inadequate, standard working hours can protect employees’ benefit yet cause unemployment; but for the regions, such as Japan, where the problem does not exist, standard working hour would only lead to unemployment.[94] In addition, he said the effect of standard working hour is similar to that of giving overtime pay, say, making employees to favor overtime work more. In this sense, standard working hour does not match its principle: to shorten work time and increase recreation time of employees.[95] He believed that the key point is to help employees to achieve work-life balance and get a win-win situation of employers and employees.		Francis Lui, Head and Professor of the Department of Economics of Hong Kong University of Science and Technology, believed that the standard working hour may not lower work time but increase unemployment. He used Japan as an example to illustrate that the implementation of standard working hours lowered productivity per head and demotivated the economy. He also said that even if the standard working hours can shorten employees’ weekly working hours, they may need to work for more years to earn sufficient amount of money for retirement, i.e. delay their retirement age. The working time in the whole life may not change.[96]		Lok-sang Ho, Professor of Economics and Director of the Centre for Public Policy Studies of Lingnan University, pointed out that “as different employees perform various jobs and under different degrees of pressures, it may not appropriate to establish standard working hours in Hong Kong; and he proposed 50-hour maximum work week to protect workers health.[97]		Singapore enacts an 8-hour normal work day, a 44-hour normal working week, and a maximum 48-hour work week. It is to note that if the employee works no more than five days a week, the employee’s normal working day is 9-hour and the working week is 44 hours. Also, if the number of hours worked of the worker is less than 44 hours every alternate week, the 44-hour weekly limit may be exceeded in the other week. Yet, this is subjected to the pre-specification in the service contract and the maximum should not exceed 48 hours per week or 88 hours in any consecutive two week time. In addition, a shift worker can work up to 12 hours a day, provided that the average working hours per week do not exceed 44 over a consecutive 3-week time. The overtime allowance per overtime hour must not be less than 1.5 times of the employee’s hour basic rates.[98]		The Kapauku people of Papua think it is bad luck to work two consecutive days. The !Kung Bushmen work just two-and-a-half days per week, rarely more than six hours per day.[99]		The work week in Samoa is approximately 30 hours,[100] and although average annual Samoan cash income is relatively low, by some measures, the Samoan standard of living is quite good.		In India, particularly in smaller companies, someone generally works for 11 hours a day and 6 days a week. No Overtime is paid for extra time. Law enforcement is negligible in regulating the working hours. A typical office will open at 09:00 or 09:30 and officially end the work day at about 19:00. However, many workers and especially managers will stay later in the office due to additional work load. However, large Indian companies and MNC offices located in India tend to follow a 5-day, 8- to 9-hour per day working schedule. The Government of India in some of its offices also follows a 5-day week schedule.[citation needed]		Nigeria has public servants that work 35 hours per week.[citation needed]		Many modern workplaces are experimenting with accommodating changes in the workforce and the basic structure of scheduled work. Flextime allows office workers to shift their working time away from rush-hour traffic; for example, arriving at 10:00 am and leaving at 6:00 pm. Telecommuting permits employees to work from their homes or in satellite locations (not owned by the employer), eliminating or reducing long commute times in heavily populated areas. Zero-hour contracts establish work contracts without minimum-hour guarantees; workers are paid only for the hours they work.		
Volunteering is generally considered an altruistic activity where an individual or group provides services for no financial gain "to benefit another person, group or organization".[1] Volunteering is also renowned for skill development and is often intended to promote goodness or to improve human quality of life. Volunteering may have positive benefits for the volunteer as well as for the person or community served.[2] It is also intended to make contacts for possible employment. Many volunteers are specifically trained in the areas they work, such as medicine, education, or emergency rescue. Others serve on an as-needed basis, such as in response to a natural disaster.						The verb was first recorded in 1755. It was derived from the noun volunteer, in C.1600, "one who offers himself for military service," from the Middle French voluntaire.[3] In the non-military sense, the word was first recorded during the 1630s. The word volunteering has more recent usage—still predominantly military—coinciding with the phrase community service.[3][4] In a military context, a volunteer army is a military body whose soldiers chose to enter service, as opposed to having been conscripted. Such volunteers do not work "for free" and are given regular pay.		During this time, America experienced the Great Awakening. People became aware of the disadvantaged and realized the cause for movement against slavery. Younger people started helping the needy in their communities[citation needed]. In 1851, the first YMCA in the United States was started, followed seven years later by the first YWCA. During the American Civil War, women volunteered their time to sew supplies for the soldiers and the "Angel of the Battlefield" Clara Barton and a team of volunteers began providing aid to servicemen. Barton founded the American Red Cross in 1881 and began mobilizing volunteers for disaster relief operations,including relief for victims of the Johnstown Flood in 1889.		The Salvation Army is one of the oldest and largest organizations working for disadvantaged people. Though it is a charity organization, it has organized a number of volunteering programs since its inception.[5] Prior to the 19th century, few formal charitable organizations existed to assist people in need.		In the first few decades of the 20th century, several volunteer organizations were founded, including the Rotary International, Kiwanis International, Association of Junior Leagues International, and Lions Clubs International.		The Great Depression saw one of the first large-scale, nationwide efforts to coordinate volunteering for a specific need. During World War II, thousands of volunteer offices supervised the volunteers who helped with the many needs of the military and the home front, including collecting supplies, entertaining soldiers on leave, and caring for the injured.[5]		After World War II, people shifted the focus of their altruistic passions to other areas, including helping the poor and volunteering overseas. A major development was the Peace Corps in the United States in 1960. When President Lyndon B. Johnson declared a War on Poverty in 1964, volunteer opportunities started to expand and continued into the next few decades. The process for finding volunteer work became more formalized, with more volunteer centers forming and new ways to find work appearing on the World Wide Web.[5]		According to the Corporation for National and Community Service (in 2012), about 64.5 million Americans, or 26.5 percent of the adult population, gave 7.9 billion hours of volunteer service worth $175 billion. This calculates at about 125–150 hours per year or 3 hours per week at a rate of $22 per hour. Volunteer hours in the UK are similar; the data for other countries is unavailable.		In 1960, after the so called revolutionary war in Cuba ended, Ernesto Che Guevara created the concept of volunteering work. It was created with the intention that workers across the country volunteer a few hour of work on their work centers.		Many schools on all education levels offer service-learning programs, which allow students to serve the community through volunteering while earning educational credit.[6] According to Alexander Astin in the foreword to Where's the Learning in Service-Learning? by Janet Eyler and Dwight E. Giles, Jr.,"...we promote more wide-spread adoption of service-learning in higher education because we see it as a powerful means of preparing students to become more caring and responsible parents and citizens and of helping colleges and universities to make good on their pledge to 'serve society.'"[7] When describing service learning, the Medical Education at Harvard says, "Service learning unites academic study and volunteer community service in mutually reinforcing ways. ...service learning is characterized by a relationship of partnership: the student learns from the service agency and from the community and, in return,gives energy, intelligence, commitment, time and skills to address human and community needs."[6] Volunteering in service learning seems to have the result of engaging both mind and heart, thus providing a more powerful learning experience; according to Janet Eyler and Dwight E. Giles,it succeeds by the fact that it "...fosters student development by capturing student interest..."[7]:1–2,8 While not recognized by everyone as a legitimate approach, research on the efficacy of service learning has grown.[7]:xv-xvii Janet Eyler and Dwight E. Giles conducted a national study of American college students to ascertain the significance of service learning programs,[7]:xvi According to Eyler and Giles,"These surveys, conducted before and after a semester of community service, examine the impact of service-learning on students."[7]:xvi They describe their experience with students involved in service-learning in this way: "Students like service-learning. When we sit down with a group of students to discuss service-learning experiences, their enthusiasm is unmistakable. ...it is clear that [the students]believe that what they gain from service-learning differs qualitatively from what they often derive from more traditional instruction."[7]:1–2		Skills-based volunteering is leveraging the specialized skills and the talents of individuals to strengthen the infrastructure of nonprofits, helping them build and sustain their capacity to successfully achieve their missions.[8] This is in contrast to traditional volunteering, where specific training is not required.[citation needed] The average hour of traditional volunteering is valued by the Independent Sector at between $18–20 an hour.[9] Skills-based volunteering is valued at $40–500 an hour, depending on the market value of the time.[10][not in citation given]		An increasingly popular form of volunteering among young people, particularly gap year students and graduates, is to travel to communities in the developing world to work on projects with local organisations. Activities include teaching English, working in orphanages, conservation, assisting non-governmental organizations and medical work. International volunteering often aims to give participants valuable skills and knowledge in addition to benefits to the host community and organization.[11]		Also called e-volunteering or online volunteering, virtual volunteering is a volunteer who completes tasks, in whole or in part, offsite from the organization being assisted. They use the Internet and a home, school, telecenter or work computer, or other Internet-connected device, such as a PDA or smartphone. Virtual volunteering is also known as cyber service, telementoring, and teletutoring, as well as various other names. Virtual volunteering is similar to telecommuting, except that instead of online employees who are paid, these are online volunteers who are not paid.[12][13]		Micro-volunteering is a task performed via an internet-connected device. An individual typically does this task in small, un-paid increments of time. Micro-volunteering is distinct from "virtual volunteering" in that it typically does not require the individual volunteer to go through an application process, screening process, or training period.[14][15]		Environmental volunteering refers to the volunteers who contribute towards environmental management or conservation. Volunteers conduct a range of activities including environmental monitoring, ecological restoration such as re-vegetation and weed removal, protecting endangered animals, and educating others about the natural environment.[16]		Volunteering often plays a pivotal role in the recovery effort following natural disasters, such as tsunamis, floods, droughts, hurricanes, and earthquakes. For example, the 2004 Indian Ocean earthquake and tsunami attracted a large number of volunteers worldwide, deployed by non-governmental organizations, government agencies, and the United Nations.[17][18]		During the 2012 hurricane Sandy emergency, Occupy Sandy volunteers, formed a laterally organized rapid-response team that provided much needed help during and after the storm, from food to shelter to reconstruction. It is an example of mutualism at work, pooling resources and assistance and leveraging social media.		Resource poor schools around the world rely on government support or on efforts from volunteers and private donations, in order to run effectively. In some countries, whenever the economy is down, the need for volunteers and resources increases greatly.[19] There are many opportunities available in school systems for volunteers. Yet, there are not many requirements in order to volunteer in a school system. Whether one is a high school or TEFL (Teaching English as a Foreign Language) graduate or college student, most schools require just voluntary and selfless effort.[20]		Much like the benefits of any type of volunteering there are great rewards for the volunteer, student, and school. In addition to intangible rewards, volunteers can add relevant experience to their resumes. Volunteers who travel to assist may learn foreign culture and language.		Volunteering in schools can be an additional teaching guide for the students and help to fill the gap of local teachers. Cultural and language exchange during teaching and other school activities can be the most essential learning experience for both students and volunteers.[20]		CSR Diary is one such platform where all the community Services efforts of "student" are recognised with Volunteering Certificates (Community Services Certificates) without any charge. Community Services can be done for any cause, any purpose, any location: www.csrdiary.com		Benefacto, a volunteering brokerage, describe corporate volunteering as "Companies giving their employees an allowance of paid time off annually, which they use to volunteer at a charity of their choice."[21]		A majority of the companies at the Fortune 500 allow their employees to volunteer during work hours. These formalized Employee Volunteering Programs (EVPs), also called Employer Supported Volunteering (ESV), are regarded as a part of the companies' sustainability efforts and their social responsibility activities.[22] About 40% of Fortune 500 companies provide monetary donations, also known as volunteer grants, to nonprofits as a way to recognize employees who dedicate significant amounts of time to volunteering in the community.[23]		According to the information from VolunteerMatch, a service that provides Employee Volunteering Program solutions, the key drivers for companies that produce and manage EVPs are building brand awareness and affinity, strengthening trust and loyalty among consumers, enhancing corporate image and reputation, improving employee retention, increasing employee productivity and loyalty, and providing an effective vehicle to reach strategic goals.[24]		Community volunteering refers globally to those who work to improve their local community. This activity commonly occurs through not for profit organizations, local governments and churches; but also encompasses ad-hoc or informal groups such as recreational sports teams.[25]		There are many proven personal benefits of community volunteerism. Working together with a group of people who have different ethnicity, backgrounds, and views reduces stereotypes. Community volunteerism has also been proven to improve student's academic success.		According to Where's the Learning in Service Learning? by Janet Eyler and Dwight E. Giles, immersing oneself into service learning and serving others has many positive effects both academic and personal. Not only does surrounding oneself with new people and learning how to work together as a group help one improve teamwork and relational skills, it reduces stereotypes, increases appreciation of other cultures, and works to allow young people to find others that they relate to.		Eyler and Giles noted that at the beginning and end of a college semester that included three hours of community service a week, students reported a much higher regard for cultural differences. At the end of the semester those who had participated in service-learning were noted as saying that the most important things that they had learned were not to judge others, and to appreciate every type of person because everyone shares some similar key characteristics.		Community volunteer work has proven to be a powerful predictor in students' academic lives and college experience as a whole. Studies have shown that students who participate in community service as a part of their college course of study have a much higher correlation of completing their degree (Astin, 1992;[26] Pascarella and Terenzini, 1991[27]). In addition, college students who participate in community volunteer projects as a part of their college experience report finding a much greater relevance in their academic studies after completing community volunteer projects.[28]		In some European countries government organisations and non-government organisations provide auxiliary positions for a certain period in institutions like hospitals, schools, memorial sites and welfare institutions. The difference to other types of volunteering is that there are strict legal regulations, what organisation is allowed to engage volunteers and about the period a volunteer is allowed to work in a voluntary position. Due to that fact, the volunteer is getting a limited amount as a pocket money from the government. An organization having one of the biggest manpower in Europe is the German Federal volunteers service (Bundesfreiwilligendienst), that was founded in 2011, by having more than 35.000 federal volunteers in 2012.[29] A much older institution is the Voluntary social year (Freiwilliges Soziales Jahr) in Austria and Germany.[30][31]		Designated days, weeks and years observed by a country or as designated by the United Nations to encourage volunteering / community service		Modern societies share a common value of people helping each other; not only do volunteer acts assist others, but they also benefit the volunteering individual on a personal level.[32] Despite having similar objectives, tension can arise between volunteers and state-provided services. In order to curtail this tension, most countries develop policies and enact legislation to clarify the roles and relationships among governmental stakeholders and their voluntary counterparts; this regulation identifies and allocates the necessary legal, social, administrative, and financial support of each party. This is particularly necessary when some voluntary activities are seen as a challenge to the authority of the state(e.g., on 29 January 2001, President Bush cautioned that volunteer groups should supplement—not replace—government agencies’ work).[33]		Volunteering that benefits the state but challenges paid counterparts angers labor unions that represent those who are paid for their volunteer work; this is particularly seen in combination departments, such as volunteer fire departments.		Difficulties in the cross-national aid model of volunteering can arise when it is applied across national borders. The presence of volunteers who are sent from one state to another can be viewed as a breach of sovereignty and showing a lack of respect towards the national government of the proposed recipients. Thus, motivations are important when states negotiate offers to send aid and when these proposals are accepted, particularly if donors may postpone assistance or stop it altogether. Three types of conditionality have evolved:		Some international volunteer organizations define their primary mission as being altruistic: to fight poverty and improve the living standards of people in the developing world, (e.g. Voluntary Services Overseas has almost 2,000 skilled professionals working as volunteers to pass on their expertise to local people so that the volunteers' skills remain long after they return home). When these organizations work in partnership with governments, the results can be impressive. However, when other organizations or individual First World governments support the work of volunteer groups, there can be questions as to whether the organizations' or governments' real motives are poverty alleviation. Instead, a focus on creating wealth for some of the poor or developing policies intended to benefit the donor states is sometimes reported.[34] Many low-income countries’ economies suffer from industrialization without prosperity and investment without growth. One reason for this is that development assistance guides many Third World governments to pursue development policies that have been wasteful, ill-conceived, or unproductive; some of these policies have been so destructive that the economies could not have been sustained without outside support.[35]		Indeed, some offers of aid have distorted the general spirit of volunteering, treating local voluntary action as contributions in kind, i.e., existing conditions requiring the modification of local people’s behavior in order for them to earn the right to donors’ charity. This can be seen as patronizing and offensive to the recipients because the aid expressly serves the policy aims of the donors rather than the needs of the recipients.		Based on a case study in China, Xu and Ngai (2011) revealed that the developing grassroots volunteerism can be an enclave among various organizations and may be able to work toward the development of civil society in the developing countries. The researchers developed a "Moral Resources and Political Capital" approach to examine the contributions of volunteerism in promoting the civil society. Moral resource means the available morals could be chosen by NGOs. Political capital means the capital that will improve or enhance the NGOs’ status, possession or access in the existing political system.[36]		Moreover, Xu and Ngai (2011) distinguished two types of Moral Resources: Moral Resource-I and Moral Resource-II (ibid).		Thanks to the intellectual heritage of Blau and Duncan (1967), two types of political capital were identified:		Obviously, "Moral resource-I itself contains the self-determination that gives participants confidence in the ethical beliefs they have chosen",[40] almost any organizations may have Moral Resource-I, while not all of them have the societal recognized Moral Resource-II. However, the voluntary service organizations predominantly occupy Moral Resource-II because a sense of moral superiority makes it possible that for parties with different values, goals and cultures to work together in promoting the promotion of volunteering. Thus the voluntary service organizations are likely to win the trust and support of the masses as well as the government more easily than will the organizations whose morals are not accepted by mainstream society. In other words, Moral Resource II helps the grassroots organizations with little Political Capital I to win Political Capital-II, which is a crucial factor for their survival and growth in developing countries such as China. Therefore, the voluntary service realm could be an enclave of the development of civil society in the developing nations.[41]		Volunteering has the ability to improve the quality of life and health of those who donate their time and research has found that older adults will benefit the most from volunteering. Physical and mental ailments plaguing older adults can be healed through the simple act of helping others; however, one must be performing the good deed from a selfless nature. There are barriers that can prevent older adults from participating in volunteer work, such as socio-economic status, opinions held by others, and even current health issues. However, these barriers can be overcome so that if one would like to be involved in volunteer work they can do so. Volunteering improves not only the communities in which one serves, but also the life of the individual who is providing help to the community.		In the United States, statistics on volunteering have historically been limited.[42] In 2013, the U.S. Current Population Survey (US) included a volunteering supplement which produced statistics on volunteering.[43]		In the 1960s, Ivan Illich offered an analysis of the role of American volunteers in Mexico in his speech entitled "To Hell With Good Intentions". His concerns, along with those of critics such as Paulo Freire and Edward Said, revolve around the notion of altruism as an extension of Christian missionary ideology. In addition, he mentions the sense of responsibility/obligation as a factor, which drives the concept of noblesse oblige—first developed by the French aristocracy as a moral duty derived from their wealth. Simply stated, these apprehensions propose the extension of power and authority over indigenous cultures around the world. Recent critiques of volunteering come from Westmier and Kahn (1996) and bell hooks (née Gloria Watkins) (2004). Also, Georgeou (2012) has critiqued the impact of neoliberalism on international aid volunteering.		The field of the medical tourism (referring to volunteers who travel overseas to deliver medical care) has recently attracted negative criticism when compared to the alternative notion of sustainable capacities, i.e., work done in the context of long-term, locally-run, and foreign-supported infrastructures. A preponderance of this criticism appears largely in scientific and peer-reviewed literature.[44][45][46] Recently, media outlets with more general readerships have published such criticisms as well.[47]		
The National Average Salary (or the National Average Wage) is the mean salary for the working population of a nation. It is calculated by summing all the annual salaries of all persons in work and dividing the total by the number of workers. It is not the same as the Gross domestic product (GDP) per capita, which is calculated by dividing the GDP by the total population of a country, including the unemployed and those not in the workforce (e.g. retired people, children, students, etc.).				
Debt bondage, also known as debt slavery or bonded labour, is a person's pledge of labour or services as security for the repayment for a debt or other obligation.[1] The services required to repay the debt may be undefined, and the services' duration may be undefined.[2] Debt bondage can be passed on from generation to generation.[2]		Currently, debt bondage is the most common method of enslavement with an estimated 8.1 million people bonded to labour illegally as cited by the International Labour Organization in 2005.[3] Debt bondage has been described by the United Nations as a form of "modern day slavery" and the Supplementary Convention on the Abolition of Slavery seeks to abolish the practice.[2][4][5] Though most countries in South Asia and Sub-Saharan Africa are parties to the Convention, the practice is still prevalent primarily in these regions. It is predicted that 84 to 88% of the bonded labourers in the world are in South Asia.[4][6] Lack of prosecution or insufficient punishment of this crime are the leading causes of the practice as it exists at this scale today.[6][7]						Though the Forced Labour Convention of 1930 by the International Labour Organization, which included 187 parties, sought to bring organized attention to eradicating slavery through forms of forced labor, formal opposition to debt bondage in particular came at the Supplementary Convention on the Abolition of Slavery in 1956.[1][2] The convention in 1956[2] defined debt bondage under Article 1, section (a):		"Debt bondage, that is to say, the status or condition arising from a pledge by a debtor of his personal services or of those of a person under his control as security for a debt if the value of those services as reasonably assessed is not applied towards the liquidation of the debt or the length and nature of those services are not respectively limited and defined;"[2]		When a pledge to provide services to pay off debt is made by an individual, the employer often illegally inflates interest rates at an unreasonable amount, making it impossible for the individual to leave bonded labor.[8] When the bonded laborer dies, debts are often passed on to children.[8][9]		Although debt bondage, forced labour, and human trafficking are all defined as forms or variations of slavery, each term is distinct.[1][10][11] Debt bondage differs from forced labour and human trafficking in that a person consciously pledges to work as a means of repayment of debt without being placed into labor against will.[1][10]		Debt bondage only applies to individuals who have no hopes of leaving the labor due to inability to ever pay debt back.[1][8] Those who offer their services to repay a debt and the employer reduces the debt accordingly are not in debt bondage.[1][8]		In the 19th century, people in Asia were bonded to labor due to a variety of reasons ranging from farmers mortgaging harvests to drug addicts in need for opium in China.[12] When a natural disaster occurred or food was scarce, people willingly chose debt bondage as a means to a secure life.[12] In the early 20th century in Asia, most laborers tied to debt bondage had been born into it.[12] In certain regions, such as in Burma, debt bondage was far more common than slavery.[12] Many went into bondage to pay off interest on a loan or to pay taxes,[13] and as they worked, often on farms, lodging, meals, and clothing fees were added to the existing debt causing overall debt and interest to increase. These continued added loan values made leaving servitude unattainable.[12]		Moreover, after the development of the international economy, more workers were needed for the pre-industrial economies of Asia during the 19th century.[12] A greater demand for labor was needed in Asia to power exports to growing industrial countries like the United States and Germany.[12] Cultivation of cash crops like coffee, cocoa, and sugar and exploitation of minerals like gold and tin led farm owners to search for individuals in need of loans for the sake of keeping laborers permanently.[14] In particular, the Indian indenture system was based on debt bondage by which an estimated two million Indians were transported to various colonies of European powers to provide labor for plantations.[12] It started from the end of slavery in 1833 and continued until 1920.[12]		Important to both East and West Africa, pawnship, defined by Wilks as "the use of people in transferring their rights for settlement of debt," was common during the 17th century.[15] The system of pawnship occurred simultaneously with the slave trade in Africa.[16] Though the export of slaves from Africa to the Americas is often analyzed, slavery was rampant internally as well.[15] Development of plantations like those in Zanzibar in East Africa reflected the need for internal slaves.[12][15] Furthermore, many of the slaves that were exported were male as brutal and labor-intensive conditions favored the male body build.[12] This created gender implications for individuals in the pawnship system as more women were pawned than men and often sexually exploited within the country.[12]		After the abolition of slavery in many countries in the 19th century, Europeans still needed laborers.[16] Moreover, conditions for emancipated slaves were harsh.[12][16] Discrimination was rampant within the labor market, making attainment of a sustainable income for former slaves tough.[12] Because of these conditions, many freed slaves preferred to live through slavery-like contracts with their masters in a manner parallel to debt bondage.[16]		Debt bondage was "quite normal" in classical antiquity.[17] The poor or those who had fallen irredeemably in debt might place themselves into bondage "voluntarily"—or more precisely, might be compelled by circumstances to choose debt bondage as a way to anticipate and avoid worse terms that their creditors might impose on them.[18] In the Greco-Roman world, debt bondage was a distinct legal category into which a free person might fall, in theory temporarily, distinguished from the pervasive practice of slavery, which included enslavement as a result of defaulting on debt. Many forms of debt bondage existed in both ancient Greece and ancient Rome.[19]		Debt bondage was widespread in ancient Greece. The only city-state known to have abolished it is Athens, as early as the Archaic period under the debt reform legislation of Solon.[20] Both enslavement for debt and debt bondage were practiced in Ptolemaic Egypt.[21] By the Hellenistic period, the limited evidence indicates that debt bondage had replaced outright enslavement for debt.[21]		The most onerous debt bondage was various forms of paramonē, "indentured labor." As a matter of law, a person subjected to paramonē was categorically free, and not a slave, but in practice his freedom was severely constrained by his servitude.[22] Solon's reforms occurred in the context of democratic politics at Athens that required clearer distinctions between "free" and "slave"; as a perverse consequence, chattel slavery increased.[23]		The selling of one's own child into slavery is likely in most cases to have resulted from extreme poverty or debt, but strictly speaking is a form of chattel slavery, not debt bondage. The exact legal circumstances in Greece, however, are far more poorly documented than in ancient Rome.[22]		Nexum was a debt bondage contract in the early Roman Republic. Within the Roman legal system, it was a form of mancipatio. Though the terms of the contract would vary, essentially a free man pledged himself as a bond slave (nexus) as surety for a loan. He might also hand over his son as collateral. Although the bondsman might be subjected to humiliation and abuse, as a legal citizen he was supposed to be exempt from corporal punishment. Nexum was abolished by the Lex Poetelia Papiria in 326 BC, in part to prevent abuses to the physical integrity of citizens who had fallen into debt bondage.		Roman historians illuminated the abolition of nexum with a traditional story that varied in its particulars; basically, a nexus who was a handsome but upstanding youth suffered sexual harassment by the holder of the debt. In one version, the youth had gone into debt to pay for his father's funeral; in others, he had been handed over by his father. In all versions, he is presented as a model of virtue. Historical or not, the cautionary tale highlighted the incongruities of subjecting one free citizen to another's use, and the legal response was aimed at establishing the citizen's right to liberty (libertas), as distinguished from the slave or social outcast.[24]		Cicero considered the abolition of nexum primarily a political maneuver to appease the common people (plebs): the law was passed during the Conflict of the Orders, when plebeians were struggling to establish their rights in relation to the hereditary privileges of the patricians. Although nexum was abolished as a way to secure a loan, debt bondage might still result after a debtor defaulted.[24]		While serfdom under feudalism was the predominant political and economic system in Europe in the High Middle Ages, persisting in the Austrian Empire till 1848 and the Russian Empire until 1861 (details),[25] debt bondage (and slavery) provided other forms of unfree labour.		Though the figures differ from those of the International Labour Organization, researcher Siddharth Kara has calculated the number of slaves in the world by type, and determined that at the end of 2011 there were 18 to 20.5 million bonded laborers.[6] Bonded laborers work in industries today that produce goods including but not limited to frozen shrimp, bricks, tea, coffee, diamonds, marble, and apparel.[6]		Although India, Pakistan, and Bangladesh all have laws prohibiting debt bondage, it is estimated by Kara that 84 to 88% of the bonded laborers in the world are in South Asia.[6] Figures by the Human Rights Watch in 1999 are drastically higher estimating 40 million workers, composed mainly of children, are tied to labor through debt bondage in India alone.[28][29]		Research by Kara estimates there to be between 55,000 and 65,000 brick kilns in South Asia with 70% of them in India.[6] Other research estimates 6,000 kilns in Pakistan alone.[30] Total revenue from brick kilns in South Asia is estimated by Kara to be $13.3 to $15.2 billion.[6] Many of the brick kiln workers are migrants and travel between brick kiln locations every few months.[6][30] Kiln workers often live in extreme poverty and many began work at kilns through repayment of a starting loan averaging $150 to $200.[6] Kiln owners offer laborers "friendly loans" to avoid being criminalized in breaking bonded labor laws.[30] Bonded brick kiln laborers, including children, work in harsh and unsafe conditions as the heat from the kiln may cause heat stroke and a number of other medical conditions.[30][31] Although these laborers do have the option to default on loans, there is fear of death and violence by brick kiln owners if they choose to do so.[30]		An essential grain to the South Asian diet, rice is harvested throughout India and Nepal in particular.[11][12] In India, more than 20% of agricultural land is used to grow rice.[12] Rice mill owners often employ workers who live in harsh conditions on farms.[12] Workers receive such low wages that they must borrow money from their employers causing them to be tied to the rice mill through debt.[12] For example, in India, the average pay rate per day was $0.55 American dollars as recorded in 2006.[12] Though some workers may be able to survive minimally from their compensation, uncontrollable life events such as an illness require loans.[12][32] Families, including children, work day and night to prepare the rice for export by boiling it, drying it in the sun, and sifting through it for purification.[12] Furthermore, families who live on rice mill production sites are often excluded from access to hospitals and schools.[12]		Though there are not reliable estimates of bonded laborers in Sub-Saharan Africa to date from credible sources, the Global Slavery Index estimates the total number of those enslaved in this region is 6.25 million.[33] In countries like Ghana, it is estimated that 85% of people enslaved are tied to labor.[33] Additionally, this region includes Mauritania, the country with the highest proportion of slavery in the world as an estimated 20% of its population is enslaved through methods like debt bondage.[33]		The Environmental Justice Foundation found human rights violations in the fisheries on the coasts of South and West Africa including labor exploitation.[34] Exporter fish companies drive smaller businesses and individuals to lower profits, causing bankruptcy.[34] In many cases, recruitment to these companies occurs by luring small business owners and migrant workers through debt bondage.[34] In recruiting individual fishers, fees are sometimes charged by a broker to use ports which opens the debt cycle.[34]		After countries began to formally abolish slavery, unemployment was rampant for blacks in South Africa and Nigeria pushing black women to work as domestic workers.[13][35] Currently, estimates from the International Labour Organization state that between 800,000 and 1.1 million domestic workers are in South Africa.[36] Many of these domestic servants become bonded to labor in a process similar to other industries in Asia.[35] The wages given to servants are often so poor that loans are taken when servants are in need of more money, making it impossible to escape.[35] The hours of working for domestic servants are unpredictable, and because many servants are women, their young children are often left under the care of older children or other family members.[13][35] Moreover, these women can work up to the age of 75 and their daughters are likely to be servants in the same households.[35]		Compulsory indebtedness is common for arresting girls in forced prostitution. Especially after being transported to another region, where they are forced to work off their debt, often with 100 percent interest, and to pay every day much money for the room, food and other things, through what amounts to sexual servitude. In addition to debt bondage, the women and girls face a wide range of abuses, including illegal confinement; forced labor; rape; physical abuse; and more.[37] Their chiefs present an account, which leaves the girl with debts between 10 thousand and 90 thousand dollars because of the expensive travel, documents, health check, and other abusive taxes. Many forced prostitutes do not receive any money for years, and it is not rare, that a woman has to work for 30 years, 10 to 18 hours every day, and is leaving the brothel without any money, becoming a beggar in the streets or requiring social assistance, while the profit of her "owners" can reach millions of dollars. Those enslaved prostitutes are becoming sexual slaves, submissive and exploited without pity, and in some cases they are branded with tattoo or hot iron as property by their "owners".[38]		The International Labour Organization (ILO) estimates that $51.2 billion is made annually in the exploitation of workers through debt bondage.[39] Though the employers actively take part in accruing the debt of laborers, buyers of products and services in the country of manufacturing and abroad also contribute to the profitability of this practice.[6]		In many of the industries in which debt bondage is common like brick kilns or fisheries, entire families are often involved in paying of the debt of one individual, including children.[6][32] These children generally do not have access to education thus making it impossible to get out of poverty.[40] Moreover, if a relative who still is in debt dies, the bondage is passed on to another family member, usually the children.[40] At the International Labour Organization Convention, this cycle was labeled as the "Worst Forms of Child Labor."[40] Researchers like Basu and Chau link the occurrence of child labor through debt bondage with factors like labor rights and the stage of development of an economy.[40] Although minimum age labor laws are present in many regions with child debt bondage, the laws are not enforced especially with regard to the agrarian economy.[40]		Debt bondage has been described by the United Nations as a form of "modern day slavery"[5] and is prohibited by international law. It is specifically dealt with by article 1(a) of the United Nations 1956 Supplementary Convention on the Abolition of Slavery. It persists nonetheless especially in developing countries, which have few mechanisms for credit security or bankruptcy, and where fewer people hold formal title to land or possessions. According to some economists, like Hernando de Soto, this is a major barrier to development in these countries. For example, entrepreneurs do not dare to take risks and cannot get credit because they hold no collateral and may burden families for generations to come.		India was the first country to pass legislation directly prohibiting debt bondage through the Bonded Labor System (Abolition) Act, 1976.[6][41][42] Less than two decades later, Pakistan also passed a similar act in 1992 and Nepal passed the Kamaiya Labour (Prohibition) Act in 2002.[6] Despite the fact that these laws are in place, debt bondage in South Asia is still widespread.[6]		In India, the rise of Dalit activism, government legislation starting as early as 1949,[43] as well as ongoing work by NGOs and government offices to enforce labour laws and rehabilitate those in debt, appears to have contributed to the reduction of bonded labour there. However, according to research papers presented by the International Labour Organization, there are still many obstacles to the eradication of bonded labour in India.[44][45]		In many of the countries like South Africa, Nigeria, Mauritania, and Ghana in which debt bondage is prevalent, there are not laws that either state direct prohibition or appropriate punishment. For example, South Africa passed the Basic Conditions of Employment Act of 1997 which prohibits forced labor but the punishment is up to 3 years of jail.[7] In addition, though many of the countries in Sub-Saharan Africa have laws that vaguely prohibit debt bondage, prosecution of such crimes rarely occurs.[7]		Contemporary:		Organisational Reports		
Networking is a socioeconomic business activity by which businesspeople and entrepreneurs meet to form business relationships and to recognize, create, or act upon business opportunities,[1] share information and seek potential partners for ventures.		In the second half of the twentieth century, the concept of networking was promoted to help businesspeople to build their social capital. In the US, workplace equity advocates encouraged business networking by members of marginalized groups (e.g., women, African-Americans, etc.) to identify and address the challenges barring them from professional success. Mainstream business literature subsequently adopted the terms and concepts, promoting them as pathways to success for all career climbers. Since the closing decades of the twentieth century, "networking" has become an accepted term and concept in American society.[citation needed] In the 2000s, "networking" has expanded beyond its roots as a business practice to the point that parents meeting to share child-rearing tips to scientists meeting research colleagues are described as engaging in "networking".[2][need quotation to verify]						A business network is a type of business social network which is developed to help businesspeople connect with other managers and entrepreneurs to further each other's business interests by forming mutually beneficial business relationships. There are several prominent business networking organizations that create models of business networking activity that, when followed, allow the business person to build new business relationships and generate business opportunities at the same time. A professional network service is an implementation of information technology in support of business networking. Chambers of Commerce and other business-oriented groups may also organize networking activities.		Many business people contend business networking is a more cost-effective method of generating new business than advertising or public relations efforts. This is because business networking is a low-cost activity that involves more personal commitment than company money. Country-specific examples of informal networking are guanxi in China, blat in Russia, good ol' boy network in America, and old boy network in the UK.		In the case of a formal business network, its members may agree to meet weekly or monthly or less frequently, with the purpose of sharing information, exchanging business leads and making referrals to fellow members. To complement this business activity, members often meet outside this circle, on their own time, and build their own one-to-one business relationship with fellow members.		Business networking can be conducted in a local business community, at a regional level (which typically happens less often because of the travel involved) or even at a national level or international level, in the form of conferences and other fora. In the 2000s, using the Internet and teleconferencing services, it is possible for businesspeople from a similar industry or sector to connect even if they live in different regions or countries. Business networking websites have grown in the 2000s. Some Internet businesses find or set up business "leads" to sell to bigger corporations and businesses looking for new clients.		Business networking can have a meaning in the ICT domain, i.e. the provision of operating support to businesses and organizations, and related value chains and value networks.		Before online business networking, there existed face-to-face networking for business. This was achieved through a number of techniques such as trade show marketing and loyalty programs. Though these techniques have been proven to still be an effective source of income, many companies now focus more on online marketing due to the ability to track every detail of a campaign and justify the expenditure involved in setting up one of these campaigns.[3] "Schmoozing" or "rubbing elbows" are expressions used among professional business professionals for introducing and meeting one another in a business context, and establishing business rapport.		Networking can be an effective way for job-seekers to gain a competitive edge over others in the job-market. The skilled networker cultivates personal relationships with prospective employers and selection panelists, in the hope that these personal affections will influence future hiring decisions. This form of networking has raised ethical concerns. The objection is that it constitutes an attempt to corrupt formal selection processes. The networker is accused of seeking non-meritocratic advantage over other candidates; advantage that is based on personal fondness rather than on any objective appraisal of which candidate is most qualified for the position.[4][5]		Many businesses use networking as a key factor in their marketing plan. It helps to develop a strong feeling of trust between those involved and play a big part in raising the profile of a company. Suppliers and businesses can be seen as networked businesses, and will tend to source the business and their suppliers through their existing relationships and those of the companies they work closely with. Networked businesses tend to be open, random, and supportive, whereas those relying on hierarchical, traditional managed approaches are closed, selective, and controlling. These phrases were first used by Thomas Power, businessman and chairman of Ecademy, an online business network, in 2009.[6]		
Employment is a relationship between two parties, usually based on a contract where work is paid for, where one party, which may be a corporation, for profit, not-for-profit organization, co-operative or other entity is the employer and the other is the employee.[1] Employees work in return for payment, which may be in the form of an hourly wage, by piecework or an annual salary, depending on the type of work an employee does or which sector she or he is working in. Employees in some fields or sectors may receive gratuities, bonus payment or stock options. In some types of employment, employees may receive benefits in addition to payment. Benefits can include health insurance, housing, disability insurance or use of a gym. Employment is typically governed by employment laws or regulations or legal contracts.						An employee contributes labor and expertise to an endeavor of an employer or of a person conducting a business or undertaking (PCBU)[2] and is usually hired to perform specific duties which are packaged into a job. In a corporate context, an employee is a person who is hired to provide services to a company on a regular basis in exchange for compensation and who does not provide these services as part of an independent business.[3]		Employer and managerial control within an organization rests at many levels and has important implications for staff and productivity alike, with control forming the fundamental link between desired outcomes and actual processes. Employers must balance interests such as decreasing wage constraints with a maximization of labor productivity in order to achieve a profitable and productive employment relationship.		The main ways for employers to find workers and for people to find employers are via jobs listings in newspapers (via classified advertising) and online, also called job boards. Employers and job seekers also often find each other via professional recruitment consultants which receive a commission from the employer to find, screen and select suitable candidates. However, a study has shown that such consultants may not be reliable when they fail to use established principles in selecting employees.[1] A more traditional approach is with a "Help Wanted" sign in the establishment (usually hung on a window or door[4] or placed on a store counter).[3] Evaluating different employees can be quite laborious but setting up different techniques to analyze their skill to measure their talents within the field can be best through assessments.[5] Employer and potential employee commonly take the additional step of getting to know each other through the process of job interview.		Training and development refers to the employer's effort to equip a newly hired employee with necessary skills to perform at the job, and to help the employee grow within the organization. An appropriate level of training and development helps to improve employee's job satisfaction.[6]		There are many ways that employees are paid, including by hourly wages, by piecework, by yearly salary, or by gratuities (with the latter often being combined with another form of payment). In sales jobs and real estate positions, the employee may be paid a commission, a percentage of the value of the goods or services that they have sold. In some fields and professions (e.g., executive jobs), employees may be eligible for a bonus if they meet certain targets. Some executives and employees may be paid in stocks or stock options, a compensation approach that has the added benefit, from the company's point of view, of helping to align the interests of the compensated individual with the performance of the company.		Employee benefits are various non-wage compensation provided to employee in addition to their wages or salaries. The benefits can include: housing (employer-provided or employer-paid), group insurance (health, dental, life etc.), disability income protection, retirement benefits, daycare, tuition reimbursement, sick leave, vacation (paid and non-paid), social security, profit sharing, funding of education, and other specialized benefits. In some cases, such as with workers employed in remote or isolated regions, the benefits may include meals. Employee benefits can improve the relationship between employee and employer and lowers staff turnover.[7]		Organizational justice is an employee's perception and judgement of employer's treatment in the context of fairness or justice. The resulting actions to influence the employee-employer relationship is also a part of organizational justice.[7]		Employees can organize into trade or labor unions, which represent the work force to collectively bargain with the management of organizations about working, and contractual conditions and services.[8]		Usually, either an employee or employer may end the relationship at any time, often subject to a certain notice period. This is referred to as at-will employment. The contract between the two parties specifies the responsibilities of each when ending the relationship and may include requirements such as notice periods, severance pay, and security measures.[8] In some professions, notably teaching, civil servants, university professors, and some orchestra jobs, some employees may have tenure, which means that they cannot be dismissed at will. Another type of termination is a layoff.		Wage labor is the socioeconomic relationship between a worker and an employer, where the worker sells their labor under a formal or informal employment contract. These transactions usually occur in a labor market where wages are market determined.[6][7] In exchange for the wages paid, the work product generally becomes the undifferentiated property of the employer, except for special cases such as the vesting of intellectual property patents in the United States where patent rights are usually vested in the original personal inventor. A wage laborer is a person whose primary means of income is from the selling of his or her labor in this way.[8]		In modern mixed economies such as that of the OECD countries, it is currently the dominant form of work arrangement. Although most work occurs following this structure, the wage work arrangements of CEOs, professional employees, and professional contract workers are sometimes conflated with class assignments, so that "wage labor" is considered to apply only to unskilled, semi-skilled or manual labor.[9]		Wage labor, as institutionalized under today's market economic systems, has been criticized,[8] especially by both mainstream socialists and anarcho-syndicalists,[9][10][11][12] using the pejorative term wage slavery.[13][14] Socialists draw parallels between the trade of labor as a commodity and slavery. Cicero is also known to have suggested such parallels.[15]		The American philosopher John Dewey posited that until "industrial feudalism" is replaced by "industrial democracy", politics will be "the shadow cast on society by big business".[16] Thomas Ferguson has postulated in his investment theory of party competition that the undemocratic nature of economic institutions under capitalism causes elections to become occasions when blocs of investors coalesce and compete to control the state.[17]		Australian employment has been governed by the Fair Work Act since 2009.[18]		Bangladesh Association of International Recruiting Agencies (BAIRA) is an association of national level with its international reputation of co-operation and welfare of the migrant workforce as well as its approximately 1200 members agencies in collaboration with and support from the Government of Bangladesh.[9]		In the Canadian province of Ontario, formal complaints can be brought to the Ministry of Labour. In the province of Quebec, grievances can be filed with the Commission des normes du travail.[12]		Pakistan has Contract Labor, Minimum Wage and Provident Funds Acts. Contract labor in Pakistan must be paid minimum wage and certain facilities are to be provided to labor. However, the Acts are not yet fully implemented.[9]		India has Contract Labor, Minimum Wage, Provident Funds Act and various other acts to comply with. Contract labor in India must be paid minimum wage and certain facilities are to be provided to labor. However, there is still a large amount of work that remains to be done to fully implement the Act.[12]		In the Philippines, private employment is regulated under the Labor Code of the Philippines by the Department of Labor and Employment.[19]		In the United Kingdom, employment contracts are categorized by the government into the following types:[20]		For purposes of U.S. federal income tax withholding, 26 U.S.C. § 3401(c) provides a definition for the term "employee" specific to chapter 24 of the Internal Revenue Code:		"For purposes of this chapter, the term “employee” includes an officer, employee, or elected official of the United States, a State, or any political subdivision thereof, or the District of Columbia, or any agency or instrumentality of any one or more of the foregoing. The term “employee” also includes an officer of a corporation."[21] This definition does not exclude all those who are commonly known as 'employees'. “Similarly, Latham’s instruction which indicated that under 26 U.S.C. § 3401(c) the category of ‘employee’ does not include privately employed wage earners is a preposterous reading of the statute. It is obvious that within the context of both statutes the word ‘includes’ is a term of enlargement not of limitation, and the reference to certain entities or categories is not intended to exclude all others.”[22]		Employees are often contrasted with independent contractors, especially when there is dispute as to the worker's entitlement to have matching taxes paid, workers compensation, and unemployment insurance benefits. However, in September 2009, the court case of Brown v. J. Kaz, Inc. ruled that independent contractors are regarded as employees for the purpose of discrimination laws if they work for the employer on a regular basis, and said employer directs the time, place, and manner of employment.[19]		In non-union work environments, in the United States, unjust termination complaints can be brought to the United States Department of Labor.[23]		Labor unions are legally recognized as representatives of workers in many industries in the United States. Their activity today centers on collective bargaining over wages, benefits, and working conditions for their membership, and on representing their members in disputes with management over violations of contract provisions. Larger unions also typically engage in lobbying activities and electioneering at the state and federal level.[19]		Most unions in America are aligned with one of two larger umbrella organizations: the AFL-CIO created in 1955, and the Change to Win Federation which split from the AFL-CIO in 2005. Both advocate policies and legislation on behalf of workers in the United States and Canada, and take an active role in politics. The AFL-CIO is especially concerned with global trade issues.[17]		According to Swedish law,[24] there are three types of employment.		There are no laws about minimum salary in Sweden. Instead there are agreements between employer organizations and trade unions about minimum salaries, and other employment conditions.		There is a type of employment contract which is common but not regulated in law, and that is Hour employment (swe: Timanställning), which can be Normal employment (unlimited), but the work time is unregulated and decided per immediate need basis. The employee is expected to be answering the phone and come to work when needed, e.g. when someone is ill and absent from work. They will receive salary only for actual work time and can in reality be fired for no reason by not being called anymore. This type of contract is common in the public sector.[25]		Young workers are at higher risk for occupational injury and face certain occupational hazards at a higher rate; this is generally due to their employment in high-risk industries. For example, in the United States, young people are injured at work at twice the rate of their older counterparts.[26] These workers are also at higher risk for motor vehicle accidents at work, due to less work experience, a lower use of seatbelts, and higher rates of distracted driving.[27][28] To mitigate this risk, those under the age of 17 are restricted from certain types of driving, including transporting people and goods under certain circumstances.[27]		High-risk industries for young workers include agriculture, restaurants, waste management, and mining.[26][27] In the United States, those under the age of 18 are restricted from certain jobs that are deemed dangerous under the Fair Labor Standards Act.[27]		Youth employment programs are most effective when they include both theoretical classroom training and hands-on training with work placements.[29]		Those older than the statutory defined retirement age may continue to work, either out of enjoyment or necessity. However, depending on the nature of the job, older workers may need to transition into less-physical forms of work to avoid injury. Working past retirement age also has positive effects, because it gives a sense of purpose and allows people to maintain social networks and activity levels.[30] Older workers are often found to be discriminated against by employers.[31]		Employment is no guarantee of escaping poverty, the International Labour Organization (ILO) estimates that as many as 40% of workers are poor, not earning enough to keep their families above the $2 a day poverty line.[25] For instance, in India most of the chronically poor are wage earners in formal employment, because their jobs are insecure and low paid and offer no chance to accumulate wealth to avoid risks.[25] According to the UNRISD, increasing labor productivity appears to have a negative impact on job creation: in the 1960s, a 1% increase in output per worker was associated with a reduction in employment growth of 0.07%, by the first decade of this century the same productivity increase implies reduced employment growth by 0.54%.[25] Both increased employment opportunities and increased labor productivity (as long as it also translates into higher wages) are needed to tackle poverty. Increases in employment without increases in productivity leads to a rise in the number of "working poor", which is why some experts are now promoting the creation of "quality" and not "quantity" in labor market policies.[25] This approach does highlight how higher productivity has helped reduce poverty in East Asia, but the negative impact is beginning to show.[25] In Vietnam, for example, employment growth has slowed while productivity growth has continued.[25] Furthermore, productivity increases do not always lead to increased wages, as can be seen in the United States, where the gap between productivity and wages has been rising since the 1980s.[25]		Researchers at the Overseas Development Institute argue that there are differences across economic sectors in creating employment that reduces poverty.[25] 24 instances of growth were examined, in which 18 reduced poverty. This study showed that other sectors were just as important in reducing unemployment, such as manufacturing.[25] The services sector is most effective at translating productivity growth into employment growth. Agriculture provides a safety net for jobs and economic buffer when other sectors are struggling.[25]		Scholars conceptualize the employment relationship in various ways.[32] A key assumption is the extent to which the employment relationship necessarily includes conflicts of interests between employers and employees, and the form of such conflicts.[33] In economic theorizing, the labor market mediates all such conflicts such that employers and employees who enter into an employment relationship are assumed to find this arrangement in their own self-interest. In human resource management theorizing, employers and employees are assumed to have shared interests (or a unity of interests, hence the label “unitarism”). Any conflicts that exist are seen as a manifestation of poor human resource management policies or interpersonal clashes such as personality conflicts, both of which can and should be managed away. From the perspective of pluralist industrial relations, the employment relationship is characterized by a plurality of stakeholders with legitimate interests (hence the label “pluralism), and some conflicts of interests are seen as inherent in the employment relationship (e.g., wages v. profits). Lastly, the critical paradigm emphasizes antagonistic conflicts of interests between various groups (e.g., the competing capitalist and working classes in a Marxist framework) that are part of a deeper social conflict of unequal power relations. As a result, there are four common models of employment:[34]		These models are important because they help reveal why individuals hold differing perspectives on human resource management policies, labor unions, and employment regulation.[35] For example, human resource management policies are seen as dictated by the market in the first view, as essential mechanisms for aligning the interests of employees and employers and thereby creating profitable companies in the second view, as insufficient for looking out for workers’ interests in the third view, and as manipulative managerial tools for shaping the ideology and structure of the workplace in the fourth view.[36]		Literature on the employment impact of economic growth and on how growth is associated with employment at a macro, sector and industry level was aggregated in 2013.[37]		Researchers found evidence to suggest growth in manufacturing and services have good impact on employment. They found GDP growth on employment in agriculture to be limited, but that value-added growth had a relatively larger impact.[25] The impact on job creation by industries/economic activities as well as the extent of the body of evidence and the key studies. For extractives, they again found extensive evidence suggesting growth in the sector has limited impact on employment. In textiles however, although evidence was low, studies suggest growth there positively contributed to job creation. In agri-business and food processing, they found impact growth to be positive.[37]		They found that most available literature focuses on OECD and middle-income countries somewhat, where economic growth impact has been shown to be positive on employment. The researchers didn't find sufficient evidence to conclude any impact of growth on employment in LDCs despite some pointing to the positive impact, others point to limitations. They recommended that complementary policies are necessary to ensure economic growth's positive impact on LDC employment. With trade, industry and investment, they only found limited evidence of positive impact on employment from industrial and investment policies and for others, while large bodies of evidence does exist, the exact impact remains contested.[37]		Researchers have also explored the relationship between employment and illicit activities. Using evidence from Africa, a research team found that a program for Liberian ex-fighters reduced work hours on illicit activities. The employment program also reduced interest in mercenary work in nearby wars. The study concludes that while the use of capital inputs or cash payments for peaceful work created a reduction in illicit activities, the impact of training alone is rather low.[38]		The balance of economic efficiency and social equity is the ultimate debate in the field of employment relations.[39] By meeting the needs of the employer; generating profits to establish and maintain economic efficiency; whilst maintaining a balance with the employee and creating social equity that benefits the worker so that he/she can fund and enjoy healthy living; proves to be a continuous revolving issue in westernized societies.[39]		Globalization has effected these issues by creating certain economic factors that disallow or allow various employment issues. Economist Edward Lee (1996) studies the effects of globalization and summarizes the four major points of concern that affect employment relations:		What also results from Lee’s (1996) findings is that in industrialized countries an average of almost 70 per cent of workers are employed in the service sector, most of which consists of non-tradable activities. As a result, workers are forced to become more skilled and develop sought after trades, or find other means of survival. Ultimately this is a result of changes and trends of employment, an evolving workforce, and globalization that is represented by a more skilled and increasing highly diverse labor force, that are growing in non standard forms of employment (Markey, R. et al. 2006).[39]		Various youth subcultures have been associated with not working, such as the hippie subculture in the 1960s and 1970s (which endorsed the idea of "dropping out" of society) and the punk subculture, in which some members live in anarchist squats (illegal housing).		One of the alternatives to work is engaging in postsecondary education at a college, university or professional school. One of the major costs of obtaining a postsecondary education is the opportunity cost of forgone wages due to not working. At times when jobs are hard to find, such as during recessions, unemployed individuals may decide to get postsecondary education, because there is less of an opportunity cost.		Workplace democracy is the application of democracy in all its forms (including voting systems, debates, democratic structuring, due process, adversarial process, systems of appeal) to the workplace.[40][41]		When an individual entirely owns the business for which they labor, this is known as self-employment. Self-employment often leads to incorporation. Incorporation offers certain protections of one's personal assets.[39] Individuals who are self-employed may own a small business. They may also be considered to be an entrepreneur.		In some countries, individuals who are not working can receive social assistance support (e.g., welfare or food stamps) to enable them to rent housing, buy food, repair or replace household goods, maintenance of children and observe social customs that require financial expenditure.		Workers who are not paid wages, such as volunteers who perform tasks for charities, hospitals or not-for-profit organizations, are generally not considered employed. One exception to this is an internship, an employment situation in which the worker receives training or experience (and possibly college credit) as the chief form of compensation.[40]		Those who work under obligation for the purpose of fulfilling a debt, such as an indentured servant, or as property of the person or entity they work for, such as a slave, do not receive pay for their services and are not considered employed. Some historians suggest that slavery is older than employment, but both arrangements have existed for all recorded history. Indentured servitude and slavery are not considered compatible with human rights and democracy.[40]		
Workplace wellness is any workplace health promotion activity or organizational policy designed to support healthy behavior in the workplace and to improve health outcomes. Known as 'corporate wellbeing' outside the US, workplace wellness often comprises activities such as health education, medical screenings, weight management programs, on-site fitness programs or facilities. These programs can be classified as primary, secondary, or tertiary health programs, depending on the goal of the specific program. Primary prevention programs usually target a fairly healthy employee population, and encourage them to more frequently engage in health behaviors that will encourage ongoing good health. Example of primary prevention programs include stress management, and exercise and healthy eating promotion. Secondary prevention programs are targeted at reducing behavior that is considered a risk factor for poor health. Examples of such programs include smoking cessation programs and screenings for high blood pressure or other cardiovascular disease related risk factors. Tertiary health programs address existing health problems, and aim to help control or reduce symptoms, or to help slow the progression of a disease or condition. Such programs might encourage employees to better adhere to specific medication or self-managed care guidelines. Workplace wellness programs can be categorized as primary, secondary, or tertiary prevention efforts, or an employer can implement programs that have elements of multiple types of prevention.[1]		The lifestyles of people in the workforce are important both for the sake of their own health and for the sake of their employer's productivity. Companies often subsidize these programs in the hope that they will save companies money in the long run by improving health, morale and productivity, although there is some controversy about evidence for the levels of return on investment.[2]		Other examples of workplace wellness organizational policies include allowing flex-time for exercise, providing on-site kitchen and eating areas, offering healthy food options in vending machines, holding “walk and talk” meetings, and offering financial and other incentives for participation.[3] In recent years, workplace wellness has been expanded from single health promotion interventions to create a more overall healthy environment including, for example standards of building and interior design to promote physical activity. This expansion is largely been in part to creating greater access and leadership support from leaders in the participating companies.		The following information comes from the Kaiser Family Foundation Summary of Findings from 2016 in order to provide current information about employer-sponsored health benefits, the Kaiser Family Foundation (Kaiser) and the Health Research & Educational Trust (HRET) conduct an annual survey of private and nonfederal public employers with three or more workers.		Many employers offer wellness or health promotion programs to help employees improve their health and avoid unhealthy behaviors. Both small and large firms offer a program in at least one of these areas: smoking cessation; weight management; behavioral or lifestyle coaching. 46% of small firms and 83% of large firms offer these. 3% percent of small firms and 16% of large firms reported collecting health information from employees through wearable devices like  a Fitbit or Apple Watch. 42% of large firms with one of these health and wellness programs offered employees a financial incentive to participate in or complete the program. Among most large  firms with an incentive for completing wellness programs, incentives include: lower premium contributions or cost sharing (34% of firms); cash, contributions to health-related savings accounts, or merchandise (76% of firms); some other type of incentive (14% of firms). Some firms separate financial incentives for different programs and some others have incentives that require participation in more than one type of program (e.g., completing an assessment and participating in a health promotion activity).		There are various types of Wellness Programs offered in firms. Biometric screening programs can help identify cardiovascular risk factors in clients. Larger firms or businesses tend to facilitate more incidences of biometric screening programs. This can be in part to the amount of leadership support that is encouraged by company leaders and then received by employees[4]						There are numerous reasons to implement workplace wellness programs into the workplace. To begin, many Americans spend the majority of their time in the workplace. Additionally, the cost of healthcare is continually rising as result of chronic diseases in the US, workplace wellness programs can help abate this cost. Workplace wellness programs can also decrease overall cost of healthcare for participants and employers.		More than 130 million Americans are employed across the United States annually. Workplace wellness programs have been shown to prevent the major shared health risk factors specifically for CVD and stroke.[5] Since preventing these major health risks through workplace wellness can help decrease costs for both parties, the implementation of these programs is important. It was as early as 1958 that evidence began to emerge that exposure to ‘‘occupational stress and strain was much higher in young male coronary patients than in equivalent healthy controls. When there is excessive psychological workplace demands coupled with low job decision latitude, stress increases as well as risk for CVD.[6] Workplace wellness programs can be implemented to help prevent this risk. Increasing physical activity is an important part to in decreasing  CVD. Since majority of Americans spend much of their adulthood in the workplace, having primary prevention programs  that emphasizes physical activity can make this disease largely preventable.[5]		For 2015-25, health spending is projected to grow at an average rate of 5.8 percent per year (4.9 percent on a per capita basis). Rising employee health care costs have put a growing financial strain on employers across the country, with about one-sixth of these costs directly related to CVD.[7] Heart disease and stroke cost the nation an estimated $316.6 billion in health care costs and lost productivity in 2011.  As these health care costs rise in the U.S., employers are seeing increased spending associated with health care for employees. Costs can be incurred by paying for care, and in lost productivity due to employee illness or absence. Reducing costs associated with preventable illness, like cardiovascular disease, is in the financial interests of both employers and employees.  In order to ensure continued reductions in the burden of cardiovascular disease, as well as the overall sustainability of the healthcare system, a paradigm shift that places more emphasis on cardiovascular health promotion throughout the life course is required.[7] Adding an emphasis of this health promotion in the workplace can help aid in the decrease of the disease.				While the stated goal of workplace wellness programs is to improve employee health, many US employers have turned to them to help alleviate the impact of enormous increases in health insurance premiums[8] experienced over the last decade. Some employers have also begun varying the amount paid by their employees for health insurance based on participation in these programs.[9] Cost-shifting strategies alone, through high copayments or coinsurance may create barriers to participation in preventive health screenings or lower medication adherence.hypertension.[10] Basically for every dollar spent on worksite wellness programs, medical costs fell by $3.27, and financial losses from lost productivity fell by $2.73.[7]		One of the reasons for the growth of healthcare costs to employers is the rise in obesity-related illnesses brought about by lack of physical activity, another is the effect of an ageing workforce and the associated increase in chronic health conditions driving higher health care utilization. In 2000 the health costs of overweight and obesity in the US were estimated at $117 billion.[11] Each year obesity contributes to an estimated 112,000 preventable deaths.[12] An East Carolina University study of individuals aged 15 and older without physical limitations found that the average annual direct medical costs were $1,019 for those who are regularly physically active and $1,349 for those who reported being inactive. Being overweight increases yearly per person health care costs by $125, while obesity increases costs by $395.[11] A survey of North Carolina Department of Health and Human Services employees found that approximately 70 cents of every healthcare dollar was spent to treat employees who had one or more chronic conditions, two thirds of which can be attributed to three major lifestyle risk factors: physical inactivity, poor diet, and tobacco use.[13] Obese employees spend 77 percent more on medications than non-obese employees and 72 percent of those medical claims are for conditions that are preventable.[14]		According to Healthy Workforce 2010 and Beyond, a joint effort of the US Partnership for Prevention and the US Chamber of Commerce, organizations need to view employee health in terms of productivity rather than as an exercise in health care cost management. The emerging discipline of Health and Productivity Management (HPM) has shown that health and productivity are “inextricably linked” and that a healthy workforce leads to a healthy bottom line. There is now strong evidence that health status can impair day-to-day work performance (e.g., presenteeism) and have a negative effect on job output and quality.[14] Current recommendations for employers are not only to help its unhealthy population become healthy but also to keep its healthy population from becoming sick. Employers are encouraged to implement population-based programs including health risk appraisals and health screenings in conjunction with targeted interventions.[10]		Investing in worksite wellness programs not only aims to improve organizational productivity and presenteesim but offers a variety of benefits associated with cost savings and resource availability. A study performed by Johnson and Johnson indicated that wellness programs saved organizations an estimated $250 million on health care costs between 2002 and 2008. Workplace wellness interventions performed on high-risk cardiovascular disease employees indicated that at the end of a six month trial, 57% were reduced to a low-risk status. These individuals received not only cardiac rehabilitation health education but exercise training as well.[15] Further, studies performed by the U.S. Department of Health and Human Services and J&J have revealed that organizations that incorporated exercise components into their wellness programs not only decreased healthcare costs by 30% but improved lost work days by 80%.[15] Thus, investing in preventative health practices has proven to not only be more cost effective in resource spending but in improving employee contributions towards high-cost health claims.		Researchers from the Centers for Disease Control and Prevention studied strategies to prevent cardiovascular disease and found that over a two- to five-year period, companies with comprehensive workplace wellness programs and appropriate health plans in place can yield $3USD to $6USD for each dollar invested and reduced the likelihood of employee heart attacks and strokes.[16] Also, a 2011 report by Health Fairs Direct which analyzed over 50 studies related to corporate and employee wellness, showed that the return on investment (ROI) on specific wellness related programs ranged between $1.17 to $6.04.[17] In general, it is estimated that worksite health promotion programs result in a benefit-to-cost ratio of $3.48 in reduced health care costs and $5.82 in lower absenteeism costs per dollar invested, according to the Missouri Department of Health & Senior Services.[18] Additionally, worksite health programs can improve productivity, increase employee satisfaction, demonstrate concern for employees, and improve morale in the workplace.[19]		Leadership involvement in wellness programs can additionally impact employee health outcomes just as well as the programs themselves. A study performed by David Chenoweth indicated the managers who were passionate and committed about their wellness programs increased employee engagement by 60%, even if their wellness goals were not achieved. Leaders are not only tasked with creating the organizational culture but also in coaching and motivating employees to be engaged in that culture.[20]		An Employee wellness program makes a possitve impect on office environment by revamping workplace morale. If your employees fit and healthy then they perform better and are quite happy. They understand that their organization is putting steps forward to ensure their well-being and health.[21]		Contrary to the many benefits of worksite wellness programs, most employers have yet to embrace the worksite wellness strategy according to the findings of the 2004 National Worksite Health Promotion Survey.[14] Only 6.9 percent of surveyed organizations met the criteria for a comprehensive health promotion program. This is far short of the 75 percent target included in the Healthy People 2010 goal which shows that there are still significant barriers to the large-scale adoption of worksite health promotion practices by organizations, both large and small.		The encouraging news is that since the 2004 report was published, there appears to be more momentum toward implementation of comprehensive work site health promotion. This is evident by pending federal legislation and the growth of employer-based health coalitions such as the National Business Group on Health, Institute for Health and Productivity Management, Center for Health Value Innovation, and the National Business Coalition on Health. Peer-based executive advocacy through the Leading by Example initiative of Partnership for Prevention is another example of this trend towards comprehensive workplace health promotion.		Low participation rates by employees could significantly limit the potential benefits of participating in workplace wellness programs, as could systematic differences between participants and non-participants. Research performed by Gallup indicated that out of the 60% of employees who were aware of their company offered a wellness program only 40% participated.[22] A 2008 study[23] from the University of Minnesota provided insight into the likelihood of employee participation in an exercise promotion program. Their findings illustrate barriers to program participation that may be applicable to other types of programs and workplace settings. Employees were offered a financial incentive to attend a designated set of fitness facilities at least 8 times per month during the study period, and researchers administered a survey to over 3,000 program participants and non-participants to better understand their decision to participate. The research team included survey questions to assess each employee's attitudes and practices related to fitness prior to the program being offered, their marginal utility related to the financial incentive offered, the marginal cost of exercising (based on the cost of time and the financial cost of fitness center membership), prior history of chronic disease, and demographic characteristics related to age, gender, race and ethnicity, income, and employment type within the university system. Based on these survey responses, researchers reported the marginal effects related to the probability of 1) signing up for the program and 2) meeting program participation criteria by exercising 8 times per month to receive the financial incentive.		Employees with a higher time cost of exercise, calculated by the campus where the employee worked and by the number of participating fitness sites in the employee's home zip code, had a lower probability of signing up for and completing the program. Younger workers (ages 18–34)were more likely to sign up for the program relative to older employees, and women were more likely to sign up for the program than men. Researchers also found that employees with diabetes or low back pain were less likely to participate.		Program participation reflects a different trend. When researchers investigated the likelihood that an individual would be a regular program exerciser, defined as a participant in the program who checked in at a participating facility at least 8 times per month, for at least 50% of the time period for which the financial incentive was offered. Program participation in this sense means that an employee both signed up and completed the criteria to receive the reward. Regular exercise were more likely to be older (ages 55+), male, and to be classified as regular exercisers before the program was offered. These findings suggest that there may be differences between employees who would like to, or intend to, participate in certain workplace programs, and those who are likely to be able to participate and benefit. While this study focuses specifically on exercise and participation, lessons regarding the time cost of participation, location barriers to participation, and age and gender differences in participation rates are all important considerations for a firm interested in designing an effective workplace wellness program, especially if the goal is to promote a new behavior.		Ongoing management support and accountability are critical to successful worksite health promotion programs. Per research performed by Gallup, "Managers are uniquely positioned to ensure that each of their employees knows about the company's wellness program and to encourage team members to take part." By engaging workers in this organizational culture, employees become 28% more likely to participate in wellness programs than the average employee. Methods in which leaders can overcome the barrier of engagement is to not only model behaviors of the program but to consistently and effectively communicate the value of the wellness programs to employees. By creating the time and focus to improve the overall health and wellness of employees, managers can present many health and costs benefits to all members of the organization.[22]		Worksite wellness programs including nutrition and physical activity components may occur separately or as part of a comprehensive worksite health promotion program addressing a broader range of objectives such as smoking cessation, stress management, and weight loss. A conceptual model has been developed by the Task Force for Community Preventive Services (The Community Guide) and serves as an analytic framework for workplace wellness and depicts the components of such comprehensive programs.[24] These components include worksite interventions including 1) environmental changes and policy strategies, 2) informational messages, and 3) behavioral and social skills or approaches.		Worksite environmental change and policy strategies are designed to make healthy choices easier. They target the whole workforce rather than individuals by modifying physical or organizational structures. Examples of environmental changes may include enabling access to healthy foods (e.g., through modification of cafeteria offerings or vending machine content) or enhancing opportunities to engage in physical activity (e.g., by providing onsite facilities for exercise). Policy strategies may involve changing rules and procedures for employees, such as offering health insurance benefits, reimbursement for health club memberships, healthy food and beverage policies or allowing time for breaks or meals at the worksite.		Informational and educational strategies attempt to build the knowledge base necessary to inform optimal health practices. Information and learning experiences facilitate voluntary adaptations of behavior conducive to health. Examples include health-related information provided on the company intranet, posters or pamphlets, nutrition education software, and information about the benefits of a healthy lifestyle, including diet and exercise. Behavioral and social strategies attempt to influence behaviors indirectly by targeting individual cognition (awareness, self-efficacy, perceived support, intentions) believed to mediate behavior changes. These strategies can include structuring the leadership involvement and social environment to provide support for people trying to initiate or maintain lifestyle behavior changes, for example, weight change. Such interventions may involve individual or group behavioral counseling, skill-building activities such as cue control, use of rewards or reinforcement, and inclusion of coworker, manager/leader or family members for support.[25]		With healthcare expenses growing, it is becoming more and more important to control costs when possible. One such method of cost containment is through Workplace Wellness programs. Wellness programs are typically employer sponsored and are created with the theory that they will encourage healthy behaviors and decrease overall health costs over time.[26] Wellness programs function as Primary Care interventions as they are an example of primary prevention methods to reduce risks to many diseases or conditions.[27]		Workplace wellness programs have been around since the 1970's [28] and have gained new popularity as the push for cost savings in the health delivery system becomes more evident as a result of high health care expenditures in the U.S. Employer wellness programs have shown to have a return on investment (ROI) of about $3 for every $1 invested over a multi-year period,[29] making them appealing to many as an effective way to achieve results and control costs.		Workplace wellness programs have many components to help improve health outcomes and decrease health disparities. These components include: smoking cessation programs, fitness center memberships, nutrition aids, and biometric screenings, often in exchange for health insurance premium reductions.[30] Workplace wellness programs benefit employers as well; while the various components of the wellness programs helps to keep employees healthy, employers are able to increase recruitment and retention of workers.[31] Some employers have also utilized penalties to improve employee participation within the company wellness program.[32] While wellness programs promote healthier lifestyles and can bring significant cost savings, concerns about invasion of privacy and participation costs have arisen.[33]		The future of wellness programs as a valid method of preventative healthcare is still up for debate. Evidence of improved health outcomes for participants is mixed in terms of effectiveness.[30] Some studies attempt to address the question if “more is always better in the workplace,” and the value that can be found through wellness program components and their outcomes. One large study though, did not find health improvements for premium incentive-based workplace weight loss programs.[34] Workplace wellness programs have the potential to lead to healthier outcomes and decreased costs, but the economics are still unclear and more research is required.		Healthy People 2020 is a blueprint for a 10-year national initiative to improve the health of all Americans, "providing 42 topics and over 1,200 objectives to guide evidence-based practice".[35] "A shorter list of high priority topics are categorized into Leading Health Indicators (LHIs) highlighting collaborative actions that can be taken to impact American health outcomes."[35] Employers can use Healthy People 2020 objectives to focus business-sponsored health promotion/disease prevention efforts and measure worksite and community-wide outcomes against national benchmarks using evidence-based practices. As defined by Healthy People 2020, a comprehensive worksite health promotion program contains five program elements:		Partnership for Prevention includes two additional components:		Additionally, the Center for Disease Control and Prevention (CDC)  indicates that a workplace health program will be “coordinated and comprehensive” if strategies include “programs, policies, benefits, environmental supports, and links to the surrounding community designed to meet health and safety of employees.”[38]  Furthermore, the Centers for Disease Control and Prevention's National Institute for Occupational Safety and Health (NIOSH), through its Total Worker Health program, offers an extensive list of resources to assist employers, employees, and practitioners with their efforts to implement and develop programs in their organizations that integrate employee health activities. Most of the publicly available reputable sources provides tips and tools, but are not "off-the-shelf" or "turn-key solutions." Organizations wishing to obtain more assistance will find there are numerous private companies offering fee-based services.		The Partnership for Prevention offers extensive background and program-specific information in its Healthy People 2020 and Beyond report which is available to anyone at no charge on the internet. Additionally, a "midcourse review" for Healthy People 2020 LHI progress can be found using their interactive infographic, which can be found by clicking here.		The Center for Disease Control and Prevention (CDC) recommends a workplace health model incorporating elements and fundamental ideas of The Community Guide's framework and Healthy People 2020 in a coordinated approach to impact health at the workplace.  This approach should be "coordinated, systematic and comprehensive".[39] Below describes the CDC Workplace Health Model.		Program success and employee engagement demands information to be obtained about the workplace, either formally (i.e. needs assessment) or informally (i.e. conversations with employees), collecting data regarding individual lifestyle, work environment, and organizational details.[39] Data should be collected for both employee interests and available aggregate data, including, but not limited to, health status, health issues or cultural survey data.[39] Engaging employees, including the leadership team, from the beginning of program planning and development will help drive commitment, responsibility, and participation; as well as, creating a culture of health and great place to work.[39]  Additional information to assist with workplace assessment can be found using the CDC Assessment Module.		Next is to develop a strategic plan that considers the pertinent assessment results from a vantage point of both the individual's actions and environmental context in accordance with the direction from the governance structure.[39] This should always be completed prior to implementation or evaluation; however, keeping the end in mind (how will I evaluate this program to know it was successful?) will help drive the overall plan. The recommended strategy for "direction leadership and organization"[40] by the CDC includes: leadership support dedicated to championing wellness and modeling behaviors;[41][42] workplace Wellness Committee, Coordinator or Council; development of a resource list of available assets; defined mission, vision, goals, objectives and strategies; comprehensive communication plan; evidence-based practices; and data collection and analysis.[39][40] A thoughtful strategic plan will select and deliver interventions, policies, and programs that are most advantageous to the particulars of the employee population.[39][40] Additional resources can be found by visiting the CDC's Planning/Workplace Governance Module.		The implementation stage is where the rubber meets the road.  Employees often see this stage as the "Wellness Program", and typically do not understand what goes into the process to provide a comprehensive strategic plan. Therefore, implementation occurs when the strategic plan executes the opportunities to support an employee's health.[39] The CDC recommends four main categories for interventions or strategies that successfully influence health: "health-related programs; health-related policies; health benefits; and environmental supports".[39] Please visit the CDC Implementation Module for additional implementaiton tools.		To determine impact and success, evaluation is crucial to the longevity of a workplace wellness program.  Everything from programs to policies to environment must be evaluated to determine return on investment (ROI), value on investment (VOI), health impact, employee satisfaction and sustainability.[39] "According to the CDC (2016), evaluations can often be overwhelming, time-consuming and expensive; so focusing on relevant, salient, and useful information is key to quality evaluation practices. An evaluation tool should be designed to support the program process, quality improvement, and identification of gaps for future strategic plans."[39] For additional evaluation details please visit "Best Practices in Evaluating Worksite Health Promotion Programs"  published in the American Journal of Health Promotion.		The CDC Workplace Health Program Development Checklist serves as a guide through this process.[39]		According to research completed by Hoffman and Kennedy-Armbruster (2015), and published by the American College of Sports Medicine Health and Fitness Journal, the top nine Workplace/Worksite Wellness Best Practices include:		Additional Workplace Wellness Best Practices can be found by visiting the following well-respected organizations:		Leadership support has been indicated as a benefit, best practice and a key element of the CDC to be part of the Workplace Health Model; therefore, understanding research to sustain these claims is important for the success of programs. Below you find research to support how leadership involvement and commitment of wellness activities (i.e. physical activity) can impact health status (i.e. cardiovascular disease status).		Lawrence et al (2015) indicate strengths and weakness of current worksite wellness models providing key considerations for the development of a strong program. Key strengths indicated were "leadership commitment, organizational culture and environmental structure" to build a culture of health, ultimately promoting the improvement among non-communicable diseases.[45] Furthermore, Lawrence et al (2015) suggest that these characteristics are indicative of a "high-quality program regardless of the model used: 1) leadership support, 2) clear importance of health and wellness by organization culture and environment, 3) program responsiveness to changing needs, 4) utilization of current technology, and 5) support from community health programs".[45]		A recent survey conducted by Harris Poll and reported the American Psychological Association (APA) (2016) stated just over 30% of Americans indicate participation in worksite wellness programs, and 44% indicate that employee wellness is supported by their workplace culture.[46] Furthermore, this data indicates when senior leaders are involved and committed to wellness program initiatives 73% of employees feel their organization supports a healthy lifestyle and their overall well-being; however, only 4 of 10 Americans said that their leadership team supports wellness initiatives.[46] Further evidence reported by RAND (2013) stated “evidence from case studies suggests that for programs to be a success, senior managers need to consider wellness an organizational priority to shift the company culture. Buy-in from direct supervisors is crucial to generate excitement and connect employees to available resources.”[47]		Furthermore, Hoert, Herd, and Hambrick (2016) used the Leading by Example instrument to find "employees experiencing higher levels of leadership support reported higher wellness program participation, lower stress, and higher levels of health behaviors".[48] Stokes, Henley and Herget (2006) reported on findings from a wellness initiative pilot program using the North Carolina Department of Health and Human Services. This organization was selected as the pilot due to the leadership support model and its large size.[49] This program focused on "reducing major chronic diseases (including cardiovascular diseases), demonstrating the effectiveness of a wellness program model that includes a full-time department-level director, establish wellness committees to sustain work environments that promote and support employee health and wellness, and change policies and environments to help employees be more active, make healthier food choices, avoid tobacco, and manage stress".[49] Results after the first year of this program indicated that 62% of employees participated in at least one wellness activity, 51% exercising more often, 50% stating wellness programs as the most popular activity, 49% eating more fruits and vegetables, 27% were closer to a healthy weight, and 106 employees stopped smoking and 149 reduced tobacco use.[49]		Finally, Ross et al (2013) report the increased importance of physical activity at the workplace with the increasing sedentary job responsibilities, and the positive effect of worksite physical activity programs have on health outcomes, including cardiovascular disease and metabolic conditions.[50] Additionally, their research indicates appropriate models for these successes are through "a health and wellness culture driven by leadership support, specialized programs designed for the employee population, and strategic plans that partner with current organizational goals".[50]		The findings mentioned above are only representative of a small sample of research to support the important role and impact leadership involvement is in workplace health programs, including physical activity programs and the impact on health status (including cardiovascular disease).  Consider leadership as a key functional unit to determine success for health behaviors and outcomes when developing a wellness program.		The framework of The Community Guide, program components (goals and objectives) set out by Health People 2020, the Workplace Health Model outlined by the CDC, and other best practices provides a comprehensive foundation for a worksite wellness platform regarding program development, implementation, and evaluation. Using the components above, an employee (or employer) can use employee interest, employee aggregate health data, and the LHIs as priorities to guide goals and objectives, develop programs and evaluation to facilitate, collaborate, and motivate their employees to improve their health. The following example will use the above mentioned workplace wellness program components as it relates to the goal of weight reduction by increased physical activity through leadership support in order to decrease cardiovascular disease, ultimately impacting the Healthy People 2020 LHI "Nutrition, Physical Activity, and Obesity".[51]  The following is a simplified example for (a fabricated) Company ABC:		An employee (or Company ABC Human Resource Department) reviews the current aggregate data provided by the employer insurance company as it relates to weight, physical activity, and cardiovascular measures, including cholesterol, blood pressure, biometric screening, physical activity, smoking, or other indicators. Company ABC will review the available company culture survey results as it relates to wellness for both social and environmental supports.  Conversations and focus groups will be established to assess and determine employee engagement, interests, concerns, and other wellness related brainstorming.		Leadership Support: The Vice President (VP) of Sales has volunteered to be the Leadership Wellness Champion to model and support wellness programs from the individual, company, and organizational perspective to drive buy-in and support from all levels of Company ABC. The VP will support the strategic plan for wellness as it fits into the organization's overall strategic plan and cultural goals.[41]		Wellness Coordinator & Committee: Company ABC has hired a Wellness Coordinator to execute these responsibilities, assess; collaborate with employees through a wellness committee; determine a strategic plan including a mission, vision, goals, and objectives; review evidence-based practices to implement; and conduct ongoing evaluations of the program.		Vision: Our organization embraces a culture of health to support and improve overall employee health and productivity.		Mission: The mission of our Worksite Wellness Program is to encourage personal and professional productivity; and physical and mental well-being through leadership, social and environmental supports to ensure the opportunity for healthy lifestyle choices in the workplace.		Goal: To improve weight status among employees increasing physical activity through leadership support to improve cardiovascular health.[41]		Objectives - over the next year (12 months)		Resources: The following list is not comprehensive; however, lists our current resources available for Company ABC: leadership support and resources, wellness budget of $100,000, onsite wellness clinic (medical), onsite fitness center, breastfeeding policy, wellness committee members, many rooms of various sizes, health insurance, telephonic health coaches, marketing support for promotions and advertising, intranet & company wellness website, wellness coordinator with designated time to invest in the strategic plan and execution, and access to local community health organizations/partners.		Communication: The Wellness Coordinator will be responsible for all communications regarding wellness interventions and programs, targeting specific communication based on the demographics of Company ABC (including level of literacy, languages, or other contributing factors).  Furthermore, the Wellness Coordinator will work with the company marketing team to develop and design promotional materials for print and the websites for consistent and clear communication.  The marketing plan timeline will be outlined with the program dates with marketing and promotion at least 30 days prior to any interventions, initiatives, policies or programs.		Using The Community Guide, Health People 2020, CDC recommendations, and other peer-reviewed research (evidence-based programs) an organization can design and implement recommended intervention, policies, programs, or environmental supports to ensure the success of the desired goals and outcomes. Based on the CDC's recommendation to include a multidimensional intervention framework,[52] the wellness coordinator for Company ABC has decided on the following programs to support the goals of physical activity through leadership support to help improve cardiovascular health.		Please see below in the section "Successful interventions" to find additional related evidence-based practices regarding leadership support for worksite wellness physical activity programs to reduce cardiovascular disease.		Utilizing past aggregate data from health insurance claims and company culture survey from Company ABC a baseline will be established prior to the program implementation. Following the wellness coordinators strategic plan for Company ABC, measures post-program (1 year) will be collected,  including an employee satisfaction survey, informal forums, a collection of corresponding year aggregate data from the insurance company and data from the annual culture survey.[57] Data analysis will be conducted on program successes, strengths, opportunities, threats, and weakness; furthermore, interpretation of results will be collected and reported back to the leadership team for review and support for the next year's strategic planning. Additional comparisons can be made to the Healthy People 2020 LHI's Report Card to determine how the employees of Company ABC are doing in comparison to the reported United States data, as well as how they are supporting the overall goals of the Healthy People 2020 goals.[58]		The overarching goal of workplace wellness programs is to improve the health of employees, but the measures of success for both health outcomes and financial benefits varies vastly from program to program. In one large study of 1,542 participants across 119 workplaces, 57.7% of participants showed significant reductions in 7 of the 10 cardiovascular health risk categories studied.[59] This was achieved through four 30-min telephone based coaching sessions per month on the 10 risk categories with additional educational materials and a Web-based health tracking system available. Over the past three years, the Global Business Coalition Health (GBCHealth) decreased their program participants' risk for developing coronary heart disease within the next 10 years by an average of 10.6 percent (overall) and an average of 32.6 percent (in participants with an elevated baseline risk).[60] This was achieved by offering tools and resources to educate participants about their cardiovascular disease risks and help them lower their risk levels or maintain their low risk status. Successful workplace interventions are known to not only improve the health of employees, but also save the company money overall. Johnson & Johnson, one of the world's largest companies, has saved $250 million on health care costs within the last decade as a result of wellness programs; from 2002 to 2008, the return was $2.71 for every dollar spent.[61] In a critical meta-analysis of literature on costs and savings associated with wellness programs, medical costs fell by about $3.27 for every dollar spent on wellness programs and absenteeism costs fell by about $2.73 for every dollar spent.[62] Evaluations of cardiovascular wellness programs have shown similar benefits. The CDC conducted a study of nine organizations with workplace health management or wellness programs and found a return on investment ranging from $1.40 to $4.90 per dollar spent. The approaches included using a health risk assessment, offering fitness facilities, providing nutrition education and providing education programs targeted to those at high risk of disease.[63]		The Centers for Disease Control and Prevention conducted a case study of a workplace wellness program at Austin, TX's Capital Metro, Austin's local transit authority.[64]		Capital Metro employs 1,282 people. In 2003, Health & Lifestyles was hired to help promote healthier lifestyles, increase employee morale, and combat rising health care costs and absenteeism rates.		Health & Lifestyles provided consultations with wellness coaches and personal trainers, a 24-hour company fitness center, personalized health assessments, and preventive screenings. The program expanded to include healthier food options, cash incentives, health newsletters, workshops, dietary counseling, smoking cessation programs, and a second fitness center. As of lately companies have begun adopting technological trends in efforts to increase participation in work site wellness programs. Companies have been embracing technology from corporate wellness companies to provide their workforce with wellness website portals, mobile applications, and health coaching.		Participants in the wellness program reported improvements in physical activity, healthy food consumption, weight loss, and blood pressure. Capital Metro's total health care costs increased by progressively smaller rates from 2003 to 2006 and then decreased from 2006 to 2007. Absenteeism has decreased by approximately 25% since the implementation of the program, and the overall return on the investment was calculated to be 2.43.		Since its beginning in 2003, the wellness program at Capital Metro has shown promising results in improving employee health and reducing costs associated with health care and absenteeism, and the financial benefits outweigh the annual investment (2.43 ROI). Employees engage in more physical activity, have better knowledge of disease management (diabetes and asthma), have better eating habits, and smoke less than they did before the program was implemented. Health care and absenteeism costs have been reduced and are continuing to decline, most likely as a result of the program. Managerial staff have reported that employee morale has increased since the program was implemented. Most importantly, however, we believe that the wellness program has the potential to reduce the prevalence and severity of chronic diseases, allowing Capital Metro employees to lead longer, healthier lives.[64]		
A severance package is pay and benefits employees receive when they leave employment at a company. In addition to their remaining regular pay, it may include some of the following:		Packages are most typically offered for employees who are laid off or retire. Severance pay was instituted to help protect the newly unemployed. Sometimes, they may be offered for those who either resign, regardless of the circumstances, or are fired. Policies for severance packages are often found in a company's employee handbook, and in many countries, they are subject to strict government regulation. Severance contracts often stipulate that employees will not sue the employer for wrongful dismissal or attempt to collect on unemployment benefits, and that if they do so, they must return the severance money.						Severance agreements are more than just a "thank you" payment from an employer. They could prevent an employee from working for a competitor and waive any right to pursue a legal claim against the former employer. Also, an employee may be giving up the right to seek unemployment compensation. An employment attorney may be contacted to assist in the evaluation and review of a severance agreement. The payments in some cases will continue only until the former employee has found another job.		In February 2010, a ruling in the Western District of Michigan held that severance pay is not subject to FICA taxes,[2] but it was overturned by the Supreme Court in March 2014.[3]		Employers are required to pay severance pay after an employee working in Puerto Rico is terminated.[4][5] Employees are not permitted to waive this payment.[6] Severance pay is not required if the employee was terminated with "just cause."[5]		Just cause is satisfied in any of the following situations: the employee had a pattern of improper or disorderly conduct; the employee worked inefficiently, belatedly, negligently, poorly; the employee repeatedly violated the employer's reasonable and written rules; the employer had a full, temporary, or partial closing of operations; the employer had technological or reorganization changes, changes in the nature of the product made, and changes in services rendered; or the employer reduced the number of employees because of an actual or expected decrease in production, sales, or profits.[7]		An employee with less than five years of employment with the employer must receive a severance payment equal to two months of salary, plus an additional one week of salary for each year of employment. An employee with more than five years but less than fifteen years of employment must receive a severance payment equal to three months of salary, plus an additional two weeks of salary for each year of employment. An employee with more than fifteen years of service must receive a severance payment equal to six months of salary, plus an additional three weeks of salary for each year of employment.[8]		In the United Kingdom Labour Law provides for Redundancy Pay.[9] The maximum amount of statutory redundancy pay is £14,250.		
A gap year is a year’s break between high school and college/university, aimed at promoting a mature outlook with which to absorb the benefits of higher education.[1] It also indicates a break before entry into graduate school. Activities range across advanced academic courses, extra-academic courses and non-academic courses, such as pre-college math courses, language studies, learning a trade, art studies, volunteer work, travel, internships, sports and more. It is also known as a sabbatical year.						In 1967, Nicholas Maclean-Bristol set up the educational volunteering charity Project Trust and sent three volunteers to Addis Ababa in Ethiopia.[citation needed]		In 1972, Gap Activity Projects was founded in the UK and later renamed Lattitude Global Volunteering in 2008.[citation needed]		In 1973, Graham "Skroo" Turner set up the company Topdeck, one of the first tour operators.[citation needed]		In 1978, the Prince of Wales and Colonel John Blashford-Snell began what is now known as Raleigh International by launching Operation Drake, an expedition voyage around the world following Sir Francis Drake's route.[citation needed]		In the United States, the gap year idea was promoted by Cornelius H. Bull to allow students more time to grow as a person, in 1980.[2]		Australians and New Zealanders have a tradition of travelling overseas independently at a young age. In New Zealand this is known as "doing an OE" (Overseas experience). Sometimes an OE is limited to one year, but often Australians and New Zealanders will remain overseas for three to five years, with many working short-term in service industry jobs to fund their continuing travels. Europe and Asia are popular destinations for doing an OE. In Australia, exchange programs and youth benefits provide many opportunities for young people to broaden their minds through travel in a gap year.[3] The concept of the OE is so ingrained in the New Zealand psyche that the national tax department devotes a section of its website telling people doing their OEs how it will contribute to the country.		The Time Credit system in Belgium entitles employees of one year per lifetime of absence from their job, in order to prevent burn-out and to provide an opportunity to pursue other important things in life.[4]		Denmark has sought to limit the number of students who take a year out, penalising students who delay their education to travel abroad or work full-time.[5] In 2006, it was announced that fewer students than before had taken a year out.[6] In April 2009, the Danish government proposed a new law which gives a bonus to students who refrain from a year out.[7]		In Ghana, most senior high school leavers have a year out from August to the August of the following year although this is not mandatory.		In Israel, it is customary for young adults who have completed their mandatory military service to go backpacking abroad in groups before starting university or a career.		Israel has also become a popular gap year travel destination for thousands of young Jewish adults from abroad each year.[8] There are over 10,000 participants annually who take a Masa Israel Journey gap year.[9]		The employment practice known as simultaneous recruiting of new graduates matches students with jobs before graduation, and the practice of a sabbatical is unusual in Japan as a result.[citation needed]		Nigerians typically enroll for a year long National service after college. It starts with a 3-week paramilitary boot-camp, from there, they are posted to a Government institution for the rest of the year.		There is no idea of gap years in Russia, Ukraine, and other post-Soviet countries at all. Men who weren't accepted to university are drafted into the army for a year.		In the Republic of South Africa a year off is common[citation needed]. This is specifically for more affluent classes[citation needed]. School leavers often travel abroad for getting further life experience[citation needed]. It is not uncommon for gap year students in South Africa to go to Cape Town to get life experience[citation needed]. It is not mandatory but fairly common for individuals to volunteer during this time doing animal welfare or tree planting[citation needed].[citation needed]		In the United Kingdom, the practice of taking a gap year – seen as an interim period of 7 or 8 months between completing secondary education and starting university – began to develop in the 1970s (“Martin, 2010). The period was seen as a time for gaining life experience through travel or volunteering. Universities appear to welcome post-gap-year applicants on the same basis as those going straight to university from previous education.[citation needed]		The number of students aged 18 opting to defer their university place in order to take a gap year reached a peak of 21,020 in 2008[10]. This figure crashed to 7,320 in 2011[10] – a year before the introduction of greatly increased tuition fees by the Conservative/Lib Dem (Cameron/Clegg) coalition government. Deferrals in 2016[10] were near their peak again although Year Out Group states its members now take more bookings from students outside the UK. Shorter gap style experiences (volunteering, expeditions, courses and work placements) are gaining in popularity, as they can be taken without the need to take a full year out of study or work.		In the United States, the practice of taking a "year off" remains the exception, but is gaining in popularity.[11] Many colleges, most notably Harvard University and Princeton University, are now encouraging students to take time off, and some have even built gap year-like programs into the curriculum. Several high schools now have counselors specifically for students interested in taking a gap year.[12] Taking a year off has recently become slightly more common for Americans, with prevailing reasons as a feeling of being burned out of classroom education and a desire to understand oneself better.[13] Some 40,000 Americans participated in 2013 in sabbatical programs, an increase of almost 20% since 2006, according to statistics compiled by the American Gap Association. Universities such as Georgetown University, New York University,[14] Amherst College, Princeton University, Harvard University, Massachusetts Institute of Technology, Middlebury College,[15] Yeshiva University,[16] and Reed College have formal policies allowing students to defer admission.[13] The Tufts University has a new program that seeks to remove financial barriers that prevent students with no money from taking a gap year after completing secondary to travel or do volunteer work in other countries.[17]		Some formal gap year programs can cost as much as $30,000, but there are also cheaper alternatives becoming more widely available; some do this by offering room and board.[18][19] For example, the National Civilian Community Corps, an AmeriCorps program, offers 18-24 year olds (no age limit for Team Leaders) an all expense paid gap year (room & board, meals, transportation, etc.) in exchange for a 10-month commitment to National and Community service.[20] AmeriCorps NCCC members travel the country in diverse teams and perform a variety of tasks such as rebuilding trails in national parks, responding to natural disasters or working as mentors for disadvantaged youth.[20] As with most AmeriCorps programs, service members receive an education award of approximately $6,000 upon completion of their service that can be used toward qualified educational expenses or student loans.[21] The zero cost to the member model AmeriCorps offers makes it an attractive alternative to costly gap year programs while leveraging taxpayer dollars to strengthen American communities. Additionally, new federal partnerships such as FEMA Corps offer traditional gap year seekers an immersive professional and team building experience that can serve as a launch pad for their careers.[22]		Some government programs designed to help students afford college prohibit students from taking a gap year. For example, the Tennessee Promise program requires that students must "Attend full-time and continuously at an eligible postsecondary institution as defined in T.C.A. § 49-4-708 in the fall term immediately following graduation or attainment of a GED or HiSET diploma; except that a student enrolling in a Tennessee College of Applied Technology (TCAT) may enroll in the summer prior to the fall term." [23]		In Venezuela, students from elite schools generally do their undergraduate studies outside of Venezuela. Gap years were unknown in Venezuela until educational consultant Nelson Agelvis, then counselor of the Moral y Luces Herzl-Bialik Jewish school in Caracas, insisted on having applicants to US colleges do them. The students went to leadership courses in Israel, PG years at elite US schools, tutorial colleges in the UK, work internships, language centers across the globe, and exploration gap years in remote countries. Today, the practice is widespread and Venezuela is a big economic contributor to the gap year, college studies and English studies industries, especially in countries such as Ireland.[24]		In Yemen, a defer year is mandatory between secondary school (High-School) and University. Unless one attends a private university, they must wait one year after secondary school before applying to University. Until the nineties it was mandatory for male graduates to go to the army for one year, and to teach in a school or work in a hospital for female graduates (and for men who cannot attend the army for health reasons).[citation needed]				
The creative class is a posited socioeconomic class identified by American economist and social scientist Richard Florida, a professor and head of the Martin Prosperity Institute at the Rotman School of Management at the University of Toronto. According to Florida, the creative class are a key driving force for economic development of post-industrial cities in the United States.						Florida describes the creative class as comprising 40 million workers (about 30 percent of the U.S. workforce). He breaks the class into two broad sections, derived from Standard Occupational Classification System codes:		In addition to these two main groups of creative people, the usually much smaller group of Bohemians is also included in the creative class.[1]		In his 2002 study, Florida concluded that the creative class would be the leading force of growth in the economy expected to grow by over 10 million jobs in the next decade, which would in 2012 equal almost 40% of the population.		The social theories advanced by Florida have sparked much debate and discussion. Florida's work proposes that a new or emergent class—or demographic segment made up of knowledge workers, intellectuals and various types of artists—is an ascendant economic force, representing either a major shift away from traditional agriculture- or industry-based economies or a general restructuring into more complex economic hierarchies.		The theses developed by Florida in various publications were drawn from, among other sources, U.S. Census Bureau demographic data, focusing first on economic trends and shifts apparent in major U.S. cities, with later work expanding the focus internationally.		A number of specific cities and regions (including California's Silicon Valley, Boston's Route 128, The Triangle in North Carolina, Austin, Seattle, Bangalore, Dublin and Sweden) have come to be identified with these economic trends. In Florida's publications, the same places are also associated with large Creative Class populations.		Florida argues that the creative class is socially relevant because of its members' ability to spur regional economic growth through innovation (2002).		Florida says that the creative class is a class of workers whose job is to create meaningful new forms (2002). It is composed of scientists and engineers, university professors, poets and architects, and also includes "people in design, education, arts, music and entertainment, whose economic function is to create new ideas, new technology and/or creative content" (Florida, 2002, p. 8). The designs of this group are seen as broadly transferable and useful. Another sector of the Creative Class includes positions that are knowledge intensive; these usually require a high degree of formal education (Florida, 2002). Examples of workers in this sector are health professionals and business managers, who are considered part of the sub-group called Creative Professionals. Their primary job is to think and create new approaches to problems. Creativity is becoming more valued in today's global society. Employers see creativity as a channel for self-expression and job satisfaction in their employees. About 38.3 million Americans and 30 percent of the American workforce identify themselves with the creative class. This number has increased by more than 10 percent in the past 20 years.		The creative class is also known for its departure from traditional workplace attire and behavior. Members of the creative class may set their own hours and dress codes in the workplace, often reverting to more relaxed, casual attire instead of business suits and ties. Creative class members may work for themselves and set their own hours, no longer sticking to the 9–5 standard. Independence is also highly regarded among the creative class and expected in the workplace (Florida, 2002).		The Creative Class is not a class of workers among many, but a group believed to bring economic growth to countries that can attract its members. The economic benefits conferred by the Creative Class include outcomes in new ideas, high-tech industry and regional growth. Even though the Creative Class has been around for centuries, the U.S. was the first large country to have a Creative Class dealing with information technology, in the 1960s and 1970s. In the 1960s less than five percent of the U.S. population was part of the Creative Class, a number that has risen to 26 percent. Seeing that having a strong Creative Class is vital in today's global economy, Europe is now almost equal with America's numbers for this group. Inter-city competition to attract members of the Creative Class has developed.		Following an empirical study across 90 nations, Rindermann et al. (2009)[2] argued that high-ability classes (or smart classes) are responsible for economic growth, stable democratic development, and positively valued political aspects (government effectiveness, rule of law, and liberty).		Florida's use of census and economic data, presented in works such as The Rise of the Creative Class (2002), Cities and the Creative Class (2004), and The Flight of the Creative Class (2007), as well as Bobos in Paradise by David Brooks (whose "bobos" roughly correspond to Florida's creative class), and NEO Power by Ross Honeywill (whose NEOs deliver a more sophisticated level of evidence), has shown that cities which attract and retain creative residents prosper, while those that do not stagnate. This research has gained traction in the business community, as well as among politicians and urban planners. Florida and other Creative Class theorists have been invited to meetings of the National Conference of Mayors and numerous economic development committees, such the Denver mayor's Task Force on Creative Spaces and Michigan governor Jennifer Granholm's Cool Cities Initiative.[3]		In Cities and the Creative Class, Florida devotes several chapters to discussion of the three main prerequisites of creative cities (though there are many additional qualities which distinguish creative magnets). For a city to attract the Creative Class, he argues, it must possess "the three 'T's": Talent (a highly talented/educated/skilled population), Tolerance (a diverse community, which has a 'live and let live' ethos), and Technology (the technological infrastructure necessary to fuel an entrepreneurial culture). In Rise of the Creative Class, Florida argues that members of the Creative Class value meritocracy, diversity and individuality, and look for these characteristics when they relocate (2002).		As Florida demonstrates in his books, Buffalo, New Orleans and Louisville are examples of cities which have tried to attract the Creative Class but, in comparison to cities which better exemplify the "three 'T's", have failed. Creative Class workers have sought out cities that better accommodate their cultural, creative, and technological needs, such as Chapel Hill, San Francisco, Washington, D.C., Austin, Seattle, Toronto, Ontario and Portland, Oregon. Florida also notes that Lexington and Milwaukee, Wisconsin have the ingredients to be a "leading city in a new economy".		The "Creativity Index" is another tool that Florida uses to describe how members of the Creative Class are attracted to a city. The Creativity Index includes four elements: "the Creative Class share of the workforce; innovation, measured as patents per capita; high tech industry, using the Milken Institute's widely accepted Tech Pole Index…; and diversity, measured by the Gay Index, a reasonable proxy for an area's openness" (2002, pp. 244–5). Using this index, Florida rates and ranks cities in terms of innovative high-tech centers, with San Francisco being the highest ranked (2002).		Florida and others have found a strong correlation between those cities and states that provide a more tolerant atmosphere toward culturally unconventional people, such as gays, artists, and musicians (exemplified by Florida's "Gay Index" and "Bohemian Index" developed in The Rise of the Creative Class), and the numbers of Creative Class workers that live and move there (2002).		Research involving the preferences and values of this new socioeconomic class has shown that where people choose to live can no longer be predicted according to conventional industrial theories (such as "people will go to where the jobs/factories are"). Creative workers are no longer bound by physical products, rather working with intellectual products. Their migration to metropolitan urban areas where creative work is available is more due to the attraction of leisure life and community rather than actual work. Although the Creative Class works towards the globalization of progressive and innovative ideas and products, they can also be considered to value local community and local autonomy. Sociologists and urban theorists have noted a gradual and broad shift of values over the past decade. Creative workers are looking for cultural, social, and technological climates in which they feel they can best "be themselves".		"The main assumption underlying this approach is that creative workers seek creative outlets in all aspects of their lives and therefore migrate to cities that actively support their preferred lifestyles" (Donegan et al., 2008, p. 181).[4]		Each year Florida and the Martin Prosperity Institute release the Global Creativity Index, an international study of nations, ranking countries on the 3Ts of economic development - talent, technology, and tolerance. "The GCI is a broad-based measure for advanced economic growth and sustainable prosperity based on the 3Ts of economic development - talent, technology, and tolerance. It rates and ranks 139 nations worldwide on each of these dimensions and on our overall measure of creativity and prosperity" (Florida et al., 2015).[5] The GCI takes into account the diversity of geographical locations noting their openness as the means for progressive ideas to prosper. "Tolerance and openness to diversity is part and parcel of the broad cultural shift toward post-materialist values... Tolerance—or, broadly speaking openness to diversity—provides an additional source of economic advantage that works alongside technology and talent" (Florida, 2012, p. 233).[6] Diversity allows these locations to attract creative individuals and therefore stimulate economic growth. The findings from the 2015 GCI measured 139 countries on their creativity and prosperity. Ranked number one on the 2015 GCI is Australia.		The diverse and individualistic lifestyles enjoyed by the Creative Class involve active participation in a variety of experiential activities. Florida (2002) uses the term [Street Level Culture] to define this kind of stimulation. Street Level Culture may include a "teeming blend of cafes, sidewalk musicians, and small galleries and bistros, where it is hard to draw the line between participant and observer, or between creativity and its creators" (p. 166). Members of the Creative Class enjoy a wide variety of activities (e.g., traveling, antique shopping, bike riding, and running) that highlight the collective interest in being participants and not spectators (Florida, 2002).		Numerous studies have found fault with the logic or empirical claims of Florida's Creative Class theory. This body of critical empirical research demonstrates how the Creative Class thesis, and the associated creative city policy prescriptions, in fact exacerbate social and economic inequalities in cities in North America,[7][8][9][10][11][12] Europe,[13][14] Australia,[15] and Asia.[16][17] Jamie Peck argues that the Creative Class theory offers no causal mechanism and suffers from circular logic.[18] John Montgomery writes that "what Florida has devised is a set of indices which simply mirror more fundamental truths about creative milieux or dynamic cities."[19] Montgomery also disagrees with the cities that Florida designates as most creative, writing that London, not Manchester and Leicester, should be one of the top in the U.K. A critique of Florida's research and theoretical framework has been developed by Matteo Pasquinelli (2006) in the context of Italian Operaismo.		Scholars in the disciplines of economics, geography, sociology, and related social sciences have challenged Florida's conception of the "creative class", particularly for the perceived fuzziness of the concept and the lack of analytical precision.[20][21][22] A number of studies have found problems with Florida's statistical indices.[23][24][25][26] Hoyman and Faricy, using Florida's own indices, find no statistical evidence that cities with higher proportions of Creative Class workers correlated with any type of economic growth from 1990–2004.[27] By using metropolitan areas as the unit of analysis, the high degree of socio-spatial variation across the metropolitan region is ignored. Studies and popular accounts have questioned whether the creative class is more likely to live in the homogenous, low-density suburban periphery.[28][29][30]		Social scientists have also identified problems with the occupational composition of the creative class. Economic geographer Stefan Kratke challenges the inclusion of financial and real estate professionals within the creative class on two accounts: 1) these individuals played a decisive role as the "dealer class" in the ongoing financial crises, and therefore cannot be considered a basis for sustainable urban and regional economic growth;[31] and 2) the financial and real estate industries (especially in headquarter cities) are economically significant regional/urban players only because they are largely "reliant on inflows of wealth created by productive activities in other regions."[32] Moreover, Kratke argues that the "political class" is also ill-suited to be included within creative class, as they are, in many cases, implicated in neoliberal financial deregulation and the rise in highly unstable urban and regional growth regimes[33] evident through real estate bubbles across the United States and in other countries. In "Urban Development and the Politics of the Creative Class", Ann Markusen argues that workers qualified as being in the Creative Class have no concept of group identity, nor are they in occupations that are inherently creative.[23] Markusen also notes that the definition of the Creative Class is based largely on educational attainment, suggesting that Florida's indices become insignificant after controlling for education. Markusen argues that Florida "does not seem to understand the nature of the occupational statistics he uses" and calls for the major occupational groups to be disaggregated.[34] She questions the inclusion of particular occupations within these broad categories such as claim adjusters, funeral directors, tax collectors, yet argues that "[t]hese occupations may indeed be creative, but so too are airplane pilots, ship engineers, millwrights, and tailors – all of whom are uncreative in Florida's tally."[28] Moreover, it is questioned whether human creativity can be conflated with education since "[p]eople at all levels of education exercise considerable inventiveness."[35]		Research shows that economic growth is experienced when the significance of scientifically/technologically and artistically creative workers is taken into account, but this macro-level conclusion can be drawn without Florida's creative class theory, which provides more of an "affirmation of contemporary class relations."[36] Other scholars have criticized the very basis for Florida's definition of "creativity" which many argue is conceived of narrowly and is only valued for the potential for financial and economic growth.[37][38] Studies have too questioned Florida's argument that jobs and economic growth follow the creative class, and the migration patterns of the creative class have been challenged.[39][40] Rather than validating Florida's causal logic that attracting the creative class will lead to economic growth, empirical research shows that successful regions pull and maintain human capital.[23][41]		The creative class thesis—and Richard Florida himself—have been criticized for what appears to be a change in Florida's prognosis for America's ailing Rust Belt cities. Florida's message was so quickly and enthusiastically adopted by cities because he argued that any city had the potential to become a vibrant, creative city with the right infrastructure investments, policies, and consulting advice.[42] A 2009 article, "The Ruse of the Creative Class", questions Florida's costly speaking engagements in struggling industrial cities in which he offered optimistic prognoses[43]—and his more recent pronouncements that many American cities may never be saved in the wake of the Great Recession.[44] The creative class thesis has also drawn criticisms for relying on inner city property development, gentrification, and urban labor markets reliant on low-wage service workers, particularly in the hospitality industry.[45][42][46][11][47][48][49][50][51][52] Florida has called for service workers' wages to rise.[53]		Creative Class Struggle, a Toronto-based collective, has brought these criticisms outside academic circles, challenging Florida's Creative Class theories as well as their widespread adoption into urban policy. The group manages an online clearinghouse for information about creative city strategies and policies, publishes a newsletter and other materials, and works to engage the media and public in critical discussion.[54] In June 2009, Creative Class Struggle and art magazine Fuse organized a public forum in Toronto to debate these issues.[55]		
Labour economics seeks to understand the functioning and dynamics of the markets for wage labour.		Labour markets or job markets function through the interaction of workers and employers. Labour economics looks at the suppliers of labour services (workers) and the demanders of labour services (employers), and attempts to understand the resulting pattern of wages, employment, and income.		In economics, labour is a measure of the work done by human beings. It is conventionally contrasted with such other factors of production as land and capital. There are theories which have developed a concept called human capital (referring to the skills that workers possess, not necessarily their actual work).						There are two sides to labour economics. Labour economics can generally be seen as the application of microeconomic or macroeconomic techniques to the labour market. Microeconomic techniques study the role of individuals and individual firms in the labour market. Macroeconomic techniques look at the interrelations between the labour market, the goods market, the money market, and the foreign trade market. It looks at how these interactions influence macro variables such as employment levels, participation rates, aggregate income and gross domestic product.		The labour force is defined as the number of people of working age, who are either employed or actively looking for work. The participation rate is the number of people in the labour force divided by the size of the adult civilian noninstitutional population (or by the population of working age that is not institutionalized). The non-labour force includes those who are not looking for work, those who are institutionalised such as in prisons or psychiatric wards, stay-at home spouses, children, and those serving in the military. The unemployment level is defined as the labour force minus the number of people currently employed. The unemployment rate is defined as the level of unemployment divided by the labour force. The employment rate is defined as the number of people currently employed divided by the adult population (or by the population of working age). In these statistics, self-employed people are counted as employed.		Variables like employment level, unemployment level, labour force, and unfilled vacancies are called stock variables because they measure a quantity at a point in time. They can be contrasted with flow variables which measure a quantity over a duration of time. Changes in the labour force are due to flow variables such as natural population growth, net immigration, new entrants, and retirements from the labour force. Changes in unemployment depend on inflows made up of non-employed people starting to look for jobs and of employed people who lose their jobs and look for new ones, and outflows of people who find new employment and of people who stop looking for employment. When looking at the overall macroeconomy, several types of unemployment have been identified, including:		Neoclassical economists view the labour market as similar to other markets in that the forces of supply and demand jointly determine price (in this case the wage rate) and quantity (in this case the number of people employed).		However, the labour market differs from other markets (like the markets for goods or the financial market) in several ways. In particular, the labour market may act as a non-clearing market. While according to neoclassical theory most markets quickly attain a point of equilibrium without excess supply or demand, this may not be true of the labour market: it may have a persistent level of unemployment. Contrasting the labour market to other markets also reveals persistent compensating differentials among similar workers.		Models that assume perfect competition in the labour market, as discussed below, conclude that workers earn their marginal product of labour.[1]		Households are suppliers of labour. In microeconomic theory, people are assumed to be rational and seeking to maximize their utility function. In the labour market model, their utility function expresses trade-offs in preference between leisure time and income from time used for labour. However, they are constrained by the hours available to them.		Let w denote the hourly wage, k denote total hours available for labour and leisure, L denote the chosen number of working hours, π denote income from non-labour sources, and A denote leisure hours chosen. The individual's problem is to maximise utility U, which depends on total income available for spending on consumption and also depends on time spent in leisure, subject to a time constraint, with respect to the chooses of labour time and leisure time:		This is shown in the graph below, which illustrates the trade-off between allocating time between leisure activities and income-generating activities. The linear constraint indicates that every additional hour of leisure undertaken requires the loss of an hour of labour and thus of the fixed amount of goods that that labour's income could purchase. Individuals must choose how much time to allocate to leisure activities and how much to working. This allocation decision is informed by the indifference curve labelled IC1. The curve indicates the combinations of leisure and work that will give the individual a specific level of utility. The point where the highest indifference curve is just tangent to the constraint line (point A), illustrates the optimum for this supplier of labour services.		If consumption is measured by the value of income obtained, this diagram can be used to show a variety of interesting effects. This is because the absolute value of the slope of the budget constraint is the wage rate. The point of optimisation (point A) reflects the equivalency between the wage rate and the marginal rate of substitution[2] of leisure for income (the absolute value of the slope of the indifference curve). Because the marginal rate of substitution of leisure for income is also the ratio of the marginal utility of leisure (MUL) to the marginal utility of income (MUY), one can conclude:		where Y is total income and the right side is the wage rate.		If the wage rate increases, this individual's constraint line pivots up from X,Y1 to X,Y2. He/she can now purchase more goods and services. His/her utility will increase from point A on IC1 to point B on IC2. To understand what effect this might have on the decision of how many hours to work, one must look at the income effect and substitution effect.		The wage increase shown in the previous diagram can be decomposed into two separate effects. The pure income effect is shown as the movement from point A to point C in the next diagram. Consumption increases from YA to YC and – since the diagram assumes that leisure is a normal good – leisure time increases from XA to XC. (Employment time decreases by the same amount as leisure increases.)		But that is only part of the picture. As the wage rate rises, the worker will substitute away from leisure and into the provision of labour—that is, will work more hours to take advantage of the higher wage rate, or in other words substitute away from leisure because of its higher opportunity cost. This substitution effect is represented by the shift from point C to point B. The net impact of these two effects is shown by the shift from point A to point B. The relative magnitude of the two effects depends on the circumstances. In some cases, such as the one shown, the substitution effect is greater than the income effect (in which case more time will be allocated to working), but in other cases the income effect will be greater than the substitution effect (in which case less time is allocated to working). The intuition behind this latter case is that the individual decides that the higher earnings on the previous amount of labour can be "spent" by purchasing more leisure.		If the substitution effect is greater than the income effect, the labour supply curve (in the adjacent diagram) will slope upwards to the right, as it does at point E for example. This individual will continue to increase his supply of labour services as the wage rate increases up to point F where he is working HF hours (each period of time). Beyond this point he will start to reduce the amount of labour hours he supplies (for example at point G he has reduced his work hours to HG) because the income effect of the wage rate has come to dominate the substitution effect. Where the supply curve is sloping upwards to the right (showing a positive wage elasticity), the substitution effect is greater than the income effect. Where it slopes upwards to the left (showing a negative wage elasticity), the income effect is greater than the substitution effect. The direction of slope may change more than once for some individuals, and the labour supply curve is different for different individuals.		Other variables that affect the labour supply decision, and can be readily incorporated into the model, include taxation, welfare, work environment, and income as a signal of ability or social contribution.		A firm's labour demand is based on its marginal physical product of labour (MPPL). This is defined as the additional output (or physical product) that results from an increase of one unit of labour (or from an infinitesimal increase in labour). (See also Production theory basics.)		Labour demand is a derived demand; that is, hiring labour is not desired for its own sake but rather because it aids in producing output, which contributes to an employer's revenue and hence profits. The demand for an additional amount of labour depends on the Marginal Revenue Product (MRP) and the marginal cost (MC) of the worker. With a perfectly competitive goods market, the MRP is calculated by multiplying the price of the end product or service by the Marginal Physical Product of the worker. If the MRP is greater than a firm's Marginal Cost, then the firm will employ the worker since doing so will increase profit. The firm only employs however up to the point where MRP=MC, and not beyond, in neoclassical economic theory.[2]		The MRP of the worker is affected by other inputs to production with which the worker can work (e.g. machinery), often aggregated under the term "capital". It is typical in economic models for greater availability of capital for a firm to increase the MRP of the worker, all else equal. Education and training are counted as "human capital". Since the amount of physical capital affects MRP, and since financial capital flows can affect the amount of physical capital available, MRP and thus wages can be affected by financial capital flows within and between countries, and the degree of capital mobility within and between countries.[3]		According to neoclassical theory, over the relevant range of outputs, the marginal physical product of labour is declining (law of diminishing returns). That is, as more and more units of labour are employed, their additional output begins to decline.		The marginal revenue product of labour can be used as the demand for labour curve for this firm in the short run. In competitive markets, a firm faces a perfectly elastic supply of labour which corresponds with the wage rate and the marginal resource cost of labour (W = SL = MFCL). In imperfect markets, the diagram would have to be adjusted because MFCL would then be equal to the wage rate divided by marginal costs. Because optimum resource allocation requires that marginal factor costs equal marginal revenue product, this firm would demand L units of labour as shown in the diagram.		The demand for labour of this firm can be summed with the demand for labour of all other firms in the economy to obtain the aggregate demand for labour. Likewise, the supply curves of all the individual workers (mentioned above) can be summed to obtain the aggregate supply of labour. These supply and demand curves can be analysed in the same way as any other industry demand and supply curves to determine equilibrium wage and employment levels.		Wage differences exist, particularly in mixed and fully/partly flexible labour markets. For example, the wages of a doctor and a port cleaner, both employed by the NHS, differ greatly. There are various factors concerning this phenomenon. This includes the MRP of the worker. A doctor's MRP is far greater than that of the port cleaner. In addition, the barriers to becoming a doctor are far greater than that of becoming a port cleaner. To become a doctor takes a lot of education and training which is costly, and only those who excel in academia can succeed in becoming doctors. The port cleaner however requires relatively less training. The supply of doctors is therefore significantly less elastic than that of port cleaners. Demand is also inelastic as there is a high demand for doctors and medical care is a necessity, so the NHS will pay higher wage rates to attract the profession.		Some labour markets have a single employer and thus do not satisfy the perfect competition assumption of the neoclassical model above. The model of a monopsonistic labour market gives a lower quantity of employment and a lower equilibrium wage rate than does the competitive model.		In many real-life situations the assumption of perfect information is unrealistic. An employer does not necessarily know how hard worker are working or how productive they are. This provides an incentive for workers to shirk from providing their full effort – since it is difficult for the employer to identify the hard-working and the shirking employees, there is no incentive to work hard and productivity falls overall, leading to the hiring of more workers and a lower unemployment rate.		One solution used recently[when?] – stock options – grants employees the chance to benefit directly from a firm's success. However, this solution has attracted criticism as executives with large stock-option packages have been suspected of acting to over-inflate share values to the detriment of the long-run welfare of the firm. Another solution, foreshadowed by the rise of temporary workers in Japan and the firing of many of these workers in response to the financial crisis of 2008, is more flexible job- contracts and -terms that encourage employees to work less than full-time by partially compensating for the loss of hours, relying on workers to adapt their working time in response to job requirements and economic conditions instead of the employer trying to determine how much work is needed to complete a given task and overestimating.[citation needed]		Another aspect of uncertainty results from the firm's imperfect knowledge about worker ability. If a firm is unsure about a worker's ability, it pays a wage assuming that the worker's ability is the average of similar workers. This wage undercompensates high-ability workers and may drive them away from the labour market. Such a phenomenon, called adverse selection, can sometimes lead to market collapse.[4]		There are many ways to overcome adverse selection in labour market. One important mechanism is called signalling, pioneered by Michael Spence.[5] In his classical paper on job signalling, Spence showed that even if formal education does not increase productivity, high-ability workers may still acquire it just to signal their abilities. Employers can then use education as a signal to infer worker ability and pay higher wages to better-educated workers. It may appear to an external observer that education has raised the marginal product of labour, without this necessarily being true.		One of the major research achievements of the 1990-2010 period was the development of a framework with dynamic search, matching, and bargaining.[6]		At the micro level, one sub-discipline eliciting increased attention in recent decades is analysis of internal labour markets, that is, within firms (or other organisations), studied in personnel economics from the perspective of personnel management. By contrast, external labour markets "imply that workers move somewhat fluidly between firms and wages are determined by some aggregate process where firms do not have significant discretion over wage setting."[7] The focus is on "how firms establish, maintain, and end employment relationships and on how firms provide incentives to employees," including models and empirical work on incentive systems and as constrained by economic efficiency and risk/incentive tradeoffs relating to personnel compensation.[8]		Many sociologists, political economists, and heterodox economists claim that labour economics tends to lose sight of the complexity of individual employment decisions.[citation needed] These decisions, particularly on the supply side, are often loaded with considerable emotional baggage and a purely numerical analysis can miss important dimensions of the process, such as social benefits of a high income or wage rate regardless of the marginal utility from increased consumption or specific economic goals.		From the perspective of mainstream economics, neoclassical models are not meant to serve as a full description of the psychological and subjective factors that go into a given individual's employment relations, but as a useful approximation of human behaviour in the aggregate, which can be fleshed out further by the use of concepts such as information asymmetry, transaction costs, contract theory etc.		Also missing from most labour market analyses is the role of unpaid labour such as unpaid internships where workers with little or no experience are allowed to work a job without pay so that they can gain experience in a particular profession. Even though this type of labour is unpaid it can nevertheless play an important part in society if not abused by employers. The most dramatic example is child raising. However, over the past 25 years an increasing literature, usually designated as the economics of the family, has sought to study within household decision making, including joint labour supply, fertility, child raising, as well as other areas of what is generally referred to as home production.[9]		The labour market, as institutionalised under today's market economic systems, has been criticised,[10] especially by both mainstream socialists and anarcho-syndicalists,[11][12][13][14] who utilise the term wage slavery[15][16] as a pejorative for wage labour. Socialists draw parallels between the trade of labour as a commodity and slavery. Cicero is also known to have suggested such parallels.[17]		According to Noam Chomsky, analysis of the psychological implications of wage slavery goes back to the Enlightenment era. In his 1791 book On the Limits of State Action, classical liberal thinker Wilhelm von Humboldt explained how "whatever does not spring from a man's free choice, or is only the result of instruction and guidance, does not enter into his very nature; he does not perform it with truly human energies, but merely with mechanical exactness" and so when the labourer works under external control, "we may admire what he does, but we despise what he is."[18] Both the Milgram and Stanford experiments have been found useful in the psychological study of wage-based workplace relations.[19]		The American philosopher John Dewey posited that until "industrial feudalism" is replaced by "industrial democracy," politics will be "the shadow cast on society by big business".[20] Thomas Ferguson has postulated in his investment theory of party competition that the undemocratic nature of economic institutions under capitalism causes elections to become occasions when blocs of investors coalesce and compete to control the state.[21]		As per anthropologist David Graeber, the earliest wage labour contracts we know about were in fact contracts for the rental of chattel slaves (usually the owner would receive a share of the money, and the slave, another, with which to maintain his or her living expenses.) Such arrangements, according to Graeber, were quite common in New World slavery as well, whether in the United States or Brazil. C. L. R. James argued that most of the techniques of human organisation employed on factory workers during the industrial revolution were first developed on slave plantations.[22]		Additionally, Marxists posit that labour-as-commodity, which is how they regard wage labour,[23] provides an absolutely fundamental point of attack against capitalism.[24] "It can be persuasively argued," noted one concerned philosopher, "that the conception of the worker's labour as a commodity confirms Marx's stigmatisation of the wage system of private capitalism as 'wage-slavery;' that is, as an instrument of the capitalist's for reducing the worker's condition to that of a slave, if not below it."[25]		
A corporate collapse typically involves the insolvency or bankruptcy of a major business enterprise. A corporate scandal involves alleged or actual unethical behavior by people acting within or on behalf of a corporation. Many recent corporate collapses and scandals have involved false or inappropriate accounting of some sort (see list at accounting scandals).						The following list of corporations involved major collapses, through the risk of job losses or size of the business, and meant entering into insolvency or bankruptcy, or being nationalised or requiring a non-market loan by a government.		Investors were paid returns out of their own money or that of other investors rather than from profits.		Madoff told his sons about his scheme and they reported him to the SEC. He was arrested the next day.		
Labour brokering is a form of outsourcing practiced in South Africa (and formerly practiced in Namibia, where it was known as labour hire) in which companies contract labour brokers to provide them with casual labour. Labour brokers are different from recruitment agencies in that labour brokers handle almost all aspects of the worker's employment (including interviews, recruitment, HR, admin, payroll, transport, etc.), whereas recruitment agencies are only responsible for sourcing candidates for employment. In essence, rather than a company hiring a worker, it hires a labour broker who hires the worker in its stead.[1]		The current statutory definition of a labour broker under South African law, as of 1 March 2009, is "any natural person who conducts or carries on any business whereby such person for reward provides a client of such business with other persons to render a service or perform work for such client, or procures such other persons for the client, for which services or work such other persons are remunerated by such person".[2]		In 2008, Namibia passed a law banning the practice of "labour hire", the Namibian term for labour brokering.[3] The Congress of South African Trade Unions (COSATU), South Africa's largest trade union federation, has called for the abolition of labour brokering in South Africa. COSATU argues that labour brokers are responsible for the increasing casualisation of labour in South Africa. Currently about 30% of the South African workforce is casualised. As casual workers receive much lower salaries than permanent employees, and as they have much lower job security, COSATU argues that labour brokering, contrary to the claims of its supporters, does not create meaningful employment and that it, in fact, violates the rights of workers.[4]		COSATU called for a national one day general strike against labour brokering and the proposed Gauteng e-toll system for 7 March 2012. On that day, the trade union federation mobilised tens of thousands of people, mostly workers, against labour brokering.[5] Nonetheless, the ruling African National Congress (ANC) dug in its heels; with Minister of Labour Affairs Mildred Oliphant stating that labour brokering "is here to stay".[6]		
Job fraud refers to fraudulent or deceptive activity or representation on the part of an employee or prospective employee toward an employer. It is not to be confused with employment fraud, where an employer scams job seekers or fails to pay wages for work performed. There are several types of job frauds that employees or potential employees commit against employers. While some may be illegal under jurisdictional laws, others do not violate law but may be held by the employer against the employee or applicant.						Résumé fraud or application fraud refers to any act that involves intentionally providing fictitious, exaggerated, or otherwise misleading information on a job application or résumé in hopes of persuading a potential employer to hire an applicant for a job for which they may be unqualified or less qualified than other applicants.[1] Depending on the nature of the offense, the type of job, and the jurisdiction where it occurs, such an act may or may not be a violation of criminal law. In any case, knowingly providing inaccurate information to an employer or potential employer, if discovered by the employer, is almost always grounds for immediate dismissal from the job or else denial of that job.		Almost half (48%) of organizations with fewer than 100 staff experienced problems with vetted employees.		39% of UK organizations have experienced a situation where their vetting procedures have allowed an employee to be hired who was later found to have lied or misrepresented themselves in their application. [2]		Younger, more junior people are more likely to have a discrepancy on their CV. Someone in a junior administrative position is 23% more likely to have a discrepancy on their CV than in a managerial role. An applicant aged under 20 is 26% more likely to have a discrepancy than a 51- to 60-year-old.[3]		Neither men nor women are statistically more likely to have a discrepancy on their CV (X-squared = 0.56, df = 1, p-value = 0.46): 24% of applications submitted by women have a discrepancy compared to 26% of those for men.[2]		Graduates have marginally fewer discrepancies: 13% of their CVs contain a discrepancy compared to 17% of non-graduates.[4]		
According to Bratton and Kacmar's article, The Dark Side of Impression Management, extreme careerism is the propensity to pursue career advancement, power, and prestige through any positive or negative non-performance based activity that is deemed necessary. These "non-performance" based activities are activities in which an employee can easily manipulate the people whom he is trying to impress.[1] Extreme careerism has become increasingly common in the business and organisational world in the 1990s and 2000s.[citation needed] In the United States, seventeen additional workdays have been added to the calendar since 1994.[citation needed]		Cultural factors influence how careerists view their occupational goals. How an individual interprets the term "career" can distinguish between extreme careerists and those who can leave their career at the door when they come home at night.		Schein[2] identifies three important aspects of cultural environments and careerism:		The term "career" was once[when?] used for the purposes of status. Career was thought of[by whom?] as a long-term job opportunity, that many, in fact would hold until retirement. In the United States especially after World War II, those who were lucky enough[citation needed] to find a career would stay with the same organization for decades. A career was seen as an upper middle class, professional service, identified as the work of a doctor, lawyer, investor, banker or teacher. "Occupations" were seen as lower-class human services jobs, such as those of a taxi driver, clerk, secretary, or waste manager. These "jobs" were not held in the high regard that "careers" were.		In the 2000s, the average American does not stay with the same company, business or organization until retirement.		In regards to commitment, an individual must rely and commit to the occupational setting, the family setting, and to his own setting.[citation needed] Careerist must determine what is the most important factor in their lives.[dubious – discuss] To the career extremist, it is the occupational setting. Some organizations require the individual to be in "work-mode" at all times, while others believe that family time is more important. Most Latin American countries value family and personal time, whereas the United States pushes for a stronger workforce in regards to careerism.[citation needed] In the United States this is mainly because of the push for education.[citation needed] Currently[when?] the United States ranks 10th among industrial countries for percentage of adults with college degrees. With this push in education many people have better careers and are then able to have the choice of family matters, personal matter, or career matters. Even though in the United States careerism is very important, family life is also a huge part of the culture. Many people start their families even while in school, then they begin their careers. Recently[when?] the importance of family matters and career matters has evolved and is becoming more and more tied together.[citation needed]		Cultures exert pressure and determine what career motives are acceptable and how their success is measured. Vyacheslav Molotov noted the role of careerism in the Soviet government in the 1930s: "Сыграл свою роль наш партийный карьеризм" [Party-oriented careerism played out its own role].[3]		Extreme careerists measure success by acknowledgements through praise and material possessions, whether it be a new office, a raise or a congratulations in front of an individual's colleagues: notice is success. In the U.S. there is an extreme drive of personal success[citation needed] and those who are ambitious are the ones who gain the power in an organization.[citation needed]		
A profession is a vocation founded upon specialized educational training, the purpose of which is to supply disinterested objective counsel and service to others, for a direct and definite compensation, wholly apart from expectation of other business gain.[1] The term is a truncation of the term "liberal profession", which is, in turn, an Anglicization of the French term "profession libérale". Originally borrowed by English users in the 19th century, it has been re-borrowed by international users from the late 20th, though the (upper-middle) class overtones of the term do not seem to survive retranslation: "liberal professions" are, according to the European Union's Directive on Recognition of Professional Qualifications (2005/36/EC) "those practiced on the basis of relevant professional qualifications in a personal, responsible and professionally independent capacity by those providing intellectual and conceptual services in the interest of the client and the public".						Medieval and early modern tradition recognized only three professions: divinity, medicine, and law [2] [3] – the so-called "learned professions".[4]		Major milestones which may mark an occupation being identified as a profession include:[3]		Applying these milestones to the historical sequence of development in the United States shows surveying achieving professional status first (note that George Washington, Thomas Jefferson, and Abraham Lincoln all worked as land surveyors before entering politics[citation needed]), followed by medicine, actuarial science, law, dentistry, civil engineering, logistics, architecture and accounting.[5]		With the rise of technology and occupational specialization in the 19th century, other bodies began to claim professional status: pharmacy, veterinary medicine, psychology, nursing, teaching, librarianship, optometry and social work, each of which could claim, using these milestones, to have become professions by 1900.[6]		Just as some professions rise in status and power through various stages, others may decline.[citation needed] Disciplines formalized more recently, such as architecture, now have equally long periods of study associated with them.[7]		Although professions may enjoy relatively high status and public prestige, not all professionals earn high salaries, and even within specific professions there exist significant inequalities of compensation; in law, for example, a corporate/insurance defense lawyer working on a billable-hour basis may earn several times what a prosecutor or public defender earns.		A profession arises when any trade or occupation transforms itself through "the development of formal qualifications based upon education, apprenticeship, and examinations, the emergence of regulatory bodies with powers to admit and discipline members, and some degree of monopoly rights."[8]		Originally, any regulation of the professions was self-regulation through bodies such as the College of Physicians or the Inns of Court. With the growing role of government, statutory bodies have increasingly taken on this role, their members being appointed either by the profession or (increasingly) by government. Proposals for the introduction or enhancement of statutory regulation may be welcomed by a profession as protecting clients and enhancing its quality and reputation, or as restricting access to the profession and hence enabling higher fees to be charged. It may be resisted as limiting the members' freedom to innovate or to practice as in their professional judgement they consider best.		An example was in 2008, when the British government proposed wide statutory regulation of psychologists. The inspiration for the change was a number of problems in the psychotherapy field, but there are various kinds of psychologist including many who have no clinical role and where the case for regulation was not so clear. Work psychology brought especial disagreement, with the British Psychological Society favoring statutory regulation of "occupational psychologists" and the Association of Business Psychologists resisting the statutory regulation of "business psychologists" – descriptions of professional activity which it may not be easy to distinguish.		Besides regulating access to a profession, professional bodies may set examinations of competence and enforce adherence to an ethical code. There may be several such bodies for one profession in a single country, an example being the accountancy bodies of the United Kingdom (ACCA, CAI, CIMA, CIPFA, ICAEW and ICAS), all of which have been given a Royal Charter, although their members are not necessarily considered to hold equivalent qualifications, and which operate alongside further bodies (AAPA, IFA, CPAA). Another example of a regulatory body that governs a profession is the Hong Kong Professional Teachers Union, which governs the conduct, rights, obligations and duties of salaried teachers working in educational institutions in Hong Kong.		Typically, individuals are required by law to be qualified by a local professional body before they are permitted to practice in that profession. However, in some countries, individuals may not be required by law to be qualified by such a professional body in order to practice, as is the case for accountancy in the United Kingdom (except for auditing and insolvency work which legally require qualification by a professional body). In such cases, qualification by the professional bodies is effectively still considered a prerequisite to practice as most employers and clients stipulate that the individual hold such qualifications before hiring their services. For example, in order to become a fully qualified teaching professional in Hong Kong working in a state or government-funded school, one needs to have successfully completed a Postgraduate Diploma in Education ("PGDE") or a bachelor's degree in Education ("BEd") at an approved tertiary educational institution or university. This requirement is set out by the Educational Department Bureau of Hong Kong, which is the governmental department that governs the Hong Kong education sector.		Professions tend to be autonomous, which means they have a high degree of control of their own affairs: "professionals are autonomous insofar as they can make independent judgments about their work".[9] This usually means "the freedom to exercise their professional judgement."[10]		However, it also has other meanings. "Professional autonomy is often described as a claim of professionals that has to serve primarily their own interests...this professional autonomy can only be maintained if members of the profession subject their activities and decisions to a critical evaluation by other members of the profession "[11] The concept of autonomy can therefore be seen to embrace not only judgement, but also self-interest and a continuous process of critical evaluation of ethics and procedures from within the profession itself.		One major implication of professional autonomy is the traditional ban on corporate practice of the professions, especially accounting, architecture, medicine, and law. This means that in many jurisdictions, these professionals cannot do business through regular for-profit corporations and raise capital rapidly through initial public offerings or flotations. Instead, if they wish to practice collectively they must form special business entities such as partnerships or professional corporations, which feature (1) reduced protection against liability for professional negligence and (2) severe limitations or outright prohibitions on ownership by non-professionals. The obvious implication of this is that all equity owners of the professional business entity must be professionals themselves. This avoids the possibility of a non-professional owner of the firm telling a professional how to do his or her job and thereby protects professional autonomy. The idea is that the only non-professional person who should be telling the professional what to do is the client; in other words, professional autonomy preserves the integrity of the two-party professional-client relationship. But because professional business entities are effectively locked out of the stock market, they tend to grow relatively slowly compared to public corporations.		Professions enjoy a high social status, regard and esteem conferred upon them by society.[12][13] This high esteem arises primarily from the higher social function of their work, which is regarded as vital to society as a whole and thus of having a special and valuable nature. All professions involve technical, specialized and highly skilled work often referred to as "professional expertise."[14] Training for this work involves obtaining degrees and professional qualifications (see Licensure) without which entry to the profession is barred (occupational closure). Updating skills through continuing education is required through training.		All professions have power.[15] This power is used to control its own members, and also its area of expertise and interests. A profession tends to dominate, police and protect its area of expertise and the conduct of its members, and exercises a dominating influence over its entire field which means that professions can act monopolist,[16] rebuffing competition from ancillary trades and occupations, as well as subordinating and controlling lesser but related trades.[17] A profession is characterized by the power and high prestige it has in society as a whole. It is the power, prestige and value that society confers upon a profession that more clearly defines it. The power of professions has led to them being referred to as conspiracies against the laity. On the other hand, professionals acquire some of their power and authority in organizations from their expertise and knowledge. As such they can bend rules, reduce bureaucratic inertia and increase problem solving and adaptability.[18]		There is considerable agreement about defining the characteristic features of a profession. They have a "professional association, cognitive base, institutionalized training, licensing, work autonomy, colleague control... (and) code of ethics",[19] to which Larson then also adds, "high standards of professional and intellectual excellence," (Larson, p. 221) that "professions are occupations with special power and prestige", (Larson, p.x) and that they comprise "an exclusive elite group," (Larson, p. 20) in all societies. Members of a profession have also been defined as "workers whose qualities of detachment, autonomy, and group allegiance are more extensive than those found among other groups...their attributes include a high degree of systematic knowledge; strong community orientation and loyalty; self-regulation; and a system of rewards defined and administered by the community of workers."[20]		A profession has been further defined as: "a special type of occupation...(possessing) corporate solidarity...prolonged specialized training in a body of abstract knowledge, and a collectivity or service orientation...a vocational sub-culture which comprises implicit codes of behavior, generates an esprit de corps among members of the same profession, and ensures them certain occupational advantages...(also) bureaucratic structures and monopolistic privileges to perform certain types of work...professional literature, legislation, etc." [21]		
In economics, a discouraged worker is a person of legal employment age who is not actively seeking employment or who does not find employment after long-term unemployment. This is usually because an individual has given up looking or has had no success in finding a job, hence the term "discouraged".		In other words, even if a person is still looking actively for a job, that person may have fallen out of the core statistics of unemployment rate after long-term unemployment and is therefore by default classified as "discouraged" (since the person does not appear in the core statistics of unemployment rate). In some cases, their belief may derive from a variety of factors including a shortage of jobs in their locality or line of work; discrimination for reasons such as age, race, sex, religion, sexual orientation, and disability; a lack of necessary skills, training, or experience; or, a chronic illness or disability.[1]		As a general practice, discouraged workers, who are often classified as marginally attached to the labor force, on the margins of the labor force, or as part of hidden unemployment, are not considered part of the labor force, and are thus not counted in most official unemployment rates—which influences the appearance and interpretation of unemployment statistics. Although some countries offer alternative measures of unemployment rate, the existence of discouraged workers can be inferred from a low employment-to-population ratio.						In the United States, a discouraged worker is defined as a person not in the labor force who wants and is available for a job and who has looked for work sometime in the past 12 months (or since the end of his or her last job if a job was held within the past 12 months), but who is not currently looking because of real or perceived poor employment prospects.[2][3][4]		The Bureau of Labor Statistics does not count discouraged workers as unemployed but rather refers to them as only "marginally attached to the labor force".[5][6][7] This means that the officially measured unemployment captures so-called "frictional unemployment" and not much else.[8] This has led some economists to believe that the actual unemployment rate in the United States is higher than what is officially reported while others suggest that discouraged workers voluntarily choose not to work.[9] Nonetheless, the U.S. Bureau of Labor Statistics has published the discouraged worker rate in alternative measures of labor underutilization under U-4 since 1994 when the most recent redesign of the CPS was implemented.[10][11]		The United States Department of Labor first began tracking discouraged workers in 1967 and found 500,000 at the time.[12] Today, In the United States, according to the U.S. Bureau of Labor Statistics as of April 2009, there are 740,000 discouraged workers.[13][14] There is an ongoing debate as to whether discouraged workers should be included in the official unemployment rate.[12] Over time, it has been shown that a disproportionate number of young people, blacks, Hispanics, and men make up discouraged workers.[15][16] Nonetheless, it is generally believed that the discouraged worker is underestimated because it does not include homeless people or those who have not looked for or held a job during the past twelve months and is often poorly tracked.[12][17]		According to the U.S. Bureau of Labor Statistics, the top five reasons for discouragement are the following:[18]		In Canada, discouraged workers are often referred to as hidden unemployed because of their behavioral pattern, and are often described as on the margins of the labour force.[19] Since the numbers of discouraged workers and of unemployed generally move in the same direction during the business cycle and the seasons (both tend to rise in periods of low economic activity and vice versa), some economists have suggested that discouraged workers should be included in the unemployment numbers because of the close association.[19]		The information on the number and composition of the discouraged worker group in Canada originates from two main sources. One source is the monthly Labour Force Survey (LFS), which identifies persons who looked for work in the past six months but who have since stopped searching. The other source is the Survey of Job Opportunities (SJO), which is much closer in design to the approach used in many other countries. In this survey, all those expressing a desire for work and who are available for work are counted, irrespective of their past job search activity.[19]		In Canada, while discouraged workers were once less educated than "average workers", they now have better training and education but still tend to be concentrated in areas of high unemployment.[1][19] Discouraged workers are not seeking a job for one of two reasons: labour market-related reasons (worker discouragement, waiting for recall to a former job or waiting for replies to earlier job search efforts) and personal and other reasons (illness or disability, personal or family responsibilities, going to school, and so on).[1]		Unemployment statistics published according to the ILO methodology may understate actual unemployment in the economy.[20] The EU statistical bureau EUROSTAT started publishing figures on discouraged workers in 2010.[21] According to the method used by EUROSTAT there are 3 categories that make up discouraged workers;		The first group are contained in the employed statistics of the European Labour Force Survey while the second two are contained in the inactive persons statistics of that survey. In 2012 there were 9.2 million underemployed part-time workers, 2.3 million jobless persons seeking a job but not immediately available for work, and 8.9 million persons available for work but not seeking it, an increase of 0.6 million for underemployed and 0.3 million for the two groups making up discouraged workers.[22]		If the discouraged workers and underemployed are added to official unemployed statistics Spain has the highest number real unemployed (8.4 Million), followed by Italy (6.4 Million), United Kingdom (5.5 Million), France (4.8 Million) and Germany (3.6 Million).		
Executive search (informally called headhunting) is a specialized recruitment service which organizations pay to seek out and recruit highly qualified candidates for senior-level and executive jobs (e.g., President, Vice-president, CEO). Headhunters may also seek out and recruit other highly specialized and/or skilled positions in organizations for which there is strong competition in the job market for the top talent, such as senior data analysts or computer programmers. The method usually involves commissioning a third-party organization, typically an executive search firm, but possibly a standalone consultant or consulting firm, to research the availability of suitable qualified candidates working for competitors or related businesses or organizations. Having identified a shortlist of qualified candidates who match the client's requirements, the executive search firm may act as an intermediary to contact the individual(s) and see if they might be interested in moving to a new employer. The executive search firm may also carry out initial screening of the candidate, negotiations on remuneration and benefits, and preparing the employment contract.						An executive search firm is a type of professional service firm that specializes in recruiting executives and other senior personnel for their client companies in various industries. Executive search agents/professionals typically have a wide range of personal contacts in their industry or field of specialty; detailed, specific knowledge of the area; and typically operate at the most senior level of executive positions. Executive search professionals are also involved throughout the hiring process, conducting detailed interviews and presenting candidates to clients selectively, when they feel the candidate meets all stated requirements and would fit into the culture of the hiring firm. Executive search firms typically have long-lasting relationships with clients spanning many years, and in such cases the suitability of candidates is paramount. It is also important that such firms operate with a high level of professionalism and confidentiality.		When corporate entities elect to use an outside executive search firm, it is usually because they lack the internal research resources, professional networks, or evaluative skills to properly recruit for themselves. Using an outside firm also allows the corporate entity the freedom of recruiting from competitors without doing so directly, and the ability to choose among candidates that would not be available through internal or passive sourcing methodologies. Executive search firms are national and international. Many specialize in a particular business industry sector. The contractual relationship between client and executive search firm falls into two broad categories: contingent and retained. Contingent recruiters are paid only upon the successful completion of the "search assignment." Retained recruiters are paid for the process, typically earning a recruiting fee in three stages based on the anticipated compensation of the executive.		High-end executive search firms get a retainer (up-front fee) to perform a specific search for a corporate officer or other senior executive position. Typically, retained searches tend to be for positions that pay upwards of US$150,000 and often far more. Search fees are typically 33.33% of the annual compensation of the recruited executive.[citation needed] Fee payments may be made in thirds, 1/3 of fee paid on initiation of the search, 1/3 paid thirty days later, and the final 1/3 paid thirty days later or upon placement of the candidate. Alternatively, a fixed fee may be established. Retained search firms provide a guarantee to do an assignment over if the hired candidate leaves before one year as long as there has not been a material change in the position requirements or management team.[citation needed] In a retained search, the fee is for the time and expertise of the search firm. The firm is employed to conduct the entire recruitment effort from startup until the candidate has started working.[citation needed]		Retained recruiters work for the organizations who are their clients, not for job candidates seeking employment, in some countries, such as the UK, recruiters are not legally permitted to charge candidates. In the U.S. job candidates may pay an up front retainer to a consulting or career counseling firms to assist them in their job search. Search firms generally commit to "off-limits" agreements. These agreements prevent a firm from approaching employees of their current clients as candidates for other clients (for instance, if a headhunter recruits the new CEO into Boeing, they will agree not to recommend Boeing executives to other companies). Since they act as management consultants working in the best interests of the clients for whom they conduct searches, it would be counterproductive to simultaneously remove talented executives from those client companies. Search firms may decline assignments from certain companies, in order to preserve their ability to recruit candidates from those companies. Some large search firms may insist on guarantees of a certain number or dollar value of searches before they will put an entire company "off-limits".		Another form of high-end executive search, delimited or engaged search, is often improperly categorized as retained search, although there are distinct differences. Similar to retained search firms, delimited/engaged search firms require an up-front fee before engaging the search. Unlike a conventional retainer, however, the delimited/engaged search commitment fee is refundable if the recruiter fails to achieve a hire or other deliverable specified in the contract. Moreover, the delimited/engaged search commitment fee does not follow the typical 1/3, 1/3, 1/3 model of retainers, but rather is a relatively small up-front fee which is discounted from the final placement fee of 25-35% of the successful candidate’s first year compensation. Both retained and delimited/engaged searches involve partial payment prior to filling the job, and the contracted recruiter has the search exclusively. Therefore, the search can be customized to the client organization’s needs, with the search professional providing a consultative service throughout the process. While both retained and delimited/engaged searches serve client employers rather than job-seeking executives, delimited/engaged search contracts always (as opposed to sometimes) state a future date when the project must be completed or the downpayment refunded.		As stated, contingent search firms are remunerated only upon the successful completion of the search—typically when the candidate accepts the position. These recruiters may earn 20% to 35% of the candidate's first-year base salary or total remuneration as a hiring fee; the fee may also be calculated to include the candidate's (that is, the successful hire's) median or expected first-year bonus payout. In any case, the fee is (as always) paid by the hiring company, not the candidate/hire. Contingent firms in some of the emerging markets may quote fees in the range of 12% to 20% as well.		Clients (companies seeking to hire) often tend to work with contingent search firms when filling mid-level positions. As contingent search firms generally rely heavily on their contacts, and seldom work on an exclusive basis, it is not rare for a client to work with a large number of contingent recruiters on the same search at the same time, in order to maximize the volume of candidate (job seeker) resumes they receive. Beyond the increased volume of candidates that such an approach allows, contingent firms do not get paid until the placement is made (a candidate is successfully hired), and thus the search risk is shifted almost entirely to the search firms. Moreover, contingent search firms often work with clients on higher percentage fee basis, relative to retained and delimited search firms as they shoulder more risk. For senior level roles, clients often prefer to work with recruiters who have performed well in the past for them and usually will end up in the hands of a retained or delimited recruiter. By working exclusively with one firm on such searches, the client generally develops a much deeper relationship with the recruiter, and receives a much higher level of service. With all methods, retained, delimited, and contingency, clients rely on search professionals to provide not just resumes, but also insightful, consultative information about the market in general.		A delimited search is often preferred by clients who are seeking a retainer-style service level, while not willing to accept the level of risk that retained search entails. While delimited search does entail up-front fees, they tend to be much smaller than total pre-placement fees that retained search entails. Moreover, delimited search professionals shoulder the risk of their own failure to execute the search within a specified time-frame, offering to refund the up-front fees in such an event. While delimited search is not as desirable for searches that are open-ended in nature, the “ticking clock” is often seen by clients as an incentive that motivates delimited search recruiters to stay more active and involved throughout the hiring process.		
Wage means payment for units of time or units of product as valued under a wage rate agreement. Today's most common unit is the hour. Many governments impose minimum wage rates upon employers to protect society. (e.g. Michigan's current minimum wage rate is: $890 per hour) However, many employers offer employees significantly higher wage rates. (e.g. Michigan's current median hourly rate for a Certified Nurse Assistant (CNA) is $1240 per hour)		Wages means remuneration ([L. remuneratio: cf. F. rémunération.] "Act or fact of remunerating"; "to give, present" an "advantage", "gift" or "reward" ) for services (or the quality thereof). In addition to receiving a wage, employees are often "put upon wages" to provide an employment advantage. These advantages may include such things as health and life insurance, paid vacation time, gifts for years of service, bonuses for high production rates and other employer provided benefits.		Payment by wage contrasts with salaried work, in which the employer pays an arranged amount at steady intervals (such as a week or month) regardless of hours worked, with commission which conditions pay on individual performance, and with compensation based on the performance of the company as a whole. Waged employees may also receive tips or gratuity paid directly by clients and employee benefits which are non-monetary forms of compensation. Since wage labour is the predominant form of work, the term "wage" sometimes refers to all forms (or all monetary forms) of employee compensation.						Wage labour involves the exchange of money for time spent at work (the latter quantity is termed labor power by Karl Marx and subsequent economists). As Moses I. Finley lays out the issue in The Ancient Economy:		The wage is the monetary measure corresponding to the standard units of working time (or to a standard amount of accomplished work, defined as a piece rate). The earliest such unit of time, still frequently used, is the day of work. The invention of clocks coincided with the elaborating of subdivisions of time for work, of which the hour became the most common, underlying the concept of an hourly wage.[2][3]		Wages were paid in the Middle Kingdom of Ancient Egypt,[4] Ancient Greece,[5] and Ancient Rome.[5]		Depending on the structure and traditions of different economies around the world, wage rates will be influenced by market forces (supply and demand), legislation, and tradition. Market forces are perhaps more dominant in the United States, while tradition, social structure and seniority, perhaps play a greater role in Japan.[6]		In a global market, the average wage for Americans in unskilled labor jobs tends to be low. While unemployment rates can be low, the global economy really caters to those who have the money and resources to access its vast potential. Looking at the Stock Market as a prime example, Lester Thurow points out that there was a real swing in levels of earning, where the top 10-20% of the country in terms of wealth absolutely thrived.[7] However, at the same time, this created a massive gap between what is successful and what is not. Furthering the problem, with the introduction of the Euro, now comes the war against its counterpart in the dollar. A drop in the dollar would signal an increase in inflation. The Euro gives companies and investors a truly viable option if they do not want to invest in dollars, further perpetuating the idea of a global market and economy. A lower price of the dollar and inflation could lead to lower real wages in America.		Even in countries where market forces primarily set wage rates, studies show that there are still differences in remuneration for work based on sex and race. For example, according to the U.S. Bureau of Labor Statistics, in 2007 women of all races made approximately 80% of the median wage of their male counterparts. This is likely due to the supply and demand for women in the market because of family obligations.[8] Similarly, white men made about 84% the wage of Asian men, and black men 64%.[9] These are overall averages and are not adjusted for the type, amount, and quality of work done.		Seventy-five million workers earned hourly wages in the United States in 2012, making up 59% of employees.[10] In the United States, wages for most workers are set by market forces, or else by collective bargaining, where a labor union negotiates on the workers' behalf. The Fair Labor Standards Act establishes a minimum wage at the federal level that all states must abide by, among other provisions. Fourteen states and a number of cities have set their own minimum wage rates that are higher than the federal level. For certain federal or state government contacts, employers must pay the so-called prevailing wage as determined according to the Davis-Bacon Act or its state equivalent. Activists have undertaken to promote the idea of a living wage rate which account for living expenses and other basic necessities, setting the living wage rate much higher than current minimum wage laws require. The minimum wage rate is there to protect the well being of the working class.[11]		Political science:		
Full employment, in macroeconomics, is the level of employment rates where there is no cyclical or deficient-demand unemployment.[1] It is defined by the majority of mainstream economists as being an acceptable level of unemployment somewhere above 0%. The discrepancy from 0% arises due to non-cyclical types of unemployment, such as frictional unemployment (there will always be people who have quit or have lost a seasonal job and are in the process of getting a new job) and structural unemployment (mismatch between worker skills and job requirements). Unemployment above 0% is seen as necessary to control inflation in capitalist economies, to keep inflation from accelerating, i.e., from rising from year to year. This view is based on a theory centering on the concept of the Non-Accelerating Inflation Rate of Unemployment (NAIRU); in the current era, the majority of mainstream economists mean NAIRU when speaking of "full" employment. The NAIRU has also been described by Milton Friedman, among others, as the "natural" rate of unemployment. Having many names, it has also been called the structural unemployment rate.		The 20th century British economist William Beveridge stated that an unemployment rate of 3% was full employment. For the United States, economist William T. Dickens found that full-employment unemployment rate varied a lot over time but equaled about 5.5 percent of the civilian labor force during the 2000s.[2] Recently, economists have emphasized the idea that full employment represents a "range" of possible unemployment rates. For example, in 1999, in the United States, the Organisation for Economic Co-operation and Development (OECD) gives an estimate of the "full-employment unemployment rate" of 4 to 6.4%. This is the estimated unemployment rate at full employment, plus & minus the standard error of the estimate.[3]		The concept of full employment of labor corresponds to the concept of potential output or potential real GDP and the long run aggregate supply (LRAS) curve. In neoclassical macroeconomics, the highest sustainable level of aggregate real GDP or "potential" is seen as corresponding to a vertical LRAS curve: any increase in the demand for real GDP can only lead to rising prices in the long run, while any increase in output is temporary.						What most neoclassical economists mean by "full" employment is a rate somewhat less than 100% employment. Others, such as the late James Tobin, have been accused of disagreeing, considering full employment as 0% unemployment.[4] However, this was not Tobin's perspective in his later work.[5]		Some see John Maynard Keynes as attacking the existence of rates of unemployment substantially above 0%:		Most readers would interpret this statement as referring to only cyclical, deficient-demand, or "involuntary" unemployment (discussed below) but not to unemployment existing as "full employment" (mismatch and frictional unemployment). This is because, writing in 1929, Keynes was discussing a period in which the unemployment rate had been persistently above most conceptions of what corresponds to full employment. That is, a situation where a tenth of the population (and thus a larger percentage of the labor force) is unemployed involves a disaster.		One major difference between Keynes and the Classical economists was that while the latter saw "full employment" as the normal state of affairs with a free-market economy (except for short periods of adjustment), Keynes saw the possibility of persistent aggregate-demand failure causing unemployment rates to exceed those corresponding to full employment. Put differently, while Classical economists saw all unemployment as "voluntary," Keynes saw the possibility that involuntary unemployment can exist when the demand for final products is low compared to potential output. This can be seen in his later and more serious work. In his General Theory of Employment, Interest, and Money, chapter 2, he used a definition that should be familiar to modern macroeconomics:		The only difference from the usual definitions is that, as discussed below, most economists would add skill/location mismatch or structural unemployment as existing at full employment. More theoretically,Keynes had two main definitions of full employment, which he saw as equivalent. His first main definition of full employment involves the absence of "involuntary" unemployment:		Put another way, the full employment and the absence of involuntary unemployment correspond to the case where the real wage equals the marginal cost to workers of supplying labor for hire on the market (the "marginal disutility of employment"). That is, the real wage rate and the amount of employment correspond to a point on the aggregate supply curve of labor that is assumed to exist. In contrast, a situation with less than full employment and thus involuntary unemployment would have the real wage above the supply price of labor. That is, the employment situation corresponds to a point above and to the left of the aggregate supply curve of labor: the real wage would be above the point on the aggregate supply curve of labor at the current level of employment; alternatively, the level of employment would be below the point on that supply curve at the current real wage.		Second, in chapter 3, Keynes saw full employment as a situation where "a further increase in the value of the effective demand will no longer be accompanied by any increase in output."		This means that at and above full employment, any increase in aggregate demand and employment corresponds primarily to increases in prices rather than output. Thus, full employment of labor corresponds to potential output.		Whilst full employment is often an aim for an economy, most economists see it as more beneficial to have some level of unemployment, especially of the frictional sort. In theory, this keeps the labor market flexible, allowing room for new innovations and investment. As in the NAIRU theory, the existence of some unemployment is required to avoid accelerating inflation.		For the United Kingdom, the OECD estimated the NAIRU (or structural unemployment) rate as being equal to 8.5% on average between 1988 and 1997, 5.9% between 1998 and 2007, 6.2%, 6.6%, and 6.7 in 2008, 2009, and 2010, then staying at 6.9% in 2011-2013. For the United States, they estimate it as being 5.8% on average between 1988 and 1997, 5.5% between 1998 and 2007, 5.8% in 2008, 6.0% in 2009, and then staying at 6.1% from 2010 to 2013. They also estimate the NAIRU for other countries.[7]		The era after the 2007-2009 Great Recession shows the relevance of this concept, for example as seen in the United States. On the one hand, in 2013 Keynesian economists such as Paul Krugman of Princeton University see unemployment rates as too high relative to full employment and the NAIRU and thus favor increasing the aggregate demand for goods and services and thus labor in order to reduce unemployment. On the other hand, pointing to shortages of some skilled workers, some businesspeople and Classical economists suggest that the U.S. economy is already at full employment, so that any demand stimulus will lead to nothing but rising inflation rates. One example was Narayana Kocherlakota, President of the Minneapolis Federal Reserve Bank, who has since changed his mind.[8]		An alternative, more normative, definition (used by some labor economists) would see "full employment" as the attainment of the ideal unemployment rate, where the types of unemployment that reflect labor-market inefficiency (such as mismatch or structural unemployment) do not exist. That is, only some frictional or voluntary unemployment would exist, where workers are temporarily searching for new jobs and are thus voluntarily unemployed. This type of unemployment involves workers "shopping" for the best jobs at the same time that employers "shop" for the best possible employees to serve their needs. Its existence can allow the best possible correspondence between workers and jobs from the points of view of both employees and employers and thus promotes the economy's efficiency.		William Beveridge defined "full employment" as where the number of unemployed workers equaled the number of job vacancies available (while preferring that the economy be kept above that full employment level in order to allow maximum economic production). But the point is that this definition allows for some unemployment. To see this, assume that frictional and mismatch unemployment can be separated. At Beveridge full employment, in the case of frictional unemployment the number of job-seekers corresponds to an equal number of job openings: as discussed above, the unemployed are "shopping" for the best possible jobs (as long as the cost of job-search is less than the expected benefit) at the same time that employers are "shopping" for the best possible employees to fill the vacancies. Similarly,at Beveridge full employment, the number of people suffering from mismatch or structural unemployment equals the number of vacancies. The problem here is that the skills and geographical locations of the unemployed workers does not correspond to the skill requirements and locations of the vacancies. In theory, Beveridge's concept full employment corresponds to that of Keynes (discussed above).		The situation with less than full employment in Beveridge's sense results either from "Classical" unemployment or "neoclassical" unemployment or from Keynesian deficient-demand unemployment. In terms of supply and demand, Classical or neoclassical unemployment results from the actual real wage exceeding the equilibrium real wage, so that the quantity of labor demanded (and the number of vacancies) is less than the quantity of labor supplied (and the number of unemployed workers). In the Classical theory, the problem is that real wages are rigid, i.e., do not fall due to an excess supply of labor. In theory, this might happen because of minimum wage laws and other interference with "free markets" that prevent the attainment of market perfection. Classical economists favor making labor markets more like the ideal competitive market—and so making real wages more flexible—in order to deal with this kind of unemployment.		The neoclassical theory, in contrast, follows John Maynard Keynes and more importantly, Milton Friedman to blame inflexible money or nominal wages for low employment relative to full employment. If the money wage is fixed, the real wage is fixed for any given average price level, so that rigid money wages have the same effect as rigid real wages when the price level is given. In this case, however, real wages can be depressed (and Beveridge full employment restored) if prices rise relative to nominal wages. Alternatively, people could wait for the persistence of high unemployment to eventually cause money wages to fall. This would have the same effect, reducing real wages and increasing the quantity of labor demanded. One of the big debates in macroeconomics is whether it is better to deal with neoclassical unemployment using a small amount of inflation or by waiting for markets to adjust.		In contrast, Keynesian deficient-demand unemployment (as explained by Don Patinkin) sees a situation with less than full employment (following Beveridge's definition) as possibly prevailing even if the actual real wage is equal to the equilibrium real wage at full employment. The problem is that the demand for final products is limited by aggregate demand failure. Low demand for products (below potential output) implies that there is a sales constraint on the labor market to the left of equilibrium so that the quantity of labor demanded is below the amount that would be demanded if the aggregate demand for products was sufficient (what Robert Clower called the notional demand for labor). In terms of neoclassical theory, the prevailing real wage is less than the marginal physical product of labor in this situation. In the absence of the sales constraint, profit-maximizing employers would hire unemployed workers as long as this inequality is true, moving the labor markets toward full employment. However, the sales constraint means that the extra product of these workers could not be sold. Thus, employers would not hire the unemployed until aggregate demand rose, which would shift the sales constraint to the right, allowing more employment of labor. In this situation, Keynesians recommend policies that raise the aggregate demand for final products and thus the aggregate demand for workers.		The economic literature concerning the Phillips Curve and the NAIRU moved away from the direct examination of labor market to focus instead on the behavior of inflation rates at different unemployment rates. That is, while Beveridge and Keynes saw full-employment unemployment as where the supply of and the demand for labor were in balance, later views saw it as a threshold which should not be crossed, since low unemployment causes serious inflation.		The theories behind the Phillips curve pointed to the inflationary costs of lowering the unemployment rate. That is, as unemployment rates fell and the economy approached full employment, the inflation rate would rise. But this theory also says that there is no single unemployment number that one can point to as the "full employment" rate. Instead, there is a trade-off between unemployment and inflation: a government might choose to attain a lower unemployment rate but would pay for it with higher inflation rates. In essence, in this view, the meaning of “full employment” is really nothing but a matter of opinion based on how the benefits of lowering the unemployment rate compare to the costs of raising the inflation rate.		Though their theory had been proposed by the Keynesian economist Abba Lerner several years before (Lerner 1951, Chapter 15), it was the work of Milton Friedman, leader of the monetarist school of economics, and Edmund Phelps that ended the popularity of this concept of full employment. In 1968, Friedman posited the theory that full employment rate of unemployment was ‘’’unique’’’ at any given time. He called it the "natural" rate of unemployment. Instead of being a matter of opinion and normative judgment, it is something we are stuck with, even if it is unknown. As discussed further, below, inflation/unemployment trade-offs cannot be relied upon. Further, rather than trying to attain full employment, Friedman argues that policy-makers should try to keep prices stable (meaning a low or even a zero inflation rate). If this policy is sustained, he suggests that a free-market economy will gravitate to the "natural" rate of unemployment automatically.		In an effort to avoid the normative connotations of the word "natural," James Tobin (following the lead of Franco Modigliani), introduced the term the “Non-Accelerating Inflation Rate of Unemployment” (NAIRU), which corresponds to the situation where the real gross domestic product equals potential output. It has been called the "inflation threshold" unemployment rate or the inflation barrier. This concept is identical to Milton Friedman’s concept of the "natural" rate but reflects the fact that there is nothing "natural" about an economy. The level of the NAIRU depends on the degree of "supply side" unemployment, i.e., joblessness that can't be abolished by high demand. This includes frictional, mismatch, and Classical unemployment. When the actual unemployment rate equals the NAIRU, there is no cyclical or deficient-demand unemployment. That is, Keynes’ involuntary unemployment does not exist.		To understand this concept, start with the actual unemployment equal to the NAIRU. Then, assume that a country’s government and its central bank use demand-side policy to reduce the unemployment rate and then attempt to keep the rate at a specific low level: rising budget deficits or falling interest rates increase aggregate demand and raise employment of labor. Thus, the actual unemployment rate falls, as going from point A to B in the nearby graph. Unemployment then stays below the NAIRU for years or more, as at point B. In this situation, the theory behind the NAIRU posits that inflation will accelerate, i.e. get worse and worse (in the absence of wage and price controls). As the short-run Phillips curve theory indicates, higher inflation rate results from low unemployment. That is, in terms of the "trade-off" theory, low unemployment can be "bought," paid for by suffering from higher inflation. But the NAIRU theory says that this is not the whole story, so that the trade-off breaks down: a persistently higher inflation rate is eventually incorporated as higher inflationary expectations. Then, if workers and employers expect higher inflation, it results in higher inflation, as higher money wages are passed on to consumers as higher prices. This causes the short run Phillips curve to shift to the right and upward, worsening the trade-off between inflation and unemployment. At a given unemployment rate, inflation accelerates. But if the unemployment rate rises to equal the NAIRU, we see higher inflation than before the expansionary policies, as at point C in the nearby diagram. The fall of the unemployment rate was temporary because it could not be sustained. In sum, the trade-off between inflation and unemployment cannot be relied upon to be stable: taking advantage of it causes it to disappear. This story fits the experience of the United States during the late 1960s, during which unemployment rates stayed low (below 4% of the civilian labor force) and inflation rates rose significantly.		Second, examine the other main case. Again start with the unemployment rate equal to the NAIRU. Then, either shrinking government budget deficits (or rising government surpluses) or rising real interest rates encourage higher unemployment. In this situation, the NAIRU theory says that inflation will get better (decelerate) if unemployment rates exceed the NAIRU for a long time. High unemployment leads to lower inflation, which in turn causes lower inflationary expectations and a further round of lower inflation. High unemployment causes the short-run inflation/unemployment trade-off to improve. This story fits the experience of the United States during the early 1980s (Paul Volcker's war against inflation), during which unemployment rates stayed high (at about 10% of the civilian labor force) and inflation rates fell significantly.		Finally, the NAIRU theory says that the inflation rate does not rise or fall when the unemployment equals the "natural" rate. This is where the term NAIRU is derived. In macroeconomics, the case where the actual unemployment rate equals the NAIRU is seen as the long-run equilibrium because there are no forces inside the normal workings of the economy that cause the inflation rate to rise or fall. The NAIRU corresponds to the long-run Phillips curve. While the short-run Phillips curve is based on a constant rate of inflationary expectations, the long-run Phillips curve reflects full adjustment of inflationary expectations to the actual experience of inflation in the economy.		As mentioned above, Abba Lerner had developed a version of the NAIRU before the modern "natural" rate or NAIRU theories were developed. Unlike the currently dominant view, Lerner saw a range of "full employment" unemployment rates. Crucially, the unemployment rate depended on the economy's institution. Lerner distinguished between "high" full employment, which was the lowest sustainable unemployment under incomes policies, and "low" full employment, i.e., the lowest sustainable unemployment rate without these policies.		Further, it is possible that the value of the NAIRU depends on government policy, rather than being "natural" and unvarying. A government can attempt to make people "employable" by both positive means (e.g. using training courses) and negative means (e.g. cuts in unemployment insurance benefits). These policies do not necessarily create full employment. Instead, the point is to reduces the amount of mismatch unemployment by facilitating the linking of unemployed workers with the available jobs by training them and or subsidizing their moving to the geographic location of the jobs.		In addition, the hysteresis hypothesis says that the NAIRU does not stay the same over time—and can change due to economic policy.[9] A persistently low unemployment rate makes it easier for those workers who are unemployed for "mismatch" reasons to move to where the jobs are and/or to attain the training necessary for the available vacancies (often by getting those jobs and receiving on-the-job training). On the other hand, high unemployment makes it more difficult for those workers to adjust, while hurting their morale, job-seeking skills, and the value of their work skills. Thus, some economists argue that British Prime Minister Margaret Thatcher's anti-inflation policies using persistently high unemployment led to higher mismatch or structural unemployment and a higher NAIRU.		Whatever the definition of full employment, it is difficult to discover exactly what unemployment rate it corresponds to. In the United States, for example, the economy saw stable inflation despite low unemployment during the late 1990s, contradicting most economists' estimates of the NAIRU.		The idea that the full-employment unemployment rate (NAIRU) is not a unique number has been seen in recent empirical research. Staiger, Stock, and Watson found that the range of possible values of the NAIRU (from 4.3 to 7.3% unemployment) was too large to be useful to macroeconomic policy-makers. Robert Eisner suggested that for 1956-95 there was a zone from about 5% to about 10% unemployment between the low-unemployment realm of accelerating inflation and the high-unemployment realm of disinflation. In between, he found that inflation falls with falling unemployment.		The active pursuit of national full employment through interventionist government policies is associated with Keynesian economics and marked the postwar agenda of many Western nations, until the stagflation of the 1970s.		Australia was the first country in the world in which full employment in a capitalist society was made official policy by its government. On May 30, 1945, The Australian Labor Party Prime Minister John Curtin and his Employment Minister John Dedman proposed a white paper in the Australian House of Representatives titled Full Employment In Australia, the first time any government apart from totalitarian regimes had unequivocally committed itself to providing work for any person who was willing and able to work. Conditions of full employment lasted in Australia from 1941 to 1975. This had been preceded by the Harvester Judgment (1907), establishing the basic wage (a living wage); while this earlier case was overturned, it remained influential.		The United States is, as a statutory matter, committed to full employment; the government is empowered to effect this goal.[10] The relevant legislation is the Employment Act (1946), initially the "Full Employment Act," later amended in the Full Employment and Balanced Growth Act (1978). The 1946 act was passed in the aftermath of World War II, when it was feared that demobilization would result in a depression, as it had following World War I in the Depression of 1920–21, while the 1978 act was passed following the 1973–75 recession and in the midst of continuing high inflation.		The law states that full employment is one of four economic goals, in concert with growth in production, price stability, balance of trade, and budget, and that the US shall rely primarily on private enterprise to achieve these goals. Specifically, the Act is committed to an unemployment rate of no more than 3% for persons aged 20 or over, and not more than 4% for persons aged 16 or over (from 1983 onwards), and the Act expressly allows (but does not require) the government to create a "reservoir of public employment" to affect this level of employment. These jobs are required to be in the lower ranges of skill and pay so as to not draw the workforce away from the private sector.		However, since the passage of this Act in 1978, the US has, as of 2012[update] never achieved this level of employment on the national level, though some states have neared it or met it, nor has such a reservoir of public employment been created.		Some, particularly Post-Keynesian economists[11][12] have suggested ensuring full employment via a job guarantee program, where those who are unable to find work in the private sector are employed by the government, the stock of thus employed public sector workers fulfilling the same function as the unemployed do in controlling inflation, without the human costs of unemployment.		
A take-home vehicle, or company car is a vehicle which companies or organisations lease or own and which employees use for their personal and business travel.[1]		There are three main reasons which explain why the provision of a company car for private use as a benefit may be attractive for both the employee and the employer. The first reason is that companies can supply the fringe benefit at lower costs than the employee is able to achieve on their own – and consequently pass it on to the employee. Secondly, the tax system may encourage the provision of cars over monetary remuneration from the perspective of both the employer and employee. Thirdly, firms may want the employee to drive in a car of certain minimum standard or have access to a suitable vehicle at all times.[1] It may also benefit the employer if there is advertising/branding on the car's paintwork (or window stickers), since if the employee uses the car during the evening/weekend, it spreads advertising in public areas more than if the car was locked up in a garage during these times.		The use of company cars is widespread in some regions. For example, business registrations account for roughly 50% of all car sales in the EU, largely due to taxation rules which give companies a strong incentive to provide the benefit.[1] The practice has been criticised by many groups who argue that the benefit encourages people to drive more (thus increasing CO2 emissions), reduces government tax revenues, distorts economic competition, and may work to neutralise other government programs and objectives.[1][2]						Police departments are among frequent participants in take-home vehicle programs, allowing officers to take home the police cars they use while on duty. It is considered to be a fringe benefit by the departments.[3] It has been viewed by some departments as a crime-fighting tool, given its cost.[4]		There is a straightforward distortion in consumer markets as consumers through tax incentives are being encouraged to consume more car services than they would have been otherwise.[1]		There is also a substantial tax loss resulting from the subsidy.[1]		Studies have shown that the subsidy encourages consumers to buy more and bigger cars than they would choose otherwise.[1] In many areas, fuel costs are also covered by the benefit, so that the marginal cost of driving may approach zero. In these areas consumers are encouraged to drive more frequently and farther than they otherwise would, and avoid other forms of transportation. Emissions of CO2 and other harmful gases are clearly higher as a result.[1]		When issued by a government agency, concern has been brought up by citizens and advocates over taxpayer money used to fund take-home vehicles. This has led some cities to cutting or reducing the number of employees to whom vehicles are offered.		In Sacramento, California, the issuing of take-home vehicles has come under scrutiny as the city has faced a budget deficit.[5]		In the city of Baltimore, the use of take-home vehicles by city employees has been questioned due to the distance that city employees drive them to their homes. It was determined in a report that two-thirds of city employees drive their vehicles outside city limits, some more than 100 mi (160 km) from the city, and the cost to taxpayers, which included fuel, was high.[6] Baltimore's former mayor Sheila Dixon was also criticized for having three tax-funded take-home vehicles parked at her house. She defended herself by saying she might need the vehicles if there were an emergency.[7]		In Dallas, the city was having trouble obtaining data in attempting to determine the cost of take-home vehicles to taxpayers.[8]		The city of Los Angeles was criticized for issuing take-home vehicles to utility employees while raising rates to customers, though the city stated it would be a minuscule part of the budget.[9]		The city of Evansville, Indiana reduced the number of take-home vehicles offered to city employees, but allowed public safety employees to keep theirs.[10]		
Dress codes are written and, more often, unwritten rules with regard to clothing. Clothing, like other aspects of human physical appearance, has a social significance, with different rules and expectations applying depending on circumstance and occasion. Even within a single day an individual may need to navigate between two or more dress codes: at a minimum those that apply at their place of work and those at home; usually this ability is a result of cultural acclimatization.[clarification needed] Different societies and cultures will have different dress norms, although Western styles are widely accepted as valid.		The dress code has built in rules or signals indicating the message being given by a person's clothing and how it is worn. This message may include indications of the person's gender, income, occupation and social class, political, ethnic and religious affiliation, attitude towards comfort, fashion, traditions, gender expression, marital status, sexual availability, and sexual orientation, etc. Clothes convey other social messages including the stating or claiming personal or cultural identity, the establishing, maintaining, or defying social group norms, and appreciating comfort and functionality.		For example, wearing expensive clothes can communicate wealth, the image of wealth, or to quality clothing. The observer sees the expensive clothes, but may misinterpret the extent to which these factors apply to the wearer. Clothing can convey a social message, even if none is intended: if the receiver's code of interpretation differs from the sender's code of communication, misinterpretation follows. However clothes may be worn because they are comfortable and practical, not to convey a message.		In every culture, current fashion governs how clothing is constructed, assembled, and worn to convey a social message. The rate of change of fashion varies, clothes and its accessories within months or days, especially in small social groups or in communications media-influenced modern societies. More extensive changes, requiring more time, money, and effort to effect, may span generations. When fashion changes, the messages communicated by clothing change.						In the Middle Ages, the European royalty and nobility used a dress code to differentiate themselves from other classes of people.		The indigenous peoples of the Pacific Northwest Coast had a complex social structure, including slaves, commoners, and nobles, and dress codes to indicate these social distinctions. John R. Jewitt, an Englishman who wrote a memoir about his years as a captive of the Nuu-chah-nulth people in 1802-1805, describes how, after some time living there, Maquinna and the chiefs decided that he must now be "considered one of them, and conform to their customs". Jewitt resented the imposition of this dress code, finding the loose untailored garments very cold, and attributed to them a subsequent illness of which he almost died. He was not allowed to cut his hair, and had to paint his face and body as a Nootka would.[1]		In most traditions, certain types of clothing are worn exclusively or predominantly by either men or women. For example, long sleeves are common for both genders, while the wearing of a skirt or a dress tends to be associated with female dress, while trousers are associated with male dress (although common for both genders). Hairdressing in some societies may also conform to a dress code, such as long hair for women and short hair for men.		In many societies, particular clothing may be a status symbol, reserved or affordable to people of high rank. For example, in Ancient Rome only senators were permitted to wear garments dyed with Tyrian purple; and, in traditional Hawaiian society, only high-ranking chiefs could wear feather cloaks and palaoa or carved whale teeth. In China before the establishment of the republic, only the emperor could wear yellow.[citation needed][dubious – discuss]		In 1996, former U. S. President Bill Clinton announced his support for the idea of school uniforms by stating, “School uniforms are one step that may help break the cycle of violence, truancy and disorder by helping young students understand what really counts is what kind of people they are.” Many school districts in the United States took up the idea.[3] By requiring students to wear a school uniform they are less likely to have something to make fun of other students for. This would cause the students to get to know one another by their personality and who they really are rather than the clothes they wear.		Military, police, and firefighters usually wear uniforms, as do workers in many industries. School children often wear school uniforms, while college and university students sometimes wear academic dress. Members of religious orders may wear uniforms known as habits. Sometimes a single item of clothing or a single accessory can declare one's occupation or rank within a profession.		In many regions of the world, national costumes and styles in clothing and ornament declare membership in a certain village, caste, religion, etc. A Scotsman declares his clan with his tartan. A French peasant woman identified her village with her cap or coif. A Palestinian woman identifies her village with the pattern of embroidery on her dress.[4]		Clothes can also proclaim dissent from cultural norms and mainstream beliefs, as well as personal independence. In 19th-century Europe, artists and writers lived la vie de Bohème and dressed to shock: George Sand in men's clothing, female emancipationists in bloomers, male artists in velvet waistcoats and gaudy neckcloths. Bohemians, beatniks, hippies, Goths, Punks, and Skinheads have continued the (countercultural) tradition in the 20th-century West.		A Sikh or Muslim man may display his religious affiliation by wearing a turban and other traditional clothing. Many Muslim women wear head or body coverings (see sartorial hijab, hijab, burqa or niqab, chador, and abaya) that proclaim their status as respectable women and cover the so-called intimate parts. A Jewish man may indicate his observance of Judaism by wearing a kippah.		Traditionally, Hindu women wear sindoor, a red powder, in the parting of their hair to indicate their married status; if widowed, they abandon sindoor and jewelry and wear simple white clothing. However, this is not true of all Hindu women; in the modern world this is not a norm and women without sindoor may not necessarily be unmarried.		In many Orthodox Jewish circles, married women wear head coverings such as a hat, snood, or wig. Additionally, after their marriage, Jewish men of Ashkenazi descent begin to wear a talit during prayer.		Men and women of the Western world may wear wedding rings to indicate their married status, and women may also wear engagement rings when they are engaged.		In New Guinea and Vanuatu, there are areas where it is customary for the men to wear nothing but penis sheaths in public - this is uncommon in more developed areas. Women wear string skirts. In remote areas of Bali, women may go topless. In America and some parts of Europe, there are nude beaches.		In the United States, a few businesses or restaurants display dress code signs requiring shoes and shirts, claiming to be there on account of a health code,[citation needed] although no such health codes exist.[5] Also, it is a common belief that there are laws against driving barefoot. However, no such laws exist. It is quite uncommon for people to be nude in public in the United States and in many circumstances, it is illegal. Many states and cities have laws and ordinances for indecent exposure and sometimes nudity can overlap with disorderly conduct. However, there are a few private beaches and resorts that cater to people who wish to be naked.		Private organisations may insist on particular dress codes or standards in particular situations.		Dress codes function on certain social occasions and for certain jobs. A military institution may require specified uniforms; if it allows the wearing of plain clothes it may place restrictions on their use.		A "formal" or white tie dress code typically means tail-coats for men and full-length evening dresses for women. "Semi-formal" has a much less precise definition but typically means an evening jacket and tie for men (known as black tie) and a dress for women. "Business casual" typically means not wearing neckties or suits, but wearing instead collared shirts, and more country trousers (not black, but more relaxed, including things such as corduroy). "Casual" typically just means clothing for the torso, legs and shoes. "Wedding Casual" defines yet another mode of dress, where guests dress respectfully, but not necessarily fancily. The uniform may consist of various items that are appropriate length and style depending on what the school suggests: for example, khaki pants or shorts, plaid skirts, a button-up collared shirt, a sweater, a coat and tie and even socks. Some schools have each grade assigned a color type which communicates what grade the student is currently in. That way if a student is lost someone is able to figure out what grade they are in just by looking at the color of their shirt. If the student is younger, older students and faculty are able to look out for them and make sure they are safe. Organisations which seek to maintain standards of modesty have difficulties with sheer and see-through clothing.		Dress codes usually set a lower limit on body covering. However, sometimes it can specify the opposite: for example, in UK gay jargon, dress code, means people who dress in a militaristic manner. Dress code nights in nightclubs, and elsewhere, are deemed to specifically target people who have militaristic fetishes (e.g. leather/skinhead men).		See also shoe etiquette, mourning, sharia, Dress code (Western).		Noncommunicative dress code violations in public schools are violations that are without implications of hate, gang-affiliation, etc.[6] Communicative dress code violations are violations of an explicit nature, where the clothing has implications of hate, violence, gang-affiliation, etc.[6]		In cases where dress code rules in public school systems have been violated by noncommunicative clothing, courts repeatedly legitimise dress code discrimination based on gender.[7] Amongst the transgender populations, gender based dress codes are primarily enforced against individuals who are biologically able to remain within their own gender group, but that may wish to transcend to another gender.[clarification needed] [7]		White collar work place clothing has changed significantly through the years. In a corporate office, appropriate clothes are clean, business casual clothes such as (for men) a dress shirt, polo shirt, and trousers, or other similar outfits. Suits, neckties, and other formal wear are usually only required in law offices and financial sector offices. Previous business dress code eras (the 1950s in the U.S.) featured standardized business clothes that strongly differentiated what was acceptable and unacceptable for men and women to wear while working. Today, the two styles have merged; women's work clothes expanded to include the suit (and its variants) in addition to the usual dresses, skirts, and blouses; men's clothes have expanded to include garments and bright colours.[citation needed]		Casual wear entered business culture with the advent of the Silicon Valley, California, technology company featuring casual work clothes on the job. Additionally, some companies set aside days — generally Fridays ("dress-down Friday", "casual Friday") — when workers may wear casual clothes. The clothing a company requires its worker to wear on the job varies with the occupation and profession.		Some businesses observe that anti-discrimination law restricts their determining what is appropriate and inappropriate workplace clothing. Yet, in fact, most businesses have much authority in determining and establishing what work place clothes they can require of their workers. Generally, a carefully drafted dress code applied consistently does not violate anti-discrimination laws.[8]		Business casual dress, also "smart casual", is a popular work place dress code that emerged in white-collar workplaces in Western countries in the 1990s, especially in the United States and Canada. Many information technology businesses in Silicon Valley were early adopters of this dress code. In contrast to formal business wear such as suits and neckties (the international standard business attire), the business casual dress code has no generally accepted definition; its interpretation differs widely among organizations and is often a cause of sartorial confusion among workers.		The job search engine Monster.com offers this definition: In general, business casual means dressing professionally, looking relaxed, yet neat and pulled together. A more pragmatic definition is that business casual dress is the mid ground between formal business clothes and street clothes. Examples of clothing combinations considered appropriate for work by businesses that consider themselves as using the business-casual dress code are:		Generally, neckties are excluded from business casual dress, unless worn in nontraditional ways. The acceptability of blue jeans and denim cloth clothing varies — some businesses consider them to be sloppy and informal.		Inverse dress codes, sometimes referred to as "undress code", set forth an upper bound, rather than a lower bound, on body covering. An example of an undress code is the one commonly enforced in modern communal bathing facilities. For example, in the public bath SchwabenQuellen, no clothing of any kind is allowed in the sauna part of the resort. Other, less strict undress codes are common in public pools, especially indoor pools, in which shoes and shirts are disallowed.		Places where social nudity is practiced may be "clothing optional," or nudity may be compulsory, with exceptions. See issues in social nudity.		Some clothing faux pas may occur intentionally for reasons of fashion or personal preference. For example, people may wear intentionally oversized clothing. For instance, the teenage boys of rap duo Kris Kross of the early 1990s wore all of their clothes backwards and extremely baggy.		Social attitudes to clothing have brought about various rules and social conventions, such as keeping the body covered, and not showing underwear in public. The backlash against these social norms has become a traditional form of rebellion. Over time, western societies have gradually adopted more casual dress codes in the workplace, school, and leisure. This has especially been the case since the early 1960s.		
In employment law, constructive dismissal, also called constructive discharge or constructive termination, occurs when an employee resigns as a result of the employer creating a hostile work environment. Since the resignation was not truly voluntary, it is in effect, a termination. For example, when an employer makes life extremely difficult for an employee, to attempt to have the employee resign, rather than outright firing the employee, the employer is trying to effect a constructive discharge.		The exact legal consequences differ between different countries, but generally a constructive dismissal leads to the employee's obligations ending and the employee acquiring the right to make claims against the employer.		The employee may resign over a single serious incident or over a pattern of incidents. Generally, the employee must have resigned soon after the incident.						In the United States, constructive discharge has differing meanings depending on the jurisdiction.[1] The U.S. Equal Employment Community Commission has provided a 3-part test to determine whether or not a constructive discharge has occurred: (1) a reasonable person in the complainant’s position would have found the working conditions intolerable; (2) conduct that constituted discrimination against the complainant created the intolerable working conditions; and (3) the complainant’s involuntary resignation resulted from the intolerable working conditions.[2] In California, the California Supreme Court defines constructive discharge as follows:		"In order to establish a constructive discharge, an employee must plead and prove, by the usual preponderance of the evidence standard, that the employer either intentionally created or knowingly permitted working conditions that were so intolerable or aggravated at the time of the employee's resignation that a reasonable employer would realize that a reasonable person in the employee's position would be compelled to resign."[3]		Canadian courts recognize there are circumstances in which the employer, although not acting explicitly to terminate an individual’s employment, alters the employment relationship’s terms and conditions to such a degree that an employee is entitled to regard the employer’s conduct as a termination, and claim wrongful dismissal, just as if they had been let go without any notice or termination pay in lieu of notice.[4]		Constructive dismissal arises from the failure of the employer to live up to the essential obligations of the employment relationship, regardless of whether the employee signed a written employment contract. Employment law implies into employment relationships a common-law set of terms and conditions applicable to all employees. For example, once agreed upon, wages are implicitly locked in by the common-law of contract as an essential term of the employment relationship. In this regard, it is a constructive dismissal if an employer fails to pay an employee.		An employer’s breach of the employment contract releases the employee from their obligation to perform under the contract, and thus treat themselves as dismissed. Hence, a constructive dismissal always becomes a wrongful dismissal.		Changes to the Employment Relationship		Typically, the first way to claim constructive dismissal involves an employer making substantial changes to the employment contract, such as:		In addition, failure on the part of an employer to provide employment standards (i.e. overtime pay, vacation pay, etc.), can result in a constructive dismissal.		Nevertheless, for an employee to have a successful case for constructive dismissal, the employer’s breach must be fundamental. What is “fundamental” depends on the circumstances, and not all changes to the employment relationship give rise to a constructive dismissal. For example, administrative, i.e. non-disciplinary, suspensions might not amount to a constructive dismissal if imposed in good faith and justified by legitimate business reasons (i.e. lack of work). As well, a small reduction in salary, in tough times, and administered rationally, might not be a constructive dismissal.[5]		Toxic Work Environments		An employee may also be able to claim a constructive dismissal based on an employer’s conduct, rather than a change to a specific or implied term of the employment contract. Here, the second way to claim constructive dismissal examines whether the employer’s (or employee of the employer) course of conduct, or even a single incident, demonstrates an intention to no longer be bound by the written or implied employment contract. An example of this kind of constructive dismissal is a “toxic work environment”. In this regard, if a work environment is so poisoned that a reasonable person wouldn’t be expected to return, then constructive dismissal is likely.		A toxic work environment is classically defined as unjustified criticism as well as vague and unfounded accusations of poor performance, especially where authority and respect with co-workers had been seriously undermined and compromised. Another example of toxic work environment is where the employer fails to prevent workplace harassment.[4]		In United Kingdom law, constructive dismissal is defined by the Employment Rights Act 1996 section 95(1)c:[6]		The employee terminates the contract under which he is employed (with or without notice) in circumstances in which he is entitled to terminate it without notice by reason of the employer's conduct.		The circumstances in which an employee is entitled are defined in common law. The notion of constructive dismissal most often arises from a fundamental breach of the term of trust and confidence implied in all contracts of employment. In order to avoid such a breach "[a]n employer must not, without reasonable or proper cause, conduct himself in a manner calculated or likely to destroy or seriously damage the relationship of trust and confidence between the employer and the employee."[7] Whilst a breach can be of the implied term of trust and confidence, a fundamental breach of any of the express or implied terms of a contract of employment is sufficient.		The Department of Trade and Industry states:		A tribunal may rule that an employee who resigns because of conduct by his or her employer has been 'constructively dismissed'. For a tribunal to rule in this way the employer's action has to be such that it can be regarded as a significant breach of the employment contract indicating that he or she intends no longer to be bound by one or more terms of the contract: an example of this might be where the employer arbitrarily demotes an employee to a lower rank or poorer paid position. The contract is what has been agreed between the parties, whether orally or in writing, or a combination of both, together with what must necessarily be implied to make the contract workable.[8]		Following a constructive dismissal, claim for "unfair dismissal" and a claim for "wrongful dismissal" may arise.		Although they tend to blend into one in a tribunal, strictly there are two types of constructive dismissal: statutory and common law.		At common law[9] the requirement is acceptance of a repudiatory breach, which means the employer has indicated it no longer considers itself bound by an essential term of the contract, e.g. the requirement to pay wages or the requirement not to destroy the mutual bond of trust and confidence. It matters not if the employer did not mean to repudiate the contract.[10]		Under statute[6] the requirement is employer's "conduct" allowing the employee to "terminate without notice"; as this can only happen with a repudiatory breach it amounts to the same thing.		A common mistake is to assume that constructive dismissal is exactly the same as unfair treatment of an employee – it can sometimes be that treatment that can be considered generally evenhanded nevertheless makes life so difficult that the employee is in essence forced to resign[11] (e.g., a fair constructive dismissal might be a unilateral change of contract justified by a bigger benefit to the business than the inconvenience to the employee), but the Employment Appeal Tribunal doubts that it will be very often that the employer can breach ERA96 s98(4) whilst being fair.		A constructive dismissal occurs when the employer's serious breach causes[12] the employee to accept that the contract has been terminated, by resigning. The fairness of it would have to be looked at separately under a statutory claim for unfair dismissal.		The problems for the employer are that constructive dismissal is a contractual claim, which can be made in a tribunal for up to £25,000 or in court without limit, and, by dismissing constructively, it by definition misses out on the correct procedure meaning that even if the reason was fair, the decision was probably not, and so an unfair dismissal usually arises, creating a statutory claim alongside the contractual claim.		The court can look behind the lack of, or different, stated reason given by the employee at the time of resignation to establish that a cover story was in fact a resignation caused by fundamental breach.[13]		The person causing the dismissal does not need the authority to dismiss, as long as they acted in the course of employment.[6][14]		Constructive dismissal is typically caused by:-		A flexibility clause does not allow the employer to change a type of job[38] as it is implied that the flexibility is to operate within the original job.		A mobility clause is subject to the implied term of mutual trust which prevents the employer from sending an employee to the other side of the country without adequate notice or from doing anything which makes it impossible for the employee to keep his side of the bargain.[39]		There is no right to automatic pay rises.[40] Nor is a smoking ban a breach.[41]		The employee's conduct is irrelevant to liability, although it can affect quantum; in other words it cannot get the employer off the hook, but could reduce compensation if he helped bring about his own downfall.		The conduct by the employer could be:		The employee has to resign within a reasonable time of the trigger, which is the one-off outrage or the last straw. The employee could work under protest while he or she finds a new job.[45]		If the employer alleges that the employee waived a breach by not resigning, each breach needs to be looked at to see if it was waived separately,[42] but even if a breach was waived, the last straw revives it for the purpose of determining whether overall there was a repudiation.[10]		If the employer alleges that the employee has affirmed a breach by not resigning, the employee could point out that no consideration was paid for it and so no contract change has been accepted. Acceptance of a replacement job would prove affirmation.[46]		An employee who stays on for a year after refusing to sign a new contract does not necessarily accept it.[47]		The last straw does not have to be similar to the earlier string of events or even unreasonable or blameworthy – it need only be related to the obligation of trust and confidence and enough that when added to the earlier events the totality is a repudiation.[48]		Although the employer's breach must be serious enough to entitle the employee to resign without notice, the employee is entitled to give notice if they prefer, so that they could enjoy the benefit of wages during the notice period.		To prevent the employer alleging that the resignation was caused by a job offer, the employee should resign first and then seek a new job during the notice period.		During the notice period, the employer could make the employee redundant[49] or summarily dismiss them, if it has the grounds to do so fairly. Otherwise, the reason for termination will be resignation and not dismissal, since the employee cannot serve a counternotice.[50]		
A repetitive strain injury (RSI) is an "injury to the musculoskeletal and nervous systems that may be caused by repetitive tasks, forceful exertions, vibrations, mechanical compression, or sustained or awkward positions".[1] RSIs are also known as cumulative trauma disorders, repetitive stress injuries, repetitive motion injuries or disorders, musculoskeletal disorders, and occupational or sports overuse syndromes.				Repetitive strain injury (RSI) and associative trauma orders are umbrella terms used to refer to several discrete conditions that can be associated with repetitive tasks, forceful exertions, vibrations, mechanical compression, or sustained/awkward positions.[1][2] Examples of conditions that may sometimes be attributed to such causes include edema, tendinosis (or less often tendinitis), carpal tunnel syndrome, cubital tunnel syndrome, De Quervain syndrome, thoracic outlet syndrome, intersection syndrome, golfer's elbow (medial epicondylitis), tennis elbow (lateral epicondylitis), trigger finger (so-called stenosing tenosynovitis), radial tunnel syndrome, and focal dystonia.[1][2][3]		Since the 1970s there has been a worldwide increase in RSIs of the arms, hands, neck, and shoulder attributed to the widespread use of typewriters/computers in the workplace that require long periods of repetitive motions in a fixed posture.[4]		Specific sources of discomfort have been popularly referred to by terms such as Blackberry thumb, iPod finger, mouse arm disease, PlayStation thumb,[5] Rubik's wrist or "cuber's thumb",[6] stylus finger,[7] raver's wrist,[8] and Emacs pinky, among others.		Some examples of symptoms experienced by patients with RSI are aching, pulsing pain, tingling and extremity weakness, initially presenting with intermittent discomfort and then, with a higher degree of frequency.[9]		Workers in certain fields are at risk of repetitive strains. Most occupational injuries are musculoskeletal disorders, and many of these are caused by cumulative trauma rather than a single event.[10] Miners and poultry workers, for example, must make repeated motions which can cause tendon, muscular, and skeletal injuries.[11][12]		RSIs are assessed using a number of objective clinical measures. These include effort-based tests such as grip and pinch strength, diagnostic tests such as Finkelstein's test for Dequervain's tendinitis, Phalen's Contortion, Tinel's Percussion for carpal tunnel syndrome, and nerve conduction velocity tests that show nerve compression in the wrist. Various imaging techniques can also be used to show nerve compression such as x-ray for the wrist, and MRI for the thoracic outlet and cervico-brachial areas.		The most-often prescribed treatments for early-stage RSIs include analgesics, myofeedback, biofeedback, physical therapy, relaxation, and ultrasound therapy.[3] Low-grade RSIs can sometimes resolve themselves if treatments begin shortly after the onset of symptoms. However, some RSIs may require more aggressive intervention including surgery and can persist for years.		General exercise has been shown to decrease the risk of developing RSI.[13] Doctors sometimes recommend that RSI sufferers engage in specific strengthening exercises, for example to improve sitting posture, reduce excessive kyphosis, and potentially thoracic outlet syndrome.[14] Modifications of posture and arm use (human factors and ergonomics) are often recommended.[3][15]		Although seemingly a modern phenomenon, RSIs have long been documented in the medical literature. In 1700, the Italian physician Bernardino Ramazzini first described RSI in more than 20 categories of industrial workers in Italy, including musicians and clerks.[16] Carpal tunnel syndrome was first identified by the British surgeon James Paget in 1854.[17]		Lee Jackson's 2006 work "A Dictionary of Victorian London", quotes a paragraph from the April 1875 issue of The Graphic describing "telegraphic paralysis".		The Swiss surgeon Fritz de Quervain first identified De Quervain’s tendinitis in Swiss factory workers in 1895.[18] The French neurologist Jules Tinel (1879–1952) developed his percussion test for compression of the median nerve in 1900.[19][20][21] The American surgeon George Phalen improved the understanding of the aetiology of carpal tunnel syndrome with his clinical experience of several hundred patients during the 1950s and 1960s.[22]		
Recruitment (hiring) is a core function of human resource management. It is the first step of appointment. Recruitment refers to the overall process of attracting, selecting and appointing suitable candidates for jobs (either permanent or temporary) within an organization. Recruitment can also refer to processes involved in choosing individuals for unpaid positions, such as voluntary roles or unpaid trainee roles. Managers, human resource generalists and recruitment specialists may be tasked with carrying out recruitment, but in some cases public-sector employment agencies, commercial recruitment agencies, or specialist search consultancies are used to undertake parts of the process. Internet-based technologies to support all aspects of recruitment have become widespread.[1]						In situations where multiple new jobs are created and recruited for the first time or vacancies are there or the naturein such documents as job descriptions and job specifications. Often, a company already has job descriptions for existing positions. Where already drawn up, these documents may require review and updating to reflect current requirements. Prior to the recruitment stage, a person specification should be finalized.[2]		Sourcing is the use of one or more strategies to attract or identify candidates to fill job vacancies. It may involve internal and/or external recruitment advertising, using appropriate media, such as job portals,local or national newspapers, social media, business media, specialist recruitment media, professional publications, window advertisements, job centers, or in a variety of ways via the internet.		Alternatively, employers may use recruitment consultancies or agencies to find otherwise scarce candidates—who, in many cases, may be content in their current positions and are not actively looking to move. This initial research for candidates—also called name generation—produces contact information for potential candidates, whom the recruiter can then discreetly contact and screen.[2]		Various psychological tests can assess a variety of KSAOs, including literacy. Assessments are also available to measure physical ability. Recruiters and agencies may use applicant tracking systems to filter candidates, along with software tools for psychometric testing and performance-based assessment.[3] In many countries, employers are legally mandated to ensure their screening and selection processes meet equal opportunity and ethical standards.[2]		Employers are likely to recognize the value of candidates who encompass soft skills such as interpersonal or team leadership.[4] Many companies, including multinational organizations and those that recruit from a range of nationalities, are also often concerned about whether candidate fits the prevailing company culture.[5]		The word disability carries few positive connotations for most employers. Research has shown that employer biases tend to improve through first-hand experience and exposure with proper supports for the employee[6] and the employer making the hiring decisions. As for most companies, money and job stability are two of the contributing factors to the productivity of a disabled employee, which in return equates to the growth and success of a business. Hiring disabled workers produce more advantages than disadvantages.[7] There is no difference in the daily production of a disabled worker.[8] Given their situation, they are more likely to adapt to their environmental surroundings and acquaint themselves with equipment, enabling them to solve problems and overcome adversity as with other employees. The U.S. IRS grants companies Disabled Access Credit when they meet eligibility criteria.[9]		Many major corporations recognize the need for diversity in hiring to compete successfully in a global economy.[10] Other organizations, for example universities and colleges, have been slow to embrace diversity as an essential value for their success.[11]		Recruitment Process Outsourcing, or commonly known as "RPO" is a form of business process outsourcing (BPO) where a company engages a third party provider to manage all or part of its recruitment process.		Internal recruitment (not to be confused with internal recruiters!) refers to the process of a candidate being selected from the existing workforce to take up a new job in the same organization, perhaps as a promotion, or to provide career development opportunity, or to meet a specific or urgent organizational need. Advantages include the organization's familiarity with the employee and their competencies insofar as they are revealed in their current job, and their willingness to trust said employee. It can be quicker and have a lower cost to hire someone internally.[12]		An employee referral program is a system where existing employees recommend prospective candidates for the job offered, and in some organizations if the suggested candidate is hired, the employee receives a cash bonus.[13]		Niche firms tend to focus on building ongoing relationships with their candidates, as the same candidates may be placed many times throughout their careers. Online resources have developed to help find niche recruiters.[14] Niche firms also develop knowledge on specific employment trends within their industry of focus (e.g., the energy industry) and are able to identify demographic shifts such as aging and its impact on the industry.[15]		Social recruiting is the use of social media for recruiting including sites like Facebook and Twitter or career-oriented social networking sites such as LinkedIn and XING.[16][17] It is a rapidly growing sourcing technique, especially with middle-aged people. On Google+, the fastest-growing age group is 45–54. On Twitter, the expanding generation is people from ages 55–64.[18]		Mobile recruiting is a recruitment strategy that uses mobile technology to attract, engage and convert candidates. Mobile recruiting is often cited as a growing opportunity for recruiters to connect with candidates more efficiently with "over 89% of job seekers saying their mobile device will be an important tool and resource for their job search."[19]		Some recruiters work by accepting payments from job seekers, and in return help them to find a job. This is illegal in some countries, such as in the United Kingdom, in which recruiters must not charge candidates for their services (although websites such as LinkedIn may charge for ancillary job-search-related services). Such recruiters often refer to themselves as "personal marketers" and "job application services" rather than as recruiters.		Using Multiple-criteria decision analysis tools such as Analytic Hierarchy Process (AHP) and combining it with conventional recruitment methods provides an added advantage by helping the recruiters to make decisions when there are several diverse criteria to be considered or when the applicants lack past experience; for instance recruitment of fresh university graduates.[20]		In some companies where the recruitment volume is high, it is common to see a multi tier recruitment model where the different sub-functions are being group together to achieve efficiency.		An example of a 3 tier recruitment model:		
In the United States, a furlough (/ˈfɜːrloʊ/; from Dutch: verlof, "leave of absence") is a temporary leave of employees due to special needs of a company, which may be due to economic conditions at the specific employer or in the economy as a whole. These involuntary furloughs may be short or long term, and many of those affected may seek other temporary employment during that time.						In the United States, involuntary furloughs concerning federal government employees may be of a sudden and immediate nature. Such was the case in February 2010, when a single Senate objection prevented emergency funding measures from being implemented. As a result, 2000 federal workers for the Department of Transportation were immediately furloughed as of March 1, 2010.[1] The longest such shutdown was December 16, 1995, to January 6, 1996, which affected all non-essential employees, shutting down many services including National Institutes of Health, visa and passport processing, parks, and many others. This happened again on October 1, 2013.[2]		The United States Congress failed to pass a re-authorization of funding for the Federal Aviation Administration, and as a result, furloughed about 4,000 workers at midnight on July 22, 2011.		Congress was on the verge of forcing a government shutdown on April 8, 2011, if their plan to reduce the federal budget deficit was not resolved, which would have caused the furlough of 800,000 out of two million civilian federal employees.[3][4]		The first federal government furloughs of 2013 went into effect as a result of budget sequestration (or sequester) – the automatic spending cuts in particular categories of federal outlays. (This procedure was first used in the Gramm–Rudman–Hollings Balanced Budget Act of 1985.) The sequesters were designed to take place if the federal deficit exceeded a set of fixed deficit targets. In 2013 specifically, sequestration refers to a section of the Budget Control Act of 2011 (BCA) that was initially set to begin on January 1, 2013, as an austerity fiscal policy. These cuts were postponed by two months by the American Taxpayer Relief Act of 2012 until March 1, when this law went into effect. At that time, most federal departments and agencies began furloughing their employees in order to meet their spending cut targets. For the Department of Defense, almost all of the civilian workforce as well as most full-time, dual-status military technicians of the National Guard and the Reserves were affected. The initial furlough requirement was 176 working hours per affected employee, which was later cut to 88 hours. Due to cost-cutting measures in other areas, this furlough was further reduced to a total of 48 working hours per DoD civilian and full-time Reserve Component member.		Later, on October 1, 2013, at 12:01 am EDT, Congress' inability to agree on a spending bill led to a government shutdown. During the shutdown, most "non-essential" government employees were furloughed. This resulted in approximately 800,000 government workers being put on a leave beginning October 1. Congress later unanimously voted to restore pay to the furloughed workers.[5][6]		Board members of various school districts as well as universities implemented "furlough days" in 2009. This made students pay the same rate, if not more for their education while providing fewer educational days by forcing educators and staff members to take the day off. In states such as Georgia, the Board of Regents of the University System of Georgia included a clause so that mandatory furlough days are implemented but no classes are lost during the 2009–2010 academic year.[7]		In California, the State Employee Trades Council (SETC) voted to implement a mandatory two-day-per-month furlough policy for the staff and faculty of the CSU system.[8] The furloughs, intended to prevent layoffs, began in August 2009, and ended in June 2010. The 10% cut saved about $270 million of the CSU's $564 million budget deficit.[9]		During the global recession of 2009 companies such as Intel, Toyota, and Gannett implemented furloughs.[10]		In the federal government shutdown of 2013 federal contractors such as Lockheed Martin and United Technologies considered furloughing their own employees.[11]		The term furlough in employment can also refer to annual leave, long service leave, time off based on a company-planned schedule. For example, with a "work three weeks, off one week" schedule, a company's workforce is divided into four groups. Each group, in turn, takes a week off on furlough while the remainder work. It can also refer to a vacation from missionary work, military leave, or, in the case of convicts, parole, probation, conjugal visit, or work release.		
A time clock, sometimes known as a clock card machine or punch clock or time recorder, is a mechanical (or electronic) timepiece used to assist in tracking the hours worked by an employee of a company.		In mechanical time clocks this was accomplished by inserting a heavy paper card, called a time card, into a slot on the time clock. When the time card hit a contact at the rear of the slot, the machine would print day and time information (a timestamp) on the card.		One or more time cards could serve as a timesheet or provide the data to fill one. This allowed a timekeeper to have an official record of the hours an employee worked to calculate the pay owed an employee.		The terms Bundy clock, bundy clock, or just bundy[1] have been used in Australian English for time clocks. The term comes from brothers Willard and Harlow Bundy.						An early and influential time clock, sometimes described as the first, was invented on November 20, 1888, by Willard Le Grand Bundy,[2] a jeweler in Auburn, New York. His patent of 1890[3] speaks of mechanical time recorders for workers in terms that suggest that earlier recorders already existed, but Bundy's had various improvements; for example, each worker had his own key. A year later his brother, Harlow Bundy, organized the Bundy Manufacturing Company,[4][5] and began mass-producing time clocks.		In 1900, the time recording business of Bundy Manufacturing, along with two other time equipment businesses, was consolidated into the International Time Recording Company (ITR).[6][7][8][9]		In 1911, ITR, Bundy Mfg., and two other companies were amalgamated (via stock acquisition), forming a fifth company, Computing-Tabulating-Recording Company (CTR), which would later change its name to IBM.[10]		The Bundy Clock (see image left) was used by Birmingham City Transport to ensure that bus drivers did not depart from outlying termini before the due time; now preserved at Walsall Arboretum.		In 1909, Halbert P. Gillette explained about the state of the art around time clocks in those days:		An example of this other form of time clock, made by IBM, is pictured on the right. The face shows employee numbers which would be dialed up by employees entering and leaving the factory. The day and time of entry and exit was punched onto cards inside the box.[12]		In 1958, IBM's Time Equipment Division was sold to the Simplex Time Recorder Company. However, in the United Kingdom ITR (a subsidiary of IBM United Kingdom Ltd.) was the subject of a management buy-out in 1963 and reverted to International Time Recorders. In 1982, International Time Recorders was acquired by Blick Industries of Swindon, England, who were themselves later absorbed by Stanley Security Systems.		The first punched-card system to be linked to a Z80 microprocessor was developed by Kronos Incorporated in the late 1970s and introduced as a product in 1979.[13]		In the late 20th century, time clocks started to move away from the mechanical machines to computer-based, electronic time and attendance systems. The employee either swipes a magnetic stripe card, scans a barcode, brings an RFID (radio-frequency identification) tag into proximity with a reader, enters an employee number or uses a biometric reader to identify the employee to the system. These systems are much more advanced than the mechanical time clock: various reports can be generated, including on compliance with the European Working Time Directive, and a Bradford factor report. Employees can also use the gadget to request holidays, enter absenteeism requests and view their worked hours. User interfaces can be personalized and offer robust self-service capabilities.		Electronic time clock machines are manufactured in many designs by companies in China and sold under various brand names in places around the world, with accompanying software to extract the data from a single time clock machine, or several machines, and process the data into reports. In most cases local suppliers offer technical support and in some cases installation services.		More recently, time clocks have started to adopt technology commonly seen in phones and tablets - called 'Smartclocks'. The "state of the art" smartclocks come with multi-touch screens, full color displays, real time monitoring for problems, wireless networking and over the air updates. Some of the smartclocks use front-facing cameras to capture employee clock-ins to deter "buddy clocking", a problem usually requiring expensive biometric clocks. With the increasing popularity of cloud-based software, some of the newer time clocks are built to work seamlessly with the cloud.[14]		A basic time clock will just stamp the date and time on a time card, similar to a parking validation machine. These will usually be activated by a button that a worker must press to stamp their card, or stamp upon full insertion. Some machines use punch hole cards instead of stamping, which can facilitate automated processing on machinery not capable of optical character recognition.		There are also variations based on manufacture and machine used, and whether the user wants to record weekly or monthly recordings. The time cards usually have the workdays, "time in", and "time out" areas marked on them so that employees can "punch in" or "punch out" in the correct place. The employee may be responsible for lining up the correct area of the card to be punched or stamped. Some time clocks feature a bell or signal relay to alert employees as to a certain time or break.[citation needed]		Fraudulent operation of time clocks can include overstamping, where one time is stamped over another, and buddy stamping, where a friend clocks in another member of staff.		Self-calculating machines are similar to basic time clocks. Nevertheless, at the end of each period the total time recorded is added up allowing for quicker processing by human resources or payroll. These machines sometimes have other functions such as automatic stamping, dual-colour printing, and automated column shift.[citation needed]		Software based time and attendance systems are similar to paper-based systems, but they rely on computers and check-in terminals. They are backed up with software that can be integrated with the human resources department and in some cases payroll software. These types of systems are becoming more popular but due to high initial costs they are usually only adopted by large business of over 30 employees. Despite this they can save a business a lot of money every year by cutting down errors and reducing administration time.[citation needed]		With the mass market proliferation of mobile devices (smart phones, handheld devices), new types of self-calculating time tracking systems have been invented which allow a mobile workforce – such as painting companies or construction companies - to track employees 'on' and 'off' hours. This is generally accomplished through either a mobile application, or an IVR based phone call in system. Using a mobile device allows enterprises to better validate that their employees or suppliers are physically 'clocking in' at a specific location using the GPS functionality of a mobile phone for extra validation.		Biometric time clocks are a feature of more advanced time and attendance systems. Rather than using a key, code or chip to identify the user, they rely on a unique attribute of the user, such as a hand print, finger print, finger vein, palm vein, facial recognition, iris or retina. The user will have their attribute scanned into the system. Biometric readers are often used in conjunction with an access control system, granting the user access to a building, and at the same time clocking them in recording the time and date. These systems also attempt to cut down on fraud such as "buddy clocking." When combined with an access control system they can help prevent other types of fraud such as 'ghost employees', where additional identities are added to payroll but don't exist.		
Continuing education (similar to further education in the United Kingdom and Ireland) is an all-encompassing term within a broad list of post-secondary learning activities and programs. The term is used mainly in the United States and Canada.		Recognized forms of post-secondary learning activities within the domain include: degree credit courses by non-traditional students, non-degree career training, workforce training, and formal personal enrichment courses (both on-campus and online).[1][2]		General continuing education is similar to adult education, at least in being intended for adult learners, especially those beyond traditional undergraduate college or university age.		Frequently, in the United States and Canada continuing education courses are delivered through a division or school of continuing education of a college or university known sometimes as the university extension or extension school. The Organisation for Economic Co-operation and Development argued, however, that continuing education should be "'fully integrated into institutional life rather than being often regarded as a separate and distinctive operation employing different staff' if it is to feed into mainstream programmes and be given the due recognition deserved by this type of provision".[3]						The Chautauqua Institution, originally the Chautauqua Lake Sunday School Assembly, was founded in 1874 "as an educational experiment in out-of-school, vacation learning. It was successful and broadened almost immediately beyond courses for Sunday school teachers to include academic subjects, music, art and physical education."[4]		Cornell University was among higher education institutions that began offering university-based continuing education, primarily to teachers, through extension courses in the 1870s. As noted in the Cornell Era of February 16, 1877, the university offered a "Tour of the Great Lakes" program for "teachers and others" under the direction of Professor Theodore B. Comstock, head of Cornell's department of geology.[5]		The University of Wisconsin–Madison began its continuing education program in 1907.[6][7] The New School for Social Research, founded in 1919, was initially devoted to adult education.[8] In 1969, Empire State College, a unit of the State University of New York, was the first institution in the US to exclusively focus on providing higher education to adult learners. In 1976 the University of Florida created its own Division of Continuing Education and most courses were offered on evenings or weekends to accommodate the schedules of working students.[9]		Within the domain of continuing education, professional continuing education is a specific learning activity generally characterized by the issuance of a certificate or continuing education units (CEU) for the purpose of documenting attendance at a designated seminar or course of instruction. Licensing bodies in a number of fields (such as teaching and healthcare) impose continuing education requirements on members who hold licenses to continue practicing a particular profession. These requirements are intended to encourage professionals to expand their foundations of knowledge and stay up-to-date on new developments.		Depending on the field, these requirements may be satisfied through college or university coursework, extension courses or conferences and seminars attendance. Although individual professions may have different standards, the most widely accepted standard, developed by the International Association for Continuing Education & Training, is that ten contact hours equals one Continuing Education Unit.[10] Not all professionals use the CEU convention. For example, the American Psychological Association accredits sponsors of continuing education such as PsychContinuingEd.com and uses simply a CE approach. In contrast to the CEU, the CE credit is typically one CE credit for each hour of contact.		In the spring of 2009, Eduventures, a higher education consulting firm, released the results of a study that illustrated that the recession had made a significant impact on the views of prospective continuing education students. A survey of 1,500 adults who planned to enroll in a course or program within the next two years determined that while nearly half of respondents believed that the value of education had risen due to the recession, over two-thirds said the state of the economy had affected their plans to pursue continuing education.[11]		The method of delivery of continuing education can include traditional types of classroom lectures and laboratories. However, many continuing education programs make heavy use of distance education, which not only includes independent study, but can also include videotaped material, broadcast programming or online education which has more recently dominated the distance learning community.		
Technological unemployment is the loss of jobs caused by technological change. Such change typically includes the introduction of labour-saving "mechanical-muscle" machines or more efficient "mechanical-mind" processes (automation). Just as horses employed as prime movers were gradually made obsolete by the automobile, humans' jobs have also been affected throughout modern history. Historical examples include artisan weavers reduced to poverty after the introduction of mechanised looms. During World War II, Alan Turing's Bombe machine compressed and decoded thousands of man-years worth of encrypted data in a matter of hours. A contemporary example of technological unemployment is the displacement of retail cashiers by self-service tills.		That technological change can cause short-term job losses is widely accepted. The view that it can lead to lasting increases in unemployment has long been controversial. Participants in the technological unemployment debates can be broadly divided into optimists and pessimists. Optimists agree that innovation may be disruptive to jobs in the short term, yet hold that various compensation effects ensure there is never a long-term negative impact on jobs. Whereas pessimists contend that at least in some circumstances, new technologies can lead to a lasting decline in the total number of workers in employment. The phrase "technological unemployment" was popularised by John Maynard Keynes in the 1930s. Yet the issue of machines displacing human labour has been discussed since at least Aristotle's time.		Prior to the 18th century both the elite and common people would generally take the pessimistic view on technological unemployment, at least in cases where the issue arose. Due to generally low unemployment in much of pre-modern history, the topic was rarely a prominent concern. In the 18th century fears over the impact of machinery on jobs intensified with the growth of mass unemployment, especially in Great Britain which was then at the forefront of the Industrial Revolution. Yet some economic thinkers began to argue against these fears, claiming that overall innovation would not have negative effects on jobs. These arguments were formalised in the early 19th century by the classical economists. During the second half of the 19th century, it became increasingly apparent that technological progress was benefiting all sections of society, including the working class. Concerns over the negative impact of innovation diminished. The term "Luddite fallacy" was coined to describe the thinking that innovation would have lasting harmful effects on employment.		The view that technology is unlikely to lead to long term unemployment has been repeatedly challenged by a minority of economists. In the early 1800s these included Ricardo himself. There were dozens of economists warning about technological unemployment during brief intensifications of the debate that spiked in the 1930s and 1960s. Especially in Europe, there were further warnings in the closing two decades of the twentieth century, as commentators noted an enduring rise in unemployment suffered by many industrialised nations since the 1970s. Yet a clear majority of both professional economists and the interested general public held the optimistic view through most of the 20th century.		In the second decade of the 21st century, a number of studies have been released suggesting that technological unemployment may be increasing worldwide. Further increases are forecast for the years to come. While many economists and commentators still argue such fears are unfounded, as was widely accepted for most of the previous two centuries, concern over technological unemployment is growing once again.[1][2][3] A report in Wired in 2017 quotes knowledgeable people such as economist Gene Sperling and management professor Andrew McAfee on the idea that handling existing and impending job loss to automation is a "significant issue".[4] Regarding a recent claim by Treasury Secretary Steve Mnuchin that automation is not "going to have any kind of big effect on the economy for the next 50 or 100 years", says McAfee, "I don't talk to anyone in the field who believes that."[4] Recent technological innovations have the potential to render humans obsolete with the professional, white-collar, low-skilled, creative fields, and other "mental jobs".[5][6]						All participants in the technological employment debates agree that temporary job losses can result from technological innovation. Similarly, there is no dispute that innovation sometimes has positive effects on workers. Disagreement focuses on whether it is possible for innovation to have a lasting negative impact on overall employment. Levels of persistent unemployment can be quantified empirically, but the causes are subject to debate. Optimists accept short term unemployment may be caused by innovation, yet claim that after a while, compensation effects will always create at least as many jobs as were originally destroyed. While this optimistic view has been continually challenged, it was dominant among mainstream economists for most of the 19th and 20th centuries.[8][9] For example, labor economists Jacob Mincer and Stephan Danninger develop an empirical study using micro-data from the Panel Study of Income Dynamics, and find that although in the short run, technological progress seems to have unclear effects on aggregate unemployment, it reduces unemployment in the long run. When they include a 5-year lag, however, the evidence supporting a short-run employment effect of technology seems to disappear as well, suggesting that technological unemployment "appears to be a myth".[10]		The concept of structural unemployment, a lasting level of joblessness that does not disappear even at the high point of the business cycle, became popular in the 1960s. For pessimists, technological unemployment is one of the factors driving the wider phenomena of structural unemployment. Since the 1980s, even optimistic economists have increasingly accepted that structural unemployment has indeed risen in advanced economies, but they have tended to blame this on globalisation and offshoring rather than technological change. Others claim a chief cause of the lasting increase in unemployment has been the reluctance of governments to pursue expansionary policies since the displacement of Keynesianism that occurred in the 1970s and early 80s.[8][11][12] In the 21st century, and especially since 2013, pessimists have been arguing with increasing frequency that lasting worldwide technological unemployment is a growing threat.[9][13][14][15]		Compensation effects are labour-friendly consequences of innovation which "compensate" workers for job losses initially caused by new technology. In the 1820s, several compensation effects were described by Say in response to Ricardo's statement that long term technological unemployment could occur. Soon after, a whole system of effects was developed by Ramsey McCulloch. The system was labelled "compensation theory" by Marx, who proceeded to attack the ideas, arguing that none of the effects were guaranteed to operate. Disagreement over the effectiveness of compensation effects has remained a central part of academic debates on technological unemployment ever since.[12][16]		Compensation effects include:		The "by new machines" effect is now rarely discussed by economists; it is often accepted that Marx successfully refuted it.[12] Even pessimists often concede that product innovation associated with the "by new products" effect can sometimes have a positive effect on employment. An important distinction can be drawn between 'process' and 'product' innovations.[note 1] Evidence from Latin America seems to suggest that product innovation significantly contributes to the employment growth at the firm level, more so than process innovation.[17] The extent to which the other effects are successful in compensating the workforce for job losses has been extensively debated throughout the history of modern economics; the issue is still not resolved.[12][18] One such effect that potentially complements the compensation effect is job multiplier. According to research developed by Enrico Moretti, with each additional skilled job created in high tech industries in a given city, more than two jobs are created in the non-tradable sector. His findings suggest that technological growth and the resulting job-creation in high-tech industries might have a more significant spillover effect than we have anticipated.[19] Evidence from Europe also supports such a job multiplier effect, showing local high-tech jobs could create five additional low-tech jobs.[20]		Many economists now pessimistic about technological unemployment accept that compensation effects did largely operate as the optimists claimed through most of the 19th and 20th century. Yet they hold that the advent of computerisation means that compensation effects are now less effective. An early example of this argument was made by Wassily Leontief in 1983. He conceded that after some disruption, the advance of mechanization during the Industrial Revolution actually increased the demand for labour as well as increasing pay due to effects that flow from increased productivity. While early machines lowered the demand for muscle power, they were unintelligent and needed large armies of human operators to remain productive. Yet since the introduction of computers into the workplace, there is now less need not just for muscle power but also for human brain power. Hence even as productivity continues to rise, the lower demand for human labour may mean less pay and employment.[12][14][21] However, this argument is not fully supported by more recent empirical studies. One research done by Erik Brynjolfsson and Lorin M. Hitt in 2003 presents direct evidence that suggests a positive short-term effect of computerization on firm-level measured productivity and output growth. In addition, they find the long-term productivity contribution of computerization and technological changes might even be greater.		The term "Luddite fallacy" is sometimes used to express the view that those concerned about long term technological unemployment are committing a fallacy, as they fail to account for compensation effects. People who use the term typically expect that technological progress will have no long term impact on employment levels, and eventually will raise wages for all workers, because progress helps to increase the overall wealth of society. The term is based on the early 19th century example of the Luddites. During the 20th century and the first decade of the 21st century, the dominant view among economists has been that belief in long term technological unemployment was indeed a fallacy. More recently, there has been increased support for the view that the so-called fallacy may after all be correct.[9][23][24]		There are two underlying premises for why long-term difficulty could develop. The one that has traditionally been deployed is that ascribed to the Luddites (whether or not it is a truly accurate summary of their thinking), which is that there is a finite amount of work available and if machines do that work, there can be no other work left for humans to do. Economists call this the lump of labour fallacy, arguing that in reality no such limitation exists. However, the other premise is that it is possible for long-term difficulty to arise that has nothing to do with any lump of labour. In this view, the amount of work that can exist is infinite, but (1) machines can do most of the "easy" work, (2) the definition of what is "easy" expands as information technology progresses, and (3) the work that lies beyond "easy" (the work that requires more skill, talent, knowledge, and insightful connections between pieces of knowledge) may require greater cognitive faculties than most humans are able to supply, as point 2 continually advances. This latter view is the one supported by many modern advocates of the possibility of long-term, systemic technological unemployment.		A common view among those discussing the effect of innovation on the labour market has been that it mainly hurts those with low skills, while often benefiting skilled workers. According to scholars such as Lawrence F. Katz, this may have been true for much of the twentieth century, yet in the 19th century, innovations in the workplace largely displaced costly skilled artisans, and generally benefited the low skilled. While 21st century innovation has been replacing some unskilled work, other low skilled occupations remain resistant to automation, while white collar work requiring intermediate skills is increasingly being performed by autonomous computer programs.[25][26][27]		Some recent studies however, such as a 2015 paper by Georg Graetz and Guy Michaels, found that at least in the area they studied – the impact of industrial robots – innovation is boosting pay for highly skilled workers while having a more negative impact on those with low to medium skills.[28] A 2015 report by Carl Benedikt Frey, Michael Osborne and Citi Research, agreed that innovation had been disruptive mostly to middle-skilled jobs, yet predicted that in the next ten years the impact of automation would fall most heavily on those with low skills.[29]		Geoff Colvin at Forbes argued that predictions on the kind of work a computer will never be able to do have proven inaccurate. A better approach to anticipate the skills on which humans will provide value would be to find out activities where we will insist that humans remain accountable for important decisions, such as with judges, CEOs, bus drivers and government leaders, or where human nature can only be satisfied by deep interpersonal connections, even if those tasks could be automated.[30]		In contrast, others see even skilled human laborers being obsolete. Oxford academics Carl Benedikt Frey and Michael A Osborne have predicted computerization could make nearly half of jobs redundant within 10 to 20 years,[31] of the 702 professions assessed, they found a strong correlation between education and income with ability to be automated, with office jobs and service work being some of the more at risk.[32] In 2012 co-founder of Sun Microsystems Vinod Khosla predicted that 80% of medical doctors jobs would be lost in the next two decades to automated machine learning medical diagnostic software.[33]		There has been a lot of empirical research that attempts to quantify the impact of technological unemployment, mostly done at the microeconomic level. Most existing firm-level research has found a labor-friendly nature of technological innovations. For example, German economists Stefan Lachenmaier and Horst Rottmann find that both product and process innovation have a positive effect on employment. Interestingly, they also find that process innovation has a more significant job creation effect than product innovation.[34] This result is supported by evidence in the United States as well, which shows that manufacturing firm innovations have a positive effect on the total number of jobs, not just limited to firm-specific behavior.[35]		At the industry level, however, researchers have found mixed results with regard to the employment effect of technological changes. A 2017 study on manufacturing and service sectors in 11 European countries suggests that positive employment effect of technological innovations only exist in the medium- and high-tech sectors.There also seems to be a negative correlation between employment and capital formation, which suggests that technological progress could potentially be labor-saving given that process innovation is often incorporated in investment.[36]		Limited macroeconomic analysis has been done to study the relationship between technological shocks and unemployment. The small amount of existing research, however, suggests mixed results. Italian economist Marco Vivarelli finds that the labor-saving effect of process innovation seems to have affected the Italian economy more negatively than the United States. On the other hand, the job creating effect of product innovation could only be observed in the United States, not Italy.[37] Another study in 2013 finds a more transitory, rather than permanent, unemployment effect of technological change.[38]		There have been four main approaches that attempt to capture and document technological innovation quantitatively. The first one, proposed by Jordi Gali in 1999 and further developed by Neville Francis and Valerie A. Ramey in 2005, is to use long-run restrictions in a Vector Autoregression (VAR) to identify technological shocks, assuming that only technology affects long-run productivity.[39][40]		The second approach is from Susanto Basu, John Fernald and Miles Kimball.[41] They create a measure of aggregate technology change with augmented Solow residuals, controlling for aggregate, non-technological effects such as non-constant returns and imperfect competition.		The third method, initially developed by John Shea in 1999, takes a more direct approach and employs observable indicators such as Research and Development (R&D) spending, and number of patent applications.[42] This measure of technological innovation is very widely used in empirical research, since it does not rely on the assumption that only technology affects long-run productivity, and fairly accurately captures the output variation based on input variation. However, there are limitations with direct measures such as R&D. For example, since R&D only measures the input in innovation, the output is unlikely to be perfectly correlated with the input. In addition, R&D fails to capture the indeterminate lag between developing a new product or service, and bringing it to market.[43]		The fourth approach, constructed by Michelle Alexopoulos, looks at the number of new titles published in the fields of technology and computer science to reflect technological progress, which turns out to be consistent with R&D expenditure data.[44] Compared with R&D, this indicator captures the lag between changes in technology.		According to author Gregory Woirol, the phenomenon of technological unemployment is likely to have existed since at least the invention of the wheel.[46] Ancient societies had various methods for relieving the poverty of those unable to support themselves with their own labour. Ancient China and ancient Egypt may have had various centrally run relief programmes in response to technological unemployment dating back to at least the second millennium BC.[47] Ancient Hebrews and adherents of the ancient Vedic religion had decentralised responses where aiding the poor was encouraged by their faiths.[47] In ancient Greece, large numbers of free labourers could find themselves unemployed due to both the effects of ancient labour saving technology and to competition from slaves ("machines of flesh and blood"[48]). Sometimes, these unemployed workers would starve to death or were forced into slavery themselves although in other cases they were supported by handouts. Pericles responded to perceived technological unemployment by launching public works programmes to provide paid work to the jobless. Conservatives criticized Pericle's programmes for wasting public money but were defeated.[49]		Perhaps, the earliest example of a scholar discussing the phenomenon of technological unemployment occurs with Aristotle, who speculated in Book One of Politics that if machines could become sufficiently advanced, there would be no more need for human labour.[50]		Similar to the Greeks, ancient Romans, responded to the problem of technological unemployment by relieving poverty with handouts. Several hundred thousand families were sometimes supported like this at once.[47] Less often, jobs were directly created with public works programmes, such as those launched by the Gracchi. Various emperors even went as far as to refuse or ban labour saving innovations.[51][52] Labour shortages began to develop in the Roman empire towards the end of the second century AD, and from this point mass unemployment in Europe appears to have largely receded for over a millennium.[53]		The medieval and early renaissance period saw the widespread adoption of newly invented technologies as well as older ones which had been conceived yet barely used in the Classical era.[54] Mass unemployment began to reappear in Europe in the 15th century, partly as a result of population growth, and partly due to changes in the availability of land for subsistence farming caused by early enclosures.[55] As a result of the threat of unemployment, there was less tolerance for disruptive new technologies. European authorities would often side with groups representing subsections of the working population, such as Guilds, banning new technologies and sometimes even executing those who tried to promote or trade in them.[56]		In Great Britain, the ruling elite began to take a less restrictive approach to innovation somewhat earlier than in much of continental Europe, which has been cited as a possible reason for Britain's early lead in driving the Industrial Revolution.[57] Yet concern over the impact of innovation on employment remained strong through the 16th and early 17th century. A famous example of new technology being refused occurred when the inventor William Lee invited Queen Elizabeth I to view a labour saving knitting machine. The Queen declined to issue a patent on the grounds that the technology might cause unemployment among textile workers. After moving to France and also failing to achieve success in promoting his invention, Lee returned to England but was again refused by Elizabeth's successor James I for the same reason.[14]		Especially after the Glorious Revolution, authorities became less sympathetic to workers concerns about losing their jobs due to innovation. An increasingly influential strand of Mercantalist thought held that introducing labour saving technology would actually reduce unemployment, as it would allow British firms to increase their market share against foreign competition. From the early 18th century workers could no longer rely on support from the authorities against the perceived threat of technological unemployment. They would sometimes take direct action, such as machine breaking, in attempts to protect themselves from disruptive innovation. Schumpeter notes that as the 18th century progressed, thinkers would raise the alarm about technological unemployment with increasing frequency, with von Justi being a prominent example.[58] Yet Schumpeter also notes that the prevailing view among the elite solidified on the position that technological unemployment would not be a long term problem.[14][55]		It was only in the 19th century that debates over technological unemployment became intense, especially in Great Britain where many economic thinkers of the time were concentrated. Building on the work of Dean Tucker and Adam Smith, political economists began to create what would become the modern discipline of economics.[note 2] While rejecting much of mercantilism, members of the new discipline largely agreed that technological unemployment would not be an enduring problem. In the first few decades of the 19th century, several prominent political economists did, however, argue against the optimistic view, claiming that innovation could cause long-term unemployment. These included Sismondi,[59] Malthus , J S Mill, and from 1821, Ricardo himself.[60] As arguably the most respected political economist of his age, Ricardo's view was challenging to others in the discipline. The first major economist to respond was Jean-Baptiste Say, who argued that no one would introduce machinery if they were going to reduce the amount of product,[note 3] and that as Say's Law states that supply creates its own demand, any displaced workers would automatically find work elsewhere once the market had had time to adjust.[61] Ramsey McCulloch expanded and formalised Say's optimistic views on technological unemployment, and was supported by others such as Charles Babbage, Nassau Senior and many other lesser known political economists. Towards the middle of the 19th century, Karl Marx joined the debates. Building on the work of Ricardo and Mill, Marx went much further, presenting a deeply pessimistic view of technological unemployment; his views attracted many followers and founded an enduring school of thought but mainstream economics was not dramatically changed. By the 1870s, at least in Great Britain, technological unemployment faded both as a popular concern and as an issue for academic debate. It had become increasingly apparent that innovation was increasing prosperity for all sections of British society, including the working class. As the classical school of thought gave way to Neoclassical economics, mainstream thinking was tightened to take into account and refute the pessimistic arguments of Mill and Ricardo.[62]		For the first two decades of the 20th century, mass unemployment was not the major problem it had been in the first half of the 19th. While the Marxist school and a few other thinkers still challenged the optimistic view, technological unemployment was not a significant concern for mainstream economic thinking until the mid to late 1920s. In the 1920s mass unemployment re-emerged as a pressing issue within Europe. At this time the U.S. was generally more prosperous, but even there urban unemployment had begun to increase from 1927. Rural American workers had been suffering job losses from the start of the 1920s; many had been displaced by improved agricultural technology, such as the tractor. The centre of gravity for economic debates had by this time moved from Great Britain to the United States, and it was here that the 20th centuries two great periods of debate over technological unemployment largely occurred.[63]		The peak periods for the two debates were in the 1930s and the 1960s. According to economic historian Gregory R Woirol, the two episodes share several similarities.[64] In both cases academic debates were preceded by an outbreak of popular concern, sparked by recent rises in unemployment. In both cases the debates were not conclusively settled, but faded away as unemployment was reduced by an outbreak of war – World War II for the debate of the 1930s, and the Vietnam war for the 1960s episodes. In both cases, the debates were conducted within the prevailing paradigm at the time, with little reference to earlier thought. In the 1930s, optimists based their arguments largely on neo-classical beliefs in the self-correcting power of markets to automatically reduce any short-term unemployment via compensation effects. In the 1960s, faith in compensation effects was less strong, but the mainstream Keynesian economists of the time largely believed government intervention would be able to counter any persistent technological unemployment that was not cleared by market forces. Another similarity was the publication of a major Federal study towards the end of each episode, which broadly found that long-term technological unemployment was not occurring (though the studies did agree innovation was a major factor in the short term displacement of workers, and advised government action to provide assistance.)[note 4][64]		As the golden age of capitalism came to a close in the 1970s, unemployment once again rose, and this time generally remained relatively high for the rest of the century, across most advanced economies. Several economists once again argued that this may be due to innovation, with perhaps the most prominent being Paul Samuelson.[65] A number of popular works warning of technological unemployment were also published. These included James S. Albus's 1976 book titled Peoples' Capitalism: The Economics of the Robot Revolution;[66][67] David F. Noble with works published in 1984[68] and 1993;[69] Jeremy Rifkin and his 1995 book The End of Work;[70] and the 1996 book The Global Trap[71] In general, the closing decades of the 20th century saw much more concern expressed over technological unemployment in Europe, compared with the U.S.[72] For the most part, other than during the periods of intense debate in the 1930s and 60s, the consensus in the 20th century among both professional economists and the general public remained that technology does not cause long-term joblessness.[73]		The general consensus that innovation does not cause long-term unemployment held strong for the first decade of the 21st century although it continued to be challenged by a number of academic works,[12][18] and by popular works such as Marshall Brain's Robotic Nation[75] and Martin Ford's The Lights in the Tunnel: Automation, Accelerating Technology and the Economy of the Future.[76]		Concern about technological unemployment grew in 2013 due in part to a number of studies predicting substantially increased technological unemployment in forthcoming decades and empirical evidence that, in certain sectors, employment is falling worldwide despite rising output, thus discounting globalization and offshoring as the only causes of increasing unemployment.[13][14][77]		In 2013, professor Nick Bloom of Stanford University stated there had recently been a major change of heart concerning technological unemployment among his fellow economists.[78] In 2014 the Financial Times reported that the impact of innovation on jobs has been a dominant theme in recent economic discussion.[79] According to the academic and former politician Michael Ignatieff writing in 2014, questions concerning the effects of technological change have been "haunting democratic politics everywhere".[80] Concerns have included evidence showing worldwide falls in employment across sectors such as manufacturing; falls in pay for low and medium skilled workers stretching back several decades even as productivity continues to rise; the increase in often precarious platform mediated employment; and the occurrence of "jobless recoveries" after recent recessions. The 21st century has seen a variety of skilled tasks partially taken over by machines, including translation, legal research and even low level journalism. Care work, entertainment, and other tasks requiring empathy, previously thought safe from automation, have also begun to be performed by robots.[13][14][81][82][83][84]		Former U.S. Treasury Secretary and Harvard economics professor Lawrence Summers stated in 2014 that he no longer believed automation would always create new jobs and that "This isn't some hypothetical future possibility. This is something that's emerging before us right now."[note 5][7][85][86] While himself an optimist about technological unemployment, professor Mark MacCarthy stated in the fall of 2014 that it is now the "prevailing opinion" that the era of technological unemployment has arrived.[74]		At the 2014 Davos meeting, Thomas Friedman reported that the link between technology and unemployment seemed to have been the dominant theme of that year's discussions. A survey at Davos 2014 found that 80% of 147 respondents agreed that technology was driving jobless growth.[87] At the 2015 Davos, Gillian Tett found that almost all delegates attending a discussion on inequality and technology expected an increase in inequality over the next five years, and gives the reason for this as the technological displacement of jobs.[88] 2015 saw Martin Ford win the Financial Times and McKinsey Business Book of the Year Award for his Rise of the Robots: Technology and the Threat of a Jobless Future, and saw the first world summit on technological unemployment, held in New York. In late 2015, further warnings of potential worsening for technological unemployment came from Andy Haldane, the Bank of England's chief economist, and from Ignazio Visco, the governor of the Bank of Italy.[89][90]		Other economists, however, remain optimistic about the prospects to avoid long-term technological unemployment. In 2014, Pew Research canvassed 1,896 technology professionals and economists and found a split of opinion: 48% of respondents believed that new technologies would displace more jobs than they would create by the year 2025, while 52% maintained that they would not.[91] Not all recent empirical studies have found evidence to support the pessimistic view of technological unemployment. A study released in 2015, examining the impact of industrial robots in 17 countries between 1993 and 2007, found no overall reduction in employment was caused by the robots, and that there was a slight increase in overall wages.[28] Economics professor Bruce Chapman from Australian National University has advised that studies such as Frey and Osborne's tend to overstate the probability of future job losses, as they don't account for new employment likely to be created, due to technology, in what are currently unknown areas.[92]		Research by the Oxford Martin School showed that employees engaged in "tasks following well-defined procedures that can easily be performed by sophisticated algorithms" are at risk of displacement. The study, published in 2013, shows that automation can affect both skilled and unskilled work and both high and low-paying occupations; however, low-paid physical occupations are most at risk.[14] However, according to a study published in McKinsey Quarterly[93] in 2015 the impact of computerization in most cases is not replacement of employees but automation of portions of the tasks they perform.[94]		Historically, innovations were sometimes banned due to concerns about their impact on employment. Since the development of modern economics, however, this option has generally not even been considered as a solution, at least not for the advanced economies. Even commentators who are pessimistic about long-term technological unemployment invariably consider innovation to be an overall benefit to society, with JS Mill being perhaps the only prominent western political economist to have suggested prohibiting the use of technology as a possible solution to unemployment.[16]		Gandhian economics called for a delay in the uptake of labour saving machines until unemployment was alleviated, however this advice was largely rejected by Nehru who was to become prime minister once India achieved its independence. The policy of slowing the introduction of innovation so as to avoid technological unemployment was however implemented in the 20th century within China under Mao's administration.[96][97][98]		The use of various forms of subsidies has often been accepted as a solution to technological unemployment even by conservatives and by those who are optimistic about the long term effect on jobs. Welfare programmes have historically tended to be more durable once established, compared with other solutions to unemployment such as directly creating jobs with public works. Despite being the first person to create a formal system describing compensation effects, Ramsey McCulloch and most other classical economists advocated government aid for those suffering from technological unemployment, as they understood that market adjustment to new technology was not instantaneous and that those displaced by labour-saving technology would not always be able to immediately obtain alternative employment through their own efforts.[16]		Several commentators have argued that traditional forms of welfare payment may be inadequate as a response to the future challenges posed by technological unemployment, and have suggested a basic income as an alternative. People advocating some form of basic income as a solution to technological unemployment include Martin Ford, [99] Erik Brynjolfsson,[79] Robert Reich and Guy Standing. Reich has gone as far as to say the introduction of a basic income, perhaps implemented as a negative income tax is "almost inevitable",[100] while Standing has said he considers that a basic income is becoming "politically essential".[101] Since late 2015, new basic income pilots have been announced in Finland, the Netherlands, and Canada. Further recent advocacy for basic income has arisen from a number of technology entrepreneurs, the most prominent being Sam Altman, president of Y Combinator.[102]		Skepticism about basic income includes both right and left elements, and proposals for different forms of it have come from all segments of the spectrum. For example, while the best-known proposed forms (with taxation and distribution) are usually thought of as left-leaning ideas that right-leaning people try to defend against, other forms have been proposed even by libertarians, such as von Hayek and Friedman. Republican president Nixon's Family Assistance Plan (FAP) of 1969, which had much in common with basic income, passed in the House but was defeated in the Senate.[103]		One objection to basic income is that it could be a disincentive to work, but evidence from older pilots in India, Africa, and Canada indicates that this does not happen and that a basic income encourages low-level entrepreneurship and more productive, collaborative work. Another objection is that funding it sustainably is a huge challenge. While new revenue-raising ideas have been proposed such as Martin Ford's wage recapture tax, how to fund a generous basic income remains a debated question, and skeptics have dismissed it as utopian. Even from a progressive viewpoint, there are concerns that a basic income set too low may not help the economically vulnerable, especially if financed largely from cuts to other forms of welfare.[101][104][105][106]		To better address both the funding concerns and concerns about government control, one alternative model is that the cost and control would be distributed across the private sector instead of the public sector. Companies across the economy would be required to employ humans, but the job descriptions would be left to private innovation, and individuals would have to compete to be hired and retained. This would be a for-profit sector analog of basic income, that is, a market-based form of basic income. It differs from a job guarantee in that the government is not the employer (rather, companies are) and there is no aspect of having employees who "cannot be fired", a problem that interferes with economic dynamism. The economic salvation in this model is not that every individual is guaranteed a job, but rather just that enough jobs exist that massive unemployment is avoided and employment is no longer solely the privilege of only the very smartest or highly trained 20% of the population. Another option for a market-based form of basic income has been proposed by the Center for Economic and Social Justice (CESJ) as part of "a Just Third Way" (a Third Way with greater justice) through widely distributed power and liberty. Called the Capital Homestead Act,[107] it is reminiscent of James S. Albus's Peoples' Capitalism[66][67] in that money creation and securities ownership are widely and directly distributed to individuals rather than flowing through, or being concentrated in, centralized or elite mechanisms.		Improved availability to quality education, including skills training for adults, is a solution that in principle at least is not opposed by any side of the political spectrum, and welcomed even by those who are optimistic about long-term technological employment. Improved education paid for by government tends to be especially popular with industry. However, several academics have argued that improved education alone will not be sufficient to solve technological unemployment, pointing to recent declines in the demand for many intermediate skills, and suggesting that not everyone is capable in becoming proficient in the most advanced skills.[25][26][27] Kim Taipale has said that "The era of bell curve distributions that supported a bulging social middle class is over... Education per se is not going to make up the difference."[108] while back in 2011 Paul Krugman argued that better education would be an insufficient solution to technological unemployment.[109]		Programmes of Public works have traditionally been used as way for governments to directly boost employment, though this has often been opposed by some, but not all, conservatives. Jean-Baptiste Say, although generally associated with free market economics, advised that public works could be a solution to technological unemployment.[110] Some commentators, such as professor Mathew Forstater, have advised that public works and guaranteed jobs in the public sector may be the ideal solution to technological unemployment, as unlike welfare or guaranteed income schemes they provide people with the social recognition and meaningful engagement that comes with work.[111][112]		For less developed economies, public works may be an easier to administrate solution compared to universal welfare programmes.[21] As of 2015, calls for public works in the advanced economies have been less frequent even from progressives, due to concerns about sovereign debt[citation needed]. A partial exception is for spending on infrastructure, which has been recommended as a solution to technological unemployment even by economists previously associated with a neoliberal agenda, such as Larry Summers.[113]		In 1870, the average American worker clocked up about 75 hours per week. Just prior to World War II working hours had fallen to about 42 per week, and the fall was similar in other advanced economies. According to Wassily Leontief, this was a voluntary increase in technological unemployment. The reduction in working hours helped share out available work, and was favoured by workers who were happy to reduce hours to gain extra leisure, as innovation was at the time generally helping to increase their rates of pay.[21]		Further reductions in working hours have been proposed as a possible solution to unemployment by economists including John R. Commons, Lord Keynes and Luigi Pasinetti. Yet once working hours have reached about 40 hours per week, workers have been less enthusiastic about further reductions, both to prevent loss of income and as many value engaging in work for its own sake. Generally, 20th-century economists had argued against further reductions as a solution to unemployment, saying it reflects a Lump of labour fallacy.[114] In 2014, Google's co-founder, Larry Page, suggested a four-day workweek, so as technology continues to displace jobs, more people can find employment.[85][115][116]		Several solutions have been proposed which don't fall easily into the traditional left-right political spectrum. This includes broadening the ownership of robots and other productive capital assets. Enlarging the ownership of technologies has been advocated by people including James S. Albus[66][117] John Lanchester,[118] Richard B. Freeman,[105] and Noah Smith.[119] Jaron Lanier has proposed a somewhat similar solution: a mechanism where ordinary people receive "nano payments" for the big data they generate by their regular surfing and other aspects of their online presence.[120]		The Zeitgeist Movement (TZM), The Venus Project (TVP) as well as various individuals and organizations propose structural changes towards a form of a post-scarcity economy in which people are 'freed' from their automatable, monotonous jobs, instead of 'losing' their jobs. In the system proposed by TZM all jobs are either automated, abolished for bringing no true value for society (such as ordinary advertising), rationalized by more efficient, sustainable and open processes and collaboration or carried out based on altruism and social relevance (see also: Whuffie), opposed to compulsion or monetary gain.[121][122][123][124][125] The movement also speculates that the free time made available to people will permit a renaissance of creativity, invention, community and social capital as well as reducing stress.[121]		The threat of technological unemployment has occasionally been used by free market economists as a justification for supply side reforms, to make it easier for employers to hire and fire workers. Conversely, it has also been used as a reason to justify an increase in employee protection.[11][126]		Economists including Larry Summers have advised a package of measures may be needed. He advised vigorous cooperative efforts to address the "myriad devices" – such as tax havens, bank secrecy, money laundering, and regulatory arbitrage – which enable the holders of great wealth to avoid paying taxes, and to make it more difficult to accumulate great fortunes without requiring "great social contributions" in return. Summers suggested more vigorous enforcement of anti-monopoly laws; reductions in "excessive" protection for intellectual property; greater encouragement of profit-sharing schemes that may benefit workers and give them a stake in wealth accumulation; strengthening of collective bargaining arrangements; improvements in corporate governance; strengthening of financial regulation to eliminate subsidies to financial activity; easing of land-use restrictions that may cause estates to keep rising in value; better training for young people and retraining for displaced workers; and increased public and private investment in infrastructure development, such as energy production and transportation.[7][85][86][127]		Michael Spence has advised that responding to the future impact of technology will require a detailed understanding of the global forces and flows technology has set in motion. Adapting to them "will require shifts in mindsets, policies, investments (especially in human capital), and quite possibly models of employment and distribution".[note 6][128]		Since the publication of their 2011 book Race Against The Machine, MIT professors Andrew McAfee and Erik Brynjolfsson have been prominent among those raising concern about technological unemployment. The two professors remain relatively optimistic however, stating "the key to winning the race is not to compete against machines but to compete with machines".[129][130][131][132][133][134][135]		Notes		Citations		Sources		
An occupational disease is any chronic ailment that occurs as a result of work or occupational activity. It is an aspect of occupational safety and health. An occupational disease is typically identified when it is shown that it is more prevalent in a given body of workers than in the general population, or in other worker populations. The first such disease to be recognised, squamous-cell carcinoma of the scrotum, was identified in chimney sweep boys by Sir Percival Pott in 1775[citation needed]. Occupational hazards that are of a traumatic nature (such as falls by roofers) are not considered to be occupational diseases.		Under the law of workers' compensation in many jurisdictions, there is a presumption that specific disease are caused by the worker being in the work environment and the burden is on the employer or insurer to show that the disease came about from another cause. Diseases compensated by national workers compensation authorities are often termed occupational diseases. However, many countries do not offer compensations for certain diseases like musculoskeletal disorders caused by work (e.g. in Norway). Therefore, the term work-related diseases is utilized to describe diseases of occupational origin. This term however would then include both compensable and non-compensable diseases that have occupational origins.						Some well-known occupational diseases include:		Occupational lung diseases include asbestosis among asbestos miners and those who work with friable asbestos insulation, as well as black lung (coalworker's pneumoconiosis) among coal miners, silicosis among miners and quarrying and tunnel operators and byssinosis among workers in parts of the cotton textile industry.		Occupational asthma has a vast number of occupations at risk.		Bad indoor air quality may predispose for diseases in the lungs as well as in other parts of the body.		Occupational skin diseases are ranked among the top five occupational diseases in many countries.[1]		Occupational skin diseases and conditions are generally caused by chemicals and having wet hands for long periods while at work. Eczema is by far the most common, but urticaria, sunburn and skin cancer are also of concern.[2]		Contact dermatitis due to irritation is inflammation of the skin which results from a contact with an irritant.[3] It has been observed that this type of dermatitis does not require prior sensitization of the immune system. There have been studies to support that past or present atopic dermatitis is a risk factor for this type of dermatitis.[4] Common irritants include detergents, acids, alkalies, oils, organic solvents and reducing agents.[5]		The acute form of this dermatitis develops on exposure of the skin to a strong irritant or caustic chemical. This exposure can occur as a result of accident at a workplace . The irritant reaction starts to increase in its intensity within minutes to hours of exposure to the irritant and reaches its peak quickly. After the reaction has reached its peak level, it starts to heal. This process is known as decrescendo phenomenon.[6] The most frequent potent irritants leading to this type of dermatitis are acids and alkaline solutions.[7] The symptoms include redness and swelling of the skin along with the formation of blisters.		The chronic form occurs as a result of repeated exposure of the skin to weak irritants over long periods of time.[8]		Clinical manifestations of the contact dermatitis are also modified by external factors such as environmental factors (mechanical pressure, temperature, and humidity) and predisposing characteristics of the individual (age, sex, ethnic origin, preexisting skin disease, atopic skin diathesis, and anatomic region exposed.[9]		Another occupational skin disease is Glove related hand urticaria. It has been reported as an occupational problem among the health care workers. This type of hand urticaria is believed to be caused by repeated wearing and removal of the gloves. The reaction is caused by the latex or the nitrile present in the gloves.[10]		High-risk occupations include:[2]		Donald Hunter in his classic history of occupational diseases discusses many example of occupational diseases.[11] They include:		Prevention measures include avoidance of the irritant through its removal from the workplace or through technical shielding by the use of potent irritants in closed systems or automation, irritant replacement or removal [12] and personal protection of the workers.		In order to better prevent and control occupational disease, most countries revise and update their related laws, most of them greatly increasing the penalties in case of breaches of the occupational disease laws. Occupational disease prevention, in general legally regulated, is part of good supply chain management and enables companies to design and ensure supply chain social compliance schemes as well as monitor their implementation to identify and prevent occupational disease hazards.		
Full-time employment is employment in which a person works a minimum number of hours defined as such by his/her employer. Full-time employment often comes with benefits that are not typically offered to part-time, temporary, or flexible workers, such as annual leave, sickleave, and health insurance. Part-time jobs are mistakenly thought by some to not be careers. However, legislation exists to stop employers from discriminating against part-time workers so this should not be a factor when making decisions on career advancement. They generally pay more than part-time jobs per hour, and this is similarly discriminatory if the pay decision is based on part-time status as a primary factor. The Fair Labor Standards Act (FLSA) does not define full-time employment or part-time employment. This is a matter generally to be determined by the employer (US Department of Labor). The definition by employer can vary and is generally published in a company's Employee Handbook. Companies commonly require from 35 to 40 hours per week to be defined as full-time and therefore eligible for benefits.		Full-Time status varies between company and is often based on the shift the employee must work during each work week. The "standard" work week consists of five eight-hour days, commonly served between 9:00AM to 5:00PM or 10:00AM to 6:00pm totaling 40 hours. While a four-day week generally consists of four ten-hour days; it may also consist of as little as nine hours for a total of a 36-hour work week. Twelve-hour shifts are often three days per week, unless the company has the intention of paying out the employee overtime. Overtime is legally paid out anytime an employee works more than 40 hours per week. The legal minimum for overtime starts at Base Pay + One-Half. The increased payout is considered to compensate slightly for the increased fatigue which a person experiences on such long shifts. Shifts can also be very irregular, as in retail, but are still full-time if the required number of hours is reached. There are some situations where a person who needs full-time work is dropped to part-time, which is sometimes a form of constructive dismissal to avoid paying unemployment benefits to a laid-off worker.						Full-time workweeks:		A person working more than full-time is working overtime, and may be entitled to extra per-hour wages (but not salary).		“Full-time” can also be used in reference to a student (usually in higher education) who takes a full load of course work each academic term. The distinction between a full-time and part-time student varies markedly from country to country. As an example, in the United States a student is commonly defined as being in full-time education when they undertake 12 or more credit hours. This translates to 12 "hours" (often of 50 minutes instead of 60 minutes each) in class per week. "Lab hours" often count for less, only as one-half or one-third of a credit hour.		International students must maintain full-time status for student visas.[13] Adult students (typically up to age 22 or 23) may also fall under their parents' health insurance (and possibly car insurance and other services) if they are full-time, except for one term per year (usually summer). Students may also be eligible for elected office in student government or other student organizations only if they are full-time. The Department of Labor has a full-time student program which allows employers to pay no less than 85% of the minimum wage to the student/employee.[14]		
A graduate school (sometimes shortened as grad school) is a school that awards advanced academic degrees (i.e. master's and doctoral degrees) with the general requirement that students must have earned a previous undergraduate (bachelor's) degree[1][2] with a high grade point average. A distinction is typically made between graduate schools (where courses of study vary in the degree to which they provide training for a particular profession) and professional schools, which offer specialized advanced degrees in professional fields such as medicine, nursing, business, engineering, or law. The distinction between graduate schools and professional schools is not absolute, as various professional schools offer graduate degrees (e.g., some nursing schools offer a master's degree in nursing). Also, some graduate degrees train students for a specific profession (e.g. an MSc or a PhD in epidemiology trains a person to be an epidemiologist).		Many universities award graduate degrees; a graduate school is not necessarily a separate institution. While the term "graduate school" is typical in the United States and often used elsewhere (e.g. Canada), "postgraduate education" is also used in some English-speaking countries (Australia, Canada, Ireland, India, Bangladesh, New Zealand, Pakistan and the UK) to refer to the spectrum of education beyond a bachelor's degree. Those attending graduate schools are called "graduate students" (in both American and British English), or often in British English as "postgraduate students" and, colloquially, "postgraduates" and "postgrads". Degrees awarded to graduate students include master's degrees, doctoral degrees, and other postgraduate qualifications such as graduate certificates and professional degrees.		Producing original research is a significant component of graduate studies in the humanities (e.g., English literature, history, philosophy), sciences (e.g., biology, chemistry, zoology) and social sciences (e.g., sociology). This research typically leads to the writing and defense of a thesis or dissertation. In graduate programs that are oriented towards professional training (e.g., MPA, MBA, MHA), the degrees may consist solely of coursework, without an original research or thesis component. The term "graduate school" is primarily North American. Additionally, in North America, the term does not usually refer to medical school (whose students are called "medical students"), and only occasionally refers to law school or business school; these are often collectively termed professional schools. Graduate students in the humanities, sciences and social sciences often receive funding from the school (e.g., fellowships or scholarships) and/or a teaching assistant position or other job; in the profession-oriented grad programs, students are less likely to get funding, and the fees are typically much higher.		Although graduate school programs are distinct from undergraduate degree programs, graduate instruction (in the US, Australia and other countries) is often offered by some of the same senior academic staff and departments who teach undergraduate courses. Unlike in undergraduate programs, however, it is less common for graduate students to take coursework outside their specific field of study at graduate or graduate entry level. At the Ph.D. level, though, it is quite common to take courses from a wider range of study, for which some fixed portion of coursework, sometimes known as a residency, is typically required to be taken from outside the department and college of the degree-seeking candidate, to broaden the research abilities of the student. Some institutions[which?] designate separate graduate versus undergraduate staff and denote other divisions.[not verified in body]						Graduate degrees in Brazil are called "postgraduate" degrees, and can be taken only after an undergraduate education has been concluded".		In Canada, the Schools and Faculties of Graduate Studies are represented by the Canadian Association of Graduate Studies (CAGS) or Association canadienne pour les études supérieures (ACES). The Association brings together 58 Canadian universities with graduate programs, two national graduate student associations, and the three federal research-granting agencies and organizations having an interest in graduate studies.[3] Its mandate is to promote, advance, and foster excellence in graduate education and university research in Canada. In addition to an annual conference, the association prepares briefs on issues related to graduate studies including supervision, funding, and professional development.		Admission to a master's program generally requires a bachelor's degree in a related field, with sufficiently high grades usually ranging from B+ and higher (note that different schools have different letter grade conventions, and this requirement may be significantly higher in some faculties), and recommendations from professors. Some schools require samples of the student's writing as well as a research proposal. At English-speaking universities, applicants from countries where English is not the primary language are required to submit scores from the Test of English as a Foreign Language (TOEFL).		Admission to a doctoral program typically requires a master's degree in a related field, sufficiently high grades, recommendations, samples of writing, a research proposal, and an interview with a prospective supervisor. Requirements are often set higher than those for a master's program. In exceptional cases, a student holding an honours BA with sufficiently high grades and proven writing and research abilities may be admitted directly to a Ph.D. program without the requirement to first complete a master's. Many Canadian graduate programs allow students who start in a master's to "reclassify" into a Ph.D. program after satisfactory performance in the first year, bypassing the master's degree.		Students must usually declare their research goal or submit a research proposal upon entering graduate school; in the case of master's degrees, there will be some flexibility (that is, one is not held to one's research proposal, although major changes, for example from premodern to modern history, are discouraged). In the case of Ph.D.s, the research direction is usually known as it will typically follow the direction of the master's research.		Master's degrees can be completed in one year but normally take at least two; they typically may not exceed five years. Doctoral degrees require a minimum of two years but frequently take much longer, although not usually exceeding six years.		Graduate students may take out student loans, but instead they often work as teaching or research assistants. Students often agree, as a condition of acceptance to a programme, not to devote more than twelve hours per week to work or outside interests. Various universities in Canada have different policies in terms of how much funding is available. This funding may also be different within a university in each of the disciplines. Many universities offer Evening MBA programs where students are able to work full-time while obtaining an MBA through evening classes, which allows them to pay their way through school.[4]		For Masters students, funding is generally available to first-year students whose transcripts reflect exceptionally high grades; this funding can also be obtained in the second year of studies. Funding for Ph.D. students comes from a variety of sources, and many universities waive tuition fees for doctoral candidates (This may also occur for masters students of some universities). Funding is available in the form of scholarships, bursaries and other awards, both private and public.		Both master's and doctoral programs may be done by coursework or research or a combination of the two, depending on the subject and faculty. Most faculties require both, with the emphasis on research, and with coursework being directly related to the field of research.		Master's candidates undertaking research are typically required to complete a thesis comprising some original research and ranging from seventy to two-hundred pages. Some fields may require candidates to study at least one foreign language if they have not already earned sufficient foreign-language credits. Some faculties require candidates to defend their thesis, but many do not. Those that do not often have a requirement of taking two additional courses, minimum, in lieu of preparing a thesis.		Ph.D. candidates undertaking research must typically complete a thesis, or dissertation, consisting of original research representing a significant contribution to their field, and ranging from two-hundred to five-hundred pages. Most Ph.D. candidates will be required to sit comprehensive examinations—examinations testing general knowledge in their field of specialization—in their second or third year as a prerequisite to continuing their studies, and must defend their thesis as a final requirement. Some faculties require candidates to earn sufficient credits in a third or fourth foreign language; for example, most candidates in modern Japanese topics must demonstrate ability in English, Japanese, and Mandarin, while candidates in pre-modern Japanese topics must demonstrate ability in English, Japanese, Classical Chinese, and Classical Japanese language.		At English-speaking Canadian universities, both master's and Ph.D. theses may be presented in English or in the language of the subject (German for German literature, for example), but if this is the case, an extensive abstract must be also presented in English. In exceptional circumstances, a thesis may be presented in French.		French-speaking universities have varying sets of rules; some will accept students with little knowledge of French if they can communicate with their supervisors (usually in English).		The écoles doctorales ("Doctoral schools") are educational structures similar in focus to graduate schools, but restricted at PhD level. These schools have the responsibilities of providing students with a structured doctoral training in a disciplinary field. The field of the school is related to the strength of the university : while some have two or three schools (typically "Arts and Humanities" and "Natural and Technological Sciences"), others have more specialized schools (History, Aeronautics, etc.).		Admission to a doctoral program requires a master's degree, both research-oriented and disciplinary focused. High marks are required (typically a très bien honour, equating a cum laude), but the acceptance is linked to a decision of the School Academical Board.		A large share of the funding offered to junior researchers is channeled through the école doctorale, mainly in the shape of three-years "Doctoral Fellowships" (contrats doctoraux). These fellowships are awarded after submitting a biographical information, undergraduate and graduate transcripts where applicable, letters of recommendation, and research proposal, then an oral examination by an Academical Committee.		The traditional and most common way of obtaining a doctorate in Germany is by doing so individually under supervision of a single professor (Doktorvater or Doktormutter) without any formal curriculum. During their studies, doctoral students are enrolled at university while being employed simultaneously either at the university itself, at a research institute or at a company as a researcher.[5]		With the establishment of Graduiertenkollegs funded by the Deutsche Forschungsgemeinschaft (DFG), the German Research Foundation, in the early 1990s, the concept of a graduate school was introduced to the German higher education system. Unlike the American model of graduate schools, only doctoral students participate in a Graduiertenkolleg. In contrast to the traditional German model of doctoral studies, a Graduiertenkolleg aims to provide young researchers with a structured doctoral training under supervision of a team of professors within an excellent research environment. A Graduiertenkolleg typically consists of 20-30 doctoral students, about half of whom are supported by stipends from the DFG or another sponsor. The research programme is usually narrowly defined around a specific topic and has an interdisciplinary aspect. The programme is set up for a specific period of time (up to nine years if funded by the DFG). The official English translation of the term Graduiertenkolleg is Research Training Group.		In 2006, a different type of graduate school, termed Graduiertenschule ("graduate school"), was established by the DFG as part of the German Universities Excellence Initiative. They are thematically much broader than the focused Graduiertenkollegs and consist often of 100-200 doctoral students.		The term "graduate school" is used more widely by North American universities than by those in the UK. However, numerous universities in the UK have formally launched graduate schools, including the University of Birmingham, Durham University, Keele University, the University of Nottingham, Bournemouth University, Queen's University Belfast and the University of London, which includes graduate schools at King's College London, Royal Holloway and University College London. They often coordinate the supervision and training of candidates for doctorates.		While most graduate programs will have a similar list of general admission requirements, the importance placed on each type of requirement can vary drastically between graduate schools, departments within schools, and even programs within departments. The best way to determine how a graduate program will weigh admission materials is to ask the person in charge of graduate admissions at the particular program being applied to. Admission to graduate school requires a bachelor's degree. High grades in one's field of study are important—grades outside the field less so. The Graduate Record Examination standardized test is required by almost all graduate schools, while other additional standardised tests (such as the Graduate Management Admission Test (GMAT) and Graduate Record Examination (GRE) Subject Tests) scores may be required by some institutions or programs.[6][7] In addition, good letters of recommendation from undergraduate instructors are often essential,[8] as strong recommendation letters from mentors or supervisors of undergraduate research experience provide evidence that the applicant can perform research and can handle the rigors of a graduate school education.		Within the sciences and some social sciences, previous research experience may be important.[6][9] By contrast, within most humanities disciplines, an example of academic writing normally suffices. Many universities require a personal statement (sometimes called Statement of purpose or Letter of Intent), which may include indications of the intended area(s) of research;[7] how detailed this statement is or whether it is possible to change one's focus of research depends strongly on the discipline and department to which the student is applying.		In some disciplines or universities, graduate applicants may find it best to have at least one recommendation from their research work outside of the college where they earned their bachelor's degree;[citation needed] however, as with previous research experience, this may not be very important in most humanities disciplines.		Some schools set minimum GPAs and test scores below which they will not accept any applicants;[10] this reduces the time spent reviewing applications. On the other hand, many other institutions often explicitly state that they do not use any sort of cut-offs in terms of GPA or the GRE scores. Instead, they claim to consider many factors, including past research achievements, the compatibility between the applicant's research interest and that of the faculty, the statement of purpose and the letters of reference, as stated above. Some programs also require professors to act as sponsors. Finally, applicants from non-English speaking countries often must take the Test of English as a Foreign Language (TOEFL).[11]		At most institutions, decisions regarding admission are not made by the institution itself but the department to which the student is applying. Some departments may require interviews before making a decision to accept an applicant.[7] Most universities adhere to the Council of Graduate Schools' Resolution Regarding Graduate Scholars, Fellows, Trainees, and Assistants, which gives applicants until April 15 to accept or reject offers that contain financial support.[12]		In addition to traditional "degree-seeking" applications for admission, many schools allow students to apply as "non degree-seeking".[13] Admission to the Non Degree category is usually restricted primarily to those who may benefit professionally from additional study at the graduate level. For example, current primary, middle grades and secondary education teachers wishing to gain re-certification credit most commonly apply as Non Degree-Seeking students.		Graduate students often declare their intended degree (master's or doctorate) in their applications. In some cases, master's programs allow successful students to continue toward the doctorate degree. Additionally, doctoral students who have advanced to candidacy but not filed a dissertation ("ABD," for "all but dissertation") often receive master's degrees and an additional master's called a Master of Philosophy (MPhil), or a Candidate of Philosophy (C.Phil.) degree. The master's component of a doctorate program often requires one or two years.		Many graduate programs require students to pass one or several examinations in order to demonstrate their competence as scholars.[6] In some departments, a comprehensive examination is often required in the first year of doctoral study, and is designed to test a student's background undergraduate-level knowledge. Examinations of this type are more common in the sciences and some social sciences but relatively unknown in most humanities disciplines.		Most graduate students perform teaching duties, often serving as graders and tutors. In some departments, they can be promoted to lecturer status, a position that comes with more responsibility.		Doctoral students generally spend roughly their first two to three years taking coursework and begin research by their second year if not before. Many master's and all specialist students will perform research culminating in a paper, presentation, and defense of their research. This is called the master's thesis (or, for Educational Specialist students, the specialist paper). However, many US master's degree programs do not require a master's thesis, focusing instead primarily on course work or on "practicals" or "workshops." Some students complete a final culminating project or "capstone" rather than a thesis. Such "real-world" experience may typically require a candidate work on a project alone or in a team as a consultant, or consultants, for an outside entity approved or selected by the academic institution and under faculty supervision.		In the second and third years of study, doctoral programs often require students to pass more examinations.[6] Programs often require a Qualifying Examination ("Quals"), a PhD Candidacy Examination ("Candidacy"), or a General Examination ("Generals"), designed to ensure students have a grasp of a broad sample of their discipline, and/or one or several Special Field Examinations ("Specials"), which test students in their narrower selected areas of specialty within the discipline. If these examinations are held orally, they may be known colloquially as "orals." For some social science and many humanities disciplines, where graduate students may or may not have studied the discipline at the undergraduate level, these exams will be the first set and be based either on graduate coursework or specific preparatory reading (sometimes up to a year's work in reading).		In all cases, comprehensive exams are normally both stressful and time consuming and must be passed to be allowed to proceed on to the dissertation. Passing such examinations allows the student to stay, begin doctoral research, and rise to the status of a doctoral candidate, while failing usually results in the student leaving the program or re-taking the test after some time has passed (usually a semester or a year). Some schools have an intermediate category, passing at the master's level, which allows the student to leave with a master's without having completed a master's dissertation.		For the next several years the doctoral candidate primarily performs his or her research. Usually this lasts three to eight years, though a few finish more quickly, and some take substantially longer. In total, the typical doctoral degree takes between four and eight years from entering the program to completion, though this time varies depending upon the department, dissertation topic, and many other factors. For example, astronomy degrees take five to six years on average, but observational astronomy degrees take six to seven due to limiting factors of weather, while theoretical astronomy degrees take five.		Though there is substantial variation among universities, departments, and individuals, humanities and social science doctorates on average take somewhat longer to complete than natural science doctorates. These differences are due to the differing nature of research between the humanities and some social sciences and the natural sciences and to the differing expectations of the discipline in coursework, languages, and length of dissertation. However, time required to complete a doctorate also varies according to the candidate's abilities and choice of research. Some students may also choose to remain in a program if they fail to win an academic position, particularly in disciplines with a tight job market; by remaining a student, they can retain access to libraries and university facilities, while also retaining an academic affiliation, which can be essential for conferences and job-searches.		Traditionally, doctoral programs were only intended to last three to four years and, in some disciplines (primarily the natural sciences), with a helpful advisor and a light teaching load, it is possible for the degree to be completed in that amount of time. However, increasingly many disciplines, including most humanities, set their requirements for coursework, languages, and the expected extent of dissertation research by the assumption that students will take five years minimum or six to seven years on average; competition for jobs within these fields also raises expectations on the length and quality of dissertations considerably.		Competition for jobs within certain fields, such as the life sciences, is so great that almost all students now enter a second training period after graduate school called a postdoctoral fellowship. In total most life scientists will invest 12–14 years in low-paid training positions and only 14% will obtain tenure track jobs (Miller McCune, the real science gap). The average age at which life scientists obtain their first R01 grant to conduct independent research is now 42.		In some disciplines, doctoral programs can average seven to ten years. Archaeology, which requires long periods of research, tends towards the longer end of this spectrum. The increase in length of degree is a matter of great concern for both students and universities, though there is much disagreement on potential solutions to this problem.		In general, there is less funding available to students admitted to master's degrees than for students admitted to Ph.D. or other doctoral degrees. Many departments, especially those in which students have research or teaching responsibilities, offer Ph.D. students tuition waivers and a stipend that pays for most expenses. At some elite universities, there may be a minimum stipend established for all Ph.D. students, as well as a tuition waiver. The terms of these stipends vary greatly, and may consist of a scholarship or fellowship, followed by teaching responsibilities. At many elite universities, these stipends have been increasing, in response both to student pressure and especially to competition among the elite universities for graduate students.		In some fields, research positions are more coveted than teaching positions because student researchers are typically paid to work on the dissertation they are required to complete anyway, while teaching is generally considered a distraction from one's work. Research positions are more typical of science disciplines; they are relatively uncommon in humanities disciplines, and where they exist, rarely allow the student to work on their own research. Science PhD students can apply for individual NRSA fellowships from the NIH or fellowships from private foundations. US universities often also offer competitive support from NIH-funded training programs. One example is the Biotechnology Training Program – University of Virginia. Departments often have funds for limited discretionary funding to supplement minor expenses such as research trips and travel to conferences.		A few students can attain funding through dissertation improvement grants funded by the National Science Foundation (NSF), or through similar programs in other agencies. Many students are also funded as lab researchers by faculty who have been funded by private foundations or by the NSF, National Institutes of Health (NIH), or federal "mission agencies" such as the Department of Defense or the Environmental Protection Agency. The natural sciences are typically well funded, so that most students can attain either outside or institutional funding, but in the humanities, not all do. Some humanities students borrow money during their coursework, then take full-time jobs while completing their dissertations. Students in the social sciences are less well funded than are students in the natural and physical sciences, but often have more funding opportunities than students in the humanities, particularly as science funders begin to see the value of social science research.		Funding differs greatly by departments and universities; some universities give five years of full funding to all Ph.D. students, though often with a teaching requirement attached; other universities do not. However, because of the teaching requirements, which can be in the research years of the Ph.D., even the best funded universities often do not have funding for humanities or social science students who need to do research elsewhere, whether in the United States or overseas.[citation needed] Such students may find funding through outside funders such as private foundations, such as the German Marshall Fund or the Social Science Research Council (SSRC).		Foreign students are typically funded the same way as domestic (US) students, although federally subsidized student and parent loans and work-study assistance are generally limited to U.S. citizens and nationals, permanent residents, and approved refugees.[14] Moreover, some funding sources (such as many NSF fellowships) may only be awarded to domestic students. International students often have unique financial difficulties such as high costs to visit their families back home, support of a family not allowed to work due to immigration laws, tuition that is expensive by world standards, and large fees: visa fees by U.S. Citizenship and Immigration Services, and surveillance fees under the Student and Exchange Visitor Program of the United States Department of Homeland Security.[15]		At many universities, graduate students are employed by their university to teach classes or do research. While all graduate employees are graduate students, many graduate students are not employees. MBA students, for example, usually pay tuition and do not have paid teaching or research positions. In many countries graduate employees have collectively organized labor unions in order to bargain a contract with their university. In Canada, for example, almost all graduate employees are members of a CUPE local.		In the United States there are many graduate employee unions at public universities. The Coalition of Graduate Employee Unions lists 25 recognized unions at public universities on its website. Private universities, however, are covered under the National Labor Relations Act rather than state labor laws and until 2001 there were no recognized unions at private universities.		Many graduate students see themselves as akin to junior faculty, but with significantly lower pay.[citation needed] Many graduate students feel that teaching takes time that would better be spent on research, and many point out that there is a vicious circle in the academic labor economy. Institutions that rely on cheap graduate student labor have no need to create expensive professorships, so graduate students who have taught extensively in graduate school can find it immensely difficult to get a teaching job when they have obtained their degree. Many institutions depend heavily on graduate student teaching: a 2003 report by agitators for a graduate student union at Yale,[16] for instance, claims that "70% of undergraduate teaching contact hours at Yale are performed by transient teachers: graduate teachers, adjunct instructors, and other teachers not on the tenure track." The state of Michigan leads in terms of progressive policy regarding graduate student unions with five universities recognizing graduate employee unions: Central Michigan University, Michigan State University, the University of Michigan, Wayne State University, and Western Michigan University.		The United Auto Workers (under the slogan "Uniting Academic Workers") and the American Federation of Teachers are two international unions that represent graduate employees. Private universities' administrations often oppose their graduate students when they try to form unions, arguing that students should be exempt from labor laws intended for "employees". In some cases unionization movements have met with enough student opposition to fail. At the schools where graduate employees are unionized, which positions are unionized vary. Sometimes only one set of employees will unionize (e.g. teaching assistants, residential directors); at other times, most or all will. Typically, fellowship recipients, usually not employed by their university, do not participate.		When negotiations fail, graduate employee unions sometimes go on strike. While graduate student unions can use the same types of strikes that other unions do, they have also made use of teach-ins, work-ins, marches, rallies, and grade strikes. In a grade strike, graduate students refuse to grade exams and papers and, if the strike lasts until the end of the academic term, also refuse to turn in final grades. Another form of job action is known as "work-to-rule", in which graduate student instructors work exactly as many hours as they are paid for and no more.		
A vocation (from Latin vocātiō, meaning 'a call, summons'[1]) is an occupation to which a person is specially drawn or for which he/she is suited, trained, or qualified. Though now often used in non-religious contexts, the meanings of the term originated in Christianity.						Use of the word "vocation" before the sixteenth century referred firstly to the "call" by God[2] to an individual, or calling of all humankind to salvation, particularly in the Vulgate, and more specifically to the "vocation" to the priesthood, or to the religious life, which is still the usual sense in Roman Catholicism. Roman Catholicism recognizes marriage, single life, religious, and ordained life as the four vocations.[3][not in citation given] Martin Luther,[4] followed by John Calvin, placed a particular emphasis on vocations, or divine callings, as potentially including most secular occupations, though this idea was by no means new.[5]		Calvinism developed complex ideas about different types of vocations of the first type, connected with the concepts of Predestination, Irresistible grace, and the elect. There are the vocatio universalis, the vocatio specialis, only extended to some. There were also complex distinctions between internal and external, and the "vocatio efficax" and "inefficax" types of callings.[6] Hyper-Calvinism, unusually, rejects the idea of a "universal call", a vocation, to repent and believe, held by virtually all other Christian groups.		In Protestantism the call from God to devote one's life to him by joining the clergy is often covered by the English equivalent term "call", whereas in Roman Catholicism "vocation" is still used.		Both senses of the word "call" are used in 1 Corinthians 7:20, where Paul says "Let every man abide in the same calling wherein he was called" (KJV).		The idea of vocation is central to the Christian belief that God has created each person with gifts and talents oriented toward specific purposes and a way of life. In the broadest sense, as stated in the Catechism of the Catholic Church, "Love is the fundamental and innate vocation of every human being" (CCC 2392). More specifically, in the Orthodox and Catholic Churches, this idea of vocation is especially associated with a divine call to service to the Church and humanity through particular vocational life commitments such as marriage to a particular person, consecration as a religious, ordination to priestly ministry in the Church and even a holy life as a single person. In the broader sense, Christian vocation includes the use of one's gifts in their profession, family life, church and civic commitments for the sake of the greater common good.		Many Christian theologians appeal to the Old Testment Book of Genesis in regards to work. According to Genesis 1, human beings were created in the image of God, and according to Genesis 2, Adam was placed in the Garden of Eden to "work it and keep it" (2:15, ESV). Dorothy L. Sayers has argued that "work is the natural exercise and function of man – the creature who is made in the image of his Creator."[7] Likewise, John Paul II said in Laborem exercens that by his work, man shares in the image of his Creator.		Christian theologians see the Fall of man profoundly affecting human work. In Genesis 3:17, God said to Adam, "cursed is the ground because of you; in pain you shall eat of it all the days of your life" (ESV). Leland Ryken points out that, because of the Fall, "many of the tasks we perform in a fallen world are inherently distasteful and wearisome."[8] Through the Fall, work has become toil, but John Paul II says that work is a good thing for man in spite of this toil, and "perhaps, in a sense, because of it" because work is something that corresponds to man's dignity and through it he achieves fulfilment as a human being.[9] The Fall also means that a work ethic is needed. As a result of the Fall work has become subject to the abuses of idleness on the one hand, and overwork on the other. Drawing on Aristotle, Ryken suggests that the moral ideal is the golden mean between the two extremes of being lazy and being a workaholic.[10]		Some Christian theologians also draw on the doctrine of redemption to discuss the concept of work. Oliver O'Donovan points out that although work is a gift of creation, it is "ennobled into mutual service in the fellowship of Christ."[11]		Leland Ryken argues for seeing the call of God to a particular occupation as a reflection of the gospel call, and suggests that this implies vocational loyalty – "modern notions of job become deficient" and "the element of arbitrariness of one's choice of work" is removed.[12]		Since the establishment of Vocational Guidance in 1908 by the engineer Frank Parsons, the use of the term “vocation” has evolved, with emphasis shifting to an individual's development of talents and abilities in the choice and enjoyment of a career. This semantic expansion has meant some diminution of reference to the term's religious meanings in everyday usage.[13]		These books have attempted to define or clarify the term vocation.		
Whereas a career comprises the work activities that can be identified with a particular job or profession, having multiple careers is the growing trend in the late 20th century and early 21st century. These multiple careers can either be concurrent (where a worker has two simultaneous careers) or sequential (where a worker adopts a new career after having worked for some time in another career). Both may occur for different reasons.		Sandra Kerka (2003) reports that "'studies in the United States at the end of the seventies already showed that between 10 and 30 percent of the economically active population had experienced at least one career change in a 5-year period' (Teixeria & Gomes, 2000, p. 78). Of 91 skilled young adults in Germany, only one-third had continuous careers in the first 8 years after graduation and over half were employed in other occupations at least once (Heinz 2002). The phenomenon of reverse transfer provides an indirect clue: Townsend (2003) found that 62% of bachelor's-degree holders who enroll in community colleges were seeking an associate degree or certificate in order to make a career change." [1]						Workers with concurrent multiple careers adopt a "hyphenated" professional identity. A "teacher-painter" might refer to an individual who works for nine months out of the year as an Elementary School Teacher and three (summer) months out of the year as a painter. A "doctor-potter" might refer to an individual who works as an ENT-physician during the day, but works within a ceramics studio at night. Some consider the hyphen "-homemaker" or "-caregiver" as suggestive of another type of concurrent multiple career worker. That is, a "lawyer-homemaker" works as attorney and is also in charge of domestic duties at home. Increasingly, as adults must care for younger generation children and older generation parents, the "X-caregiver" worker has emerged — where a worker completes the tasks of career-X and simultaneously cares for the needs of children and elders. Some note that many members of the working class have long been concurrent workers out of economic necessity. A quarter of the British workforce works like this.		Workers can adopt concurrent multiple careers for a host of reasons including: economic (such as poverty or striving for additional wealth), educational (such as multiple degrees in multiple fields), or personal (such as interest or lack of fulfillment in one career). Economist, Richard Florida, among others suggests that some "hyphenates" pursue multiple concurrent careers in order to fulfill creative needs. A "doctor-potter," for example, might pursue ceramics for creative fulfillment as well as profit and professional development.		Author and New York Times columnist Marci Alboher popularized the term "slash careers" to describe multiple concurrent careers in her book One Person/Multiple Careers: A New Model for Work Life Success (2007). Instead of hyphenation, Alboher uses slash to demarcate concurrent multiple careers, as in "art dealer/yoga instructor" or "baker/comedian/web designer".		Workers with sequential multiple careers adopt a changing professional identity over time. Thus, a worker may devote 10–20 years of his/her life to one career and then switch to a related career or an entirely new one. As life-expectancy increases, as retirement benefits decrease, and as educational opportunities expand — workers may increasingly find themselves forced to fulfill the goals of one career and then adopt another. Some view this as an opportunity to expand meaning and purpose into later life, while others see this trend as an unfortunate economic and social reality.		Lloyd, Delia (June 20, 2008). "The job changer's bibles". The International Herald Tribune.		Goldsmith, Marshall (June 23, 2007). "Unleashing Your Many Identities". Business Week.		Savannah Guthrie (Correspondent). (2007, November 26). "Baby boomers juggling more jobs". Today [Television Broadcast]. New York: National Broadcasting Company.		The Shifting Careers column and blog in The New York Times.		
Workplace phobia is an anxiety disorder and specific phobia associated with workspace.		
Flextime (also spelled flexitime [British English], flex-time) is a flexible hours schedule that allows workers to alter workday start and finish times.[1] In contrast to traditional[2] work arrangements that require employees to work a standard 9 a.m. to 5 p.m. day, flextime typically involves a "core" period of the day during which employees are required to be at work (e.g., between 11 a.m. and 3 p.m.), and a "bandwidth" period within which all required hours must be worked (e.g., between 5:30 a.m. and 7:30 p.m.).[3] The working day outside of the "core" period is "flexible time", in which employees can choose when they work, subject to achieving total daily, weekly or monthly hours within the "bandwidth" period set by employers,[3] and subject to the necessary work being done[citation needed]. The total working time required of employees on flextime schedules is the same as that required under traditional work schedules.[3] A flextime policy allows staff to determine when they will work, while a flexplace policy allows staff to determine where they will work. Advantages include allowing employees to coordinate their work hours with public transport schedules,[citation needed] with the schedules of their children, and with daily traffic patterns to avoid high congestion times such as rush hour. Some claim that flexible working will change the nature of the way we work.[4]						The industrial perspective of flexible working[citation needed] emphasizes on the practical definition of flexibility. Employees being allowed to work from many different places as long as their level of production is maintained if not increased.[5] Moreover, research reports[6] gave quantitative interpretation backed by statistical evidences showing the changing attitude of organisations in different countries and especially the UK toward flexible working. For example, 50% of companies in the UK started to consider flexible working as a common practice and 73% of the managers in the survey showed an ultimate support to it. On the other hand, employees showed great preference to flexible working to the point that 40% of workers in the UK choose it over salary.[7] Also, greater[citation needed] focus was put[8][9] to explain the increased demand for such arrangements by both stakeholders which was clarified by their advantages of contributing to high quality of output results while creating the perfect working conditions for workers.		Additionally, as seen recently, most business organisations have started to introduce flexible working patterns for their employee as a way to increase their productivity level, increasing profitability. Flexible working is also seen as a family-friendly policy, which leads to a good work life balance for employees. Some examples of organisations with flexible working arrangement include Agilent technologies, NetApp, Qualcomm Inc.[10]		Flexible working arrangements may be a way for organisations to expand and increase their operations nationally and internationally at lower cost, in comparison to permanent or non-flexible working arrangements.[11] While both employees and employers acknowledge the benefits of flexible working, drawbacks might include extra expenses and responsibilities the organization could incur in order to provide these arrangements and the decreased benefits offered to employee in accordance to their reduced working hours.[12]		Flexible working was academically introduced in 1970[13] and since then this topic continues to be the interest of many research papers. For four decades, academic papers have contributed to the increased knowledge and interest in flexible working. A descriptive background of the evolution of the concept of flexibility as well as highlighting the main factors contributed to its growth were the main focus of academic studies.[14] Also, they deliver evidence of the significant amount and the ongoing increase in the use of flexible working in many countries.[15] Few empirical studies were conducted to reflect the relation between flexible working and other variables. As a response to the empirical gap, a study by Origo and Pagani[16] based on a sample of European countries, gave a deep analysis of the concept of flexible working by testing the level of heterogeneity in the effect of ﬂexibility on job satisfaction and found some positive link with some aspects of the job while negative or no relation was found against other aspects.		Academicians see flexible working as an element which can both prevent and create opportunities. The findings revealed generally positive relationship between flexible working and perceptions of job quality in term of work-life balance and helping to improve and control autonomy particularly for remote workers, but some factors such as opportunities for advancement will be negatively affected due to the variations on different dimensions of job quality.[17] Flexible employment is one of the vital factor in the European Union policy discourse. It is a mean to reduce unemployment, increase economic and social cohesion, maintain economic competitiveness and enhance equal opportunities between women and men.[18]		Haller founded a company in the UK in 1971 and registered the trademark "Flextime", the mark remains the property of that company's successor hfx Ltd. In spring 2003, 17.7% of men and 26.7% of women were employed with flexitime arrangements in the United Kingdom, (Office for National Statistics 2003).[19] In the United Kingdom, flexitime working is commonplace in both the private and public sectors. The practice is often found in administrative and back office functions of commercial organisations and local councils.		In 2003, the UK Government introduced legislation[20] that gave parents of children under 6, or the parents of disabled children under 18, the right in law to request a flexible working arrangement from their employer. A survey in 2005 by the National Office of Statistics[21] showed that 71% of female workers and 60% of male workers were aware of the rights created under the 2003 legislation. Between 2003 and 2005 more than 14% of all workers had requested a change to flexible working. Since April 2007 the right to request flexible working also applies to carers of adults.		On 13 November 2012 Deputy Prime Minister Nick Clegg announced plans to extend the right to request flexible working to all employees,[22] this legislation takes effect in April 2014. Lawyers have suggested that this will lead to "major headaches" for employers.[23]		Now being enforced by the law on 30 June 2014, industrial reports concentrate on workers right to request for flexible working and how it is guided by Advisory, Conciliation and Arbitration Service (ACAS). They explained how this code is designed to help employers, employees and their representatives dealing with disciplinary and grievance situations in the workplace.[24]		Shift workers are generally excluded from flextime schemes as are senior managers.[citation needed] Other groups of workers for whom flextime arrangements are rare include those who serve the public during specific opening times.		The advantages of Flextime for the individual include a better work-life balance, fewer commutes, less fatigue, more days off, lower sickness rates. The benefits for the company include; better motivated workers, more efficient and effective operation, less fatigued workers, so fewer errors; they get people working overtime hours without paying overtime rates, fewer facilities required, and lower sickness rates.		For employers, flextime can aid the recruitment and retention of staff. It has been a particularly popular option in 2009 for employers trying to reduce staff costs without having to make redundancies during the recession. It can also help provide staff cover outside normal working hours and reduce the need for overtime. Additionally flextime can also improve the provision of equal opportunities to staff unable to work standard hours.		Flextime can give employees greater freedom to organize their working lives to suit personal needs. In addition, travelling can be cheaper and easier if it is out of peak time.		In Florida, flextime workers, like salaried workers, are exempted from insurance regulations, and are given broad leeway in setting their own work schedule. Unlike exempted salaried workers, employers are still required to pay overtime to a flextime worker if they work more than 40 hours per week; some employers avoid this policy by dismissing[citation needed] their employees shortly before their scheduled working hours have been completed. In addition, the employer will usually require that a flextime employee works a minimum number of hours each week.		In recent years, the term "flextime" has acquired a more controversial definition when used to describe proposals to overhaul the nation's overtime regulations. Under one such proposal by the Bush administration made public on August 5, 2004, employers would not be required to pay non-exempt employees overtime for working more than 40 hours in a week so long as the employee works no more than 80 hours over a two-week period. For example, a worker could be required to work 70 hours one week and receive no overtime compensation as long as they work 10 hours or less the following week. Such arrangements are opposed by trade unions such as the AFL-CIO.		In certain industries and disciplines, such as information technology, flextime permits workers to vary their schedule. For example, they may opt to work four 10-hour days per week, taking Monday or Friday off. Another flextime schedule is to work nine-hour days Monday through Thursday, an eight-hour day on Friday, taking every other Friday off. Workers may arrange to coordinate their days off so that their responsibilities are adequately covered.		Other workers may opt simply to come in early, such as 5 or 6 a.m., and leave in the mid-afternoon, or come in late and therefore leave late. One benefit of such a schedule is that commuting times occur outside of the congested rush hour traffic within a given geographic region. Flextime arrangements also help parents: one parent works 10 a.m – 6 p.m. and is in charge of the children before school / daycare, while the other parent works 7 a.m. – 3 p.m. and is in charge of the children after school / daycare. This allows parents time to commute.[25] Flextime is also beneficial to workers pursuing an education.		It is an ongoing part of the work-life balance discussions in many companies.		Flexi-time in Australia is usually referred to accumulated overtime hours that an employee can build up and exchange for the equivalent amount of time off. (Example: Jane works 7 a.m. – 3 p.m. Monday to Friday. Over the past month, Jane has worked 8 hours overtime meaning she is eligible for a paid day off.)		If employees accumulate too many flex hours, they are required to perform a "flex burndown", as they are burning down the flex. Similarly, taking a flex day off is known as "flexing".		It is implemented formally in the Australian Federal Public Service and is available for staff in most state and territory government departments. With current changes to industrial relations laws (2006), from State to Federal level there are no new published guidelines (online) for flexi-time.		Flexi-time has also been implemented in the Victorian Public Service.		There are many different methods used for recording working time ranging from sophisticated software (computer programs) to handwritten time sheets. Most of these methods are associated with the payment of wages in return for hours worked. As a result, they often do not address a fundamental difference of most flexible working systems – namely the intention of flexible working to allow an employee to "trade hours" with their employer in return for a fixed wage (Hayward, Bruce; Fong, Barry; Thornton, Alex (December 2007), "The Third Work-Life Balance Employer Survey: Main Findings" (PDF), UK Govt. Department for Business, Enterprise and Regulatory Reform ).		"Millennial" is the name most commonly used to describe those born between 1980 and 2000. As the right to request for flexible working is extended to all by law, many benefits are expected to rise. With "Millennials" becoming the interest of many organisations, flexible working seems to attract them.[26] As ethnic diversity and high level of education are their main characteristics, it is seen that Millennials are more likely to change their jobs more than the previous generation for economic reasons. Additionally, due to the delay in the retirement of baby boomers' generation, gaps in workforce were created, waiting for the Millennial generation to fill them.[27]		Flexible Working pattern is a form of working arrangement which enables employees to decide the time, duration, and location of their work.[28] Flexible working patterns has gained the interest of both academics and industrial practitioners in recent years, the reason is mainly because of its implementation into law in 1930. From Literature evidences highlight the fundamental importance of flexible working to both academics and industrialist as a means to establish a good work –life balance for employees, explaining how a good work-life balance for employees makes employees increase their efficiency of work, which in turns leads to increase in productivity of the organisation.[29][30]		Academics literature have identified some benefits of flexible working patterns to employees such as life satisfaction, better wellbeing, and overall a good work-life balance,[31] but some researchers argue that although there are such benefits, there are some negative effect such as work intensity, job insecurity associated with flexible working arrangement. Research works such as Evans et al., (2000) also highlight that flexible working pattern may not be applicable to all occupational fields, the authors also heighted the medical profession as one of such fields.[32]		Industrial sources also have been able to highlight one of the positive effects of flexible working patterns as being able to attract highly qualified professionals, but Brookins[33] established some negative effects flexible working patterns had to employers as it adds expenses and responsibility on the organisation, negative availability perspectives of employees on the customers, and employee availability.[better source needed]		Both academics and industrial sources were established that in some professions flexible working arrangement may not be available or its availability will have a negative perspective on employees by others with a non-flexible arrangement, example of such profession is the medical profession. The researcher done by Evans et al., (2000) on flexible working patterns in medical profession emphasized how some medical doctors may attributes negative perception with colleagues with flexible working pattern. In 1930, Employees in the United Kingdom were given the right to request for flexible working arrangement, but there was no instructions or guideline on the way for this would work. Flexible working concept is also a relatively new form of working arrangement and this has limited its application in other parts of the world such as some region in Africa.		Flexible working patterns is a working arrangement that enable employees to determine the duration, time and location of their work. It has been seen both by academics and industrial sources to have benefit sure as increase of work-life balance for employee, which in turns leads to increase in productivity for the employer or organisation. Organisations hoping to adopt this form of working pattern for its employee should conduct research on how flexible working pattern is done in its industries, to avoid the enquiring a large expenses some researchers have associated the conducted flexible working arrangement.		Flexible working time accounts is also known as deposited working-time accounts or work bank account system. It is derived from the German Federal Labor Government's reform program, which was passed by the German Federal Government on August 21, 2002. Then Federal Chancellor, Gerhard Schröder, announced that the program will invite the former director of human resources management of Volkswagen company, Peter Hartz, chaired the Labour Market Reform Committee. This program's goal is to make the rigidity of the labor system more flexible and to change the old social welfare policy, in order to lighten heavy financial burden.[34]		The concept of flexible working time accounts is to establish labor-self accounts, and labors can save their working hours, just like saving money, into their own accounts. The working hours in their accounts are their assets, so that employers and workers both sides can increase or decrease the work required by each other without affecting the salaries and welfare. While achieving the purpose of flexible labor, and the account-system may be short-term, long or permanent (life-time) of the convention.[34]		Flexible working time accounts system has the following four characteristics [35]		
Coaching is a form of development in which a person called a coach supports a learner or client in achieving a specific personal or professional goal by providing training and guidance.[1] The learner is sometimes called a coachee. Occasionally, coaching may mean an informal relationship between two people, of whom one has more experience and expertise than the other and offers advice and guidance as the latter learns; but coaching differs from mentoring in focusing on specific tasks or objectives, as opposed to more general goals or overall development.[1][2][3]		The first use of the term "coach" in connection with an instructor or trainer arose around 1830 in Oxford University slang for a tutor who "carried" a student through an exam.[4] The word "coaching" thus identified a process used to transport people from where they are to where they want to be. The first use of the term in relation to sports came in 1861.[4] Historically the development of coaching has been influenced by many fields of activity, including adult education, the Human Potential Movement, large-group awareness training (LGAT) groups such as "est", leadership studies, personal development, and psychology.[5][6]		Professional coaching uses a range of communication skills (such as targeted restatements, listening, questioning, clarifying etc.) to help clients shift their perspectives and thereby discover different approaches to achieve their goals.[7] These skills can be used in almost all types of coaching. In this sense, coaching is a form of "meta-profession" that can apply to supporting clients in any human endeavor, ranging from their concerns in health, personal, professional, sport, social, family, political, spiritual dimensions, etc. There may be some overlap between certain types of coaching activities.[5]		The concept of ADHD coaching was first introduced in 1994 by psychiatrists Edward M. Hallowell and John J. Ratey in their book Driven to Distraction.[8] ADHD coaching is a specialized type of life coaching that uses specific techniques designed to assist individuals with attention-deficit hyperactivity disorder. The goal of ADHD coaching is to mitigate the effects of executive function deficit, which is a typical impairment for people with ADHD.[9] Coaches work with clients to help them better manage time, organize, set goals and complete projects.[10] In addition to helping clients understand the impact ADHD has had on their lives, coaches can help clients develop "work-around" strategies to deal with specific challenges, and determine and use individual strengths. Coaches also help clients get a better grasp of what reasonable expectations are for them as individuals, since people with ADHD "brain wiring" often seem to need external mirrors for accurate self-awareness about their potential despite their impairment.[11]		Unlike psychologists or psychotherapists, ADHD coaches do not provide any therapy or treatment: their focus is only on daily functioning and behaviour aspects of the disorder.[12] The ultimate goal of ADHD coaching is to help clients develop an "inner coach", a set of self-regulation and reflective planning skills to deal with daily life challenges.[13] A 2010 study from Wayne State University evaluated the effectiveness of ADHD coaching on 110 students with ADHD. The research team concluded that the coaching "was highly effective in helping students improve executive functioning and related skills as measured by the Learning and Study Strategies Inventory (LASSI)."[14] Yet, not every ADHD person needs a coach and not everyone can benefit from using a coach.[15]		Business coaching is a type of human resource development for business leaders. It provides positive support, feedback and advice on an individual or group basis to improve personal effectiveness in the business setting. Business coaching is also called executive coaching,[16] corporate coaching or leadership coaching. Coaches help their clients advance towards specific professional goals. These include career transition, interpersonal and professional communication, performance management, organizational effectiveness, managing career and personal changes, developing executive presence, enhancing strategic thinking, dealing effectively with conflict, and building an effective team within an organization. An industrial organizational psychologist is one example of executive coach. Business coaching is not restricted to external experts or providers. Many organizations expect their senior leaders and middle managers to coach their team members to reach higher levels of performance, increased job satisfaction, personal growth, and career development. Research studies suggest that executive coaching has a positive impact on workplace performance.[17]		In some countries, there is no certification or licensing required to be a business or executive coach, and membership of a coaching organization is optional. Further, standards and methods of training coaches can vary widely between coaching organizations. Many business coaches refer to themselves as consultants, a broader business relationship than one which exclusively involves coaching.[18]		Career coaching focuses on work and career and is similar to career counseling. Career coaching is not to be confused with life coaching, which concentrates on personal development. Another common term for a career coach is career guide.		Christian coaching is common among religious organizations and churches.[citation needed] A Christian coach is not a pastor or counselor (although he may also be qualified in those disciplines), but rather someone who has been professionally trained to address specific coaching goals from a distinctively Christian or biblical perspective. Although various training courses exist, there is no single regulatory body for Christian coaching. Some[which?] of the Christian coaching programs are based on the works of Henry Cloud, John Townsend, and John C. Maxwell.[citation needed]		Co-coaching is a structured practice of coaching between peers with the goal of learning improved coaching techniques.		Financial coaching is a relatively new form of coaching that focuses on helping clients overcome their struggle to attain specific financial goals and aspirations they have set for themselves. Financial coaching is a one-on-one relationship in which the coach works to provide encouragement and support aimed at facilitating attainment of the client's financial plans. A financial coach, also called money coach, typically focuses on helping clients to restructure and reduce debt, reduce spending, develop saving habits, and develop financial discipline. In contrast, the term financial adviser refers to a wider range of professionals who typically provide clients with financial products and services. Although early research links financial coaching to improvements in client outcomes, much more rigorous analysis is necessary before any causal linkages can be established.[19]		Health coaching is becoming recognized as a new way to help individuals "manage" their illnesses and conditions, especially those of a chronic nature.[20] The coach will use special techniques, personal experience, expertise and encouragement to assist the coachee in bringing his/her behavioral changes about, while aiming for lowered health risks and decreased healthcare costs.[21] The National Society of Health Coaches (NSHC) has differentiated the term health coach from wellness coach.[21] According to the NSHC, health coaches are qualified "to guide those with acute or chronic conditions and/or moderate to high health risk", and wellness coaches provide guidance and inspiration "to otherwise 'healthy' individuals who desire to maintain or improve their overall general health status".[21]		Homework coaching focuses on equipping a student with the study skills required to succeed academically. This approach is different from regular tutoring which typically seeks to improve a student's performance in a specific subject.[22]		Coaching in education is seen as a useful intervention to support students, faculty and administrators in educational organizations.[23] For students, opportunities for coaching include collaborating with fellow students to improve grades and skills, both academic and social; for teachers and administrators, coaching can help with transitions into new roles.[23]		Life coaching is the process of helping people identify and achieve personal goals. Although life coaches may have studied counseling psychology or related subjects, a life coach does not act as a therapist, counselor, or health care provider, and psychological intervention lies outside the scope of life coaching.		Relationship coaching is the application of coaching to personal and business relationships.[24]		In sports, a coach is an individual that provides supervision and training to the sports team or individual players. Sports coaches are involved in administration, athletic training, competition coaching, and representation of the team and the players.		Since the mid-1990s, coaching professional associations such as the Association for Coaching (AC), the European Mentoring and Coaching Council (EMCC), the International Association of Coaching (IAC), and the International Coach Federation (ICF) have worked towards developing training standards.[1]:287–312[25] Psychologist Jonathan Passmore noted in 2016:[1]:3		While coaching has become a recognized intervention, sadly there are still no standards or licensing arrangements which are widely recognized. Professional bodies have continued to develop their own standards, but the lack of regulation means anyone can call themselves a coach. [...] Whether coaching is a profession which requires regulation, or is professional and requires standards, remains a matter of debate.		One of the challenges in the field of coaching is upholding levels of professionalism, standards and ethics.[25] To this end, coaching bodies and organizations have codes of ethics and member standards.[1]:287–312[26] However, because these bodies are not regulated, and because coaches do not need to belong to such a body, ethics and standards are variable in the field.[25][27] In February 2016, the AC and the EMCC launched a "Global Code of Ethics" for the entire industry; individuals, associations, and organizations are invited to become signatories to it.[28][29]:1		With the growing popularity of coaching, many colleges and universities now offer coach training programs that are accredited by a professional association.[30] Some courses offer a life coach certificate after just a few days of training,[31] but such courses, if they are accredited at all, are considered "à la carte" training programs, "which may or may not offer start to finish coach training," according to the ICF.[32] In contrast, "all-inclusive" training programs accredited by the ICF, for example, require a minimum of 125 student contact hours, 10 hours of mentor coaching and a performance evaluation process.[33][34] This is very little training in comparison to the training requirements of some other helping professions: for example, licensure as a counseling psychologist in the State of California requires 3,000 hours of supervised professional experience.[35] However, the ICF, for example, offers a "Master Certified Coach" credential that requires demonstration of "2,500 hours (2,250 paid) of coaching experience with at least 35 clients"[36] and a "Professional Certified Coach" credential with fewer requirements.[37] Other professional bodies similarly offer entry-level, intermediate, and advanced coach accreditation options.[38] Some coaches are both certified coaches and licensed counseling psychologists, integrating coaching and counseling.[39]		Critics see life coaching as akin to psychotherapy but without the legal restrictions and state regulation of psychologists.[25][40][41][42] There are no state regulation/licensing requirements for coaches. Due to lack of regulation, people who have no formal training or certification can legally call themselves life or wellness coaches.[43]		
A practice firm (also known as a practice enterprise,[1][2] or virtual company[3]) is a simulated company that is run like a real business, simulating a normal company's business procedures, products and services, and resembles a real company in its form, organization and function. Practice firms trade with each other in a virtual market; their actions are however legally void. Practice firms are organised nationally and internationally, and take part in trade fairs for practice firms.[4] There are around 5000 practice firms in Europe, and at least 2500 outside Europe.[3] Practice firms are often supported by government programs, and usually aim to provide vocational training over a period of around six months, without providing certificates of achievement.[5] Virtual capital, as well as virtual government services, taxes and regulatory organisations, are provided by the network to which the practice firm belongs.[6]		Practice firms provide a means for unemployed people to upgrade their skills, widen their network of contacts, and maintain the dignity associated with the appearance of gainful employment.[3] A practice firm is organised and operated by trainees, with a facilitator acting as a coach.[7] Practice firms are also often supported by "mentor companies" who provide guidance and technical information.[7]		Although European students already exchanged business letters as part of their education in 1920, the first complete German practice firm was founded in 1954. The oldest practice firm that is still active today was founded in 1960.[8]		
Employee monitoring is the act of surveying employee activity. Organizations engage in employee monitoring to track performance, avoid legal liability, protect trade secrets, and address other security concerns. The practice may impact employee satisfaction due to its impact on privacy.						If employees use company computers for their work, companies often utilize employee monitoring software that allow them to track everything employees do on their computers. For example, what emails were received, what applications were used and what keys were pressed.		Employees' phone call details as well as actual conversations can be recorded during monitoring. The exact number and duration of each call, and the idle time between calls, can go into an automatic log for analysis.[1] In the United States, the Omnibus Crime Control and Safe Streets Act of 1968 provides some privacy protections for employees. See Omnibus Crime Control and Safe Streets Act of 1968 § Employee Privacy.		One of the most effective forms of employee monitoring is through the use of video surveillance equipment. Video feeds of employee activities are fed back to a central location where they are either recorded or monitored live by another person. "This is a benefit because it provides an unbiased method of performance evaluation and prevents the interference of a manager's feelings in an employee's review (Mishra and Crampton, 1998)." Management can review the performance of an employee by checking the surveillance and detecting problems before they become too costly.[2]		In the United States, the Electronic Communications Privacy Act provides some privacy protections regarding monitoring of employees' email messages and other electronic communications. See Electronic Communications Privacy Act § Employee Privacy.		For employees that do not work in a static location, supervisors may choose to track their location. Common examples of this are delivery and transportation industries. In some of these cases the employee monitoring is incidental as the location is tracked for other purposes, such as determining the amount of time before a parcel will be delivered, or which taxi is closest.		Employee surveillance may lead to an executive's decision on whether to promote or demote and employee or in some cases even fire them.		Different techniques can be used, e.g. employees' cell phone or mobile phone tracking.		In arenas where employees are not paid to their full labor product, mass video surveillance is an industrial organization method deployed as a psychological tactic upon the proletariats psyche. Conceived by F. W. Taylor, though not available for many decades thereafter, video surveillance ensures near perpetual activity, or maximum exploitation. This method is favored in hotels to monitor housekeeping staff.		Employee monitoring often is in conflict with employees' privacy.[3] Monitoring often collects not only work-related activities, but also employee's personal, not related to work information. Monitoring does not mean that there are no limits to what should be collected. Monitoring in the workplace may put employers and employees at odds because both sides are trying to protect personal interests. Employees want to maintain privacy while employers want to ensure company resources aren't misused. In any case, companies can maintain ethical monitoring policies by avoiding indiscriminate monitoring of employees' activities.[4] The employee needs to understand what is expected of them while the employer needs to establish that rule.		In Canada, it is illegal to perform invasive monitoring, such as reading an employee's emails, unless it can be shown that it is a necessary precaution and there are no other alternatives.[5] In Maryland everyone in the conversation must give consent before the conversation can be recorded. The state of California requires that monitored conversations have a beep at certain intervals or there must be a message informing the caller that the conversations may be recorded, take note that this is not informing the company representative which calls are being recorded. Other states, including Connecticut, New York, Pennsylvania, Colorado and New Jersey, also have laws relating to when a conversation can be recorded		The following uses of employee information are generally considered legal:		According to Computer Monitoring: The Hidden War Of Control,"The employer of today has the ability and legal right to read e-mail, review files stored on a company computer, examine computer usage, and track individual employee computer activities. The idea of anonymous actions is an illusion. Every action between a network and the computers connected to it can be tracked. Every action by an individual worker on a computer can be tracked, analyzed and used against the employee. The protections and freedoms guaranteed by the U.S. Constitution and Bill of Rights are there to protect the individual from the Government and do not generally apply to the normal employee/employer relationship."[6]		Employee Monitoring can be used to monitor the safety and productivity of the employees but it also may help businesses financially. From the dishonest unethical employee who steals time and money from the business to the redefining of unprofitable processes in monitoring employee actions, employee monitoring allows for the growth of financial profits from a small investment. The monitoring of employees can help in the protection of employees and it can help as protection in litigation by employees for job related issues such as failing to perform, illegal activities and harassment claims. According to the American Management Association almost half (48%) of the companies surveyed use video monitoring to counter theft, violence and sabotage. Only 7% use video surveillance to track employees' on-the-job performance. Most employers notify employees of anti-theft video surveillance (78%) and performance-related video monitoring (89%), (Retrieved from the article The Latest on Workplace Monitoring and Surveillance on humanresources.about.com)[7] In an article in Labour Economics, it has been argued that forbidding employers to track employees' on-the-job performance can make economic sense according to efficiency wage theory, while surveillance to prevent illegal activities should be allowed.[8]		
Professional development is learning to earn or maintain professional credentials such as academic degrees to formal coursework, conferences and informal learning opportunities situated in practice. It has been described as intensive and collaborative, ideally incorporating an evaluative stage.[1] There are a variety of approaches to professional development, including consultation, coaching, communities of practice, lesson study, mentoring, reflective supervision and technical assistance.[2]						The University of Management and Technology notes the use of the phrase "professional development" from 1857 onwards.[citation needed]		In the training of school staff in the United States, "[t]he need for professional development [...] came to the forefront in the 1960's".[3]		A wide variety of people, such as teachers, military officers and non-commissioned officers, health care professionals, lawyers, accountants and engineers engage in professional development. Individuals may participate in professional development because of an interest in lifelong learning, a sense of moral obligation, to maintain and improve professional competence, to enhance career progression, to keep abreast of new technology and practices, or to comply with professional regulatory requirements.[4][5] Many American states have professional development requirements for school teachers. For example, Arkansas teachers must complete 60 hours of documented professional development activities annually.[6] Professional development credits are named differently from state to state. For example, teachers: in Indiana are required to earn 90 Continuing Renewal Units (CRUs) per year;[7] in Massachusetts, teachers need 150 Professional Development Points (PDPs);[8] and in Georgia, must earn 10 Professional Learning Units (PLUs).[9] American and Canadian nurses, as well as those in the United Kingdom, have to participate in formal and informal professional development (earning Continuing education units, or CEUs) in order to maintain professional registration.[10][11][12]		In a broad sense, professional development may include formal types of vocational education, typically post-secondary or poly-technical training leading to qualification or credential required to obtain or retain employment. Professional development may also come in the form of pre-service or in-service professional development programs. These programs may be formal, or informal, group or individualized. Individuals may pursue professional development independently, or programs may be offered by human resource departments. Professional development on the job may develop or enhance process skills, sometimes referred to as leadership skills, as well as task skills. Some examples for process skills are 'effectiveness skills', 'team functioning skills', and 'systems thinking skills'.[13][14]		Professional development opportunities can range from a single workshop to a semester-long academic course, to services offered by a medley of different professional development providers and varying widely with respect to the philosophy, content, and format of the learning experiences. Some examples of approaches to professional development include:[2]		Initial professional development (IPD) is defined as "a period of development during which an individual acquires a level of competence necessary in order to operate as an autonomous professional".[15] Professional associations may recognise the successful completion of IPD by the award of chartered or similar status. Examples of professional bodies that require IPD prior to the award of professional status are the Institute of Mathematics and its Applications,[16] the Institution of Structural Engineers,[17] and the Institution of Occupational Safety and Health.[18]		Continuing professional development (CPD) or continuing professional education (CPE) is continuing education to maintain knowledge and skills. Most professions have CPD obligations. Examples are the Royal Institution of Chartered Surveyors,[19] American Academy of Financial Management,[20] safety professionals with the International Institute of Risk & Safety Management (IIRSM)[21] or the Institution of Occupational Safety and Health (IOSH),[22] and medical and legal professionals, who are subject to continuing medical education or continuing legal education requirements, which vary by jurisdiction.		
An exit interview is a survey conducted with an individual who is separating from an organization or relationship. Most commonly, this occurs between an employee and an organization, a student and an educational institution, or a member and an association. An organization can use the information gained from an exit interview to assess what should be improved, changed, or remain intact. More so, an organization can use the results from exit interviews to reduce employee, student, or member turnover and increase productivity and engagement, thus reducing the high costs associated with turnover. Some examples of the value of conducting exit interviews include shortening the recruiting and hiring process, reducing absenteeism, improving innovation, sustaining performance, and reducing possible litigation if issues mentioned in the exit interview are addressed. It is important for each organization to customize its own exit interview in order to maintain the highest levels of survey validity and reliability.		The exit interview fits into the separation stage of the employee life cycle (ELC). This stage, the last one of the ELC, spans from the moment an employee becomes disengaged until his or her departure from the organization. This is the key time that an exit interview should be administered because the employee’s feelings regarding his or her departure are fresh in mind. An off-boarding process allows both the employer and employee to properly close the existing relationship so that company materials are collected, administrative forms are completed, knowledge base and projects are transferred or documented, feedback and insights are gathered through exit interviews, and any loose ends are resolved.						Exit interviews in business are focused on employees that are leaving a company or when employees have completed a significant project. The purpose of this exit interview is to glean feedback from employees in order to improve aspects of the organization, better retain employees, and reduce turnover. During this interview employees will be asked why they are leaving, what specifically influenced their decision to leave, whether or not they are going to another company and what that company they are going to offers that their current company does not. Businesses can use this information to better align their HR strategy with what employees look for in an organization and enact programs and practices that will influence top talent to stay at the organization.		In the past, exit interview data was being collected by the organization but not much was being done in terms of interpreting the data and making it actionable. Today there are metrics, analytics, benchmarks, and best practices that help organizations make sense of and use the data towards proactive organizational retention programs. Recently an array of exit interview software has been developed and popularized. These programs facilitate and streamline the employee separation process, allow surveys to be completed via the web, make separation and retention trends easy to identify, and amass actionable data which can increase organizational effectiveness and productivity. Additionally, some of these programs make it possible to quantify data gleaned from the surveys to more accurately understand why employees are leaving the organization.		Common questions include reasons for leaving, job satisfaction, frustrations, and feedback concerning company policies or procedures. Questions may relate to the work environment, supervisors, compensation, the work itself, and the company culture.		Examples:		[1]		Exit interview participation rates vary depending on the method used to conduct the exit interviews. Paper-and-pencil exit interviews provide the lowest participation rates at approximately 30 - 35%. The highest participation rates are achieved using online exit interviews. The average participation rates for organizations using online exit interviews is 65%. [2]		Exit interviews in education are conducted with students who have graduated from an educational institution. These interviews are meant to gather information about students’ experience while attending that institution, what they benefited from, what was missing, and what could be improved to enhance the experience of the next generation of students who attend that institution. This type of interview can also point to areas in which the institution should invest more or less resources to enhance a student’s learning and development experience.		Exit interviews in associations are administered to members who decide to end membership with an association. These interviews provide feedback to an association regarding what caused the member to leave, what can be improved, and how resources can better be allocated.[citation needed]		During elections, pollsters may conduct random exit polls.		There are various methods of conducting exit interviews, each with their benefits and disadvantages with regards to the depth of participation, various biases that may happen, or the format of the information gathered.		A voice interview can be conducted by an internal agent (i.e. an HR department) or an external agent (i.e. HR exit interview consulting firm). Questions are normally asked in a structured order. Voice interviews allow for higher complexity and depth of participation than other methods, because it allows the interviewer to pose follow-up questions, capture ideas through tone indications, probe for answers and clarifications, and ask for examples. However, this allows for bias on the part of the interviewer and the interviewee. This bias can represent itself in the way a question is asked and the length of an answer to a question. Also, an employee may find it difficult to verbalize constructive critique (particularly when the interview is conducted face-to-face) due to social pressure not to upset the other participant (the interviewer).		A voice interview needs to be administered by a professional, and while this allows for higher quality data to be gathered, it is the most expensive option. Data collected needs to be entered manually into a tracking system.		Voice interviews have low participation rates over the medium of telephone due to caller ID being collected[clarification needed]		Exit interviews taken in paper form allows interviews to be conducted with those who do not have Internet access, and allows for the option of anonymity. However, it takes longer to receive feedback, and respondents who are not literate would find it difficult to use this medium. Information must also be entered into a tracking system manually for this medium.		Interviews conducted through a Web interface have the advantage of having a high reliability, flexibility, and privacy[clarification needed], as it is completed by the respondent. The exit interview would also be accessible at the convenience of the respondent, anywhere where the respondent can find Internet access, and they would receive quick feedback. However, a respondent would not be able to complete the interview if they do not have Internet access and may find it difficult if they are not literate or have sufficient technical knowledge.		This method has a low administration cost, and data is entered automatically into any relevant system. Statistics and reporting information can be accessed in real time.		IVRs are reliable methods of taking exit interviews because they are accessible by phone, a very widespread and reliable technology. However, IVRs have fallen out of favor due to the cost effectiveness of web based options that yield data at similar or higher quality. In comparison to other options, it is difficult to get rich data from an IVR, or to adjust and change it, since any changes require new voice recordings to be made.		
In computing, a Digital Object Identifier or DOI is a persistent identifier or handle used to uniquely identify objects, standardized by the ISO.[1] An implementation of the Handle System,[2][3] DOIs are in wide use mainly to identify academic, professional, and government information, such as journal articles, research reports and data sets, and official publications though they also have been used to identify other types of information resources, such as commercial videos.		A DOI aims to be "resolvable", usually to some form of access to the information object to which the DOI refers. This is achieved by binding the DOI to metadata about the object, such as a URL, indicating where the object can be found. Thus, by being actionable and interoperable, a DOI differs from identifiers such as ISBNs and ISRCs which aim only to uniquely identify their referents. The DOI system uses the indecs Content Model for representing metadata.		The DOI for a document remains fixed over the lifetime of the document, whereas its location and other metadata may change. Referring to an online document by its DOI provides more stable linking than simply using its URL, because if its URL changes, the publisher only needs to update the metadata for the DOI to link to the new URL.[4][5][6]		The developer and administrator of the DOI system is the International DOI Foundation (IDF), which introduced it in 2000.[7] Organizations that meet the contractual obligations of the DOI system and are willing to pay to become a member of the system can assign DOIs.[8] The DOI system is implemented through a federation of registration agencies coordinated by the IDF.[9] By late April 2011 more than 50 million DOI names had been assigned by some 4,000 organizations,[10] and by April 2013 this number had grown to 85 million DOI names assigned through 9,500 organizations.						A DOI is a type of Handle System handle, which takes the form of a character string divided into two parts, a prefix and a suffix, separated by a slash. The prefix identifies the registrant of the identifier, and the suffix is chosen by the registrant and identifies the specific object associated with that DOI. Most legal Unicode characters are allowed in these strings, which are interpreted in a case-insensitive manner. The prefix usually takes the form 10.NNNN, where NNNN is a series of at least 4 numbers greater than or equal to 1000, whose limit depends only on the total number of registrants.[11][12] The prefix may be further subdivided with periods, like 10.NNNN.N.[13]		For example, in the DOI name 10.1000/182, the prefix is 10.1000 and the suffix is 182. The "10." part of the prefix distinguishes the handle as part of the DOI namespace, as opposed to some other Handle System namespace,[A] and the characters 1000 in the prefix identify the registrant; in this case the registrant is the International DOI Foundation itself. 182 is the suffix, or item ID, identifying a single object (in this case, the latest version of the DOI Handbook).		DOI names can identify creative works (such as texts, images, audio or video items, and software) in both electronic and physical forms, performances, and abstract works[14] such as licenses, parties to a transaction, etc.		The names can refer to objects at varying levels of detail: thus DOI names can identify a journal, an individual issue of a journal, an individual article in the journal, or a single table in that article. The choice of level of detail is left to the assigner, but in the DOI system it must be declared as part of the metadata that is associated with a DOI name, using a data dictionary based on the indecs Content Model.		The official DOI Handbook explicitly states that DOIs should display on screens and in print in the format "doi:10.1000/182".[15] Contrary to the DOI Handbook, CrossRef, a major DOI registration agency, recommends displaying a URL (for example, https://doi.org/10.1000/182) instead of the officially specified format (for example, doi:10.1000/182)[16][17] This URL provides the location of an HTTP proxy server which will redirect web accesses to the correct online location of the linked item.[8][18] This recommendation is primarily based on the assumption that the DOI is being displayed without being hyper-linked to its appropriate URL – the argument being that without the hyperlink it is not as easy to copy-and-paste the full URL to actually bring up the page for the DOI, thus the entire URL should be displayed, allowing people viewing the page containing the DOI to copy-and-paste the URL, by hand, into a new window/tab in their browser in order to go to the appropriate page for the document the DOI represents.		Major applications of the DOI system currently include:		In the Organisation for Economic Co-operation and Development's publication service OECD iLibrary, each table or graph in an OECD publication is shown with a DOI name that leads to an Excel file of data underlying the tables and graphs. Further development of such services is planned.[19]		A multilingual European DOI registration agency activity, mEDRA, and a Chinese registration agency, Wanfang Data, are active in non-English language markets.		The IDF designed the DOI system to provide a form of persistent identification, in which each DOI name permanently and unambiguously identifies the object to which it is associated. It also associates metadata with objects, allowing it to provide users with relevant pieces of information about the objects and their relationships. Included as part of this metadata are network actions that allow DOI names to be resolved to web locations where the objects they describe can be found. To achieve its goals, the DOI system combines the Handle System and the indecs Content Model with a social infrastructure.		The Handle System ensures that the DOI name for an object is not based on any changeable attributes of the object such as its physical location or ownership, that the attributes of the object are encoded in its metadata rather than in its DOI name, and that no two objects are assigned the same DOI name. Because DOI names are short character strings, they are human-readable, may be copied and pasted as text, and fit into the URI specification. The DOI name-resolution mechanism acts behind the scenes, so that users communicate with it in the same way as with any other web service; it is built on open architectures, incorporates trust mechanisms, and is engineered to operate reliably and flexibly so that it can be adapted to changing demands and new applications of the DOI system.[20] DOI name-resolution may be used with OpenURL to select the most appropriate among multiple locations for a given object, according to the location of the user making the request.[21] However, despite this ability, the DOI system has drawn criticism from librarians for directing users to non-free copies of documents that would have been available for no additional fee from alternative locations.[22]		The indecs Content Model as used within the DOI system associates metadata with objects. A small kernel of common metadata is shared by all DOI names and can be optionally extended with other relevant data, which may be public or restricted. Registrants may update the metadata for their DOI names at any time, such as when publication information changes or when an object moves to a different URL.		The International DOI Foundation (IDF) oversees the integration of these technologies and operation of the system through a technical and social infrastructure. The social infrastructure of a federation of independent registration agencies offering DOI services was modelled on existing successful federated deployments of identifiers such as GS1 and ISBN.		A DOI name differs from commonly used Internet pointers to material, such as the Uniform Resource Locator (URL), in that it identifies an object itself as a first-class entity, rather than the specific place where the object is located at a certain time. It implements the Uniform Resource Identifier (Uniform Resource Name) concept and adds to it a data model and social infrastructure.[23]		A DOI name also differs from standard identifier registries such as the ISBN, ISRC, etc. The purpose of an identifier registry is to manage a given collection of identifiers, whereas the primary purpose of the DOI system is to make a collection of identifiers actionable and interoperable, where that collection can include identifiers from many other controlled collections.[24]		The DOI system offers persistent, semantically-interoperable resolution to related current data and is best suited to material that will be used in services outside the direct control of the issuing assigner (e.g., public citation or managing content of value). It uses a managed registry (providing social and technical infrastructure). It does not assume any specific business model for the provision of identifiers or services and enables other existing services to link to it in defined ways. Several approaches for making identifiers persistent have been proposed. The comparison of persistent identifier approaches is difficult because they are not all doing the same thing. Imprecisely referring to a set of schemes as "identifiers" doesn't mean that they can be compared easily. Other "identifier systems" may be enabling technologies with low barriers to entry, providing an easy to use labeling mechanism that allows anyone to set up a new instance (examples include Persistent Uniform Resource Locator (PURL), URLs, Globally Unique Identifiers (GUIDs), etc.), but may lack some of the functionality of a registry-controlled scheme and will usually lack accompanying metadata in a controlled scheme. The DOI system does not have this approach and should not be compared directly to such identifier schemes. Various applications using such enabling technologies with added features have been devised that meet some of the features offered by the DOI system for specific sectors (e.g., ARK).		A DOI name does not depend on the object's location and, in this way, is similar to a Uniform Resource Name (URN) or PURL but differs from an ordinary URL. URLs are often used as substitute identifiers for documents on the Internet (better characterised as Uniform Resource Identifiers) although the same document at two different locations has two URLs. By contrast, persistent identifiers such as DOI names identify objects as first class entities: two instances of the same object would have the same DOI name.		DOI name resolution is provided through the Handle System, developed by Corporation for National Research Initiatives, and is freely available to any user encountering a DOI name. Resolution redirects the user from a DOI name to one or more pieces of typed data: URLs representing instances of the object, services such as e-mail, or one or more items of metadata. To the Handle System, a DOI name is a handle, and so has a set of values assigned to it and may be thought of as a record that consists of a group of fields. Each handle value must have a data type specified in its <type> field, which defines the syntax and semantics of its data. While a DOI persistently and uniquely identifies the object to which it is assigned, DOI resolution may not be persistent, due to technical and administrative issues.		To resolve a DOI name, it may be input to a DOI resolver, such as doi.org.		Another approach, which avoids typing or cutting-and-pasting into a resolver is to include the DOI in a document as a URL which uses the resolver as an HTTP proxy, such as http://doi.org/ (preferred)[25] or http://dx.doi.org/, both of which support HTTPS. For example, the DOI 10.1000/182 can be included in a reference or hyperlink as https://doi.org/10.1000/182. This approach allows users to click on the DOI as a normal hyperlink. Indeed, as previously mentioned, this is how CrossRef recommends that DOIs always be represented (preferring HTTPS over HTTP), so that if they are cut-and-pasted into other documents, emails, etc., they will be actionable.		Other DOI resolvers and HTTP Proxies include http://hdl.handle.net, http://doi.medra.org, https://doi.pangaea.de/. At the beginning of the year 2016, a new class of alternative DOI resolvers was started by http://doai.io. This service is unusual in that it tries to find a non-paywalled version of a title and redirects you to that instead of the publisher's version.[26][27] Since then, other open-access favoring DOI resolvers have been created, notably https://oadoi.org/ in October 2016.[28] While traditional DOI resolvers solely rely on the Handle System, alternative DOI resolvers first consult open access resources such as BASE (Bielefeld Academic Search Engine).[26][28]		An alternative to HTTP proxies is to use one of a number of add-ons and plug-ins for browsers, thereby avoiding the conversion of the DOIs to URLs,[29] which depend on domain names and may be subject to change, while still allowing the DOI to be treated as a normal hyperlink. For example. the CNRI Handle Extension for Firefox, enables the browser to access Handle System handles or DOIs like hdl:4263537/4000 or doi:10.1000/1 directly in the Firefox browser, using the native Handle System protocol. This plug-in can also replace references to web-to-handle proxy servers with native resolution. A disadvantage of this approach for publishers is that, at least at present, most users will be encountering the DOIs in a browser, mail reader, or other software which does not have one of these plug-ins installed.		The International DOI Foundation (IDF), a non-profit organisation created in 1998, is the governance body of the DOI system.[30] It safeguards all intellectual property rights relating to the DOI system, manages common operational features, and supports the development and promotion of the DOI system. The IDF ensures that any improvements made to the DOI system (including creation, maintenance, registration, resolution and policymaking of DOI names) are available to any DOI registrant. It also prevents third parties from imposing additional licensing requirements beyond those of the IDF on users of the DOI system.		The IDF is controlled by a Board elected by the members of the Foundation, with an appointed Managing Agent who is responsible for co-ordinating and planning its activities. Membership is open to all organizations with an interest in electronic publishing and related enabling technologies. The IDF holds annual open meetings on the topics of DOI and related issues.		Registration agencies, appointed by the IDF, provide services to DOI registrants: they allocate DOI prefixes, register DOI names, and provide the necessary infrastructure to allow registrants to declare and maintain metadata and state data. Registration agencies are also expected to actively promote the widespread adoption of the DOI system, to cooperate with the IDF in the development of the DOI system as a whole, and to provide services on behalf of their specific user community. A list of current RAs is maintained by the International DOI Foundation. The IDF is recognized as one of the federated registrars for the Handle System by the DONA Foundation (of which the IDF is a board member), and is responsible for assigning Handle System prefixes under the top-level 10 prefix.[31]		Registration agencies generally charge a fee to assign a new DOI name; parts of these fees are used to support the IDF. The DOI system overall, through the IDF, operates on a not-for-profit cost recovery basis.		The DOI system is an international standard developed by the International Organization for Standardization in its technical committee on identification and description, TC46/SC9.[32] The Draft International Standard ISO/DIS 26324, Information and documentation – Digital Object Identifier System met the ISO requirements for approval. The relevant ISO Working Group later submitted an edited version to ISO for distribution as an FDIS (Final Draft International Standard) ballot,[33] which was approved by 100% of those voting in a ballot closing on 15 November 2010.[34] The final standard was published on 23 April 2012.[1]		DOI is a registered URI under the info URI scheme specified by IETF RFC 4452. info:doi/ is the infoURI Namespace of Digital Object Identifiers.[35]		The DOI syntax is a NISO standard, first standardised in 2000, ANSI/NISO Z39.84-2005 Syntax for the Digital Object Identifier.[36]		The maintainers of the DOI system have deliberately not registered a DOI namespace for URNs, stating that:		URN architecture assumes a DNS-based Resolution Discovery Service (RDS) to find the service appropriate to the given URN scheme. However no such widely deployed RDS schemes currently exist.... DOI is not registered as a URN namespace, despite fulfilling all the functional requirements, since URN registration appears to offer no advantage to the DOI System. It requires an additional layer of administration for defining DOI as a URN namespace (the string urn:doi:10.1000/1 rather than the simpler doi:10.1000/1) and an additional step of unnecessary redirection to access the resolution service, already achieved through either http proxy or native resolution. If RDS mechanisms supporting URN specifications become widely available, DOI will be registered as a URN.		
The Minimum Wage Ordinance Cap. 608 is an ordinance enacted by the Legislative Council of Hong Kong to introduce a minimum wage in Hong Kong in July 2010.[1] The executive branch proposed a minimum wage of HK$28 (~US$3.61) per hour in November 2010, which the Legislative Council voted to accept after much debate in January 2011.[2][3] It came into effect on 1 May 2011.[4] Prior to this, there had also been a fixed minimum wage for one specific class of workers, foreign domestic helpers, of HK$3,740/month.[5]		The Hong Kong statutory minimum wage for non-domestic workers is HK$34.5 (~US$4.43) per hour,[6] effective 1 May 2017.		On 1 May 2017, the statutory minimum wage increased to $34.5 per hour.[7]						Hong Kong had some legislation relating to the minimum wage as early as 1932; the Governor was granted the right, but was not obliged, to establish a minimum wage.[1] The Trade Boards Ordinance also gave the governor (and after 1997, the Chief Executive) the power to set minimum wages for piece-rate and time-rate work, and established penalties for non-compliance.[8] However, no governor exercised these powers.[1] In 2006, legislators floated a proposal for a voluntary minimum wage.[1] The executive branch formed a Minimum Wage Provisional Commission in February 2009 to research and eventually set a proposed wage floor.[9]		More debate came about on the possibility of a minimum wage in 2010. Legislator Tommy Cheung, who represents the catering functional constituency, suggested that the minimum wage be no greater than HK$20.[10] This earned him the derogatory nickname "Twenty-dollar Cheung".[fn 1] He later amended his proposal to HK$24.[11] Lam Woon-kwong of the Equal Opportunities Commission also indicated he had no objection to a lower minimum wage for disabled people.[12] Chief Executive Donald Tsang was opposed to the whole concept of a minimum wage, according to legislator Lee Cheuk-yan of the Hong Kong Confederation of Trade Unions. Other voices of opposition included the free-market think tank Lion Rock Institute, as well as Miriam Lau of the Liberal Party, who gave estimates that between 30,000 and 170,000 jobs would be lost as a result of the proposal, depending on the wage adopted.[1]		The Minimum Wage Bill was passed on 15 July 2010 by a vote of 53–1 after extensive debate which included the tabling of 34 amendments. The lone opposition vote came from Paul Tse, a functional constituency legislator representing the tourism sector.[13] The bill required the Chief Executive to propose a minimum wage level, which LegCo would then either approve or reject the amount. The law did not give LegCo the power to amend the amount.[14] The proposed minimum wage had been expected to be between HK$23 and HK$33 per hour.[1] Among the amendments:		On 10 November 2010, a HK$28 (~US$3.60) per hour rate was recommended by the Provisional Minimum Wage Commission and adopted by the Chief Executive-in-Council.[2][14] The Legislative Council voted to accept the proposed wage on 5 January 2011.[3] It came into force on 1 May 2011.[4]		The law does not mandate that meal breaks and rest days be paid; Secretary for Labour and Welfare Matthew Cheung stated that this should be decided by private negotiation between employers and employees.[15] There were fears that the implementation of the law might actually lead to lower take-home pay for low-income workers who currently receive paid meal breaks. In November 2010, before the minimum wage came into effect, fast-food chain Cafe de Coral had forced staff to sign new contracts that would give them a pay raise but see their paid meal breaks forfeited, effectively leading to lower pay.[16] In April 2011, Edward Cheng, president of the Hong Kong Association of Property Management Companies, the largest property management association in Hong Kong, stated that he would appeal to their members to retain paid meal breaks for estate security guards where possible; however, he pointed out that the property owners themselves would have to approve any consequent increases in management fees.[17]		On 1 May 2013, the statutory minimum wage is reviewed, and to be set at the level at $30 (~US$3.87).[6] This is to be in effect from 1 May 2013 to 30 April 2015, as the minimum wage is set to be reviewed every two years.		On 1 May 2015, the statutory minimum wage is reviewed and to be set at $32.5 (~US$4.19).[6] This is to be in effect from 1 May 2015 to 30 April 2017.		Foreign domestic helpers' minimum wages are inflation-adjusted annually for contracts about to be signed, and apply for the duration of the contract.[18] Furthermore, FDHs are entitled to one 24-hour rest period each week. An employer's failure to meet this minimum level may result in a fine as high as HK$350,000 and three years' imprisonment.[5]		The minimum wage for FDHs was reduced by HK$190 (5%) in 1999.[18] Again in April 2003, in a deflationary environment, the Government announced a HK$400 reduction in pay, to HK$3,270, "due to the steady drop in a basket of economic indicators since 1999."[19] This led to lawsuits by some Filipinos in Hong Kong.[20] The minimum allowable wage was raised by HK$80 to HKHK$3,480 per month for contracts signed on or after 6 June 2007.[21] Another HK$100 cost of living adjustment took effect for all employment contracts signed on or after 17 July 2008, increasing the minimum wage to HK$3,580 per month.[5]		
McJob is slang for a low-paying, low-prestige dead-end job that requires few skills and offers very little chance of intracompany advancement.[1] The term McJob comes from the name of the fast-food restaurant McDonald's, but is used to describe any low-status job – regardless of the employer – where little training is required, staff turnover is high, and workers' activities are tightly regulated by managers.[2]						"McJob" was in use at least as early as 1986, according to the Oxford English Dictionary (OED), which defines it as "An unstimulating, low-paid job with few prospects, esp. one created by the expansion of the service sector."[3] Lack of job security is common.		The term was coined by sociologist Amitai Etzioni, and appeared in the Washington Post on August 24, 1986 in the article "McJobs are Bad for Kids".[4] The term was popularized by Douglas Coupland's 1991 novel Generation X: Tales for an Accelerated Culture, described therein as "a low-pay, low-prestige, low-dignity, low benefit, no-future job in the service sector. Frequently considered a satisfying career choice by people who have never held one."[5]		The term appears in the 1994 novel Interface (by Neal Stephenson and George Jewsbury) to describe in the abstract positions that are briefly held and underpaid. In the 1999 British film Human Traffic, one character's work in a generic burger outlet is referred to as a McJob.		In the face of objections from McDonald's, the term "McJob" was added to Merriam-Webster's Collegiate Dictionary in 2003,.[6] In an open letter to Merriam-Webster, McDonald's CEO, James Cantalupo denounced the definition as a "slap in the face" to all restaurant employees, and stated that "a more appropriate definition of a 'McJob' might be 'teaches responsibility'". Merriam-Webster responded that "[they stood] by the accuracy and appropriateness of [their] definition."		On 20 March 2007, the BBC reported that the UK arm of McDonald's planned a public petition to have the OED's definition of "McJob" changed.[7][8] Lorraine Homer from McDonald's stated that the company feels the definition is "out of date and inaccurate".[9] McDonald's UK CEO, Peter Beresford, described the term as "demeaning to the hard work and dedication displayed by the 67,000 McDonald's employees throughout the UK".[10] The company would prefer the definition to be rewritten to "reflect a job that is stimulating, rewarding ... and offers skills that last a lifetime".[11][12]		These comments run counter to the principle that dictionaries simply record linguistic usage rather than judge it, and that dropping the entry for "McJob" would be a precedent for bowdlerising definitions of other derogatory terms.[11] McDonald's attempted to get all of its workers to sign the petition but many refused on the grounds that the current definition is accurate despite the company's complaint.		During the aforementioned arguments that broke out when Merriam-Webster included "McJob" in its new edition, McDonald's officials implied the company might bring a lawsuit against the dictionary based on this trademark issue, but never did so. McDonald's disputes that its jobs are poor, because the company has been nominated for employee awards that are created by employers.[13][14][15][16][17][18] However, this was contradicted in the outcome of the UK McLibel court case, in which the judges ruled that it was fair to say that McDonald's employees worldwide "do badly in terms of pay and conditions".[19]		There are often wide variations in how workers are actually treated depending on the local franchise owner. Some employees start out in entry-level McJobs and later become assistant managers or managers, continuing to work at the same franchise for many years; however this is the exception rather than the norm. McDonald's advertises that its CEO, Jim Skinner, began working at the company as a regular restaurant employee, and that 20 of its top 50 managers began work as regular crew members.[20]		According to Jim Cantalupo, former CEO of McDonald's, the perception of fast-food work being boring and mindless is inaccurate, and over 1,000 of the people who now own McDonald's franchises began behind the counter.[21] Because McDonald's has over 400,000 employees and high turnover, Cantalupo's contention has been criticized as being invalid, working to highlight the exception rather than the rule.[22]		In 2006, McDonald's undertook an advertising campaign in the United Kingdom to challenge the perceptions of the McJob. The campaign, developed by Barkers Advertising and supported by research conducted by Adrian Furnham, professor of psychology at University College London, highlighted the benefits of working for the organization, stating that they were "Not bad for a McJob". So confident were McDonald's of their claims that they ran the campaign on the giant screens of London's Piccadilly Circus.[23]		McJOBS (plural, uppercase) was first registered as a trademark by McDonald's on May 16, 1984, as a name and image for "training handicapped persons as restaurant employees". The trademark lapsed in February 1992, and was declared "Canceled"[24] by the United States Patent and Trademark Office. Following the October 1992 publication of Generation X in paperback, McDonald's restored the trademark.[19][25]				
Career Development is the lifelong process of managing learning, work, leisure, and transitions in order to move toward a personally determined and evolving preferred future.		In educational development, career development provides a person, often a student, focus for selecting a career or subject(s) to undertake in the future. Often educational institutions provide career counsellors to assist students with their educational development.		In organizational development (or OD), the study of career development looks at:		In today's world, more employers are looking for ways to facilitate career development and encourage their employees to drive their own careers.		In personal development, career development is:						
If Wiktionary has a definition already, change this tag to {{TWCleanup2}} or else consider a soft redirect to Wiktionary by replacing the text on this page with {{Wi}}. If Wiktionary does not have the definition yet, consider moving the whole article to Wiktionary by replacing this tag with the template {{Copy to Wiktionary}}.		This template will no longer automatically categorize articles as candidates to move to Wiktionary.		A career woman is known as a woman whose main priority in life is achieving success in her career and profession.[1] These women can also be described as more interested in her career than in being married and having children.[2]		
Career management is the combination of structured planning and the active management choice of one's own professional career. Career management was first defined in a social work doctoral thesis by Mary Valentich as the implementation of a career strategy through application of career tactics in relation to chosen career orientation (Valentich & Gripton, 1978). Career orientation referred to the overall design or pattern of one's career, shaped by particular goals and interests and identifiable by particular positions that embody these goals and interests. Career strategy pertains to the individual's general approach to the realization of career goals, and to the specificity of the goals themselves.Two general strategy approaches are adaptive and planned. Career tactics are actions to maintain oneself in a satisfactory employment situation. Tactics may be more or less assertive, with assertiveness in the work situation referring to actions taken to advance one's career interests or to exercise one's legitimate rights while respecting the rights of others.		Valentich and Gripton defined success as managing one's career effectively through the attainment of desired positions and other rewards.The outcome of successful career management should include personal fulfillment, work/life balance, goal achievement and financial security.		A career includes all types of employment ranging from semi-skilled through skilled, and semi professional to professional. Careers have often been restricted to an employment commitment to a single trade skill, profession or business firm for the entire working life of a person. In recent years, however, a career now includes changes or modifications in employment during the foreseeable future.		The following classification system with minor variations is widely used:						The career management process begins with setting goals/objectives. A relatively specific goal/objective must be formulated. This task may be quite difficult when the individual lacks knowledge of career opportunities and/or is not fully aware of their talents and abilities. However, the entire career management process is based on the establishment of defined goals/objectives whether specific or general in nature. Utilizing career assessments may be a critical step in identifying opportunities and career paths that most resonate with someone. Career assessments can range from quick and informal to more indepth. Regardless of the ones you use, you will need to evaluate them. Most assessments found today for free (although good) do not offer an in-depth evaluation.		The time horizon for the achievement of the selected goals or objectives - short term, medium term or long term - will have a major influence on their formulation.		Other elements include:		Career planning is a subset of career management. Career planning applies the concepts of Strategic planning and Marketing to taking charge of one's professional future. Career is an ongoing process and so it needs to be assessed on continuous basis. This process of re-assessing individual learning and development over a period of time is called Career Planning. According to Mondy and Noe - " Career planning is an ongoing process whereby an individual sets career goals and identifies the means to achieve them."		It is important to come up with your career planning as it gives you the much needed direction and makes it clear there where you see yourself in future. It makes you aware of your strength and weaknesses and the skills and knowledge that are required to achieve your goals in future.		A large proportion of our life is spent in achieving our career goals, thus it is very important to make sure that right steps were taken and correct planning was done in the early years of your life. There are very few lucky ones who are born with a clear mind and who knows what they want to do and where they see themselves in life ahead. But majority of us are not sure what we want from life and so it in very important to plan out things. Thus career planning is what gives your career and in some way your life, true meaning and purpose.		The process of career planning is also known as career development stages and career development model. These steps help you in planning your career and deciding about your future.[1]		Self-assessment is a process that helps you in assessing your skills, your potential, your strengths and your ability to fulfill your aims. As the name of the step suggest, you assess yourself and then, based on your analyses and keeping your strengths and weaknesses in mind, you draft your future plan. By drafting your future plan we mean that executing this step helps you to finalize the profession and career path you want to choose. Make sure that you choose and finalize more than one career, keep one or two careers in case you decide to roll back. In case the career you chose does not satisfies you or later in time you come to know that this was not meant for you then in that case you must have a backup plan.		Once you have self-analyzed yourself, the second step that awaits your attention is to fill the loopholes you have identified in the above step. By this we mean that in this step you have to see that what are the qualities and skills that are required by you to help you achieve your aims and goals. For instance you might decide that you need training or a particular course in a field in order to make you perfect for the profession you have chosen.[2]		It could be that you are interested in painting but you are not much aware of the trends or the knowledge that is required for this field. Or there can be a case where you are interested and much aware about a profession like teaching but you do not yet know that what is the niche level that is meant for you like and the subjects you can carry off pretty well.		Once you have listed the careers that are favorable in your case and the skills and improvements that are required by you in order to achieve excellence the third step requires you to do an intensive research and see that what that are findings related to career options and the skills that are required to make you champion in that. You research will be looking into following questions:		Once you have researched the feasibility of the factors that you have finalized in above steps, the next step is to show some action and translate your plans on a piece of page. This step requires you to make plan as in how you are going to achieve and fulfill the steps you have decided above. The best way to come with an action plan is to come up with small goals for oneself. Once these small goals are achieved, we can see that how much close we are to our main aim and major goal. This small step acts as a path way to the main aim.		Once you are done with small goals and the main aim, the next step remains to start implementing your plans. Keep a very close track of your activities to make sure that you are on the right track and that by following this path you are surely going to achieve you goal!		Valentich, Mary & Gripton, James (1978). "Sexism and sex differences in career management of social workers. The Social Science Journal. 15(2), 101-111.		
Human Resource Management [HRM or HR] is the management of human resources. It is designed by the HR Department[by whom?] to maximize employee performance in service of an employer's strategic objectives.[1][2][need quotation to verify] HR is primarily concerned with the management of people within organizations, focusing on policies and on systems.[3] HR departments are responsible for overseeing employee-benefits design, employee recruitment, training and development, performance appraisal, and rewarding (e.g., managing pay and benefit systems).[4] HR also concerns itself with organizational change and industrial relations, that is, the balancing of organizational practices with requirements arising from collective bargaining and from governmental laws.[5][need quotation to verify]		HR is a product of the human relations movement of the early 20th century, when researchers began documenting ways of creating business value through the strategic management of the workforce.[citation needed] It was initially dominated by transactional work, such as payroll and benefits administration, but due to globalization, company consolidation, technological advances, and further research, HR as of 2015[update] focuses on strategic initiatives like mergers and acquisitions, talent management, succession planning, industrial and labor relations, and diversity and inclusion.		Human resources focuses on maximizing employee productivity.[citation needed] HR professionals manage the human capital of an organization and focus on implementing policies and processes. They can specialise on recruiting, training, employee-relations or benefits. Recruiting specialists find and hire top talent. Training and development professionals ensure that employees are trained and have continuous development. This is done through training programs, performance evaluations and reward programs. Employee relations deals with concerns of employees when policies are broken, such as in cases involving harassment or discrimination. Someone in benefits develops compensation structures, family-leave programs, discounts and other benefits that employees can get. On the other side of the field are Human Resources Generalists or business partners. These human-resources professionals could work in all areas or be labor-relations representatives working with unionized employees.		In startup companies, trained professionals may perform HR duties. In larger companies, an entire functional group is typically dedicated to the discipline, with staff specializing in various HR tasks and functional leadership engaging in strategic decision-making across the business. To train practitioners for the profession, institutions of higher education, professional associations, and companies have established programs of study dedicated explicitly to the duties of the function. Academic and practitioner organizations may produce field-specific publications. HR is also a field of research study that is popular within the fields of management and industrial/organizational psychology, with research articles appearing in a number of academic journals, including those mentioned later in this article.		Some businesses globalize and form more diverse teams. HR departments have the role of making sure that these teams can function and that people can communicate across cultures and across borders. Due to changes in commerce, current topics in human resources include diversity and inclusion as well as using technology to advance employee engagement. In the current[update] global work environment, most companies focus on lowering employee turnover and on retaining the talent and knowledge held by their workforce.[citation needed] New hiring not only entails a high cost but also increases the risk of a newcomer not being able to replace the person who worked in a position before. HR departments strive to offer benefits that will appeal to workers, thus reducing the risk of losing corporate knowledge.						The Human Resources field evolved first in 18th century Europe from a simple idea by Robert Owen and Charles Babbage during the industrial revolution. These men knew that people were crucial to the success of an organization. They expressed that the well being of employees led to perfect work. Without healthy workers, the organization would not survive.[6] HR later emerged as a specific field in the early 20th century, influenced by Frederick Winslow Taylor (1856–1915). Taylor explored what he termed "scientific management" others later referred to "Taylorism", striving to improve economic efficiency in manufacturing jobs. He eventually keyed in on one of the principal inputs into the manufacturing process—labor—sparking inquiry into workforce productivity.[7]		Meanwhile, in England C S Myers, inspired by unexpected problems among soldiers which had alarmed generals and politicians in the First World War, set up a National Institute of Industrial Psychology,[8] setting seeds for the human relations movement, which on both sides of the Atlantic built on the research of Elton Mayo and others to document through the Hawthorne studies (1924–1932) and others how stimuli, unrelated to financial compensation and working conditions, could yield more productive workers.[9] Work by Abraham Maslow (1908–1970), Kurt Lewin (1890–1947), Max Weber (1864–1920), Frederick Herzberg (1923–2000), and David McClelland (1917–1998), forming the basis for studies in industrial and organizational psychology, organizational behavior and organizational theory, was interpreted in such a way as to further claims of legitimacy for an applied discipline.		By the time enough theoretical evidence existed to make a business case for strategic workforce management, changes in the business landscape (à la Andrew Carnegie, John Rockefeller) and in public policy (à la Sidney and Beatrice Webb, Franklin D. Roosevelt and the New Deal) had transformed the employer-employee relationship, and the discipline became formalized as "industrial and labor relations". In 1913 one of the oldest known professional HR associations—the Chartered Institute of Personnel and Development (CIPD)—started in England as the Welfare Workers' Association; it changed its name a decade later to the Institute of Industrial Welfare Workers, and again the next decade to Institute of Labour Management before settling upon its current name in 2000.[10] Likewise in the United States, the world's first institution of higher education dedicated to workplace studies—the School of Industrial and Labor Relations—formed at Cornell University in 1945.[11] In 1948 what would later become the largest professional HR association—the Society for Human Resource Management (SHRM)—formed as the American Society for Personnel Administration (ASPA).[12]		In the Soviet Union, meanwhile, Stalin's use of patronage exercised through the "HR Department" equivalent in the Bolshevik Party, its Orgburo, demonstrated the effectiveness and influence of human-resource policies and practices,[13][14] and Stalin himself acknowledged the importance of the human resource, such as in his mass deployment of it in the Gulag system.[15]		During the latter half of the 20th century, union membership declined significantly, while workforce management continued to expand its influence within organizations.[citation needed] In the USA, the phrase "industrial and labor relations" came into use to refer specifically to issues concerning collective representation, and many[quantify] companies began referring to the proto-HR profession as "personnel administration".[citation needed] Many current HR practices originated with the needs of companies in the 1950s to develop and retain talent.[16]		In the late 20th century, advances in transportation and communications greatly facilitated workforce mobility and collaboration. Corporations began viewing employees as assets rather than as cogs in a machine. "Human resources management" consequently,[citation needed] became the dominant term for the function—the ASPA even changing its name to the Society for Human Resource Management (SHRM) in 1998.[12]		"Human capital management" (HCM[17]) is sometimes used[by whom?] synonymously with "HR", although "human capital" typically refers to a more narrow view of human resources; i.e., the knowledge the individuals embody and can contribute to an organization. Likewise, other terms sometimes used to describe the field include "organizational management", "manpower management", "talent management", "personnel management", and simply "people management".		Several popular media productions have depicted HR. On the U.S. television series of The Office, HR representative Toby Flenderson is sometimes seen as a nag because he constantly reminds coworkers of company policies and government regulations.[18] Long-running American comic strip Dilbert frequently portrays sadistic HR policies through character Catbert, the "evil director of human resources".[19] An HR manager is the title character in the 2010 Israeli film The Human Resources Manager, while an HR intern is the protagonist in 1999 French film Ressources humaines. Additionally, the main character in the BBC sitcom dinnerladies, Philippa, is an HR manager. The protagonist of the Mexican telenovela Mañana Es Para Siempre is a Director of Human Resources.		Dave Ulrich lists the functions of HR as: aligning HR and business strategy, re-engineering organization processes, listening and responding to employees, and managing transformation and change.[20]		At the macro-level, HR is in charge of overseeing organizational leadership and culture. HR also ensures compliance with employment and labor laws, which differ by geography, and often oversees health, safety, and security. In circumstances where employees desire and are legally authorized to hold a collective bargaining agreement, HR will typically also serve as the company's primary liaison with the employee's representatives (usually a labor union). Consequently, HR, usually through representatives, engages in lobbying efforts with governmental agencies (e.g., in the United States, the United States Department of Labor and the National Labor Relations Board) to further its priorities.		Human Resource Management has four basic functions: staffing, training and development, motivation and maintenance. Staffing is the recruitment and selection of potential employees, done through interviewing, applications, networking, etc. Training and development is the next step in a continuous process of training and developing competent and adapted employees. Motivation is key to keeping employees highly productive. This function can include employee benefits, performance appraisals and rewards. The last function of maintenance involves keeping the employees' commitment and loyalty to the organization.		The discipline may also engage in mobility management, especially for expatriates; and it is frequently involved in the merger and acquisition process. HR is generally viewed as a support function to the business, helping to minimize costs and reduce risk.[21]		There are half a million HR practitioners in the United States and millions more worldwide.[22] The Chief HR Officer or HR Director is the highest ranking HR executive in most companies and typically reports directly to the Chief Executive Officer and works with the Board of Directors on CEO succession.[23][24]		Within companies, HR positions generally fall into one of two categories: generalist and specialist. Generalists support employees directly with their questions, grievances, and work on a range of projects within the organization. They "may handle all aspects of human resources work, and thus require an extensive range of knowledge. The responsibilities of human resources generalists can vary widely, depending on their employer's needs."[25] Specialists, conversely, work in a specific HR function. Some practitioners will spend an entire career as either a generalist or a specialist while others will obtain experiences from each and choose a path later. Being an HR manager consistently ranks as one of the best jobs, with a #4 ranking by CNN Money in 2006 and a #20 ranking by the same organization in 2009, due to its pay, personal satisfaction, job security, future growth, and benefit to society.[26][27]		Human resource consulting is a related career path where individuals may work as advisers to companies and complete tasks outsourced from companies. In 2007, there were 950 HR consultancies globally, constituting a USD $18.4 billion market. The top five revenue generating firms were Mercer, Ernst & Young, Deloitte, Watson Wyatt (now part of Towers Watson), Aon (now merged with Hewitt), and PwC consulting.[28] For 2010, HR consulting was ranked the #43 best job in America by CNN Money.[29]		Some individuals with PhDs in HR and related fields, such as industrial and organizational psychology and management, are professors who teach HR principles at colleges and universities. They are most often found in Colleges of Business in departments of HR or Management. Many professors conduct research on topics that fall within the HR domain, such as financial compensation, recruitment, and training.		Technology has had a significant impact on human resources practices. Human resources is transitioning to a more technology-based profession because utilizing technology makes information more accessible to the whole organization, eliminates time doing administrative tasks, allows businesses to function globally and cuts costs.[30] Information technology has improved HR practices in the following areas:		Recruiting has been the most influenced by information technology.[31] In the past, recruiters had relied on printing in publications and word of mouth to fill open positions. HR professionals were not able to post a job in more than one location and did not have access to millions of people, causing the lead time of new hires to be drawn out and tiresome. With the use of e-recruiting tools, HR professionals can post jobs and track applicants for thousands of jobs in various locations all in one place. Interview feedback, background and drug tests, and onboarding can all be viewed online. This helps the HR professionals keep track of all of their open jobs and applicants in a way that is faster and easier than before. E-recruiting also helps eliminate limitations of geographic location.[31] Jobs can be posted and seen by anyone with internet access. In addition to recruiting portals, HR professionals have a social media presence that allows them to attract employees through the World Wide Web. On social media they can build the company's brand by posting news about the company and photos of fun company events.		Human resources professionals generally process a considerable amount of paperwork on a daily basis. This paperwork could be anything from a department transfer request to an employee's confidential tax form. In addition to processing this paperwork, it has to be on file for a considerable period of time. The use of Human Resources Information Systems (HRIS) has made it possible for companies to store and retrieve files in an electronic format for people within the organization to access when needed. This eliminates thousands of files and frees up space within the office. Another benefit of HRIS is that it allows for information to be accessed in a timelier manner. Instead of HR professionals having to dig through files to gain information, it is accessible in seconds via the HRIS.[32] Having all of the information in one place also allows for professionals to analyze data quicker and across multiple locations because the information is in a centralized location. Examples of some Human Resources Information Systems are PeopleSoft, MyTime, SAP, Timeco, and JobsNavigator.		Technology makes it possible for human resources professionals to train new staff members in a more efficient manner. This gives employees the ability to access onboarding and training programs from anywhere. This eliminates the need for trainers to meet with new hires face to face when completing necessary paperwork to start. Training in virtual classrooms makes it possible for the HR professionals to train a large number of employees quickly and to assess their progress through computerized testing programs.[30] Some employers even incorporate an instructor with virtual training so that new hires are receiving the most vital training. Employees can take control of their own learning and development by engaging in training at a time and place of their choosing, helping them manage their work-life balance. Managers are able to track the training through the internet as well, which helps to reduce redundancy in training and training costs. Skype, virtual chat rooms, and interactive training sites are all resources that enable a more technological approach to training to enhance the experience for the new hire.		Universities offer programs of study for HR and related fields. The School of Industrial and Labor Relations at Cornell University was the world's first school for college-level study in HR.[33] It continues to offer education at the undergraduate, graduate, and professional levels; and it operates a joint degree program with the Samuel Curtis Johnson Graduate School of Management. Other universities with entire colleges dedicated to the study of HR include Pennsylvania State University, Rutgers, The State University of New Jersey School of Management and Labor Relations, Michigan State University, Indiana University, Purdue University, University of Minnesota, Xavier Labour Relations Institute at Jamshedpur-India, University of Illinois at Urbana-Champaign, Renmin University of China and the London School of Economics. In Canada, the School of Human Resources Management at York University is leading education and research in the HRM field. Many colleges and universities house departments and institutes related to the field, either within a business school or in another college. Most business schools offer courses in HR, often in their departments of management.		There are a number of professional associations, some of which offer training and certification. The Society for Human Resource Management, which is based in the United States, is the largest professional association dedicated to HR,[22] with over 285,000 members in 165 countries.[34] It offers a suite of Professional in Human Resources (PHR) certifications through its HR Certification Institute. The Chartered Institute of Personnel and Development, based in England, is the oldest professional HR association,with its predecessor institution being founded in 1918.		Several associations also serve niches within HR. The Institute of Recruiters (IOR) is a recruitment professional association, offering members education, support and training.[35] WorldatWork focuses on "total rewards" (i.e., compensation, benefits, work life, performance, recognition, and career development), offering several certifications and training programs dealing with remuneration and work-life balance. Other niche associations include the American Society for Training & Development and Recognition Professionals International.		A largely academic organization that is relevant to HR is the Academy of Management that has an HR division. This division is concerned with finding ways to improve the effectiveness of HR.[36] The Academy publishes several journals devoted in part to research on HR, including Academy of Management Journal[37] and Academy of Management Review,[38] and it hosts an annual meeting.		Academic and practitioner publications dealing exclusively with HR:		Related publications:		
Occupational health psychology (OHP) is an interdisciplinary area of psychology that is concerned with the health and safety of workers.[1][2][3] OHP addresses a number of major topic areas including the impact of occupational stressors on physical and mental health, the impact of involuntary unemployment on physical and mental health, work-family balance, workplace violence and other forms of mistreatment, accidents and safety, and interventions designed to improve/protect worker health.[1][2] OHP emerged from two distinct disciplines within applied psychology, namely, health psychology and industrial and organizational psychology, as well as occupational medicine.[4] OHP has also been informed by other disciplines including industrial sociology, industrial engineering, and economics,[5] as well as preventive medicine and public health.[6] OHP is concerned with the relationship of psychosocial workplace factors to the development, maintenance, and promotion of workers' health and that of their families.[1][6] Thus the field's focus is work-related factors that can lead to injury, disease, and distress.						The Industrial Revolution prompted thinkers, such as Karl Marx with his theory of alienation,[7] to concern themselves with the nature of work and its impact on workers.[1] Taylor's (1911) Principles of Scientific Management[8][9] as well as Mayo’s research in the late 1920s and early 1930s on workers at the Hawthorne Western Electric plant[10] helped to inject the impact of work on workers into the subject matter psychology addresses. The creation in 1948 of the Institute for Social Research (ISR) at the University of Michigan was important because of its research on occupational stress and employee health.[11][12][13]		Research in the U.K. by Trist and Bamforth (1951) suggested the reduction in autonomy that accompanied organizational changes in English coal mining operations adversely affected worker morale.[14] Arthur Kornhauser’s work in the early 1960s on the mental health of automobile workers in Michigan[15] also contributed to the development of the field.[16][17] A 1971 study by Gardell examined the impact of work organization on mental health in Swedish pulp and paper mill workers and engineers.[18] Research on the impact of unemployment on mental health was conducted at the University of Sheffield’s Institute of Work Psychology.[9] In 1970 Kasl and Cobb documented the impact of unemployment on blood pressure in U.S. factory workers.[19]		The term "occupational health psychology" first appeared in print in 1986 when Everly advocated for psychologists' role in workplace health promotion.[4][20] In 1988, in response to a dramatic increase in the number of stress-related worker compensation claims in the U.S., the National Institute for Occupational Safety and Health (NIOSH) added psychological factors to its list of factors "leading occupational health risk" (p. 201).[21][22] When this change was coupled with an increased recognition of the impact of stress on a range of problems in the workplace, NIOSH found that their stress-related programs were significantly increasing in prominence.[21] In 1990, Raymond et al.[23] argued that the time has come for doctoral-level psychologists to get interdisciplinary OHP training, integrating health psychology with public health, because creating healthy workplaces should be a goal for the field.		Established in 1987, Work & Stress is the first and "longest established journal in the fast developing discipline that is occupational health psychology" (p. 1).[24] Three years later, the American Psychological Association (APA) and NIOSH jointly organized the first international Work, Stress, and Health conference in Washington, DC. The conference has since become a biannual OHP meeting.[25] In 1996, the Journal of Occupational Health Psychology was published by APA. That same year, the International Commission on Occupational Health created the Work Organisation and Psychosocial Factors (ICOH-WOPS) scientific committee,[26][27] which focused primarily on OHP.[25] In 1999, the European Academy of Occupational Health Psychology (EA-OHP) was established at the first European Workshop on Occupational Health Psychology in Lund, Sweden.[28] That workshop is considered to be the first EA-OHP conference, the first of a continuing series of conferences EA-OHP organizes and devotes to OHP research and practice.[28]		In 2000 the informal International Coordinating Group for Occupational Health Psychology (ICGOHP) was founded for the purpose of facilitating OHP-related research, education, and practice as well as coordinating international conference scheduling.[25] Also in 2000, Work & Stress became associated with the EA-OHP.[24] In 2005, the Society for Occupational Health Psychology (SOHP) was established in the United States.[29] In 2008, SOHP joined with APA and NIOSH in co-sponsoring the Work, Stress, and Health conferences.[30] In 2017, SOHP began to publish an OHP-related journal Occupational Health Science.[31]		The main purpose of OHP research is to understand how working conditions affect worker health,[32] use that knowledge to design interventions to protect and improve worker health, and evaluate the effectiveness of such interventions.[33] The research methods used in OHP are similar to those used in other branches of psychology.		Self-report survey methodology is the most used approach in OHP research.[34] Cross-sectional designs are commonly used; case-control designs have been employed much less frequently.[35] Longitudinal designs[36] including prospective cohort studies and experience sampling studies[37] can examine relationships over time.[38][39] OHP-related research devoted to evaluating health-promoting workplace interventions has relied on quasi-experimental designs[40][41] and, less commonly, experimental approaches.[42][43]		Statistical methods commonly used in other areas of psychology are also used in OHP-related research. Statistical methods used include structural equation modeling[44] and hierarchical linear modeling[45] (HLM; also known as multilevel modeling). HLM can better adjust for similarities between employees[45] and is especially well suited to evaluating the lagged impact of work stressors on health outcomes; in this research context HLM can help minimize censoring and is well-suited to experience sampling studies.[46] Meta-analyses have been used to aggregate data (modern approaches to meta-analyses rely on HLM), and draw conclusions across multiple studies.[38]		Qualitative research methods include interviews,[47][48] focus groups,[49] and self-reported, written descriptions of stressful incidents at work.[50][51] First-hand observation of workers on the job has also been used,[52] as has participant observation.[53]		Three influential theoretical models in OHP research are the demand-control-support, demand-resources, and effort-reward imbalance models.		The most influential model in OHP research has been the original demand-control model.[1] According to the model, the combination of low levels of work-related decision latitude (i.e., autonomy and control over the job) combined with high workloads (high levels of work demands) can be particularly harmful to workers (they can lead to "job strain," a term representing the combination of low decision latitude and high workload leading to poorer mental or physical health).[54] The model suggests not only that these two job factors are related to poorer health but that high levels of decision latitude on the job will buffer or reduce the adverse health impact of high levels of demands. Research has clearly supported the idea that decision latitude and demands relate to strains, but research findings about buffering have been mixed with only some studies providing support.[55] The demand-control model asserts that job control can come in two broad forms: ‘skill discretion’ and ‘decision authority’.[56] Skill discretion refers to the level of skill and creativity required on the job and the flexibility an employee is permitted in deciding what skills to use (e.g. opportunity to use skills, similar to job variety).[57] Decision authority refers to employees being able to make decisions about their work (e.g., having autonomy).[57] These two forms of job control are traditionally assessed together in a composite measure of decision latitude; there is, however, some evidence that the two types of job control may not be similarly related to health outcomes.[56][58]		About a decade after Karasek first introduced the demand-control model, Johnson, Hall, and Theorell (1989),[59] in the context of research on heart disease, extended the model to include social isolation. Johnson et al. labeled the combination of high levels of demands, low levels of control, and low levels of coworker support “iso-strain.” The resulting expanded model has been labeled the demand–control–support (DCS) model. Research that followed the development of this model has suggested that one or more of the components of the DCS model (high psychological workload, low control, and lack of social support), if not the exact combination represented by iso-strain, have adverse effects of physical and mental health.[1]		An alternative model, the job demands-resources (JD-R) model,[60] grew out of the DCS model. In the JD-R model, the category of demands (workload) remains more or less the same as in the DCS model although the JD-R model more specifically includes physical demands. Resources, however, are defined as job-relevant features that help workers achieve work-related goals, lessen job demands, or stimulate personal growth. Control and support as per the DCS model are subsumed under resources. Resources can be external (provided by the organization) or internal (part of a worker's personal make-up). In addition to control and support, resources encompassed by the model can also include physical equipment, software, performance feedback from supervisors, the worker's own coping strategies, etc. There has not, however, been as much research on the JD-R model as there has been on the constituents of the DC or DCS model.[1]		After the DCS model, the, perhaps, second most influential model in OHP research has been the effort-reward imbalance (ERI) model. It links job demands to the rewards employees receive for the job.[61][62] That model holds that high work-related effort coupled with low control over job-related intrinsic (e.g., recognition) and extrinsic (e.g., pay) rewards triggers high levels of activation in neurohormonal pathways that, cumulatively, are thought to exert adverse effects on mental and physical health.		A number of work-related, psychosocial factors have been linked to cardiovascular disease (CVD).		Research has identified health-behavioral and biological factors that are related to increased risk for CVD. These risk factors include smoking, obesity, low density lipoprotein (the "bad" cholesterol), lack of exercise, and blood pressure. Psychosocial working conditions are also risk factors for CVD.[1] In a case-control study involving two large U.S. data sets, Murphy (1991) found that hazardous work situations, jobs that required vigilance and responsibility for others, and work that required attention to devices were related to increased risk for cardiovascular disability.[63] These included jobs in transportation (e.g., air traffic controllers, airline pilots, bus drivers, locomotive engineers, truck drivers), preschool teachers, and craftsmen. Among 30 studies involving men[64] and women,[65] most have found an association between workplace stressors and CVD.		Fredikson, Sundin, and Frankenhaeuser (1985) found that reactions to psychological stressors include increased activity in the brain axes which play an important role in the regulation of blood pressure,[66][67] particularly ambulatory blood pressure. A meta-analysis and systematic review involving 29 samples linked job strain to elevated ambulatory blood pressure.[68] Belkić et al. (2000)[69] found that many of the 30 studies covered in their review revealed that decision latitude and psychological workload exerted independent effects on CVD; two studies found synergistic effects, consistent with the strictest version of the demand-control model.[70][71] A review of 17 longitudinal studies having reasonably high internal validity found that 8 showed a significant relation between the combination of low latitude and high workload (the job strain condition) and CVD and 3 more showed a nonsignificant relation.[72] The findings, however, were clearer for men than for women, on whom data were more sparse. In a massive (n > 197,000) longitudinal study that combined data from 13 independent studies, Kivimäki et al. (2012)[73] found that, controlling for other risk factors, the combination of high levels of demands and low control at baseline increased the risk of CVD in initially healthy workers by between 20 and 30% over a follow-up period that averaged 7.5 years. In this study the effects were similar for men and women.		There is evidence that, consistent with the ERI model, high work-related effort coupled with low control over job-related rewards adversely affects cardiovascular health. At least five studies of men have linked effort-reward imbalance with CVD.[74]		Research has suggested that job loss adversely affects cardiovascular health[19][75] as well as health in general.[76][77]		There is evidence from a prospective study that job-related burnout, controlling for traditional risk factors, such as smoking and hypertension, increases the risk of coronary heart disease over the course of the next three and a half years in workers who were initially disease-free.[78]		Musculoskeletal disorders (MSDs) involve injury and pain to the joints and muscles of the body. Approximately 2.5 million workers in the US suffer from MSDs,[79] which is the third most common cause of disability and early retirement for American workers.[80] In Europe MSDs are the most often reported workplace health problem.[81] The development of musculoskelelatal problems cannot be solely explained in the basis of biomechanical factors (e.g., repetitive motion) although such factors are important contributors.[82] There has been evidence that psychosocial workplace factors (e.g., job strain) also contribute to the development of musculoskeletal problems.[82][83][84]		There are many forms of workplace mistreatment ranging from relatively minor incivility to serious cases of bullying and violence.[85]		Workplace incivility has been defined as "low-intensity deviant behavior with ambiguous intent to harm the target....Uncivil behaviors are characteristically rude and discourteous, displaying a lack of regard for others" (p. 457).[86] Incivility is distinct from violence. Examples of workplace incivility include insulting comments, denigration of the target's work, spreading false rumors, social isolation, etc. A summary of research conducted in Europe suggests that workplace incivility is common there.[87] In research on more than 1000 U.S. civil service workers, more than 70% of the sample experienced workplace incivility in the past five years. Compared to men, women were more exposed to incivility; incivility was associated with psychological distress and reduced job satisfaction.[87]		Abusive supervision is the extent to which a supervisor engages in a pattern of behavior that harms subordinates.[88][89]		Although definitions of workplace bullying vary, it involves a repeated pattern of harmful behaviors directed towards an individual by one or more others who have more power than the target.[90] Workplace bullying is sometimes termed mobbing.		Sexual harassment is behavior that denigrates or mistreats an individual due to his or her gender, creates an offensive workplace, and interferes with an individual being able to do the job.[91]		Workplace violence is a significant health hazard for employees.[1]		Most workplace assaults are nonfatal, with an annual physical assault rate of 6% in the U.S.[92] Assaultive behavior in the workplace often produces injury, psychological distress, and economic loss. One study of California workers found a rate of 72.9 non-fatal, officially documented assaults per 100,000 workers per year, with workers in the education, retail, and health care sectors subject to excess risk.[93] A Minnesota workers' compensation study found that women workers had a twofold higher risk of being injured in an assault than men, and health and social service workers, transit workers, and members of the education sector were at high risk for injury compared to workers in other economic sectors.[94] A West Virginia workers' compensation study found that workers in the health care sector and, to a lesser extent, the education sector were at elevated risk for assault-related injury.[95] Another workers' compensation study found that excessively high rates of assault-related injury in schools, healthcare, and, to a lesser extent, banking.[96] In addition to the physical injury that results from being a victim of workplace violence, individuals who witness such violence without being directly victimized are at increased risk for experiencing adverse effects, as found in a study of Los Angeles teachers.[97]		In 1996 there were 927 work-associated homicides in the United States,[98] in a labor force that numbered approximately 132,616,000.[99] The rate works out to be about 7 homicides per million workers for the one year. Men are more likely to be victims of workplace homicide than women.[94]		Research has found that psychosocial workplace factors are among the risk factors for a number of categories of mental disorder.		Workplace factors can contribute to alcohol abuse and dependence of employees. Rates of abuse can vary by occupation, with high rates in the construction and transportation industries as well as among waiters and waitresses.[100] Within the transportation sector, heavy truck drivers and material movers were at especially high risk. A prospective study of ECA subjects who were followed one year after the initial interviews provided data on newly incident cases of alcohol abuse and dependence.[101] The study found that workers in jobs that combined low control with high physical demands were at increased risk of developing alcohol problems although the findings were confined to men.		Using data from the ECA study, Eaton, Anthony, Mandel, and Garrison (1990) concluded that members of three occupational groups, lawyers, secretaries, and special education teachers (but not other types of teachers) showed elevated rates of DSM-III major depression, adjusting for social demographic factors.[102] The ECA study involved representative samples of American adults from five U.S. geographical areas, providing relatively unbiased estimates of the risk of mental disorder by occupation; however, because the data were cross-sectional, no conclusions bearing on cause-and-effect relations are warranted. Evidence from a Canadian prospective study indicated that individuals in the highest quartile of occupational stress (high-strain jobs as per the demand-control model) are at increased risk of experiencing an episode of major depression.[103] A meta-analysis that pooled the results of 11 well-designed longitudinal studies indicated that a number of facets of the psychosocial work environment (e.g., low decision latitude, high psychological workload, lack of social support at work, effort-reward imbalance, and job insecurity) increase the risk of common mental disorders such as depression.[38]		Depending on the diagnosis, severity and individual, and the job itself, personality disorders can be associated with difficulty coping with work or the workplace, potentially leading to problems with others by interfering with interpersonal relationships. Indirect effects also play a role; for example, impaired educational progress or complications outside of work, such as substance abuse and co-morbid mental disorders, can plague sufferers. However, personality disorders can also bring about above-average work abilities by increasing competitive drive or causing the sufferer to exploit his or her co-workers.[104][105]		In a case-control study, Link, Dohrenwend, and Skodol found that, compared to depressed and well control subjects, schizophrenic patients were more likely to have had jobs, prior to their first episode of the disorder, that exposed them to “noisesome” work characteristics (e.g., noise, humidity, heat, cold, etc.).[106] The jobs tended to be of higher status than other blue collar jobs, suggesting that downward drift in already-affected individuals does not account for the finding. One explanation involving a diathesis-stress model suggests that the job-related stressors helped precipitate the first episode in already-vulnerable individuals. There is some supporting evidence from the Epidemiologic Catchment Area (ECA) study.[107]		Longitudinal studies have suggested adverse working conditions can contribute to the development of psychological distress.[108] Psychological distress refers to negative affect, without the individuals necessarily meeting criteria for a psychiatric disorder.[109][110] Psychological distress is often expressed in affective (depressive), psychophysical or psychosomatic (e.g., headaches, stomach aches, etc.), and anxiety symptoms. The relation of adverse working conditions to psychological distress is thus an important avenue of research. Job satisfaction is also related to negative health outcomes.[111][112]		Parkes (1982)[113] studied the relation of working conditions to psychological distress in British student nurses. She found that in this "natural experiment," student nurses experienced higher levels of distress and lower levels of job satisfaction in medical wards than in surgical wards; compared to surgical wards, medical wards make greater affective demands on the nurses. In another study, Frese (1985)[114] concluded that objective working conditions give rise to subjective stress and psychosomatic symptoms in blue collar German workers. In addition to the above studies, a number of other well-controlled longitudinal studies have implicated work stressors in the development of psychological distress and reduced job satisfaction.[115][116]		A comprehensive meta-analysis involving 86 studies indicated that involuntary job loss is linked to increased psychological distress.[117] The impact of involuntary unemployment was comparatively weaker in countries that had greater income equality and better social safety nets.[117] The research evidence also indicates that poorer mental health slightly, but significantly, increases the risk of later job loss.[117]		Some OHP research is concerned with (a) understanding the impact of economic crises on individuals' physical and mental health and well-being and (b) calling attention to personal and organizational means for ameliorating the impact of the crisis.[118] Economic insecurity contributes, at least partly, to psychological distress and work-family conflict.[119] Ongoing job insecurity, even in the absence of job loss, is related to higher levels of depressive symptoms, psychological distress, and worse overall health.[120]		Employees must balance their working lives with their home lives. Work–family conflict is a situation in which the demands of work conflict with the demands of family, making it difficult to adequately do both, giving rise to distress.[119][121]		A number of stress management interventions have emerged that have shown demonstrable effects in reducing job stress.[122] Cognitive behavioral interventions have tended to have greatest impact on stress reduction.[122]		OHP interventions often concern both the health of the individual and the health of the organization. Adkins (1999) described the development of one such intervention, an organizational health center (OHC) at a California industrial complex.[123] The OHC helped to improve both organizational and individual health as well as help workers manage job stress. Innovations included labor-management partnerships, suicide risk reduction, conflict mediation, and occupational mental health support. OHC practitioners also coordinated their services with previously underutilized local community services in the same city, thus reducing redundancy in service delivery.		Hugentobler, Israel, and Schurman (1992) detailed a different, multi-layered intervention in a mid-sized Michigan manufacturing plant.[124] The hub of the intervention was the Stress and Wellness Committee (SWC) which solicited ideas from workers on ways to improve both their well-being and productivity. Innovations the SWC developed included improvements that ensured two-way communication between workers and management and reduction in stress resulting from diminished conflict over issues of quantity versus quality. Both the interventions described by Adkins and Hugentobler et al. had a positive impact on productivity.		Currently there are efforts under way at NIOSH to help reduce the incidence of preventable disorders (e.g., sleep apnea) among heavy-truck and tractor-trailer drivers and, concomitantly, the life-threatening accidents to which the disorders lead,[125] improve the health and safety of workers who are assigned to shift work or who work long hours,[126] and reduce the incidence of falls among iron workers.[127]		The Mental Health Advisory Teams of the United States Army employ OHP-related interventions with combat troops.[128][129] OHP also has a role to play in interventions aimed at helping first responders.[130][131]		Schmitt (2007) described three different modestly scaled OHP-related interventions that helped workers abstain from smoking, exercise more frequently, and shed weight.[132] Other OHP interventions include a campaign to improve the rates of hand washing, an effort to get workers to walk more often, and a drive to get employees to be more compliant with regard to taking prescribed medicines.[133] The interventions tended reduce organization health-care costs.[132][133]		Organizations can play a role in the health behavior of employees by providing resources to encourage healthy behavior in areas of exercise, nutrition, and smoking cessation.[134]		Although the dimensions of the problem of workplace violence vary by economic sector, one sector, education, has had some limited success in introducing programmatic, psychologically-based efforts to reduce the level of violence.[135] Research suggests that there continue to be difficulties in successfully "screening out applicants [for jobs] who may be prone to engaging in aggressive behavior,"[136] suggesting that aggression-prevention training of existing employees may be an alternative to screening. Only a small number of studies evaluating the effectiveness of training programs to reduce workplace violence currently exist.[137]		Psychological factors are an important factor in occupational accidents that can lead to injury and death of employees. An important influence on the incidence of accidents is the organization's safety climate that is employees' shared beliefs about how supervisors reward and support safety behavior.[138]		
		Retroactive overtime (ROT) is an additional amount of money that is awarded when an employee has a combination of overtime and an additional amount of money, such as a commission or a bonus that is guaranteed based upon work requirements. Overtime is required to qualify for retroactive overtime. So, if a salesperson receives a commission, but does not receive overtime, then the employee does not qualify for retroactive overtime.						Retroactive overtime is computed by using the number of hours of overtime worked for the specified payroll period to look up the coefficient percentages from the coefficient table (Form WH-134).[1] This coefficient percentage is then multiplied by the commission and/or bonus to determine the ROT amount that will be awarded to the employee in addition to the already existing overtime and commission.		The additional amount on money beyond the overtime, the commission or bonus, must be a guaranteed payment to the employee based upon specified work criteria. Here are some examples of some bonuses that qualify and do not qualify.		If an employee is awarded a known amount of money for working a certain shift or for working a number of consecutive weeks, that additional amount of money that is paid beyond the regular base pay and overtime will qualify for retroactive overtime if and only if there are also overtime hours paid during the same pay period of the qualifying bonus. You could also consider this to have an OT value of zero and add an additional look-up table value of all zeros for the percentages to use to determine the ROT amount.		If an employee is awarded a discretionary bonus that is not guaranteed based upon specific work criteria, this bonus does not qualify for retroactive overtime. A good example of this is a Christmas bonus that may be awarded to employees. This is not a guaranteed bonus that the employee will receive for meeting a specified goal but is rather a bonus that is awarded to the employee on the discretion of the company.		
Supervisor, when the meaning sought is similar to foreman, foreperson, boss, overseer, cell coach, manager, facilitator, monitor, or area coordinator, is the job title of a low level management position that is primarily based on authority over a worker or charge of a workplace.[1] A Supervisor can also be one of the most senior in the staff at the place of work, such as a Professor who oversees a PhD dissertation. Supervision, on the other hand, can be performed by people without this formal title, for example by parents. The term Supervisor itself can be used to refer to any personnel who have this task as part of their job description.		An employee is a supervisor if he has the power and authority to do the following actions (according to the Ontario Ministry of Labour):		If an employee cannot do the above, legally, he or she is probably not a supervisor, but in some other category, such as a work group leader or lead hand.		A supervisor is first and foremost an overseer whose main responsibility is to ensure that a group of subordinates get out the assigned amount of production, when they are supposed to do it and within acceptable levels of quality, costs and safety.		A supervisor is responsible for the productivity and actions of a small group of employees. The supervisor has several manager-like roles, responsibilities, and powers. Two of the key differences between a supervisor and a manager are (1) the supervisor does not typically have "hire and fire" authority, and (2) the supervisor does not have budget authority.		Lacking "hire and fire" authority means that a supervisor may not recruit the employees working in the supervisor's group nor does the supervisor have the authority to terminate an employee. The supervisor may participate in the hiring process as part of interviewing and assessing candidates, but the actual hiring authority rests in the hands of a Human Resource Manager. The supervisor may recommend to management that a particular employee be terminated and the supervisor may be the one who documents the behaviors leading to the recommendation but the actual firing authority rests in the hands of a manager.		Lacking budget authority means that a supervisor is provided a budget developed by management within which constraints the supervisor is expected to provide a productive environment for the employees of the supervisor's work group. A supervisor will usually have the authority to make purchases within specified limits. A supervisor is also given the power to approve work hours and other payroll issues. Normally, budget affecting requests such as travel will require not only the supervisor's approval but the approval of one or more layers of management.		As a member of management, a supervisor's main job is more concerned with orchestrating and controlling work rather than performing it directly.						Supervisors are uniquely positioned through direct daily employee contact to respond to employee needs, problems, and satisfaction. Supervisors are the direct link between management and the work force and can be most effective in developing job training, safety attitudes, safe working methods and identifying unsafe acts		"Doing" can take up to 70% of the time - (this varies according to the type of supervisory job - the doing involves the actual work of the department as well as the planning, controlling, scheduling, organizing, leading, etc.).[2]		Supervisors often do not require any formal education on how they are to perform their duties but are most often given on-the-job training or attend company sponsored courses. Many employers have supervisor handbooks that need to be followed. Supervisors must be aware of their legal responsibilities to ensure that their employees work safely and that the workplace that they are responsible for meets government standards.		In academia, a supervisor is a senior scientist or scholar who, along with their own responsibilities, aids and guides a postgraduate research student, or undergraduate student, in their research project; offering both moral support and scientific insight and guidance. The term is used in several countries for the doctoral advisor of a graduate student.		In colloquial British English, "gaffer" means a foreman, and is used as a synonym for "boss". In the UK, the term also commonly refers to sports coaches (football, rugby, etc.).		The term is also sometimes used colloquially to refer to an old man, an elderly rustic. The word is probably a shortening of "godfather", with "ga" from association with "grandfather". The female equivalent, "gammer", came to refer colloquially to an old lady or to a gossip.[3] The use of gaffer in this way can be seen, for example, in J.R.R. Tolkien's character Gaffer Gamgee.		In 16th century English a "gaffer" was a man who was the head of any organized group of labourers. In 16th and 17th century rural England, it was used as a title slightly inferior to "Master", similar to "Goodman", and was not confined to elderly men. The chorus of a famous Australian shearer's song, The Backblocks' Shearer (also known as Widgegoeera Joe), written by W. Tully at Nimidgee, NSW (c.1900), refers to a gaffer:		I-O psychology research on first-line supervisors suggests that supervisors with the most productive work groups have the following qualities:		Schultz & Schultz, Duane (2010). Psychology and work today. New York: Prentice Hall. pp. 169–170. ISBN 0-205-68358-4. 		
Occupational burnout is thought to result from long-term, unresolvable job stress. In 1974, Herbert Freudenberger became the first researcher to publish in a psychology-related journal a paper that used the term burnout. The paper was based on his observations of the volunteer staff (including himself) at a free clinic for drug addicts.[1] He characterized burnout by a set of symptoms that includes exhaustion resulting from work's excessive demands as well as physical symptoms such as headaches and sleeplessness, "quickness to anger," and closed thinking. He observed that the burned out worker "looks, acts, and seems depressed". After the publication of Freudenberger's original paper, interest in occupational burnout grew. Because the phrase "burnt-out" was part of the title of a 1961 Graham Greene novel, A Burnt-Out Case, which dealt with a doctor working in the Belgian Congo with patients who had leprosy, the phrase may have been in use outside the psychology literature before Freudenberger employed it.[2]		In order to study burnout, a number of researchers developed more focused conceptualizations of burnout. In one conceptualization, job-related burnout is characterized by emotional exhaustion, depersonalization (treating clients/students and colleagues in a cynical way), and reduced feelings of work-related personal accomplishment.[3][4] In another conceptualization, burnout is thought to comprise emotional exhaustion, physical exhaustion, and cognitive weariness.[5] A third conceptualization holds that burnout consists of exhaustion and disengagement.[6] The core of the three conceptualizations, as well as Freudenberger's, is exhaustion. Long limited to these dimensions, burnout is also now known to involve the full array of depressive symptoms (e.g., low mood, cognitive alterations, sleep disturbance).[7]		Originally, Maslach and her colleagues focused on burnout within human service professions (e.g., teachers, social workers).[8] She later expanded the application of burnout to include individuals in many other occupations.[3]						Burnout is not recognized as a distinct disorder in the DSM-5.[9] However, it is included in the ICD-10, but not as a disorder.[10] It can be found in the ICD under problems related to life-management difficulty (Z73).		In 1981, Christina Maslach and Susan Jackson developed the first widely used instrument for assessing burnout, namely, the Maslach Burnout Inventory (MBI).[11] Consistent with Maslach's conceptualization, the MBI operationalizes burnout as a three-dimensional syndrome consisting of emotional exhaustion, depersonalization, and reduced personal accomplishment.[11][3] Other researchers have argued that burnout should be limited to fatigue and exhaustion.[12]		A growing body of evidence suggests that burnout is etiologically, clinically, and nosologically similar to depression.[13][14][15][16][17][18] In a study that directly compared depressive symptoms in burned out workers and clinically depressed patients, no diagnostically significant differences were found between the two groups; burned out workers reported as many depressive symptoms as clinically depressed patients.[19] Moreover, a study by Bianchi, Schonfeld, and Laurent (2014) showed that about 90% of burned out workers meet diagnostic criteria for depression, suggesting that burnout may be a depressive syndrome rather than a distinct entity.[15] The view that burnout is a form of depression has found support in several recent studies.[13][14][16][17][18][20]		Evidence suggests that burnout's etiology is multifactorial, with dispositional factors playing an important role.[21][22] Cognitive dispositional factors implicated in depression have also been found to be implicated in burnout.[23] One cause of burnout includes stressors that a person is unable to cope with fully. Occupational burnout often develops slowly and may not be recognized until it has become severe. When one's expectations about a job and its reality differ, burnout can begin.		Burnout is thought to occur when a mismatch is present between the nature of the job and the person doing the job. A common indication of this mismatch is work overload, which sometimes involves a worker who survives a round of layoffs, but after the layoffs the worker finds that he or she is doing too much with too few resources. Overload may occur in the context of downsizing, which often does not narrow an organization's goals, but requires fewer employees to meet those goals.[24]		The job demands-resources model has implications for burnout, as measured by the Oldenburg Burnout Inventory (OLBI). Physical and psychological job demands were concurrently associated with the exhaustion, as measured by the OLBI.[25] Lack of job resources was associated with the disengagement component of the OLBI.		Maslach, Schaufeli and Leiter identified six risk factors for burnout: mismatch in workload, mismatch in control, lack of appropriate awards, loss of a sense of positive connection with others in the workplace, perceived lack of fairness, and conflict between values.[26]		Burnout is supposed to be a work-specific syndrome. However, this restrictive view of burnout's scope has been shown to be groundless.[27] In other words, burnout could apply to nonwork roles such as that of caregiver or student.		Some research indicates that burnout is associated with reduced job performance, coronary heart disease,[28] and mental health problems (although note the abovementioned research that suggests it is a depressive syndrome, e.g., Ahola et al., 2005[13]). Chronic burnout is also associated with cognitive impairments such as memory and attention.[29] Occupational burnout is also associated with absences, time missed from work, and thoughts of quitting.[30]		It is difficult to treat the three symptoms of exhaustion, cynicism, and inefficacy, as they react to the same preventive or treatment activities in different ways.[31] Exhaustion is more easily treated than cynicism and professional efficacy, which tend to be more resistant to treatment. Research shows that intervention actually may worsen the professional efficacy of one who originally had low professional efficacy.[32]		For the purpose of preventing occupational burnout, various stress management interventions have been shown to help improve employee health and well-being in the workplace and lower stress levels. Training employees in ways to manage stress in the workplace have also proven effective in prevention of burnout.[33] One study suggest that social-cognitive processes such as commitment to work, self-efficacy, learned resourcefulness and hope may insulate individuals from experiencing occupational burnout.[30] Increased job control is another intervention shown to help counteract exhaustion and cynicism in the workplace.[31]		Burnout prevention programs have traditionally focused on cognitive-behavioral therapy (CBT), cognitive restructuring, didactic stress management, and relaxation. CBT, relaxation techniques (including physical techniques and mental techniques), and schedule changes are the best-supported techniques for reducing and preventing burnout in a health-care specific setting. Combining both organizational and individual level activities may be the most beneficial approach to reduce symptoms. A Cochrane review reported that evidence for the efficacy of CBT in healthcare workers is of low quality, indicating that it is no better than alternative interventions.[34]		Employee rehabilitation is a tertiary preventive intervention which means the strategies used in rehabilitation are meant to alleviate, as well as prevent, burnout symptoms.[31] Such rehabilitation of the working population includes multidisciplinary activities with the intent of maintaining and improving employees' working ability and ensuring a supply of skilled and capable labor in society.		Additional prevention methods include: starting the day with a relaxing ritual; adopting healthy eating, exercising, and sleeping habits; setting boundaries; taking breaks from technology; nourishing one's creative side, and learning how to manage stress.[35][36]		While individuals can cope with the symptoms of burnout, the only way to truly prevent burnout is through a combination of organizational change and education for the individual.[24]		Maslach and Leiter postulated that burnout occurs when there is a disconnection between the organization and the individual with regard to what they called the six areas of work life: workload, control, reward, community, fairness, and values.[26] Resolving these discrepancies requires integrated action on the part of both the individual and the organization.[26] A better connection on workload means assuring adequate resources to meet demands as well as work/life balances that encourage employees to revitalize their energy.[26] A better connection on values means clear organizational values to which employees can feel committed.[26] A better connection on community means supportive leadership and relationships with colleagues rather than discord.[26]		One approach for addressing these discrepancies focuses specifically on the fairness area. In one study employees met weekly to discuss and attempt to resolve perceived inequities in their job.[37] The intervention was associated with decreases in exhaustion over time but not cynicism or inefficacy, suggesting that a broader approach is required.[26]		Stress and the workplace:		Medical:		
Wage labour (also wage labor in American English) is the socioeconomic relationship between a worker and an employer, where the worker sells his or her labour power under a formal or informal employment contract.[1] These transactions usually occur in a labour market where wages are market determined.[2] In exchange for the wages paid, the work product generally becomes the undifferentiated property of the employer, except for special cases such as the vesting of intellectual property patents in the United States where patent rights are usually vested in the employee personally responsible for the invention. A wage labourer is a person whose primary means of income is from the selling of his or her labour power in this way.		In modern mixed economies such as those of the OECD countries, it is currently the most common form of work arrangement. Although most labour is organised as per this structure, the wage work arrangements of CEOs, professional employees, and professional contract workers are sometimes conflated with class assignments, so that "wage labour" is considered to apply only to unskilled, semi-skilled or manual labour.						The most common form of wage labour currently is ordinary direct, or "full-time". This is employment in which a free worker sells his or her labour for an indeterminate time (from a few years to the entire career of the worker), in return for a money-wage or salary and a continuing relationship with the employer which it does not in general offer contractors or other irregular staff. However, wage labour takes many other forms, and explicit as opposed to implicit (i.e. conditioned by local labour and tax law) contracts are not uncommon. Economic history shows a great variety of ways, in which labour is traded and exchanged. The differences show up in the form of:		Many Socialists see wage labour as a major, if not defining, aspect of hierarchical industrial systems. Most opponents of the institution support worker self-management and economic democracy as alternatives to both wage labour and to capitalism. While most opponents of wage labour blame the capitalist owners of the means of production for its existence, most anarchists and other libertarian socialists also hold the state as equally responsible as it exists as a tool utilised by capitalists to subsidise themselves and protect the institution of private ownership of the means of production—which guarantees the concentration of capital among a wealthy elite leaving the majority of the population without access. As some opponents of wage labour take influence from Marxist propositions, many are opposed to private property, but maintain respect for personal property.		Similarly, many scholars within feminist economics critique the emphasis on wage labor, saying that most men's work is paid while most women's work is unpaid, as shown by comprehensive surveys.[3][4] Scholars in this field argue that much of this unpaid women's work supports the production of economic value and the reproduction and production of social existence.[5][6]		A point of criticism is that after people have been compelled by economic necessity to no feasible alternative than that of wage labour, exploitation occurs; thus the claim that wage labour is "voluntary" on the part of the labourer is considered a red herring as the relationship is only entered into due to systemic coercion brought about by the inequality of bargaining power between labour and capital as classes.		Wage labour has long been compared to slavery by socialists.[7][8][9][10] As a result, the term 'wage slavery' is often utilised as a pejorative for wage labour.[11] Similarly, advocates of slavery looked upon the "comparative evils of Slave Society and of Free Society, of slavery to human Masters and slavery to Capital,"[12] and proceeded to argue persuasively that wage slavery was actually worse than chattel slavery.[13] Slavery apologists like George Fitzhugh contended that workers only accepted wage labour with the passage of time, as they became "familiarized and inattentive to the infected social atmosphere they continually inhale[d]."[12]		According to Noam Chomsky, analysis of the psychological implications of wage slavery goes back to the Enlightenment era. In his 1791 book On the Limits of State Action, classical liberal thinker Wilhelm von Humboldt explained how "whatever does not spring from a man's free choice, or is only the result of instruction and guidance, does not enter into his very nature; he does not perform it with truly human energies, but merely with mechanical exactness" and so when the labourer works under external control, "we may admire what he does, but we despise what he is."[15] Both the Milgram and Stanford experiments have been found useful in the psychological study of wage-based workplace relations.[16] Additionally, as per anthropologist David Graeber, the earliest wage labour contracts we know about were in fact contracts for the rental of chattel slaves (usually the owner would receive a share of the money, and the slave, another, with which to maintain his or her living expenses.) Such arrangements, according to Graeber, were quite common in New World slavery as well, whether in the United States or Brazil.[17] C. L. R. James argued in The Black Jacobins that most of the techniques of human organisation employed on factory workers during the industrial revolution were first developed on slave plantations.[18]		For Marxists, labour-as-commodity, which is how they regard wage labour,[19] provides a fundamental point of attack against capitalism.[20] "It can be persuasively argued," noted one concerned philosopher, "that the conception of the worker's labour as a commodity confirms Marx's stigmatisation of the wage system of private capitalism as 'wage-slavery;' that is, as an instrument of the capitalist's for reducing the worker's condition to that of a slave, if not below it."[21] That this objection is fundamental follows immediately from Marx's conclusion that wage labour is the very foundation of capitalism: "Without a class dependent on wages, the moment individuals confront each other as free persons, there can be no production of surplus value; without the production of surplus-value there can be no capitalist production, and hence no capital and no capitalist!"[22]		Articles		Books		
Mentorship is a relationship in which a more experienced or more knowledgeable person helps to guide a less experienced or less knowledgeable person. The mentor may be older or younger than the person being mentored, but he or she must have a certain area of expertise. It is a learning and development partnership between someone with vast experience and someone who wants to learn.[1] Mentorship experience and relationship structure affect the "amount of psychosocial support, career guidance, role modeling, and communication that occurs in the mentoring relationships in which the protégés and mentors engaged."[2]		The person in receipt of mentorship may be referred to as a protégé (male), a protégée (female), an apprentice or, in the 2000s, a mentee. The mentor may be referred to as a godfather/godmother[3][4] or a rabbi.[5][6]		"Mentoring" is a process that always involves communication and is relationship-based, but its precise definition is elusive,[7] with more than 50 definitions currently in use.[8] One definition of the many that have been proposed, is		Mentoring is a process for the informal transmission of knowledge, social capital, and the psychosocial support perceived by the recipient as relevant to work, career, or professional development; mentoring entails informal communication, usually face-to-face and during a sustained period of time, between a person who is perceived to have greater relevant knowledge, wisdom, or experience (the mentor) and a person who is perceived to have less (the protégé)".[9]		Mentoring in Europe has existed since at least Ancient Greek times. Since the 1970s it has spread in the United States mainly in training contexts,[10] with important historical links to the movement advancing workplace equity for women and minorities,[11] and it has been described as "an innovation in American management".[12]						The roots of the practice are lost in antiquity. The word itself was inspired by the character of Mentor in Homer's Odyssey. Though the actual Mentor in the story is a somewhat ineffective old man, the goddess Athena takes on his appearance in order to guide young Telemachus in his time of difficulty.		Historically significant systems of mentorship include the guru–disciple tradition practiced in Hinduism and Buddhism, Elders, the discipleship system practiced by Rabbinical Judaism and the Christian church, and apprenticing under the medieval guild system.		In the United States, advocates for workplace equity in the second half of the twentieth century popularized the term "mentor" and concept of career mentorship as part of a larger social capital lexicon—which also includes terms such as glass ceiling, bamboo ceiling,[14] networking, role model, and gatekeeper—serving to identify and address the problems barring non-dominant groups from professional success. Mainstream business literature subsequently adopted the terms and concepts, promoting them as pathways to success for all career climbers. In 1970 these terms were not in the general American vocabulary; by the mid-1990s they had become part of everyday speech.[11]		The European Mentoring and Coaching Council, also called the EMCC, is the leading global body in terms of creating and maintaining a range of industry standard frameworks, rules and processes across the mentoring and related supervision and coaching fields e.g. a code of practice for those practising mentoring.[15][16][17]		The focus of mentoring is to develop the whole person and so the techniques are broad and require wisdom in order to be used appropriately.[18] A 1995 study of mentoring techniques most commonly used in business[19] found that the five most commonly used techniques among mentors were:		Different techniques may be used by mentors according to the situation and the mindset of the mentee, and the techniques used in modern organizations can be found in ancient education systems, from the Socratic technique of harvesting to the accompaniment method of learning used in the apprenticeship of itinerant cathedral builders during the Middle Ages.[19] Leadership authors Jim Kouzes and Barry Z. Posner[20] advise mentors to look for "teachable moments" in order to "expand or realize the potentialities of the people in the organizations they lead" and underline that personal credibility is as essential to quality mentoring as skill.		Multiple mentors: A new and upcoming trend is having multiple mentors.[21] This can be helpful because we can all learn from each other. Having more than one mentor will widen the knowledge of the person being mentored. There are different mentors who may have different strengths.		Profession or trade mentor: This is someone who is currently in the trade/profession you are entering. They know the trends, important changes and new practices that you should know to stay at the top of your career. A mentor like this would be someone you can discuss ideas regarding the field, and also be introduced to key and important people that you should know.		Industry mentor: This is someone who doesn't just focus on the profession. This mentor will be able to give insight on the industry as a whole. Whether it be research, development or key changes in the industry, you need to know.		Organization mentor: Politics in the organizations are constantly changing. It is important to be knowledgeable about the values, strategies and products that are within your company, but also when these things are changing. An organization mentor can clarify missions and strategies, and give clarity when needed.		Work process mentor: This mentor can speed quickly over the bumps, and cut through the unnecessary work. This mentor can explain the 'ins and outs' of projects, day to day tasks, and eliminate unnecessary things that may be currently going on in your work day. This mentor can help to get things done quickly and efficiently.		Technology mentor: This is an up-and-coming, incredibly important position. Technology has been rapidly improving, and becoming more a part of day to day transactions within companies. In order to perform your best, you must know how to get things done on the newest technology. A technology mentor will help with technical breakdowns, advise on systems that may work better than what you're currently using, and coach you through new technology and how to best use it and implement it into your daily life.		These mentors are only examples. There can be many more different types of mentors. Look around your workplace, your life, and see who is an expert that you can learn something from.[1]		There are two broad types of mentoring relationships: formal and informal. Formal mentoring relationships are set up by an administrative unit or office in a company or organization, which solicits and recruits qualified individuals who are willing to mentor, provides training to the mentors, and then helps to match the mentors up with a person in need of mentoring. While formal mentoring systems contain numerous structural and guidance elements, they still typically allow the mentor and mentee to have an active role in choosing who they want to work with. Formal mentoring programs which simply assign mentors to mentees without giving these individuals a say have not performed well. Even though a mentor and a mentee may seem perfectly matched "on paper", in practice, they may have different working or learning styles. As such, giving the mentor and the mentee the opportunity to help select who they want to work with is a widely used approach. Informal mentoring occurs without the use of structured recruitment, mentor training and matching services. Informal mentoring arrangements can develop naturally from business networking situations in which a more experienced individual meets a new employee, and the two strike up a rapport.		In addition to these broad types, there are also peer, situational and supervisory mentoring relationships. These tend to fall under the categories of formal and informal mentoring relationships. Informal relationships develop on their own between partners. Formal mentoring, on the other hand, refers to a structured process supported by the organization and addressed to target populations. Youth mentoring programs assist at-risk children or youth who lack role models and sponsors. In business, formal mentoring is part of talent management strategies which are used to groom key employees, newly hired graduates, high potential-employees and future leaders. The matching of mentor and mentee is often done by a mentoring coordinator, often with the help of a computerized database registry. The use of the database helps to match up mentees with mentors who have the type of experience and qualifications they are seeking.		There are formal mentoring programs that are values-oriented, while social mentoring and other types focus specifically on career development. Some mentorship programs provide both social and vocational support. In well-designed formal mentoring programs, there are program goals, schedules, training (for both mentors and protégés), and evaluation. In 2004 Metizo created the first mentoring certification for companies and business schools in order to guarantee the integrity and effectiveness of formal mentoring. Certification is attributed jointly by the organization and an external expert.[22]		There are many kinds of mentoring relationships from school or community-based relationships to e-mentoring relationships. These mentoring relationships vary and can be influenced by the type of mentoring relationship that is in effect. That is whether it has come about as a formal or informal relationship. Also there are several models have been used to describe and examine the sub-relationships that can emerge. For example, Buell describes how mentoring relationships can develop under a cloning model, nurturing model, friendship model and apprenticeship model. The cloning model is about the mentor trying to "produce a duplicate copy of him or her self." The nurturing model takes more of a "parent figure, creating a safe, open environment in which mentee can both learn and try things for him-or herself." The friendship model are more peers "rather than being involved in a hierarchical relationship." Lastly, the apprenticeship is about less "personal or social aspects... and the professional relationship is the sole focus".[23]		In the sub-groups of formal and informal mentoring relationships: peer mentoring relationships are relationships where individuals are at the same skill training, similar positions and stages of career. However, one person may be more knowledgeable in a certain aspect or another, but they can help each other to progress in their work. A lot of time, peer relationships provide a lot of support, empathy and advice because the situations are quite similar.		Situational mentoring: Short-term relationships in which a person mentors for a specific purpose. This could be a company bringing an expert in regarding social media, or internet safety. This expert can mentor employees to make them more knowledgeable about a specific topic or skill.		Supervisory mentoring: This kind of mentoring has'go to' people who are supervisors. These are people who have answers to many questions, and can advise to take the best plan of action. This can be a conflict of interest relationship because many supervisors do not feel comfortable also being a mentor.[24]		Mentoring circles: Participants from all levels of the organization propose and own a topic. They then meet in groups to discuss the topic, which motivates them to grow and become more knowledgeable. Flash mentoring is ideal for job shadowing, reverse mentoring, and more.		Flash mentoring: Creates a low-pressure environment for mentoring that focuses on single meetings rather than a traditional, long-term mentoring relationship.[25]		Meta-analysis of 112 individual research studies found mentoring has significant behavioral, attitudinal, health-related, relational, motivational, and career benefits.[26] Especially in the workplace, there are many benefits to developing a mentorship program for new and current employees.		Career development: Setting up a career development mentoring program for employees enables an organization to help junior employees to learn the skills and behaviours from senior employees that the junior employees need to advance to higher-responsibility positions. This type of mentoring program can help to align organizational goals with employees' personal career goals (of progressing within the organization). It gives employees the ability to advance professionally and learn more about their work. This collaboration also gives employees a feeling of engagement with the organization, which can lead to better retention rates and increased employee satisfaction.		High potential mentoring: The most talented employees in organizations tend to be difficult to retain, as they are usually seeking greater challenges and responsibilities, and they are likely to leave for a different organization if they do not feel that they are being given the opportunity to develop. Top talent, whether in an innovation or management role, have incredible potential to make great things happen for an organization. Creating a mentoring program for high-potential employees that gives them one-on-one guidance from senior leaders can help to build the engagement of these talented employees, give them the opportunity to develop, and increase their retention in the organization.		Diversity mentoring: One of the top ways to innovate is by bringing in new ideas from senior employees and leaders from underrepresented groups (e.g., women, ethnic minorities, etc.). Who is an underrepresented group depends on the industry sector and country. In many Western countries, women and ethnic minorities are significantly underrepresented in executive positions and boards of directors. In some traditionally gender segregated occupations, such as education and nursing, however, women may be the dominant gender in the workforce. Mentors from underrepresented groups can empower employees from underrepresented groups to increase their confidence to take on higher-responsibility tasks and prepare for leadership roles. By developing employees from diverse groups, this can give the organization access to new ideas, new ways of looking at problems, and new perspectives. This also brings cultural awareness and intercultural dialogue into the workplace.		Reverse mentoring: While mentoring typically involves a more experienced, typically older employee or leader providing guidance to a younger employee, the opposite approach can also be used. In the 2000s, with the rise of digital innovations, Internet applications and social media, in some cases, new, young employees are more familiar with these technologies than senior employees in the organizations. The younger generations can help the older generations to expand and grow towards current trends. Everyone has something to bring to the table, this creates a "two way street" within companies where younger employees can see the larger picture, and senior employees can learn from young employees.		Knowledge transfer mentoring: Employees must have a certain set of skills in order to accomplish the tasks at hand. Mentoring is a great approach to help employees get organized, and give them access to an expert that can give feedback, and help answer questions that they may not know where to find answers to.[27]		Mentorship provides critical benefits to individuals as well as organizations. Although mentorship can be important for an individual's career advancement, in the United States it historically has been most apparent in relation to the advancement of women and minorities in the workplace. Until recent decades, American men in dominant ethnic groups gained most of the benefits of mentorship without consciously identifying it as an advancement strategy. American women and minorities, in contrast, more pointedly identified and pursued mentorship in the second half of the twentieth century as they sought to achieve the professional success they had long been denied.[11]		In a 1958 study, Margaret Cussler showed that, for each female executive she interviewed who did not own her own company, "something—or someone—gave her a push up the ladder while others halted on a lower rung." Cussler concluded that the relationship between the "sponsor and protégé" (the vocabulary of "mentorship" was not yet in common use) was the "magic formula" for success.[28] By the late 1970s, numerous publications had established the centrality of mentorship to business success for everyone and particularly for women trying to break into the male-dominated business world. These publications noted the many specific benefits provided by mentorship, which included insider information, education, guidance, moral support, inspiration, sponsorship, an example to follow, protection, promotion, the ability to "bypass the hierarchy," the projection of the superior's "reflected power," access to otherwise invisible opportunities, and tutelage in corporate politics.[11]		This literature also showed the value of these benefits. A Harvard Business Review survey of 1,250 top executives published in 1979, for example, showed that most had been mentored or sponsored and that those who received such assistance reported higher income, a better education, a quicker path to achievement, and more job satisfaction than those who did not.[29] The literature particularly emphasized the necessity of mentoring for businesswomen's success.[11] For example, although women made up less than one percent of the executives in the Harvard Business Review survey, all of these women reported being mentored.[29] In subsequent decades, as mentoring became a widely valued phenomenon in the United States, women and minorities in particular continued to develop mentoring relationships consciously as they sought professional advancement.[11]		Research in the 1970s, partly in response to a study by Daniel Levinson,[30] led some women and African Americans to question whether the classic "white male" model was available or customary for people who are newcomers in traditionally white male organizations. In 1978 Edgar Schein described multiple roles for successful mentors.[31][clarification needed]		Two of Schein's students, Davis and Garrison, undertook to study successful leaders of both genders and at least two races. Their research presented evidence for the roles of: cheerleader, coach, confidant, counsellor, developer of talent, "griot" (oral historian for the organization or profession), guardian, guru, inspiration, master, "opener of doors", patron, role model, pioneer, "seminal source", "successful leader", and teacher.[32] They described multiple mentoring practices which have since been given the name of "mosaic mentoring" to distinguish this kind of mentoring from the single mentor approach.		Mosaic mentoring is based on the concept that almost everyone can perform one or another function well for someone else — and also can learn along one of these lines from someone else. The model is seen as useful for people who are "non-traditional" in a traditional setting, such as people of color and women in a traditionally white male organization. The idea has been well received in medical education literature.[33] There are also mosaic mentoring programs in various faith-based organizations.[citation needed]		Corporate mentoring programs are used by mid-size to large organizations to further the development and retention of employees. Mentoring programs may be formal or informal and serve a variety of specific objectives including acclimation of new employees, skills development, employee retention and diversity enhancement.		Formal mentoring programs offer employees the opportunity to participate in an organized mentoring program. Participants join as a mentor, protégé or both by completing a mentoring profile. Mentoring profiles are completed as written forms on paper or computer or filled out via an online form as part of an online mentoring system. Protégés are matched with a mentor by a program administrator or a mentoring committee, or may self-select a mentor depending on the program format.		Informal mentoring takes places in organizations that develop a culture of mentoring but do not have formal mentoring in place. These companies may provide some tools and resources and encourage managers to accept mentoring requests from more junior members of the organization.[34]		A study of 1,162 employees found that "satisfaction with a mentoring relationship had a stronger impact on attitudes than the presence of a mentor, whether the relationship was formal or informal, or the design of a formal mentoring program."[35] So even when a mentoring relationship is established, the actual relationship is more important than the presence of a relationship.		New-hire mentoring programs are set up to help new employees acclimate more quickly into the organization. In new-hire mentoring programs, newcomers to the organization (protégés) are paired with more experienced people (mentors) in order to obtain information, good examples, and advice as they advance. It has been claimed that new employees who are paired with a mentor are twice as likely to remain in their job than those who do not receive mentorship.[36]		These mentoring relationships provide substance for career growth, and benefit both the mentor and the protégé. For example, the mentor gets to show leadership by giving back and perhaps being refreshed about their own work. The organization receives an employee that is being gradually introduced and shaped by the organization's culture and operation because they have been under the mentorship of an experienced member. The person being mentored networks, becomes integrated easier in an organization, gets experience and advice along the way.[37] It has been said that "joining a mentor's network and developing one's own is central to advancement" and this is possibly why those mentored tend to do well in their organizations.[37]		In the organizational setting, mentoring usually "requires unequal knowledge",[9] but the process of mentorship can differ. Bullis describes the mentoring process in the forms of phase models. Initially, the "mentee proves himself or herself worthy of the mentor's time and energy". Then cultivation occurs which includes the actual "coaching...a strong interpersonal bond between mentor and mentee develops". Next, under the phase of separation "the mentee experiences more autonomy". Ultimately, there is more of equality in the relationship, termed by Bullis as Redefinition.[38]		High-potential mentoring programs are used to groom up-and-coming employees deemed to have the potential to move up into leadership or executive roles. Here the employee (protégé) is paired with a senior-level leader (or leaders) for a series of career-coaching interactions. These programs tend to be smaller than more general mentoring programs and mentees must be selected based on a list of eligibility criteria to participate. Another method of high-potential mentoring is to place the employee in a series of jobs in disparate areas of an organization (e.g., human resources, sales, operations management, etc.) all for short periods of time, so they can learn in a "hands-on", practical fashion, about the organization's structure, culture, and methods.		Mentees are matched with mentors by a designated mentoring committee or mentoring administrator usually consisting of senior members of the training, learning and development group and/or the human resources departments. The matching committee reviews the mentors' profiles and the coaching goals sought out by the mentees and makes matches based on areas for development, mentor strengths, overall experience, skill set, location and objectives.		Mentoring technology, typically based on computer software, can be used to facilitate matches allowing mentees to search and select a mentor based on their own development and coaching needs and interests. This mentee-driven methodology increases the speed in which matches are created and reduces the amount of administrative time required to manage the program.[39] The quality of matches increases as well with self-match programs because the greater the involvement of the mentee in the selection of their mentor, the better the outcome of the mentorship.[40] There are a variety of online mentoring technology programs available that can be utilized to facilitate this mentee-driven matching process.		Speed mentoring follows some of the procedures of speed dating. Mentors and mentees are introduced to each other in short sessions, allowing each person to meet multiple potential matches in a very short timeframe. Speed mentoring occur as a one-time event in order for people "to meet potential mentors to see if there is a fit for a longer term engagement."[41]		In many secondary and post-secondary schools, mentorship programs are offered to support students in program completion, confidence building and transitioning to further education or the workforce. There are also peer mentoring programs designed specifically to bring under-represented populations into science and engineering.[citation needed] The Internet has brought university alumni closer to graduating students. Graduate university alumni are engaging with current students in career mentorship through interview questions and answers. The students with the best answers receive professional recommendations from industry experts build a more credible CV.		Instructional coaches are former teachers or principals that have shown effectiveness in their work of teaching or leading and go through additional training to learn more about the technical skills needed to be an effective coach.[42] In her book The Art of Coaching, Elena Aguilar recommends that a coach "must have been an effective teacher for at least five years."[42] Though skills that were effective in the classroom are a must, the coach must also be confident in working with adults, bringing strong listening, communication, and data analysis skills to the coaching position.[42] Ultimately, an instructional coach is a former teacher who was successful in the classroom and is respected in the field, with the respect carrying over into this new position.[43]		Coaches seek to work one-on-one with teachers or in a small group setting with teachers to build student achievement in the classroom based on data collected and discussed by both teacher or coach.[43] According to Melinda Mangin and KaiLonnie Dunsmore, instructional coaching models may include: "cognitive coaching, clinical supervision, peer coaching and mentoring, formal literacy coaching, informal coaching, or a mixed model.[44] Other researchers have described categories of coaching such as data-oriented, student-oriented, managerial, and coaches who work with individual teachers or with groups of teachers".[45][46] Ultimately, coaching roles are designed to increase teacher capacity and push teacher improvement through learning opportunities.[46] The practice of instructional coaching is embedded within the work of a teacher, not in isolation of their everyday teaching. In other words, the coach works with the teacher throughout the school year and meets during the school day with the teacher regarding current lessons, planning, and the observations/data collected. The discussions between the instructional coach and teacher are built upon mutual respect and a trusting relationship through confidentiality.[43] Overall, instructional coaching is meant to serve as professional development for the teacher(s).[43]		A coach's main responsibility in this way is to change practice and build knowledge on "new instructional materials, programs, and initiatives" with the teacher.[46] This professional development can come through discussion, but also can come in other forms. Instructional coaches can model lessons and instructional strategies in the teachers' classroom to show examples and have teachers feel more confident in using these strategies.[47] Teacher observations is one of the most powerful ways that coaches can put data for change in front of teachers. Coaches doing observations and collecting data to debrief with teachers helps paint a picture for teacher improvement.[47]		According to a three-year research study done by the Pennsylvania Institute for Instructional Coaching, there was increase in student success when instructional coaching was used in the classroom. This, however, could not be viewed as solely "instructional coaching" in isolation of other factors.[48] The coaching "model emphasize[d] the simultaneous use of four strategies: one-on-one teacher engagement; evidence-based literacy practices applied across the curriculum; data analytics; and reflection on practice."[48] Yet, teachers have shared that:		In addition to this, "the most effective professional development model is thought to involve follow-up activities, usually in the form of long-term support, coaching in teachers' classrooms, or ongoing interaction with colleagues."[49] In most cases, instructional coaching can provide this support and meet this definition of effective professional development.		There should also be support from administration around the instructional coaching to align the work of the coach and teacher with the school's mission or vision.[42] Knight focuses on the partnership with the principal being at the core of successful coaching. Knight explains that the principal and the instructional coach need to be aligned in their goals for the coaching occurring.[47] If they have differing desired outcomes for teaching, then the teacher will be receiving mixed messages and caught between improvement and a standstill.[42] Aguilar suggests that coaches continually ask about the school's goals as well as action steps to meet these and bring into daily coaching.[42]		In conjunction with this partnership and observations, Knight's belief of data usage is critical for teacher improvement during coaching sessions. Knight shares how giving opinions and telling a teacher how to improve stops the learning for the teacher and instead creates a barrier between the coach and teacher and makes the teacher expect hand-holding. Instead, the data needs to tell a story for the teacher to determine moves to try to improve. This allows ownership for the teacher as well as understanding of their work in conjunction with the work.[47]		The relationships and trust between the coach and coachee are a critical component of coaching.[42][47] A coach having specific content knowledge and respect in a teacher's field of teaching would help build trust. Another way to build this trust is through confidentiality. By keeping all conversations confidential and sticking to that, the coachee knows that your word is good. In addition to relationship building, it is important to let the coachee feel comfortable talking to you about anything—there may need to be the time when a crisis they are facing trumps conversation about the lesson.[42] Starting a coaching conversation about how life is going for a coachee is also important to relationship building.		According to Nelson and Sassi, "knowledge of pedagogical process and content knowledge must be fused" in both understanding teaching and observing teaching.[50] For example, an instructional coach that is working with a math teacher should know "current mathematics education reform efforts are built on the notion that the ideas in a subject, and the ways in which students and teachers work with the ideas, matter."[50][51] It seems clear that a deep pedagogical knowledge as well as deep content specific knowledge are required for the teacher to have confidence in the coach and for the coach to be able to step in and assume the role of the teacher.		Knowledge that coaches need to be effective span just content and pedagogical knowledge. Aguilar uses the ladder of inference to allow coaches to evaluate their own thoughts, and ultimately use this ladder to help principals and teachers evaluate their own beliefs before jumping to assumptions. Aguilar states that her "list of beliefs has changed over the years. You can change yours, too. The point is to be mindful of the beliefs from which we're working and to notice the effect of working from those beliefs." Beliefs can change about approaches to teaching, classroom management, or even content knowledge.[42]		The blended mentoring is a mix of on-site and online events, projected to give to career counselling and development services the opportunity to adopt mentoring in their ordinary practice.		In the reverse mentoring situation, the mentee has less overall experience (typically as a result of age) than the mentor (who is typically older), but the mentee has more knowledge in a particular area, and as such, reverses the typical constellation. Examples are when young internet or mobile savvy millennial generation teens train executives in using their high end smartphones. They in turn sometimes offer insight in business processes.		The concept of mentoring has entered the business domain as well. This is different from being an apprentice; a business mentor provides guidance to a business owner or an entrepreneur on the entrepreneur's business.[citation needed] An apprentice learns a trade by working on the job with the "employer".		A 2012 literature review by EPS-PEAKS investigated the practice of business mentoring, with a focus on the Middle-East and North Africa region.[52] The review found strong evidence to suggest that business mentoring can have real benefits for entrepreneurs, but highlights some key factors that need to be taken into account when designing mentoring programmes for this to be the case, such as the need to balance a formal and informal approach and to appropriately match mentors and mentees.		
Unemployment benefits (depending on the jurisdiction also called unemployment insurance or unemployment compensation) are payments made by the state or other authorized bodies to unemployed people. In the United States, benefits are funded by a compulsory governmental insurance system, not taxes on individual citizens. Depending on the jurisdiction and the status of the person, those sums may be small, covering only basic needs, or may compensate the lost time proportionally to the previous earned salary.		Unemployment benefits are generally given only to those registering as unemployed, and often on conditions ensuring that they seek work and do not currently have a job, and are validated as being laid off and not fired for cause in most states.		In some countries, a significant proportion of unemployment benefits are distributed by trade/labour unions, an arrangement known as the Ghent system.						The first unemployment benefit scheme was introduced in the United Kingdom with the National Insurance Act 1911 under the Liberal Party government of H. H. Asquith. The popular measures were to combat the increasing influence of the Labour Party among the country's working-class population. The Act gave the British working classes a contributory system of insurance against illness and unemployment. It only applied to wage earners, however, and their families and the unwaged had to rely on other sources of support, if any.[1] Key figures in the implementation of the Act included Robert Laurie Morant, and William Braithwaite.		By the time of its implementation, the benefit was criticized by communists, who thought such insurance would prevent workers from starting a revolution, while employers and tories saw it as a "necessary evil".[2]		The scheme was based on actuarial principles and it was funded by a fixed amount each from workers, employers, and taxpayers. It was restricted to particular industries, particularly more volatile ones like shipbuilding, and did not make provision for any dependants. After one week of unemployment, the worker was eligible for receiving 7 shillings/week for up to 15 weeks in a year. By 1913, 2.3 million were insured under the scheme for unemployment benefit.		The Unemployment Insurance Act 1920 created the dole system of payments for unemployed workers.[3] The dole system provided 39 weeks of unemployment benefits to over 11 million workers—practically the entire civilian working population except domestic service, farm workers, railroad men, and civil servants.		Unemployment benefits were introduced in Germany in 1927, and in most European countries in the period after the Second World War with the expansion of the welfare state. Unemployment insurance in the United States originated in Wisconsin in 1932.[4] Through the Social Security Act of 1935, the federal government of the United States effectively encouraged the individual states to adopt unemployment insurance plans.		In Argentina, successive administrations have used a variety of passive and active labour market interventions to protect workers against the consequences of economic shocks and the government's key institutional response to combat the increase in poverty and unemployment created by the crisis was the launch of an active unemployment assistance programme called Plan Jefas y Jefes de Hogar Desocupados (Program for Unemployed Heads of Households).		In Australia, social security benefits, including unemployment benefits, are funded through the taxation system. There is no compulsory national unemployment insurance fund. Rather, benefits are funded in the annual Federal Budget by the National Treasury and are administrated and distributed throughout the nation by the government agency, Centrelink. Benefit rates are indexed to the Consumer Price Index and are adjusted twice a year according to inflation or deflation.		There are two types of payment available to those experiencing unemployment. The first, called Youth Allowance, is paid to young people aged 16–20 (or 15, if deemed to meet the criteria for being considered 'independent' by Centrelink). Youth Allowance is also paid to full-time students aged 16–24, and to full-time Australian Apprenticeship workers aged 16–24. People aged below 18 who have not completed their High School education, are usually required to be in full-time education, undertaking an apprenticeship or doing training to be eligible for Youth Allowance. For single people under 18 years of age living with a parent or parents the basic rate is A$91.60 per week. For over-18- to 20-year-olds living at home this increases to A$110.15 per week. For those aged 18–20 not living at home the rate is A$167.35 per week. There are special rates for those with partners and/or children.		The second kind of payment is called Newstart Allowance and is paid to unemployed people over the age of 21 and under the pension eligibility age. To receive a Newstart payment, recipients must be unemployed, be prepared to enter into an Employment Pathway Plan (previously called an Activity Agreement) by which they agree to undertake certain activities to increase their opportunities for employment, be Australian Residents and satisfy the income test (which limits weekly income to A$32 per week before benefits begin to reduce, until one's income reaches A$397.42 per week at which point no unemployment benefits are paid) and the assets test (an eligible recipient can have assets of up to A$161,500 if he or she owns a home before the allowance begins to reduce and $278,500 if he or she does do not own a home). The rate of Newstart allowance as at the 12th January 2010 for single people without children is A$228 per week, paid fortnightly. (This does not include supplemental payments such as Rent Assistance.) Different rates apply to people with partners and/or children.		The system in Australia is designed to support recipients no matter how long they have been unemployed. In recent years the former Coalition government under John Howard has increased the requirements of the Activity Agreement, providing for controversial schemes such as Work for the Dole, which requires that people on benefits for 6 months or longer work voluntarily for a community organisation regardless of whether such work increases their skills or job prospects. Since the Labor government under Kevin Rudd was elected in 2008, the length of unemployment before one is required to fulfill the requirements of the Activity Agreement (which has been renamed the Employment Pathway Plan) has increased from six to twelve months. There are other options available as alternatives to the Work for the Dole scheme, such as undertaking part-time work or study and training, the basic premise of the Employment Pathway Plan being to keep the welfare recipient active and involved in seeking full-time work.		For people renting their accommodation, unemployment benefits are supplemented by Rent Assistance, which, for single people as at 29 June 2012, begins to be paid when weekly rent is more than A$53.40. Rent Assistance is paid as a proportion of total rent paid (75 cents per dollar paid over $53.40 up to the maximum). The maximum amount of rent assistance payable is A$60.10 per week, and is paid when the total weekly rent exceeds A$133.54 per week. Different rates apply to people with partners and/or children, or who are sharing accommodation.		External links		In Canada, the system now known as Employment Insurance was formerly called Unemployment Insurance. The name was changed in 1996, in order to alleviate perceived negative connotations. In 2015, Canadian workers pay premiums of 1.88%[5] of insured earnings in return for benefits if they lose their jobs.		The Employment and Social Insurance Act was passed in 1935 during the Great Depression by the government of R.B. Bennett as an attempted Canadian unemployment insurance programme. It was, however, ruled unconstitutional by the Supreme Court of Canada as unemployment was judged to be an insurance matter falling under provincial responsibility. After a constitutional amendment was agreed to by all of the provinces, a reference to "Unemployment Insurance" was added to the matters falling under federal authority under the Constitution Act, 1867, and the first Canadian system was adopted in 1940. Because of these problems Canada was the last major Western country to bring in an employment insurance system. It was extended dramatically by Pierre Trudeau in 1971 making it much easier to get. The system was sometimes called the 10/42, because one had to work for 10 weeks to get benefits for the other 42 weeks of the year. It was also in 1971 that the UI program was first opened up to maternity and sickness benefits, for 15 weeks in each case.		The generosity of the Canadian UI programme was progressively reduced after the adoption of the 1971 UI Act. At the same time, the federal government gradually reduced its financial contribution, eliminating it entirely by 1990. The EI system was again cut by the Progressive Conservatives in 1990 and 1993, then by the Liberals in 1994 and 1996. Amendments made it harder to qualify by increasing the time needed to be worked, although seasonal claimants (who work long hours over short periods) turned out to gain from the replacement, in 1996, of weeks by hours to qualify. The ratio of beneficiaries to unemployed, after having stood at around 40 percent for many years, rose somewhat during the 2009 recession but then fell back again to the low 40s.[6] Some unemployed persons are not covered for benefits (e.g. self-employed workers), while others may have exhausted their benefits, did not work long enough to qualify, or quit or were dismissed from their job. The length of time one could take EI has also been cut repeatedly. The 1994 and 1996 changes contributed to a sharp fall in Liberal support in the Atlantic provinces in the 1997 election.		In 2001, the federal government increased parental leave from 10 to 35 weeks, which was added to preexisting maternity benefits of 15 weeks. In 2004, it allowed workers to take EI for compassionate care leave while caring for a dying relative, although the strict conditions imposed make this a little used benefit. In 2006, the Province of Quebec opted out of the federal EI scheme in respect of maternity, parental and adoption benefits, in order to provide more generous benefits for all workers in that province, including self-employed workers. Total EI spending was $19.677 billion for 2011-2012 (figures in Canadian dollars).[7]		Employers contribute 1.4 times the amount of employee premiums. Since 1990, there is no government contribution to this fund. The amount a person receives and how long they can stay on EI varies with their previous salary, how long they were working, and the unemployment rate in their area. The EI system is managed by Service Canada, a service delivery network reporting to the Minister of Employment and Social Development Canada.		A bit over half of EI benefits are paid in Ontario and the Western provinces but EI is especially important in the Atlantic provinces, which have higher rates of unemployment. Many Atlantic workers are also employed in seasonal work such as fishing, forestry or tourism and go on EI over the winter when there is no work. There are special rules for fishermen making it easier for them to collect EI. EI also pays for maternity and parental leave, compassionate care leave, and illness coverage. The programme also pays for retraining programmes (EI Part II) through labour market agreements with the Canadian provinces.		A significant part of the federal fiscal surplus of the Jean Chrétien and Paul Martin years came from the EI system. Premiums were reduced much less than falling expenditures - producing, from 1994 onwards, EI surpluses of several billion dollars per year, which were added to general government revenue.[8] The cumulative EI surplus stood at $57 billion at March 31, 2008,[9] nearly four times the amount needed to cover the extra costs paid during a recession.[10] This drew criticism from Opposition parties and from business and labour groups, and has remained a recurring issue of the public debate. The Conservative Party,[11] chose not to recognize those EI surpluses after being elected in 2006. Instead, the Conservative government cancelled the EI surpluses entirely in 2010, and required EI contributors to make up the 2009, 2010 and 2011 annual deficits by increasing EI premiums. On December 11, 2008, the Supreme Court of Canada rejected a court challenge launched against the federal government by two Quebec unions, who argued that EI funds had been misappropriated by the government.[12]		External links		The level of benefit is set between the minimum wage and the minimum living allowance by individual provinces, autonomous regions and municipalities.[citation needed]		Each Member State of the European Union has its own system and in general a worker should claim unemployment benefits in the country where they last worked. For a person working in a country other than their country of residency (a cross-border worker), they will have to claim benefits in their country of residence. More specific rules and information is available on the Your Europe portal.		Two systems run in parallel, combining a Ghent system and a minimum level of support provided by Kela, an agency of the national government. Unionization rates are high (70%), and union membership comes with membership in an unemployment fund. Additionally, there are non-union unemployment funds. Usually benefits require 26 weeks of 18 hours per week on average, and the unemployment benefit is 60% of the salary and lasts for 500 days.[13] When this is not available, Kela can pay either regular unemployment benefit or labor market subsidy benefits. The former requires a degree and two years of full-time work. The latter requires participation in training, education, or other employment support, which may be mandated on pain of losing the benefit, but may be paid after the regular benefits have been either maxed out or not available.[14] Although the unemployment funds handle the payments, most of the funding is from taxes and compulsory tax-like unemployment insurance charges.		Regardless of whether benefits are paid by Kela or from an unemployment fund, the unemployed person receives assistance from the Työ- ja elinkeinokeskus (TE-keskus, or the "Work and Livelihood Centre"), a government agency which helps people to find jobs and employers to find workers. In order to be considered unemployed, the seeker must register at the TE-keskus as unemployed. If the jobseeker does not have degree, the agency can require the jobseeker to apply to a school.		If the individual does not qualify for any unemployment benefit he may still be eligible for the housing benefit (asumistuki) from Kela and municipal social welfare provisions (toimeentulotuki). They are not unemployment benefits and depend on household income, but they have in practice become the basic income of many long-term unemployed.		France uses a quasi Ghent system, under which unemployment benefits are distributed by an independent agency (UNEDIC) in which unions and Employer organisations are equally represented. UNEDIC is responsible for 3 benefits: ARE, ACA and ASR The main ARE scheme requires a minimum of 122 days membership in the preceding 24 months and certain other requirements before any claims can be made. Employers pay a contribution on top of the pre-tax income of their employees, which together with the employee contribution, fund the scheme.		The maximum unemployment benefit is (as of March 2009) 57.4% of EUR 162 per day (Social security contributions ceiling in 2011), or 6900 euros per month.[15] Claimants receive 57,4% of their average daily salary of the last 12 months preceding unemployment with the average amount being 1,111 euros per month.[16] In France tax and other payroll taxes are paid on unemployment benefits. In 2011 claimants received the allowance for an average 291 days.		Germany has two different types of unemployment benefits.		The unemployment benefit I in Germany is also known as the unemployment insurance. The insurance is administered by the federal employment agency and funded by employee and employer contributions. This in stark contrast to FUTA in the US and other systems; where only employers make contributions. Participation (and thus contributions) are generally mandatory for both employee and employer.		All workers with a regular employment contract (abhängig Beschäftigte), except freelancers and certain civil servants (Beamte), contribute to the system. Since 2006, certain previously excluded workers have been able to opt into the system on a voluntary basis.		The system is financed by contributions from employees and employers. Employees pay 1.5% of their gross salary below the social security threshold and employers pay 1.5% contribution on top of the salary paid to the employee. The contribution level was reduced from 3.25% for employees and employers as part of labour market reforms known as Hartz. Contributions are paid only on earnings up to the social security ceiling (2012: 5,600 EUR).		The system is largely self-financed but also receives a subsidy from the state to run the Jobcenters.		Unemployed workers are entitled to:		Unemployed benefit is paid to workers who have contributed at least during 12 months preceding their loss of a job. The allowance is paid for half of the period that the worker has contributed. Claimants get 60% of their previous net salary (capped at the social security ceiling), or 67% for claimants with children. The maximum benefit is therefore 2964 Euros (in 2012). In 2011 the federal Work Agency had revenues and expenses of 37.5 bn EUR[17]		After a change in German law effective since 2008, provided their job history qualifies them, benefit recipients aged 50 to 54 now receive an unemployment benefit for 15 months, those 55 to 57 for 18 months and those 58 or older receive benefits for 24 months. For those under the age of 50 who have not been employed for more than 30 months in a job which paid into the social security scheme, full unemployment benefit can be received for a maximum period of 12 months. Note how the duration of eligibility is variegated in Germany to account for the difficulty older people have re-entering the job market.		If a worker is not eligible for the full unemployment benefits or after receiving the full unemployment benefit for the maximum of 12 months, he is able to apply for benefits from the so-called Arbeitslosengeld II (Hartz IV) programme, an open-ended welfare programme which, unlike the US system, ensures people do not fall into penury. A person receiving Hartz IV benefits is paid 404 EUR (2016) a month for living expenses plus the cost of adequate housing (including heating) and health care. Couples can receive benefits for each partner including their children. Additionally, children can get "benefits for education and participation". Germany does not have an EBT (electronic benefits transfer) card system in place and, instead, disburses welfare in cash or via direct deposit onto the recipient's bank account. People who receive Hartz 4 are obligated to seek for jobs and can be forced to take part in social programs or Mini jobs in order to receive this Hartz 4 money. Most of these programs and Mini jobs are lasting like a full time job each day, 5 days a week to simulate real jobs.		Unemployment benefits in Greece are administered through ΟΑΕΔ (Greek: Οργανισμός Απασχόλησης Εργατικού Δυναμικού, Manpower Employment Organization) and are available only to laid-off salaried workers with full employment and social security payments during the previous two years. The self-employed do not qualify, and neither do those with other sources of income. The monthly benefit is fixed at the "55% of 25 minimum daily wages", and is currently 360 euros per month,[18][19] with a 10% increase for each under-age child. Recipients are eligible for at most twelve months; the exact duration depends on the collected number of ensema ένσημα, that is social security payment coupons-stamps collected (i.e. days of work) during the 14 months before being laid off; the minimum number of such coupons, under which there is no eligibility, is 125, collected in the first 12 of the 14 aforementioned months. Eligibility since January 1, 2013, has been further constrained in that one applying for unemployment benefits for a second or more time, must not have received more than the equivalent of 450 days of such benefits during the last four years since the last time one had started receiving such benefits; if one has received unemployment benefits in this period for more than 450 days then there is no eligibility while if one has received less, then one is only eligible for at most the remaining days up until the maximum of 450 days is reached.[19]		People aged 18 and over and who are unemployed in Ireland can apply for either the Jobseeker's Allowance (Liúntas do Lucht Cuardaigh Fostaíochta) or the Jobseeker's Benefit (Sochar do Lucht Cuardaigh Fostaíochta). Both are paid by the Department of Social Protection and are nicknamed "the dole".		Unemployment benefit in Ireland can be claimed indefinitely for as long as the individual remains unemployed. The standard payment is €188 per week for those aged 26 and over. For those aged 18 to 24 the rate is €100 per week. For those aged 25 the weekly rate is €144. Payments can be increased if the unemployed has dependents. For each adult dependent, another €124.80 is added, €100 if the recipient (as opposed to the dependent) is aged 18 to 24, and for each child dependent €29.80 is added.		There are more benefits available to unemployed people, usually on a special or specific basis. Benefits include the Rent Supplement, the Mortgage Interest Supplement and the Fuel Allowance, among others. People on a low income (which includes those on JA/JB) are entitled to a Medical Card (although this must be applied for separately from the Health Service Executive) which provides free health care, optical care, limited dental care, aural care and subsidised prescription drugs carrying a €2.50 per item charge to a maximum monthly contribution of €25 per household (as opposed to subsidised services like non medical-card holders).		To qualify for Jobseekers Allowance, claimants must satisfy the "Habitual Residence Condition": they must have been legally in the state (or the Common Travel Area) for two years or have another good reason (such as lived abroad and are returning to Ireland after become unemployed or deported). This condition does not apply to Jobseekers Benefit (which is based on Social Insurance payments).		More information on each benefit can be found here:		In Israel, unemployment benefits are paid by Bituah Leumi (the National Insurance Institute), to which workers must pay contributions. Eligible workers must immediately register with the Employment Service Bureau upon losing their jobs or jeaopordize their eligibility, and the unemployment period is considered to start upon registration with the Employment Service Bureau. To be eligible for unemployment benefits, an employee must have completed a "qualifying period" of work for which unemployment insurance contributions were paid, which varies between 300 and 360 days. Employees who were involuntarily terminated from their jobs or who terminated their own employment and can provide evidence of having done so for a justified reason are eligible for immediately receiving unemployment benefits, while those who are deemed to have terminated their employment of their own volition with no justified reason will only begin receiving unemployment benefits 90 days from the start of their unemployment period. Unemployment benefits are paid daily, with the amount calculated based on the employee's previous income over the past six months, but not exceeding the daily average wage for the first 125 days of payment and two-thirds of the daily average wage from the 126th day onwards. During the unemployment period, the Employment Service Bureau assists in helping locate suitable work and job training, and regularly reporting to the Employment Service Bureau is a condition for continuing to receive unemployment benefits. A person who was offered suitable work or training by the Employment Service Bureau but refused will only receive unemployment benefits 90 days after the date of the refusal, and 30 days' worth of unemployment benefits will be deducted for each subsequent refusal.[20][21][22]		Unemployment benefits in Italy consists mainly in cash transfers based on contributions (Assicurazione Sociale per l'Impiego, ASPI), up to the 75 percent of the previous wages for up to sixteen months. Other measures are:		In the Italian unemployment insurance system all the measures are income-related, and they have an average decommodification level. The basis for entitlement is always employment, with more specific conditions for each case, and the provider is quite always the state. An interesting feature worthy to be discussed is that the Italian system takes in consideration also the economic situation of the employers, and aims as well at relieving them from the costs of crisis.		Unemployment benefits in Japan are called "unemployment insurance" and are closer to the US or Canadian "user pays" system than the taxpayer funded systems in place in countries such as Britain, New Zealand, or Australia. It is paid for by contributions by both the employer and employee.[23]		On leaving a job, employees are supposed to be given a "Rishoku-hyo" document showing their ID number (the same number is supposed to be used by later employers), employment periods, and pay (which contributions are linked to). The reason for leaving is also documented separately. These items affect eligibility, timing, and amount of benefits.[24] The length of time that unemployed workers can receive benefits depends on the age of the employee, and how long they have been employed and paying in.[25]		It is supposed to be compulsory for most full-time employees.[26] If they have been enrolled for at least 6 months and are fired or made redundant, leave the company at the end of their contract, or their contract is non-renewed, the now-unemployed worker will receive unemployment insurance. If a worker quit of their own accord they may have to wait between 1 and 3 months before receiving any payment.		Mexico lacks a national unemployment insurance system, with the only such system being in Mexico City. Unemployed residents of Mexico City are eligible for unemployment benefits for up to 6 months. If a citizen resides elsewhere then all he/she must do is move to Mexico City and get a job for six months to qualify and be an employee.		Unemployment benefits in the Netherlands was introduced in 1949. Separate schemes exist for mainland Netherlands and for the Caribbean Netherlands.		The scheme in mainland Netherlands entails that, according to the Werkloosheidswet (Unemployment Law, WW), employers are responsible for paying the contributions to the scheme, which are deducted from the salary received by the employees. In 2012 the contribution was 4.55% of gross salary up to a ceiling of 4,172 euros per month. The first 1,435.75 euros of an employees gross salaries are not subject to the 4.55% contribution.		Benefits are paid for a maximum period of 38 months and claimants get 75% of last salary for 2 months and 70% thereafter with a maximum benefit of 3128 euros, depending on how long the claimant has been employed previously. Workers older than 50 years who are unemployed for over 2 months are entitled to a special benefit called the IOAW, if they do not receive the regular unemployment benefit (WW).		In New Zealand, Jobseeker Support, previously known as the Unemployment Benefit and also known as "the dole" provides income support for people who are looking for work or training for work. It is one of a number of benefits administered by Work and Income, a service of the Ministry of Social Development.		To get this benefit, a person must meet the conditions and obligations specified in section 88A to 123D Social Security Act 1964. These conditions and obligations cover things such as age, residency status, and availability to work.[27]		The amount that is paid depends on things such as the person's age, income, marital status and whether they have children. It is adjusted annually on 1 April and in response to changes in legislature. Some examples of the maximum after tax weekly rate at 1 April 2011 are:		More information about this benefit and the amounts paid are on the Work and Income website.[29]		External links		The Spanish unemployment benefits system is part of the Social security system of Spain. Benefits are managed by the State Public Employment Agency (SEPE).The basis for entitlement is having contributed for a minimum period during the time preceding unemployment, with further conditions that may be applicable. The system comprises contributory benefits and non-contributory benefits.		Contributory benefits are payable to those unemployed persons with a minimum of 12 months contributions over a period of 6 years preceding unemployment. The benefit is payable for 1/3 of the contribution period. The benefit amount is 70% of the legal reference salary plus additional amounts for persons with dependants. The benefit reduces to 60% of the reference salary after 6 months. The minimum benefit is 497 euros per month and the maximum is 1087,20 euros per month for a single person.[30] The non-contributory allowance is available to those persons who are no longer entitled to the contributory pension and who do not have income above 75% of the national minimum wage.		Sweden uses the Ghent system, under which a significant proportion of unemployment benefits are distributed by union unemployment funds. Unemployment benefits are divided into a voluntary scheme with income related compensation up to a certain level and a comprehensive scheme that provides a lower level of basic support. The voluntary scheme requires a minimum of 12 months membership and 6 months employment during that time before any claims can be made. Employers pay a fee on top of the pre-tax income of their employees, which together with membership fees, fund the scheme (see Unemployment funds in Sweden).		The maximum unemployment benefit is (as of July 2016) SEK 980 per day. During the first 200 days the unemployed will receive 80 percent of his or her normal income during the last 12 months. From day 201-300 this goes down to 70 percent and from day 301-450 the insurance covers 65 percent of the normal income (only available for parents to children under the age of 18). In Sweden tax is paid on unemployment benefits, so the unemployed will get a maximum of about SEK 10,000 per month during the first 100 days (depending on the municipality tax rate). In other currencies, as of June 2017, this means a maximum of approximately £900, $1,150, or €1,000, each month after tax. Private insurance is also available, mainly through professional organisations, to provide income related compensation that otherwise exceeds the ceiling of the scheme. The comprehensive scheme is funded by tax.		Saudi Arabia is an economic welfare state with free medical care[31] and unemployment benefits.[32] However, the country relies not on taxation but mainly oil revenues to maintain the social and economic services to its populace.		Payment: 2000 SAR (USD $534) for only 12 months for unemployed person from ages 18–35		External links		JSA for a single person is changed annually, and at August 3, 2012 the maximum payable was £71.00 per week for a person aged over 25 and £56.25 per week for a person aged 18–24.[33] The rules for couples where both are unemployed are more complex, but a maximum of £112.55 per week is payable, dependent on age and other factors. Income-based JSA is reduced for people with savings of over £6,000, by a reduction of £1 per week per £250 of savings, up to £16,000. People with savings of over £16,000 are not able to get IB-JSA at all.[34] The British system provides rent payments as part of a separate scheme called Housing Benefit.		Unemployment benefit is commonly referred to as "the dole"; to receive the benefit is to be "on the dole". "Dole" here is an archaic expression meaning "one's allotted portion", from the synonymous Old English word dāl. [35]		In the United States, there are 50 state unemployment insurance programs plus one each in the District of Columbia, Puerto Rico and United States Virgin Islands. Though policies vary by state, unemployment benefits generally pay eligible workers up to $450 per week maximum.[36] Benefits are generally paid by state governments, funded in large part by state and federal payroll taxes levied against employers, to workers who have become unemployed through no fault of their own. Eligibility requirements for unemployment insurance vary by state, but generally speaking, employees not fired for misconduct ("terminated for cause") are eligible for unemployment benefits, while those fired for misconduct (this sometimes can include misconduct committed outside the workplace, such as a problematic social media post or committing a crime) are not.[37] In every state, employees who quit their job without "good cause" are not eligible for unemployment benefits, but the definition of good cause varies by state. In some states, being fired for misconduct permanently bars the employee from receiving unemployment benefits, while in others it only disqualifies the employee for a short period. This compensation is classified as a type of social welfare benefit. According to the Internal Revenue Code, these types of benefits are to be included in a taxpayer's gross income.[38]		The standard time-length of unemployment compensation is six months, although extensions are possible during economic downturns. During the Great Recession, unemployment benefits were extended to 99 weeks.[citation needed]		The Supreme Court held that federal unemployment law is constitutional and does not violate the Tenth Amendment in Steward Machine Company v. Davis, 301 U.S. 548 (1937).		Unemployment insurance is a federal-state program financed through federal and state payroll taxes (federal and state UI taxes).[39] In most states employers pay state and federal unemployment taxes if:		To facilitate this program, the U.S. Congress passed the Federal Unemployment Tax Act (FUTA), which authorizes the Internal Revenue Service (IRS) to collect an annual federal employer tax used to fund state workforce agencies. FUTA covers the costs of administering the Unemployment Insurance and Job Service programs in all states. In addition, FUTA pays one-half of the cost of extended unemployment benefits which are triggered in periods of high state unemployment. FUTA also provides a fund from which states UI funds may borrow to pay benefits. As originally established, the states paid the federal government.[39]		The FUTA tax rate was originally three percent of taxable wages collected from employers who employed at least four employees,[40] and employers could deduct up to 90 percent of the amount due if they paid taxes to a state to support a system of unemployment insurance which met Federal standards,[4] but the rules have recently changed. The FUTA tax rate is now, as of June 30, 2011, 6.0 percent of taxable wages of employees who meet both the above and following criteria,[39] and the taxable wage base is the first $7,000 paid in wages to each employee during a calendar year.[39] Employers who pay the state unemployment tax on time receive an offset credit of up to 5.4 percent regardless of the rate of tax they pay their state. Therefore, the net FUTA tax rate is generally 0.6 percent (6.0 percent - 5.4 percent), for a maximum FUTA tax of $42.00 per employee, per year (.006 X $7,000 = $42.00). State law determines individual state unemployment insurance tax rates.[39] In the United States, unemployment insurance tax rates use experience rating.[41]		Although the taxable wage base for each state/territory is at least $7,000 as mandated by FUTA, only four states or territories still remain at this minimum.[42] These states/territories include Arizona, California, Florida, and Puerto Rico. Florida and Puerto Rico maintain tax rates similar to those of other states, but Arizona and California both have a higher maximum tax rate. Florida's minimum tax rate is 0.1% and the state maximum is 5.4%[43] and in Puerto Rico, employers are taxed between 2.4% and 5.4% depending on their experience rating.[44] As of 2015, Arizona's minimum was 0.03%, but its maximum was 7.79%.[45] California's tax rate on the taxable wage base is currently higher than the federal minimum of 6.0%. Employers are currently on a tax schedule that requires them to pay a minimum of 1.5% and a maximum 6.2% of the taxable wage base.[46] Even with the federal tax credit of 5.4%, Arizona employers could end up paying $175 per employee ((.0779-.054) x $7,000) and California employers could pay $56 per employee ((.062-.054) x $7,000) versus the FUTA maximum of $42.		Within the above constraints, the individual states and territories raise their own contributions and run their own programs. The federal government sets broad guidelines for coverage and eligibility, but states vary in how they determine benefits and eligibility.		Federal rules are drawn by the United States Department of Labor, Employment and Training Administration. For most states, the maximum period for receiving benefits is 26 weeks. There is an extended benefit program (authorized through the Social Security Acts) that may be triggered by the state unemployment rate. Congress has often passed temporary programs to extend benefits during recessions. This was done with the Temporary Extended Unemployment Compensation (TEUC) program in 2002-2003, which has since expired,[47] and remained in force through June 2, 2010, with the Extended Unemployment Compensation 2008 legislation.[48] In July 2010, legislation that provides an extension of federal extended unemployment benefits through November 2010 was signed by the president. The legislation extended benefits for 2.3 million unemployed workers who had exhausted their unemployment benefits.		The federal government lends money to the states for unemployment insurance when states run short of funds which happens when the state's UI fund cannot cover the cost of current benefits. A high unemployment rate shrinks UI tax revenues and increases expenditures on benefits. State UI finances and the need for loans are exacerbated when a state cuts taxes and increases benefits. FUTA loans to state funds are repaid with interest.		Congressional actions to increase penalties for states incurring large debts for unemployment benefits led to state fiscal crises in the 1980s.[citation needed]		To Keynesians, unemployment insurance acts as an automatic stabilizer.[49] Benefits automatically increase when unemployment is high and fall when unemployment is low, smoothing the business cycle; however, others claim that the taxation necessary to support this system serves to decrease employment.[citation needed]		In order to receive benefits, a person must have worked for at least one quarter in the previous year and have been laid-off by an employer. Workers who were temporary or were paid under the table are not eligible for unemployment insurance. If a worker quits without good cause or is fired for misconduct, then they are normally not eligible for UI benefits. There are five common reasons a claim for unemployment benefits are denied: the worker is unavailable for work, the worker quit his or her job without good cause, the worker was fired for misconduct, refusing suitable work, and unemployment resulting from a labor dispute.[50][51] In practice, it is only practical to verify whether the worker quit or was fired. If the worker's claim is denied, then they have the right to appeal. If the worker was fired for misconduct, then the employer has the burden to prove substantially that the termination of employment is a misconduct defined by individual states laws.[52] However, if the employee quit their job, then they must prove that their voluntary separation must be good cause.		Generally, the worker must be unemployed through no fault of his/her own although workers often file for benefits they are not entitled to; when the employer demonstrates that the unemployed person quit or was fired for cause the worker is required to pay back the benefits they received. The unemployed person must also meet state requirements for wages earned or time worked during an established period of time (referred to as a “base period”) to be eligible for benefits. In most states, the base period is usually the first four out of the last five completed calendar quarters prior to the time that the claim is filed.[53] Unemployment benefits are based on reported covered quarterly earnings. The amount of earnings and the number of quarters worked are used to determine the length and value of the unemployment benefit. The average weekly in 2010 payment was $293.[54]		As a result of the American Recovery and Reinvestment Act passed in February 2009, many unemployed people receive up to 99 weeks of unemployment benefits; this may depend on State legislation. Before the passage of the American Recovery and Reinvestment Act, the maximum number of weeks allowed was 26.		It generally takes two weeks for benefit payments to begin, the first being a "waiting week", which is not reimbursed, and the second being the time lag between eligibility for the program and the first benefit actually being paid.		To begin a claim, the unemployed worker must apply for benefits through a state unemployment agency.[53] In certain instances, the employer initiates the process. Generally, the certification includes the affected person affirming that they are "able and available for work", the amount of any part-time earnings they may have had, and whether they are actively seeking work. These certifications are usually accomplished either over the Internet or via an interactive voice response telephone call, but in a few states may be by mail. After receiving an application, the state will notify the individual if they qualify and the rate they will receive every week. The state will also review the reason for separation from employment. Many states require the individual to periodically certify that the conditions of the benefits are still met.		If a worker's reason for separation from their last job is due to some reason other than a "lack of work," a determination will be made about whether they are eligible for benefits. Generally, all determinations of eligibility for benefits are made by the appropriate State under its law or applicable federal laws. If a worker is disqualified or denied benefits, they have the right to file an appeal within an established time-frame. The State will advise a worker of his or her appeal rights. An employer may also appeal a determination if they do not agree with the State's determination regarding the employee's eligibility.[53] If the worker's claim is denied, then they have the right to appeal. If the worker was fired for misconduct, then the employer has the burden to prove substantially that the termination of employment is a misconduct defined by individual states laws.[52] However, if the employee quit their job, then they must prove that their voluntary separation must be good cause. Success rate of unemployment appeals is two-thirds, or 67% of the time for the most claimants.[55][56]		Each Thursday, the Department of Labor issues the Unemployment Insurance Weekly Claims Report.[57] Its headline number is the seasonally adjusted estimate for the initial claims for unemployment for the previous week in the United States. This statistic, because it is published weekly, is depended on as a current indicator of the labor market and the economy generally.		In 2016, the number of people on unemployment benefits fell to around 1.74 the lowest in the last 4 decades. [58]		Twice a year, the Office of Management and Budget delivers an economic assessment of the unemployment insurance program as it relates to budgetary issues.[59] As it relates to the FY 2012 budget, the OMB reports that the insured unemployment rate (IUR) is projected to average 3.6% in both FY 2011 and in FY 2012. State unemployment regular benefit outlays are estimated at $61 billion in FY 2011 and $64.3 billion in FY 2012, down somewhat from Midsession estimates.[59] Outlays from state trust fund accounts are projected to exceed revenues and interest income by $16.0 billion in FY 2011 and $15.1 billion in FY 2012.[59] State trust fund account balances, net of loans, are projected to continue to fall, from -$27.4 billion at the end of FY 2010 to -$62.7 billion at the end of FY 2013, before starting to grow again.[59] Net balances are not projected to become positive again until well beyond FY 2016. Up to 40 states are projected to continue borrowing heavily from the Federal Unemployment Account (FUA) over the next few years.[59] The aggregate loan balance is projected to increase from $40.2 billion at the end of FY 2010 to a peak end-of-year balance of $68.3 billion in FY 2013. Due to the high volume of state loans and increased EB payments, FUA and EUCA are projected to borrow $26.7 billion from the general fund in FY 2011 and an additional $19.4 billion in FY 2012, with neither account projected to return to a net positive balance before 2016.[59] The general fund advances must be repaid with interest.[59]		The economic argument for unemployment insurance comes from the principle of adverse selection. One common criticism of unemployment insurance is that it induces moral hazard, the fact that unemployment insurance lowers on-the-job effort and reduces job-search effort.		Adverse selection refers to the fact that “workers who have the highest probability of becoming unemployed have the highest demand for unemployment insurance.”[60] Adverse selection causes profit maximizing private insurance agencies to set high premiums for the insurance because there is a high likelihood they will have to make payments to the policyholder. High premiums work to exclude many individuals who otherwise might purchase the insurance. “A compulsory government program avoids the adverse selection problem. Hence, government provision of UI has the potential to increase efficiency. However, government provision does not eliminate moral hazard.”[60]		“At the same time, those workers who managed to obtain insurance might experience more unemployment otherwise would have been the case.”[60] The private insurance company would have to determine whether the employee is unemployed through no fault of their own, which is difficult to determine. Incorrect determinations could result in the payout of significant amounts for fraudulent claims or alternately failure to pay legitimate claims. This leads to the rationale that if government could solve either problem that government intervention would increase efficiency.		In the Great Recession, the “moral hazard” issue of whether unemployment insurance—and specifically extending benefits past the maximum 99 weeks—significantly encourages unemployment by discouraging workers from finding and taking jobs was expressed by Republican legislators. Conservative economist Robert Barro found that benefits raised the unemployment rate 2%.[61][62] Disagreeing with Barro's study were Berkeley economist Jesse Rothstein, who found the “vast majority” of unemployment was due to “demand shocks” not “[unemployment insurance]-induced supply reductions.”[62][63] A study by Rothstein of extensions of unemployment insurance to 99 weeks during the Great Recession to test the hypothesis that unemployment insurance discourages people from seeking jobs found the overall effect of UI on unemployment was to raise it by no more than one-tenth of one percent.[64][65]		A November 2011 report by the Congressional Budget Office found that even if unemployment benefits convince some unemployed to ignore job openings, these openings were quickly filled by new entrants into the labor market.[62][66] A survey of studies on unemployment insurance's effect on employment by the Political Economy Research Institute found that unemployed who collected benefits did not find themselves out of work longer than those who didn’t have unemployment benefits; and that unemployed workers did not search for work more or reduce their wage expectations once their benefits ran out.[62][67]		One concern over unemployment insurance increasing unemployment is based on experience rating benefit uses which can sometimes be imperfect. That is, the cost to the employer in increased taxes is less than the benefits that would be paid to the employee upon layoff. The firm in this instance believes that it is more cost effective to lay off the employee, causing more unemployment than under perfect experience rating.[60]		An alternative rationale for unemployment insurance is that it may allow for improving the quality of matches between workers and firms. Marimon and Zilibotti argued that although a more generous unemployment benefit system may indeed increase the unemployment rate, it may also help improve the average match quality.[68] A similar point is made by Mazur who analyzed the welfare and inequality effects of a policy reform giving entitlement for unemployment insurance to quitters.[69] Arash Nekoei and Andrea Weber present empirical evidence from Austria that extending unemployment benefit duration raises wages by improving reemployment firm quality.[70] Similarly, Tatsiramos studied data from European countries and found that although unemployment insurance does increase unemployment duration, the duration of subsequent employment tends to be longer (suggesting better match quality).[71]		An alternative to unemployment insurance intended to reduce the moral hazard costs would introduce mandated individual saving accounts for workers to draw on after being laid off. The plan, by Martin Feldstein would pay any positive account balance at retirement to the employee.[72]		Another issue with unemployment insurance relates to its effects on state budgets. During recessionary time periods, the number of unemployed rises and they begin to draw benefits from the program. The longer the recession lasts, depending on the state’s starting UI program balance, the quicker the state begins to run out of funds. The recession that began in December 2007 and ended in June 2009 has significantly impacted state budgets. According to The Council of State Governments, by March 18, 2011, 32 states plus the Virgin Islands had borrowed nearly $45.7 billion. The Labor Department estimates by the fourth quarter of 2013, as many as 40 states may need to borrow more than $90 billion to fund their unemployment programs and it will take a decade or more to pay off the debt.[73]		Possible policy options for states to shore up the unemployment insurance funds include lowering benefits for recipients and/or raising taxes on businesses. Kentucky took the approach of raising taxes and lowering benefits to attempt to balance its unemployment insurance program. Starting in 2010, a claimant’s weekly benefits will decrease from 68% to 62% and the taxable wage base will increase from $8,000 to $12,000, over a ten-year period. These moves are estimated to save the state over $450 million.[74]		The argument for taxation of social welfare benefits is that they result in a realized gain for a taxpayer. The argument against taxation is that the benefits are generally less than the federal poverty level.		Unemployment compensation has been taxable by the federal government since 1987.[75] Code Section 85 deemed unemployment compensation included in gross income.[76] Federal taxes are not withheld from unemployment compensation at the time of payment unless requested by the recipient using Form W-4V.[75][77] In 2003, Rep. Philip English introduced legislation to repeal the taxation of unemployment compensation, but the legislation did not advance past committee.[75][78] Most states with income tax consider unemployment compensation to be taxable.[75] Prior to 1987, unemployment compensation amounts were excluded from federal gross income.[79] For the US Federal tax year of 2009, as a result of the signing of the American Recovery and Reinvestment Act of 2009 signed by Barack Obama on February 17, 2009 the first $2,400 worth of unemployment income received during the 'tax year' of 2009 will be exempted from being considered as taxable income on the Federal level, when American taxpayers file their 2009 IRS tax return paperwork in early 2010.		Job sharing or work sharing and short time or short-time working refer to situations or systems in which employees agree to or are forced to accept a reduction in working time and pay. These can be based on individual agreements or on government programs in many countries that try to prevent unemployment. In these, employers have the option of reducing work hours to part-time for many employees instead of laying off some of them and retaining only full-time workers. For example, employees in 18 states of the United States can then receive unemployment payments for the hours they are no longer working.[80]		In 2013, it was reported that most U.S. states deliver unemployment benefits to recipients who do not have a bank account through a prepaid debit card.[81] The federal government uses the Direct Express Debit Mastercard prepaid debit card offered by Mastercard and Comerica Bank to give some federal assistance payments to people who do not have bank accounts. Many states have similar programs for unemployment payments and other assistance.		International Labour Organization has adopted the Employment Promotion and Protection against Unemployment Convention, 1988 for promotion of employment against unemployment and social security including unemployment benefit.		
A maximum wage, also often called a wage ceiling, is a legal limit on how much income an individual can earn.[1] It is a prescribed limitation which can be used to affect change in an economic structure, but its effects are unrelated to the benefits of minimum wage laws used currently by some states to enforce minimum earnings.[2] A maximum wage does not directly redistribute wealth, but it does limit the nominal income of specific workers within a society.		Advocates argue that a maximum wage could limit the possibility for inflation of a currency or economy, similar to the way a minimum wage may limit deflation. If these hypotheses are true, implementing both pieces of legislation would achieve an economy with wages that cannot inflate or deflate past the point of the relative maximum/minimum wage (respectively). Accordingly, wages in the economy would hover between the maximum and minimum, and the populace would live between the two wage points. Supporters say a maximum wage could also reduce devaluation of a currency by limiting the amount any member of the populace can earn, and consequently effectively limiting the availability of currency.		Critics argue that a maximum wage on the upper class merely restructures compensation and benefits, moving the excess income beyond the reach of more direct taxation policies. These critics point out that in practice a maximum wage only limits the nominal income of workers, and as such has none of the redistribution benefits that a progressive income tax table with uncapped income would have. Economists of the monetarist school hold that the ability of a maximum wage to limit inflation is false; instead they believe that inflation is controlled by growth in the money supply according to the quantity theory of money, rather than through growth in actual wages.						At present, Cuba has an active maximum wage law, where individuals cannot earn more than 20 U.S. dollars per month.[3] Egypt passed a maximum wage law in July, 2014 and faced a subsequent brain drain of the top employees within the banking sector.[4] A vote to implement a maximum wage law in Switzerland failed with only a 34.7% vote for approval.[5]		No major economy has a direct earnings limit, though some economies do incorporate the policy of highly progressive tax structures in the form of scaled taxation.		A maximum liquid wealth policy restricts the amount of liquid wealth an individual is permitted to maintain, while giving them unrestricted access to non-liquid assets. That is to say, an individual may earn as much as they like during a given time period, but all earnings must be re-invested (spent) within an equivalent time period; all earnings not re-invested within this time period would be seized.		This policy is only arguably a valid maximum wage implementation, as it does not actually restrict the wages a person is allowed to maintain, but only restricts the amount of actual currency they are allowed to hold at any given time. Proponents of the policy argue that it enforces the ideals of a maximum wage without restricting actual capital growth or economic incentive.		Proponents believe wealth that is not re-invested in the economy is harmful to economic growth; that actual liquid currency not re-invested timely is indicative of an unfair trade, in which an individual has paid more for a good/service than the good/service was worth. This stems from the belief that currency should represent the actual value of a good or service.		When this policy is imposed, individual savings can only be held as solid assets like stocks, bonds, business, and property. Opponents argue that since a maximum liquid wealth policy makes no allowance for individual savings, it therefore assumes the non-importance of a bank and the loans that banks provide. Loans being essential to the economy, opponents argue, banks are an essential economic institution. Proponents of the maximum liquid wealth policy respond that government could be directly responsible for supplying loans to individuals; they also add that such an arrangement could result in vastly lower interest rates. Of course, proponents of limited government would not find this situation ideal.		A relative earnings limit is a limit imposed upon a business, to the amount of compensation an individual is allowed, as a specific multiple of a company's lowest earner; or directly relative to the number of individuals a company employs and the average compensation provided to each individual employee, not including a certain percentage of the company's top earners. The former implementation has the advantage of limiting wage gaps. The latter implementation has the advantage of encouraging employment opportunities, as increasing employment would be a way for employers to boost their maximum earnings. A compromise would be to base the limit upon the number of employees had by a specific company and the compensation of that company's lowest earner.		A weakness in this method is that a company can simply hire outside firms to keep low wage employees off their payroll, while only having the top earning employees on the company's payroll, effectively bypassing the limits. However, the hiring of external employees will come at a higher total cost and will reduce company profits, something against which executives are often measured and compensated.		To moderate self-employed individuals, the maximum could be based on the average compensation of the nation's employed (GDP Per Capita) and a specific multiplier. The number of self-employed individuals with no employees and who earn excessively will be extremely limited, such a measure will unlikely to be implemented.		A direct earnings limit is a limit placed directly, usually as a number in terms of currency, upon the amount of compensation any individual is allowed to earn in a given time period.		In 2011 Venezuela announced that from January 2012 its public officials would be subject to salary limits, with different types of official positions subject to different maximum salaries. At the highest level, officials may receive salaries no higher than 12 times the minimum wage. State governors, for example, may receive a maximum of 9 times the minimum wage.[6]		Scaled taxation is a method of progressive taxation that raises the rate at which the principal sum is taxed, directly relative to the amount of the principal. This type of taxation is normally applied to income taxes, although other types of taxation can be scaled.		In the case of a maximum wage, a scaled tax would be applied so that the top earners in a society would be taxed extremely large percentages of their income. Modern income tax systems, allowing salary raises to be reflected by a raise in after tax income, tax each individual note of currency in each particular bracket at the same rate.[7] An example follows.		In England, the Statute of Artificers 1563 implemented statutes of compulsory labor and fixed maximum wage scales; Justices of the Peace could fix wages according "to the plenty or scarcity of the time".		To counteract the increase in prevailing wages due to scarcity of labor, American colonies in the 17th century created a ceiling wage and minimum hours of employment.[8]		In the early Soviet Union, in the period 1920–1932, communist party members were subject to a maximum wage, the partmaximum. Its demise is seen as the onset of the rise of the nomenklatura class of Soviet apparatchiks. The idea that any individual could earn money by their labor, instead of earning for the community, undermined the initial principles of communism.		In 1942, during World War II, US President Franklin D. Roosevelt proposed a maximum income of $25,000 during the war:[9][10]		At the same time, while the number of individual Americans affected is small, discrepancies between low personal incomes and very high personal incomes should be lessened; and I therefore believe that in time of this grave national danger, when all excess income should go to win the war, no American citizen ought to have a net income, after he has paid his taxes, of more than $25,000 a year. It is indefensible that those who enjoy large incomes from State and local securities should be immune from taxation while we are at war. Interest on such securities should be subject at least to surtaxes.		This was proposed to be implemented by a 100% marginal tax on all income over $40,000 (after-tax income of $25,000). While this was not implemented, the Revenue Act of 1942 implemented an 88% marginal tax rate on income over $200,000, together with a 5% "Victory Tax" with post-war credits, hence temporarily yielding a 93% top tax rate (though 5% was subsequently returned in credits).[9]		After decades of social democratic governments, the Swedish children's author Astrid Lindgren faced an infamous marginal tax rate of 102% in 1976, in effect creating a wage ceiling. Though the example was partly due to inverted loop holes in the tax code, the figure was seen as an important catalyst for the results in the election that year, in which the Social Democratic Party lost power after 40 consecutive years in power. After a "tax rebellion" and demanded the top marginal tax rates were reduced to 50% in the late 1980s.[citation needed]		Since the 1990s, the chief proponent of a maximum wage in the United States has been Sam Pizzigati;[11] see References, particularly (Pizzigati 2004).		In his 2000 run for the Green Party presidential nomination, Jello Biafra called for a maximum wage of $100,000 in the United States, and the reduction of the income tax to zero for all income below that level. Biafra claimed he would increase taxes for the wealthy and reduce taxes for those in the lower and middle classes.[citation needed] Many Green parties have a maximum wage in their manifesto, which they argue would prevent conspicuous consumption and the subsequent environmental damage that they believe ensues, while allowing the financing of jobs and a guaranteed minimum income for the poorest workers.		In his campaign for the French presidency in 2012, Jean-Luc Mélenchon argued in favour of a tax rate of 100% on incomes over €360,000.[12]		In the United Kingdom until 1901, individual clubs had set their own wage policies. That year, the Football League ratified a maximum weekly wage for footballers of £4 (2012: £368). This severely limited the ability of the best players in the country to forgo the need to take paid employment outside of football and, this in turn, led to the formation of The Players' Union in 1907.		By Summer 1928 players could earn a weekly maximum of £8 (2012: £408), although clubs routinely found ways to increase this.[13] Arsenal player Eddie Hapgood supplemented his income by fashion modelling and advertising chocolate.[14]		Association football retained the maximum wage until January 1961, at which point it was abolished after Jimmy Hill, chairman of the Professional Footballers' Association, threatened strike action.[15] Before then players earned a maximum of £20 (2012: £377) a week, which was then around the average wage for a British worker.[15] In justifying the strike action, one union representative stated that he "admired people in the mining community but it didn't mean they could cope with marking Stanley Matthews on a Saturday afternoon".[15] Johnny Haynes became football's first £100-a-week player days after the maximum wage was abolished, and ten years later George Best was earning £1,000 (2012: £12,000) a week.[15]		
An avocation is an activity that someone engages in as a hobby outside their main occupation. There are many examples of people whose professions were the ways that they made their livings, but for whom their activities outside of their workplaces were their true passions in life.[1][2] Occasionally, as with Lord Baden-Powell and others, people who pursue an avocation are more remembered by history for their avocation than for their professional career.		Many times a person's regular vocation may lead to an avocation. Many forms of humanitarian campaigning, such as work for organizations like Amnesty International and Greenpeace, may be done by people involved in the law or human rights issues as part of their work.[3]		Many people involved with youth work pursue this as an avocation.[4]		But yield who will to their separation, My object in living is to unite My avocation and my vocation As my two eyes make one in sight. Only where love and need are one, And the work is play for mortal stakes, Is the deed ever really done For heaven and the future's sakes.				
Reserve army of labour is a concept in Karl Marx's critique of political economy.[1] It refers to the unemployed and under-employed in capitalist society. It is synonymous with "industrial reserve army" or "relative surplus population", except that the unemployed can be defined as those actually looking for work and that the relative surplus population also includes people unable to work. The use of the word "army" refers to the workers being conscripted and regimented in the workplace in a hierarchy, under the command or authority of the owners of capital.		Marx did not invent the term "reserve army of labour". It was already being used by Friedrich Engels in his 1845 book The Condition of the Working Class in England.[2] What Marx did was theorize the reserve army of labour as a necessary part of the capitalist organization of work.		Prior to what Marx regarded as the start of the capitalist era in human history (i.e. before the 16th century), structural unemployment on a mass scale rarely existed, other than that caused by natural disasters and wars.[3] In ancient societies, all people who could work necessarily had to work, otherwise they would starve; a slave or a serf by definition could not become "unemployed". There was normally very little possibility of "earning a crust" without working at all, and the usual attitude toward beggars and idlers was harsh.[4] Children began to work already at a very early age.						Although the idea of the industrial reserve army of labour is closely associated with Marx, it was already in circulation in the British labour movement by the 1830s.[5] Friedrich Engels discussed the reserve army of labour before Marx did, in Engels's famous book The condition of the working class in England (1845). The first mention of the reserve army of labour in Marx's writing occurs in a manuscript he wrote in 1847, but did not publish:		"Big industry constantly requires a reserve army of unemployed workers for times of overproduction. The main purpose of the bourgeois in relation to the worker is, of course, to have the commodity labour as cheaply as possible, which is only possible when the supply of this commodity is as large as possible in relation to the demand for it, i.e., when the overpopulation is the greatest. Overpopulation is therefore in the interest of the bourgeoisie, and it gives the workers good advice which it knows to be impossible to carry out. Since capital only increases when it employs workers, the increase of capital involves an increase of the proletariat, and, as we have seen, according to the nature of the relation of capital and labour, the increase of the proletariat must proceed relatively even faster. The... theory... which is also expressed as a law of nature, that population grows faster than the means of subsistence, is the more welcome to the bourgeois as it silences his conscience, makes hard-heartedness into a moral duty and the consequences of society into the consequences of nature, and finally gives him the opportunity to watch the destruction of the proletariat by starvation as calmly as any other natural event without bestirring himself, and, on the other hand, to regard the misery of the proletariat as its own fault and to punish it. To be sure, the proletarian can restrain his natural instinct by reason, and so, by moral supervision, halt the law of nature in its injurious course of development." - Karl Marx, Wages, December 1847[6]		The idea of the labour force as an "army" occurs also in Part 1 of the Communist Manifesto, written by Marx and Engels in 1848:		"Modern Industry has converted the little workshop of the patriarchal master into the great factory of the industrial capitalist. Masses of labourers, crowded into the factory, are organised like soldiers. As privates of the industrial army they are placed under the command of a perfect hierarchy of officers and sergeants. Not only are they slaves of the bourgeois class, and of the bourgeois State; they are daily and hourly enslaved by the machine, by the overlooker, and, above all, by the individual bourgeois manufacturer himself. The more openly this despotism proclaims gain to be its end and aim, the more petty, the more hateful and the more embittering it is."		Marx introduces the concept of the reserve army of labour in chapter 25 of the first volume of Das Kapital,[7] twenty years later in 1867, stating that:		"capitalistic accumulation itself... constantly produces, and produces in the direct ratio of its own energy and extent, a relatively redundant population of workers, i.e., a population of greater extent than suffices for the average needs of the valorisation of capital, and therefore a surplus-population... It is the absolute interest of every capitalist to press a given quantity of labour out of a smaller, rather than a greater number of labourers, if the cost is about the same... The more extended the scale of production, the stronger this motive. Its force increases with the accumulation of capital."		His argument is that as capitalism develops, the organic composition of capital will increase, which means that the mass of constant capital grows faster than the mass of variable capital. Fewer workers can produce all that is necessary for society's requirements. In addition, capital will become more concentrated and centralized in fewer hands.		This being the absolute historical tendency, part of the working population will tend to become surplus to the requirements of capital accumulation over time. Paradoxically, the larger the wealth of society, the larger the industrial reserve army will become. One could add that the larger the wealth of society, the more people it can also support who do not work.		However, as Marx develops the argument further, it also becomes clear that, depending on the state of the economy, the reserve army of labour will either expand or contract, alternately being absorbed or expelled from the employed workforce. Thus,		"Taking them as a whole, the general movements of wages are exclusively regulated by the expansion and contraction of the industrial reserve army, and these again correspond to the periodic changes of the industrial cycle. They are, therefore, not determined by the variations of the absolute number of the working population, but by the varying proportions in which the working-class is divided into active and reserve army, by the increase or diminution in the relative amount of the surplus-population, by the extent to which it is now absorbed, now set free."		Marx concludes that: "Relative surplus-population is therefore the pivot upon which the law of demand and supply of labour works." The availability of labour influences wage rates, and the larger the unemployed workforce grows, the more this forces down wage rates; conversely, if there are plenty jobs available and unemployment is low, this tends to raise the average level of wages—in that case workers are able to change jobs rapidly to get better pay.		Marx discusses the army of labor and the reserve army in Capital, Ch. 14, Counteracting Factors, Section IV. The Army of Labor consists in those working-class people employed in average or better than average jobs. Not every one in the working class gets one of these jobs. There are then four other categories where members of the working class might find themselves: the stagnant pool, the floating reserves, the latent reserve, and pauperdom. Finally, people may leave the army and the reserve army by turning to criminality, Marx refers to such people as lumpenproletariat.[8]		Marx then analyses the reserve army of labour in detail, using data on Britain where he lived.		There are five main controversies about the concept of the reserve army of labour.		Some writers have interpreted Marx's argument to mean that an absolute immiseration of the working class would occur as the broad historical trend. Thus, the workers would become more and more impoverished, and unemployment would constantly grow.[9] This is of course not really credible in the light of the facts, because in various epochs and countries, workers' living standards have definitely improved rather than declined. In some periods, unemployment had been reduced to a very small amount. In the Great Depression, about one in four workers became unemployed, but towards the end of the post-war boom unemployment in richer countries reduced to a very low level. However, economic historian Paul Bairoch estimated in the mid-1980s that in Latin America, Africa and Asia, “total inactivity” among the population was “on the order of 30-40% of potential working man-hours” – a situation without historical precedent, “except perhaps in the case of ancient Rome.”[10]		Other writers, such as Ernest Mandel and Roman Rosdolsky,[11][12] argued that in truth Marx had no theory of an absolute immiseration of the working class; at most one could say that the rich-poor gap continues to grow, i.e. the wealthy get wealthier much more than ordinary workers improve their living standards. In part, the level of unemployment also seems to be based on the balance of power between social classes, and state policy. Governments can allow unemployment to rise, but also implement job-creating policies, which makes unemployment levels partly a political result.		If chapter 25 of Marx's Capital, Volume I is read carefully, it is plain that Marx does not actually say what many critics accuse him of.[13] Marx himself says that the "absolute general law of capitalist accumulation" is that the more that capital grows in size and value, the bigger the working class becomes, and the larger the pauperized sections of the working class and the industrial reserve army become.[14] He does not say however that the whole working class becomes pauperized, but rather that the part of it which is pauperized grows in size. He then carefully qualifies this argument, by saying that the absolute general law is "like all other laws... modified in its working by many circumstances."[14] Next, Marx says that in proportion as capital accumulates, the situation of the worker, be his payment high or low, must "grow worse."[15] It is quite clear from the context though that by "worse" Marx does not primarily mean poverty. He means instead, as he says himself explicitly, that "all means of development of production undergo a dialectical inversion so that they become means of domination and exploitation of the producers."[15] He is talking about "worse" in the sense of "inhuman", "more exploited" or "alienated".		Another dispute concerns the notion of "overpopulation".[16] In Marx's own time, Malthus raised dire predictions that population growth enabled by capitalist wealth would exceed the food supply required to sustain that population. As noted, for Marx, "overpopulation" was really more an ideologically loaded term or social construct, and Marxists have argued there is no real problem here, as enough food can be produced for all; if there is a problem, it lies in the way that food is produced and distributed.		Marx argued that there are no substantive laws of population that hold good for all time; instead, each specific mode of production has its own specific demographic laws. If there was "overpopulation" in capitalist society, it was overpopulation relative to the requirements of capital accumulation. Consequently, demography could not simply just count people in various ways, it also had to study the social relations between them as well. If there are enough resources on the planet to provide all people with a decent life, the argument that there are "too many people" is rather dubious.		People cannot help being born and being there, but the concept of overpopulation can easily suggest that part of the people do not really deserve to be there, or that they should not exist. From there, it is only another step to hating part of the human race, and to feel justified in wiping that part out (or at least subject people to compulsory sterilization). If people believe that each human being has a right to be there and enjoy life, there cannot be "overpopulation". At most one could say that there are too many people living in a particular area. Even so, people can get used to living with remarkably little personal space.		The counter-argument is that in many poor countries, people get far more children than they can reasonably feed, support and provide with a decent life, under the circumstances. If people had fewer children, with the aid of contraception, then it would put much less pressure on already scarce resources, and enable a better life for the living.		In the social welfare area, there are also perpetual disputes about the extent to which unemployment is voluntarily chosen by people, or involuntary, whether it is forced on people or whether it is their own choice.[17] In the Great Depression of the 1930s, when unemployment rose to 20–30% of the working population in many countries, people generally believed it was involuntary. But if unemployment levels are relatively low, the argument that unemployment is a matter of choice is more often heard.[18] It could be, that there simply are no jobs for the unemployed, but it could also be that there is a mismatch between the skills that the unemployed have, and the type of skilled labour for which there is a demand. If the latter is the case, then it could be argued that if the unemployed were willing to retrain or do a different kind of work, then they could get a job. In a sense, it is always possible to get a job, if a person is prepared to accept a low salary, but people might not want to work below a certain minimum amount of salary.[citation needed]		There are endless debates about the best way to measure unemployment, its costs and its effects, and to what extent a degree of unemployment is inevitable in any country with a developed labour market.[19] According to the NAIRU concept, price stability in market-based societies necessarily requires a certain amount of unemployment. One reason a reserve army of the unemployed exists in market economies, it is argued, is that if the level of unemployment is too low it will stimulate price inflation. However, the validity of this argument depends also on state economic policy, and on the ability of workers to raise their wages. If for example trade unions are legally blocked from organizing workers, then even if unemployment is relatively low, average wages can be kept low; the only way that individual workers have in that case to raise their income, is to work more hours or work themselves up to better-paying jobs.		Normally, the government measure of unemployment defines "unemployed" as "without any job, but actively looking for work". There are also people defined as "jobless", who want work but are not, or no longer, actively looking for work because they are discouraged, etc. This official view of the matter is closely linked to the administration of unemployment benefits. To be entitled for an unemployment benefit, it is a requirement that the beneficiary is actively looking for work.		There are also many controversies about hidden unemployment. Hidden unemployment means that people are not counted or considered as unemployed, although in reality they are unemployed. For example, young people will stay in the family home, in schooling, or in some make-work scheme, because they can't find a paid job. People might also have a job, but they might be under-employed, because they cannot get more working hours or they cannot get a job for which they are qualified. People might also drop out of the official labour force, because they are discouraged, and no longer actively looking for work - they are no longer counted as unemployed, although they are. Governments can also subsidize employment of people who would otherwise be unemployed, or put people on benefits even although they could be working. It may be that workers are hired, but that they do nothing while at work.		On the one side, governments often try to make unemployment seem as low as possible, because that is politically desirable. On the other side, governments also often provide "broader" and "narrower" measures of unemployment. For example, the US Bureau of Labor Statistics provides six measures of labor underutilization (U-1, U-2, U-3, U-4, U-5, and U-6). The U-3 rate is the "official" unemployment rate.		Marx was writing in the mid-19th century, and his discussion of unemployment may therefore be, in part, out of date, especially if one considers only particular developed countries. However, his analysis may continue to be quite valid if considered globally.[20] The ILO reports that the proportion of jobless has been steadily increasing since the beginning of the financial crisis in 2008.		In 2007 the ILO standard global unemployment measure stood at 169.7 million. In 2012, five years later, the ILO global unemployment rate reached 5.9% of the civilian labour force (195.4 million, or a net 25.7 million more), 0.5 percentage points higher than the 5.4% rate before the financial crisis. The official global unemployment rate was expected to have risen to 6% of the civilian labour force in 2013. Over 30 million jobs were still needed to return total employment to the level that prevailed before the financial crisis. It was expected in 2013 that, globally, about 205 million people would be unemployed in 2014 and 214 million in 2018.[21] However, the official total of the unemployed was subsequently (in 2017) forecast to be just over 201 million persons in that year – with an additional rise of 2.7 million expected in 2018.[22] The official world total of unemployed in the labour force is approximately equal to the total number of employed workers in the USA, Canada and Mexico put together.		The official unemployment figures do not include jobless people who have dropped out of the labour force altogether because they can't find work. They include only those actually looking for work. The global unemployment rate is strongly influenced by population growth - the more population, the more unemployed and employed in absolute numbers. However, the proportion of jobless people is now rising every year and is expected to continue rising for quite some time.		Among the world's unemployed, the ILO estimates that roughly half the global total are young people aged 15 to 24. In the rich countries, it often does not matter so much if young people are unemployed at that age, but in the Middle East, Asia, Africa and Latin America, where most of the unemployed youths are, it is often a much more serious problem.[23]		In recent years, there has been a growing use in Marxist and anarchist theory of the concept of "the precariat", to describe a growing reliance on temporary, part-time workers with precarious status, who share aspects of the proletariat and the reserve army of labor.[24] Precarious workers do work part-time or full-time in temporary jobs, but they cannot really earn enough to live on, and depend partly on friends or family, or on state benefits, to survive. Typically they do not become truly "unemployed", but they don't have a decent job to go to either.[25]		Although non-employed people who are unable or uninterested in performing legal paid work are not considered among the "unemployed," the concept of "conjunctural unemployment" is used in economics nowadays.[26] Economists often distinguish between short-term "frictional" or "cyclical" unemployment, and longer-term "structural unemployment". Sometimes there is a shortterm mismatch between the demand and supply of labour, at other times there is much less total demand for labour than supply for a long time. If there is no possibility for getting a job at all in the foreseeable future, many younger people decide to migrate or emigrate to a place where they can find work.		
A golden parachute is an agreement between a company and an employee (usually upper executive) specifying that the employee will receive certain significant benefits if employment is terminated. Most definitions specify the employment termination is as a result of a merger or takeover,[1][2][3] also known as "Change-in-control benefits",[4] but more recently the term has been used to describe perceived excessive CEO (and other executives) severance packages unrelated to change in ownership (also known as a golden handshake).[5] The benefits may include severance pay, cash bonuses, stock options, or other benefits.						The first use of the term "golden parachute" is credited to a 1961 attempt by creditors to oust Howard Hughes from control of Trans World Airlines. The creditors provided Charles C. Tillinghast Jr. an employment contract that included a clause that would pay him money in the event that he lost his job.[6]		The use of golden parachutes expanded greatly in the early 1980s in response to the large increase in the number of takeovers and mergers.		In Europe the highest "change-in-control benefits" have been for French executives, as of 2006 according to a study by the Hay Group human resource management firm.[citation needed] French executives receive roughly the double of their salary and bonus in their golden parachute.[citation needed]		News reference volume of the term "golden parachute" spiked in late 2008 during the global economic recession, and 2008 US Presidential Debates.[7] Despite the poor economy, in the two years before 2012 a study by the professional services firm Alvarez & Marsal found a 32% increase in the value of "change-in-control benefits" provided to US executives.[4] In late 2011, USA Today reported several CEO retirement packages in excess of $100 million, "raising eyebrows even among those accustomed to oversized payouts".[5]		In the 1980s, golden parachutes prompted shareholder suits challenging the parachutes' validity, SEC "termination agreement disclosure rules" in 1986, and provisions in the Deficit Reduction Act of 1984 aimed at limiting the size of future parachutes[8] with a special tax on payouts that topped three times annual pay.[9] In the 1990s in the United States, some government efforts were made to diminish "change-in-control benefits". As of 1996, Section 280G of the Internal Revenue Code denies a corporation a deduction for any excess "parachute payment" made to a departing employee, and Section 4999 imposes on the recipient a nondeductible 20% excise tax, in addition to regular income and Social Security taxes. The idea being the expenses are in excess of reasonable compensation for personal services[10]		The 2010 United States Dodd-Frank Act includes in its provisions a mandate for shareholder votes on any future adoption of a golden parachute by publicly traded firms.[11] In Switzerland, a referendum which "would give shareholders the power to veto executive pay plans, including golden parachutes" was put to a vote on March 3, 2013.[12] Voters approved measures limiting CEO pay and outlawing golden parachutes.[13]		One study found golden parachutes associated with an increased likelihood of either receiving an acquisition offer or being acquired, a lower premium (in share price) in the event of an acquisition, and higher (unconditional) expected acquisition premiums. It found firms adopting golden parachutes have lower market value compared to assets of the company and that their value continues to decline during and after adopting golden parachutes.[11]		"Gratuitous" payments made to CEOs on agreeing to have their companies acquired (i.e. payments made to CEOs by the acquiring company not mandated under the CEO's contract at the time the company is acquired) have been criticized. A "prominent" mergers and acquisitions lawyer told the New York Times that `I have had a number of situations where we've gone to management looking to do a deal and been stopped at the door until a compensation arrangement was signed, sealed, and delivered.` Another lawyer told the Times: `Publicly, we have to call these things retention bonuses. Privately, sometimes it's the only way we would have got the deal done. It's a kickback. ...[14][15]		A study investigating acquirer-paid sweeteners at 311 large-firm acquisitions completed between 1995 and 1997 found that CEOs of the acquired companies accept lower acquisition premiums when the acquirer promised them a high-ranking managerial post after the acquisition.[15][16]		On June 24, 2013, The Wall Street Journal reported that McKesson Corporation Chairman and CEO John Hammergren's pension benefits of $159 million had set a record for "the largest pension on file for a current executive of a public company, and almost certainly the largest ever in corporate America." A study in 2012 by GMI Ratings, which tracks executive pay, found that 60% of CEOs at S&P 500 companies have pensions, and their value averages $11.5 million.[17]		On June 29, 2013, The New York Times reported on research findings suggesting that "despite years of public outcry against such deals, multimillion-dollar severance packages are still common," and they continue to become "more complex and opaque."[18]		Proponents of golden parachutes argue that the parachutes provide benefits to stockholders:		While critics have pointed out that:		
A part-time contract is a form of employment that carries fewer hours per week than a full-time job. They work in shifts. The shifts are often rotational. Workers are considered to be part-time if they commonly work fewer than 30 hours per week.[1] According to the International Labour Organization, the number of part-time workers has increased from one-fourth to a half in the past 20 years in most developed countries, excluding the United States.[1] There are many reasons for working part-time, including the desire to do so, having one's hours cut back by an employer and being unable to find a full-time job. The International Labour Organisation Convention 175 requires that part-time workers be treated no less favourably than full-time workers.[2]		In some cases the nature of the work itself may require that the employees be classified part as part-time workers. For example, some amusement parks are closed during winter months and keep only a skeleton crew on hand for maintenance and office work. As a result of this cutback in staffing during the off season, employees who operate rides, run gaming stands, or staff concession stands may be classified as part-time workers owing to the months long down time during which they may be technically employed but unable to work.		"Part-time" can also be used in reference to a student (usually in higher education) who takes only a few courses, rather than a full load of coursework each semester.						In the EU, there is a strong East/West divide, where: "in Central and Eastern European countries part-time work remains a marginal phenomenon even among women, while the Western countries have embraced it much more widely." The highest percentage of part-time work is in the Netherlands (see below) and the lowest in Bulgaria. There is also a gap between women (32.1% EU average in 2015) and men (8.9%).[3]		The Netherlands has by far the highest percentage of part-time workers in the EU[3] and in the OECD.[4] In 2012, 76.9% of women and 24.9% of men worked part-time.[5] The high percentage of women working part-time has been explained by social norms and the historical context of the country, where women were among the last in Europe to enter the workforce, and when they did, most of them did so on a part-time basis; according to The Economist, fewer Dutch men had to fight in the World Wars of the 20th century, and so Dutch women did not experience working for pay at rates women in other countries did. The wealth of the country, coupled with the fact that "[Dutch] politics was dominated by Christian values until the 1980s" meant that Dutch women were slower to enter into the workforce.[6] Recent research led by professor Stijn Baert (Ghent University) debunked the idea that part-time work by students is an asset for their CV in respect of later employment chances.[7]		Part-time employment in Australia involves a comprehensive framework. Part-time employees work fewer hours than their full-time counterparts within a specific industry. This can vary, but is generally less than 32 hours per week. Part-time employees within Australia are legally entitled to paid annual leave, sick leave, and having maternity leave etc. except it is covered on a 'pro-rata' (percentage) basis depending on the hours worked each week. Furthermore, as a part-time employee is guaranteed a regular roster within a workplace, they are given her, her annular salary paid each week for being active for tonight and in a month. Employers within Australia are obliged to provide minimum notice requirements for termination, redundancy and change of rostered hours in relation to part-time workers [1]. As of January 2010, the number of part-time workers within Australia is approximately 3.3 million out of the 10.9 million individuals within the Australian workforce [2].		In Canada, part-time workers are those who usually work fewer than 30 hours per week at their main or only job.[8] In 2007, just over 1 in every 10 employees aged 25 to 54 worked part-time. A person who has a part-time placement is often contracted to a company or business in which they have a set of terms they agree with. 'Part-time' can also be used in reference to a student(usually in higher education) who works only few hours a day. Usually students from different nations (India, China, Mexico etc.) prefer Canada for their higher studies due to the availability of more part-time jobs.[citation needed]		According to the Bureau of Labor Statistics, working part-time is defined as working between 1 and 34 hours per week.[9] In 2007, 18.3 million Americans worked part-time.[10] Typically, part-time employees in the United States are not entitled to employee benefits, such as health insurance. The Institute for Women's Policy Research reports that females are nine times likelier than males to work in a part-time capacity over a full-time capacity as a result of caregiving demands of their family members.[11][12]		Increasing use of part-time workers in the United States is associated with employee scheduling software often resulting in expansion of the part-time workforce, reduction of the full-time workforce and scheduling which is unpredictable and inconvenient.[13][14][15]		
Penal labour is a generic term for various kinds of unfree labour which prisoners are required to perform, typically manual labour. The work may be light or hard, depending on the context. Forms of sentence involving penal labour have included involuntary servitude, penal servitude and imprisonment with hard labour. The term may refer to several related scenarios: labour as a form of punishment, the prison system used as a means to secure labour, and labour as providing occupation for convicts. These scenarios can be applied to those imprisoned for political, religious, war, or other reasons as well as to criminal convicts.		Large-scale implementations of penal labour include labour camps, prison farms, penal colonies, penal military units, penal transportation, or aboard prison ships.						Punitive labour, also known as convict labour, prison labour, or hard labour, is a form of forced labour used in both past and present as an additional form of punishment beyond imprisonment alone. Punitive labour encompasses two types: productive labour, such as industrial work; and intrinsically pointless tasks used as primitive occupational therapy, punishment and/or physical torment.		Sometimes authorities turn prison labour into an industry, as on a prison farm or in a prison workshop. In such cases, the pursuit of income from their productive labour may even overtake the preoccupation with punishment and/or reeducation as such of the prisoners, who are then at risk of being exploited as slave-like cheap labour (profit may be minor after expenses, e.g. on security).		On the other hand, for example in Victorian prisons, inmates commonly were made to work the treadmill: in some cases, this was productive labour to grind grain; in others, it served no purpose. Similar punishments included turning the crank machine or carrying cannonballs.[1] Semi-punitive labour also included oakum-picking: teasing apart old tarry rope to make caulking material for sailing vessels.		Imprisonment with hard labour was first introduced into English law with the Criminal Law Act 1776 (6 Geo III c 43),[2] also known as the "Hulks Act", which authorized prisoners being put to work on improving the navigation of the River Thames in lieu of transportation to the North American colonies, which had become impossible due to the American Revolutionary War.[3]		The Penal Servitude Act 1853 (16 & 17 Vict c 99)[4] substituted penal servitude for transportation to a distant British colony, except in cases where a person could be sentenced to transportation for life or for a term not less than fourteen years. Section 2 of the Penal Servitude Act 1857 (20 & 21 Vict c 3)[5] abolished the sentence of transportation in all cases and provided that in all cases a person who would otherwise have been liable to transportation would be liable to penal servitude instead. Section 1 of the Penal Servitude Act 1891[6] makes provision for enactments which authorise a sentence of penal servitude but do not specify a maximum duration. It must now be read subject to section 1(1) of the Criminal Justice Act 1948.		Sentences of penal servitude were served in convict prisons and were controlled by the Home Office and the Prison Commissioners. After sentencing, convicts would be classified according to the seriousness of the offence of which they were convicted and their criminal record. First time offenders would be classified in the Star class; persons not suitable for the Star class, but without serious convictions would be classified in the intermediate class. Habitual offenders would be classified in the Recidivist class. Care was taken to ensure that convicts in one class did not mix with convicts in another.		Penal servitude included hard labour as a standard feature. Although it was prescribed for severe crimes (e.g. rape, attempted murder, wounding with intent, by the Offences against the Person Act 1861) it was also widely applied in cases of minor crime, such as petty theft and vagrancy, as well as victimless behaviour deemed harmful to the fabric of society. Notable recipients of hard labour under British law include Oscar Wilde (after his conviction for gross indecency) and John William Gott (a terminally ill trouser salesman convicted of blasphemy).		Labour was sometimes useful. In Inveraray Jail from 1839 prisoners worked up to ten hours a day. Most male prisoners made herring nets or picked oakum (Inveraray was a busy herring port); those with skills were often employed where the skills could be used, such as shoemaking, tailoring or joinery. Female prisoners picked oakum, knitted stockings or sewed.[1]		Forms of labour for punishment included the treadmill, shot drill, and the crank machine.[1]		Treadmills for punishment were used in prisons in Britain from 1818 until the second half of the 19th century; they often took the form of large paddle wheels some 20 feet in diameter with 24 steps around a six-foot cylinder. Prisoners had to work six or more hours a day, climbing the equivalent of 5,000 to 14,000 vertical feet. While the purpose was mainly punitive, the mills could have been used to grind grain, pump water, or operate a ventilation system.[7]		Shot drill involved stooping without bending the knees, lifting a heavy cannonball slowly to chest height, taking three steps to the right, replacing it on the ground, stepping back three paces, and repeating, moving cannonballs from one pile to another.[1]		The crank machine was a device which turned a crank by hand which in turn forced four large cups or ladles through sand inside a drum, doing nothing useful. Male prisoners had to turn the handle 14,400 times a day, as registered on a dial. The warder could make the task harder by tightening an adjusting screw, hence the slang term "screw" for prison warder.[1]		The British penal colonies in Australia between 1788 and 1868 provide a major historical example of convict labour, as described above: during that period, Australia received thousands of transported convict labourers, many of whom had received harsh sentences for minor misdemeanours in Britain or Ireland.		As late as 1885, 75% of all prison inmates were involved in some sort of productive endeavour, mostly in private contract and leasing systems. By 1935 the portion of prisoners working had fallen to 44%, and almost 90% of those worked in state-run programmes rather than for private contractors.[8]		Penal servitude was abolished for England and Wales by section 1(1) of the Criminal Justice Act 1948.[9] Every enactment conferring power on a court to pass a sentence of penal servitude in any case must be construed as conferring power to pass a sentence of imprisonment for a term not exceeding the maximum term of penal servitude for which a sentence could have been passed in that case immediately before the commencement of that Act.		Imprisonment with hard labour was abolished by section 1(2) of that Act.		Penal servitude was abolished for Northern Ireland by section 1(1) of the Criminal Justice Act (Northern Ireland) 1953.[10] Every enactment which operated to empower a court to pass a sentence of penal servitude in any case now operates so as to empower that court to pass a sentence of imprisonment for a term not exceeding the maximum term of penal servitude for which a sentence could have been passed in that case immediately before the commencement of that Act.		Imprisonment with hard labour was abolished by section 1(2) of that Act.		Penal Servitude was abolished for Scotland by section 16(1) of the Criminal Justice (Scotland) Act 1949 on 12 June 1950.		Imprisonment with hard labour was abolished by section 16(2) of that Act.		Every enactment conferring power on a court to pass a sentence of penal servitude in any case must be construed as conferring power to pass a sentence of imprisonment for a term not exceeding the maximum term of penal servitude for which a sentence could have been passed in that case immediately before 12 June 1950. But this does not empower any court, other than the High Court, to pass a sentence of imprisonment for a term exceeding three years.		See section 221 of the Criminal Procedure (Scotland) Act 1975 and section 307(4) of the Criminal Procedure (Scotland) Act 1995		The Criminal Justice Act 1954 abolished the distinction between imprisonment with and without hard labour and replaced 'reformative detention' with 'corrective training',[11] which was later abolished on 30 June 2002.[12]		Prison inmates can work [13] either for the prison (directly, by performing tasks linked to prison operation, or for the Régie Industrielle des Établissements Pénitentiaires, which produces and sells merchandises) or for a private company, in the framework of a prison/company agreement for leasing inmate labour. Work ceased being compulsory for sentenced inmates in France in 1987. From the French Revolution of 1789, the prison system has been governed by a new penal code.[14] Some prisons became quasi-factories, in the nineteenth century, many discussions focused on the issue of competition between free labour and prison labour. Prison work was temporarily prohibited during the revolution of 1848. Prison labour then specialised in the production of goods sold to government departments (and directly to prisons, for example guards' uniforms), or in small low-skilled manual labour (mainly subcontracting to small local industries).[15]		In pre-Maoist China, a system of labor camps for political prisoners operated by the Kuomintang forces of Chiang Kai-shek existed during the Chinese Civil War from 1938–1949. Young activists and students accused of supporting Mao Zedong and his Communists were arrested and re-educated in the spirit of anti-communism at the Northwestern Youth Labor Camp.[16]		After the Communists took power in 1949 and established the People's Republic of China, laojiao (Re-education through labor) and laogai (Reform through labor) was (and still is in some cases) used as a way to punish political prisoners. They were intended not only for criminals, but also for those deemed to be counter-revolutionary (political and/or religious prisoners).[17] According to Al Jazeera special report on slavery, China has the largest penal labour system in the world today. Often these prisoners are used to produce products for export to the West.[18]		North Korean prison camps can be differentiated into internment camps for political prisoners (Kwan-li-so in Korean) and reeducation camps (Kyo-hwa-so in Korean).[19] According to human rights organizations, the prisoners face forced hard labor in all North Korean prison camps.[20][21] The conditions are harsh and life-threatening[22] and prisoners are subject to torture and inhumane treatment.[23][24]		Most Japanese prisoners are required to engage in prison labour, often in manufacturing parts which are then sold cheaply to private Japanese companies. This practice has raised charges of unfair competition since the prisoners' wages are far below market rate.		(Hard) penal labour does not exist in the Netherlands, but a light variant (Dutch: taakstraf) is one of the four primary punishments which can be imposed on a convicted offender.[25] The person who is sentenced to a taakstraf must perform some service to the community. The maximum punishment is 240 hours, according to article 22c, part 2 of Wetboek van Strafrecht.[26] The labour must be done in his free time. Reclassering Nederland keeps track of those who were sentenced to taakstraffen.[27][28]		Federal Prison Industries (UNICOR or FPI) is a wholly owned United States government corporation created in 1934 that uses penal labor from the Federal Bureau of Prisons (BOP) to produce goods and services. FPI is restricted to selling its products and services to federal government agencies and has no access to the commercial market.[29] State prison systems also use penal labor and have their own penal labor divisions.		The 13th Amendment of the American Constitution in 1865 explicitly allows penal labor as it states that "neither slavery nor involuntary servitude, except as a punishment for a crime whereof the party shall have been duly convicted, shall exist within the United States, or any place subject to their jurisdiction."[30][31] Unconvicted detainees awaiting trial cannot be forced to participate in forced rehabilitative labor programs in prison as it violates the Thirteenth Amendment.		The "convict lease" system became popular throughout the South following the American Civil War and into the 20th century. Since the impoverished state governments could not afford penitentiaries, they leased out prisoners to work at private firms. Reformers abolished convict leasing in the 20th-century Progressive Era. At the same time, labor has been required at many prisons.		In 1934, federal prison officials concerned about growing unrest in prisons lobbied to create a work program. Private companies got involved again in 1979, when Congress passed a law establishing the Prison Industry Enhancement Certification Program which allows employment opportunities for prisoners in some circumstances.[32]		Penal labor is sometimes used as a punishment in the U.S. military.[33]		Over the years, the courts have held that inmates may be required to work and are not protected by the constitutional prohibition against involuntary servitude.[34] Correctional standards promulgated by the American Correctional Association provide that sentenced inmates, who are generally housed in maximum, medium, or minimum security prisons, be required to work and be paid for that work.[35] Some states require, as with Arizona, all able-bodied inmates to work.[36]		Penal servitude was abolished for the Republic of Ireland by section 11(1) of the Criminal Law Act, 1997.[37]		Every enactment conferring a power on a court to pass a sentence of penal servitude in any case must be treated as an enactment empowering that court to pass a sentence of imprisonment for a term not exceeding the maximum term of penal servitude for which a sentence could have been passed in that case immediately before the commencement of the Criminal Law Act 1997.		In the case of any enactment in force on 5 August 1891 (the date on which section 1 of the Penal Servitude Act 1891 came into force) whereby a court had, immediately before the commencement of the Criminal Law Act 1997, power to pass a sentence of penal servitude, the maximum term of imprisonment may not exceed five years or any greater term authorised by the enactment.		Imprisonment with hard labour was abolished by section 11(3) of that Act.		Another historically significant example of forced labour was that of political prisoners and other persecuted people in labour camps, especially in totalitarian regimes since the 20th century where millions of convicts were exploited and often killed by hard labour and bad living conditions. For much of the history of the Soviet Union and other Communist states, political opponents of these governments were often sentenced to forced labour camps. The Soviet Gulag camps were a continuation of the punitive labour system of Imperial Russia known as katorga, but on a larger scale. Most inmates in the Gulag were ordinary criminals: between 1934 and 1953 there were only two years, 1946 and 1947, when the number counter-revolutionary prisoners exceeded that of ordinary criminals, partly because the Soviet state had amnestied 1 million ordinary criminals as part of the victory celebrations in 1945.[38]:343 At the height of the purges in the 1930s political prisoners made up 12 percent of the camp population; at the time of Stalin's death just over one-quarter. In the 1930s, many ordinary criminals were guilty of crimes that would have been punished with a fine or community service in the 1920s. They were victims of harsher laws from the early 1930s, driven, in part, by the need for more prison camp labor.[39]:930		Between 1930 and 1960, the Soviet regime created many Lager labour camps in Siberia and Central Asia.[40][41] There were at least 476 separate camp complexes, each one comprising hundreds, even thousands of individual camps.[42] It is estimated that there may have been 5-7 million people in these camps at any one time. In later years the camps also held victims of Joseph Stalin's purges as well as World War II prisoners. It is possible that approximately 10% of prisoners died each year.[43] Out of the 91,000 Germans captured alive after the Battle of Stalingrad, only 6,000 survived the Gulag and returned home.[44] Many of these prisoners, however, had died of illness contracted during the siege of Stalingrad and in the forced march into captivity.[45] More than half of all deaths occurred in 1941-1944, mostly as a result of the deteriorating food and medicine supplies caused by wartime shortages.[39]:927		Probably the worst of the camp complexes were the three built north of the Arctic Circle at Kolyma, Norilsk and Vorkuta.[46][47] Prisoners in Soviet labour camps were worked to death with a mix of extreme production quotas, brutality, hunger and the harsh elements.[48] In all, more than 18 million people passed through the Gulag,[49] with further millions being deported and exiled to remote areas of the Soviet Union.[50] The fatality rate was as high as 80% during the first months in many camps. Immediately after the start of the German invasion of the Soviet Union during World War II, the NKVD massacred about 100,000 prisoners who awaited deportation either to NKVD prisons in Moscow or to the Gulag. Michael McFaul, in his New York Times article of 11 June 2003, entitled 'Books of the Times; Camps of Terror, Often Overlooked',[51] has this to say about the state of contemporary dialogue on Soviet slavery:		It should now be known to all serious scholars that the camps began under Lenin and not Stalin. It should be recognized by all that people were sent to the camps not because of what they did, but because of who they were. Some may be surprised to learn about the economic function that the camps were designed to perform. Under Stalin, the camps were simply a crueler but equally inefficient way to exploit labor in the cause of building socialism than the one practiced outside the camps in the Soviet Union. Yet, even this economic role of the camps has been exposed before.		What is remarkable is that the facts about this monstrous system so well documented in Applebaum's book are still so poorly known and even, by some, contested. For decades, academic historians have gravitated away from event-focused history and toward social history. Yet, the social history of the gulag somehow has escaped notice. Compared with the volumes and volumes written about the Holocaust, the literature on the gulag is thin.		The article draws attention to Anne Applebaum's Pulitzer Prize-winning text Gulag: A History.[52]		In a number of penal systems, the inmates have the possibility of a job. This may serve several purposes. One goal is to give an inmate a meaningful way to occupy their prison time and a possibility of earning some money. It may also play an important role in resocialisation: inmates may acquire skills that would help them to find a job after release. It may also have an important penological function: reducing the monotony of prison life for the inmate, keeping inmates busy on productive activities, rather than, for example, potentially violent or antisocial activities, and helping to increase inmate fitness, and thus decrease health problems, rather than letting inmates succumb to a sedentary lifestyle.[53]		The classic occupation in 20th-century British prisons was sewing mailbags. This has diversified into areas such as engineering, furniture making, desktop publishing, repairing wheelchairs and producing traffic signs, but such opportunities are not widely available, and many prisoners who work perform routine prison maintenance tasks (such as in the prison kitchen) or obsolete unskilled assembly work (such as in the prison laundry) that is argued to be no preparation for work after release.[54] Classic 20th-century American prisoner work involved making license plates; the task is still being performed by inmates in certain areas.[55]		A significant amount of controversy has arisen with regard to the use of prison labour if the prison in question is privatized. Many of these privatized prisons exist in the Southern United States, where roughly 7% of the prison population are within privately owned institutions.[57] Goods produced through this penal labour are regulated through the Ashurst-Sumners Act which criminalizes the interstate transport of such goods.		The advent of automated production in the 20th and 21st century has reduced the availability of unskilled physical work for inmates.		ONE3ONE Solutions, formerly the Prison Industries Unit in Britain, has proposed the development of in-house prison call centers.[58]		
A no call, no show is an absence from the workforce without notifying the employer.[1] This form of absence is generally deemed inconsiderate and unprofessional, or passive.		When workers miss work, especially in jobs where one's workload would require to be substituted for the day (teachers, cashiers, servers, etc.), it is generally expected that they call in advance to inform of his or her absence so that their workload can be completed by the present workers. Many businesses have forms of punishments as a result of no call, no shows such as counseling statements, suspension, and possibly termination of employment.[2]		
A layoff[1] is the temporary suspension or permanent termination of employment of an employee or, more commonly, a group of employees (collective layoff)[2] for business reasons, such as personnel management or downsizing an organization. Originally, layoff referred exclusively to a temporary interruption in work, or employment[3] but this has evolved to a permanent elimination of a position in both British and US English,[1][not in citation given] requiring the addition of "temporary" to specify the original meaning of the word. A layoff is not to be confused with wrongful termination. Laid off workers or displaced workers are workers who have lost or left their jobs because their employer has closed or moved, there was insufficient work for them to do, or their position or shift was abolished (Borbely, 2011).[4][5] Downsizing in a company is defined to involve the reduction of employees in a workforce. Downsizing in companies became a popular practice in the 1980s and early 1990s as it was seen as a way to deliver better shareholder value as it helps to reduce the costs of employers (downsizing, 2015). Indeed, recent research on downsizing in the U.S.,[6] UK,[7] and Japan[8][9] suggests that downsizing is being regarded by management as one of the preferred routes to help declining organizations, cutting unnecessary costs, and improve organizational performance.[10] Usually a layoff occurs as a cost-cutting measure.						Euphemisms are often used to "soften the blow" in the process of firing and being fired.[11][12] The term "layoff" originally meant a temporary interruption in work[3] (and usually pay). The term became a euphemism for permanent termination of employment and now usually means that, requiring the addition of "temporary" to refer to the original meaning. Many other euphemisms have been coined for "(permanent) layoff", including "downsizing", "excess reduction", "rightsizing", "leveraging synergies", "delayering", "smartsizing", "redeployment", "workforce reduction", "workforce optimization", "simplification", "force shaping", "recussion", and "reduction in force" (also called "RIF", especially in the government employment sector). "Mass layoff" is defined by the United States Department of Labor as 50 or more workers laid off from the same company around the same time. "Attrition" implies that positions will be eliminated as workers quit or retire. "Early retirement" means workers may quit now yet still remain eligible for their retirement benefits later. While "redundancy" is a specific legal term in UK labour law. When an employer is faced with work of a particular type ceasing or diminishing at a particular location,[13] it may be perceived[by whom?] as obfuscation. Firings imply misconduct or failure while layoffs imply economic forces beyond the employer's and employees' control, especially in the face of a recession such as the one that began in the late 2000s.		RIF - A generic reduction in force, of undetermined method. Often pronounced like the word riff rather than spelled out. Sometimes used as a verb, as in "the employees were pretty heavily riffed".		eRIF – Layoff notice by email.		IRIF - Involuntary reduction in force - The employee(s) did not voluntarily choose to leave the company. This usually implies that the method of reduction involved either layoffs, firings, or both, but would not usually imply resignations or retirements. If the employee is fired rather than laid off, the term "with cause" may be appended to indicate that the separation was due to this employee's performance and/or behavior, rather than being financially motivated.		VRIF - Voluntary reduction in force - The employee(s) did play a role in choosing to leave the company, most likely through resignation or retirement. In some instances, a company may exert pressure on an employee to make this choice, perhaps by implying that a layoff or termination would otherwise be imminent, or by offering an attractive severance or early retirement package. Conversely, the company is not obliged to accept an employees decision and may not accept every employee who volunteers for a VRIF.		WFR - Work force reduction.		Following the recession of 2007-2008, the public sector has seen significantly smaller job growth in employment versus the private sector. As the public sector declines the demand for services from the private sector decline as well. Layoffs in the public sector have put limitations on the growth rate of the private sector, inevitably burdening the entire flow of markets.		The risk of being laid off varies depending on the workplace and country a person is working in. Unemployment compensation in any country or workplace is a function which has two main factors. The first factor of unemployment compensation depends on the distribution of unemployment benefits in a workplace outlined in an employee handbook. The second factor is the risk of inequality being conditioned upon the political regime type in the country an employee is working in.[14] For example, in Canada, Dorion (1995) states that white-collar workers who have been made redunant receive higher re-employment wages, than those in blue-collar occupations as there job is regarded as being of a higher status than blue collar workers, whereas in France, Margolis (1999, 2002) finds that workers who have been laid off have smaller unemployment periods and higher re-employment earnings in comparison with other unemployed people.[15] The amount of compensation will usually depend on what level the employee holds in the company.		Packages may also vary if the employee is laid off, or voluntarily quits in the face of a layoff (VRIF). The method of separation may have an effect on a former employee's ability to collect whatever form of unemployment compensation might be available in their jurisdiction. In many U.S. states, workers who are laid off can file an unemployment claim and receive compensation. Depending on local or state laws, workers who leave voluntarily are generally ineligible to collect unemployment benefits, as are those who are fired for gross misconduct. Also, lay-offs due to a firm's moving production overseas may entitle one to increased re-training benefits. Some companies in the United States utilize Supplemental Unemployment Benefits.[16] Since they were first introduced by organized labor and the Department of Labor in the early 1950s, and first issued in a Revenue Ruling by the IRS in 1956,[17] SUB-Pay Plans have enabled employers to supplement the receipt of state unemployment insurance benefits for employees that experience an involuntary layoff. By establishing severance payments as SUB-Pay benefits, the payments are not considered wages for FICA, FUTA, and SUI tax purposes, and employee FICA tax. To qualify for SUB-Pay benefits, the participant must be eligible for state unemployment insurance benefits and the separation benefit must be paid on a periodic basis. There have also been increasing concerns about the organizational effectiveness of the post-downsized ‘anorexic organization’. The benefits, which organizations claim to be seeking from downsizing, center on savings in labor costs, speedier decision making, better communication, reduced product development time, enhanced involvement of employees and greater responsiveness to customers (De Meuse et al. 1997, p. 168). However, some writers draw attention to the ‘obsessive’ pursuit of downsizing to the point of self-starvation marked by excessive cost cutting, organ failure and an extreme pathological fear of becoming inefficient. Hence ‘trimming’ and ‘tightening belts’ are the order of the day.[18]		Traditionally, layoffs directly affect the employee, however the employee terminated is not alone in this. Layoffs affect the workplace environment and the economy as well as the employee. Layoffs have a widespread effect and the three main components of layoff effects are in the workplace, to the employee, and effects to the economy.		Effects of layoffs in the workplace: Layoffs have remained the greatest way for a company to cut costs. Although from the employer's perspective a layoff is beneficial for the business, layoffs create an uncertainty in the workplace environment and lowers other employees' job security as well as creates an apprehension and fear of termination for the remaining employees, and subsequently lowers overall motivation in the workplace environment. According to Healing the Wounds: Overcoming the trauma of Layoffs and Revitalizing Downsized Organizations,[19] in the post-layoff environment, there is a need for empathy, tangibility, self-knowledge, and relentlessly seeking customers among the surviving employees. The remaining employees may have feelings of survivors guilt. Optimism is critical for rebuilding the workplace environment because employees look to their leaders for stability and predictability. No matter the position in an organization, employees will look for job security.		Effects of layoffs to the employee: Employees (or former employees in this case) can be affected in a couple of different ways. When an employee is laid off his or her general trust in long-term work may decrease, reducing expectations upon rehire. After an employee withstands a layoff, the effects can trickle into future employment and attitudes. Layoffs in the workplace often leave the former employer less inclined to trust future employers which can lead to behavioral conflicts among co-workers and management. Despite new employers not being responsible for a prior circumstances, job performance may still be affected by incoming employers. Many companies work to make layoffs as minimally burdensome to the employee. at times employers may layoff multiple people at once to soften the impact.		Effects of layoffs in the American Economy: Layoffs create lower job security overall, and an increased competitiveness for available and opening positions. Layoffs have generally two major effects on the economy and stockholders. The way layoffs affect the economy varies from the industry that is doing the layoffs and the size of the layoff. If an industry that employs a majority of a region (freight in the northeast for example) suffers and has to lay employees off, there will be mass unemployment in an economically rich area. This can have leave ripple effects nationwide. Unemployment is the biggest effect on the economy that can come from layoffs.		In (French speaking) Belgium the term Procédure Renault has become a synonym for the consultation process leading to mass redundancies, due to a controversial mass layoff and resultant legislation in the late 1990s. When an employee has been laid off in Australia their employer has to give them redundancy pay, which is also known as severance pay. The only time that a redundancy payment doesn’t have to be paid is if an employee is casual, working for a small business or has worked for a business for less than twelve months. The redundancy compensation payment for employees depends on the length of time an employee has worked for an employer which excludes unpaid leave. If an employer can’t afford the redundancy payment they are supposed to give their employee, once making them redundant, or they find their employee another job that is suitable for the employee. An employer is able to apply for a reduction in the amount of money they have to pay the employee they have made redundant. An employer can do this by applying to the Fair Work Commission for a redundancy payment reduction.[20] A layoff is also known as a retrenchment in (South African English). In the UK, permanent termination due to elimination of a position is usually called redundancy.[2] Certain countries (such as Belgium, Netherlands, Portugal, Spain, Italy, France and Germany), distinguish between leaving the company of one's own free will, in which case the person is not entitled to unemployment benefits, but may receive a onetime payment and leaving a company as part of a reduction in labour force size, in which case the person is entitled to them. A RIF reduces the number of positions, rather than laying off specific people, and is usually accompanied by internal redeployment.		
An employment counsellor, also known as a career development professional, advises, coaches, provides information to, and supports people who are planning, seeking and managing their life/work direction.						Career development professionals help clients of all ages:		Working with clients individually or in groups, career development professionals may:		Career development professionals may work in a variety of settings but usually work in offices where they can conduct private interviews with clients and in classrooms or boardrooms where they conduct group sessions. Depending on the organization, their hours of work may include some evening and weekend work.		Career development professionals need the following characteristics:		They should enjoy consulting with people, compiling information and working with clients to develop innovative solutions to problems.		Most career development professionals have post-secondary education in a related discipline such as psychology, education, social work or human resources development. Increasingly, employers are looking for applicants who have a certificate, diploma or degree in career development, or an equivalent combination of education and experience. One of the certifications that is available is the National Certified Counselor (NCC) credential.		
Sabbatical or a sabbatical (from Latin: sabbaticus, from Greek: sabbatikos (σαββατικός), from Hebrew: shabbat (שבת) (i.e., Sabbath), literally a "ceasing") is a rest from work, or a break, often lasting from two months to a year. The concept of sabbatical has a source in shmita, described in several places in the Bible. For example, in Leviticus 25, there is a commandment to desist from working the fields during the seventh year. Strictly speaking, this means a sabbatical would last one year.						The main Bible passage for sabbatical concepts is Genesis 2:2–3, in which God rested (literally, "ceased" from his labour) after creating the universe, and it is applied to people (Jew and Gentile, slave and free) and even to beasts of burden in one of the Ten Commandments (Exodus 20:8–11, reaffirmed in Deuteronomy 5:12–15). All agriculture was stopped during these periods, so even the land itself was given a Sabbath.		In recent times, "sabbatical" has come to mean any extended absence in the career of an individual in order to achieve something. In the modern sense, one takes sabbatical typically to fulfill some goal, e.g., writing a book or travelling extensively for research. Some universities and other institutional employers of scientists, physicians, and academics offer the opportunity to qualify for paid sabbatical as an employee benefit, called sabbatical leave. Some companies offer unpaid sabbatical for people wanting to take career breaks; this is a growing trend in the United Kingdom, with 20% of companies having a career break policy, and a further 10% considering introducing one.[1]		In British and Irish students' unions, particularly in higher education institutions, students can be elected to become sabbatical officers of their students' union, either taking a year out of their study (in the academic year following their election) or remaining at the institution for a year following completion of study. Sabbatical officers are usually provided with a living allowance or stipend.[citation needed]		
A golden handshake is a clause in an executive employment contract that provides the executive with a significant severance package in the case that the executive loses their job through firing, restructuring, or even scheduled retirement.[1] This can be in the form of cash, equity, and other benefits, and is often accompanied by an accelerated vesting of stock options. According to Investopedia, a golden handshake is similar to, but more generous than a golden parachute because it not only provides monetary compensation and/or stock options at the termination of employment, it includes the same severance packages executives would get at retirement.[2]		The term originated in Britain in the mid-1960s. It was coined by the city editor of the Daily Express, Frederick Ellis.[3] It later gained currency in New Zealand in the late 1990s over the controversial departures of various state sector executives.[4][5]		Typically, "golden handshakes" are offered only to high-ranking executives by major corporations and may entail a value measured in millions of dollars. Golden handshakes are given to offset the risk inherent in taking the new job, since high-ranking executives have a high likelihood of being fired and since a company requiring an outsider to come in at such a high level may be in a precarious financial position. Their use has caused some investors concern since they do not specify that the executive had to perform well. In some high-profile instances, executives cashed in their stock options, while under their stewardship their companies lost millions of dollars and thousands of workers were laid off.[examples needed]						Golden handshakes may create perverse incentives for top executives to facilitate the sale of the company they are managing by artificially reducing its stock price.		It is fairly easy for a top executive to reduce the price of their company's stock due to information asymmetry. The executive can accelerate accounting of expected expenses, delay accounting of expected revenue, engage in off balance sheet transactions to make the company's profitability appear temporarily poorer, or simply promote and report severely conservative (e.g. pessimistic) estimates of future earnings. Such seemingly adverse earnings news will be likely to (at least temporarily) reduce share price. (This is again due to information asymmetries, since it is more common for top executives to do everything they can to window dress their company's earnings forecasts).		A reduced share price makes a company an easier takeover target. When the company gets bought out (or taken private) - at a dramatically lower price - the takeover artist gains a windfall from the former top executive's actions to surreptitiously reduce share price. This can represent tens of billions of dollars (questionably) transferred from previous shareholders to the takeover artist. The former top executive is then rewarded with a golden handshake for presiding over the firesale that can sometimes be in the hundreds of millions of dollars for one or two years of work. (This is nevertheless an excellent bargain for the takeover artist, who will tend to benefit from developing a reputation of being very generous to parting top executives). This is just one example of some of the principal-agent / perverse incentive issues involved with golden handshakes and golden parachutes.		Similar issues occur when a publicly held asset or non-profit organization undergoes privatization. Top executives often reap tremendous monetary benefits when a government owned or non-profit entity is sold to private hands. Just as in the example above, they can facilitate this process by making the entity appear to be in financial crisis - this reduces the sale price (to the profit of the purchaser), and makes non-profits and governments more likely to sell. Ironically, it can also contribute to a public perception that private entities are more efficiently run, thus again reinforcing the political will to sell off public assets. Again, due to asymmetric information, policy makers and the general public see a government owned firm that was a financial 'disaster' - miraculously turned around by the private sector (and typically resold) within a few years.		
Occupational stress is stress related to one's job. Occupational stress often stems from unexpected responsibilities and pressures that do not align with a person's knowledge, skills, or expectations, inhibiting one's ability to cope. Occupational stress can increase when workers do not feel supported by supervisors or colleagues, or feel as if they have little control over work processes.[1]						Because stress results from the complex interactions between a large system of interrelated variables, there are several psychological theories and models that address occupational stress.[2][3][4]		Person Environment Fit Model: This model "suggests that the match between a person and their work environment is key in influencing their health. For healthy conditions, it is necessary that employees’ attitudes, skills, abilities and resources match the demands of their job, and that work environments should meet workers’ needs, knowledge, and skills potential. Lack of fit in either of these domains can cause problems, and the greater the gap or misfit (either subjective or objective) between the person and their environment, the greater the strain as demands exceed abilities, and need exceeds supply. These strains can relate to health related issues, lower productivity, and other work problems. Defense mechanisms, such as denial, reappraisal of needs, and coping, also operate in the model, to try and reduce subjective misfit" [4]		Job Characteristics Model: This model "focuses on important aspects of job characteristics, such as skill variety, task identity, task significance, autonomy, and feedback. These characteristics are proposed to lead to ‘critical psychological states’ of experienced meaningfulness, and experienced responsibility and knowledge of outcomes. It is proposed that positive or negative work characteristics give rise to mental states which lead to corresponding cognitive and behavioral outcomes, e.g. motivation, satisfaction, absenteeism, etc. In conjunction with the model, Hackman and Oldham (1980) developed the Job Diagnostic Survey, a questionnaire for job analysis, which implies key types of job-redesign including combining tasks, creating feedback methods, job enrichment, etc." [4]		Diathesis-Stress Model: This model looks at behaviors as a susceptibility burden together with stress from life experiences.[5][6] It is useful to distinguish stressful job conditions or stressors from an individual's reactions or strains.[7] Strains can be mental, physical or emotional. Occupational stress can occur when there is a discrepancy between the demands of the environment/workplace and an individual’s ability to carry out and complete these demands.[8][9] Often a stressor can lead the body to have a physiological reaction that can strain a person physically as well as mentally. A variety of factors contribute to workplace stress such as excessive workload, isolation, extensive hours worked, toxic work environments, lack of autonomy, difficult relationships among coworkers and management, management bullying, harassment and lack of opportunities or motivation to advancement in one’s skill level.[10]		Jobs-Demand Resources Model: This model posits that strain are a response to imbalance between demands of one's job and the resources he or she has to deal with those demands.		Effort-Reward Imbalance Model: This model focuses on the reciprocal relationship between efforts and rewards at work. "More specifically, the ERI Model claims that work characterized by both high efforts and low rewards represents a reciprocity deficit between high ‘costs’ and low ‘gains’, which could elicit negative emotions in exposed employees. The accompanying feelings may cause sustained strain reactions. So, working hard without receiving adequate appreciation or being treated fairly are examples of a stressful imbalance. Another assumption of the ERI Model concerns individual differences in the experience of effort-reward imbalance. It is assumed that employees characterized by a motivational pattern of excessive job-related commitment and a high need for approval (i.e., overcommitment) will respond with more strain reactions to an effort-reward imbalance, in comparison with less overcommitted people." [12]		Sources of occupational stress come from:[10]		These individual sources demonstrate that stress can occur specifically when a conflict arises from the job demands of the employee and the employee itself. If not handled properly, the stress can become distress.[13]		Distress is a prevalent and costly problem in today's workplace. About one-third of workers report high levels of stress.[8] 20-30% of workers in different sectors of the European Union reported in 2007 that they believed work-related stress was potentially affecting their health.[14] Three-quarters of employees believe the worker has more on-the-job stress than a generation ago.[15] In Great Britain, a sixth of the workforce experiences occupational stress every year.[14] Evidence also suggests that distress is the major cause of turnover in organizations.[8] With continued distress at the workplace, workers will develop psychological and physiological dysfunctions and decreased motivation in excelling in their position.[10] Increased levels of job stress are determined by the awareness of having little control but lots of demands in the work area.[16] Occupational stress and its sequelae represent the majority of work-related illnesses causing missed work days.[14] Those in the protective services, transportation and materials moving, building grounds cleaning and maintenance, and healthcare are more susceptible to both work injuries and illnesses, as well as work-related stress.[17]		Stress-related disorders encompass a broad array of conditions, including psychological disorders (e.g., depression, anxiety, post-traumatic stress disorder) and other types of emotional strain (e.g., dissatisfaction, fatigue, tension, etc.), maladaptive behaviors (e.g., aggression, substance abuse), and cognitive impairment (e.g., concentration and memory problems). In turn, these conditions may lead to poor work performance, higher absenteeism, less work productivity or even injury.[10][14] "If untreated, consistently high stress can become a chronic condition, which can exacerbate existing mental health conditions and chronic physical conditions (diabetes, hypertension, weak immune system). These conditions not only diminish the well-being of workers and increase the employer's health benefits expenses, they contribute to increased injury incidence. Consistently high levels of stress increase the risk of occupational injury. A study of light/short haul truckers, a group that experiences high rates of injury and mental health issues, found that frequent stress increased the odds of occupational injury by 350%." [17]		Job stress is also associated with various biological reactions that may lead ultimately to compromised health, such as cardiovascular disease,[14][18] or in extreme cases death. Due to the high pressure and demands in the work place the demands have been shown to be correlated with increased rates of heart attack, hypertension and other disorders. In New York, Los Angeles, and London, among other municipalities, the relationship between job stress and heart attacks is acknowledged.[19]		Problems at work are more strongly associated with health complaints than are any other life stressor-more so than even financial problems or family problems.[20] Occupational stress accounts for more than 10% of work-related health claims.[21] Many studies suggest that psychologically demanding jobs that allow employees little control over the work process increase the risk of cardiovascular disease. Research indicates that job stress increases the risk for development of back and upper-extremity musculoskeletal disorders.[22] Other disorders that can be caused or exacerbated by occupational stress include sleep disorders, headache, mood disorders, upset stomach, hypertension, high cholesterol, autoimmune disease, cardiovascular disease, depression, and anxiety. Stress at work can also increase the risk of acquiring an infection and the risk of accidents at work.[23]		High levels of stress are associated with substantial increases in health service utilization.[8] Workers who report experiencing stress at work also show excessive health care utilization. In a 1998 study of 46,000 workers, health care costs were nearly 50% greater for workers reporting high levels of stress in comparison to “low risk” workers. The increment rose to nearly 150%, an increase of more than $1,700 per person annually, for workers reporting high levels of both stress and depression.[24] Health care costs increase by 200% in those with depression and high occupational stress.[23] Additionally, periods of disability due to job stress tend to be much longer than disability periods for other occupational injuries and illnesses.[25]		Physiological reactions to stress can have consequences for health over time. Researchers have been studying how stress affects the cardiovascular system, as well as how work stress can lead to hypertension and coronary artery disease. These diseases, along with other stress-induced illnesses tend to be quite common in American work-places.[26] There are four main physiological reactions to stress:[27]		Men and women are exposed to many of the same stressors.[28] Although men and women might not differ in overall strains, women are more likely to experience psychological distress, whereas men experience more physical strain. Desmarais and Alksnis suggest two explanations for the greater psychological distress of women. First, the genders may differ in their awareness of negative feelings, leading women to express and report strains, whereas men deny and inhibit such feelings. Second, the demands to balance work and family result in more overall stressors for women that leads to increased strain.[28]		The Kenexa Research Institute released a global survey of almost 30,000 workers which showed that females suffered more workplace distress than their male counterparts. According to the survey, women's stress level were 10% higher for those in supervisory positions, 8% higher stress in service and production jobs than men, and 6% higher in middle and upper management than men in the same position.[29]		Job stress results from various interactions of the worker and the environment of the work they perform their duties. Location, gender, environment, and many other factors contribute to the buildup of stress. Job stress results from the interaction of the worker and the conditions of work. Views differ on the importance of worker characteristics versus working conditions as the primary cause of job stress. The differing viewpoints suggest different ways to prevent stress at work. Differences in individual characteristics such as personality and coping skills can be very important in predicting whether certain job conditions will result in stress. In other words, what is stressful for one person may not be a problem for someone else. This viewpoint underlies prevention strategies that focus on workers and ways to help them cope with demanding job conditions.[8] In general, occupational stress is caused by a mismatch between perceived effort and perceived reward, and/or a sense of low control in a job with high demands. Low social support at work and job insecurity can also increase occupational stress.[14] Psychosocial stressors are a major cause of occupational stress.[23]		Although the importance of individual differences cannot be ignored, scientific evidence suggests that certain working conditions are stressful to most people. Such evidence argues for a greater emphasis on working conditions as the key source of job stress, and for job redesign as a primary prevention strategy.[8] Large surveys of working conditions, including conditions recognized as risk factors for job stress, were conducted in member states of the European Union in 1990, 1995, and 2000. Results showed a time trend suggesting an increase in work intensity. In 1990, the percentage of workers reporting that they worked at high speeds at least one-quarter of their working time was 48%, increasing to 54% in 1995 and to 56% in 2000. Similarly, 50% of workers reported they work against tight deadlines at least one-fourth of their working time in 1990, increasing to 56% in 1995 and 60% in 2000. However, no change was noted in the period 1995–2000 (data not collected in 1990) in the percentage of workers reporting sufficient time to complete tasks.[30]		In an occupational setting, dealing with workload can be stressful and serve as a stressor for employees. There are three aspects of workload that can be stressful.		Workload as a work demand is a major component of the demand-control model of stress.[32] This model suggests that jobs with high demands can be stressful, especially when the individual has low control over the job. In other words, control serves as a buffer or protective factor when demands or workload is high. This model was expanded into the demand-control-support model that suggests that the combination of high control and high social support at work buffers the effects of high demands.[33]		As a work demand, workload is also relevant to the job demands-resources model of stress that suggests that jobs are stressful when demands (e.g., workload) exceed the individual's resources to deal with them.[34]		A substantial percentage of Americans work very long hours. By one estimate, more than 26% of men and more than 11% of women worked 50 hours per week or more in 2000. These figures represent a considerable increase over the previous three decades, especially for women. According to the Department of Labor, there have been a rise in increasing amount of hours in the work place by employed women, an increase in extended work weeks (>40 hours) by men, and a considerable increase in combined working hours among working couples, particularly couples with young children.[35][36]		A person's status in the workplace can also affect levels of stress. While workplace stress has the potential to affect employees of all categories; those who have very little influence to those who make major decisions for the company. However, less powerful employees (that is, those who have less control over their jobs) are more likely to suffer stress than powerful workers. Managers as well as other kinds of workers are vulnerable to work overload.[37]		Economic factors that employees are facing in the 21st century have been linked to increased stress levels. Researchers and social commentators have pointed out that the computer and communications revolutions have made companies more efficient and productive than ever before. This boon in productivity however, has caused higher expectations and greater competition, putting more stress on the employee.[26]		The following economic factors may lead to workplace stress:		Bullying in the workplace can also contribute to stress. This can be broken down into five different categories:[10]		This in effect can create a hostile work environment for the employees that, which in turn, can affect their work ethic and contribution to the organization.[38]		Thomas suggests that there tends to be a higher level of stress with people who work or interact with a narcissist, which in turn increases absenteeism and staff turnover.[39] Boddy finds the same dynamic where there is corporate psychopath in the organisation.[40]		Interpersonal conflict among people at work has been shown to be one of the most frequently noted stressors for employees.[41][42] Conflict has been noted to be an indicator of the broader concept of workplace harassment.[43] It relates to other stressors that might co-occur, such as role conflict, role ambiguity, and workload. It also relates to strains such as anxiety, depression, physical symptoms, and low levels of job satisfaction.[43]		Women are more likely than men to experience sexual harassment, especially for those working in traditionally masculine occupations. In addition, a study indicated that sexual harassment negatively affects workers' psychological well-being.[19][44] Another study found that level of harassment at workplaces lead to differences in performance of work related tasks. High levels of harassment were related to the worst outcomes, and no harassment was related to least negative outcomes. In other words, women who had experienced a higher level of harassment were more likely to perform poorly at workplaces.[44]		Lower occupational groups are at higher risk of work-related ill health than higher occupational groups. This is in part due to adverse work and employment conditions. Furthermore, such conditions have greater effects on ill-health to those in lower socio-economic positions.[45]		Stressful working conditions can lead to three types of strains: Behavioral (e.g., absenteeism or poor performance), physical (e.g., headaches or coronary heart disease), and psychological (e.g., anxiety or depressed mood).[46][47] Physical symptoms that may occur because of occupational stress include fatigue, headache, upset stomach, muscular aches and pains, weight gain or loss, chronic mild illness, and sleep disturbances. Psychological and behavioral problems that may develop include anxiety, irritability, alcohol and drug use, feeling powerless and low morale.[48][47] The spectrum of effects caused by occupational stress includes absenteeism, poor decision making, lack of creativity, accidents, organizational breakdown or even sabotage.[49] If exposure to stressors in the workplace is prolonged, then chronic health problems can occur including stroke. An examination was of physical and psychological effects of workplace stress was conducted with a sample of 552 female blue collar employees of a microelectronics facility. It was found that job-related conflicts were associated with depressive symptoms, severe headaches, fatigue, rashes, and other multiple symptoms.[50] Studies among the Japanese population specifically showed a more than 2-fold increase in the risk of total stroke among men with job strain (combination of high job demand and low job control).[51] Those in blue-collar or maual labor jobs are more likely to develop heart disease compared to those in white-collar jobs.[52] Along with the risk of stroke, stress can raise the risk of high blood pressure, immune system dysfunction, coronary artery disease.[14][53] Prolonged occupational stress can lead to occupational burnout.[14] Occupational stress can also disrupt relationships.[23]		The effects of job stress on chronic diseases are more difficult to ascertain because chronic diseases develop over relatively long periods of time and are influenced by many factors other than stress. Nonetheless, there is some evidence that stress plays a role in the development of several types of chronic health problems—including cardiovascular disease, musculoskeletal disorders, and psychological disorders.[8][14] Job stress and strain has been associated with poor mental health and wellbeing over a 12-year period.[47]		Occupational stress has negative effects for organizations and employers. Occupational stress is the cause of approximately 40% of turnover and 50% of workplace absences. The annual cost of occupational stress and its effects in the US is estimated to be over $60 billion to employers and $250–300 billion to the economy.[23]		A combination of organizational change and stress management is often the most useful approach for preventing stress at work.[8][14] Both organizations and employees can employ strategies at organizational and individual levels.[14] Generally, organizational level strategies include job procedure modification and employee assistance programs (EAP). Individual level strategies include taking vacation. Getting a realistic job preview to understand the normal workload and schedules of the job will also help people to identify whether or not the job fit them.		How an Organization Can Prevent Job Stress[54]		An insurance company conducted several studies on the effects of stress prevention programs in hospital settings. Program activities included (1) employee and management education on job stress, (2) changes in hospital policies and procedures to reduce organizational sources of stress, and (3) the establishment of employee assistance programs. In one study, the frequency of medication errors declined by 50% after prevention activities were implemented in a 700-bed hospital. In a second study, there was a 70% reduction in malpractice claims in 22 hospitals that implemented stress prevention activities. In contrast, there was no reduction in claims in a matched group of 22 hospitals that did not implement stress prevention activities.[57]		Telecommuting is another way organizations can help reduce stress for their workers. Employees defined telecommuting as "an alternative work arrangement in which employees perform tasks elsewhere that are normally done in a primary or central workplace, for at least some portion of their work schedule, using electronic media to interact with others inside and outside the organization." One reason that telecommuting gets such high marks is that it allows employees more control over how they do their work. Telecommuters reported more job satisfaction and less desire to find a new job. Employees that worked from home also had less stress, improved work/life balance and higher performance rating by their managers.[58]		A systematic review of stress-reduction techniques among healthcare workers found that cognitive behavioral training lowered emotional exhaustion and feelings of lack of personal accomplishment.[59]		Signs and symptoms of excessive job and workplace stress include:[60]		Both yoga and mindful-based stress reduction have been shown to reduce work-related stress.[61][62] Nursings participating in cognitive behavioral interventions less perceived stress, a greater sense of coherence, and increased mood.		Expanding Research on Stress: Contemporary opinions hold that jobs designed to support skill variety, task identity, significance, autonomy, and feedback, while providing for existence and growth needs, will sustain a healthier, greater satisfied workforce.		For team-oriented work environments, Patrick Lencioni’s[63] The Five Dysfunctions of a Team profiles the behavior of cohesive teams:		1. They trust one another		2. They engage in unfiltered conflict around ideas		3. They commit to decisions and plans of action		4. They hold one another accountable for delivering against those plans		5. They focus on the achievement of collective results.		For immediate individual stress management, rudimentary mental coping strategies may be adopted in the work environment.		
Workplace incivility has been defined as low-intensity deviant behavior with ambiguous intent to harm the target. Uncivil behaviors are characteristically rude and discourteous, displaying a lack of regard for others.[1] The authors hypothesize there is an "incivility spiral" in the workplace made worse by "asymmetric global interaction".[1]		Incivility is distinct from violence. The reduction of workplace incivility is a fertile area for applied psychology research.						A summary of research conducted in Europe suggests that workplace incivility is common there.[2] In research on more than 1000 U.S. civil service workers, Cortina, Magley, Williams, and Langhout (2001) found that more than 70% of the sample experienced workplace incivility in the past five years.[2] Similarly, Laschinger, Leiter, Day, and Gilin found that among 612 staff nurses, 67.5% had experienced incivility from their supervisors and 77.6% had experienced incivility from their coworkers.[3] In addition, they found that low levels of incivility along with low levels of burnout and an empowering work environment were significant predictors of nurses' experiences of job satisfaction and organizational commitment.[3] Incivility was associated with occupational stress and reduced job satisfaction. Other research shows that workplace incivility relates to job stress, depression, and life satisfaction as well.[4]		After conducting more than six hundred interviews with "employees, managers, and professionals in varying industries across the United States" and collecting "survey data from an additional sample of more than 1,200 employees, managers, and professionals representing all industrial categories in the United States and Canada", researchers Christine M. Pearson and Christine L. Porath wrote in 2004 that "The grand conclusion: incivility does matter. Whether its costs are borne by targets, their colleagues, their organizations, their families, their friends outside work, their customers, witnesses to the interactions, or even the instigators themselves, there is a price to be paid for uncivil encounters among coworkers."[5] Citing previous research (2000) Pearson writes that "more than half the targets waste work time worrying about the incident or planning how to deal with or avert future interactions with the instigator. Nearly 40 percent reduced their commitment to the organization; 20 percent told us that they reduced their work effort intentionally as a result of the incivility, and 10 percent of targets said that they deliberately cut back the amount of time they spent at work."[6]		Studies suggest that social support can buffer the negative effects of workplace incivility. Individuals who felt emotionally and organizationally socially supported reported fewer negative consequences (less depression and job stress, and higher in job and life satisfaction) of workplace incivility compared to those who felt less supported.[4] Research also suggests that the negative effects of incivility can be offset by feelings of organizational trust and high regard for one’s workgroup.[7]		Examples at the more subtle end of the spectrum include:[1]		Somewhere between the extremes are numerous everyday examples of workplace rudeness and impropriety such as:[8]		Other overt forms of incivility might include emotional tirades and losing one's temper.[8]		A number of studies have shown that women are more likely than men to experience workplace incivility and its associated negative outcomes.[10][11] Research also shows that employees who witness incivility directed toward female coworkers have lower psychological wellbeing, physical health, and job satisfaction, which in turn relates lowered commitment toward the organization and higher job burnout and turnover intentions.[12] Miner-Rubino and Cortina (2004) found that observing incivility toward women related to increased work withdrawal for both male and female employees, especially in work contexts where there were more men.[13]		Other research shows that incivility directed toward same-gender coworkers tends to lead to more negative emotionality for observers.[14] While both men and women felt anger, fear, and anxiety arising from same-gender incivility, women additionally reported higher levels of demoralization after witnessing such mistreatment.[14] Furthermore, the negative effects of same-gender incivility were more pronounced for men observing men mistreating other men than for women observing women mistreating other women.[14] Miner and Eischeid (2012) suggest this disparity reflects men perceiving uncivil behavior as a “clear affront to the power and status they have learned to expect for their group in interpersonal interactions.”[14]		Motherhood status has also been examined as a possible predictor of being targeted for incivility in the workplace.[15] This research shows that mothers with three or more children report more incivility than women with two, one, or zero children.[15] Fathers, on the other hand, report more incivility than men without children, but still less than mothers. While motherhood appears to predict increases in workplace incivility, results also showed that the negative outcomes associated with incivility were mitigated by motherhood status. Fatherhood status, on the other hand, did not mitigate the relationship between incivility and outcomes. Childless women reported more workplace incivility than childless men, and showed a stronger relationship between incivility and negative outcomes than childless men, mothers, and fathers.[15]		Workplace bullying overlaps to some degree with workplace incivility but tends to encompass more intense and typically repeated acts of disregard and rudeness. Negative spirals of increasing incivility between organizational members can result in bullying,[16] but isolated acts of incivility are not conceptually bullying despite the apparent similarity in their form and content. In case of bullying, the intent of harm is less ambiguous, an unequal balance of power (both formal and informal) is more salient, and the target of bullying feels threatened, vulnerable and unable to defend himself or herself against negative recurring actions.[17][18]		Another related notion is petty tyranny, which also involves a lack of consideration towards others, although petty tyranny is more narrowly defined as a profile of leaders and can also involve more severe forms of abuse of power and of authority.[1]		
Outplacement is the support service provided by responsible organizations, keen to support individuals who are exiting the business (voluntarily or involuntarily) - to help former employees transition to new jobs and help them re-orient themselves in the job market.[1] A consultancy firm usually provides the outplacement services which are paid for by the former employer and are achieved usually through practical advice, training materials and workshops. Some companies may offer psychological support.		Outplacement is either delivered through individual one-on-one sessions or in a group format. Topics include career guidance, career evaluation, job search skills, targeting the job market, resume writing, interview preparation, developing networks, and negotiation.[2]		Consultants support individuals seeking a new job, and also those looking to start a new business, retire, or structure a portfolio of activities. Programs have time limits, ranging from a few months to more extended periods, such as 12 months and are offered at all levels of the organization, from workers to corporate employees.		Outplacement provides former employees structure and guidance towards their new career option, and preserves the morale of those who remain in the Company who see that colleagues are given the necessary support when they leave the company.								Although the term outplacement was coined more than 30 years ago by the founder of Challenger, Gray & Christmas, a Chicago-based career consultancy, new entrants such as VelvetJobs have taken over the field through innovation and modern offerings.		Forbes reported in 2016 on the transformation of the outplacement industry:		Meet VelvetJobs: Transforming The $5 Billion Career Transition Industry[3]		With the increased rates of downsizing, rightsizing, redundancies and layoffs, particularly during the 1980s and 1990s,[4] businesses increasingly found a need for some form of assistance in reducing the trauma of redundancy for both departing employees and those who remain. Indeed, research shows that losing one's job is one of the most stressful experiences a person can face other than death and divorce.[5]		The Wall Street Journal reported in 2009 that U.S. corporations were dissatisfied with the quality of outplacement services they received:		As demand rises in the $4 billion-a-year outplacement business, providers increasingly offer standardized services, which some workers say offer little value. Businesses anxious to shed former employees quickly and cheaply impose time limits that hamper effectiveness. Few employers track whether outplacement works.[6]		The best outplacement programs provide ongoing support, as it is often the case that after the individual has not been able to find a new job after searching for a number of weeks that they need the most help. Many companies will stop providing support after an allotted time although some companies provide support for as long as the individual needs it. Some also track the success rate of re-employment to help evaluate their services.		In the 2009 movie Up in the Air George Clooney plays the lead role of Ryan Bingham, an unpleasant man who travels the world to dismiss employees on behalf of employers who didn't want to do so themselves. Many people have unfortunately confused this with the role of an outplacement consultant, who are much more focused on delivering benefits to employees than the character in the film! .		
Employee benefits and (especially in British English) benefits in kind (also called fringe benefits, perquisites, or perks) include various types of non-wage compensation provided to employees in addition to their normal wages or salaries.[1] In instances where an employee exchanges (cash) wages for some other form of benefit is generally referred to as a 'salary packaging' or 'salary exchange' arrangement. In most countries, most kinds of employee benefits are taxable to at least some degree.		Examples of these benefits include: housing (employer-provided or employer-paid) furnished or not, with or without free utilities; group insurance (health, dental, life etc.); disability income protection; retirement benefits; daycare; tuition reimbursement; sick leave; vacation (paid and non-paid); social security; profit sharing; employer student loan contributions; conveyance; domestic help (servants); and other specialized benefits.		The purpose of employee benefits is to increase the economic security of staff members, and in doing so, improve worker retention across the organization.[2] As such, it is one component of reward management.		The term perks is often used colloquially to refer to those benefits of a more discretionary nature. Often, perks are given to employees who are doing notably well and/or have seniority. Common perks are take-home vehicles, hotel stays, free refreshments, leisure activities on work time (golf, etc.), stationery, allowances for lunch, and—when multiple choices exist—first choice of such things as job assignments and vacation scheduling. They may also be given first chance at job promotions when vacancies exist.						The Bureau of Labor Statistics,[3] like the International Accounting Standards Board,[4] defines employee benefits as forms of indirect expenses. Robert Klonoski, Adjunct Professor of Business at Mary Baldwin University, argued that, while accounting for benefits is a valuable consideration, it does little to explain the reasons why organizations would want to offer benefits to their employees.[5] Managers tend to view compensation and benefits in terms of their ability to attract and retain employees, as well as in terms of their ability to motivate them.[6]		Employees – along with potential employees – tend to view benefits that are mandated by regulation differently from benefits that are discretionary, that is, those that are not mandated but are simply designed to make a compensation package more attractive. Benefits that are mandated are thought of as creating employee rights or entitlements, while discretionary benefits are intended to inspire employee loyalty and increase job satisfaction.[7] Based on this, Klonoski proposed definitions of both discretionary and non-discretionary benefits as a manager would view them: "Discretionary employee benefits are those organizational programs and practices that are not mandated by regulation or market forces, and that improve employee performance by increasing job satisfaction and/or organizational loyalty. Non-discretionary employee benefits are those organizational programs and practices that are mandated by regulation or market forces, and that create an employee right, entitlement, or expectation."[5]:62		Viewed from this perspective, things like casual dress codes, flextime, and telecommuting can be considered employee "benefits" whether or not they produce an expense to the organization offering them. If employees prefer to dress casually or to have flexible hours or to work from home they may be inclined to seek and less likely to leave employers that offer these things.[5]:63		Employee benefits in Canada usually refer to employer sponsored life, disability, health, and dental plans. Such group insurance plans are a top-up to existing provincial coverage. An employer provided group insurance plan is coordinated with the provincial plan in the respective province or territory, therefore an employee covered by such a plan must be covered by the provincial plan first. The life, accidental death and dismemberment and disability insurance component is an employee benefit only. Some plans provide a minimal dependent life insurance benefit as well. The healthcare plan may include any of the following: hospital room upgrades (Semi-Private or Private), medical services/supplies and equipment, travel medical (60 or 90 days per trip), registered therapists and practitioners (i.e. physiotherapists, acupuncturists, chiropractors, etc.), prescription requiring drugs, vision (eye exams, contacts/lenses), and Employee Assistance Programs. The dental plan usually includes Basic Dental (cleanings, fillings, root canals), Major Dental (crowns, bridges, dentures) and/or Orthodontics (braces).		Other than the employer sponsored health benefits described above, the next most common employee benefits are group savings plans (Group RRSPs and Group Profit Sharing Plans), which have tax and growth advantages to individual saving plans.		Employee benefits in the United States include relocation assistance; medical, prescription, vision and dental plans; health and dependent care flexible spending accounts; retirement benefit plans (pension, 401(k), 403(b)); group-term life and long term care insurance plans; legal assistance plans; medical second opinion programs, adoption assistance; child care benefits and transportation benefits; Paid time off (PTO) in the form of vacation and sick pay. Benefits may also include formal or informal employee discount programs that grant workers access to specialized offerings from local and regional vendors (like movies and theme park tickets, wellness programs, discounted shopping, hotels and resorts, and so on).[8]		Companies that offer these types of work-life perks seek to raise employee satisfaction, corporate loyalty, and worker retention by providing valuable benefits that go beyond a base salary figure.[8] Fringe benefits are also thought of as the costs of retaining employees other than base salary.[9] The term "fringe benefits" was coined by the War Labor Board during World War II to describe the various indirect benefits which industry had devised to attract and retain labor when direct wage increases were prohibited.		Some fringe benefits (for example, accident and health plans, and group-term life insurance coverage up to US$50,000) may be excluded from the employee's gross income and, therefore, are not subject to federal income tax in the United States. Some function as tax shelters (for example, flexible spending, 401(k), or 403(b) accounts). These benefit rates often change from year to year and are typically calculated using fixed percentages that vary depending on the employee’s classification.		Normally, employer-provided benefits are tax-deductible to the employer and non-taxable to the employee. The exception to the general rule includes certain executive benefits (e.g. golden handshake and golden parachute plans) or those that exceed federal or state tax-exemption standards.		American corporations may also offer cafeteria plans to their employees. These plans offer a menu and level of benefits for employees to choose from. In most instances, these plans are funded by both the employees and by the employer(s). The portion paid by employees is deducted from their gross pay before federal and state taxes are applied. Some benefits would still be subject to the Federal Insurance Contributions Act tax (FICA), such as 401(k)[10] and 403(b) contributions; however, health premiums, some life premiums, and contributions to flexible spending accounts are exempt from FICA.		If certain conditions are met, employer provided meals and lodging may be excluded from an employee's gross income. If meals are furnished (1) by the employer; (2) for the employer's convenience; and (3) provided on the business premises of the employer they may be excluded from the employee's gross income per section 119(a). In addition, lodging furnished by the employer for its convenience on the business premise of the employer (which the employee is required to accept as a condition of employment) is also excluded from gross income. Importantly, section 119(a) only applies to meals or lodging furnished "in kind." Therefore, cash allowances for meals or lodging received by an employee are included in gross income.		Employee benefits provided through ERISA (Employee Retirement Income Security Act) are not subject to state-level insurance regulation like most insurance contracts, but employee benefit products provided through insurance contracts are regulated at the state level.[11] However, ERISA does not generally apply to plans by governmental entities, churches for their employees, and some other situations.[12]		Under the Obamacare or ACA’s Employer Shared Responsibility provisions, certain employers, known as applicable large employers are required to offer minimum essential coverage that is affordable to their full-time employees or else make the employer shared responsibility payment to the IRS.[13]		Private firms in the US have come up with certain unusual perquisites.[14]		In the United States paid time off, in the form of vacation days or sick days, is not required by federal or state law.[12] Despite that fact, many United States businesses offer some form of paid leave. In the United States, 86% of workers at large businesses and 69% of employees at small business receive paid vacation days.[15]		In the UK, Employee Benefits are categorised by three terms: Flexible Benefits (Flex) and Flexible Benefits Packages, Voluntary Benefits and Core Benefits.		Core Benefits is the term given to benefits which all staff enjoy, such as pension, life insurance, income protection, and holiday.		Flexible benefits, often called a "flex scheme", is where employees are allowed to choose how a proportion of their remuneration is paid or they are given a benefits budget by their employer to spend. Currently around a third of UK employers operate such a scheme.[16] How flexible benefits schemes are structured has remained fairly consistent over the years, although the definition of flex has changed quite a lot since it first arrived in the UK in the 1980s. When flex first emerged, it was run as a formal scheme for a set contract period, through which employees could opt in and out of a selection of employer-paid benefits, select employee-paid benefits, or take the cash. In recent years increasing numbers of UK companies have used the tax and national insurance savings gained through the implementation of salary sacrifice benefits to fund the implementation of flexible benefits. In a salary sacrifice arrangement an employee gives up the right to part of the cash remuneration due under their contract of employment. Usually the sacrifice is made in return for the employer's agreement to provide them with some form of non-cash benefit. The most popular types of salary sacrifice benefits include childcare vouchers and pensions.		A number of external consultancies exist that enable organizations to manage Flex packages and they centre around the provision of an Intranet or Extranet website where employees can view their current flexible benefit status and make changes to their package. Adoption of flexible benefits has grown considerably, with 62% of employers in a 2012 survey offering a flexible benefit package, and a further 21% planning to do so in the future[17] This has coincided with increased employee access to the internet and studies suggesting that employee engagement can be boosted by their successful adoption.[18] Suppliers of flexible benefits technology in the UK include Mercer, Jardine Lloyd Thompson Group, Staffcare, Aon Hewitt, Thomsons Online Benefits and Vebnet.[19] The valuation of Thomsons Online Benefits was at "close to £100m" in 2013.[20]		Voluntary Benefits is the name given to a collection of benefits that employees choose to opt-in for and pay for personally, although, as with flex plans, many employers make use of salary sacrifice schemes where the employee reduces their salary in exchange for the employer paying for the perk. These tend to include benefits such as the government-backed (and therefore tax-efficient) cycle to work, pension contributions and childcare vouchers (providers include Edenred, Employers For Childcare Vouchers, Busybees, Sodexo, Fideliti, KiddiVouchers, Co-operative Employee Benefits and Early Years Vouchers Ltd) and also specially arranged discounts on retail and leisure vouchers, gym membership and discounts at local shops and restaurants (providers include Xexec). They can be run in-house or arranged by an external employee benefits consultant.		In a number of countries (e.g., Australia, New Zealand and Pakistan), the 'fringe benefits' are subject to the Fringe Benefits Tax (FBT), which applies to most, although not all, fringe benefits. In India, the fringe benefits tax was abolished in 2009.[21]		In the United States, employer-sponsored health insurance was considered taxable income until 1954.[22]		In the UK, benefits are often taxed at the individual's normal tax rate,[23] which can prove expensive if there is no financial advantage to the individual from the benefit.		The UK system of state pension provision is dependent upon the payment of National Insurance Contributions. Salary exchange schemes result in reduced payments and so are may reduce the state benefits, most notably the State Second Pension.		
A work accident, workplace accident, occupational accident, or accident at work is a "discrete occurrence in the course of work" leading to physical or mental occupational injury.[1] According to the International Labour Organization (ILO), more than 337 million accidents happen on the job each year, resulting, together with occupational diseases, in more than 2.3 million deaths annually.[2]		The phrase "in the course of work" can include work-related accidents happening off the company's premises, and can include accidents caused by third parties, according to Eurostat. The definition of work accident includes accidents occurring "while engaged in an economic activity, or at work, or carrying on the business of the employer" according to the ILO.		The phrase "physical or mental harm" means any injury, disease, or death. Occupational accidents differ from occupational diseases as accidents are unexpected and unplanned occurrences (e.g., mine collapse), while occupational diseases are "contracted as a result of an exposure over a period of time to risk factors arising from work activity" (e.g., miner's lung).[3]		Incidents that fall within the definition of occupational accidents include cases of acute poisoning, attacks by humans and animals, insects etc., slips and falls on pavements or staircases, traffic collisions, and accidents on board means of transportation in the course of work, accidents in airports, stations and so on.		There is no consensus as to whether commuting accidents (i.e. accidents on the way to work and while returning home after work) should be considered to be work accidents. The ESAW methodology excludes them; the ILO includes them in its conventions concerning Health & Safety at work, although it lists them as a separate category of accidents;[4] and some countries (e.g., Greece) do not distinguish them from other work accidents.[5]		A fatal accident at work is defined as an accident which leads to the death of a victim. The time within which the death may occur varies among countries: In Netherlands an accident is registered as fatal if the victim dies during the same day that the accident happened, in Germany if death came within 30 days, while Belgium, France and Greece set no time limit.[6]		Where the accidents involve multiple fatalities they are often referred to as industrial disasters.[7]						Although many workplace accidents[8] have relatively minor repercussions, which could result in just a paper cut or scratch, others can have more serious – and potentially fatal – consequences.		For instance, there are some industries in which individuals are more exposed to occupational hazards than others, such as the construction trade. This had the highest rate of fatal injuries out of all other industry sections in 2011/12. During this period,[9] falls accounted for 51% of construction injuries resulting in death, demonstrating that builders are more likely to fall from height than those who work in less dangerous locations, such as an office.		An independent watchdog – the Health and Safety Executive (HSE) – aims to reduce the number of work-related fatalities and injuries within Great Britain, publishing statistics that show the different – and most common – types of reported workplace injuries across a range of sectors.		For example, the HSE reported that, between 2011 and 2012, incidents such as falls from height, becoming trapped by a falling structure, and being struck by a vehicle or moving object, were the reasons for the majority of fatalities to British workers.[10]		It was also revealed that slips, trips or falls were responsible for more than 50% of serious injuries to employees. Furthermore, the majority of incidents that resulted in employees taking more than three days off work – or affected their ability to perform their usual duties over this period – were caused by handling accidents.		Although some accidents at work can have minor effects, the HSE statistics revealed that more than 27 million working days were lost between 2011 and 2012 due to occupational illness or personal injury, proving that these incidents can have serious repercussions.		Accidents arise from unsafe behavior and/or unsafe conditions. An important factor is the safety climate or safety culture of an organization. Safety culture concerns how workplace safety is managed, consisting of the shared attitudes, beliefs, perceptions, and values among employees.[11] Faulty equipment can also cause serious personal injuries, a common example being accidents from faulty ladders. If the rubber feet are absent, the base of the aluminium stile can slip suddenly on a hard floor and the user fall.		According to the Health and Safety Executive,[12] employers who implement suitable measures to prevent accidents in the workplace could reap a number of benefits. As well as reducing the number of injuries at work, managers could also:		Harry Potter and the Deathly Hallows movie, year 2009, David Holmes, a stuntman, paralysed after getting blown by explosion, suffered impact to a wall .[13]		Transformers:Dark of the Moon, year 2010, a stuntwoman suffered brain damage after her car was hit by a cable then crash to central barrier .[13]		The Expendables 2, year 2011, Kun Lieu, stuntman, died because of accidental explosion during filming.[13]		Resident Evil: The Final Chapter movie, year 2015, Olivia Jackson, stuntwoman, had her arm amputated after her motorbike crashed to a metal camera arm during high-speed chase .[14]		Midnight Rider movie, 20 February 2014, camera assistant, Sarah Jones, died after getting hit by CSX freight train during filming [13]		The Dark Knight movie, year 2007, cameraman Conway Wickliffe died when his car crashed to a tree while following a stunt car batmobile [13]		Steep video game, 18 July 2016, free-skier, Matilda Hargin née Rapaport died from an avalanche while filming an advertisement.[15]		
A résumé (/ˈrɛzʊmeɪ/, REZ-u-may or /rɛzʊˈmeɪ/; less frequently /ˈrɛzjʊmeɪ/ or /rɛzjʊˈmeɪ/; French: [ʁezyme]),[a] also spelled resume,[1] is a document used by a person to present their backgrounds and skills. Résumés can be used for a variety of reasons, but most often they are used to secure new employment.[2]		A typical résumé contains a "summary" of relevant job experience and education, as its French origin (and its translation into Spanish as "resumen") implies. The résumé is usually one of the first items, along with a cover letter and sometimes an application for employment, which a potential employer sees regarding the job seeker and is typically used to screen applicants, often followed by an interview.		The curriculum vitae (CV) used for employment purposes in the UK (and in other European countries) is more akin to the résumé—a shorter, summary version of one's education and experience—than to the longer and more detailed CV that is expected in U.S. academic circles.		Generally, the résumé is substantially shorter than a CV in English Canada, the U.S. and Australia.[3]						As has been indicated above, the word résumé comes from the French word résumé meaning "summary".[4] Leonardo da Vinci is credited with the first résumé though his "résumé" takes the form of a letter written about 1481–1482 to a potential employer, Ludovico Sforza.[5][6] For the next roughly 450 years, the résumé continued to be a mere description of a person, and included their abilities and past employment. In the early 1900s, résumés listed things like weight, height, marital status, and religion. It wasn't until 1950 that the résumé evolved into something more than words written on scraps of paper. By then, résumés were considered very much mandatory, and started to include things like personal interests and hobbies. It wasn't until the 1970s, the beginning of the digital age, that résumés took on a more professional look in terms of presentation and content.[7]		In many contexts, a résumé is typically limited to one or two pages of size A4 or letter-size, highlighting only those experiences and qualifications that the author considers most relevant to the desired position. Many résumés contain keywords or skills that potential employers are looking for via applicant tracking systems, make heavy use of active verbs, and display content in a flattering manner. Acronyms and credentials after the applicant's name should be spelled out fully in the appropriate section of the resume, greater chance of being found in a computerized keyword scan.[8] Résumés can vary in style and length, but should always contain accurate contact information of the job seeker.		A résumé is a marketing tool in which the content should be adapted to suit each individual job application or applications aimed at a particular industry. The transmission of résumés directly to employers became increasingly popular as late as 2002.[citation needed] Job seekers were able to circumvent the job application process and reach employers through direct email contact and résumé blasting, a term meaning the mass distribution of résumés to increase personal visibility within the job market. However, the mass distribution of résumés to employers can often have a negative effect on the applicant's chances of securing employment as the résumés tend not to be tailored for the specific positions the applicant is applying for. It is usually, therefore, more sensible to optimize the résumé for each position applied for and its keywords. In order to keep track of all experiences, keeping a 'master résumé' document is recommended, providing job-seekers with the ability to customize a tailored résumé while making sure extraneous information is easily accessible for future use if needed.		The complexity or simplicity of various résumé formats tends to produce results varying from person to person, for the occupation, and to the industry. Résumés or CVs used by medical professionals, professors, artists and people in other specialized fields may be comparatively longer. For example, an artist's résumé, typically excluding any non-art-related employment, may include extensive lists of solo and group exhibitions.		Résumés may be organized in different ways. The following are some of the more common résumé formats:		A reverse chronological résumé lists a candidate's job experiences in chronological order, generally covering the previous 10 to 15 years. Positions are listed with starting and ending dates. Current positions on a résumé typically list the starting date to the present. The reverse chronological résumé format[9] is most commonly used by those who are not professional résumé writers. In using this format, the main body of the document becomes the Professional Experience section, starting from the most recent experience and moving chronologically backwards through a succession of previous experience. The reverse chronological résumé works to build credibility through experience gained, while illustrating career growth over time and filling all gaps in a career trajectory. A chronological résumé is not recommended to job seekers with gaps in their career summaries. In the United Kingdom the chronological résumé tends to extend only as far back as the applicant's GCSE/Standard Grade qualifications.		A functional résumé lists work experience and skills sorted by skill area or job function.		The functional résumé is used to focus on skills that are specific to the type of position being sought. This format directly emphasizes specific professional capabilities and utilizes experience summaries as its primary means of communicating professional competency. In contrast, the chronological résumé format will briefly highlight these competencies prior to presenting a comprehensive timeline of career growth through reverse chronological listings, with the most recent experience listed first. The functional résumé works well for those making a career change, having a varied work history or with little work experience. A functional résumé is also preferred for applications to jobs that require very specific skills or clearly defined personality traits. A functional résumé is a good method for highlighting particular skills or experiences, especially when those particular skills or experiences may have derived from a role which was held some time ago. Rather than focus on the length of time that has passed, the functional résumé allows the reader to identify those skills quickly.		The hybrid résumé balances the functional and chronological approaches. A résumé organized this way typically leads with a functional list of job skills, followed by a chronological list of employers. The hybrid has a tendency to repeat itself and is, therefore, less widely used than the other two.		As the search for employment has become more electronic, it is common for employers only to accept résumés electronically, either out of practicality or preference. This has changed much about the way résumés are written, read, and processed. Some career experts are pointing out that today a paper-based resume is an exception rather than the rule.[10]		Many employers now find candidates' résumés through search engines, which makes it more important for candidates to use appropriate keywords when writing a résumé.[11] Larger employers use Applicant Tracking Systems to search, filter, and manage high volumes of résumés. Job ads may direct applicants to email a résumé to a company or visit its website and submit a résumé in an electronic format.		Many employers, and recruitment agencies working on their behalf, insist on receiving résumés in a particular file format. Some require Microsoft Word documents, while others will only accept résumés formatted in HTML, PDF, or plain ASCII text.		Another consideration for electronic résumé documents is that they are parsed with natural language processors. Resume parsers may correctly interpret some parts of the content of the résumé but not other parts. The best résumé parsers capture a high percentage of information regarding location, names, titles, but are less accurate with skills, industries and other less structured or rapidly changing data.[12] Résumé written in a standard format are more likely to be correctly interpreted by résumé parsers, and thereby will make the candidate more findable.		One advantage for employers to online résumés is the significant cost saving compared to traditional hiring methods.[13] Another is that potential employers no longer have to sort through massive stacks of paper.		As the Internet becomes more driven by multimedia, job-seekers have sought to take advantage of the trend by moving their résumés away from the traditional paper and email media.		Video, infographic, and even Vine résumés have gained popularity, though mainly in the creative and media industries.[14]		This trend has attracted criticism from human resources management professionals, who warn that this may be a passing fad and point out that multimedia-based résumés may be overlooked by recruiters whose workflow is designed only to accommodate a traditional résumé format.[15]		Many résumé development agencies offer résumé evaluation services wherein they evaluate the résumé and suggest any necessary changes. Candidates are free to either do those changes themselves or may take help of the agency itself. Some career fields include a special section listing the lifelong works of the author: for computer-related fields, the softography; for musicians and composers, the discography; for actors, a filmography.		Keeping résumés online has become increasingly common for people in professions that benefit from the multimedia and rich detail that are offered by an HTML résumé, such as actors, photographers, graphic designers, developers, dancers, etc.[16] Job seekers are finding an ever-increasing demand to have an electronic version of their résumé available to employers and professionals who use Internet recruiting.[17] Online résumé distribution services have emerged to allow job seekers to distribute their résumés to numerous employers of their choice through email.[18]		In some sectors, particularly in the startup community, use of traditional résumé has seen a consistent decline.[19] While standalone résumés are still used to apply for jobs, job-seekers may also view their résumés as one of a number of assets which form their personal brand and work together to strengthen their job application. In this scenario, résumés are generally used to provide a potential employer with factual information (e.g., achievements), while the social media platforms give insight into the job-seekers' motivations and personality.		
Lifelong learning is the "ongoing, voluntary, and self-motivated"[1] pursuit of knowledge for either personal or professional reasons. Therefore, it not only enhances social inclusion, active citizenship, and personal development, but also self-sustainability, as well as competitiveness and employability.[2]		Evolved from the term "life-long learners", created by Leslie Watkins and used by Professor Clint Taylor (CSULA) and Superintendent for the Temple City Unified School District's mission statement in 1993, the term recognizes that learning is not confined to childhood or the classroom but takes place throughout life and in a range of situations. Allen Tough (1979), Canadian educator and researcher, asserts that almost 70% of learning projects are self-planned.[3]		During the last fifty years, constant scientific and technological innovation and change has had profound effects on how learning is understood. Learning can no longer be divided into a place and time to acquire knowledge (school) and a place and time to apply the knowledge acquired (the workplace).[4] Instead, learning can be seen as something that takes place on an ongoing basis from our daily interactions with others and with the world around us. It can take the form of formal learning or informal learning, or self-directed learning.						Lifelong learning is being recognized by traditional colleges and universities as valid in addition to degree attainment. Some learning is accomplished in segments or interest categories and can still be valuable to the individual and community. The economic impact of educational institutions at all levels will continue to be significant into the future as formal courses of study continue and interest-based subjects are pursued. The institutions produce educated citizens who buy goods and services in the community and the education facilities and personnel generate economic activity during the operations and institutional activities. Similar to health facilities, educational institutions are among the top employers in many cities and towns of the world. Whether brick-and-mortar institutions or on-line schools, there is a great economic impact worldwide from learning, including lifelong learning, for all age groups. The lifelong learners, including persons with academic or professional credentials, tend to find higher-paying occupations, leaving monetary, cultural, and entrepreneurial impressions on communities, according to educator Cassandra B. Whyte.[5][6]		Although the term is widely used in a variety of contexts its meaning is often unclear.[7] A learning approach that can be used to define lifelong learning is heutagogy.[8]		There are several established contexts for lifelong learning beyond traditional "brick and mortar" schooling:		E-learning is available at most colleges and universities or to individuals learning independently. There are even online courses being offered for free by many institutions.		One new (2008 and beyond) expression of lifelong learning is the massive open online course (a MOOC), in which a teacher or team offers a syllabus and some direction for the participation of hundreds, sometimes thousands, of learners. Most MOOCs do not offer typical "credit" for courses taken, which is why they are interesting and useful examples of lifelong learning.		Lifelong learning is defined as "all learning activity undertaken throughout life, with the aim of improving knowledge, skills and competences within a personal, civic, social and/or employment-related perspective".[9] It is often considered learning that occurs after the formal education years of childhood (where learning is instructor-driven – pedagogical) and into adulthood (where the learning is individually-driven – andragogical). It is sought out naturally through life experiences as the learner seeks to gain knowledge for professional or personal reasons.'Knowledge results from the combination of grasping experience and transforming it' (Kolb 1984: 41). The concept of lifelong learning has become of vital importance with the emergence of new technologies that change how we receive and gather information, collaborate with others, and communicate.		As technology rapidly changes, individuals must adapt and learn to meet everyday demands. However, throughout life, an individual's functional capacities may also change. Assistive technologies are also important considerations under the umbrella of emerging technology and lifelong learning. Access to informal and formal learning opportunities for individuals with disabilities may be dependent upon low and high tech assistive technology.		The emergence of Web 2.0 technologies has great potential to support lifelong learning endeavors, allowing for informal, just-in-time, day-to-day learning.[10] Constant change is emerging as the new normal. In order to survive and thrive, organizations and individuals must be able to adjust, and enhance their knowledge and skills to meet evolving needs. This means the most important thing someone can learn is how to learn.[11] An understanding of web 2.0 tools is critical to keeping up with a changing world and the information explosion.		The professions in particular are recognizing the importance of developing practitioners to be lifelong learners. Nowadays, formal training is only a beginning; knowledge is accumulating at such a fast rate that one must continue to learn to be effective (Williams, 2001). Indeed, most professions mandate that their members continue learning in order to maintain their license to practice. (Merriam, Caffarella, & Baumgartner, 2007).[12] Having said this, what are the characteristics or skills that a lifelong learner will need to develop. Reflective learning and critical thinking can help a learner to become more self-reliant through learning how to learn, thus making them better able to direct, manage, and control their own learning process (Candy, 1990).[13] Sipe (1995) studied experimentally "open" teachers and found that they valued self-directed learning, collaboration, reflection, and challenge; risk taking in their learning was seen as an opportunity, not a threat. Dunlap and Grabinger (2003) make the case that in order to prepare students in higher education to be lifelong learners, we must develop their capacity for self-direction, metacognition awareness, and a disposition toward learning (Merriam, Caffarella, & Baumgartner, 2007).[12]		While the study of metacognition originally gave educational psychologists insights into what differentiated successful students from their less successful peers, it is increasingly being used to inform teaching that aims to make students more aware of their learning processes, and show them how to regulate those processes for more effective learning throughout their lives.[14]		Educators can employ Cognitive Strategy Instruction (CSI)[15][16] as a means to help learners develop their metacognition. Again, learners who are better equipped to create learning strategies for themselves will have more success in achieving their cognitive goals.[14]		As lifelong learning is "lifelong, lifewide, voluntary, and self-motivated"[1] learning to learn, that is, learning how to recognize learning strategies, and monitor and evaluate learning, is a pre-condition for lifelong learning. Metacognition is an essential first step in developing lifelong learning.		The Delors Report[17] proposed an integrated vision of education based on two key paradigms: lifelong learning and the four pillars of learning. The report proposed a holistic conceptual framework of learning, that of the 'four pillars of learning'. It argued that formal education tends to emphasize the acquisition of knowledge to the detriment of other types of learning essential to sustaining human development. It stressed the need to think of learning over the life course, and to address how everyone can develop relevant skills, knowledge and attitudes for work, citizenship and personal fulfillment.[18] The four pillars of learning are:		It is important to note that the four pillars of learning were envisaged against the backdrop of the notion of 'lifelong learning', itself an adaptation of the concept of 'lifelong education' as initially conceptualized in the 1972 Faure publication Learning to Be.[19][18]		In India and elsewhere, the "University of the Third Age" (U3A) provides an example of the almost spontaneous emergence of autonomous learning groups accessing the expertise of their own members in the pursuit of knowledge and shared experience. No prior qualifications and no subsequent certificates feature in this approach to learning for its own sake and, as participants testify, engagement in this type of learning in later life can indeed 'prolong active life'.		In Sweden the successful concept of study circles, an idea launched almost a century ago, still represents a large portion of the adult education provision. The concept has since spread, and for instance, is a common practice in Finland as well. A study circle is one of the most democratic forms of a learning environment that has been created. There are no teachers and the group decides on what content will be covered, scope will be used, as well as a delivery method.		Sometimes lifelong learning aims to provide educational opportunities outside standard educational systems – which can be cost-prohibitive, if it is available at all. On the other hand, formal administrative units devoted to this discipline exist in a number of universities. For example, the 'Academy of Lifelong Learning' is an administrative unit within the University-wide 'Professional and Continuing Studies' unit at the University of Delaware.[20] Another example is the Jagiellonian University Extension (Wszechnica Uniwersytetu Jagiellonskiego), which is one of the most comprehensive Polish centers for lifelong learning (open learning, organizational learning, community learning).[21]		In recent years, 'lifelong learning' has been adopted in the UK as an umbrella term for post-compulsory education that falls outside of the UK higher education system – further education, community education, work-based learning and similar voluntary, public sector and commercial settings.		Most colleges and universities in the United States encourage lifelong learning to non-traditional students. Professional licensure and certification courses are also offered at many universities, for instance for teachers, social services providers, and other professionals. Some colleges even enable adults to earn credit for the college-level learning gained through work, volunteer and other experiences.[22]		Bangladesh Open University (BOU) has six schools and is offering 23 formal and 19 nonformal programs.[23] The number of enrolled students in formal programs for 2016 was 433,413.[23] Most of the courses of BOU are for professional development and most of the students are professional people who are getting scope to study in flexible hours.[23] BOU is the only public institution in the country that imparts education in distance mode.[23] In place of campus based teaching, this university uses technology including electronic devices to reach people in different corners of the country.[23]		In Canada, the federal government's Lifelong Learning Plan[24] allows Canadian residents to withdraw funds from their Registered Retirement Savings Plan to help pay for lifelong learning, but the funds can only be used for formal learning programs at designated educational institutions.		Priorities for lifelong and life-wide learning vary between the emphasis being placed on economic development (towards a learning economy) and on social development (towards a learning society). For instance, China places its policies for the promotion of lifelong learning in a human resource development (HRD) perspective, in keeping with other states in the region, such as the Republic of Korea, Singapore and Malaysia, whose central governments have done much to foster HRD while encouraging entrepreneurship to thrive on a large scale. China’s efforts to develop lifelong learning policies to reach a very wide range of individuals throughout the life course illustrate how lifelong learning policies can be effective for learners in different settings, and that progress can be made, despite obstacles and setbacks. This experience has relevance for a variety of countries, even though their culture, priorities and resources may be very different from those of China.[25]		Mainstream economic analysis has highlighted increased levels of primary and secondary education as a key driver of long-term economic growth. Data show that initial levels of educational attainment explain about half the difference in growth rates between East Asia and sub- Saharan Africa between 1965 and 2010. At the individual level, the knowledge and skills workers acquire through education and training make them more productive. Provision of good quality education can improve the knowledge and skills of a whole population beyond what traditional or informal systems can achieve. For business, educated and highly skilled workers foster productivity gains and technological change, through either innovation or imitation of processes developed elsewhere. At the societal level, education expansion helps build social and institutional capital, which has a strong impact on the investment climate and growth; it also helps in building social trust, developing participatory societies, strengthening the rule of law and supporting good governance.[26]		To learn how to add open-license text to Wikipedia articles, please see Wikipedia:Adding open license text to Wikipedia.		To learn how to add open-license text to Wikipedia articles, please see Wikipedia:Adding open license text to Wikipedia.		To learn how to add open-license text to Wikipedia articles, please see Wikipedia:Adding open license text to Wikipedia.		
The wage curve is the negative relationship between the levels of unemployment and wages that arises when these variables are expressed in local terms. According to David Blanchflower and Andrew Oswald (1994, p. 5), the wage curve summarizes the fact that "A worker who is employed in an area of high unemployment earns less than an identical individual who works in a region with low joblessness."		One way to understand the wage curve is as follows. The labour supply of each individual is positively correlated to wages, therefore the higher is the hourly wage offered, the more hours an individual is willing to work. However, there is a limit to which every person would be willing to sacrifice an hour of leisure or rest, for an hour's worth of wages. Let's say that X is the maximum amount of hours a person can work, and $A is the minimum hourly wage rate he expects in return. Any wage $B, greater than $A, will increase the worker's daily wage without increasing the hours of work. So if you need more hours of work than X, you need to hire more people.		Say you need to purchase Y hours of labour from the labour market. Let us assume that Y = 4X. This means that Y is four times as much as one labourer's maximum labour offer. If you pay $A an hour then you can hire 4 labourers to work for X hours each. However, depending on the labour market conditions, other options are open:		Say that there are not very many jobs in the labour market, unemployment is high and a lot of people are under employed (working much fewer than X hours). In this situation the going rate is likely to be lower than $A as it is very unlikely that an employee would be asked to work for X hours. You would save money by hiring more than 4 labourers with each of them working fewer than X hours.		Say the labour market is tight and most people are already working X hours a day. It is very hard to find people who are not already earning $A an hour, and because of that you must match the money offer elsewhere in order to get someone to work for you. The wage level in this scenario would be higher than the earlier scenario.		In short - the lower unemployment is and the fewer the laborers there are available, the higher the wages. The contrary is true when the unemployment is high. This is the essence of the wage curve.		It is utilised to explain why within a country, some regions suffer worse unemployment than others. Labourers could but, for whatever reasons, are unwilling to migrate from regions with high unemployment low wage area to low unemployment high wage areas.		One of the reasons why unemployed labourers would not want to migrate to other areas with plenty of jobs is because of home-ownership. The worker might be deterred from moving because of the costs involved in selling off his / her home and moving. Blanchflower and Oswald have found that the unemployment rate is positively correlated to home-ownership rate in a cross country study.		However recently some micro econometric evidence suggests that the relationship between homeownership and unemployment is slightly more complicated. People who are employed are more likely to be able to afford a mortgage, and are therefore more likely to have bought their own home. The evidence indicates that the employed home-owners are less likely to become unemployed, and they are also more likely to be employed in jobs with high stability and therefore less likely to change jobs. The unemployed home-owners are more likely to find jobs within the local areas, and less likely to find jobs which would necessitate a move. The overall picture suggests that although home-owners are reluctant to move around, they are more often than not employed, and therefore not contributing the unemployment rate.		
Overtime is the amount of time someone works beyond normal working hours. The term is also used for the pay received for this time. Normal hours may be determined in several ways:		Most nations have overtime labour laws designed to dissuade or prevent employers from forcing their employees to work excessively long hours. These laws may take into account other considerations than the humanitarian, such as preserving the health of workers so that they may continue to be productive, or increasing the overall level of employment in the economy. One common approach to regulating overtime is to require employers to pay workers at a higher hourly rate for overtime work. Companies may choose to pay workers higher overtime pay even if not obliged to do so by law, particularly if they believe that they face a backward bending supply curve of labour.		Overtime pay rates can cause workers to work longer hours than they would at a flat hourly rate. Overtime laws, attitudes toward overtime and hours of work vary greatly from country to country and between different economic sectors.						Time off in lieu; compensatory time; or comp time refers to a type of work schedule arrangement that allows (or requires) workers to take time off instead of, or in addition to, receiving overtime pay. A worker may receive overtime pay plus equal time off for each hour worked on certain agreed days, such as public holidays.		In the United States, such arrangements are currently legal in the public sector but not in the private sector.[1]		For example, non-exempt workers must receive at least one and one half times their normal hourly wage for every hour worked beyond 40 hours in a work week. For example, workers who clock 48 hours in one week would receive the pay equivalent to 52 hours of work (40 hours + 8 hours at 1.5 times the normal hourly wage). With comp time, the worker could (or would have to) forgo the 12 hours of overtime pay and instead take 8 paid hours off at some future date.[clarification needed][citation needed]		In some other jurisdictions, such as Canada, employers might be required to pay the overtime at the higher rate (e.g. 1.5 times the normal rate), but also be allowed to require time off in lieu at the normal rate. Thus, an employee might work 48 hours in one week, and 32 hours the next week (assuming over 40 hours is overtime), and be paid an extra amount equivalent to 4 hours work (8 multiplied by 0.5).		In Australia, such arrangements both in the private and public sector are common.		In some cases, particularly when employees are represented by a labour union, overtime may be paid at a higher rate than 1.5 times the hourly pay. In some factories, for example, if workers are required to work on a Sunday, they may be paid twice their regular rate, i.e., double time.		Directives issued by the European Union must be incorporated into law by member states.		Directives 93/104/EC (1993), 2000/34/EC (2000), which limited working hours, were consolidated into 2003/88/EC (2003). Employers and employees can agree to opt out, under certain circumstances.		The directives require:[2]		The directives apply to:		Exemptions:		In Japan the Labour Standards Act (労働基準法) of 1947 provides for an eight-hour work day and 40-hour workweek with at least one day off per week. The act requires a premium of at least 25% over the ordinary hourly wage for any overtime work, 35% for any work on prescribed off days, and an additional 25% for any work between 10 pm and 5 am.[3] Employers must enter into an overtime agreement with a labour representative prior to any overtime work by employees, and this agreement must stipulate to the maximum number of overtime hours that an employee may work, which may be no more than 15 hours per week, 45 hours per month and 360 hours per year.[4]		In the United States the Fair Labor Standards Act of 1938 applies to employees in industries engaged in or producing goods for interstate commerce. The FLSA establishes a standard work week of 40 hours for certain kinds of workers, and mandates payment for overtime hours to those workers of one and one-half times the workers' normal rate of pay for any time worked above 40 hours. The law creates two broad categories of employees, those who are "exempt" from the regulation and those who are "non-exempt". Under the law, employers are not required to pay exempt employees overtime but must do so for non-exempt employees. Independent contractors are not considered employees and therefore are not protected by the FLSA. Several factors determine whether a worker is an employee, who might be entitled to overtime compensation, or an independent contractor, who would not be so entitled. The employment agreement stating that a party is an independent contractor does not make it necessarily so. The nature of a job determines whether an employee is entitled to overtime pay, not employment status or the field of work. [5] Classes of workers who are exempt from the regulation include certain types of administrative, professional, and executive employees. To qualify as an administrative, professional, or executive employee and therefore not be entitled to overtime, three tests must be passed based on salary basis, duties, and salary level. The tests vary between administrative, professional, and executive employees based on their different duties and salary levels. There are many other classes of workers who may be exempt including outside salespeople, certain agricultural employees, certain live-in employees, and certain transportation employees. Employees can neither waive their FLSA protections nor abridge them by contract.		An employer may not retaliate[6] against an employee for filing a complaint or instituting a proceeding based on the FLSA. An employer that engages in any form of verifiable retaliation would be liable under the Fair Labor Standards Act Section 216(b) for equitable relief including reinstatement, promotion, payment of lost wages, and payment of liquidated damages. Acts of retaliation include terminating employment, disrupting the workplace, threats, acts of physical violence, and constructive discharge.		Out of approximately 120 million American workers, nearly 50 million are exempt from overtime laws (US Department of Labor, Wage and Hour Division, 1998). In 2004, the United States was 7th out of 24 OECD countries in terms of annual working hours per worker. (See Working time for a complete listing.) In 2015, the United States Department of Labor proposed dramatic changes to certain exemptions from federal minimum wage and overtime requirements. These changes are anticipated to take effect in July 2016, but as of January 2016, still are pending final approval.[7] Proposed changes include: setting the minimum salary level required for exemption for full-time salaried workers at $970 per week, or $50,440 annually (an increase from the current $455 per week, or $23,660 annually) Increasing the total annual compensation required to exempt highly compensated employees to $122,148 annually (from the current $100,000 annually).[8]		On August 23, 2004, President George W. Bush and the Department of Labor proposed changes to regulations governing implementation of the law. According to one study, the changes would have had significant impact on the number of workers covered by overtime laws and have exempted several million additional workers.[9] The Bush administration maintained that the practical impact on working Americans would be minimal and that the changes would help clarify an outdated regulation. In particular, the new rules would have allowed more companies to offer flextime to their workers in lieu of overtime.		The state of California's overtime laws involve overlapping statutes, regulations, and precedents that govern the compensation of employees in California. While the governing federal law is the Fair Labor Standards Act (29 USC 201-219), California overtime law is codified in provisions of the California Labor Code and in Wage orders of the Industrial Welfare Commission[10] Because there are two sources of applicable law (federal and state), a California employer must comply with both.		In California, based on California Labor Code 1171, only an employment relationship is required for overtime rules to apply. Under the California Industrial Welfare Commission Wage Orders, an "employer" is "any person ... who directly or indirectly, or through an agent or any other person, employs or exercises control over wages, hours, or working conditions of any person." Under the California Labor Code, an "employee" is "[any] person, including aliens and minors, rendering actual service in any business for an employer, whether gratuitously or for wages or pay, whether the wages or pay are measured by the standard of time, piece, task, commission, or other method of calculation, and whether the service is rendered on a commission, concessionaire, or other basis." Independent contractors are not employees covered by overtime laws, so it is important to determine if a worker is an independent contractor or an employee.		California overtime laws differ from federal overtime laws in many respects. Foremost, pursuant to California Labor Code Section 510, non-exempt employees must be compensated at one and a half times the regular rate of pay for all hours worked in excess of eight hours in a workday, 40 hours in a workweek and the first eight hours of a seventh consecutive workday. Employees in California are entitled to double-time for working more than twelve hour workdays or more than eight hours on the seventh consecutive workday of a single workweek. Under federal law there are only 40 hour weekly overtime limits. This eight hour overtime limit in California frequently gives rise to wage-and-hour litigation for violations of state, but not federal, labour laws.		For example, "comp time" schemes, where employers tell employees that since they worked 10 hours on Monday they can work 6 hours on Tuesday, are illegal because even though the employees are not working more than 40 hours for the purposes of overtime compensation under federal law, they are working more than 8 hours for purposes of California overtime law and rounding the 6- and 10-hour workdays to two 8-hour workdays would cheat the employee out of two hours of overtime pay.		Perhaps the biggest difference between California and federal overtime law relates to the administrative exemption's "primarily engaged" in duties that meet the test for the exemption requirement, such as duties that involve exercising independent discretion and judgment as set forth in the controversial Order No. 4. Whereas under the Fair Labor Standards Act "primarily engaged" does not necessarily mean at least half, under California wage-and-hour laws, less than half of exempt duties automatically eliminates the overtime exemption.		
Workplace harassment is the belittling or threatening behavior directed at an individual worker or a group of workers[1]		Recently, matters of workplace harassment have gained interest among practitioners and researchers as it is becoming one of the most sensitive areas of effective workplace management. In Asian countries, it attracted lots of attention from researchers and governments since the 1980s, because a significant source of work stress is associated with aggressive behaviors at workplace.[2] Third world countries are far behind Asian countries in that there are limited efforts to investigate the questions on workplace harassment. It is almost unseen and the executive leaders (managers) are almost reluctant or unconscious about it in the third world countries.[3] Under occupational health and safety laws around the world,[4] workplace harassment and workplace bullying are identified as being core psychosocial hazards.[5]						Workplace harassment is also known by many other names. "Mobbing", "workplace bullying", "workplace mistreatment", "workplace aggression", and "workplace abuse" are all either synonymous or belong to the category of workplace harassment.[6] Workplace harassment includes different types of discrimination and acts of violation that are not confined to one specific group. The wide-ranging types of workplace harassment can be loosely categorized into emotional and physical abuse. All of these forms of workplace harassment target various groups, including women, racial minorities, homosexuals, and immigrants. In essence, workplace harassment requires pluralistic understanding, because it cannot be delineated in one coherent and concrete definition.[7]		Acknowledging the difficulty of formulating a universal definition of workplace harassment, Ezer broadly defines workplace harassment as "irrational repeated behavior towards an employee or group of employees, which represents a health and security risk.[8] Any act of discrimination or assault that systematically disadvantage the employees is considered workplace harassment.[8] Workplace harassment can contribute to deterioration of physical and emotional health.[8]		According to Rosa Brook, the concept of workplace harassment is based on two premises.[7] Firstly, regardless of gender, race, sexuality or any other defining characteristic, every person should be given the right to be "free from abusive treatment in the workplace".[7] With freedom from abuse given as a basic human right, any form of discomfort or discrimination in workplace becomes labeled as an act of harassment.[7] Secondly, the issues caused by workplace harassment affect the victims in harmful ways. Discrimination in the workplace hinders victims from successful advancement in their careers, limiting the capabilities of the victim.[7]		A common misconception about workplace harassment is that workplace harassment is simply sexual harassment in the context of a workplace.[9] While sexual harassment is a prominent form of workplace harassment, the United States Department of Labor defines workplace harassment as being more than just sexual harassment.[9] "It may entail 'quid pro quo' harassment, which occurs in cases in which employment decisions or treatment are based on submission to or rejection of unwelcome conduct, typically conduct of a sexual nature. Workplace harassment may also consist of offensive conduct based on one or more of the protected groups above that is so severe or pervasive that it creates a hostile or offensive work environment or when it results in an adverse employment decision (such as being fired or demoted)."[9] Thus, workplace harassment is a bigger category that encompasses sexual harassment.		The vastly different harassments imposed on the victims can be categorized into two different types, physical abuse and emotional abuse. Physical abuse refers to sexual assault and violence on body, while emotional abuse refers to imposing stress and bullying. Anderson and Militello found that often managers exhibiting harassing behavior were allowed to maintain their jobs because their behavior was seen to increase productivity in the short term. A study done by Kathleen D. Ryan and Daniel K Oestereich, Driving Fear Out of the Workplace, found that many of these behaviors can range from subtle emotional cues to outward physical threats and can include; silence, direct insults and even angry outbursts. Whether these actions are intentional or brought on by stress, the result can cause the employee to feel humiliated, isolated and may cause them to lash out at others.[10]		Physical harassment in the workplace takes many forms. Sexual assault is one form of widely known physical harassment. Sexual assault in the workplace has gained media and academic attention majorly in the 90s after a series of famous sex scandals.[11]"Among the most notorious are the 1991 congressional hearings on the alleged sexual harassment of Anita Hill by Clarence Thomas, nominee to the Supreme Court; the sexual assault on female officers at a party during the 1991 annual convention of Navy fighter pilots; the dismissal of Air Force pilot Kelly Flinn for adultery in 1997; the 1998 trial and acquittal of the top ranking Army enlisted man on charges of sexual harassment; and the independent counsel investigations of President Clinton's sexual affairs with subordinates."[11] With this cascade of sex scandals, the media and scholars have focused on developing more studies on sexual harassment in workplaces. Sexual assault becomes difficult to define, as the distinction between sexual harassment and consensual sexual behaviors is not finely delineated.[11] Some occupations require a higher tolerance to sexual behaviors, such as waitresses and tour guides.[11] More specifically, the employers for these occupations expect the workers to comply with the level of sexual interactions the workers would have with the customers.[11] This unquestioned expectation from the employers then pushes the workers to see only two options. The workers would have to accept the sexual harassment from customers as "part of the job", or report the sexual harassment to the manager and get fired.[11] Adding onto the pressure, reporting sexual assault comes with criticism from co-workers, as they see the sexual assault as part of the job requirement.[11]		The prevalence of sexual harassment at work is high. For example, a study by the U.S. Merit Systems Protection Board in 1981 shows that among the female government employees, 33 percent experienced sexual comments, 26 percent had unwanted physical touching, and 15 percent was pressured for dates.[12] Moreover, "Nearly 10% had been directly pressured for sexual cooperation, and a similar percentage described repeated telephone calls and unwelcome letters or notes."[12] Other than this example, Fitzgerald states that "the enormity of such figures is difficult to grasp, indicating as they do that virtually millions of women are subjected to experiences ranging from insults to assault—many on an ongoing or recurrent basis— as the price of earning a living."[12]		Another form of physical harassment at work is workplace violence. Workplace violence is defined as physical threats and assaults targeted at employees. There are two main perpetrators for workplace violence: criminals who approached as clients, and co-workers.[13] The criminals assert violence through the forms of robberies and homicides, and the rate of homicides in the workplace has risen significantly over the past 20 years.[13] According to the National Institute for Occupational Safety and Health (NIOSH), 9,937 workplace homicides happened in the time period of 1980 to 1992, which averages out to about 800 homicides per year.[13] "In 1989, homicide was the third leading cause of death in the workplace for all employees.[14] By 1993, homicide had become the second leading cause of death on-the-job for all employees and had become the leading cause of death for women."[14] Most of these homicides are by criminals, as the Bureau of Labor Statistics found that only 59 of the 1,063 were co-worker related homicides, and the rest were made by criminals.[14]		The workplace violence perpetrated by co-workers tends to be less obvious.[14] The Northwestern National Life (1993) study showed 15 percent of respondents experienced physical attack at work, and 14 percent of respondents reported being physically attacked in the past 12 months.[14] The acts of violence in workplace consist of pushing or shoving, fistfights, and rape.[14] The SHRM study that interviewed 1,016 human resource professionals, "22% reported incidents of pushing or shoving, 13% reported fist fights, and 1% reported rape or sexual assault."[14] Much of the physical violence on workers is preceded by physiological aggression, hinting that emotional harassment may be the cause for workplace violence.[15]		Unlike physical harassment, emotional harassment is unnoticeable and also viewed as being more socially acceptable.[16] Naturally, emotional harassment in the workplace gets less attention than physical harassment in the workplace, which perpetuates the issue of emotional harassment in the workplace.[16] According to Keashly, emotional harassment can be defined as "the hostile verbal and nonverbal behaviors that are not explicitly tied to sexual or racial content yet are directed at gaining compliance from others."[16] In short, emotional harassment is manipulation of people's actions through social behaviors.		One common form of emotional abuse in workplace is bullying. Also known as mobbing, workplace bullying "is a long lasting, escalated conflict with frequent harassing actions systematically aimed at a target person."[17] Specific actions of workplace bullying include the following: false accusations of mistakes and errors, hostile glares and other intimidating non-verbal behaviors, yelling, shouting, and screaming, exclusion and the "silent treatment," withholding resources and information necessary to the job, behind-the-back sabotage and defamation, use of put-downs, insults, and excessively harsh criticism, and unreasonably heavy work demands designed to ensure failure.[18] The 2014 Workplace Bullying Institute/Zogby national survey shows that 27 percent have experienced workplace bullying in the past, and seven percent of employees currently suffer workplace bullying.[18] In addition, "more than 97% of nurse managers reported experiencing abuse, whereas 60% of retail industry workers, 23% of faculty and university staff, and 53% of business school students reported abuse at work."[19] The areas of industry in which emotional abuse happens are not limited to one, but rather they range from hospitals, universities, manufacturing plants, research industries, and social service agencies.[19]		With such frequency of workplace bullying to various groups of people, many theories exist in discussing the causes of workplace bullying. One side argues that the bullying targets are in fact responsible for the bullying.[17] More specifically, some physicians and psychologists attribute the cause of workplace bullying to the target employee's mental disorders, such as general anxiety disorder, instead of the working situation.[17] The opposite argument contends that the cause of workplace bullying lies in the organizational problems and poor leadership skills. Another argument states that workplace bullying is a multi-causal phenomenon, as different factors can play their respective roles in building the tension.[20] Despite this plethora of arguments, Zapf addresses that academic analysis of the cause is difficult.[17] Getting the perspective of perpetrators and potential bystanders is unrealistic, and therefore the studies are primarily focused on victims' interviews.[17]		The victims of workplace harassment can be separated into three categories, based on gender, sexuality, and race. While one group experiences workplace harassment more frequently than others, workplace harassment still affects wide range of population.		Both men and women are victims of workplace harassment. Workplace harassment for women dates back to women's first foray into the workforce, as early as colonial times. The most common form of workplace harassment that women face is sexual harassment.[12] According to Fitzgerald, one of every two women experiences workplace harassment in their working or academic lives.[12] The most common form of sexual harassment is the unwanted and unavoidable sexual attention from co-workers.[12] A study of government employees shows the inescapable, uncomfortable sexual attention takes varying forms.[12] 33% of respondents had been called by sexual remarks, 26% of respondents faced physical touching, and 15% respondents were pressured to go on a date.[12] The more explicit forms of sexual harassment are shown by court cases, such as Meritor v. Vinson (1986), Robinson v. Jacksonville Shipyards (1991), and others.[12] In Meritor v. Vinson, "Michele Vinson, an employee of Meritor Savings Bank, was forced to have sex with her boss between 40 and 50 times."[12] The boss harassed her by fondling her in public, following her to the bathroom, and frequently raping her.[12] In Robinson v. Jacksonville Shipyards, Robinson requested to put down the pornographic materials in Jacksonville Shipyard workplace. The pornographic material included "a pinup showing a meat spatula pressed against a woman's pubic area and another featuring a nude woman holding a whip."[12]		While workplace harassment against women has been a frequent subject of study for more than 20 years, workplace harassment against men rarely receives attention and is not subjected to many studies.[21] However, the Bureau of Justice Statistics shows that "among people victimized while working or on duty, male victims outnumbered females by about 2 to 1."[22] Men experience less workplace sexual harassment than women, as only 16.7% of men reported rape/sexual assault, but men face more workplace violence.[22] 72% of men was robbed in their workplace, 74.4% of men experienced aggravated assault, and 66.1% of men experienced simple assault.[22]		The Williams Institute 2011 study shows that "In the American workforce, more than eight million people (or 4 percent of the U.S. workforce) identify as lesbian, gay, bisexual, or transgender (LGBT)."[23] Even so, the LGBT group has faced constant discrimination and harassment in workplaces, as shown by court cases and historical events.[23] One common form of workplace harassment for LGBT community is the psychological and physical strain in hiding their sexuality in a heterosexist workplace environment.[24] Other form of workplace harassment is direct harassment from the public after disclosing one's sexuality.[24] Because an LGBT individual experiences explicit verbal assault, physical violence, and hate crimes after disclosing sexuality, the LGBT community more often than not conceals its sexuality in workplaces.[24]		Many studies show that culturally stigmatized groups face more workplace harassments.[25] With changes in the political and social scenes in America, subtle and daily harassment is more common than blatant and explicit harassment today.[25] A study by Deitch, Barsky, Butz and et al. shows that black Americans face more mistreatment in workplaces than white Americans.[25] The mistreatments and harassments do not explicitly "reference race or discrimination as the cause of the treatment", because overt racism is prohibited in workplaces.[25] However, the statistics show race is "significantly associated with mistreatment" and that black Americans in general report significantly more "minor, pervasive mistreatment and unfairness on the job."[25] The study suggests the discrimination and harassments may intensify for Black Americans in a job with fewer people of the same race, such as "token" Black employee or "solo" employees.[25] In addition, not only Blacks but also Asian Americans, and other minority races all face "a higher rate of homicide than their proportion of the work force would suggest."[26] Of the eighth of the workforce experiencing homicide, more than a fourth of the population is an ethnic minority.[26]		The intensity of workplace harassment is positively correlated with the level of alcohol use.[27] One of the motives that people drink is "to self-medicate distressful feelings resulting from problematic social conditions".[27] Thus, the negative social distress faced in workplaces is linked with increased consumption of alcohol.[27] Moreover, because workplace harassment cannot be clearly delineated like sexual or racial harassment, victims do not counteract by legal and institution responses.[27] Rather, they rely on drinking to cope with the emotional distress.[27]		Nolen-Hoeksema and Harrell's 2002 study shows that while both women and men are at risk of alcoholism under workplace harassment, men are more likely to cope by drinking than women do, as women use their relatively wider social connections to attain the emotional support.[28] However, a 2004 survey of a random sample of employees at a heavy machinery assembly plant shows that women are more sensitive and receptive of workplace harassment, and therefore women have "a greater propensity to drink".[29] The negative drinking effects are more severe for women than they are for men.[28]		One mail survey that was completed at four points in time by a cohort of 1654 employees has shown that the positive correlation between consumption of drinking and levels of workplace harassment continues after retirement.[30] Even when the immediate stressors are not present, the victims still retain the increased use of alcohol.[30] The study attributes the reason for the lasting effect is that "appropriate alcohol consumption may have functioned to somewhat inhibit the self-medication of stress-induced distress during work role occupancy".[30]		PTSD is commonly known as a "war wound", yet it also affects workers,[31] "when a worker suffers PTSD, the workplace for that person has become a war zone".[31] Several studies show that many workplace harassment victims experience Post-Traumatic Stress Disorder (PTSD).[32] For example, a study that interviewed about 100 victims of workplace harassment shows that "a majority of the respondents exceed recommended threshold-values indicating PTSD".[33] The study also demonstrate that based on the duration and persistency of the workplace harassment, the levels of PTSD differ.[33] The more recent and frequent the workplace harassment occurred, the more severe their symptoms of PTSD were.[33]		A study by Mikklesen and Einarsen also reports that 76 percent of its respondents experienced PTSD.[32] Nevertheless, Mikklesen and Einarsen qualify the idea that workplace harassment directly leads to PTSD.[32] They argue that the causes of PTSD symptoms of the victims are primarily attributed to other traumatic events rather than the workplace harassment itself.[32] Therefore, the study concludes the "exposure to other traumatic life events may increase victims' vulnerability" to their sensitivity to workplace harassment.[32]		Other than alcoholism and PTSD, victims of workplace harassment also experience other negative psychological effects.[34] An analysis of self-reported health symptoms, and physiological stress reactivity of 437 employees shows that compared to the employees who have not experienced workplace harassment, employees who have experienced exhibited higher level of anxiety and nervousness.[35] Another study's survey of 156 victims of workplace harassment shows that 79.4 percent of respondents suffer from stress, 64.7 percent from depressive symptoms, 64 percent from tiredness, 59 percent from lack of confidence, 58 percent from humiliation and guilt, and 58 percent from nightmares.[34]		Title VII of the Civil Rights Act of 1964 is used as a tool to eradicate workplace harassment.[36] Title VII lists the following actions of employers unlawful:		"(1) to fail or refuse to hire or to discharge any individual, or otherwise to discriminate against any individual with respect to [her or] his compensation, terms, conditions, or privileges of employment, because of such individual's race, color, religion, sex, or national origin; or (2) to limit, segregate, or classify [her or] his employees discrimination based on race, color, religion, sex and national origin … in any way which would deprive... any individual of employment opportunities or otherwise adversely affect [her or] his status as an employee, because of such individual's race, color, religion, sex, or national origin."[37]		"Most courts consider it consistent with the intent of Congress to interpret the Act liberally, and therefore, coverage under Title VII is very broad".[38] This allows victims of workplace harassment primarily use Title VII to assert their legal actions.[38] In addition, the Equal Employment Opportunity Commission (EEOC), a governmental committee that prohibits discrimination in workplace, administers the practices and violations of Title VII.[38] It issued and amended the "Guidelines on Discrimination of Sex", a more specified interpretation of Title VII.[38]		Vinson, an employee at the Meritor Savings Bank, alleged that her supervisor, Sidney Taylor, has raped and sexually harassed her starting from her first day of employment for four years.[39] However, she did not address to Taylor or higher authority, because she was afraid of dismissal.[39] Meritor Savings Bank vs. Vinson case ruled that hostile environment is considered a violation of Title VII.[39] This decision "legitimized this area of the law for complainants and, for the first time, put employers on notice that unwelcome sexual conduct will not be tolerated in the workplace."[39] This court case also added that violation of Title VII does not have to be "tangible" and "economic".[39]		Robinson, as one of the few female employees at the Jacksonville Shipyard, filed a sexual harassment report against Jacksonville Shipyard.[40] She attested that all of the pornographic images and remarks objectified her.[40] This case received high media attention, as the ACLU of Florida and ACLU of Women's Rights Project defended different parties.[40] ACLU of Florida primarily held the free speech principles, while in contrast, ACLU Women's Rights Project addressed the equality principle.[40] They openly disagreed and showed "disagreement among civil libertarians on how to apply free speech- and equality principles to the facts at issue in a workplace sexual harassment case."[40] The District Court upheld the ACLU Women's Rights Project's side, as "The District Court did not undertake the proper inquiry in determining liability.[40] Instead, the District Court proceeded from the erroneous assumption that expression can constitute harassment merely because an employee finds it offensive."[40]		
The Oxford English Dictionary (OED) is a descriptive dictionary of the English language, published by the Oxford University Press.[1] It traces the historical development of the English language, providing a comprehensive resource to scholars and academic researchers, as well as describing usage in its many variations throughout the world.[2][3] The second edition came to 21,728 pages in 20 volumes, published in 1989.		Work began on the dictionary in 1857, but it was not until 1884 that it began to be published in unbound fascicles as work continued on the project, under the name of A New English Dictionary on Historical Principles; Founded Mainly on the Materials Collected by The Philological Society. In 1895, the title The Oxford English Dictionary (OED) was first used unofficially on the covers of the series, and in 1928 the full dictionary was republished in ten bound volumes. In 1933, the title The Oxford English Dictionary fully replaced the former name in all occurrences in its reprinting as twelve volumes with a one-volume supplement. More supplements came over the years until 1989, when the second edition was published. Since 2000, a third edition of the dictionary has been underway, approximately a third of which is now complete.[citation needed]		The first electronic version of the dictionary was made available in 1988. The online version has been available since 2000, and as of April 2014 was receiving over two million hits per month. The third edition of the dictionary will probably only appear in electronic form; Nigel Portwood, chief executive of Oxford University Press, thinks it unlikely that it will ever be printed.[4][5]						As a historical dictionary, the Oxford English Dictionary explains words by showing their development rather than merely their present-day usages.[6] Therefore, it shows definitions in the order that the sense of the word began being used, including word meanings which are no longer used. Each definition is shown with numerous short usage quotations; in each case, the first quotation shows the first recorded instance of the word that the editors are aware of and, in the case of words and senses no longer in current usage, the last quotation is the last known recorded usage. This allows the reader to get an approximate sense of the time period in which a particular word has been in use, and additional quotations help the reader to ascertain information about how the word is used in context, beyond any explanation that the dictionary editors can provide.		The format of the OED's entries has influenced numerous other historical lexicography projects. The forerunners to the OED, such as the early volumes of the Deutsches Wörterbuch, had initially provided few quotations from a limited number of sources, whereas the OED editors preferred larger groups of quite short quotations from a wide selection of authors and publications. This influenced later volumes of this and other lexicographical works.[7]		According to the publishers, it would take a single person 120 years to "key in" the 59 million words of the OED second edition, 60 years to proofread them, and 540 megabytes to store them electronically.[8] As of 30 November 2005, the Oxford English Dictionary contained approximately 301,100 main entries. Supplementing the entry headwords, there are 157,000 bold-type combinations and derivatives;[9] 169,000 italicized-bold phrases and combinations;[10] 616,500 word-forms in total, including 137,000 pronunciations; 249,300 etymologies; 577,000 cross-references; and 2,412,400 usage quotations. The dictionary's latest, complete print edition (second edition, 1989) was printed in 20 volumes, comprising 291,500 entries in 21,730 pages. The longest entry in the OED2 was for the verb set, which required 60,000 words to describe some 430 senses. As entries began to be revised for the OED3 in sequence starting from M, the longest entry became make in 2000, then put in 2007, then run in 2011.[11][12][13]		Despite its impressive size, the OED is neither the world's largest nor the earliest exhaustive dictionary of a language. Another earlier large dictionary is the Grimm brothers' dictionary of the German language, begun in 1838 and completed in 1961. The first edition of the Vocabolario degli Accademici della Crusca is the first great dictionary devoted to a modern European language (Italian) and was published in 1612; the first edition of Dictionnaire de l'Académie française dates from 1694. The official dictionary of Spanish is the Diccionario de la lengua española (produced, edited, and published by the Real Academia Española), and its first edition was published in 1780. The Kangxi dictionary of Chinese was published in 1716.[14]		The dictionary began as a Philological Society project of a small group of intellectuals in London (and unconnected to Oxford University):[15]:103–4,112 Richard Chenevix Trench, Herbert Coleridge, and Frederick Furnivall, who were dissatisfied with the existing English dictionaries. The Society expressed interest in compiling a new dictionary as early as 1844,[16] but it was not until June 1857 that they began by forming an "Unregistered Words Committee" to search for words that were unlisted or poorly defined in current dictionaries. In November, Trench's report was not a list of unregistered words; instead, it was the study On Some Deficiencies in our English Dictionaries, which identified seven distinct shortcomings in contemporary dictionaries:[17]		The Society ultimately realized that the number of unlisted words would be far more than the number of words in the English dictionaries of the 19th century, and shifted their idea from covering only words that were not already in English dictionaries to a larger project. Trench suggested that a new, truly comprehensive dictionary was needed. On 7 January 1858, the Society formally adopted the idea of a comprehensive new dictionary.[15]:107–8 Volunteer readers would be assigned particular books, copying passages illustrating word usage onto quotation slips. Later the same year, the Society agreed to the project in principle, with the title A New English Dictionary on Historical Principles (NED).[18]:ix–x		Richard Chenevix Trench (1807–1886) played the key role in the project's first months, but his Church of England appointment as Dean of Westminster meant that he could not give the dictionary project the time that it required. He withdrew and Herbert Coleridge became the first editor.[19]:8–9		On 12 May 1860, Coleridge's dictionary plan was published and research was started. His house was the first editorial office. He arrayed 100,000 quotation slips in a 54 pigeon-hole grid.[19]:9 In April 1861, the group published the first sample pages; later that month, Coleridge died of tuberculosis, aged 30.[18]:x		Furnivall then became editor; he was enthusiastic and knowledgeable, yet temperamentally ill-suited for the work.[15]:110 Many volunteer readers eventually lost interest in the project, as Furnivall failed to keep them motivated. Furthermore, many of the slips had been misplaced.		Furnivall believed that, since many printed texts from earlier centuries were not readily available, it would be impossible for volunteers to efficiently locate the quotations that the dictionary needed. As a result, he founded the Early English Text Society in 1864 and the Chaucer Society in 1868 to publish old manuscripts.[18]:xii Furnivall's preparatory efforts lasted 21 years and provided numerous texts for the use and enjoyment of the general public, as well as crucial sources for lexicographers, but they did not actually involve compiling a dictionary. Furnivall recruited more than 800 volunteers to read these texts and record quotations. While enthusiastic, the volunteers were not well trained and often made inconsistent and arbitrary selections. Ultimately, Furnivall handed over nearly two tons of quotation slips and other materials to his successor.[20]		In the 1870s, Furnivall unsuccessfully attempted to recruit both Henry Sweet and Henry Nicol to succeed him. He then approached James Murray, who accepted the post of editor. In the late 1870s, Furnivall and Murray met with several publishers about publishing the dictionary. In 1878, Oxford University Press agreed with Murray to proceed with the massive project; the agreement was formalized the following year.[15]:111–2 The dictionary project finally had a publisher 20 years after the idea was conceived. It was another 50 years before the entire dictionary was complete.		Late in his editorship, Murray learned that a prolific reader named W. C. Minor was a criminal lunatic.[15]:xiii Minor was a Yale University-trained surgeon and military officer in the American Civil War, and was confined to Broadmoor Asylum for the Criminally Insane after killing a man in London. Minor invented his own quotation-tracking system, allowing him to submit slips on specific words in response to editors' requests. The story of Murray and Minor later served as the central focus of The Surgeon of Crowthorne (US title: The Professor and the Madman[15]), a popular book about the creation of the OED.		During the 1870s, the Philological Society was concerned with the process of publishing a dictionary with such an immense scope. They had pages printed by publishers, but no publication agreement was reached; both the Cambridge University Press and the Oxford University Press were approached. The OUP finally agreed in 1879 (after two years of negotiating by Sweet, Furnivall, and Murray) to publish the dictionary and to pay Murray, who was both the editor and the Philological Society president. The dictionary was to be published as interval fascicles, with the final form in four 6,400-page volumes. They hoped to finish the project in ten years.[19]:1		Murray started the project, working in a corrugated iron outbuilding called the "Scriptorium" which was lined with wooden planks, book shelves, and 1,029 pigeon-holes for the quotation slips.[18]:xiii He tracked and regathered Furnivall's collection of quotation slips, which were found to concentrate on rare, interesting words rather than common usages. For instance, there were ten times as many quotations for abusion as for abuse.[21] He appealed, through newspapers distributed to bookshops and libraries, for readers who would report "as many quotations as you can for ordinary words" and for words that were "rare, obsolete, old-fashioned, new, peculiar or used in a peculiar way".[21] Murray had American philologist and liberal arts college professor Francis March manage the collection in North America; 1,000 quotation slips arrived daily to the Scriptorium and, by 1880, there were 2,500,000.[19]:15		The first dictionary fascicle was published on 1 February 1884—twenty-three years after Coleridge's sample pages. The full title was A New English Dictionary on Historical Principles; Founded Mainly on the Materials Collected by The Philological Society; the 352-page volume, words from A to Ant, cost 12s 6d.[19]:251 The total sales were a disappointing 4,000 copies.[22]:169		The OUP saw that it would take too long to complete the work with unrevised editorial arrangements. Accordingly, new assistants were hired and two new demands were made on Murray.[19]:32–33 The first was that he move from Mill Hill to Oxford, which he did in 1885. Murray had his Scriptorium re-erected on his new property.[18]:xvii[19]		Murray resisted the second demand: that if he could not meet schedule, he must hire a second, senior editor to work in parallel to him, outside his supervision, on words from elsewhere in the alphabet. Murray did not want to share the work, feeling that he would accelerate his work pace with experience. That turned out not to be so, and Philip Gell of the OUP forced the promotion of Murray's assistant Henry Bradley (hired by Murray in 1884), who worked independently in the British Museum in London beginning in 1888. In 1896, Bradley moved to Oxford University.[19]		Gell continued harassing Murray and Bradley with his business concerns—containing costs and speeding production—to the point where the project's collapse seemed likely. Newspapers reported the harassment, particularly the Saturday Review, and public opinion backed the editors.[22]:182–83 Gell was fired, and the university reversed his cost policies. If the editors felt that the dictionary would have to grow larger, it would; it was an important work, and worth the time and money to properly finish.		Neither Murray nor Bradley lived to see it. Murray died in 1915, having been responsible for words starting with A–D, H–K, O–P, and T, nearly half the finished dictionary; Bradley died in 1923, having completed E–G, L–M, S–Sh, St, and W–We. By then, two additional editors had been promoted from assistant work to independent work, continuing without much trouble. William Craigie started in 1901 and was responsible for N, Q–R, Si–Sq, U–V, and Wo–Wy.[18]:xix The OUP had previously thought London too far from Oxford but, after 1925, Craigie worked on the dictionary in Chicago, where he was a professor.[18]:xix[19] The fourth editor was Charles Talbut Onions, who compiled the remaining ranges starting in 1914: Su–Sz, Wh–Wo, and X–Z.[23]		In 1919–1920, J. R. R. Tolkien was employed by the OED, researching etymologies of the Waggle to Warlock range;[24] later he parodied the principal editors as "The Four Wise Clerks of Oxenford" in the story Farmer Giles of Ham.[25]		By early 1894, a total of 11 fascicles had been published, or about one per year: four for A–B, five for C, and two for E.[18] Of these, eight were 352 pages long, while the last one in each group was shorter to end at the letter break (which eventually became a volume break). At this point, it was decided to publish the work in smaller and more frequent instalments; once every three months beginning in 1895 there would be a fascicle of 64 pages, priced at 2s 6d. If enough material was ready, 128 or even 192 pages would be published together. This pace was maintained until World War I forced reductions in staff.[18]:xx Each time enough consecutive pages were available, the same material was also published in the original larger fascicles.[18]:xx Also in 1895, the title Oxford English Dictionary (OED) was first used. It then appeared only on the outer covers of the fascicles; the original title was still the official one and was used everywhere else.[18]:xx		The 125th and last fascicle covered words from Wise to the end of W and was published on 19 April 1928, and the full dictionary in bound volumes followed immediately.[18]:xx		William Shakespeare is the most-quoted writer in the completed dictionary, with Hamlet his most-quoted work. George Eliot (Mary Ann Evans) is the most-quoted female writer. Collectively, the Bible is the most-quoted work (but in many different translations); the most-quoted single work is Cursor Mundi.[8]		Between 1928 and 1933, enough additional material had been compiled to make a one-volume supplement, so the dictionary was reissued as the set of 12 volumes and a one-volume supplement in 1933.[18]		In 1933, Oxford had finally put the dictionary to rest; all work ended, and the quotation slips went into storage. However, the English language continued to change and, by the time 20 years had passed, the dictionary was outdated.[26]		There were three possible ways to update it. The cheapest would have been to leave the existing work alone and simply compile a new supplement of perhaps one or two volumes; but then anyone looking for a word or sense and unsure of its age would have to look in three different places. The most convenient choice for the user would have been for the entire dictionary to be re-edited and retypeset, with each change included in its proper alphabetical place; but this would have been the most expensive option, with perhaps 15 volumes required to be produced. The OUP chose a middle approach: combining the new material with the existing supplement to form a larger replacement supplement.		Robert Burchfield was hired in 1957 to edit the second supplement;[27] Onions turned 84 that year but was still able to make some contributions, as well. The work on the supplement was expected to take about seven years.[26] It actually took 29 years, by which time the new supplement (OEDS) had grown to four volumes, starting with A, H, O, and Sea. They were published in 1972, 1976, 1982, and 1986 respectively, bringing the complete dictionary to 16 volumes, or 17 counting the first supplement.		Burchfield emphasized the inclusion of modern-day language and, through the supplement, the dictionary was expanded to include a wealth of new words from the burgeoning fields of science and technology, as well as popular culture and colloquial speech. Burchfield said that he broadened the scope to include developments of the language in English-speaking regions beyond the United Kingdom, including North America, Australia, New Zealand, South Africa, India, Pakistan, and the Caribbean. Burchfield also removed some smaller entries that had been added to the 1933 supplement, for reasons of space;[28] in 2012, an analysis by lexicographer Sarah Ogilvie revealed that many of these entries were in fact foreign loanwords, despite Burchfield's attempt to include more such words. The proportion was estimated from a sample calculation to amount to 17% of the foreign loan words and words from regional forms of English. Many of these had only a single recorded usage, but it ran against what was thought to be the established OED editorial practice and a perception that he had opened up the dictionary to "World English".[29][30][31]		By the time the new supplement was completed, it was clear that the full text of the dictionary would now need to be computerized. Achieving this would require retyping it once, but thereafter it would always be accessible for computer searching – as well as for whatever new editions of the dictionary might be desired, starting with an integration of the supplementary volumes and the main text. Preparation for this process began in 1983, and editorial work started the following year under the administrative direction of Timothy J. Benbow, with John A. Simpson and Edmund S. C. Weiner as co-editors.[32] In 2016, Simpson published his memoir chronicling his years at the OED. See The Word Detective: Searching for the Meaning of It All at the Oxford English Dictionary - A Memoir. Basic Books, New York.		And so the New Oxford English Dictionary (NOED) project began. In the United States, more than 120 typists of the International Computaprint Corporation (now Reed Tech) started keying in over 350,000,000 characters, their work checked by 55 proof-readers in England.[32] Retyping the text alone was not sufficient; all the information represented by the complex typography of the original dictionary had to be retained, which was done by marking up the content in SGML.[32] A specialized search engine and display software were also needed to access it. Under a 1985 agreement, some of this software work was done at the University of Waterloo, Canada, at the Centre for the New Oxford English Dictionary, led by Frank Tompa and Gaston Gonnet; this search technology went on to become the basis for the Open Text Corporation.[33] Computer hardware, database and other software, development managers, and programmers for the project were donated by the British subsidiary of IBM; the colour syntax-directed editor for the project, LEXX, was written by Mike Cowlishaw of IBM.[34] The University of Waterloo, in Canada, volunteered to design the database. A. Walton Litz, an English professor at Princeton University who served on the Oxford University Press advisory council, was quoted in Time as saying "I've never been associated with a project, I've never even heard of a project, that was so incredibly complicated and that met every deadline."[35]		By 1989, the NOED project had achieved its primary goals, and the editors, working online, had successfully combined the original text, Burchfield's supplement, and a small amount of newer material, into a single unified dictionary. The word "new" was again dropped from the name, and the second edition of the OED, or the OED2, was published. The first edition retronymically became the OED1.		The Oxford English Dictionary 2 was printed in 20 volumes. For the first time, there was no attempt to start them on letter boundaries, and they were made roughly equal in size. The 20 volumes started with A, B.B.C., Cham, Creel, Dvandva, Follow, Hat, Interval, Look, Moul, Ow, Poise, Quemadero, Rob, Ser, Soot, Su, Thru, Unemancipated, and Wave.		The content of the OED2 is mostly just a reorganization of the earlier corpus, but the retypesetting provided an opportunity for two long-needed format changes. The headword of each entry was no longer capitalized, allowing the user to readily see those words that actually require a capital letter.[36] Murray had devised his own notation for pronunciation, there being no standard available at the time, whereas the OED2 adopted the modern International Phonetic Alphabet.[36][37] Unlike the earlier edition, all foreign alphabets except Greek were transliterated.[36]		The British quiz show Countdown has awarded the leather-bound complete version to the champions of each series since its inception in 1982.[38]		When the print version of the second edition was published in 1989, the response was enthusiastic. Author Anthony Burgess declared it "the greatest publishing event of the century", as quoted by the Los Angeles Times.[39] Time dubbed the book "a scholarly Everest",[35] and Richard Boston, writing for The Guardian, called it "one of the wonders of the world".[40]		The supplements and their integration into the second edition were a great improvement to the OED as a whole, but it was recognized that most of the entries were still fundamentally unaltered from the first edition. Much of the information in the dictionary published in 1989 was already decades out of date, though the supplements had made good progress towards incorporating new vocabulary. Yet many definitions contained disproven scientific theories, outdated historical information, and moral values that were no longer widely accepted.[41][42] Furthermore, the supplements had failed to recognize many words in the existing volumes as obsolete by the time of the second edition's publication, meaning that thousands of words were marked as current despite no recent evidence of their use.[43]		Accordingly, it was recognized that work on a third edition would have to begin to rectify these problems.[41] The first attempt to produce a new edition came with the Oxford English Dictionary Additions Series, a new set of supplements to complement the OED2 with the intention of producing a third edition from them.[44] The previous supplements appeared in alphabetical installments, whereas the new series had a full A–Z range of entries within each individual volume, with a complete alphabetical index at the end of all words revised so far, each listed with the volume number which contained the revised entry.[44]		However, in the end only three Additions volumes were published this way, two in 1993 and one in 1997,[45][46][47] each containing about 3,000 new definitions.[8] The possibilities of the World Wide Web and new computer technology in general meant that the processes of researching the dictionary and of publishing new and revised entries could be vastly improved. New text search databases offered vastly more material for the editors of the dictionary to work with, and with publication on the Web as a possibility, the editors could publish revised entries much more quickly and easily than ever before.[48] A new approach was called for, and for this reason it was decided to embark on a new, complete revision of the dictionary.		Beginning with the launch of the first OED Online site in 2000, the editors of the dictionary began a major revision project to create a completely revised third edition of the dictionary (OED3), expected to be completed in 2037[49][50] at a projected cost of about £34 million.[51]		Revisions were started at the letter M, with new material appearing every three months on the OED Online website. The editors chose to start the revision project from the middle of the dictionary in order that the overall quality of entries be made more even, since the later entries in the OED1 generally tended to be better than the earlier ones. However, in March 2008, the editors announced that they would alternate each quarter between moving forward in the alphabet as before and updating "key English words from across the alphabet, along with the other words which make up the alphabetical cluster surrounding them".[52] With the relaunch of the OED Online website in December 2010, alphabetical revision was abandoned altogether.[53]		The revision is expected to roughly double the dictionary in size.[5][54] Apart from general updates to include information on new words and other changes in the language, the third edition brings many other improvements, including changes in formatting and stylistic conventions to make entries clearer to read and enable more thorough searches to be made by computer, more thorough etymological information, and a general change of focus away from individual words towards more general coverage of the language as a whole.[48][55] While the original text drew its quotations mainly from literary sources such as novels, plays, and poetry, with additional material from newspapers and academic journals, the new edition will reference more kinds of material that were unavailable to the editors of previous editions, such as wills, inventories, account books, diaries, journals, and letters.[54]		John Simpson was the first chief editor of the OED3. He retired in 2013 and was replaced by Michael Proffitt, who is the eighth chief editor of the dictionary.[56]		The production of the new edition takes full advantage of computer technology, particularly since the June 2005 inauguration of the whimsically named "Perfect All-Singing All-Dancing Editorial and Notation Application", or "Pasadena". With this XML-based system, the attention of lexicographers can be directed more to matters of content than to presentation issues such as the numbering of definitions. The new system has also simplified the use of the quotations database, and enabled staff in New York to work directly on the dictionary in the same way as their Oxford-based counterparts.[57]		Other important computer uses include internet searches for evidence of current usage, and e-mail submissions of quotations by readers and the general public.[58]		Wordhunt was a 2005 appeal to the general public for help in providing citations for 50 selected recent words, and produced antedatings for many. The results were reported in a BBC TV series, Balderdash and Piffle. The OED's small army of devoted readers continue to contribute quotations: the department currently receives about 200,000 a year.[59]		OED currently contains over 600,000 entries.[60]		In 1971, the 13-volume OED1 (1933) was reprinted as a two-volume Compact Edition, by photographically reducing each page to one-half its linear dimensions; each compact edition page held four OED1 pages in a four-up ("4-up") format. The two volume letters were A and P; the first supplement was at the second volume's end.		The Compact Edition included, in a small slip-case drawer, a magnifying glass to help in reading reduced type. Many copies were inexpensively distributed through book clubs. In 1987, the second supplement was published as a third volume to the Compact Edition. In 1991, for the OED2, the compact edition format was re-sized to one-third of original linear dimensions, a nine-up ("9-up") format requiring greater magnification, but allowing publication of a single-volume dictionary. It was accompanied by a magnifying glass as before and A User's Guide to the "Oxford English Dictionary", by Donna Lee Berg.[61] After these volumes were published, though, book club offers commonly continued to sell the two-volume 1971 Compact Edition.[25]		Once the text of the dictionary was digitized and online, it was also available to be published on CD-ROM. The text of the first edition was made available in 1987.[62] Afterward, three versions of the second edition were issued. Version 1 (1992) was identical in content to the printed second edition, and the CD itself was not copy-protected. Version 2 (1999) included the Oxford English Dictionary Additions of 1993 and 1997.		Version 3.0 was released in 2002 with additional words from the OED3 and software improvements. Version 3.1.1 (2007) added support for hard disk installation, so that the user does not have to insert the CD to use the dictionary. It has been reported that this version will work on operating systems other than Microsoft Windows, using emulation programs.[63][64] Version 4.0 of the CD has been available since June 2009 and works with Windows 7 and Mac OS X (10.4 or later).[65] This version uses the CD drive for installation, running only from the hard drive.		On 14 March 2000, the Oxford English Dictionary Online (OED Online) became available to subscribers.[66] The online database contains the entire OED2 and is updated quarterly with revisions that will be included in the OED3 (see above). The online edition is the most up-to-date version of the dictionary available. The OED web site is not optimized for mobile devices, but the developers have stated that there are plans to provide an API that would enable developers to develop different interfaces for querying the OED.[67]		The price for an individual to use this edition is £195 or US$295 every year, even after a reduction in 2004; consequently, most subscribers are large organizations such as universities. Some public libraries and companies have subscribed, as well, including public libraries in the United Kingdom, where access is funded by the Arts Council,[68] and public libraries in New Zealand.[69][70] Individuals who belong to a library which subscribes to the service are able to use the service from their own home without charge.		The OED's utility and renown as a historical dictionary have led to numerous offspring projects and other dictionaries bearing the Oxford name, though not all are directly related to the OED itself.		The Shorter Oxford English Dictionary, originally started in 1902 and completed in 1933,[72] is an abridgement of the full work that retains the historical focus, but does not include any words which were obsolete before 1700 except those used by Shakespeare, Milton, Spenser, and the King James Bible.[73] A completely new edition was produced from the OED2 and published in 1993,[74] with further revisions following in 2002 and 2007.		The Concise Oxford Dictionary is a different work, which aims to cover current English only, without the historical focus. The original edition, mostly based on the OED1, was edited by Francis George Fowler and Henry Watson Fowler and published in 1911, before the main work was completed.[75] Revised editions appeared throughout the twentieth century to keep it up to date with changes in English usage.		In 1998 the New Oxford Dictionary of English (NODE) was published. While also aiming to cover current English, NODE was not based on the OED. Instead, it was an entirely new dictionary produced with the aid of corpus linguistics.[76] Once NODE was published, a similarly brand-new edition of the Concise Oxford Dictionary followed, this time based on an abridgement of NODE rather than the OED; NODE (under the new title of the Oxford Dictionary of English, or ODE) continues to be principal source for Oxford's product line of current-English dictionaries, including the New Oxford American Dictionary, with the OED now only serving as the basis for scholarly historical dictionaries.		The OED lists British headword spellings (e.g., labour, centre) with variants following (labor, center, etc.). For the suffix more commonly spelt -ise in British English, OUP policy dictates a preference for the spelling -ize, e.g., realize vs. realise and globalization vs. globalisation. The rationale is etymological, in that the English suffix is mainly derived from the Greek suffix -ιζειν, (-izein), or the Latin -izāre.[77] However, -ze is also sometimes treated as an Americanism insofar as the -ze suffix has crept into words where it did not originally belong, as with analyse (British English), which is spelt analyze in American English.[78][79]		Despite, and at the same time precisely because of, its claim of authority[80] on the English language, the Oxford English Dictionary has been criticised since at least the 1960s from various angles. It has become a target precisely because of its scope, its claims to authority, its British-centredness and relative neglect of World Englishes,[81] its implied but not acknowledged focus on literary language and, above all, its influence. The OED, as a commercial product, has always had to manoeuvre a thin line between PR, marketing and scholarship and one can argue that its biggest problem is the critical uptake of the work by the interested public. In his review of the 1982 supplement,[82] University of Oxford linguist Roy Harris writes that criticizing the OED is extremely difficult because "one is dealing not just with a dictionary but with a national institution", one that "has become, like the English monarchy, virtually immune from criticism in principle". He further notes that neologisms from respected "literary" authors such as Samuel Beckett and Virginia Woolf are included, whereas usage of words in newspapers or other less "respectable" sources hold less sway, even though they may be commonly used. He writes that the OED's "[b]lack-and-white lexicography is also black-and-white in that it takes upon itself to pronounce authoritatively on the rights and wrongs of usage", faulting the dictionary's prescriptive rather than descriptive usage. To Harris, this prescriptive classification of certain usages as "erroneous" and the complete omission of various forms and usages cumulatively represent the "social bias[es]" of the (presumably well-educated and wealthy) compilers. However, the identification of "erroneous and catachrestic" usages is being removed from third edition entries,[83][84] sometimes in favour of usage notes describing the attitudes to language which have previously led to these classifications.[85]		Harris also faults the editors' "donnish conservatism" and their adherence to prudish Victorian morals, citing as an example the non-inclusion of "various centuries-old 'four-letter words'" until 1972. However, no English dictionary included such words, for fear of possible prosecution under British obscenity laws, until after the conclusion of the Lady Chatterley's Lover obscenity trial in 1960. The first dictionary to include the word fuck was the Penguin English Dictionary of 1965.[86] Joseph Wright's English Dialect Dictionary had included shit in 1905.[87]		The OED's claims of authority have also been questioned by linguists such as Pius ten Hacken, who notes that the dictionary actively strives towards definitiveness and authority but can only achieve those goals in a limited sense, given the difficulties of defining the scope of what it includes.[88]		Founding editor James Murray was also reluctant to include scientific terms, despite their documentation, unless he felt that they were widely enough used. In 1902, he declined to add the word "radium" to the dictionary.[89][90]		In contrast, Tim Bray, co-creator of Extensible Markup Language (XML), credits the OED as the developing inspiration of that markup language.[91] Similarly, author Anu Garg, founder of Wordsmith.org, has called the Oxford English Dictionary a "lex icon".[92]		
Recruitment (hiring) is a core function of human resource management. It is the first step of appointment. Recruitment refers to the overall process of attracting, selecting and appointing suitable candidates for jobs (either permanent or temporary) within an organization. Recruitment can also refer to processes involved in choosing individuals for unpaid positions, such as voluntary roles or unpaid trainee roles. Managers, human resource generalists and recruitment specialists may be tasked with carrying out recruitment, but in some cases public-sector employment agencies, commercial recruitment agencies, or specialist search consultancies are used to undertake parts of the process. Internet-based technologies to support all aspects of recruitment have become widespread.[1]						In situations where multiple new jobs are created and recruited for the first time or vacancies are there or the naturein such documents as job descriptions and job specifications. Often, a company already has job descriptions for existing positions. Where already drawn up, these documents may require review and updating to reflect current requirements. Prior to the recruitment stage, a person specification should be finalized.[2]		Sourcing is the use of one or more strategies to attract or identify candidates to fill job vacancies. It may involve internal and/or external recruitment advertising, using appropriate media, such as job portals,local or national newspapers, social media, business media, specialist recruitment media, professional publications, window advertisements, job centers, or in a variety of ways via the internet.		Alternatively, employers may use recruitment consultancies or agencies to find otherwise scarce candidates—who, in many cases, may be content in their current positions and are not actively looking to move. This initial research for candidates—also called name generation—produces contact information for potential candidates, whom the recruiter can then discreetly contact and screen.[2]		Various psychological tests can assess a variety of KSAOs, including literacy. Assessments are also available to measure physical ability. Recruiters and agencies may use applicant tracking systems to filter candidates, along with software tools for psychometric testing and performance-based assessment.[3] In many countries, employers are legally mandated to ensure their screening and selection processes meet equal opportunity and ethical standards.[2]		Employers are likely to recognize the value of candidates who encompass soft skills such as interpersonal or team leadership.[4] Many companies, including multinational organizations and those that recruit from a range of nationalities, are also often concerned about whether candidate fits the prevailing company culture.[5]		The word disability carries few positive connotations for most employers. Research has shown that employer biases tend to improve through first-hand experience and exposure with proper supports for the employee[6] and the employer making the hiring decisions. As for most companies, money and job stability are two of the contributing factors to the productivity of a disabled employee, which in return equates to the growth and success of a business. Hiring disabled workers produce more advantages than disadvantages.[7] There is no difference in the daily production of a disabled worker.[8] Given their situation, they are more likely to adapt to their environmental surroundings and acquaint themselves with equipment, enabling them to solve problems and overcome adversity as with other employees. The U.S. IRS grants companies Disabled Access Credit when they meet eligibility criteria.[9]		Many major corporations recognize the need for diversity in hiring to compete successfully in a global economy.[10] Other organizations, for example universities and colleges, have been slow to embrace diversity as an essential value for their success.[11]		Recruitment Process Outsourcing, or commonly known as "RPO" is a form of business process outsourcing (BPO) where a company engages a third party provider to manage all or part of its recruitment process.		Internal recruitment (not to be confused with internal recruiters!) refers to the process of a candidate being selected from the existing workforce to take up a new job in the same organization, perhaps as a promotion, or to provide career development opportunity, or to meet a specific or urgent organizational need. Advantages include the organization's familiarity with the employee and their competencies insofar as they are revealed in their current job, and their willingness to trust said employee. It can be quicker and have a lower cost to hire someone internally.[12]		An employee referral program is a system where existing employees recommend prospective candidates for the job offered, and in some organizations if the suggested candidate is hired, the employee receives a cash bonus.[13]		Niche firms tend to focus on building ongoing relationships with their candidates, as the same candidates may be placed many times throughout their careers. Online resources have developed to help find niche recruiters.[14] Niche firms also develop knowledge on specific employment trends within their industry of focus (e.g., the energy industry) and are able to identify demographic shifts such as aging and its impact on the industry.[15]		Social recruiting is the use of social media for recruiting including sites like Facebook and Twitter or career-oriented social networking sites such as LinkedIn and XING.[16][17] It is a rapidly growing sourcing technique, especially with middle-aged people. On Google+, the fastest-growing age group is 45–54. On Twitter, the expanding generation is people from ages 55–64.[18]		Mobile recruiting is a recruitment strategy that uses mobile technology to attract, engage and convert candidates. Mobile recruiting is often cited as a growing opportunity for recruiters to connect with candidates more efficiently with "over 89% of job seekers saying their mobile device will be an important tool and resource for their job search."[19]		Some recruiters work by accepting payments from job seekers, and in return help them to find a job. This is illegal in some countries, such as in the United Kingdom, in which recruiters must not charge candidates for their services (although websites such as LinkedIn may charge for ancillary job-search-related services). Such recruiters often refer to themselves as "personal marketers" and "job application services" rather than as recruiters.		Using Multiple-criteria decision analysis tools such as Analytic Hierarchy Process (AHP) and combining it with conventional recruitment methods provides an added advantage by helping the recruiters to make decisions when there are several diverse criteria to be considered or when the applicants lack past experience; for instance recruitment of fresh university graduates.[20]		In some companies where the recruitment volume is high, it is common to see a multi tier recruitment model where the different sub-functions are being group together to achieve efficiency.		An example of a 3 tier recruitment model:		
Educational technology is "the study and ethical practice of facilitating learning and improving performance by creating, using, and managing appropriate technological processes and resources".[1]		Educational technology is the use of both physical hardware and educational theoretics. It encompasses several domains, including learning theory, computer-based training, online learning, and, where mobile technologies are used, m-learning.[2] Accordingly, there are several discrete aspects to describing the intellectual and technical development of educational technology:		An educational technologist is someone who is trained in the field of educational technology. Educational technologists try to analyze, design, develop, implement and evaluate process and tools to enhance learning.[3] While the term educational technologist is used primarily in the United States, learning technologist is synonymous and used in the UK[4] as well as Canada.		Richey defined educational technology as "the study and ethical practice of facilitating learning and improving performance by creating, using and managing appropriate technological processes and resources".[5] The Association for Educational Communications and Technology (AECT) denoted instructional technology as "the theory and practice of design, development, utilization, management, and evaluation of processes and resources for learning".[6][7][8] As such, educational technology refers to all valid and reliable applied education sciences, such as equipment, as well as processes and procedures that are derived from scientific research, and in a given context may refer to theoretical, algorithmic or heuristic processes: it does not necessarily imply physical technology. Educational technology is the process of integrating technology into education in a positive manner that promotes a more diverse learning environment and a way for students to learn how to use technology as well as their common assignments		Given this definition, educational technology is an inclusive term for both the material tools and the theoretical foundations for supporting learning and teaching. Educational technology is not restricted to high technology.[9] Education technology is anything that enhances classroom learning in the utilization of blended or online learning.[10]		However, modern electronic educational technology is an important part of society today.[11] Educational technology encompasses e-learning, instructional technology, information and communication technology (ICT) in education, EdTech, learning technology, multimedia learning, technology-enhanced learning (TEL), computer-based instruction (CBI), computer managed instruction, computer-based training (CBT), computer-assisted instruction or computer-aided instruction (CAI),[12] internet-based training (IBT), flexible learning, web-based training (WBT), online education, digital educational collaboration, distributed learning, computer-mediated communication, cyber-learning, and multi-modal instruction, virtual education, personal learning environments, networked learning, virtual learning environments (VLE) (which are also called learning platforms), m-learning, ubiquitous learning and digital education.		Each of these numerous terms has had its advocates, who point up potential distinctive features.[13] However, many terms and concepts in educational technology have been defined nebulously; for example, Fiedler's review of the literature found a complete lack agreement of the components of a personal learning environment.[14] Moreover, Moore saw these terminologies as emphasizing particular features such as digitization approaches, components or delivery methods rather than being fundamentally dissimilar in concept or principle.[13] For example, m-learning emphasizes mobility, which allows for altered timing, location, accessibility and context of learning;[15] nevertheless, its purpose and conceptual principles are those of educational technology.[13]		In practice, as technology has advanced, the particular "narrowly defined" terminological aspect that was initially emphasized by name has blended into the general field of educational technology.[13] Initially, "virtual learning" as narrowly defined in a semantic sense implied entering an environmental simulation within a virtual world,[16][17] for example in treating posttraumatic stress disorder (PTSD).[18][19] In practice, a "virtual education course" refers to any instructional course in which all, or at least a significant portion, is delivered by the Internet. "Virtual" is used in that broader way to describe a course that is not taught in a classroom face-to-face but through a substitute mode that can conceptually be associated "virtually" with classroom teaching, which means that people do not have to go to the physical classroom to learn. Accordingly, virtual education refers to a form of distance learning in which course content is delivered by various methods such as course management applications, multimedia resources, and videoconferencing.[20]		As a further example, ubiquitous learning emphasizes an omnipresent learning milieu.[21] Educational content, pervasively embedded in objects, is all around the learner, who may not even be conscious of the learning process: students may not have to do anything in order to learn, they just have to be there.[21][22] The combination of adaptive learning, using an individualized interface and materials, which accommodate to an individual, who thus receives personally differentiated instruction, with ubiquitous access to digital resources and learning opportunities in a range of places and at various times, has been termed smart learning.[23][24][25] Smart learning is a component of the smart city concept.[26][27]		Helping people learn in ways that are easier, faster, surer, or less expensive can be traced back to the emergence of very early tools, such as paintings on cave walls.[28][29] Various types of abacus have been used. Writing slates and blackboards have been used for at least a millennium.[30] From their introduction, books and pamphlets have held a prominent role in education. From the early twentieth century, duplicating machines such as the mimeograph and Gestetner stencil devices were used to produce short copy runs (typically 10–50 copies) for classroom or home use. The use of media for instructional purposes is generally traced back to the first decade of the 20th century[31] with the introduction of educational films (1900s) and Sidney Pressey's mechanical teaching machines (1920s). The first all multiple choice, large-scale assessment was the Army Alpha, used to assess the intelligence and more specifically the aptitudes of World War I military recruits. Further large-scale use of technologies was employed in training soldiers during and after WWII using films and other mediated materials, such as overhead projectors. The concept of hypertext is traced to the description of memex by Vannevar Bush in 1945.		Slide projectors were widely used during the 1950s in educational institutional settings. Cuisenaire rods were devised in the 1920s and saw widespread use from the late 1950s.		In 1960, the University of Illinois initiated a classroom system based in linked computer terminals where students could access informational resources on a particular course while listening to the lectures that were recorded via some form of remotely linked device like a television or audio device.[32]		In the mid 1960s Stanford University psychology professors Patrick Suppes and Richard C. Atkinson experimented with using computers to teach arithmetic and spelling via Teletypes to elementary school students in the Palo Alto Unified School District in California.[33][34] Stanford's Education Program for Gifted Youth is descended from those early experiments.		In 1971, Ivan Illich published a hugely influential book called, Deschooling Society, in which he envisioned "learning webs" as a model for people to network the learning they needed. The 1970s and 1980s saw notable contributions in computer-based learning by Murray Turoff and Starr Roxanne Hiltz at the New Jersey Institute of Technology[35] as well as developments at the University of Guelph in Canada.[36] In the UK, the Council for Educational Technology supported the use of educational technology, in particular administering the government's National Development Programme in Computer Aided Learning[37] (1973–77) and the Microelectronics Education Programme (1980–86).		By the mid-1980s, accessing course content became possible at many college libraries. In computer-based training (CBT) or computer-based learning (CBL), the learning interaction was between the student and computer drills or micro-world simulations.		Digitized communication and networking in education started in the mid-1980s. Educational institutions began to take advantage of the new medium by offering distance learning courses using computer networking for information. Early e-learning systems, based on computer-based learning/training often replicated autocratic teaching styles whereby the role of the e-learning system was assumed to be for transferring knowledge, as opposed to systems developed later based on computer supported collaborative learning (CSCL), which encouraged the shared development of knowledge.		Videoconferencing was an important forerunner to the educational technologies known today. This work was especially popular with museum education. Even in recent years, videoconferencing has risen in popularity to reach over 20,000 students across the United States and Canada in 2008–2009. Disadvantages of this form of educational technology are readily apparent: image and sound quality is often grainy or pixelated; videoconferencing requires setting up a type of mini-television studio within the museum for broadcast, space becomes an issue; and specialised equipment is required for both the provider and the participant.[38]		The Open University in Britain[36] and the University of British Columbia (where Web CT, now incorporated into Blackboard Inc., was first developed) began a revolution of using the Internet to deliver learning,[39] making heavy use of web-based training, online distance learning and online discussion between students.[40] Practitioners such as Harasim (1995)[41] put heavy emphasis on the use of learning networks.		With the advent of World Wide Web in the 1990s, teachers embarked on the method using emerging technologies to employ multi-object oriented sites, which are text-based online virtual reality systems, to create course websites along with simple sets of instructions for its students.		Text book publishers also explored ways to utilize both the Internet and CD ROM technology as an extension to traditional learning. In 1994, Simon and Schuster was the one of first to pioneer in this area, launching the New Media Group through its then Higher-Ed subsidiary Prentice Hall. Among the New Media Group's members was future MP3 Newswire publisher Richard Menta, whose key project was the Guest Lecture Series. This series was the first successful delivery of online video lectures to universities.[42] The inaugural lecture was streamed in December 1996 with Harvard physics professor Dr. Eric Mazur presenting on Peer Instruction.[43]		By 1994, the first online high school had been founded. In 1997, Graziadei described criteria for evaluating products and developing technology-based courses that include being portable, replicable, scalable, affordable, and having a high probability of long-term cost-effectiveness.[44]		Improved Internet functionality enabled new schemes of communication with multimedia or webcams. The National Center for Education Statistics estimate the number of K-12 students enrolled in online distance learning programs increased by 65 percent from 2002 to 2005, with greater flexibility, ease of communication between teacher and student, and quick lecture and assignment feedback.		According to a 2008 study conducted by the U.S Department of Education, during the 2006–2007 academic year about 66% of postsecondary public and private schools participating in student financial aid programs offered some distance learning courses; records show 77% of enrollment in for-credit courses with an online component.[45] In 2008, the Council of Europe passed a statement endorsing e-learning's potential to drive equality and education improvements across the EU.[46]		Computer-mediated communication (CMC) is between learners and instructors, mediated by the computer. In contrast, CBT/CBL usually means individualized (self-study) learning, while CMC involves educator/tutor facilitation and requires scenarization of flexible learning activities. In addition, modern ICT provides education with tools for sustaining learning communities and associated knowledge management tasks.		Students growing up in this digital age have extensive exposure to a variety of media.[47][48] Major high-tech companies such as Google, Verizon and Microsoft have funded schools to provide them the ability to teach their students through technology, in the hope that this would lead to improved student performance.[49]		2015 was the first year that private nonprofit organizations enrolled more online students than for-profits, although public universities still enrolled the highest number of online students. In the fall of 2015, more than 6 million students enrolled in at least one online course.[50]		Various pedagogical perspectives or learning theories may be considered in designing and interacting with educational technology. E-learning theory examines these approaches. These theoretical perspectives are grouped into three main theoretical schools or philosophical frameworks: behaviorism, cognitivism and constructivism.		This theoretical framework was developed in the early 20th century based on animal learning experiments by Ivan Pavlov, Edward Thorndike, Edward C. Tolman, Clark L. Hull, and B.F. Skinner. Many psychologists used these results to develop theories of human learning, but modern educators generally see behaviorism as one aspect of a holistic synthesis. Teaching in behaviorism has been linked to training, emphasizing the animal learning experiments. Since behaviorism consists of the view of teaching people how to something with rewards and punishments, it is related to training people.[51]		B.F. Skinner wrote extensively on improvements of teaching based on his functional analysis of verbal behavior[52][53] and wrote "The Technology of Teaching",[54][55] an attempt to dispel the myths underlying contemporary education as well as promote his system he called programmed instruction. Ogden Lindsley developed a learning system, named Celeration, that was based on behavior analysis but that substantially differed from Keller's and Skinner's models.		Cognitive science underwent significant change in the 1960s and 1970s. While retaining the empirical framework of behaviorism, cognitive psychology theories look beyond behavior to explain brain-based learning by considering how human memory works to promote learning. The Atkinson-Shiffrin memory model and Baddeley's working memory model were established as theoretical frameworks. Computer Science and Information Technology have had a major influence on Cognitive Science theory. The Cognitive concepts of working memory (formerly known as short term memory) and long term memory have been facilitated by research and technology from the field of Computer Science. Another major influence on the field of Cognitive Science is Noam Chomsky. Today researchers are concentrating on topics like cognitive load, information processing and media psychology. These theoretical perspectives influence instructional design.[56]		Educational psychologists distinguish between several types of constructivism: individual (or psychological) constructivism, such as Piaget's theory of cognitive development, and social constructivism. This form of constructivism has a primary focus on how learners construct their own meaning from new information, as they interact with reality and with other learners who bring different perspectives. Constructivist learning environments require students to use their prior knowledge and experiences to formulate new, related, and/or adaptive concepts in learning (Termos, 2012[57]). Under this framework the role of the teacher becomes that of a facilitator, providing guidance so that learners can construct their own knowledge. Constructivist educators must make sure that the prior learning experiences are appropriate and related to the concepts being taught. Jonassen (1997) suggests "well-structured" learning environments are useful for novice learners and that "ill-structured" environments are only useful for more advanced learners. Educators utilizing a constructivist perspective may emphasize an active learning environment that may incorporate learner centered problem-based learning, project-based learning, and inquiry-based learning, ideally involving real-world scenarios, in which students are actively engaged in critical thinking activities. An illustrative discussion and example can be found in the 1980s deployment of constructivist cognitive learning in computer literacy, which involved programming as an instrument of learning.[58]:224 LOGO, a programming language, embodied an attempt to integrate Piagetan ideas with computers and technology.[58][59] Initially there were broad, hopeful claims, including "perhaps the most controversial claim" that it would "improve general problem-solving skills" across disciplines.[58]:238 However, LOGO programming skills did not consistently yield cognitive benefits.[58]:238 It was "not as concrete" as advocates claimed, it privileged "one form of reasoning over all others," and it was difficult to apply the thinking activity to non-LOGO-based activities.[60] By the late 1980s, LOGO and other similar programming languages had lost their novelty and dominance and were gradually de-emphasized amid criticisms.[61]		The extent to which e-learning assists or replaces other learning and teaching approaches is variable, ranging on a continuum from none to fully online distance learning.[62][63] A variety of descriptive terms have been employed (somewhat inconsistently) to categorize the extent to which technology is used. For example, 'hybrid learning' or 'blended learning' may refer to classroom aids and laptops, or may refer to approaches in which traditional classroom time is reduced but not eliminated, and is replaced with some online learning.[64][65][66] 'Distributed learning' may describe either the e-learning component of a hybrid approach, or fully online distance learning environments.[62]		E-learning may either be synchronous or asynchronous. Synchronous learning occurs in real-time, with all participants interacting at the same time, while asynchronous learning is self-paced and allows participants to engage in the exchange of ideas or information without the dependency of other participants′ involvement at the same time.		Synchronous learning refers to the exchange of ideas and information with one or more participants during the same period. Examples are face-to-face discussion, online real-time live teacher instruction and feedback, Skype conversations, and chat rooms or virtual classrooms where everyone is online and working collaboratively at the same time. Since students are working collaboratively, synchronized learning helps students create an open mind because they have to listen and learn from their peers. Synchronized learning fosters online awareness and improves many students' writing skills.[67]		Asynchronous learning may use technologies such as email, blogs, wikis, and discussion boards, as well as web-supported textbooks,[68] hypertext documents, audio[69] video courses, and social networking using web 2.0. At the professional educational level, training may include virtual operating rooms. Asynchronous learning is beneficial for students who have health problems or who have child care responsibilities. They have the opportunity to complete their work in a low stress environment and within a more flexible time frame.[40] In asynchronous online courses, students proceed at their own pace. If they need to listen to a lecture a second time, or think about a question for a while, they may do so without fearing that they will hold back the rest of the class. Through online courses, students can earn their diplomas more quickly, or repeat failed courses without the embarrassment of being in a class with younger students. Students have access to an incredible variety of enrichment courses in online learning, and can participate in college courses, internships, sports, or work and still graduate with their class.		Computer-based training (CBT) refers to self-paced learning activities delivered on a computer or handheld device such as a tablet or smartphone. CBT initially delivered content via CD-ROM, and typically presented content linearly, much like reading an online book or manual. For this reason, CBT is often used to teach static processes, such as using software or completing mathematical equations. Computer-based training is conceptually similar to web-based training (WBT) which are delivered via Internet using a web browser.		Assessing learning in a CBT is often by assessments that can be easily scored by a computer such as multiple choice questions, drag-and-drop, radio button, simulation or other interactive means. Assessments are easily scored and recorded via online software, providing immediate end-user feedback and completion status. Users are often able to print completion records in the form of certificates.		CBTs provide learning stimulus beyond traditional learning methodology from textbook, manual, or classroom-based instruction. CBTs can be a good alternative to printed learning materials since rich media, including videos or animations, can be embedded to enhance the learning.		Help, CBTs pose some learning challenges. Typically, the creation of effective CBTs requires enormous resources. The software for developing CBTs (such as Flash or Adobe Director) is often more complex than a subject matter expert or teacher is able to use. The lack of human interaction can limit both the type of content that can be presented and the type of assessment that can be performed, and may need supplementation with online discussion or other interactive elements.		Computer-supported collaborative learning (CSCL) uses instructional methods designed to encourage or require students to work together on learning tasks, allowing social learning. CSCL is similar in concept to the terminology, "e-learning 2.0"[70] and "networked collaborative learning" (NCL).[71] With Web 2.0 advances, sharing information between multiple people in a network has become much easier and use has increased.[72]:1[73] One of the main reasons for its usage states that it is "a breeding ground for creative and engaging educational endeavors."[72]:2 Learning takes place through conversations about content and grounded interaction about problems and actions. This collaborative learning differs from instruction in which the instructor is the principal source of knowledge and skills. The neologism "e-learning 1.0" refers to direct instruction used in early computer-based learning and training systems (CBL). In contrast to that linear delivery of content, often directly from the instructor's material, CSCL uses social software such as blogs, social media, wikis, podcasts, cloud-based document portals (such as Google Docs and Dropbox), and discussion groups and virtual worlds such as Second Life.[74] This phenomenon has been referred to as Long Tail Learning.[75] Advocates of social learning claim that one of the best ways to learn something is to teach it to others.[75] Social networks have been used to foster online learning communities around subjects as diverse as test preparation and language education.[76] mobile-assisted language learning (MALL) is the use of handheld computers or cell phones to assist in language learning.		Collaborative apps allow students and teachers to interact while studying. An example is MathChat, which allows cooperative problem solving and answer feedback.[77] Some apps can also provide an opportunity to revise or learn new topics independently in a simulated classroom environment. A popular example is Khan Academy,[78] which offers material in math, biology, chemistry, economics, art history and many others. It has the advantage of blending learning styles as the app offers many videos for visual and auditory learners, as well as exercises and tasks to solve for the kinesthetic learners. Other apps are designed after games, which provide a fun way to revise. When the experience is enjoyable the students become more engaged. Games also usually come with a sense of progression, which can help keep students motivated and consistent while trying to improve. Examples of educational games are Dragon Box, Mind Snacks, Code Spells and many more.[79]		Classroom 2.0 refers to online multi-user virtual environments (MUVEs) that connect schools across geographical frontiers. Known as "eTwinning", computer-supported collaborative learning (CSCL) allows learners in one school to communicate with learners in another that they would not get to know otherwise,[80][81] enhancing educational outcomes[citation needed] and cultural integration. Examples of classroom 2.0 applications are Blogger and Skype.[82]		Further, many researchers distinguish between collaborative and cooperative approaches to group learning. For example, Roschelle and Teasley (1995) argue that "cooperation is accomplished by the division of labour among participants, as an activity where each person is responsible for a portion of the problem solving", in contrast with collaboration that involves the "mutual engagement of participants in a coordinated effort to solve the problem together."[83]		This is an instructional strategy in which computer-assisted teaching is integrated with classroom instruction. Students are given basic essential instruction, such as lectures, before class instead of during class. This frees up classroom time for teachers to more actively engage with learners.[84]		Educational media and tools can be used for:		Numerous types of physical technology are currently used:[85][86] digital cameras, video cameras, interactive whiteboard tools, document cameras, electronic media, and LCD projectors. Combinations of these techniques include blogs, collaborative software, ePortfolios, and virtual classrooms.		Radio offers a synchronous educational vehicle, while streaming audio over the internet with webcasts and podcasts can be asynchronous. Classroom microphones, often wireless, can enable learners and educators to interact more clearly.		Video technology[87] has included VHS tapes and DVDs, as well as on-demand and synchronous methods with digital video via server or web-based options such as streamed video from YouTube, Teacher Tube, Skype, Adobe Connect, and webcams. Telecommuting can connect with speakers and other experts. Interactive digital video games are being used at K-12 and higher education institutions.[88]		Collaborative learning is a group-based learning approach in which learners are mutually engaged in a coordinated fashion to achieve a learning goal or complete a learning task. With recent developments in smartphone technology, the processing powers and storage capabilities of modern mobiles allow for advanced development and use of apps. Many app developers and education experts have been exploring smartphone and tablet apps as a medium for collaborative learning.		Computers and tablets enable learners and educators to access websites as well as programs such as Microsoft Word, PowerPoint, PDF files, and images. Many mobile devices support m-learning.		Mobile devices such as clickers and smartphones can be used for interactive audience response feedback.[89] Mobile learning can provide performance support for checking the time, setting reminders, retrieving worksheets, and instruction manuals.[90][91]		OpenCourseWare (OCW) gives free public access to information used in undergraduate and graduate programs. Participating institutions are MIT,[92][93] Harvard, Yale, Princeton, Stanford, University of Pennsylvania, and University of Michigan.[94]		Google Classroom allows instructors to create, administer, and grade assignments. While Google Classroom ultimately strives to create a paperless learning environment, there are many different types of learner; a learning environment like the one that Google Classroom projects does not work for everyone.[citation needed]		Group webpages, blogs, wikis, and Twitter allow learners and educators to post thoughts, ideas, and comments on a website in an interactive learning environment.[95][96][97] Social networking sites are virtual communities for people interested in a particular subject to communicate by voice, chat, instant message, video conference, or blogs.[98] The National School Boards Association found that 96% of students with online access have used social networking technologies, and more than 50% talk online about schoolwork. Social networking encourages collaboration and engagement[99] and can be a motivational tool for self-efficacy amongst students.[100] Every student has his or her own learning requirements, and a Web 2.0  educational  framework  provides  enough resources, learning styles, communication tools and flexibility to accommodate this diversity.[101]		Webcams and webcasting have enabled creation of virtual classrooms and virtual learning environment.[102] Webcams are also being used to counter plagiarism and other forms of academic dishonesty that might occur in an e-learning environment.		There are three types of whiteboards.[103] The initial whiteboards, analogous to blackboards, date from the late 1950s. The term whiteboard is also used metaphorically to refer to virtual whiteboards in which computer software applications simulate whiteboards by allowing writing or drawing. This is a common feature of groupware for virtual meeting, collaboration, and instant messaging. Interactive whiteboards allow learners and instructors to write on the touch screen. The screen markup can be on either a blank whiteboard or any computer screen content. Depending on permission settings, this visual learning can be interactive and participatory, including writing and manipulating images on the interactive whiteboard.[103]		Screencasting allows users to share their screens directly from their browser and make the video available online so that other viewers can stream the video directly.[104] The presenter thus has the ability to show their ideas and flow of thoughts rather than simply explain them as simple text content. In combination with audio and video, the educator can mimic the one-on-one experience of the classroom. Learners have an ability to pause and rewind, to review at their own pace, something a classroom cannot always offer.		A virtual learning environment (VLE), also known as a learning platform, simulates a virtual classroom or meetings by simultaneously mixing several communication technologies. For example, web conferencing software such as GoToTraining, WebEx Training or Adobe Connect enables students and instructors to communicate with each other via webcam, microphone, and real-time chatting in a group setting. Participants can raise hands, answer polls or take tests. Students are able to whiteboard and screencast when given rights by the instructor, who sets permission levels for text notes, microphone rights and mouse control.[105]		A virtual classroom provides the opportunity for students to receive direct instruction from a qualified teacher in an interactive environment. Learners can have direct and immediate access to their instructor for instant feedback and direction. The virtual classroom provides a structured schedule of classes, which can be helpful for students who may find the freedom of asynchronous learning to be overwhelming. In addition, the virtual classroom provides a social learning environment that replicates the traditional "brick and mortar" classroom. Most virtual classroom applications provide a recording feature. Each class is recorded and stored on a server, which allows for instant playback of any class over the course of the school year. This can be extremely useful for students to retrieve missed material or review concepts for an upcoming exam. Parents and auditors have the conceptual ability to monitor any classroom to ensure that they are satisfied with the education the learner is receiving.		In higher education especially, a virtual learning environment (VLE) is sometimes combined with a management information system (MIS) to create a managed learning environment, in which all aspects of a course are handled through a consistent user interface throughout the institution. Physical universities and newer online-only colleges offer select academic degrees and certificate programs via the Internet. Some programs require students to attend some campus classes or orientations, but many are delivered completely online. Several universities offer online student support services, such as online advising and registration, e-counseling, online textbook purchases, student governments and student newspapers.		Augmented reality (AR) provides students and teachers the opportunity to create layers of digital information, that includes both virtual world and real world elements, to interact with in real time. There are already a variety of apps which offer a lot of variations and possibilities.		Media psychology involves the application of theories in psychology to media and is a growing specialty in learning and educational technology.		E-learning authoring tools are software or online services that enable users to create courses, simulations, or other educational experiences. These tools typically support conventional, presentation-like courses, and may enable screen recording, multimedia, interactivity, quizzes, and non-linear or adaptive approaches.[106] An example of an e-learning authoring tools is Adobe Captivate.		Typing software allows users to practice their typing skills. Some of these programs include Typing Instructor, Typing Master, and Mavis Beacon Keyboarding Kidz.		A learning management system (LMS) is software used for delivering, tracking and managing training and education. For example, an LMS tracks attendance, time on task, and student progress. Educators can post announcements, grade assignments, check on course activity, and participate in class discussions. Students can submit their work, read and respond to discussion questions, and take quizzes.[95] An LMS may allow teachers, administrators, students, and permitted additional parties (such as parents if appropriate) to track various metrics. LMSs range from systems for managing training/educational records to software for distributing courses over the Internet and offering features for online collaboration. The creation and maintenance of comprehensive learning content requires substantial initial and ongoing investments of human labor. Effective translation into other languages and cultural contexts requires even more investment by knowledgeable personnel.[107]		Internet-based learning management systems include Canvas, Blackboard Inc. and Moodle. These types of LMS allow educators to run a learning system partially or fully online, asynchronously or synchronously. Blackboard can be used for K-12 education, Higher Education, Business, and Government collaboration.[108] Moodle is a free-to-download Open Source Course Management System that provides blended learning opportunities as well as platforms for distance learning courses.[109] Eliademy is a free cloud-based Course Management System that provides blended learning opportunities as well as platforms for distance learning courses.		A learning content management system (LCMS) is software for author content (courses, reusable content objects). An LCMS may be solely dedicated to producing and publishing content that is hosted on an LMS, or it can host the content itself. The Aviation Industry Computer-Based Training Committee (AICC) specification provides support for content that is hosted separately from the LMS.		A recent trend in LCMSs is to address this issue through crowdsourcing (cf.SlideWiki[110]).		Computer-aided assessment (e-assessment) ranges from automated multiple-choice tests to more sophisticated systems. With some systems, feedback can be geared towards a student's specific mistakes or the computer can navigate the student through a series of questions adapting to what the student appears to have learned or not learned. Formative assessment sifts out the incorrect answers, and these questions are then explained by the teacher. The learner then practices with slight variations of the sifted out questions. The process is completed by summative assessment using a new set of questions that only cover the topics previously taught.		An electronic performance support system (EPSS) is, according to Barry Raybould, "a computer-based system that improves worker productivity by providing on-the-job access to integrated information, advice, and learning experiences".[111] Gloria Gery defines it as "an integrated electronic environment that is available to and easily accessible by each employee and is structured to provide immediate, individualized on-line access to the full range of information, software, guidance, advice and assistance, data, images, tools, and assessment and monitoring systems to permit job performance with minimal support and intervention by others".[112][113]		A training management system or training resource management system is a software designed to optimize instructor-led training management. Similar to an enterprise resource planning (ERP), it is a back office tool which aims at streamlining every aspect of the training process: planning (training plan and budget forecasting), logistics (scheduling and resource management), financials (cost tracking, profitability), reporting, and sales for for-profit training providers.[114] For example, a training management system can be used to schedule instructors, venues and equipment through graphical agendas, optimize resource utilization, create a training plan and track remaining budgets, generate reports and share data between different teams.		While training management systems focus on managing instructor-led training, they can complete an LMS. In this situation, an LMS will manage e-learning delivery and assessment, while a training management system will manage ILT and back-office budget planning, logistic and reporting.[115]		Content and design architecture issues include pedagogy and learning object re-use. One approach looks at five aspects:[116]		Pedagogical elements are defined as structures or units of educational material. They are the educational content that is to be delivered. These units are independent of format, meaning that although the unit may be delivered in various ways, the pedagogical structures themselves are not the textbook, web page, video conference, Podcast, lesson, assignment, multiple choice question, quiz, discussion group or a case study, all of which are possible methods of delivery.		Much effort has been put into the technical reuse of electronically based teaching materials and in particular creating or re-using learning objects. These are self-contained units that are properly tagged with keywords, or other metadata, and often stored in an XML file format. Creating a course requires putting together a sequence of learning objects. There are both proprietary and open, non-commercial and commercial, peer-reviewed repositories of learning objects such as the Merlot repository. Sharable Content Object Reference Model (SCORM) is a collection of standards and specifications that applies to certain web-based e-learning. Other specifications such as Schools Framework[citation needed] allow for the transporting of learning objects, or for categorizing metadata (LOM).		Various forms of electronic media are a feature of preschool life.[117] Although parents report a positive experience, the impact of such use has not been systematically assessed.[117]		The age when a given child might start using a particular technology such as a cellphone or computer might depend on matching a technological resource to the recipient's developmental capabilities, such as the age-anticipated stages labeled by Swiss psychologist, Jean Piaget.[118] Parameters, such as age-appropriateness, coherence with sought-after values, and concurrent entertainment and educational aspects, have been suggested for choosing media.[119]		E-learning is utilized by public K–12 schools in the United States as well as private schools. Some e-learning environments take place in a traditional classroom, others allow students to attend classes from home or other locations. There are several states that are utilizing virtual school platforms for e-learning across the country that continue to increase. For example: Digital technology is becoming increasingly commonplace in K-12 education, and many researchers argue that it will save money and transform schools into more effective institutions.[120] With technology having such a heavy influence in our society it is no doubt that schools are looking into technology to teach students. Meanwhile, despite the debate over the effectiveness of computerized education, all-online K-12 schools are proliferating nationwide, and enrollment in online courses is soaring.[120] Technology can make anyone's life much easier and it is proven that it can help students in an effective way. But in the digitized world of 21st-century education, computers are increasingly taking on the teachers' role.[120] Computers can now "hear" students speak, for example, correct their pronunciation and evaluate their progress over time, says Michael L. Kamil, a professor emeritus at the Stanford University School of Education.[120] Many experts claim that digital education is the future. "If we want our kids to be prepared for life after school in the 21st century, we need to consider technology a basic element of public education," said New York's Deputy Chancellor of EducationJohn White.[120] And many schools are making it a requirement to take an online class. Digital learning has been getting a boost in localities across the nation this year.[120] For example, Idaho became the first state to require high-school students to complete two or more online courses to receive a diploma.[120] It is no doubt that digital education is increasing in our educational system.Virtual school enables students to log into synchronous learning or asynchronous learning courses anywhere there is an internet connection.		E-learning is increasingly being utilized by students who may not want to go to traditional brick and mortar schools due to severe allergies or other medical issues, fear of school violence and school bullying and students whose parents would like to homeschool but do not feel qualified.[121] Online schools create a haven for students to receive a quality education while almost completely avoiding these common problems. Online charter schools also often are not limited by location, income level or class size in the way brick and mortar charter schools are.[122]		E-learning also has been rising as a supplement to the traditional classroom. Students with special talents or interests outside of the available curricula use e-learning to advance their skills or exceed grade restrictions.[123] Some online institutions connect students with instructors via web conference technology to form a digital classroom.		National private schools are also available online. These provide the benefits of e-learning to students in states where charter online schools are not available. They also may allow students greater flexibility and exemption from state testing.		Virtual education in K-12 schooling often refers to virtual schools, and in higher education to virtual universities. Virtual schools are "cybercharter schools"[124] with innovative administrative models and course delivery technology.[124]		Online college course enrollment has seen a 29% increase in enrollment with nearly one third of all college students, or an estimated 6.7 million students are currently enrolled in online classes.[125][126] In 2009, 44 percent of post-secondary students in the USA were taking some or all of their courses online, which was projected to rise to 81 percent by 2014.[127]		Although a large proportion of for-profit higher education institutions now offer online classes, only about half of private, non-profit schools do so. Private institutions may become more involved with on-line presentations as the costs decrease. Properly trained staff must also be hired to work with students online.[128] These staff members need to understand the content area, and also be highly trained in the use of the computer and Internet. Online education is rapidly increasing, and online doctoral programs have even developed at leading research universities.[129]		Although massive open online courses (MOOCs) may have limitations that preclude them from fully replacing college education,[130] such programs have significantly expanded. MIT, Stanford and Princeton University offer classes to a global audience, but not for college credit.[131] University-level programs, like edX founded by Massachusetts Institute of Technology and Harvard University, offer wide range of disciplines at no charge, while others permit students to audit a course at no charge but require a small fee for accreditation. MOOCs have not had a significant impact on higher education and declined after the initial expansion, but are expected to remain in some form.[132]		Private organizations also offer classes, such as Udacity, with free computer science classes, and Khan Academy, with over 3,900 free micro-lectures available via YouTube. Distributed open collaborative course (DOCC) sees itself as a counter-movement to MOOC, emphasizing decentralized teaching.[133] University of the People is a non-profit accredited online university. Coursera offers online courses. According to Fortune magazine, over a million people worldwide have enrolled in free online courses.[134]		Companies with spread out distribution chains use e-learning for staff training and development and to bring customers information about the latest product developments. Continuing professional development (CPD) can deliver regulatory compliance updates and staff development of valuable workplace skills. For effectiveness and competitive learning performance, scoring systems are designed to give live feedback on decision-making in complex (mobile) learning scenarios.[135]		There is an important need for recent, reliable, and high-quality health information to be made available to the public as well as in summarized form for public health providers.[136] Providers have indicated the need for automatic notification of the latest research, a single searchable portal of information, and access to grey literature.[137] The Maternal and Child Health (MCH) Library is funded by the U.S. Maternal and Child Health Bureau to screen the latest research and develop automatic notifications to providers through the MCH Alert. Another application in public health is the development of mHealth (use of mobile telecommunication and multimedia into global public health). MHealth has been used to promote prenatal and newborn services, with positive outcomes. In addition, "Health systems have implemented mHealth programs to facilitate emergency medical responses, point-of-care support, health promotion and data collection."[138] In low and middle income countries, mHealth is most frequently used as one-way text messages or phone reminders to promote treatment adherence and gather data.[139]		There has also been a growing interest in e-learning as a beneficial educational method for students with attention deficit hyperactivity disorder (ADHD). With the growing popularity in e-learning among K-12 and higher education, the opportunity to take online classes is becoming increasingly important for students of all ages.[140] However, students with ADHD and special needs face different learning demands compared to the typical developing learner. This is especially significant considering the dramatic rise in ADHD diagnoses in the last decade among both children and adults.[141] Compared to the traditional face-to-face classroom, e-learning and virtual classrooms require a higher level of executive functions, which is the primary deficit associated with ADHD.[142] Although ADHD is not specifically named in the Rehabilitation Act of 1973, students with ADHD who have symptoms that interfere with their learning or ability may be eligible for assistive technology. Some examples of the resources that may help interest students and adults with ADHD consist of, computer software, brain games, timers, calendars, voice recognition devices, screen magnifiers, and talking books.[143]		Wolf lists 12 executive function skills necessary for students to succeed in postsecondary education: plan, set goals, organize, initiate, sustain attention/effort, flexibility, monitor, use feedback, structure, manage time, manage materials, and follow through.[144] These skills, along with strong independent and self-regulated learning, are especially pronounced in the online environment and as many ADHD students suffer from a deficit in one or more of these executive functions, this presents a significant challenge and accessibility barrier to the current e-learning approach.[145][146]		Some have noted that current e-learning models are moving towards applying a constructivism learning theory[147] that emphasizes a learner-centered environment[148] and postulates that everyone has the ability to construct their own knowledge and meaning through a process of problem solving and discovery.[149] However, some principles of constructivism may not be appropriate for ADHD learners; these principles include active learning, self-monitoring, motivation, and strong focus.[147]		Despite the limitations, students with special needs, including ADHD, have expressed an overall enthusiasm for e-learning and have identified a number e-learning benefits, including: availability of online course notes, materials and additional resources; the ability to work at an independent pace and spend extra time formulating thoughtful responses in class discussions; help in understanding course lecture/content; ability to review lectures multiple times; and enhanced access to and communication with the course instructor.[145][150]		The design of e-learning platforms in ways that enable universal access has received attention from several directions, including the World Wide Web Consortium's Web Accessibility Initiative (WAI). WAI provides universal formatting standards for websites so they can remain accessible to people with disabilities. For example, developing or adopting e-learning material can enable accessibility for people with visual impairment.[151][152] The Perkins School for the Blind offers learning resources tailored for the visually impaired, including webcasts, webinars, downloadable science activities, and an online library that has access to over 40,000 resource materials on blindness and deaf blindness.[153]		Online education may appear to be a promising alternative for students with physical and sensory disabilities because they get to work at their own pace and in their own home. However, not all online programs are equal when it comes to their resources for students with disabilities. Students with disabilities who wish to enroll in online education must either be able to advocate for themselves and their own rights or have a person who is willing to advocate for them. The American with Disabilities Act states that online programs must provide appropriate accommodations for students with disabilities, but has not specifically defined what that means. "Once students with disabilities are accepted into an online program, they should prepare to be direct and open about what they need to succeed, experts say" (Haynie).[154]		Educational technology, particularly in online learning environments, can allow students to use real identity, pseudonym, or anonymous identity during classroom communication. Advantages in anonymizing race, age, and gender are increased student participation[155] and increased cross-cultural communication.[156] Risks include increased cyberbullying, and aggressive or hostile language.[156][157][158]		Effective technology use deploys multiple evidence-based strategies concurrently (e.g. adaptive content, frequent testing, immediate feedback, etc.), as do effective teachers.[159] Using computers or other forms of technology can give students practice on core content and skills while the teacher can work with others, conduct assessments, or perform other tasks.[159][160] Through the use of educational technology, education is able to be individualized for each student allowing for better differentiation and allowing students to work for mastery at their own pace.[161]		Modern educational technology can improve access to education, including full degree programs.[162] It enables better integration for non-full-time students, particularly in continuing education,[162] and improved interactions between students and instructors.[163] Learning material can be used for long distance learning and are accessible to a wider audience.[164] Course materials are easy to access.[165] In 2010, 70.3% of American family households had access to the internet.[166] In 2013, according to Canadian Radio Television and Telecommunications Commission Canada, 79% of homes have access to the internet.[167] Students can access and engage with numerous online resources at home. Using online resources such as Khan Academy or TED Talks can help students spend more time on specific aspects of what they may be learning in school, but at home. Schools like MIT have made certain course materials free online.[168] Although some aspects of a classroom setting are missed by using these resources, they are helpful tools to add additional support to the educational system. The necessity to pay for transport to the educational facility is removed.		Students appreciate the convenience of e-learning, but report greater engagement in face-to-face learning environments.[169]		According to James Kulik, who studies the effectiveness of computers used for instruction, students usually learn more in less time when receiving computer-based instruction and they like classes more and develop more positive attitudes toward computers in computer-based classes.[170] Students can independently solve problems.[163] There are no intrinsic age-based restrictions on difficulty level, i.e. students can go at their own pace. Students editing their written work on word processors improve the quality of their writing. According to some studies, the students are better at critiquing and editing written work that is exchanged over a computer network with students they know.[165] Studies completed in "computer intensive" settings found increases in student-centric, cooperative and higher order learning, writing skills, problem solving, and using technology.[171] In addition, attitudes toward technology as a learning tool by parents, students and teachers are also improved.		Employers' acceptance of online education has risen over time.[172] More than 50% of human resource managers SHRM surveyed for an August 2010 report said that if two candidates with the same level of experience were applying for a job, it would not have any kind of effect whether the candidate's obtained degree was acquired through an online or a traditional school. Seventy-nine percent said they had employed a candidate with an online degree in the past 12 months. However 66% said candidates who get degrees online were not seen as positively as a job applicant with traditional degrees.[172]		The use of educational apps generally has positive effect on learning. Pre- and post- tests reveal that the use of apps on mobile devices reduces the achievement gap between struggling and average students.[173] Some educational apps improve group work by allowing students to receive feedback on answers and promoting collaboration in solving problems, examples of these apps can be found in the third paragraph. The benefits of app-assisted learning have been exhibited in all age groups. Kindergarten students that use iPads show much higher rates of literacy than non-users. Medical students at University of California Irvine that utilized iPad academically have been reported to score 23% higher on national exams than previous classes that did not. Mobile devices and apps have also been shown to assist in the education of disabled students, with one study reporting increased engagement and accelerated comprehension and learning.[174]		Recent e-learning studies modulated that the use of e-learning systems in university contexts increases productivity and allows learners to accomplish their tasks in more effective ways Aparicio, Bacao & Oliveira[175] model explains more than 66% of the variation of the perceived individual performance and 60% of the perceived organizational performance, due to the e-learning systems use, learners' satisfaction, and to individualism/collectivism. Latest studies on e-learning success determinants demonstrate that students with high grit levels (stamina, perseverance and consistency of interest) achieve better results in learning.[176]		Some experts believe that students have grown up with technology and thus can use it creatively if properly facilitated by teachers.[177] Many educators argue that students will abuse technology if allowed to use it in class; however, banning it may lead to more abuse as Johnson states in his article "Taming the Chaos" about handling technology in the classroom. He explains that allowing its use will help those students that want to learn, while those who don't want to learn will continue to abuse its use – this is a different issue about motivation.[178]		Many US states spend large sums of money on technology. However, as of 2013[update], none were looking at technology return on investment (ROI) to connect expenditures on technology with improved student outcomes.[179]		New technologies are frequently accompanied by unrealistic hype and promise regarding their transformative power to change education for the better or in allowing better educational opportunities to reach the masses. Examples include silent film, broadcast radio, and television, none of which have maintained much of a foothold in the daily practices of mainstream, formal education.[180] Technology, in and of itself, does not necessarily result in fundamental improvements to educational practice.[181] The focus needs to be on the learner's interaction with technology—not the technology itself. It needs to be recognized as "ecological" rather than "additive" or "subtractive". In this ecological change, one significant change will create total change.[182]		According to Branford et al., "technology does not guarantee effective learning" and inappropriate use of technology can even hinder it.[183] A University of Washington study of infant vocabulary shows that it is slipping due to educational baby DVDs. Published in the Journal of Pediatrics, a 2007 University of Washington study on the vocabulary of babies surveyed over 1,000 parents in Washington and Minnesota. The study found that for every one hour that babies 8–16 months of age watched DVDs and Videos they knew 6-8 fewer of 90 common baby words than the babies that did not watch them. Andrew Meltzoff, a surveyor in this study states that the result makes sense, that if the baby's 'alert time' is spent in front of DVDs and TV, instead of with people speaking, the babies are not going to get the same linguistic experience. Dr. Dimitri Chistakis, another surveyor reported that the evidence is mounting that baby DVDs are of no value and may be harmful.[184][185][186][187]		Adaptive instructional materials tailor questions to each student's ability and calculate their scores, but this encourages students to work individually rather than socially or collaboratively (Kruse, 2013). Social relationships are important but high-tech environments may compromise the balance of trust, care and respect between teacher and student.[188]		Massively open online courses (MOOCs), although quite popular in discussions of technology and education in developed countries (more so in US), are not a major concern in most developing or low-income countries. One of the stated goals of MOOCs is to provide less fortunate populations (i.e., in developing countries) an opportunity to experience courses with US-style content and structure. However, research shows only 3% of the registrants are from low-income countries and although many courses have thousands of registered students only 5-10% of them complete the course.[189] MOOCs also implies that certain curriculum and teaching methods are superior and this could eventually wash over (or possibly washing out) local educational institutions, cultural norms and educational traditions.[190]		With the Internet and social media, using educational apps makes the students highly susceptible to distraction and sidetracking. Even though proper use has shown to increase student performances, being distracted would be detrimental. Another disadvantage is increased potential for cheating. Smartphones can be very easy to hide and use inconspicuously, especially if their use is normalized in the classroom. These disadvantages can be managed with strict rules and regulations on mobile phone use.		Electronic devices such as cellphones and computers facilitate rapid access to a stream of sources, each of which may receive cursory attention. Michel Rich, an associate professor at Harvard Medical School and executive director of the center on Media and Child Health in Boston, said of the digital generation, "Their brains are rewarded not for staying on task, but for jumping to the next thing. The worry is we're raising a generation of kids in front of screens whose brains are going to be wired differently."[191] Students have always faced distractions; computers and cellphones are a particular challenge because the stream of data can interfere with focusing and learning. Although these technologies affect adults too, young people may be more influenced by it as their developing brains can easily become habituated to switching tasks and become unaccustomed to sustaining attention.[191] Too much information, coming too rapidly, can overwhelm thinking.[192]		Technology is "rapidly and profoundly altering our brains."[193] High exposure levels stimulate brain cell alteration and release neurotransmitters, which causes the strengthening of some neural pathways and weakening of others. This leads to heightened stress levels on the brain that, at first, boost energy levels, but, over time, actually augment memory, impair cognition, lead to depression, alter the neural circuitry of the hippocampus, amygdala and prefrontal cortex. These are the brain regions that control mood and thought. If unchecked, the underlying structure of the brain could be altered.[191][193] Over-stimulation due to technology may begin too young. When children are exposed before the age of seven, important developmental tasks may be delayed, and bad learning habits might develop, which "deprives children of the exploration and play that they need to develop."[194] Media psychology is an emerging specialty field that embraces electronic devices and the sensory behaviors occurring from the use of educational technology in learning.		According to Lai, "the learning environment is a complex system where the interplay and interactions of many things impact the outcome of learning."[195] When technology is brought into an educational setting, the pedagogical setting changes in that technology-driven teaching can change the entire meaning of an activity without adequate research validation. If technology monopolizes an activity, students can begin to develop the sense that "life would scarcely be thinkable without technology."[196]		Leo Marx considered the word "technology" itself as problematic,[197] susceptible to reification and "phantom objectivity", which conceals its fundamental nature as something that is only valuable insofar as it benefits the human condition. Technology ultimately comes down to affecting the relations between people, but this notion is obfuscated when technology is treated as an abstract notion devoid of good and evil. Langdon Winner makes a similar point by arguing that the underdevelopment of the philosophy of technology leaves us with an overly simplistic reduction in our discourse to the supposedly dichotomous notions of the "making" versus the "uses" of new technologies, and that a narrow focus on "use" leads us to believe that all technologies are neutral in moral standing.[196]:ix–39 These critiques would have us ask not, "How do we maximize the role or advancement of technology in education?", but, rather, "What are the social and human consequences of adopting any particular technology?"		Winner viewed technology as a "form of life" that not only aids human activity, but that also represents a powerful force in reshaping that activity and its meaning.[196]:ix–39 For example, the use of robots in the industrial workplace may increase productivity, but they also radically change the process of production itself, thereby redefining what is meant by "work" in such a setting. In education, standardized testing has arguably redefined the notions of learning and assessment. We rarely explicitly reflect on how strange a notion it is that a number between, say, 0 and 100 could accurately reflect a person's knowledge about the world. According to Winner, the recurring patterns in everyday life tend to become an unconscious process that we learn to take for granted. Winner writes,		By far the greatest latitude of choice exists the very first time a particular instrument, system, or technique is introduced. Because choices tend to become strongly fixed in material equipment, economic investment, and social habit, the original flexibility vanishes for all practical purposes once the initial commitments are made. In that sense technological innovations are similar to legislative acts or political foundings that establish a framework for public order that will endure over many generations. (p. 29)		When adopting new technologies, there may be one best chance to "get it right." Seymour Papert (p. 32) points out a good example of a (bad) choice that has become strongly fixed in social habit and material equipment: our "choice" to use the QWERTY keyboard.[198] The QWERTY arrangement of letters on the keyboard was originally chosen, not because it was the most efficient for typing, but because early typewriters were prone to jam when adjacent keys were struck in quick succession. Now that typing has become a digital process, this is no longer an issue, but the QWERTY arrangement lives on as a social habit, one that is very difficult to change.		Neil Postman endorsed the notion that technology impacts human cultures, including the culture of classrooms, and that this is a consideration even more important than considering the efficiency of a new technology as a tool for teaching.[182] Regarding the computer's impact on education, Postman writes (p. 19):		What we need to consider about the computer has nothing to do with its efficiency as a teaching tool. We need to know in what ways it is altering our conception of learning, and how in conjunction with television, it undermines the old idea of school.		There is an assumption that technology is inherently interesting so it must be helpful in education; based on research by Daniel Willingham, that is not always the case. He argues that it does not necessarily matter what the technological medium is, but whether or not the content is engaging and utilizes the medium in a beneficial way.[199]		The concept of the digital divide is a gap between those who have access to digital technologies and those who do not.[200] Access may be associated with age, gender, socio-economic status, education, income, ethnicity, and geography.[200][201]		According to a report by the Electronic Frontier Foundation, large amounts of personal data on children is collected by electronic devices that are distributed in schools in the United States. Often far more information than necessary is collected, uploaded and stored indefinitely. Aside name and date of birth, this information can include the child's browsing history, search terms, location data, contact lists, as well as behavioral information.[202]:5 Parents are not informed or, if informed, have little choice.[202]:6 According to the report, this constant surveillance resulting from educational technology can "warp children's privacy expectations, lead them to self-censor, and limit their creativity".[202]:7		Since technology is not the end goal of education, but rather a means by which it can be accomplished, educators must have a good grasp of the technology and its advantages and disadvantages. Teacher training aims for effective integration of classroom technology.[203]		The evolving nature of technology may unsettle teachers, who may experience themselves as perpetual novices.[204] Finding quality materials to support classroom objectives is often difficult. Random professional development days are inadequate.[204]		According to Jenkins, "Rather than dealing with each technology in isolation, we would do better to take an ecological approach, thinking about the interrelationship among different communication technologies, the cultural communities that grow up around them, and the activities they support."[201] Jenkins also suggested that the traditional school curriculum guided teachers to train students to be autonomous problem solvers.[201] However, today's workers are increasingly asked to work in teams, drawing on different sets of expertise, and collaborating to solve problem.[201] Learning styles and the methods of collecting information have evolved, and "students often feel locked out of the worlds described in their textbooks through the depersonalized and abstract prose used to describe them".[201] These twenty-first century skills can be attained through the incorporation and engagement with technology.[205] Changes in instruction and use of technology can also promote a higher level of learning among students with different types of intelligence.[206]		There are two distinct issues of assessment: the assessment of educational technology[201][207] and assessment with technology.[208]		Assessments of educational technology have included the Follow Through project.		Educational assessment with technology may be either formative assessment or summative assessment. Instructors use both types of assessment to understand student progress and learning in the classroom. Technology has helped teachers create better assessments to help understand where students who are having trouble with the material are having issues.		Formative assessment is more difficult, as the perfect form is ongoing and allows the students to show their learning in different ways depending on their learning styles. Technology has helped some teachers make their formative assessments better, particularly through the use of classroom response systems (CRS).[209] A CRS is a tool in which the students each have a handheld device that partners up with the teacher's computer. The instructor then asks multiple choice or true or false questions and the students answer on their device.[209] Depending on the software used, the answers may then be shown on a graph so students and teacher can see the percentage of students who gave each answer and the teacher can focus on what went wrong.[210] Some examples of CRSs are Quizzler, Turning Systems, and the quiz aspect of the Mastering Programs (for example Mastering Physics or Mastering Chemistry).		Summative assessments are more common in classrooms and are usually set up to be more easily graded, as they take the form of tests or projects with specific grading schemes. One huge benefit to tech-based testing is the option to give students immediate feedback on their answers. When students get these responses, they are able to know how they are doing in the class which can help push them to improve or give them confidence that they are doing well.[211] Technology also allows for different kinds of summative assessment, such as digital presentations, videos, or anything else the teacher/students may come up with, which allows different learners to show what they learned more effectively.[211] Teachers can also use technology to post graded assessments online for students to have a better idea of what a good project is.		Electronic assessment uses information technology. It encompasses several potential applications, which may be teacher or student oriented, including educational assessment throughout the continuum of learning, such as computerized classification testing, computerized adaptive testing, student testing, and grading an exam. E-Marking is an examiner led activity closely related to other e-assessment activities such as e-testing, or e-learning which are student led. E-marking allows markers to mark a scanned script or online response on a computer screen rather than on paper.		There are no restrictions to the types of tests that can use e-marking, with e-marking applications designed to accommodate multiple choice, written, and even video submissions for performance examinations. E-marking software is used by individual educational institutions and can also be rolled out to the participating schools of awarding exam organisations. e-marking has been used to mark many well known high stakes examinations, which in the United Kingdom include A levels and GCSE exams, and in the US includes the SAT test for college admissions. Ofqual reports that e-marking is the main type of marking used for general qualifications in the United Kingdom.		In 2007, the International Baccalaureate implemented e-marking. In 2012, 66% of nearly 16 million exam scripts were "e-marked" in the United Kingdom.[212] Ofqual reports that in 2015, all key stage 2 tests in the United Kingdom will be marked onscreen.		In 2014, the Scottish Qualifications Authority (SQA) announced that most of the National 5 question papers would be e-marked.[213]		In June 2015, the Odisha state government in India announced that it planned to use e-marking for all Plus II papers from 2016.[214]		The importance of self-assessment through tools made available on Educational Technology platforms has been growing. Self-assessment in education technology relies on students analyzing their strengths, weaknesses and areas where improvement is possible to set realistic goals in learning, improve their educational performances and track their progress.[215][216] One of the unique tools for self-assessment made possible by education technology is Analytics. Analytics is data gathered on the student's activities on the learning platform, drawn into meaningful patterns that leads to a valid conclusion, usually through the medium of data visualization such as graphs.		The five key sectors of the e-learning industry are consulting, content, technologies, services and support.[217] Worldwide, e-learning was estimated in 2000 to be over $48 billion according to conservative estimates.[218] Commercial growth has been brisk.[219][220] In 2014, the worldwide commercial market activity was estimated at $6 billion venture capital over the past five years,[219]:38 with self-paced learning generating $35.6 billion in 2011.[219]:4 North American e-learning generated $23.3 billion in revenue in 2013, with a 9% growth rate in cloud-based authoring tools and learning platforms.[219]:19		Educational technologists and psychologists apply basic educational and psychological research into an evidence-based applied science (or a technology) of learning or instruction. In research, these professions typically require a graduate degree (Master's, Doctorate, Ph.D., or D.Phil.) in a field related to educational psychology, educational media, experimental psychology, cognitive psychology or, more purely, in the fields of educational, instructional or human performance technology or instructional design. In industry, educational technology is utilized to train students and employees by a wide range of learning and communication practitioners, including instructional designers, technical trainers, technical communication and professional communication specialists, technical writers, and of course primary school and college teachers of all levels. The transformation of educational technology from a cottage industry to a profession is discussed by Shurville et al.[221]		
In criminology, corporate crime refers to crimes committed either by a corporation (i.e., a business entity having a separate legal personality from the natural persons that manage its activities), or by individuals acting on behalf of a corporation or other business entity (see vicarious liability and corporate liability). Some negative behaviours by corporations may not actually be criminal; laws vary between jurisdictions. For example, some jurisdictions allow insider trading.		Corporate crime overlaps with:						An 1886 decision of the United States Supreme Court, in Santa Clara County v. Southern Pacific Railroad 118 U.S. 394 (1886), has been cited by various courts in the US as precedent to maintain that a corporation can be defined legally as a "person", as described in the Fourteenth Amendment to the U.S. Constitution. The Fourteenth Amendment stipulates that,		No State shall make or enforce any law which shall abridge the privileges or immunities of citizens of the United States; nor shall any State deprive any person of life, liberty, or property, without due process of law; nor deny to any person within its jurisdiction the equal protection of the laws.		In English law, this was matched by the decision in Salomon v Salomon & Co [1897] AC 22. In Australian law, under the Corporations Act 2001 (Cth), a corporation is legally a "person".		Corporate crime has become politically sensitive in some countries. In the United Kingdom, for example, following wider publicity of fatal accidents on the rail network and at sea, the term is commonly used in reference to corporate manslaughter and to involve a more general discussion about the technological hazards posed by business enterprises (see Wells: 2001).		In the United States, the Sarbanes-Oxley Act of 2002 was passed to reform business practices, including enhanced corporate responsibility, financial disclosures, and combat fraud,[1] following the highly publicized scandals of Enron, Worldcom, Freddie Mac, Lehman Brothers, and Bernie Madoff. Company chief executive officer (CEO) and company chief financial officer (CFO) are required to personally certify financial reports to be accurate and compliant with applicable laws, with criminal penalties for willful misconduct including monetary fines up to $5,000,000 and prison sentence up to 20 years.[2]		The Law Reform Commission of New South Wales offers an explanation of such criminal activities:		Corporate crime poses a significant threat to the welfare of the community. Given the pervasive presence of corporations in a wide range of activities in our society, and the impact of their actions on a much wider group of people than are affected by individual action, the potential for both economic and physical harm caused by a corporation is great (Law Reform Commission of New South Wales: 2001).		Similarly, Russell Mokhiber and Robert Weissman (1999) assert:		At one level, corporations develop new technologies and economies of scale. These may serve the economic interests of mass consumers by introducing new products and more efficient methods of mass production. On another level, given the absence of political control today, corporations serve to destroy the foundations of the civic community and the lives of people who reside in them.		Behavior can be regulated by the civil law (including administrative law) or the criminal law. In deciding to criminalize particular behavior, the legislature is making the political judgment that this behavior is sufficiently culpable to deserve the stigma of being labelled as a crime. In law, corporations can commit the same offences as natural persons. Simpson (2002) avers that this process should be straightforward because a state should simply engage in victimology to identify which behavior causes the most loss and damage to its citizens, and then represent the majority view that justice requires the intervention of the criminal law. But states depend on the business sector to deliver a functioning economy, so the politics of regulating the individuals and corporations which supply that stability become more complex. For the views of Marxist criminology, see Snider (1993) and Snider & Pearce (1995), for Left realism, see Pearce & Tombs (1992) and Schulte-Bockholt (2001), and for Right Realism, see Reed & Yeager (1996). More specifically, the historical tradition of sovereign state control of prisons is ending through the process of privatisation. Corporate profitability in these areas therefore depends on building more prison facilities, managing their operations, and selling inmate labor. In turn, this requires a steady stream of prisoners able to work. (Kicenski: 2002)		Bribery and corruption are problems in the developed world, and the corruption of public officials is thought to be a serious problem in developing countries, and an obstacle to development.		Edwin Sutherland's definition of white collar crime also is related to notions of corporate crime. In his landmark definition of white collar crime he offered these categories of crime:		One paper discusses some of the issues that arise in the relationship between private sector and corruption. The findings can be summarized as follows:				
A recession-proof job is a job that one is likely to be able to find even during hard economic times. Though these jobs are not truly "recession-proof," they have a continual demand for workers, thereby increasing the chances that one who has the skills will be likely to find employment.[1][2]		What makes a job so-called recession-proof is society's perpetual need and heavy demand for the service related jobs. Certain fields, such as health care, education, law enforcement, and various computer-related occupations are thereby always in demand. But as to which specific jobs are the most recession-proof, this varies in different eras, as the times change, and each recession differs.[3] Also, the geographic locality may make a difference.		When a recession occurs, many people, especially those who have lost their jobs, those whose jobs have been threatened, or those who fear losing their jobs are motivated to seek education to be able to obtain recession-proof employment in their future.[4]		
A career is an individual's journey through learning, work and other aspects of life.		Career or Careers may also refer to:		
A work-at-home scheme is a get-rich-quick scam in which a victim is lured by an offer to be employed at home, very often doing some simple task in a minimal amount of time with a large amount of income that far exceeds the market rate for the type of work. The true purpose of such an offer is for the perpetrator to extort money from the victim, either by charging a fee to join the scheme, or requiring the victim to invest in products whose resale value is misrepresented.[1]		Work-at-home schemes have been around for decades, with the classic "envelope stuffing" scam originating in the United States during the Depression in the 1920s and 1930s.[2] In this scam, the worker is offered entry to a scheme where they can earn $2 for every envelope they fill. After paying a small $2 fee to join the scheme, the victim is sent a flyer template for the self-same work-from-home scheme, and instructed to post these advertisements around their local area – the victim is simply "stuffing envelopes" with flyer templates that perpetuate the scheme.[2] Originally found as printed adverts in newspapers and magazines, variants of this scam have expanded into more modern media, such as television and radio adverts, and forum posts on the Internet.		In some countries, law enforcement agencies work to fight work-at-home schemes. In 2006, the United States Federal Trade Commission established Project False Hopes, a federal and state law enforcement sweep that targets bogus business opportunity and work at home scams. The crackdown involved more than 100 law enforcement actions by the FTC, the Department of Justice, the United States Postal Inspection Service, and law enforcement agencies in eleven states.[1]		Legitimate work-at-home opportunities do exist, and many people do their jobs in the comfort of their own homes, but anyone seeking such an employment opportunity should be wary of accepting a home employment offer. A 2007 report in the United States suggested that about 97% of work-at-home offers were scams.[3] Many legitimate jobs at home require some form of post-high-school education, such as a college degree or certificate, or trade school, and some experience in the field in an office or other supervised setting. Additionally, many legitimate at-home jobs are not like those in schemes are portrayed to be, as they are often performed at least some of the time in the company's office, require more self discipline than a traditional job, and have a higher risk of firing.						Common types of work found in work-at-home schemes include:		Some adverts offer legitimate forms of work that really do exist, but exaggerate the salary and understate the effort that will have to be put into the job, or exaggerate the amount of work that will be available. Many such ads do not even specify the type of work that will be performed. Some similar schemes do not advertise work that would be performed at home, but may instead offer occasional, sporadic work away from home for large payments, paired with a lot of free time. Some common offers fitting this description are acting as extras, mystery shopping (which in reality requires hard work, is paid close to minimum wage, and most importantly, does not require an up-front fee to join) and working as a nanny.[4] [5] [6]		Signs of a work-at-home scam versus a legitimate job may include:		The consequences of falling for a work-at-home scheme may be as follows:[8]		
Onboarding, also known as organizational socialization, refers to the mechanism through which new employees acquire the necessary knowledge, skills, and behaviors to become effective organizational members and insiders.[1] Tactics used in this process include formal meetings, lectures, videos, printed materials, or computer-based orientations to introduce newcomers to their new jobs and organizations. Research has demonstrated that these socialization techniques lead to positive outcomes for new employees such as higher job satisfaction, better job performance, greater organizational commitment, and reduction in occupational stress and intent to quit.[2][3][4] These outcomes are particularly important to an organization looking to retain a competitive advantage in an increasingly mobile and globalized workforce. In the United States, for example, up to 25% of workers are organizational newcomers engaged in an onboarding process.[5] The term Induction is used instead in regions such as Australia, New Zealand, Canada, and parts of Europe. [6] This is known in some parts of the world as training.[7]				Onboarding is a multifaceted operation influenced by a number of factors pertaining to both the individual newcomer and the organization. Researchers have separated these factors into three broad categories: new employee characteristics, new employee behaviors, and organizational efforts.[8] New employee characteristics are individual differences across incoming workers, ranging from personality traits to previous work experiences. New employee behaviors refer to the specific actions carried out by newcomers as they take an active role in the socialization process. Finally, organizational efforts help facilitate the process of acclimating a new worker to an establishment through activities such as orientation or mentoring programs.		Research has shown evidence that employees with certain personality traits and experiences adjust to an organization more quickly.[9] These are a proactive personality, the "Big Five", curiosity, and greater experience levels.		"Proactive personality" refers to the tendency to take charge of situations and achieve control over one's environment. This type of personality predisposes some workers to engage in behaviors such as information seeking that accelerate the socialization process, thus helping them to adapt more efficiently and become high-functioning organizational members.[1] Empirical evidence also demonstrates that a proactive personality is related to increased levels of job satisfaction and performance.[10][11]		The Big Five personality traits—openness, conscientiousness, extraversion, agreeableness, and neuroticism—have been linked to onboarding success, as well. Specifically, new employees who are proactive or particularly open to experience are more likely to seek out information, feedback, acceptance, and relationships with co-workers. They also exhibit higher levels of adjustment and tend to frame events more positively.[3]		Curiosity also plays a substantial role in the newcomer adaptation process and is defined as the "desire to acquire knowledge" that energizes individual exploration of an organization's culture and norms.[12] Individuals with a curious disposition tend to frame challenges in a positive light and eagerly seek out information to help them make sense of their new organizational surroundings and responsibilities, leading to a smoother onboarding experience.[13]		Employee experience levels also affect the onboarding process such that more experienced members of the workforce tend to adapt to a new organization differently from, for example, a new college graduate starting his or her first job. This is because seasoned employees can draw from past experiences to help them adjust to their new work settings and therefore may be less affected by specific socialization efforts because they have (a) a better understanding of their own needs and requirements at work (2002).[14] and (b) are more familiar with what is acceptable in the work context.[15][16] Additionally, veteran workers may have used their past experiences to seek out organizations in which they will be a better fit, giving them an immediate advantage in adapting to their new jobs.[17]		Certain behaviors enacted by incoming employees, such as building relationships and seeking information and feedback, can help facilitate the onboarding process. Newcomers can also quicken the speed of their adjustment by demonstrating behaviors that assist them in clarifying expectations, learning organizational values and norms, and gaining social acceptance.[1]		Information seeking occurs when new employees ask questions of their co-workers and superiors in an effort to learn about their new job and the company's norms, expectations, procedures, and policies. Miller and Jablin (1991) developed a typology of information sought after by new hires. These include referent information, understanding what is required to function on the job (role clarity); appraisal information, understanding how effectively the newcomer is able to function in relation to job role requirements (self-efficacy); and finally, relational information, information about the quality of relationships with current organizational employees (social acceptance). By actively seeking information, employees can effectively reduce uncertainties about their new jobs and organizations and make sense of their new working environments.[18] Newcomers can also passively seek information via monitoring their surroundings or by simply viewing the company website or handbook.[1] Research has shown that information seeking by incoming employees is associated with social integration, higher levels of organizational commitment, job performance, and job satisfaction in both individualistic and collectivist cultures.[19]		Feedback seeking is similar to information seeking, but it is focused on a new employee's particular behaviors rather than on general information about the job or company. Specifically, feedback seeking refers to new employee efforts to gauge how to behave in their new organization. A new employee may ask co-workers or superiors for feedback on how well he or she is performing certain job tasks or whether certain behaviors are appropriate in the social and political context of the organization. In seeking constructive criticism about their actions, new employees learn what kinds of behaviors are expected, accepted, or frowned upon within the company or work group, and when they incorporate this feedback and adjust their behavior accordingly, they begin to blend seamlessly into the organization.[20] Instances of feedback inquiry vary across cultural contexts such that individuals high in self-assertiveness and cultures low in power distance report more feedback seeking than newcomers in cultures where self-assertiveness is low and power distance is high.[21]		Also called networking, relationship building involves an employee's efforts to develop camaraderie with co-workers and even supervisors. This can be achieved informally through simply talking to their new peers during a coffee break or through more formal means such as taking part in pre-arranged company events. Research has shown relationship building to be a key part of the onboarding process, leading to outcomes such as greater job satisfaction and better job performance,[2] as well as decreased stress.[4]		Organizations also invest a great amount of time and resources into the training and orientation of new company hires. Organizations differ in the variety of socialization activities they offer in order to integrate productive new workers. Possible activities include their socialization tactics, formal orientation programs, recruitment strategies, and mentorship opportunities.		Socialization tactics, or orientation tactics, are designed based on an organization's needs, values, and structural policies. Some organizations favor a more systematic approach to socialization, while others follow a more "sink or swim" approach in which new employees are challenged to figure out existing norms and company expectations without guidance.		John Van Maanen and Edgar H. Schein have identified at least six major tactical dimensions that characterize and represent all of the ways in which organizations may differ in their approaches to socialization.		Collective socialization refers to the process of taking a group of recruits who are facing a given boundary passage and putting them through the same set of experiences together. Examples of this include: basic training/boot camp for a military organization, pledging for fraternities/sororities, education in graduate schools, and so forth. Socialization in the Individual mode allows newcomers to accumulate unique experiences separate from other newcomers. Examples of this process include: Apprenticeship programs, specific internships, "on-the-job" training, etc.[22]		Formal socialization refers to those tactics in which newcomers are more or less segregated from others and trained on the job. These processes can be witnessed with such socialization programs as police academies, internships, and apprenticeships. Informal socialization processes, on the other hand, involve little separation between newcomers and the existing employees, nor is there any effort made to distinguish the newcomer’s role specifically. Informal tactics provides a non-interventional environment for recruits to learn their new roles via trial and error. Examples of informal socialization include on-the-job training assignments, apprenticeship programs with no clearly defined role, and more generally, any situation in which a newcomer is placed into a work group with no recruit role.[22]		Sequential socialization refers to the degree to which an organization or occupation specifies discrete and identifiable steps for the newcomers to know what phases they need to go through. Random socialization occurs when the sequences of steps leading to the targeted role are unknown, and the entire progression is quite ambiguous. In other words, while there are numerous steps or stages leading to specific organizational roles, there is necessarily no specific order in which the steps should be taken.[22]		This dimension refers to the extent to which the steps have a timetable developed by the organization and communicated to the recruit in order to convey when the socialization process is complete. Fixed socialization provides a recruit with the exact knowledge of the time it will take complete a given passage. For instance, some management trainees can be put on " fast tracks" where they are required to accept new rotational assignment on an annual basis despite their own preferences. Variable socialization processes gives a newcomer no specific timetable, but a few clues as to when to expect a given boundary passage. This type of socialization is commonly associated upwardly mobile careers within business organizations because of several uncontrolled factors such as the state of the economy or turnover rates which determine whether any given newcomer will be promoted to a higher level or not.[22]		A serial socialization process refers to experienced members of the organization grooming the newcomers who are about to occupy similar positions within the organization. These experience members essentially serve as role models for the inexperienced newcomers. A prime example of serial socialization would be a rookie police officer getting assigned patrol duties with an experienced veteran who has been in law enforcement for a lengthy period of time. Disjunctive socialization, in contrast, refers to when newcomers are not following the guidelines of their predecessors, and there are no role models to inform new recruits on how to fulfill their duties.[22]		This tactic refers to the degree to which a socialization process either affirms or disaffirms the identity of the newly entering recruit. Investiture socialization processes sanction and document for newcomers the viability and efficacy of the personal characteristics that they bring to the organization. When organizations use this socialization process it prefers that the recruit remains the exact way that he or she naturally behaves and the organization merely makes use of the skills, values, and attitudes that the recruit is believed to have in their possession. Divestiture socialization, on the other hand, is a process that organizations use to reject and remove the certain personal characteristics of a recruit. Many occupations and organizations require newcomers to sever previous ties, and forget old habits in order to create a new self-image based upon new assumptions.[22]		Thus, tactics influence the socialization process by defining the type of information newcomers receive, the source of this information, and the ease of obtaining it.[22]		Building upon the work of Van Maanen and Schein, Jones (1986) proposed that the previous six dimensions could be reduced to two categories: institutionalized and individualized socialization. Companies that use institutionalized socialization tactics implement structured step-by-step programs, enter into an orchestrated orientation as a group, and receive help from an assigned role model or mentor. Examples of organizations using institutionalized tactics include the military, in which new recruits undergo extensive training and socialization activities through a participative cohort, as well as incoming freshmen at universities, who may attend orientation weekends before beginning classes.		On the opposite end of the spectrum, other organizations use individualized socialization tactics in which the new employee immediately starts working on his or her new position and figures out company norms, values, and expectations along the way. In this orientation system, individuals must play a more proactive role in seeking out information and initiating work relationships.[23]		Regardless of the socialization tactics utilized, formal orientation programs can facilitate understanding of company culture, and introduces new employees to their work roles and the organizational social environment. Formal orientation programs may consist of lectures, videotapes, and written material, while other organizations may rely on more usual approaches. More recent approaches such as computer-based orientations and Internets have been used by organizations to standardize training programs across branch locations. A review of the literature indicates that orientation programs are successful in communicating the company's goals, history, and power structure.[24]		Recruitment events play a key role in identifying which prospective employees are a good fit with an organization. Recruiting events allow employees to gather initial information about an organization's expectations and company culture. By providing a realistic job preview of what life inside the organization is like, companies can weed out potential employees who are clearly a misfit to an organization and individuals can identify which employment agencies are the most suitable match for their own personal values, goals, and expectations. Research has shown that new employees who receive a great amount of accurate information about the job and the company tend to adjust better.[25] Organizations can also provide realistic job previews by offering internship opportunities.		Mentorship has demonstrated importance in the socialization of new employees.[26][27] Ostroff and Kozlowski (1993) discovered that newcomers with mentors become more knowledgeable about the organization than did newcomers without mentors. Mentors can help newcomers better manage their expectations and feel comfortable with their new environment through advice-giving and social support.[28] Chatman (1991) found that newcomers are more likely to have internalized the key values of their organization's culture if they had spent time with an assigned mentor and attended company social events. Literature has also suggested the importance of demographic matching between organizational mentors and protégés.[26] Enscher & Murphy (1997) examined the effects of similarity (race and gender) on the amount of contact and quality of mentor relationships. Results indicate that liking, satisfaction, and contact were higher in conditions of perceived mentor-protégé similarity.[29] But what often separates rapid on-boarders from their slower counterparts is not the availability of a mentor but the presence of a "buddy," someone of whom the newcomer can comfortably ask questions that are either trivial ("How do I order office supplies?") or politically sensitive ("Whose opinion really matters here?").[30] Like mentors, buddies can be people who are officially assigned by a manager or who simply emerge informally (a nearby co-worker, for instance) as an easily accessible resource and confidant.[31] Furthermore, buddies can help establish relationships with co-workers in ways that can't always be facilitated by a newcomer's manager or mentor.[32]		In order to increase the success of an onboarding program, it is important for an organization to monitor how well their new hires are adjusting to their new roles, responsibilities, peers, supervisors, and the organization at large. Researchers have noted that role clarity, self-efficacy, social acceptance, and knowledge of organizational culture are particularly good indicators of well-adjusted new employees who have benefitted from an effective onboarding system.		Role clarity describes a new employee's understanding of his or her job responsibilities and organizational role. One of the goals of an onboarding process is to aid newcomers in reducing ambiguity and uncertainty so that it is easier for them to get their jobs done correctly and efficiently. Because there often is a disconnect between the chief responsibilities listed in a job description and the specific, repeatable tasks that employees must complete to be successful in their roles, it's vital that managers are trained to discuss exactly what they expect from their employees.[33] A poor onboarding program, for example, may produce employees who exhibit sub-par productivity because they are unsure of their exact roles and responsibilities. On the other hand, a strong onboarding program would produce employees who are especially productive because they know exactly what is expected of them in their job tasks and their organizational role. Given this information, it is easy to see why an organization would benefit substantially from increasing role clarity for a new employee. Not only does role clarity imply greater productivity, but it has also been linked to both job satisfaction and organizational commitment.[34]		Self-efficacy is the degree to which new employees feel capable of successfully completing their assigned job tasks and fulfilling their responsibilities. It makes logical sense that employees who feel as though they can get the job done would fare better than those who feel overwhelmed in their new positions, and unsurprisingly, researchers have found that job satisfaction, organizational commitment, and turnover are all correlated with feelings of self-efficacy.[3]		Social acceptance gives new employees the support needed to be successful. While role clarity and self-efficacy are important to a newcomer's ability to meet the requirements of a job, the feeling of "fitting in" can do a lot for one's perception of the work environment and has been demonstrated to increase commitment to an organization and decrease turnover.[3] If an employee feels well received by his or her peers, a personal investment in the organization develops, and leaving becomes less likely.		Knowledge of organizational culture refers to how well a new employee understands a company's values, goals, roles, norms, and overall organizational environment. For example, some organizations may have very strict, yet unspoken, rules of how interactions with superiors should be conducted or whether overtime hours are the norm and an expectation. Knowledge of one's organizational culture is important for the newcomer looking to adapt to a new company, as it allows for social acceptance and aids in completing work tasks in a way that meets company standards. Overall, knowledge of organizational culture has been linked to increased satisfaction and commitment, as well as decreased turnover.[35]		Historically, organizations have overlooked the influence of business practices in shaping enduring work attitudes and thus have continually underestimated their impact on financial success.[36] Employees' job attitudes are particularly important from an organization's perspective because of their link to employee engagement and performance on the job. Employee engagement attitudes, such as satisfaction with one's job and organizational commitment or loyalty, have important implications for an employee's work performance and intentions to stay with or quit an organization. This translates into strong monetary gains for organizations as research has demonstrated that individuals who are highly satisfied with their jobs and who exhibit high organizational commitment are likely to perform better and remain in an organization, whereas individuals who have developed negative attitudes (are highly dissatisfied and unattached to their jobs) are characterized by low performance and high turnover rates.[36][37] Unengaged employees are very costly to organizations in terms of slowed performance and rehiring expenses. Since, attitudinal formations begin from the initial point of contact with an organization, practitioners would be wise to take advantage of positive attitudinal development during socialization periods in order to ensure a strong, productive, and dedicated workforce.		Although the outcomes of organizational socialization have been positively associated with the process of uncertainty reduction, they may not necessarily be desirable to all organizations. Jones (1986) as well as Allen and Meyer (1990) found that socialization tactics were related to commitment, but they were negatively correlated to role clarity.[23][38] Because formal socialization tactics insulate the newcomer from their full responsibilities while "learning the ropes", there is a potential for role confusion once expected to fully enter the organization. In some cases though, organizations may even desire a certain level of person-organizational misfit in order to achieve outcomes via innovative behaviors.[8] Depending on the culture of the organization, it may be more desirable to increase ambiguity despite the potentially negative connection with organizational commitment.		Additionally, socialization researchers have had major concerns over the length of time that it takes newcomers to adjust. There has been great difficulty determining the role that time plays, but once the length of the adjustment is determined, organizations can make appropriate recommendations regarding what matters most in various stages of the adjustment process.[8]		Further criticisms include the use of special orientation sessions to educate newcomers about the organization and strengthen their organizational commitment. While these sessions have been found to be often formal and ritualistic, several studies have found them unpleasant or traumatic.[39] Orientation sessions are a frequently used socialization tactic, however, employees have not found them to be helpful, nor has any research provided any evidence for their benefits.[40][41][42][43][44]		Executive onboarding is the application of general onboarding principles to helping new executives become productive members of an organization. Practically, executive onboarding involves acquiring, accommodating, assimilating and accelerating new executives.[45] Proponents emphasize the importance of making the most of the "honeymoon" stage of a hire, a period which has been described by various sources as either the first 90 to 100 days or the first full year.[46][47][48]		Effective onboarding of new executives can be one of the most important contributions any hiring manager, direct supervisor or human resources professional can make to long-term organizational success, because executive onboarding done right can improve productivity and executive retention, and build shared corporate culture. A study of 20,000 searches revealed that 40 percent of executives hired at the senior level are pushed out, fail, or quit within 18 months.[49]		Onboarding may be especially valuable for externally recruited executives transitioning into complex roles, because it may be difficult for those individuals to uncover personal, organizational, and role risks in complicated situations when they don't have formal onboarding assistance.[50] Onboarding is also an essential tool for executives promoted into new roles and/or transferred from one business unit to another.[51]		It is often valuable to have new executives start some onboarding activities in the "Fuzzy Front End" even before their first day.[52] This is one of ten steps executives can follow to accelerate their onboarding.[53]		The effectiveness of socialization varies depending on the structure of the organization, intra-group communication, and the ease of joining or leaving the organization.[54] These are precisely the dimensions along which online organizations differ from conventional ones. This sporadic, lean communication makes the development and maintenance of social relationship with other group members more difficult to accomplish compared to communication in offline groups,[55] weakening one route toorganizational commitment.[56] Moreover, joining and leaving online communities typically involves such much less cost than joining and leaving a conventional employment organization, which results in lower level of continuance commitment.[57]		Socialization processes in most online communities are informal and individualistic, as compared with socialization in conventional organizations.[58] For example, lurkers in online communities typically have no opportunities for formal mentorship, because they are less likely to be known to existing members of the community. Another example is WikiProjects, the task-oriented group in Wikipedia, rarely use institutional socialization tactics to socialize new members who join them,[59] as they rarely assign the new member a mentor or provide clear guidelines. A third example is the socialization of newcomers to the Python open-source software development community.[60] Even though there exists clear workflows and distinct social roles, socialization process is still informal.		Online organizations, though not as often, adopt institutionalized socialization as well. One general example that applies to many online organizations is the FAQ-style documents we were able to find forums or other resources for training new comers or socializing them to the project. These standardized FAQs often can help documents to familiarize newcomers with how the community operates, which can be thought of as a variant of collective socialization. Another example is World of Warcraft (WoW), which offers formal and sequential socialization. When new players start the game, they are placed in an area that is isolated from more experienced players, and only have the opportunity to interact with other new players.		Some, including scholars at MIT Sloan, suggest that practitioners should seek to design an onboarding strategy that takes individual newcomer characteristics into consideration and encourages proactive behaviors, such as information seeking, that help facilitate the development of role clarity, self-efficacy, social acceptance, and knowledge of organizational culture. Research has consistently shown that doing so produces valuable outcomes such as high job satisfaction (the extent to which one enjoys the nature of his or her work), organizational commitment (the connection one feels to an organization), and job performance in employees, as well as lower turnover rates and decreased intent to quit.[61]		In terms of structure, empirical evidence indicates that formal institutionalized socialization is the most effective onboarding method.[24] New employees who complete these kinds of programs tend to experience more positive job attitudes and lower levels of turnover in comparison to those who undergo individualized tactics.[8][62] Some evidence suggests that in-person onboarding techniques are more effective than virtual ones. Though it may initially appear to be less expensive for a company to use a standard computer-based orientation program to introduce their new employees to the organization, research has demonstrated that employees learn more about their roles and company culture through face-to-face orientation.[63]		
An employment contract or contract of employment is a kind of contract used in labour law to attribute rights and responsibilities between parties to a bargain. The contract is between an "employee" and an "employer". It has arisen out of the old master-servant law, used before the 20th century. But generally, the contract of employment denotes a relationship of economic dependence and social subordination. In the words of the controversial labour lawyer Sir Otto Kahn-Freund,		"the relation between an employer and an isolated employee or worker is typically a relation between a bearer of power and one who is not a bearer of power. In its inception it is an act of submission, in its operation it is a condition of subordination, however much the submission and the subordination may be concealed by the indispensable figment of the legal mind known as the 'contract of employment'. The main object of labour law has been, and... will always be a countervailing force to counteract the inequality of bargaining power which is inherent and must be inherent in the employment relationship."[1]						A contract of employment usually defined to mean the same as a "contract of service".[2] A contract of service has historically been distinguished from a contract for the supply of services, the expression altered to imply the dividing line between a person who is "employed" and someone who is "self-employed". The purpose of the dividing line is to attribute rights to some kinds of people who work for others. This could be the right to a minimum wage, holiday pay, sick leave, fair dismissal,[3] a written statement of the contract, the right to organise in a union, and so on. The assumption is that genuinely self-employed people should be able to look after their own affairs, and therefore work they do for others should not carry with it an obligation to look after these rights.		In Roman law the equivalent dichotomy was that between locatio conductio operarum (employment contract) and locatio conductio operis (contract for services).[4][5]		The terminology is complicated by the use of many other sorts of contracts involving one person doing work for another. Instead of being considered an "employee", the individual could be considered a "worker" (which could mean less employment legislation protection) or as having an "employment relationship" (which could mean protection somewhere in between) or a "professional" or a "dependent entrepreneur", and so on. Different countries will take more or less sophisticated, or complicated approaches to the question.		Main articles: Labour economics and Contemporary slavery		Anarcho-syndicalists and other socialists who criticise wage slavery, e.g. David Ellerman and Carole Pateman, posit that the employment contract is a legal fiction in that it recognises human beings juridically as mere tools or inputs by abdicating responsibility and self-determination, which the critics argue are inalienable. As Ellerman points out, "[t]he employee is legally transformed from being a co-responsible partner to being only an input supplier sharing no legal responsibility for either the input liabilities [costs] or the produced outputs [revenue, profits] of the employer's business."[6] Such contracts are inherently invalid "since the person remain[s] a de facto fully capacitated adult person with only the contractual role of a non-person" as it is impossible to physically transfer self-determination.[7] As Pateman argues:		The contractarian argument is unassailable all the time it is accepted that abilities can 'acquire' an external relation to an individual, and can be treated as if they were property. To treat abilities in this manner is also implicitly to accept that the 'exchange' between employer and worker is like any other exchange of material property . . . The answer to the question of how property in the person can be contracted out is that no such procedure is possible. Labour power, capacities or services, cannot be separated from the person of the worker like pieces of property.[8]		
A cover letter, covering letter, motivation letter, motivational letter or a letter of motivation is a letter of introduction attached to, or accompanying another document such as a résumé or curriculum vitae.[1]						Job seekers frequently send a cover letter along with their curriculum vitae or applications for employment as a way of introducing themselves to potential employers and explaining their suitability for the desired positions.[2] Employers may look for individualized and thoughtfully written cover letters as one method of screening out applicants who are not sufficiently interested in their positions or who lack necessary basic skills.[1] Cover letters are typically divided into two categories:[3]		Résumé cover letters may also serve as marketing devices for prospective job seekers. Cover letters are used in connection with many business documents such as loan applications (mortgage loan), contract drafts and proposals, and executed documents. Cover letters may serve the purpose of trying to catch the reader's interest or persuade the reader of something, or they may simply be an inventory or summary of the documents included along with a discussion of the expected future actions the sender or recipient will take in connection with the documents.		
A job fair, also referred commonly as a career fair or career expo, is speed dating for companies and professionals job seekers. A job fair is an event in which employers, recruiters, and schools give information to potential employees. Job seekers attend these while trying to make a good impression to potential coworkers by speaking face-to-face with one another, filling out résumés, and asking questions in attempt to get a good feel on the work needed. Likewise, online job fairs are held, giving job seekers another way to get in contact with probable employers using the internet.		In colleges, job fairs are commonly used for entry-level job recruitment. Job seekers use this opportunity to meet with them and attempt to stand out from other people and get an overview of what it’s like to work for a company or a sector that seem interesting to them.[1]		Career expositions usually include company or organization tables or booths where resumes can be collected and business cards can be exchanged. Often sponsored by career centers, job fairs provide a convenient location for students to meet employers and perform first interviews. This is also an opportunity for companies to meet with students and talk to them about their expectations from them as students and answer their potential questions such as the degree or work experience needed.[2]		Online job fairs offer many of the same conveniences of regular career fairs. An online job fair uses a virtual platform which allows employers to discuss with potential new nominees for the job they’re offering. This is a way of interacting with them virtually and practical to get to know who they are. A virtual career fair include many services such as video, live chats, downloadable material and many more to make it the more helpful both for the recruiter and the job seeker. After having applied online to positions, many more people are also trying their luck with in-person job fairs.[3]		
Coaching is a form of development in which a person called a coach supports a learner or client in achieving a specific personal or professional goal by providing training and guidance.[1] The learner is sometimes called a coachee. Occasionally, coaching may mean an informal relationship between two people, of whom one has more experience and expertise than the other and offers advice and guidance as the latter learns; but coaching differs from mentoring in focusing on specific tasks or objectives, as opposed to more general goals or overall development.[1][2][3]		The first use of the term "coach" in connection with an instructor or trainer arose around 1830 in Oxford University slang for a tutor who "carried" a student through an exam.[4] The word "coaching" thus identified a process used to transport people from where they are to where they want to be. The first use of the term in relation to sports came in 1861.[4] Historically the development of coaching has been influenced by many fields of activity, including adult education, the Human Potential Movement, large-group awareness training (LGAT) groups such as "est", leadership studies, personal development, and psychology.[5][6]		Professional coaching uses a range of communication skills (such as targeted restatements, listening, questioning, clarifying etc.) to help clients shift their perspectives and thereby discover different approaches to achieve their goals.[7] These skills can be used in almost all types of coaching. In this sense, coaching is a form of "meta-profession" that can apply to supporting clients in any human endeavor, ranging from their concerns in health, personal, professional, sport, social, family, political, spiritual dimensions, etc. There may be some overlap between certain types of coaching activities.[5]		The concept of ADHD coaching was first introduced in 1994 by psychiatrists Edward M. Hallowell and John J. Ratey in their book Driven to Distraction.[8] ADHD coaching is a specialized type of life coaching that uses specific techniques designed to assist individuals with attention-deficit hyperactivity disorder. The goal of ADHD coaching is to mitigate the effects of executive function deficit, which is a typical impairment for people with ADHD.[9] Coaches work with clients to help them better manage time, organize, set goals and complete projects.[10] In addition to helping clients understand the impact ADHD has had on their lives, coaches can help clients develop "work-around" strategies to deal with specific challenges, and determine and use individual strengths. Coaches also help clients get a better grasp of what reasonable expectations are for them as individuals, since people with ADHD "brain wiring" often seem to need external mirrors for accurate self-awareness about their potential despite their impairment.[11]		Unlike psychologists or psychotherapists, ADHD coaches do not provide any therapy or treatment: their focus is only on daily functioning and behaviour aspects of the disorder.[12] The ultimate goal of ADHD coaching is to help clients develop an "inner coach", a set of self-regulation and reflective planning skills to deal with daily life challenges.[13] A 2010 study from Wayne State University evaluated the effectiveness of ADHD coaching on 110 students with ADHD. The research team concluded that the coaching "was highly effective in helping students improve executive functioning and related skills as measured by the Learning and Study Strategies Inventory (LASSI)."[14] Yet, not every ADHD person needs a coach and not everyone can benefit from using a coach.[15]		Business coaching is a type of human resource development for business leaders. It provides positive support, feedback and advice on an individual or group basis to improve personal effectiveness in the business setting. Business coaching is also called executive coaching,[16] corporate coaching or leadership coaching. Coaches help their clients advance towards specific professional goals. These include career transition, interpersonal and professional communication, performance management, organizational effectiveness, managing career and personal changes, developing executive presence, enhancing strategic thinking, dealing effectively with conflict, and building an effective team within an organization. An industrial organizational psychologist is one example of executive coach. Business coaching is not restricted to external experts or providers. Many organizations expect their senior leaders and middle managers to coach their team members to reach higher levels of performance, increased job satisfaction, personal growth, and career development. Research studies suggest that executive coaching has a positive impact on workplace performance.[17]		In some countries, there is no certification or licensing required to be a business or executive coach, and membership of a coaching organization is optional. Further, standards and methods of training coaches can vary widely between coaching organizations. Many business coaches refer to themselves as consultants, a broader business relationship than one which exclusively involves coaching.[18]		Career coaching focuses on work and career and is similar to career counseling. Career coaching is not to be confused with life coaching, which concentrates on personal development. Another common term for a career coach is career guide.		Christian coaching is common among religious organizations and churches.[citation needed] A Christian coach is not a pastor or counselor (although he may also be qualified in those disciplines), but rather someone who has been professionally trained to address specific coaching goals from a distinctively Christian or biblical perspective. Although various training courses exist, there is no single regulatory body for Christian coaching. Some[which?] of the Christian coaching programs are based on the works of Henry Cloud, John Townsend, and John C. Maxwell.[citation needed]		Co-coaching is a structured practice of coaching between peers with the goal of learning improved coaching techniques.		Financial coaching is a relatively new form of coaching that focuses on helping clients overcome their struggle to attain specific financial goals and aspirations they have set for themselves. Financial coaching is a one-on-one relationship in which the coach works to provide encouragement and support aimed at facilitating attainment of the client's financial plans. A financial coach, also called money coach, typically focuses on helping clients to restructure and reduce debt, reduce spending, develop saving habits, and develop financial discipline. In contrast, the term financial adviser refers to a wider range of professionals who typically provide clients with financial products and services. Although early research links financial coaching to improvements in client outcomes, much more rigorous analysis is necessary before any causal linkages can be established.[19]		Health coaching is becoming recognized as a new way to help individuals "manage" their illnesses and conditions, especially those of a chronic nature.[20] The coach will use special techniques, personal experience, expertise and encouragement to assist the coachee in bringing his/her behavioral changes about, while aiming for lowered health risks and decreased healthcare costs.[21] The National Society of Health Coaches (NSHC) has differentiated the term health coach from wellness coach.[21] According to the NSHC, health coaches are qualified "to guide those with acute or chronic conditions and/or moderate to high health risk", and wellness coaches provide guidance and inspiration "to otherwise 'healthy' individuals who desire to maintain or improve their overall general health status".[21]		Homework coaching focuses on equipping a student with the study skills required to succeed academically. This approach is different from regular tutoring which typically seeks to improve a student's performance in a specific subject.[22]		Coaching in education is seen as a useful intervention to support students, faculty and administrators in educational organizations.[23] For students, opportunities for coaching include collaborating with fellow students to improve grades and skills, both academic and social; for teachers and administrators, coaching can help with transitions into new roles.[23]		Life coaching is the process of helping people identify and achieve personal goals. Although life coaches may have studied counseling psychology or related subjects, a life coach does not act as a therapist, counselor, or health care provider, and psychological intervention lies outside the scope of life coaching.		Relationship coaching is the application of coaching to personal and business relationships.[24]		In sports, a coach is an individual that provides supervision and training to the sports team or individual players. Sports coaches are involved in administration, athletic training, competition coaching, and representation of the team and the players.		Since the mid-1990s, coaching professional associations such as the Association for Coaching (AC), the European Mentoring and Coaching Council (EMCC), the International Association of Coaching (IAC), and the International Coach Federation (ICF) have worked towards developing training standards.[1]:287–312[25] Psychologist Jonathan Passmore noted in 2016:[1]:3		While coaching has become a recognized intervention, sadly there are still no standards or licensing arrangements which are widely recognized. Professional bodies have continued to develop their own standards, but the lack of regulation means anyone can call themselves a coach. [...] Whether coaching is a profession which requires regulation, or is professional and requires standards, remains a matter of debate.		One of the challenges in the field of coaching is upholding levels of professionalism, standards and ethics.[25] To this end, coaching bodies and organizations have codes of ethics and member standards.[1]:287–312[26] However, because these bodies are not regulated, and because coaches do not need to belong to such a body, ethics and standards are variable in the field.[25][27] In February 2016, the AC and the EMCC launched a "Global Code of Ethics" for the entire industry; individuals, associations, and organizations are invited to become signatories to it.[28][29]:1		With the growing popularity of coaching, many colleges and universities now offer coach training programs that are accredited by a professional association.[30] Some courses offer a life coach certificate after just a few days of training,[31] but such courses, if they are accredited at all, are considered "à la carte" training programs, "which may or may not offer start to finish coach training," according to the ICF.[32] In contrast, "all-inclusive" training programs accredited by the ICF, for example, require a minimum of 125 student contact hours, 10 hours of mentor coaching and a performance evaluation process.[33][34] This is very little training in comparison to the training requirements of some other helping professions: for example, licensure as a counseling psychologist in the State of California requires 3,000 hours of supervised professional experience.[35] However, the ICF, for example, offers a "Master Certified Coach" credential that requires demonstration of "2,500 hours (2,250 paid) of coaching experience with at least 35 clients"[36] and a "Professional Certified Coach" credential with fewer requirements.[37] Other professional bodies similarly offer entry-level, intermediate, and advanced coach accreditation options.[38] Some coaches are both certified coaches and licensed counseling psychologists, integrating coaching and counseling.[39]		Critics see life coaching as akin to psychotherapy but without the legal restrictions and state regulation of psychologists.[25][40][41][42] There are no state regulation/licensing requirements for coaches. Due to lack of regulation, people who have no formal training or certification can legally call themselves life or wellness coaches.[43]		
Health insurance is insurance that covers the whole or a part of the risk of a person incurring medical expenses, spreading the risk over a large number of persons. By estimating the overall risk of health care and health system expenses over the risk pool, an insurer can develop a routine finance structure, such as a monthly premium or payroll tax, to provide the money to pay for the health care benefits specified in the insurance agreement. The benefit is administered by a central organization such as a government agency, private business, or not-for-profit entity. According to the Health Insurance Association of America, health insurance is defined as "coverage that provides for the payments of benefits as a result of sickness or injury. It includes insurance for losses from accident, medical expense, disability, or accidental death and dismemberment" (pg. 225).[1]						A health insurance policy is:		The individual insured person's obligations may take several forms:[2]		Prescription drug plans are a form of insurance offered through some health insurance plans. In the U.S., the patient usually pays a copayment and the prescription drug insurance part or all of the balance for drugs covered in the formulary of the plan. Such plans are routinely part of national health insurance programs. For example, in the province of Quebec, Canada, prescription drug insurance is universally required as part of the public health insurance plan, but may be purchased and administered either through private or group plans, or through the public plan.[4]		Some, if not most, health care providers in the United States will agree to bill the insurance company if patients are willing to sign an agreement that they will be responsible for the amount that the insurance company doesn't pay. The insurance company pays out of network providers according to "reasonable and customary" charges, which may be less than the provider's usual fee. The provider may also have a separate contract with the insurer to accept what amounts to a discounted rate or capitation to the provider's standard charges. It generally costs the patient less to use an in-network provider.		The Commonwealth Fund, in its annual survey, "Mirror, Mirror on the Wall", compares the performance of the health care systems in Australia, New Zealand, the United Kingdom, Germany, Canada and the U.S. Its 2007 study found that, although the U.S. system is the most expensive, it consistently under-performs compared to the other countries.[6] One difference between the U.S. and the other countries in the study is that the U.S. is the only country without universal health insurance coverage.		The Commonwealth Fund completed its thirteenth annual health policy survey in 2010.[8] A study of the survey "found significant differences in access, cost burdens, and problems with health insurance that are associated with insurance design".[8] Of the countries surveyed, the results indicated that people in the United States had more out-of-pocket expenses, more disputes with insurance companies than other countries, and more insurance payments denied; paperwork was also higher although Germany had similarly high levels of paperwork.[8]		The public health system is called Medicare. It ensures free universal access to hospital treatment and subsidised out-of-hospital medical treatment. It is funded by a 1.5% tax levy on all taxpayers, an extra 1% levy on high income earners, as well as general revenue.		The private health system is funded by a number of private health insurance organizations. The largest of these is Medibank Private Limited, which was, until 2014, a government-owned entity, when it was privatized and listed on the Australian Stock Exchange.		Some private health insurers are 'for profit' enterprises such as Australian Unity, and some are non-profit organizations such as HCF and the HBF Health Fund (HBF). Some, such as Police Health, have membership restricted to particular groups, but the majority have open membership. Membership to most health funds is now also available through comparison websites like moneytime, Compare the Market, iSelect Ltd., Choosi, ComparingExpert [2] and YouCompare. These comparison sites operate on a commission-basis by agreement with their participating health funds. The Private Health Insurance Ombudsman also operates a free website which allows consumers to search for and compare private health insurers' products, which includes information on price and level of cover.[9]		Most aspects of private health insurance in Australia are regulated by the Private Health Insurance Act 2007. Complaints and reporting of the private health industry is carried out by an independent government agency, the Private Health Insurance Ombudsman. The ombudsman publishes an annual report that outlines the number and nature of complaints per health fund compared to their market share [10]		The private health system in Australia operates on a "community rating" basis, whereby premiums do not vary solely because of a person's previous medical history, current state of health, or (generally speaking) their age (but see Lifetime Health Cover below). Balancing this are waiting periods, in particular for pre-existing conditions (usually referred to within the industry as PEA, which stands for "pre-existing ailment"). Funds are entitled to impose a waiting period of up to 12 months on benefits for any medical condition the signs and symptoms of which existed during the six months ending on the day the person first took out insurance. They are also entitled to impose a 12-month waiting period for benefits for treatment relating to an obstetric condition, and a 2-month waiting period for all other benefits when a person first takes out private insurance. Funds have the discretion to reduce or remove such waiting periods in individual cases. They are also free not to impose them to begin with, but this would place such a fund at risk of "adverse selection", attracting a disproportionate number of members from other funds, or from the pool of intending members who might otherwise have joined other funds. It would also attract people with existing medical conditions, who might not otherwise have taken out insurance at all because of the denial of benefits for 12 months due to the PEA Rule. The benefits paid out for these conditions would create pressure on premiums for all the fund's members, causing some to drop their membership, which would lead to further rises in premiums, and a vicious cycle of higher premiums-leaving members would ensue.		The Australian government has introduced a number of incentives to encourage adults to take out private hospital insurance. These include:		Health care is mainly a constitutional, provincial government responsibility in Canada (the main exceptions being federal government responsibility for services provided to aboriginal peoples covered by treaties, the Royal Canadian Mounted Police, the armed forces, and members of parliament). Consequently, each province administers its own health insurance program. The federal government influences health insurance by virtue of its fiscal powers – it transfers cash and tax points to the provinces to help cover the costs of the universal health insurance programs. Under the Canada Health Act, the federal government mandates and enforces the requirement that all people have free access to what are termed "medically necessary services," defined primarily as care delivered by physicians or in hospitals, and the nursing component of long term residential care. If provinces allow doctors or institutions to charge patients for medically necessary services, the federal government reduces its payments to the provinces by the amount of the prohibited charges. Collectively, the public provincial health insurance systems in Canada are frequently referred to as Medicare. This public insurance is tax-funded out of general government revenues, although British Columbia and Ontario levy a mandatory premium with flat rates for individuals and families to generate additional revenues – in essence a surtax. Private health insurance is allowed, but in six provincial governments only for services that the public health plans do not cover, for example, semi-private or private rooms in hospitals and prescription drug plans. Four provinces allow insurance for services also mandated by the Canada Health Act, but in practice there is no market for it. All Canadians are free to use private insurance for elective medical services such as laser vision correction surgery, cosmetic surgery, and other non-basic medical procedures. Some 65% of Canadians have some form of supplementary private health insurance; many of them receive it through their employers.[15] Private-sector services not paid for by the government account for nearly 30 percent of total health care spending.[16]		In 2005, the Supreme Court of Canada ruled, in Chaoulli v. Quebec, that the province's prohibition on private insurance for health care already insured by the provincial plan violated the Quebec Charter of Rights and Freedoms, and in particular the sections dealing with the right to life and security, if there were unacceptably long wait times for treatment, as was alleged in this case. The ruling has not changed the overall pattern of health insurance across Canada but has spurred on attempts to tackle the core issues of supply and demand and the impact of wait times.[17]		The national system of health insurance was instituted in 1945, just after the end of the Second World War. It was a compromise between Gaullist and Communist representatives in the French parliament. The Conservative Gaullists were opposed to a state-run healthcare system, while the Communists were supportive of a complete nationalisation of health care along a British Beveridge model.		The resulting programme is profession-based: all people working are required to pay a portion of their income to a not-for-profit health insurance fund, which mutualises the risk of illness, and which reimburses medical expenses at varying rates. Children and spouses of insured people are eligible for benefits, as well. Each fund is free to manage its own budget, and used to reimburse medical expenses at the rate it saw fit, however following a number of reforms in recent years, the majority of funds provide the same level of reimbursement and benefits.		The government has two responsibilities in this system.		Today, this system is more or less intact. All citizens and legal foreign residents of France are covered by one of these mandatory programs, which continue to be funded by worker participation. However, since 1945, a number of major changes have been introduced. Firstly, the different health care funds (there are five: General, Independent, Agricultural, Student, Public Servants) now all reimburse at the same rate. Secondly, since 2000, the government now provides health care to those who are not covered by a mandatory regime (those who have never worked and who are not students, meaning the very rich or the very poor). This regime, unlike the worker-financed ones, is financed via general taxation and reimburses at a higher rate than the profession-based system for those who cannot afford to make up the difference. Finally, to counter the rise in health care costs, the government has installed two plans, (in 2004 and 2006), which require insured people to declare a referring doctor in order to be fully reimbursed for specialist visits, and which installed a mandatory co-pay of 1 € (about $1.45) for a doctor visit, 0,50 € (about 80¢) for each box of medicine prescribed, and a fee of 16–18 € ($20–25) per day for hospital stays and for expensive procedures.		An important element of the French insurance system is solidarity: the more ill a person becomes, the less the person pays. This means that for people with serious or chronic illnesses, the insurance system reimburses them 100% of expenses, and waives their co-pay charges.		Finally, for fees that the mandatory system does not cover, there is a large range of private complementary insurance plans available. The market for these programs is very competitive, and often subsidised by the employer, which means that premiums are usually modest. 85% of French people benefit from complementary private health insurance.[18][19]		Germany has the world's oldest national social health insurance system,[20] with origins dating back to Otto von Bismarck's Sickness Insurance Law of 1883.[21][22]		Currently 85% of the population is covered by a basic health insurance plan provided by statute, which provides a standard level of coverage. The remainder opt for private health insurance,[23] which frequently offers additional benefits. According to the World Health Organization, Germany's health care system was 77% government-funded and 23% privately funded as of 2004.[24]		The government partially reimburses the costs for low-wage workers, whose premiums are capped at a predetermined value. Higher wage workers pay a premium based on their salary. They may also opt for private insurance, which is generally more expensive, but whose price may vary based on the individual's health status.[25]		Reimbursement is on a fee-for-service basis, but the number of physicians allowed to accept Statutory Health Insurance in a given locale is regulated by the government and professional societies.		Co payments were introduced in the 1980s in an attempt to prevent over utilization. The average length of hospital stay in Germany has decreased in recent years from 14 days to 9 days, still considerably longer than average stays in the United States (5 to 6 days).[26][27] Part of the difference is that the chief consideration for hospital reimbursement is the number of hospital days as opposed to procedures or diagnosis. Drug costs have increased substantially, rising nearly 60% from 1991 through 2005. Despite attempts to contain costs, overall health care expenditures rose to 10.7% of GDP in 2005, comparable to other western European nations, but substantially less than that spent in the U.S. (nearly 16% of GDP).[28]		Germans are offered three kinds of social security insurance dealing with the physical status of a person and which are co-financed by employer and employee: health insurance, accident insurance, and long-term care insurance.		Germany has a universal multi-payer system with two main types of health insurance: law enforced health insurance (or public health insurance) (Gesetzliche Krankenversicherung (GKV)) and private insurance (Private Krankenversicherung (PKV)). Both systems struggle with the increasing cost of medical treatment and the changing demography. About 87.5% of the persons with health insurance are members of the public system, while 12.5% are covered by private insurance (as of 2006).[29] There are many differences between the public health insurance and private insurance. In general the benefits and costs in the private insurance are better for young people without family. There are hard salary requirements to join the private insurance because it is getting more expensive advanced in years.[30]		The statutory health insurance (est. in 1883) is part of the German social insurance system, together with the statutory accident insurance (est. 1883), the statutory old age and disability insurance (est. in 1889), the unemployment insurance (est. in 1927) and the long term care insurance (est. in 1995).[citation needed]		Since 2009, health insurance is mandatory for anyone living in Germany.[31]		The statutory health insurance is a compulsory insurance for employees with a yearly income below €54.900 (in 2015, adjusted annually) and others.		With the 'Imperial Bill of 15 June 1883' and its update from 10 April 1892 the health insurance bill was created, which introduced compulsory health insurance for workers. Austria followed Germany in 1888, Hungary in 1891 and Switzerland in 1911.		On 29 April 1869 the county health insurance ill[clarification needed] in Bavaria created the first law that introduced and regulated health insurance for low income earners. It was limited to individuals with an income less than 2000 Mark per year and guaranteed the insured person a 60% minimum income during sickness.[citation needed]		Function of the statutory health insurance according to § 1 SGB V is to preserve, recreate or improve health of the insured person. According to § 27 SGB V this includes to "subdue the afflictions of illness".[32]		All insured fundamentally have the same entitlement for benefits. The scope of benefits is regulated in SGB V ("social insurance bill five") and limited by § 1 SGB V. Benefits have to be adequate, appropriate and economic and shall not exceed the necessary for the insured.[33]		Additional benefits can only be granted based on particular regulations based on formal law. These are e.g. additional service for the prevention of sickness, care at home, household support, rehabilitation etc.[citation needed]		Based on the principle of solidarity and compulsory membership, the calculation of fees differs from private health insurance in that it does not depend on personal health or health criteria like age or sex, but is connected to one's personal income by a fixed percentage. The aim is to cover the risk of high cost from illness that an individual can not bear alone.[citation needed]		The German legislature has reduced the number of public health insurance organisations from 1209 in 1991 down to 123 in 2015.[34]		The public health insurance organisations (Krankenkassen) are the Ersatzkassen (EK), Allgemeine Ortskrankenkassen (AOK), Betriebskrankenkassen (BKK), Innungskrankenkassen (IKK), Knappschaft (KBS), and Landwirtschaftliche Krankenkasse (LKK).[35]		As long as a person has the right to choose his or her health insurance, he or she can join any insurance that is willing to include the individual.		Accident insurance (Unfallversicherung) is covered by the employer and basically covers all risks for commuting to work and at the workplace.		Long-term care (Pflegeversicherung[37]) is covered half and half by employer and employee and covers cases in which a person is not able to manage his or her daily routine (provision of food, cleaning of apartment, personal hygiene, etc.). It is about 2% of a yearly salaried income or pension, with employers matching the contribution of the employee.		In 2013 a state funded private care insurance was introduced ("Private Pflegeversicherung").[38] Insurance contracts that fit certain criteria are subsidised with 60 Euro per year. It is expected that the number of contracts will grow from 400,000 by end of 2013 to over a million within the next few years.[39] These contracts have been criticized by consumer rights foundations.[40]		There are two major types of insurance programs available in Japan – Employees Health Insurance (健康保険 Kenkō-Hoken), and National Health Insurance (国民健康保険 Kokumin-Kenkō-Hoken). National Health insurance is designed for people who are not eligible to be members of any employment-based health insurance program. Although private health insurance is also available, all Japanese citizens, permanent residents, and non-Japanese with a visa lasting one year or longer are required to be enrolled in either National Health Insurance or Employees Health Insurance.		In 2006, a new system of health insurance came into force in the Netherlands. This new system avoids the two pitfalls of adverse selection and moral hazard associated with traditional forms of health insurance by using a combination of regulation and an insurance equalization pool. Moral hazard is avoided by mandating that insurance companies provide at least one policy which meets a government set minimum standard level of coverage, and all adult residents are obliged by law to purchase this coverage from an insurance company of their choice. All insurance companies receive funds from the equalization pool to help cover the cost of this government-mandated coverage. This pool is run by a regulator which collects salary-based contributions from employers, which make up about 50% of all health care funding, and funding from the government to cover people who cannot afford health care, which makes up an additional 5%.		The remaining 45% of health care funding comes from insurance premiums paid by the public, for which companies compete on price, though the variation between the various competing insurers is only about 5%. However, insurance companies are free to sell additional policies to provide coverage beyond the national minimum. These policies do not receive funding from the equalization pool, but cover additional treatments, such as dental procedures and physiotherapy, which are not paid for by the mandatory policy.		Funding from the equalization pool is distributed to insurance companies for each person they insure under the required policy. However, high-risk individuals get more from the pool, and low-income persons and children under 18 have their insurance paid for entirely. Because of this, insurance companies no longer find insuring high risk individuals an unappealing proposition, avoiding the potential problem of adverse selection.		Insurance companies are not allowed to have co-payments, caps, or deductibles, or to deny coverage to any person applying for a policy, or to charge anything other than their nationally set and published standard premiums. Therefore, every person buying insurance will pay the same price as everyone else buying the same policy, and every person will get at least the minimum level of coverage.		Since 1974, New Zealand has had a system of universal no-fault health insurance for personal injuries through the Accident Compensation Corporation (ACC). The ACC scheme covers most of the costs of related to treatment of injuries acquired in New Zealand (including overseas visitors) regardless of how the injury occurred, and also covers lost income (at 80 percent of the employee's pre-injury income) and costs related to long-term rehabilitation, such as home and vehicle modifications for those seriously injured. Funding from the scheme comes from a combination of levies on employers' payroll (for work injuries), levies on an employee's taxable income (for non-work injuries to salary earners), levies on vehicle licensing fees and petrol (for motor vehicle accidents), and funds from the general taxation pool (for non-work injuries to children, senior citizens, unemployed people, overseas visitors, etc.)		Rwanda is one of a handful of low income countries that has implemented community-based health insurance schemes in order to reduce the financial barriers that prevent poor people from seeking and receiving needed health services. This scheme has helped reach 90% of the country's population with health care coverage.[41][42]		Healthcare in Switzerland is universal[43] and is regulated by the Swiss Federal Law on Health Insurance. Health insurance is compulsory for all persons residing in Switzerland (within three months of taking up residence or being born in the country).[44][45] It is therefore the same throughout the country and avoids double standards in healthcare. Insurers are required to offer this basic insurance to everyone, regardless of age or medical condition. They are not allowed to make a profit off this basic insurance, but can on supplemental plans.[43]		The universal compulsory coverage provides for treatment in case of illness or accident and pregnancy. Health insurance covers the costs of medical treatment, medication and hospitalization of the insured. However, the insured person pays part of the costs up to a maximum, which can vary based on the individually chosen plan, premiums are then adjusted accordingly. The whole healthcare system is geared towards to the general goals of enhancing general public health and reducing costs while encouraging individual responsibility.		The Swiss healthcare system is a combination of public, subsidised private and totally private systems. Insurance premiums vary from insurance company to company, the excess level individually chosen (franchise), the place of residence of the insured person and the degree of supplementary benefit coverage chosen (complementary medicine, routine dental care, semi-private or private ward hospitalisation, etc.).		The insured person has full freedom of choice among the approximately 60 recognised healthcare providers competent to treat their condition (in his region) on the understanding that the costs are covered by the insurance up to the level of the official tariff. There is freedom of choice when selecting an insurance company to which one pays a premium, usually on a monthly basis. The insured person pays the insurance premium for the basic plan up to 8% of their personal income. If a premium is higher than this, the government gives the insured person a cash subsidy to pay for any additional premium.		The compulsory insurance can be supplemented by private "complementary" insurance policies that allow for coverage of some of the treatment categories not covered by the basic insurance or to improve the standard of room and service in case of hospitalisation. This can include complementary medicine, routine dental treatment and private ward hospitalisation, which are not covered by the compulsory insurance.		As far as the compulsory health insurance is concerned, the insurance companies cannot set any conditions relating to age, sex or state of health for coverage. Although the level of premium can vary from one company to another, they must be identical within the same company for all insured persons of the same age group and region, regardless of sex or state of health. This does not apply to complementary insurance, where premiums are risk-based.		Switzerland has an infant mortality rate of about 3.6 out of 1,000. The general life expectancy in 2012 was for men 80.5 years compared to 84.7 years for women.[46] These are the world's best figures.[47]		The UK's National Health Service (NHS) is a publicly funded healthcare system that provides coverage to everyone normally resident in the UK. It is not strictly an insurance system because (a) there are no premiums collected, (b) costs are not charged at the patient level and (c) costs are not pre-paid from a pool. However, it does achieve the main aim of insurance which is to spread financial risk arising from ill-health. The costs of running the NHS (est. £104 billion in 2007-8)[48] are met directly from general taxation. The NHS provides the majority of health care in the UK, including primary care, in-patient care, long-term health care, ophthalmology, and dentistry.		Private health care has continued parallel to the NHS, paid for largely by private insurance, but it is used by less than 8% of the population, and generally as a top-up to NHS services. There are many treatments that the private sector does not provide. For example, health insurance on pregnancy is generally not covered or covered with restricting clauses. Typical exclusions for Bupa schemes (and many other insurers) include:		ageing, menopause and puberty; AIDS/HIV; allergies or allergic disorders; birth control, conception, sexual problems and sex changes; chronic conditions; complications from excluded or restricted conditions/ treatment; convalescence, rehabilitation and general nursing care ; cosmetic, reconstructive or weight loss treatment; deafness; dental/oral treatment (such as fillings, gum disease, jaw shrinkage, etc); dialysis; drugs and dressings for out-patient or take-home use† ; experimental drugs and treatment; eyesight; HRT and bone densitometry; learning difficulties, behavioural and developmental problems; overseas treatment and repatriation; physical aids and devices; pre-existing or special conditions; pregnancy and childbirth; screening and preventive treatment; sleep problems and disorders; speech disorders; temporary relief of symptoms.[49] († = except in exceptional circumstances)		There are a number of other companies in the United Kingdom which include, among others, ACE Limited, AXA, Aviva, Bupa, Groupama Healthcare, WPA and PruHealth. Similar exclusions apply, depending on the policy which is purchased.		Recently (2009) the main representative body of British Medical physicians, the British Medical Association, adopted a policy statement expressing concerns about developments in the health insurance market in the UK. In its Annual Representative Meeting which had been agreed earlier by the Consultants Policy Group (i.e. Senior physicians) stating that the BMA was "extremely concerned that the policies of some private healthcare insurance companies are preventing or restricting patients exercising choice about (i) the consultants who treat them; (ii) the hospital at which they are treated; (iii) making top up payments to cover any gap between the funding provided by their insurance company and the cost of their chosen private treatment." It went in to "call on the BMA to publicise these concerns so that patients are fully informed when making choices about private healthcare insurance."[50] The practice of insurance companies deciding which consultant a patient may see as opposed to GPs or patients is referred to as Open Referral.[51] The NHS offers patients a choice of hospitals and consultants and does not charge for its services.		The private sector has been used to increase NHS capacity despite a large proportion of the British public opposing such involvement.[52] According to the World Health Organization, government funding covered 86% of overall health care expenditures in the UK as of 2004, with private expenditures covering the remaining 14%.[24]		Nearly one in three patients receiving NHS hospital treatment is privately insured and could have the cost paid for by their insurer. Some private schemes provide cash payments to patients who opt for NHS treatment, to deter use of private facilities. A report, by private health analysts Laing and Buisson, in November 2012, estimated that more than 250,000 operations were performed on patients with private medical insurance each year at a cost of £359 million. In addition, £609 million was spent on emergency medical or surgical treatment. Private medical insurance does not normally cover emergency treatment but subsequent recovery could be paid for if the patient were moved into a private patient unit.[53]		The United States health care system relies heavily on private health insurance, which is the primary source of coverage for most Americans. As of 2012[update] about 61% of Americans had private health insurance according to the Centers for Disease Control and Prevention.[54][55] The Agency for Healthcare Research and Quality (AHRQ) found that in 2011, private insurance was billed for 12.2 million U.S. inpatient hospital stays and incurred approximately $112.5 billion in aggregate inpatient hospital costs (29% of the total national aggregate costs).[56] Public programs provide the primary source of coverage for most senior citizens and for low-income children and families who meet certain eligibility requirements. The primary public programs are Medicare, a federal social insurance program for seniors and certain disabled individuals; and Medicaid, funded jointly by the federal government and states but administered at the state level, which covers certain very low income children and their families. Together, Medicare and Medicaid accounted for approximately 63 percent of the national inpatient hospital costs in 2011.[56] SCHIP is a federal-state partnership that serves certain children and families who do not qualify for Medicaid but who cannot afford private coverage. Other public programs include military health benefits provided through TRICARE and the Veterans Health Administration and benefits provided through the Indian Health Service. Some states have additional programs for low-income individuals.[57]		In the late 1990s and early 2000s, health advocacy companies began to appear to help patients deal with the complexities of the healthcare system. The complexity of the healthcare system has resulted in a variety of problems for the American public. A study found that 62 percent of persons declaring bankruptcy in 2007 had unpaid medical expenses of $1000 or more, and in 92% of these cases the medical debts exceeded $5000. Nearly 80 percent who filed for bankruptcy had health insurance.[58] The Medicare and Medicaid programs were estimated to soon account for 50 percent of all national health spending.[59] These factors and many others fueled interest in an overhaul of the health care system in the United States. In 2010 President Obama signed into law the Patient Protection and Affordable Care Act. This Act includes an 'individual mandate' that every American must have medical insurance (or pay a fine). Health policy experts such as David Cutler and Jonathan Gruber, as well as the American medical insurance lobby group America's Health Insurance Plans, argued this provision was required in order to provide "guaranteed issue" and a "community rating," which address unpopular features of America's health insurance system such as premium weightings, exclusions for pre-existing conditions, and the pre-screening of insurance applicants. During March 26–28, the Supreme Court heard arguments regarding the validity of the Act. The Patient Protection and Affordable Care Act was determined to be constitutional on June 28, 2012. SCOTUS determined that Congress had the authority to apply the individual mandate within its taxing powers.[60]		In the late 19th century, "accident insurance" began to be available, which operated much like modern disability insurance.[61][62] This payment model continued until the start of the 20th century in some jurisdictions (like California), where all laws regulating health insurance actually referred to disability insurance.[63]		Accident insurance was first offered in the United States by the Franklin Health Assurance Company of Massachusetts. This firm, founded in 1850, offered insurance against injuries arising from railroad and steamboat accidents. Sixty organizations were offering accident insurance in the U.S. by 1866, but the industry consolidated rapidly soon thereafter. While there were earlier experiments, the origins of sickness coverage in the U.S. effectively date from 1890. The first employer-sponsored group disability policy was issued in 1911.[64]		Before the development of medical expense insurance, patients were expected to pay health care costs out of their own pockets, under what is known as the fee-for-service business model. During the middle-to-late 20th century, traditional disability insurance evolved into modern health insurance programs. One major obstacle to this development was that early forms of comprehensive health insurance were enjoined by courts for violating the traditional ban on corporate practice of the professions by for-profit corporations.[65] State legislatures had to intervene and expressly legalize health insurance as an exception to that traditional rule. Today, most comprehensive private health insurance programs cover the cost of routine, preventive, and emergency health care procedures, and most prescription drugs (but this is not always the case).		Hospital and medical expense policies were introduced during the first half of the 20th century. During the 1920s, individual hospitals began offering services to individuals on a pre-paid basis, eventually leading to the development of Blue Cross organizations.[64] The predecessors of today's Health Maintenance Organizations (HMOs) originated beginning in 1929, through the 1930s and on during World War II.[66][67]		The Employee Retirement Income Security Act of 1974 (ERISA) regulated the operation of a health benefit plan if an employer chooses to establish one, which is not required. The Consolidated Omnibus Budget Reconciliation Act of 1985 (COBRA) gives an ex-employee the right to continue coverage under an employer-sponsored group health benefit plan.		Historically, Health maintenance organizations (HMO) tended to use the term "health plan", while commercial insurance companies used the term "health insurance". A health plan can also refer to a subscription-based medical care arrangement offered through HMOs, preferred provider organizations, or point of service plans. These plans are similar to pre-paid dental, pre-paid legal, and pre-paid vision plans. Pre-paid health plans typically pay for a fixed number of services (for instance, $300 in preventive care, a certain number of days of hospice care or care in a skilled nursing facility, a fixed number of home health visits, a fixed number of spinal manipulation charges, etc.). The services offered are usually at the discretion of a utilization review nurse who is often contracted through the managed care entity providing the subscription health plan. This determination may be made either prior to or after hospital admission (concurrent utilization review).		There are different options available to both employers and employees. There are different types of plans, including health savings accounts and plans with a high or low deductible. The plans that have the high deductibles typically cost the employee less for the monthly premiums, but the part they pay for each time they use their insurance, as well as the overall deductible before the insurance covers anything is much higher. These types of plans are good for the people who rarely go to the doctor and need little health care. The lower deductible plans are typically more expensive, however, they save the employee from having to spend a lot of money out of pocket for services and treatment. The recent trend for employers is to offer the high deductible plans, called consumer-driven healthcare plans, because it costs them less overall for the care their employees need, but it is a lower monthly premium for the employees.[68]		Comprehensive health insurance pays a percentage of the cost of hospital and physician charges after a deductible (usually applies to hospital charges) or a co-pay (usually applies to physician charges, but may apply to some hospital services) is met by the insured. These plans are generally expensive because of the high potential benefit payout — $1,000,000 to $5,000,000 is common — and because of the vast array of covered benefits.[69]		Scheduled health insurance plans are not meant to replace a traditional comprehensive health insurance plans and are more of a basic policy providing access to day-to-day health care such as going to the doctor or getting a prescription drug. In recent years[when?], these plans have taken the name "mini-med plans" or association plans. The term "association" is often used to describe them because they require membership in an association that must exist for some other purpose than to sell insurance. Examples include the Health Care Credit Union Association. These plans may provide benefits for hospitalization and surgical, but these benefits will be limited. Scheduled plans are not meant to be effective for catastrophic events. These plans cost much less than comprehensive health insurance. They generally pay limited benefits amounts directly to the service provider, and payments are based upon the plan's "schedule of benefits". As of 2005[update], "annual benefit maxima for a typical mini scheduled health insurance plan may range from $1,000 to $25,000".[70]		A recent study by PricewaterhouseCoopers examining the drivers of rising health care costs in the U.S. pointed to increased utilization created by increased consumer demand, new treatments, and more intensive diagnostic testing, as the most significant.[71] However, Wendell Potter, a long-time PR representative for the health insurance industry, has noted that the group which sponsored this study, AHIP, is a front-group funded by various insurance companies.[72] People in developed countries are living longer. The population of those countries is aging, and a larger group of senior citizens requires more intensive medical care than a young, healthier population. Advances in medicine and medical technology can also increase the cost of medical treatment. Lifestyle-related factors can increase utilization and therefore insurance prices, such as: increases in obesity caused by insufficient exercise and unhealthy food choices; excessive alcohol use, smoking, and use of street drugs. Other factors noted by the PWC study included the movement to broader-access plans, higher-priced technologies, and cost-shifting from Medicaid and the uninsured to private payers.[71]		Other researchers note that doctors and other healthcare providers are rewarded for merely treating patients rather than curing them and that patients insured through employer group policies have incentives to go to the absolute best HCPs rather than the most cost-effective ones.[73]		The price of health insurance for retired and active duty military personnel has gone up from $19 billion a decade ago to $52 billion in 2012. TRICARE, the government veteran's health insurance program, makes up nine percent of the total budget for the Department of Defense.[74]		In 2007, 87% of Californians had some form of health insurance.[75] Services in California range from private offerings: HMOs, PPOs to public programs: Medi-Cal, Medicare, and Healthy Families (SCHIP).		California developed a solution to assist people across the state and is one of the few states to have an office devoted to giving people tips and resources to get the best care possible. California's Office of the Patient Advocate was established July 2000 to publish a yearly Health Care Quality Report Card[76] on the top HMOs, PPOs, and Medical Groups and to create and distribute helpful tips and resources to give Californians the tools needed to get the best care.[77]		Additionally, California has a Help Center that assists Californians when they have problems with their health insurance. The Help Center is run by the Department of Managed Health Care, the government department that oversees and regulates HMOs and some PPOs.		The state passed healthcare reform in 2006 in order to greater decrease the uninsured rate among its citizens. The federal Patient Protection and Affordable Care Act (colloquially known as "Obamacare") is largely based on Massachusetts' health reform.[78] Due to that colloquialism, the Massachusetts reform has been nicknamed as "Romneycare" after then-Governor Mitt Romney.[79]		As of 2017, Massachusetts has the highest rate of insured citizens in the United States at 97%.[80] [81]		In the USA, insurers will often only make use of health care providers that are independently surveyed by a recognized quality assurance program, such as being accredited by accreditation schemes such as the Joint Commission and the American Accreditation Healthcare Commission.[82]		
Restructuring is the corporate management term for the act of reorganizing the legal, ownership, operational, or other structures of a company for the purpose of making it more profitable, or better organized for its present needs. Other reasons for restructuring include a change of ownership or ownership structure, demerger, or a response to a crisis or major change in the business such as bankruptcy, repositioning, or buyout. Restructuring may also be described as corporate restructuring, debt restructuring and financial restructuring.		Executives involved in restructuring often hire financial and legal advisors to assist in the transaction details and negotiation. It may also be done by a new CEO hired specifically to make the difficult and controversial decisions required to save or reposition the company. It generally involves financing debt, selling portions of the company to investors, and reorganizing or reducing operations.		The basic nature of restructuring is a zero-sum game. Strategic restructuring reduces financial losses, simultaneously reducing tensions between debt and equity holders to facilitate a prompt resolution of a distressed situation.		Corporate debt restructuring is the reorganization of companies’ outstanding liabilities. It is generally a mechanism used by companies which are facing difficulties in repaying their debts. In the process of restructuring, the credit obligations are spread out over longer duration with smaller payments. This allows company’s ability to meet debt obligations. Also, as part of process, some creditors may agree to exchange debt for some portion of equity. It is based on the principle that restructuring facilities available to companies in a timely and transparent matter goes a long way in ensuring their viability which is sometimes threatened by internal and external factors. This process tries to resolve the difficulties faced by the corporate sector and enables them to become viable again.		Steps:						In corporate restructuring, valuations are used as negotiating tools and more than third-party reviews designed for litigation avoidance. This distinction between negotiation and process is a difference between financial restructuring and corporate finance.[1]		Historically, European banks handled non-investment grade lending and capital structures that were fairly straightforward. Nicknamed the "London Approach" in the UK, restructurings focused on avoiding debt write-offs rather than providing distressed companies with an appropriately sized balance sheet. This approach became impractical in the 1990s with private equity increasing demand for highly leveraged capital structures that created the market in high-yield and mezzanine debt. Increased volume of distressed debt drew in hedge funds and credit derivatives deepened the market—trends outside the control of both the regulator and the leading commercial banks.		A company that has been restructured effectively will theoretically be leaner, more efficient, better organized, and better focused on its core business with a revised strategic and financial plan. If the restructured company was a leverage acquisition, the parent company will likely resell it at a profit if the restructuring has proven successful.		
A banishment room (also known as a chasing-out-room and a boredom room) is a modern employee exit management strategy whereby employees are transferred to another department where they are assigned meaningless work until they become disheartened enough to quit.[1][2][3] Since the resignation is voluntary, the employee would not be eligible for certain benefits. The legality and ethicality of the practice is questionable and may be construed as constructive dismissal in some regions.		The practice, which is not officially acknowledged, is common in Japan which has strong labor laws and a tradition of permanent employment.		
A workaholic is a person who works compulsively. While the term generally implies that the person enjoys their work, it can also alternately imply that they simply feel compelled to do it. There is no generally accepted medical definition of such a condition, although some forms of stress, impulse control disorder, obsessive-compulsive personality disorder, and obsessive-compulsive disorder can be work-related.						The word itself is a portmanteau word composed of work and alcoholic. Its first known appearance, according to the Oxford English Dictionary, came in Canada in the Toronto Daily Star of 5 April 1947, page 6, with a punning allusion to Alcoholics Anonymous:		If you are cursed with an unconquerable craving for work, call Workaholics Synonymous, and a reformed worker will aid you back to happy idleness.[1]		The term workaholic refers to various types of behavioral patterns, with each having its own valuation. For instance, workaholism is sometimes used by people wishing to express their devotion to one's career in positive terms. The "work" in question is usually associated with a paying job, but it may also refer to independent pursuits such as sports, music, art and science. However, the term is more often used to refer to a negative behavioral pattern that is popularly characterized by spending an excessive amount of time on working, an inner compulsion to work hard, and a neglect of family and other social relations.		Researchers have found that in many cases, incessant work-related activity continues even after impacting the subject's relationships and physical health. Causes of it are thought to be anxiety, low self-esteem and intimacy problems. Further, workaholics tend to have an inability to delegate work tasks to others and tend to obtain high scores on personality traits such as neuroticism, perfectionism and conscientiousness.		Clinical psychologist Professor Bryan E. Robinson identifies two axes for workaholics: work initiation and work completion. He associates the behavior of procrastination with both "Savoring Workaholics" (those with low work initiation/low work completion) and "Attention-Deficit Workaholics" (those with high work initiation and low work completion), in contrast to "Bulimic" and "Relentless" workaholics - both of whom have high work completion.[2]		Workaholism in Japan is considered a serious social problem leading to early death, often on the job, a phenomenon dubbed karōshi. Overwork was popularly blamed for the fatal stroke of Prime Minister of Japan Keizō Obuchi, in the year 2000.[3] Death from overwork is not a uniquely Japanese phenomenon; in 2013, a Bank of America intern in London died after working for 72 hours straight.[4]		Workaholics feel the urge of being busy all the time, to the point that they often perform tasks that are not required or necessary for project completion. As a result, they tend to be inefficient workers, since they focus on being busy, instead of focusing on being productive. In addition, workaholics tend to be less effective than other workers because they have difficulty working as part of a team, trouble delegating or entrusting co-workers, or organizational problems due to taking on too much work at once.[5] Furthermore, workaholics often suffer sleep deprivation, which results in impaired brain and cognitive function.[6]		
Human trafficking is the trade of humans, most commonly for the purpose of forced labour, sexual slavery, or commercial sexual exploitation for the trafficker or others.[1][2] This may encompass providing a spouse in the context of forced marriage,[3][4][5] or the extraction of organs or tissues,[6][7] including for surrogacy and ova removal.[8] Human trafficking can occur within a country or trans-nationally. Human trafficking is a crime against the person because of the violation of the victim's rights of movement through coercion and because of their commercial exploitation. Human trafficking is the trade in people, especially women and children, and does not necessarily involve the movement of the person from one place to another.		According to the International Labour Organization (ILO), forced labor alone (one component of human trafficking) generates an estimated $150 billion in profits per annum as of 2014.[9] In 2012, the I.L.O. estimated that 21 million victims are trapped in modern-day slavery. Of these, 14.2 million (68%) were exploited for labor, 4.5 million (22%) were sexually exploited, and 2.2 million (10%) were exploited in state-imposed forced labor.[10]		Human trafficking is thought to be one of the fastest-growing activities of trans-national criminal organizations.[11]		Human trafficking is condemned as a violation of human rights by international conventions. In addition, human trafficking is subject to a directive in the European Union.[12]						Although human trafficking can occur at local levels, it has transnational implications, as recognized by the United Nations in the Protocol to Prevent, Suppress and Punish Trafficking in Persons, especially Women and Children (also referred to as the Trafficking Protocol or the Palermo Protocol), an international agreement under the UN Convention against Transnational Organized Crime (CTOC) which entered into force on 25 December 2003. The protocol is one of three which supplement the CTOC.[13] The Trafficking Protocol is the first global, legally binding instrument on trafficking in over half a century, and the only one with an agreed-upon definition of trafficking in persons. One of its purposes is to facilitate international cooperation in investigating and prosecuting such trafficking. Another is to protect and assist human trafficking's victims with full respect for their rights as established in the Universal Declaration of Human Rights. The Trafficking Protocol, which now has 169 parties,[14] defines human trafficking as:		(a) [...] the recruitment, transportation, transfer, harbouring or receipt of persons, by means of threat or use of force or other forms of coercion, of abduction, of fraud, of deception, of the abuse of power or of a position of vulnerability or of the giving or receiving of payments or benefits to achieve the consent of a person having control over another person, for the purpose of exploitation. Exploitation shall include, at a minimum, the exploitation of the prostitution of others or other forms of sexual exploitation, forced labour or services, slavery or practices similar to slavery, servitude or the removal, manipulation or implantation of organs;		(b) The consent of a victim of trafficking in persons to the intended exploitation set forth in sub-paragraph (a) of this article shall be irrelevant where any of the means set forth in subparagraph (a) have been used; (c) The recruitment, transportation, transfer, harbouring or receipt of a child for the purpose of exploitation shall be considered "trafficking in persons" even if this does not involve any of the means set forth in sub-paragraph (a) of this article; (d) "Child" shall mean any person under eighteen years of age.[15]		In 2014, the International Labour Organization estimated $150 billion in annual profit is generated from forced labor alone.[16]		The average cost of a human trafficking victim today is USD $90 whereas the average slave in 1800 America cost the equivalent to USD $40,000.[17]		Human trafficking differs from people smuggling, which involves a person voluntarily requesting or hiring another individual to covertly transport them across an international border, usually because the smuggled person would be denied entry into a country by legal channels. Though illegal, there may be no deception or coercion involved. After entry into the country and arrival at their ultimate destination, the smuggled person is usually free to find their own way. According to the International Centre for Migration Policy Development (ICMPD), people smuggling is a violation of national immigration laws of the destination country, and does not require violations of the rights of the smuggled person. Human trafficking, on the other hand, is a crime against a person because of the violation of the victim's rights through coercion and exploitation.[18] Unlike most cases of people smuggling, victims of human trafficking are not permitted to leave upon arrival at their destination.		While smuggling requires travel, trafficking does not. Trafficked people are held against their will through acts of coercion, and forced to work for or provide services to the trafficker or others. The work or services may include anything from bonded or forced labor to commercial sexual exploitation.[1][2] The arrangement may be structured as a work contract, but with no or low payment, or on terms which are highly exploitative. Sometimes the arrangement is structured as debt bondage, with the victim not being permitted or able to pay off the debt.		Bonded labor, or debt bondage, is probably the least known form of labor trafficking today, and yet it is the most widely used method of enslaving people. Victims become "bonded" when their labor, the labor they themselves hired and the tangible goods they bought are demanded as a means of repayment for a loan or service in which its terms and conditions have not been defined or in which the value of the victims' services is not applied toward the liquidation of the debt. Generally, the value of their work is greater than the original sum of money "borrowed."[19]		Forced labor is a situation in which victims are forced to work against their own will under the threat of violence or some other form of punishment; their freedom is restricted and a degree of ownership is exerted. Men are at risk of being trafficked for unskilled work, which globally generates 31 billion USD according to the International Labour Organization.[20] Forms of forced labor can include domestic servitude, agricultural labor, sweatshop factory labor, janitorial, food service and other service industry labor, and begging.[19] Some of the products that can be produced by forced labor are: clothing, cocoa, bricks, coffee, cotton, and gold.[21]		The International Organization for Migration (IOM), the single largest global provider of services to victims of trafficking, reports receiving an increasing number of cases in which victims were subjected to forced labor. A 2012 study observes that "… 2010 was particularly notable as the first year in which IOM assisted more victims of labor trafficking than those who had been trafficked for purposes of sexual exploitation."[22]		Child labour is a form of work that may be hazardous to the physical, mental, spiritual, moral, or social development of children and can interfere with their education. According to the International Labour Organization, the global number of children involved in child labor has fallen during the past decade – it has declined by one third, from 246 million in 2000 to 168 million children in 2012.[23] Sub-Saharan Africa is the region with the highest incidence of child labour, while the largest numbers of child-workers are found in Asia and the Pacific.[23]		The UN Office on Drugs and Crime (UNODC) has further assisted many non-governmental organizations in their fight against human trafficking. The 2006 armed conflict in Lebanon, which saw 300,000 domestic workers from Sri Lanka, Ethiopia and the Philippines jobless and targets of traffickers, led to an emergency information campaign with NGO Caritas Migrant to raise human-trafficking awareness. Additionally, an April 2006 report, Trafficking in Persons: Global Patterns, helped to identify 127 countries of origin, 98 transit countries and 137 destination countries for human trafficking. To date, it is the second most frequently downloaded UNODC report. Continuing into 2007, UNODC supported initiatives like the Community Vigilance project along the border between India and Nepal, as well as provided subsidy for NGO trafficking prevention campaigns in Bosnia and Herzegovina and Croatia.[25] Public service announcements have also proved useful for organizations combating human trafficking. In addition to many other endeavors, UNODC works to broadcast these announcements on local television and radio stations across the world. By providing regular access to information regarding human-trafficking, individuals are educated how to protect themselves and their families from being exploited.		The United Nations Global Initiative to Fight Human Trafficking (UN.GIFT) was conceived to promote the global fight on human trafficking, on the basis of international agreements reached at the UN. UN.GIFT was launched in March 2007 by UNODC with a grant made on behalf of the United Arab Emirates. It is managed in cooperation with the International Labour Organization (ILO); the International Organization for Migration (IOM); the UN Children's Fund (UNICEF); the Office of the High Commissioner for Human Rights (OHCHR); and the Organization for Security and Co-operation in Europe (OSCE).		Within UN.GIFT, UNODC launched a research exercise to gather primary data on national responses to trafficking in persons worldwide. This exercise resulted in the publication of the Global Report on Trafficking in Persons in February 2009. The report gathers official information for 155 countries and territories in the areas of legal and institutional framework, criminal justice response and victim assistance services.[23] UN.GIFT works with all stakeholders — governments, business, academia, civil society and the media — to support each other's work, create new partnerships, and develop effective tools to fight human trafficking.		The Global Initiative is based on a simple principle: human trafficking is a crime of such magnitude and atrocity that it cannot be dealt with successfully by any government alone. This global problem requires a global, multi-stakeholder strategy that builds on national efforts throughout the world. To pave the way for this strategy, stakeholders must coordinate efforts already underway, increase knowledge and awareness, provide technical assistance, promote effective rights-based responses, build capacity of state and non-state stakeholders, foster partnerships for joint action, and above all, ensure that everybody takes responsibility for this fight. By encouraging and facilitating cooperation and coordination, UN.GIFT aims to create synergies among the anti-trafficking activities of UN agencies, international organizations and other stakeholders to develop the most efficient and cost-effective tools and good practices.[26]		UN.GIFT aims to mobilize state and non-state actors to eradicate human trafficking by reducing both the vulnerability of potential victims and the demand for exploitation in all its forms, ensuring adequate protection and support to those who fall victim, and supporting the efficient prosecution of the criminals involved, while respecting the fundamental human rights of all persons. In carrying out its mission, UN.GIFT will increase the knowledge and awareness on human trafficking, promote effective rights-based responses, build capacity of state and non-state actors, and foster partnerships for joint action against human trafficking. For more information view the UN.GIFT Progress Report 2009.[26][27] UNODC efforts to motivate action launched the Blue Heart Campaign Against Human Trafficking on 6 March 2009,[28] which Mexico launched its own national version of in April 2010.[29][30] The campaign encourages people to show solidarity with human trafficking victims by wearing the blue heart, similar to how wearing the red ribbon promotes transnational HIV/AIDS awareness.[31] On 4 November 2010, U.N. Secretary-General Ban Ki-moon launched the United Nations Voluntary Trust Fund for Victims of Trafficking in Persons to provide humanitarian, legal and financial aid to victims of human trafficking with the aim of increasing the number of those rescued and supported, and broadening the extent of assistance they receive.[32]		In December 2012, UNODC published the new edition of the Global Report on Trafficking in Persons.[33] The Global Report on Trafficking in Persons 2012 has revealed that 27 per cent of all victims of human trafficking officially detected globally between 2007 and 2010 are children, up 7 per cent from the period 2003 to 2006.		The Global Report recorded victims of 136 different nationalities detected in 118 countries between 2007 and 2010, during which period, 460 different flows were identified. Around half of all trafficking took place within the same region with 27 per cent occurring within national borders. One exception is the Middle East, where most detected victims are East and South Asians. Trafficking victims from East Asia have been detected in more than 60 countries, making them the most geographically dispersed group around the world. There are significant regional differences in the detected forms of exploitation. Countries in Africa and in Asia generally intercept more cases of trafficking for forced labour, while sexual exploitation is somewhat more frequently found in Europe and in the Americas. Additionally, trafficking for organ removal was detected in 16 countries around the world.The Report raises concerns about low conviction rates – 16 per cent of reporting countries did not record a single conviction for trafficking in persons between 2007 and 2010. As of November 2015, 169 countries have ratified the United Nations Trafficking in Persons Protocol, of which UNODC is the guardian.[14] Significant progress has been made in terms of legislation: as of 2012, 83 per cent of countries had a law criminalizing trafficking in persons in accordance with the Protocol.[34]		In 2002, Derek Ellerman and Katherine Chon founded a non-government organization called Polaris Project to combat human trafficking. In 2007, Polaris instituted the National Human Trafficking Resource Center (NHTRC) where[35] callers can report tips and receive information on human trafficking.[36] Polaris' website and hotline informs the public about where cases of suspected human trafficking have occurred within the United States. The website records calls on a map.[37]		In 2007, the U.S. Senate designated 11 January as a National Day of Human Trafficking Awareness in an effort to raise consciousness about this global, national and local issue.[38] In 2010, 2011, 2012 and 2013, President Barack Obama proclaimed January as National Slavery and Human Trafficking Prevention Month.[39] Along with these initiatives libraries across the United States are beginning to contribute to human trafficking awareness. Slowly, libraries are turning into educational centers for those who are not aware of this issue. They are collaborating with other organizations to train staff members to spot human trafficking victims and find ways to help them.[40]		In 2014, DARPA funded the Memex program with the explicit goal of combating human trafficking via domain-specific search.[41] The advanced search capacity, including its ability to reach into the dark web has already allowed for prosecution of human trafficking cases.[42]		Because of its size and the access to its large airport, Atlanta, Georgia is known as the core of trafficking in the United States. A 2014 study by Urban Institute showed that some traffickers, or "pimps", in Atlanta grossed over $32,000 in one week.[43]		On 3 May 2005, the Committee of Ministers adopted the Council of Europe Convention on Action against Trafficking in Human Beings (CETS No. 197).[44] The Convention was opened for signature in Warsaw on 16 May 2005 on the occasion of the 3rd Summit of Heads of State and Government of the Council of Europe. On 24 October 2007, the Convention received its tenth ratification thereby triggering the process whereby it entered into force on 1 February 2008. As of June 2017, the Convention has been ratified by 47 states (including Belarus, a non-Council of Europe state), with Russia being the only state to not have ratified (nor signed).[45]		While other international instruments already exist in this field, the Council of Europe Convention, the first European treaty in this field, is a comprehensive treaty focusing mainly on the protection of victims of trafficking and the safeguard of their rights. It also aims to prevent trafficking and to prosecute traffickers. In addition, the Convention provides for the setting up of an effective and independent monitoring mechanism capable of controlling the implementation of the obligations contained in the Convention.		The Convention is not restricted to Council of Europe member states; non-member states and the European Union also have the possibility of becoming Party to the Convention. In 2013 Belarus became the first non-Council of Europe member state to accede to the Convention.[46][47]		The Convention established a Group of Experts on Action against Trafficking in Human Beings (GRETA) which monitors the implementation of the Convention through country reports. As of 1 March 2013, GRETA has published 17 country reports.[48]		Complementary protection against sex trafficking of children is ensured through the Council of Europe Convention on the Protection of Children against Sexual Exploitation and Sexual Abuse (signed in Lanzarote, 25 October 2007). The Convention entered into force on 1 July 2010.[49] As of May 2016, the Convention has been ratified by 41 states, with another 6 states having signed but not yet ratified.[50]		In addition, the European Court of Human Rights of the Council of Europe in Strasbourg has passed judgments concerning trafficking in human beings which violated obligations under the European Convention on Human Rights: Siliadin v. France,[51] judgment of 26 July 2005, and Rantsev v. Cyprus and Russia,[52] judgment of 7 January 2010.		In 2003, the OSCE established an anti-trafficking mechanism aimed at raising public awareness of the problem and building the political will within participating states to tackle it effectively.		The OSCE actions against human trafficking are coordinated by the Office of the Special Representative for Combating the Traffic of Human Beings.[53] In January 2010, Maria Grazia Giammarinaro became the OSCE Special Representative and Co-ordinator for Combating Trafficking in Human Beings. Dr. Giammarinaro (Italy) has been a judge at the Criminal Court of Rome since 1991. She served from 2006 until 2009 in the European Commission's Directorate-General for Justice, Freedom and Security in Brussels, where she was responsible for work to combat human trafficking and sexual exploitation of children, as well as for penal aspects of illegal immigration within the unit dealing with the fight against organized crime. During this time, she co-ordinated the Group of Experts on Trafficking in Human Beings of the European Commission. From 2001 to 2006 she was a judge for preliminary investigation in the Criminal Court of Rome. Prior to that, from 1996 she was Head of the Legislative Office and Adviser to the Minister for Equal Opportunities. From 2006 to December 2009 the office was headed by Eva Biaudet, a former Member of Parliament and Minister of Health and Social Services in her native Finland.[citation needed]		The activities of the Office of the Special Representative range from training law enforcement agencies to tackle human trafficking to promoting policies aimed at rooting out corruption and organised crime. The Special Representative also visits countries and can, on their request, support the formation and implementation of their anti-trafficking policies. In other cases the Special Representative provides advice regarding implementation of the decisions on human trafficking, and assists governments, ministers and officials to achieve their stated goals of tackling human trafficking.[citation needed]		In India, the trafficking in persons for commercial sexual exploitation, forced labor, forced marriages and domestic servitude is considered an organized crime. The Government of India applies the Criminal Law (Amendment) Act 2013, active from 3 February 2013, as well as Section 370 and 370A IPC, which defines human trafficking and "provides stringent punishment for human trafficking; trafficking of children for exploitation in any form including physical exploitation; or any form of sexual exploitation, slavery, servitude or the forced removal of organs." Additionally, a Regional Task Force implements the SAARC Convention on the prevention of Trafficking in Women and Children.[54]		Shri R.P.N.Singh, India's Minister of State for Home Affairs, launched a government web portal, the Anti Human Trafficking Portal, on 20 February 2014. The official statement explained that the objective of the on-line resource is for the "sharing of information across all stakeholders, States/UTs[Union Territories] and civil society organizations for effective implementation of Anti Human Trafficking measures."[54] The key aims of the portal are:		Also on 20 February, the Indian government announced the implementation of a Comprehensive Scheme that involves the establishment of Integrated Anti Human Trafficking Units (AHTUs) in 335 vulnerable police districts throughout India, as well as capacity building that includes training for police, prosecutors and judiciary. As of the announcement, 225 Integrated AHTUs had been made operational, while 100 more AHTUs were proposed for the forthcoming financial year.[54]		The '3P Anti-trafficking Policy Index' measures the effectiveness of government policies to fight human trafficking based on an evaluation of policy requirements prescribed by the United Nations Protocol to Prevent, Suppress and Punish Trafficking in Persons, especially Women and Children (2000).[55]		The policy level is evaluated using a five-point scale, where a score of five indicates the best policy practice, while score 1 is the worst. This scale is used to analyze the main three anti-trafficking policy areas: (i) prosecuting (criminalizing) traffickers, (ii) protecting victims, and (iii) preventing the crime of human trafficking. Each sub-index of prosecution, protection and prevention is aggregated to the overall index with an unweighted sum, with the overall index ranging from a score of 3 (worst) to 15 (best). It is available for up to 177 countries over the 2000-2009 period (on an annual basis).		The outcome of the Index shows that anti-trafficking policy has overall improved over the 2000-2009 period. Improvement is most prevalent in the prosecution and prevention areas worldwide. An exception is protection policy,[56] which shows a modest deterioration in recent years.		In 2009 (the most recent year of the evaluation), seven countries demonstrate the highest possible performance in policies for all three dimensions (overall score 15). These countries are Germany, Australia, the Netherlands, Italy, Belgium, Sweden and the US. The second best performing group (overall score 14) consists of France, Norway, South Korea, Croatia, Canada, Austria, Slovenia and Nigeria. The worst performing country in 2009 was North Korea, receiving the lowest score in all dimensions (overall score 3), followed by Somalia. For more information view the Human Trafficking Research and Measurement website.[57]		In 2014, for the first time in history major leaders of many religions, Buddhist, Anglican, Catholic, and Orthodox Christian, Hindu, Jewish, and Muslim, met to sign a shared commitment against modern-day slavery; the declaration they signed calls for the elimination of slavery and human trafficking by the year 2020.[58] The signatories were: Pope Francis, Mātā Amṛtānandamayī (also known as Amma), Bhikkhuni Thich Nu Chân Không (representing Zen Master Thích Nhất Hạnh), Datuk K Sri Dhammaratana, Chief High Priest of Malaysia, Rabbi Abraham Skorka, Rabbi David Rosen, Abbas Abdalla Abbas Soliman, Undersecretary of State of Al Azhar Alsharif (representing Mohamed Ahmed El-Tayeb, Grand Imam of Al-Azhar), Grand Ayatollah Mohammad Taqi al-Modarresi, Sheikh Naziyah Razzaq Jaafar, Special advisor of Grand Ayatollah (representing Grand Ayatollah Sheikh Basheer Hussain al Najafi), Sheikh Omar Abboud, Justin Welby, Archbishop of Canterbury, and Metropolitan Emmanuel of France (representing Ecumenical Patriarch Bartholomew.)[58]		One of the organizations taking the most active part in the anti-trafficking is the United Nations. In early 2016 the Permanent Mission of the Republic of Kazakhstan to the United Nations held an interactive discussion entitled "Responding to Current Challenges in Trafficking in Human Beings".[59]		One of the current efforts being done to combat human trafficking is an app called TraffickCam.[60] This app was created by the Exchange Initiative[61] and researchers at Washington University. TraffckCam was launched on June 20, 2016 and enables anyone to take photos of their hotel rooms, which then gets uploaded to a large database of hotel images. Since human trafficking victims are often found in hotel rooms for online advertisements, law enforcement and investigators can use these photos to help find and prosecute traffickers.[62]		Anti-trafficking awareness and fundraising campaigns constitute a significant portion of anti-trafficking initiatives.[63] The 24 Hour Race is one such initiative that focuses on increasing awareness among high school students in Asia.[64] The Blue Campaign is another anti-trafficking initiative that works with the U.S. Department of Homeland Security to combat human trafficking and bring freedom to exploited victims.[65]		The Blue Campaign collaborates with law enforcement, government, non-governmental, and private organizations to end human trafficking and protect victims.		[66]		Trafficking in Persons Report released in June 2016 states that "refugees and migrants; lesbian, gay, bisexual, transgender, and intersex (LGBTI) individuals; religious minorities; people with disabilities; and those who are stateless" are the most at-risk for human trafficking. Governments best protect victims from being exploited when the needs of vulnerable populations are understood.[67]		Trafficking of children involves the recruitment, transportation, transfer, harboring, or receipt of children for the purpose of exploitation. The commercial sexual exploitation of children can take many forms, including forcing a child into prostitution[68][69] or other forms of sexual activity or child pornography. Child exploitation may also involve forced labour or services, slavery or practices similar to slavery, servitude, the removal of organs,[70] illicit international adoption, trafficking for early marriage, recruitment as child soldiers, for use in begging or as athletes (such as child camel jockeys[71] or football players[72]).		IOM statistics indicate that a significant minority (35%) of trafficked persons it assisted in 2011 were less than 18 years of age, which is roughly consistent with estimates from previous years. It was reported in 2010 that Thailand and Brazil were considered to have the worst child sex trafficking records.[73]		Traffickers in children may take advantage of the parents' extreme poverty. Parents may sell children to traffickers in order to pay off debts or gain income, or they may be deceived concerning the prospects of training and a better life for their children. They may sell their children into labor, sex trafficking, or illegal adoptions.		The adoption process, legal and illegal, when abused can sometimes result in cases of trafficking of babies and pregnant women from developing countries to the West.[74] In David M. Smolin's papers on child trafficking and adoption scandals between India and the United States,[75][76] he presents the systemic vulnerabilities in the inter-country adoption system that makes adoption scandals predictable.		The United Nations Convention on the Rights of the Child at Article 34, states, "States Parties undertake to protect the child from all forms of sexual exploitation and sexual abuse".[77] In the European Union, commercial sexual exploitation of children is subject to a directive – Directive 2011/92/EU of the European Parliament and of the Council of 13 December 2011 on combating the sexual abuse and sexual exploitation of children and child pornography.[78]		The Hague Convention on Protection of Children and Co-operation in Respect of Intercountry Adoption (or Hague Adoption Convention) is an international convention dealing with international adoption, that aims at preventing child laundering, child trafficking, and other abuses related to international adoption.[79]		The Optional Protocol on the Involvement of Children in Armed Conflict seeks to prevent forceful recruitment (e.g. by guerrilla forces) of children for use in armed conflicts.[80]		Sex trafficking affects 4.5 million people worldwide.[81] Most victims find themselves in coercive or abusive situations from which escape is both difficult and dangerous.[82]		Trafficking for sexual exploitation was formerly thought of as the organized movement of people, usually women, between countries and within countries for sex work with the use of physical coercion, deception and bondage through forced debt. However, the Trafficking Victims Protection Act of 2000 (US),[83] does not require movement for the offence. The issue becomes contentious when the element of coercion is removed from the definition to incorporate facilitation of consensual involvement in prostitution. For example, in the United Kingdom, the Sexual Offences Act 2003 incorporated trafficking for sexual exploitation but did not require those committing the offence to use coercion, deception or force, so that it also includes any person who enters the UK to carry out sex work with consent as having been "trafficked."[84] In addition, any minor involved in a commercial sex act in the US while under the age of 18 qualifies as a trafficking victim, even if no force, fraud or coercion is involved, under the definition of "Severe Forms of Trafficking in Persons" in the US Trafficking Victims Protection Act of 2000.[83]		Sexual trafficking includes coercing a migrant into a sexual act as a condition of allowing or arranging the migration. Sexual trafficking uses physical or sexual coercion, deception, abuse of power and bondage incurred through forced debt. Trafficked women and children, for instance, are often promised work in the domestic or service industry, but instead are sometimes taken to brothels where they are required to undertake sex work, while their passports and other identification papers confiscated. They may be beaten or locked up and promised their freedom only after earning – through prostitution – their purchase price, as well as their travel and visa costs.[85][86]		A forced marriage is a marriage where one or both participants are married without their freely given consent.[87] Servile marriage is defined as a marriage involving a person being sold, transferred or inherited into that marriage.[88] According to ECPAT, "Child trafficking for forced marriage is simply another manifestation of trafficking and is not restricted to particular nationalities or countries".[3]		A forced marriage qualifies as a form of human trafficking in certain situations. If a woman is sent abroad, forced into the marriage and then repeatedly compelled to engage in sexual conduct with her new husband, then her experience is that of sex trafficking. If the bride is treated as a domestic servant by her new husband and/or his family, then this is a form of labor trafficking.[89]		Labour trafficking is the movement of persons for the purpose of forced labor and services.[90] It may involve bonded labor, involuntary servitude, domestic servitude, and child labor.[90] Labor trafficking happens most often within the domain of domestic work, agriculture, construction, manufacturing and entertainment; and migrant workers and indigenous people are especially at risk of becoming victims.[81]		Trafficking in organs is a form of human trafficking. It can take different forms. In some cases, the victim is compelled into giving up an organ. In other cases, the victim agrees to sell an organ in exchange of money/goods, but is not paid (or paid less). Finally, the victim may have the organ removed without the victim's knowledge (usually when the victim is treated for another medical problem/illness – real or orchestrated problem/illness). Migrant workers, homeless persons, and illiterate persons are particularly vulnerable to this form of exploitation. Trafficking of organs is an organized crime, involving several offenders:[91]		Trafficking for organ trade often seeks kidneys. Trafficking in organs is a lucrative trade because in many countries the waiting lists for patients who need transplants are very long.[92]		There are many different estimates of how large the human trafficking and sex trafficking industries are. According to scholar Kevin Bales, author of Disposable People (2004), estimates that as many as 27 million people are in "modern-day slavery" across the globe.[93][94] In 2008, the U.S. Department of State estimates that 2 million children are exploited by the global commercial sex trade.[95] In the same year, a study classified 12.3 million individuals worldwide as "forced laborers, bonded laborers or sex-trafficking victims." Approximately 1.39 million of these individuals worked as commercial sex slaves, with women and girls comprising 98%, of the 1.36 million.[96]		The enactment of the Victims of Trafficking and Violence Protection Act (TVPA) in 2000 by the United States Congress and its subsequent re-authorizations established the Department of State's Office to Monitor and Combat Trafficking in Persons, which engages with foreign governments to fight human trafficking and publishes a Trafficking in Persons Report annually. The Trafficking in Persons Report evaluates each country's progress in anti-trafficking and places each country onto one of three tiers based on their governments' efforts to comply with the minimum standards for the elimination of trafficking as prescribed by the TVPA.[97] However, questions have been raised by critical anti-trafficking scholars about the basis of this tier system, its heavy focus on compliance with state department protocols, and its failure to consider "risk" and the likely prevalence of trafficking when rating the efforts of diverse countries.[98]		In particular, there were three main components of the TVPA, commonly called the three P's:		PROTECTION: The TVPA increased the US Government's efforts to protect trafficked foreign national victims including, but not limited to: Victims of trafficking, many of whom were previously ineligible for government assistance, were provided assistance; and a non-immigrant status for victims of trafficking if they cooperated in the investigation and prosecution of traffickers (T-Visas, as well as providing other mechanisms to ensure the continued presence of victims to assist in such investigations and prosecutions).		PROSECUTION: The TVPA authorized the US Government to strengthen efforts to prosecute traffickers including, but not limited to: Creating a series of new crimes on trafficking, forced labor, and document servitude that supplemented existing limited crimes related to slavery and involuntary servitude; and recognizing that modern-day slavery takes place in the context of fraud and coercion, as well as force, and is based on new clear definitions for both trafficking into sexual exploitation and labor exploitation: Sex trafficking was defined as, "a commercial sex act that is induced by force, fraud, or coercion, or in which the person induced to perform such an act has not attained 18 years of age". Labor trafficking was defined as, "the recruitment, harboring, transportation, provision, or obtaining of a person for labor or services, through the use of force, fraud, or coercion for the purpose of subjection to involuntary servitude, peonage, debt bondage, or slavery".		PREVENTION: The TVPA allowed for increased prevention measures including: Authorizing the US Government to assist foreign countries with their efforts to combat trafficking, as well as address trafficking within the United States, including through research and awareness-raising; and providing foreign countries with assistance in drafting laws to prosecute trafficking, creating programs for trafficking victims, and assistance with implementing effective means of investigation.[99]		Secretary of State Hillary Rodham Clinton later identified a fourth P, "partnership", in 2009 to serve as a, "pathway to progress in the effort against modern-day slavery."		A complex set of factors fuel sex trafficking, including poverty, unemployment, social norms that discriminate against women, commercial demand for sex, institutional challenges, and globalization.		Poverty and lack of educational and economic opportunities in one's hometown may lead women to voluntarily migrate and then be involuntarily trafficked into sex work.[101][102] As globalization opened up national borders to greater exchange of goods and capital, labor migration also increased. Less wealthy countries have fewer options for livable wages. The economic impact of globalization pushes people to make conscious decisions to migrate and be vulnerable to trafficking. Gender inequalities that hinder women from participating in the formal sector also push women into informal sectors.[103]		Long waiting lists for organs in the United States and Europe created a thriving international black market. Traffickers harvest organs, particularly kidneys, to sell for large profit and often without properly caring for or compensating the victims. Victims often come from poor, rural communities and see few other options than to sell organs illegally.[104] Wealthy countries' inability to meet organ demand within their own borders perpetuates trafficking. By reforming their internal donation system, Iran achieved a surplus of legal donors and provides an instructive model for eliminating both organ trafficking and -shortage.[105]		Globalization and the rise of Internet technology has also facilitated sex trafficking. Online classified sites and social networks such as Craigslist have been under intense scrutiny for being used by johns and traffickers in facilitating sex trafficking and sex work in general. Traffickers use explicit sites and underground sites (e.g. Craigslist, Backpage, MySpace) to market, recruit, sell, and exploit females. Facebook, Twitter, and other social networking sites are suspected for similar uses. For example, Randal G. Jennings was convicted of sex trafficking five underage girls by forcing them to advertise on Craigslist and driving them to meet the customers.[citation needed] According to the National Center for Missing and Exploited Children, online classified ads reduce the risks of finding prospective customers.[106] Studies have identified the Internet as the single biggest facilitator of commercial sex trade, although it is difficult to ascertain which women advertised are sex trafficking victims.[107] Traffickers and pimps use the Internet to recruit minors, since Internet and social networking sites usage have significantly increased especially among children.[108]		Organized criminals can generate up to several thousand dollars per day from one trafficked girl, and the Internet has further increased profitability of sex trafficking and child trafficking. With faster access to a wider clientele, more sexual encounters can be scheduled.[109] Victims and clients, according to a New York City report on sex trafficking in minors, increasingly use the Internet to meet customers. Because of protests, Craigslist has since closed its adult services section. According to authorities, Backpage is now the main source for advertising trafficking victims.[110] Investigators also frequently browse online classified ads to identify potential underage girls who are trafficked.		While globalization fostered new technologies that may exacerbate sex trafficking, technology can also be used to assist law enforcement and anti-trafficking efforts. A study was done on online classified ads surrounding the Super Bowl. A number of reports have noticed increase in sex trafficking during previous years of the Super Bowl.[111] For the 2011 Super Bowl held in Dallas, Texas, the Backpage for Dallas area experienced a 136% increase on the number of posts in the Adult section on Super Bowl Sunday, where as Sundays typically have the lowest amount of posts. Researchers analyzed the most salient terms in these online ads, which suggested that many escorts were traveling across state lines to Dallas specifically for the Super Bowl, and found that the self-reported ages were higher than usual. Twitter was another social networking platform studied for detecting sex trafficking. Digital tools can be used to narrow the pool of sex trafficking cases, albeit imperfectly and with uncertainty.[112]		However, there has been no evidence found actually linking the Super Bowl – or any other sporting event – to increased trafficking or prostitution.[113][114]		Corrupt and inadequately trained police officers can be complicit in sex trafficking and/or commit violence against sex workers, including sex trafficked victims.[115]		Anti-trafficking agendas from different groups can also be in conflict. In the movement for sex workers rights, sex workers establish unions and organizations, which seek to eliminate trafficking themselves. However, law enforcement also seek to eliminate trafficking and to prosecute trafficking, and their work may infringe on sex workers' rights and agency. For example, the sex workers union DMSC (Durbar Mahila Samanwaya Committee) in Kolkata, India, has "self-regulatory boards" (SRBs) that patrol the red light districts and assist girls who are underage or trafficked. The union opposes police intervention and interferes with police efforts to bring minor girls out of brothels, on the grounds that police action might have an adverse impact on non-trafficked sex workers, especially because police officers in many places are corrupt and violent in their operations.[115] Critics argue that since sex trafficking is an economic and violent crime, it calls for law enforcement to intervene and prevent violence against victims.		Criminalization of sex work also may foster the underground market for sex work and enable sex trafficking.[101]		Difficult political situations such as civil war and social conflict are push factors for migration and trafficking. A study reported that larger countries, the richest and the poorest countries, and countries with restricted press freedom are likely to engage in more sex trafficking. Specifically, being in a transitional economy made a country nineteen times more likely to be ranked in the highest trafficking category, and gender inequalities in a country's labor market also correlated with higher trafficking rates.[116]		An annual US State Department report in June 2013 cited Russia and China as among the worst offenders in combatting forced labour and sex trafficking, raising the possibility of US sanctions being leveraged against these countries.[117] In 1997 alone as many as 175,000 young women from Russia, as well as the former Soviet Union, were sold as commodities in the sex markets of the developed countries in Europe and the Americas.[118]		In 2013, the Supreme Court of Canada declared the laws which effectively prohibited prostitution illegal. It delayed the implementation of this ruling for one year to give the parliament time to enact replacement laws, if it so desired.[119]		Abolitionists who seek an end to sex trafficking explain the nature of sex trafficking as an economic supply and demand model. In this model, male demand for prostitutes leads to a market of sex work, which, in turn, fosters sex trafficking, the illegal trade and coercion of people into sex work, and pimps and traffickers become 'distributors' who supply people to be sexually exploited. The demand for sex trafficking can also be facilitated by some pimps' and traffickers' desire for women whom they can exploit as workers because they do not require wages, safe working circumstances, and agency in choosing customers.[101]		Traffickers use physical, emotional, and psychological abuse to control and exploit their victims.		[120]		Sex trafficking victims face threats of violence from many sources, including customers, pimps, brothel owners, madams, traffickers, and corrupt local law enforcement officials. Raids as an anti-sex trafficking measure have to potential to help, and also to harm sex trafficked victims. Because of their potentially complicated legal status and their potential language barriers, the arrest or fear of arrest creates stress and other emotional trauma for trafficking victims. Victims may also experience physical violence from law enforcement during raids.[121][122] The challenges facing victims often continue of course, after their experience of "rescue" or removal from coercive sexual exploitation. In addition to coping with their past traumatic experiences, former trafficking victims often experience social alienation in the host and home countries. Stigmatization, social exclusion, and intolerance often make it difficult for former victims to integrate into their host community, or to reintegrate into their former community. Accordingly, one of the central aims of protection assistance, is the promotion of (re)integration.[123][56] Too often however, governments and large institutional donors offer little funding to support the provision of assistance and social services to former trafficking victims. As the victims are also pushed into drug trafficking, many of them face criminal sanctions also.[124]		The use of coercion by perpetrators and traffickers involves the use of extreme control. Perpetrators expose the victim to high amounts of psychological stress induced by threats, fear, and physical and emotional violence. Tactics of coercion are reportedly used in three phases of trafficking: recruitment, initiation, and indoctrination.[125] During the initiation phase, traffickers use foot-in-the-door techniques of persuasion to lead their victims into various trafficking industries. This manipulation creates an environment where the victim becomes completely dependent upon the authority of the trafficker.[125] Traffickers take advantage of family dysfunction, homelessness, and history of childhood abuse to psychologically manipulate women and children into the trafficking industry.[126]		One form of psychological coercion particularly common in cases of sex trafficking and forced prostitution is Stockholm syndrome. Many women entering into the sex trafficking industry are minors whom have already experienced prior sexual abuse.[127] Traffickers take advantage of young girls by luring them into the business through force and coercion, but more often through false promises of love, security, and protection. This form of coercion works to recruit and initiate the victim into the life of a sex worker, while also reinforcing a "trauma bond", also known as Stockholm syndrome. Stockholm syndrome is a psychological response where the victim becomes attached to his or her perpetrator.[127][128]		The goal of a trafficker is to turn a human being into a slave. To do this, perpetrators employ tactics that can lead to the psychological consequence of learned helplessness for the victims, where they sense that they no longer have any autonomy or control over their lives.[126] Traffickers may hold their victims captive, expose them to large amounts of alcohol or use drugs, keep them in isolation, or withhold food or sleep.[126] During this time the victim often begins to feel the onset of depression, guilt and self-blame, anger and rage, and sleep disturbances, PTSD, numbing, and extreme stress. Under these pressures, the victim can fall into the hopeless mental state of learned helplessness.[125][129][130]		For victims of specifically trafficked for the purpose of forced prostitution and sexual slavery, initiation into the trade is almost always characterized by violence.[126] Traffickers hunt down their victims and employ practices of sexual abuse, torture, brainwashing, repeated rape and physical assault until the victim submits to his or her fate as a sexual slave. Victims experience verbal threats, social isolation, and intimidation before they accept their role as a prostitute.[131]		For those enslaved in situations of forced labor, learned helplessness can also manifest itself through the trauma of living as a slave. Reports indicate that captivity for the person and financial gain of their owners adds additional psychological trauma. Victims are often cut off from all forms of social connection, as isolation allows the perpetrator to destroy the victim's sense of self and increase his or her dependence on the perpetrator.[125]		Human trafficking victims may experience complex trauma as a result of repeated cases of intimate relationship trauma over long periods of time including, but not limited to, sexual abuse, domestic violence, forced prostitution, or gang rape. Complex trauma involves multifaceted conditions of depression, anxiety, self-hatred, dissociation, substance abuse, self-destructive behaviors, medical and somatic concerns, despair, and revictimization. Psychology researchers report that, although similar to posttraumatic stress disorder (PTSD), Complex trauma is more expansive in diagnosis because of the effects of prolonged trauma.[132]		Victims of sex trafficking often get "branded"[133] by their traffickers or pimps. These tattoos usually consist of bar codes or the trafficker's name or rules. Even if a victim escapes their trafficker's control or gets rescued, these tattoos are painful reminders of their past and results in emotional distress. To get these tattoos removed or covered-up can cost hundreds of dollars.[134]		Psychological reviews have shown that the chronic stress experienced by many victims of human trafficking can compromise the immune system.[126] Several studies found that chronic stressors (like trauma or loss) suppressed cellular and humoral immunity.[129] Victims may develop STDs and HIV/AIDS.[135] Perpetrators frequently use substance abuse as a means to control their victims, which leads to compromised health, self-destructive behavior, and long-term physical harm.[136] Furthermore, victims have reported treatment similar to torture, where their bodies are broken and beaten into submission.[136][137]		Children are especially vulnerable to these developmental and psychological consequences of trafficking due to their age. In order to gain complete control of the child, traffickers often destroy physical and mental health of the children through persistent physical and emotional abuse.[138] Victims experience severe trauma on a daily basis that devastates the healthy development of self-concept, self-worth, biological integrity, and cognitive functioning.[139] Children who grow up in constant environments of exploitation frequently exhibit antisocial behavior, over-sexualized behavior, self-harm, aggression, distrust of adults, dissociative disorders, substance abuse, complex trauma, and attention deficit disorders.[128][138][139][140] Stockholm syndrome is also a common problem for girls while they are trafficked, which can hinder them from both trying to escape, and moving forward in psychological recovery programs.[137]		Although 98% of the sex trade is composed of women and girls[137] there is an effort to gather empirical evidence about the psychological impact of abuse common in sex trafficking upon young boys.[139][141] Boys often will experience forms of post-traumatic stress disorder, but also additional stressors of social stigma of homosexuality associated with sexual abuse for boys, and externalization of blame, increased anger, and desire for revenge.		Sex trafficking increases the risk of contracting HIV/AIDS.[143] The HIV/AIDS pandemic can be both a cause and a consequence of sex trafficking. On one hand, child-prostitutes are sought by customers because they are perceived as being less likely to be HIV positive, and this demand leads to child sex trafficking. On the other hand, trafficking leads to the proliferation of HIV, because victims, being vulnerable and often young/inexperienced, cannot protect themselves properly, and get infected.[144]		According to estimates from the International Labour Organization (ILO), every year the human trafficking industry generates 32 billion USD, half of which ($15.5 billion) is made in industrialized countries, and a third of which ($9.7 billion) is made in Asia.[145] A 2011 paper published in Human Rights Review, "Sex Trafficking: Trends, Challenges and Limitations of International Law", notes that, since 2000, the number of sex-trafficking victims has risen while costs associated with trafficking have declined: "Coupled with the fact that trafficked sex slaves are the single most profitable type of slave, costing on average $1,895 each but generating $29,210 annually, [there are] stark predictions about the likely growth in commercial sex slavery in the future."[96] Sex trafficking victims rarely get a share of the money that they make through coerced sex work, which further keeps them oppressed.[146]		Miniseries about an Immigration and Customs Enforcement agent who are determined to stop the trafficking of women and children.		[147]		Both the public debate on human trafficking and the actions undertaken by the anti-human traffickers have been criticized by Zbigniew Dumienski, a former research analyst at the S. Rajaratnam School of International Studies.[148] The criticism touches upon statistics and data on human trafficking, the concept itself, and anti-trafficking measures.		According to a former Wall Street Journal columnist, figures used in human trafficking estimates rarely have identifiable sources or transparent methodologies behind them and in most (if not all) instances, they are mere guesses.[149][150] Dumienski and Laura Agustin argue that this is a result of the fact that it is impossible to produce reliable statistics on a phenomenon happening in the shadow economy.[148][151] According to a UNESCO Bangkok researcher, statistics on human trafficking may be unreliable due to overrepresentation of sex trafficking. As an example, he cites flaws in Thai statistics, who discount men from their official numbers because by law they cannot be considered trafficking victims due to their gender.[152]		A 2012 article in the International Communication Gazette examined the effect of two communication theories (agenda-building and agenda-setting) on media coverage on human trafficking in the United States and Britain. The article analyzed four newspapers including the Guardian and the Washington Post and categorized the content into various categories. Overall, the article found that sex trafficking was the most reported form of human trafficking by the newspapers that were analyzed (p. 154). Many of the other stories on trafficking were non-specific.[153]		According to Zbigniew Dumienski, the very concept of human trafficking is murky and misleading.[148] It has been argued that while human trafficking is commonly seen as a monolithic crime, in reality it is an act of illegal migration that involves various different actions: some of them may be criminal or abusive, but others often involve consent and are legal.[148] Laura Agustin argues that not everything that might seem abusive or coercive is considered as such by the migrant. For instance, she states that: 'would-be travellers commonly seek help from intermediaries who sell information, services and documents. When travellers cannot afford to buy these outright, they go into debt'.[151] Dumienski says that while these debts might indeed be on very harsh conditions, they are usually incurred on a voluntary basis.[148]		The critics of the current approaches to trafficking say that a lot of the violence and exploitation faced by illegal migrants derives precisely from the fact that their migration and their work are illegal and not primarily because of trafficking.[154] Tara McCormack, a lecturer in international relations at Brunel University, has opined in that trafficking discourse can be detrimental to the interests of migrants as it denies them agency and as it depoliticizes debates on migration.[155]		The international Save the Children organization also stated: "…The issue, however, gets mired in controversy and confusion when prostitution too is considered as a violation of the basic human rights of both adult women and minors, and equal to sexual exploitation per se…trafficking and prostitution become conflated with each other…On account of the historical conflation of trafficking and prostitution both legally and in popular understanding, an overwhelming degree of effort and interventions of anti-trafficking groups are concentrated on trafficking into prostitution."[156]		Claudia Aradau of Open University claims that NGOs involved in anti-sex trafficking often employ "politics of pity," which promotes that all trafficked victims are completely guiltless, fully coerced into sex work, and experience the same degrees of physical suffering. One critic identifies two strategies that gain pity: denunciation – attributing all violence and suffering to the perpetrator – and sentiment – exclusively depicting the suffering of the women. NGOs' use of images of unidentifiable females suffering physically help display sex trafficking scenarios as all the same. She points out that not all trafficking victims have been abducted, abused physically, and repeatedly raped, unlike popular portrayals.[157] A study of the relationships between individuals who are defined as sex-trafficking victims by virtue of having a procurer (especially minors) concluded that assumptions about victimization and human trafficking do not do justice to the complex and often mutual relationships that exist between sex workers and their third parties.[158]		Groups like Amnesty International have been critical of insufficient or ineffective government measures to tackle human trafficking. Criticism includes a lack of understanding of human trafficking issues, poor identification of victims and a lack of resources for the key pillars of anti-trafficking – identification, protection, prosecution and prevention. For example, Amnesty International has called the UK government's new anti-trafficking measures "not fit for purpose."[159]		In the UK, human trafficking cases are processed by the same officials to simultaneously determine the refugee and trafficking victim statuses of a person. However, criteria for qualifying as a refugee and a trafficking victim differ and they have different needs for staying in a country. A person may need assistance as a trafficking victim but his/her circumstances may not necessarily meet the threshold for asylum. In which case, not being granted refugee status affects their status as a trafficked victim and thus their ability to receive help. Reviews of the statistics from the National Referral Mechanism (NRM), a tool created by the Council of Europe Convention on Action against Trafficking in Human Beings (CoE Convention) to help states effectively identify and care for trafficking victims, found that positive decisions for non-European Union citizens were much lower than that of EU and UK citizens. According to data on the NRM decisions from April 2009 to April 2011, an average of 82.8% of UK and EU citizens were conclusively accepted as victims while an average of only 45.9% of non-EU citizens were granted the same status.[160] High refusal rates of non-EU people point to possible stereotypes and biases about regions and countries of origin which may hinder anti-trafficking efforts, since the asylum system is linked to the trafficking victim protection system.		Laura Agustin has suggested that, in some cases, "anti-traffickers" ascribe victim status to immigrants who have made conscious and rational decisions to cross the borders knowing they will be selling sex and who do not consider themselves to be victims.[161] There have been instances in which the alleged victims of trafficking have actually refused to be rescued[162] or run away from the anti-trafficking shelters.[163]		In a 2013 lawsuit,[164] the Court of Appeal gave guidance to prosecuting authorities on the prosecution of victims of human trafficking, and held that the convictions of three Vietnamese children and one Ugandan woman ought to be quashed as the proceedings amounted to an abuse of the court's process.[165] The case was reported by the BBC[166] and one of the victims was interviewed by Channel 4.[167]		In the U.S., services and protections for trafficked victims are related to cooperation with law enforcement. Legal procedures that involve prosecution and specifically, raids, are thus the most common anti-trafficking measures. Raids are conducted by law enforcement and by private actors and many organizations (sometimes in cooperation with law enforcement). Law enforcement perceive some benefits from raids, including the ability to locate and identify witnesses for legal processes, to dismantle "criminal networks", and to rescue victims from abuse.[121]		The problems against anti-trafficking raids are related to the problem of the trafficking concept itself, as raids' purpose of fighting sex trafficking may be conflated with fighting prostitution. The Trafficking Victims Protection Re-authorization Act of 2005 (TVPRA) gives state and local law enforcement funding to prosecute customers of commercial sex, therefore some law enforcement agencies make no distinction between prostitution and sex trafficking. One study interviewed women who have experienced law enforcement operations as sex workers and found that during these raids meant to combat human trafficking, none of the women were ever identified as trafficking victims, and only one woman was asked whether she was coerced into sex work. The conflation of trafficking with prostitution, then, does not serve to adequately identify trafficking and help the victims. Raids are also problematic in that the women involved were most likely unclear about who was conducting the raid, what the purpose of the raid was, and what the outcomes of the raid would be.[121]		Law enforcement personnel agree that raids can intimidate trafficked persons and render subsequent law enforcement actions unsuccessful. Social workers and attorneys involved in anti-sex trafficking have negative opinions about raids. Service providers report a lack of uniform procedure for identifying trafficking victims after raids. The 26 interviewed service providers stated that local police never referred trafficked persons to them after raids. Law enforcement also often use interrogation methods that intimidate rather than assist potential trafficking victims. Additionally, sex workers sometimes face violence from the police during raids and arrests and in rehabilitation centers.[121]		As raids occur to brothels that may house sex workers as well as sex trafficked victims, raids affect sex workers in general. As clients avoid brothel areas that are raided but do not stop paying for sex, voluntary sex workers will have to interact with customers underground. Underground interactions means that sex workers take greater risks, where as otherwise they would be cooperating with other sex workers and with sex worker organizations to report violence and protect each other. One example of this is with HIV prevention. Sex workers collectives monitor condom use, promote HIV testing, and cares for and monitor the health of HIV positive sex workers. Raids disrupt communal HIV care and prevention efforts, and if HIV positive sex workers are rescued and removed from their community, their treatments are disrupted, furthering the spread of AIDS.[168]		Scholars Aziza Ahmed and Meena Seshu suggest reforms in law enforcement procedures so that raids are last resort, not violent, and are transparent in its purposes and processes. Furthermore, they suggest that since any trafficking victims will probably be in contact with other sex workers first, working with sex workers may be an alternative to the raid and rescue model.[168]		Critics argue that End Demand programs are ineffective in that prostitution is not reduced, "John schools" have little effect on deterrence and portray prostitutes negatively, and conflicts in interest arise between law enforcement and NGO service providers. A study found that Sweden's legal experiment (criminalizing clients of prostitution and providing services to prostitutes who want to exit the industry in order to combat trafficking) did not reduce the number of prostitutes, but instead increased exploitation of sex workers because of the higher risk nature of their work; (no citation offered, and various other studies with very different results, are ignored) . The same study reported that johns' inclination to buy sex did not change as a result of john schools, and the programs targeted johns who are poor and colored immigrants. Some john schools also intimidate johns into not purchasing sex again by depicting prostitutes as drug addicts, HIV positive, violent, and dangerous, which further marginalizes sex workers. John schools require program fees, and police's involvement in NGOs who provide these programs create conflicts of interest especially with money involved.[169][170]		There are different feminist perspectives on sex trafficking. The third-wave feminist perspective of sex trafficking seeks to harmonize the dominant and liberal feminist views of sex trafficking. The dominant feminist view focuses on "sexualized domination", which includes issues of pornography, female sex labor in a patriarchal world, rape, and sexual harassment. Dominant feminism emphasizes sex trafficking as forced prostitution and considers the act exploitative. Liberal feminism sees all agents as capable of reason and choice. Liberal feminists support sex workers rights, and argue that women who voluntarily chose sex work are autonomous. The liberal feminist perspective finds sex trafficking problematic where it overrides consent of individuals.[171]		Third-wave feminism harmonizes the thoughts that while individuals have rights, overarching inequalities hinder women's capabilities. Third-wave feminism also considers that women who are trafficked and face oppression do not all face the same kinds of oppression. For example, third-wave feminist proponent Shelley Cavalieri identifies oppression and privilege in the intersections of race, class, and gender. Women from low socioeconomic class, generally from the Global South, face inequalities that differ from those of other sex trafficking victims. Therefore, it advocates for catering to individual trafficking victim because sex trafficking is not monolithic, and therefore there is not a one-size-fits-all intervention. This also means allowing individual victims to tell their unique experiences rather than essentializing all trafficking experiences. Lastly, third-wave feminism promotes increasing women's agency both generally and individually, so that they have the opportunity to act on their own behalf.[171]		Third-wave feminist perspective of sex trafficking is loosely related to Amartya Sen's and Martha Nussbaum's visions of the human capabilities approach to development. It advocates for creating viable alternatives for sex trafficking victims. Nussbaum articulated four concepts to increase trafficking victims' capabilities: education for victims and their children, microcredit and increased employment options, labor unions for low-income women in general, and social groups that connect women to one another.[171]		According to modern Feminists, women and girls are more prone to trafficking also because of social norms that marginalize their value and status in society. By this perspective females face considerable gender discrimination both at home and in school. Stereotypes that women belong at home in the private sphere and that women are less valuable because they do not and are not allowed to contribute to formal employment and monetary gains the same way men do further marginalize women's status relative to men. Some religious beliefs also lead people to believe that the birth of girls are a result of bad karma,[citation needed] further cementing the belief that girls are not as valuable as boys. It is generally regarded by feminists that various social norms contribute to women's inferior position and lack of agency and knowledge, thus making them vulnerable to exploitation such as sex trafficking.[172]		As of 2016, Singapore acceded to the United Nations Trafficking in Persons Protocol and affirmed on 28 September 2015 the commitment to combat people trafficking, especially women and children.[173]		
A truck system is an arrangement in which employees are paid in commodities or some money substitute (such as vouchers or token coins, called in some dialects scrip or chit) rather than with standard money. This limits employees' ability to choose how to spend their earnings—generally to the benefit of the employer. As an example, company scrip might be usable only for the purchase of goods at a company-owned store, where prices are set artificially high. The practice has been widely criticized as exploitative because there is no competition to lower prices. Legislation to curtail it, part of the larger field of labour law and employment standards, exists in many countries (for example, the British Truck Acts).[1]						The practice is ostensibly one of a free and legal exchange, whereby an employer offers something of value (typically goods, food or housing) in exchange for labor, with the result being the same as if the laborer had been paid money and then spent the money on those necessities. The word truck came into the English language within this context, from the French troquer, meaning to "exchange" or "barter". A truck system differs from this kind of open barter or payment in kind system by creating or taking advantage of a closed economic system in which workers have little or no opportunity to choose other work arrangements, and can easily become so indebted to their employers that they are unable to leave the system legally.		While this system had long existed in many parts of the world, it was widespread during the 18th and early-19th centuries in Britain. Despite a long history of legislation intended to curb truck systems (Truck Acts), they remained common into the 20th century. In a prosecution brought against a Manchester cotton manufacturer in 1827 one worker gave evidence that he had received wages of only two shillings in nine months; the rest "he was obliged to take [in goods] from the manufacturer's daughter, who was also the cashier".[2]		In Britain the truck system was sometimes referred to as the Tommy system. The 1901 edition of Brewer's Dictionary of Phrase and Fable[3] notes the Tommy shop as:		Where wages are paid to workmen who are expected to lay out a part of the money for the good of the shop. Tommy means bread or a penny roll, or the food taken by a workman in his handkerchief; it also means goods in lieu of money.		In the Midland Tour of his Rural Rides, the agriculturist and political reformer William Cobbett reports the use of "the truck or tommy system" in Wolverhampton and Shrewsbury. He describes the logic of the Tommy as:		The manner of carrying on the tommy system is this: suppose there to be a master who employs a hundred men. That hundred men, let us suppose, to earn a pound a week each. This is not the case in the iron-works; but no matter, we can illustrate our meaning by one sum as well as by another. These men lay out weekly the whole of the hundred pounds in victuals, drink, clothing, bedding, fuel, and house-rent. Now, the master finding the profits of his trade fall off very much, and being at the same time in want of money to pay the hundred pounds weekly, and perceiving that these hundred pounds are carried away at once, and given to shopkeepers of various descriptions; to butchers, bakers, drapers, hatters, shoemakers, and the rest; and knowing that, on an average, these shopkeepers must all have a profit of thirty per cent., or more, he determines to keep this thirty per cent. to himself; and this is thirty pounds a week gained as a shop-keeper, which amounts to 1,560l. a year. He, therefore, sets up a tommy shop: a long place containing every commodity that the workman can want, liquor and house-room excepted.		Although Cobbett sees nothing wrong in itself in the tommy system, he notes that The only question is in this case of the manufacturing tommy work, whether the master charges a higher price than the shop-keepers would charge; but given the guaranteed market Cobbett sees no reason why any master should ever abuse the system. However, in rural regions he notes the virtual monopoly of the shopkeeper:		I have often had to observe on the cruel effects of the suppression of markets and fairs, and on the consequent power of extortion possessed by the country shop-keepers. And what a thing it is to reflect on, that these shopkeepers have the whole of the labouring men of England constantly in their debt; have on an average a mortgage on their wages to the amount of five or six weeks, and make them pay any price that they choose to extort.		One reason for the truck system in the early history of the United States is that there was no national form of paper currency and an insufficient supply of coinage. Banknotes were the majority of the money in circulation. Banknotes were discounted relative to gold and silver (e.g. a $5 banknote might be exchanged for $4.50 of coins) and the discount depended on the financial strength of the issuing bank and distance from the bank. During financial crises many banks failed and their notes became worthless.[4][5]		The popular song "Sixteen Tons" dramatizes this scenario, with the narrator telling Saint Peter (who would welcome him to Heaven upon his death), "I can't go; I owe my soul to the company store".		
Person–environment fit (P–E fit) is defined as the degree to which individual and environmental characteristics match (Dawis, 1992; French, Caplan, & Harrison, 1982; Kristof-Brown, Zimmerman, & Johnson, 2005; Muchinsky & Monahan, 1987). Person characteristics may include an individual’s biological or psychological needs, values, goals, abilities, or personality, while environmental characteristics could include intrinsic and extrinsic rewards, demands of a job or role, cultural values, or characteristics of other individuals and collectives in the person's social environment (French et al., 1982). Due to its important implications in the workplace, person–environment fit has maintained a prominent position in Industrial and organizational psychology and related fields (for a review of theories that address person-environment fit in organizations, see Edwards, 2008).		Person–environment fit can be understood as a specific type of person–situation interaction that involves the match between corresponding person and environment dimensions (Caplan, 1987; French, Rodgers, & Cobb, 1974; Ostroff & Schulte, 2007). Even though person–situation interactions as they relate to fit have been discussed in the scientific literature for decades, the field has yet to reach consensus on how to conceptualize and operationalize person–environment fit. This is due partly to the fact that person–environment fit encompasses a number of subsets, such as person–supervisor fit and person–job fit, which are conceptually distinct from one another (Edwards & Shipp, 2007; Kristof, 1996). Nevertheless, it is generally assumed that person–environment fit leads to positive outcomes, such as satisfaction, performance, and overall well-being (Ostroff & Schulte, 2007).						Person–organization fit (P–O fit) is the most widely studied area of person–environment fit, and is defined by Kristof (1996) as, "the compatibility between people and organizations that occurs when (a) at least one entity provides what the other needs, (b) they share similar fundamental characteristics, or (c) both". High value congruence is a large facet of person–organization fit, which implies a strong culture and shared values among coworkers. This can translate to increased levels of trust and a shared sense of corporate community (Boone & Hartog, 2011). This high value congruence would in turn reap benefits for the organization itself, including reduced turnover, increased citizenship behaviors, and organizational commitment (Andrews et al., 2010; Gregory et al., 2010). The attraction–selection–attrition theory states that individuals are attracted to and seek to work for organizations where they perceive high levels of person–organization fit (Gregory et al., 2010). A strong person–organization fit can also lead to reduced turnover and increased organizational citizenship behaviors (Andrews, Baker, & Hunt, 2010)		Person–job fit, or P–J fit, refers to the compatibility between a person’s characteristics and those of a specific job (Kristof-Brown & Guay, 2011). The complementary perspective has been the foundation for person–job fit. This includes the traditional view of selection that emphasizes the matching of employee KSAs and other qualities to job demands (Ployhart, Schneider, & Schmitt, 2006). The discrepancy models of job satisfaction and stress that focus on employees’ needs and desires being met by the supplies provided by their job (Locke, 1969, 1976).		Person–group fit, or P–G fit, is a relatively new topic with regard to person–environment fit. Since person–group fit is so new, limited research has been conducted to demonstrate how the psychological compatibility between coworkers influences individual outcomes in group situations. However, a study by Boone & Hartog (2011) revealed that person–group fit is most strongly related to group-oriented outcomes like co-worker satisfaction and feelings of cohesion.		Person–person fit is conceptualized as the fit between an individual's culture preferences and those preferences of others. It corresponds to the similarity-attraction hypothesis which states people are drawn to similar others based on their values, attitudes, and opinions (Van Vianen, 2000). The most studied types are mentors and protégés, supervisors and subordinates, or even applicants and recruiters. Research has shown that person–supervisor fit is most strongly related to supervisor-oriented outcomes like supervisor satisfaction (Boone & Hartog, 2011).		Training and development on the job can be used to update or enhance skills or knowledge so employees are more in tune with the requirements and demands of their jobs, or to prepare them to make the transition into new ones. Training can be used as a socialization method, or as a way of making the employee aware of the organization’s desired values, which would aid in increasing person–organization fit (Boone & Hartog, 2011). As people learn about the organization they are working for through either company-initiated or self-initiated socialization, they should be able to be more accurate in their appraisal of fit or misfit. Furthermore, there is evidence that employees come to identify with their organization over time by mirroring its values, and socialization is a critical part of this process (Kristof-Brown & Guay, 2011).		In the workplace, performance appraisal and recognition or rewards can be used to stimulate skill-building and knowledge enhancement (Boone & Hartog, 2011), which would thereby enhance person–job fit. Expanding upon this notion, Cable and Judge (1994) showed that compensation systems have a direct effect on job search decisions, and additionally, the effects of compensation systems on job search decisions are strengthened when the applicant’s personality characteristics fit with the various components of the compensation system. When an employer’s aim is to strengthen person–organization fit, they can use performance appraisal to focus on an employee’s value and goal congruence, and ensure the individual’s goals are in line with the company’s goals.		On a group-level, organizations could evaluate the achievement of a group or team goal. Recognizing and supporting this achievement would build trust in the idea that everyone is contributing to the collective for the greater good, and aid in increasing person–group fit (Boone & Hartog, 2011).		Schneider (1987) proposed attraction–selection–attrition (ASA) model which addresses how attraction, selection and attrition could generate high levels of fit in an organization. The model is based on the proposition that it is the collective characteristics that define an organization. As a result, through the ASA process, organizations become more homogeneous with respect to people in them.		The attraction process of the model explains how employees find organizations attractive when they see congruence between characteristics of themselves and values of the organizations. The next step in ASA process is formal or informal selection procedures used by the organization during recruitment and hiring of applicants that fit the organization.		From the employee life cycle, recruitment and selection are the first stages that are taken into account when considering person–environment fit. The complementary model would posit that selection processes may work in part to select individuals whose values are compatible with the values of the organization, and screening out those whose values are incompatible (Chatman, 1991). Additionally, in accordance with supplementary fit models, an applicant will seek out and apply to organizations that they feel represent the values that he or she may have. This theory is exemplified through a study by Bretz and Judge (1994), which found that individuals who scored high on team orientation measures were likely to pick an organization that had good work–family policies in place. Along this same vein, when job searching, applicants will look for job characteristics such as the amount of participation they will have, autonomy, and the overall design of the job. These characteristics are shown to be significantly and positively related to person–organization and person–job fit (Boone & Hartog, 2011), which is positively associated the measurement of job satisfaction one year after entry (Chatman, 2011).		The last process in ASA model is attrition, which outlines that the misfitting employee would be more likely to make errors once hired, and therefore leave the organization. Thus, the people who do not fit choose or are forced to leave, and the people remaining are a more homogeneous group than those who were originally hired (Kristof-Brown & Guay, 2011), which should then result in higher levels of fit for individuals in an organization.		Lastly, the research suggests that for a better fit between an employee and a job, organization, or group to be more probable, it is important to spend an adequate amount of time with the applicant. This is because spending time with members before they enter the firm has been found to be positively associated with the alignment between individual values and firm values at entry (Chatman, 1991). Furthermore, if there are more extensive HR practices in place in the selection phase of hiring, then people are more likely to report that they experience better fits with their job and the organization as a whole (Boon et al., 2011).		There are few studies that have taken upon the task of trying to synthesize the different types of fit in order to draw significant conclusions about the true impact of fit on individual-level outcomes. However, some progress has been made, but most of the existing reviews have been non-quantitative, undifferentiated between various types of fit, or focused solely on single types of person–environment fit (Kristof-Brown et al., 2005).		Person–environment fit has been linked to a number of affective outcomes, including job satisfaction, organizational commitment, and intent to quit. Among which, job satisfaction is the attitude most strongly predicted by person–job fit (Kristof-Brown & Guay, 2011). Stress has also been demonstrated as a consequence of poor person–environment fit, especially in the absence of the complementary fit dimension (Kristof-Brown & Guay, 2011). Since main effects of E are often greater than those of P, making insufficient supplies (P > E) is more detrimental for attitudes than excess supplies (P < E). (Kristof-Brown et al., 2005)		Compatibility between the person and the environment can be assessed directly or indirectly, depending on the measure. Direct measures of perceived fit are typically used when person-environment fit is conceptualized as general compatibility. These measures ask an individual to report the fit that he or she believes exists. Examples of questions in direct measures are “How well do you think you fit in the organization?” or “How well do your skills match the requirements of your job?” An assumption is made such that individuals assess P and E characteristics and then determine how compatible they are. Although research has shown that these judgements are highly related to job attitudes (Yang et al., 2008), they have been criticized because they confound the independent effects of the person and the environment with their joint effect and do not adequately capture the psychological process by which people compare themselves to the environment (Edwards, Cable, Williamson, Lambert, & Shipp, 2006).		Indirect measures assess the person and environment separately. These measures are then used to compute an index intended to represent the fit between the person and environment, such as an algebraic, absolute, or squared difference score, or are analyzed jointly to assess the effects of fit without computing a difference score (Edwards, 1991; Kristof-Brown et al., 2005). Characteristics of the person are generally measured through self-report while characteristics of the environment can be reported by the person or by others in the person's environment. French et al. (1974, 1982) differentiated subjective fit, which are the match between P and E as they perceived by employees, from the objective fit, which is the match between P and E as distinct from the person's perception.		Up until the 1990s, studies using indirect measures of the person and environment typically operationalized fit by combining the measures into a single index representing the difference between the person and environment (Edwards, 1991; Kristof-Brown et al., 2005). Despite their intuitive appeal, difference scores are plagued with numerous conceptual and methodological problems, such as reduced reliability, conceptual ambiguity, confounded effects, untested constraints, and reducing an inherently three-dimensional relationship between the person, the environment, and the outcome to two dimensions (Cronbach, 1958; Edwards, 1994; Johns, 1981). These problems undermine the interpretation of the results of person-environment fit studies that rely on difference scores. Similar problems apply to studies that operationalize fit using profile similarity indices that compare the person and environment on multiple dimensions (Edwards, 1993).		Many of the problems with difference scores and profile similarity indices can be avoided by using polynomial regression (Edwards, 1994, 2002; Edwards & Parry, 1993). Polynomial regression involves using measures of the person and environment along with relevant higher-order terms (e.g., the squares and product of the person and environment measures) as joint predictors. In addition to avoiding problems with difference scores, polynomial regression allows for the development and testing of hypotheses that go beyond the simple functions captured by difference scores (Edwards & Shipp, 2007). The polynomial regression equation commonly used in person-environment fit research is as follows:		In this equation, E represents the environment, P represents the person, and Z is the outcome (e.g., satisfaction, well-being, performance). By retaining E, P, and Z as separate variables, results from polynomial regression equations can be translated into three-dimensional surfaces, whose properties can be formally tested using procedures set forth by Edwards and Parry (1993; see also Edwards, 2002). Studies using polynomial regression have found that the restrictive assumptions underlying difference scores are usually rejected, such that the relationship of the person and environment to outcomes is more complex than the simplified functions represented by difference scores. These findings have provided a foundation for developing fit hypotheses that are more refined than those considered in prior research, such as considering whether the effects of misfit are asymmetric and whether outcomes depend on the absolute levels of the person and environment (e.g., the effects of fit between actual and desired job complexity are likely to vary depending on whether job complexity is low or high; Edwards & Shipp, 2007).		Supplementary fit refers to the similarity between characteristics of a person and characteristics of the environment, or other persons within the environment (Kristof, 1996; Muchinsky & Monahan, 1987). Based on compatibility that derives from similarity (Kristof-Brown & Guay, 2011), a person fits into some environmental context because he/she supplements, embellishes, or possesses characteristics that are similar to other individuals in the environment (Kristof-Brown & Guay, 2011)		Complementary fit occurs when a person’s characteristics "make whole" the environment or add to it what is missing (Kristof, 1996; Muchinsky & Monahan, 1987). When individuals and environments complement one another by addressing each other’s needs, such as when an environment provides opportunities for achievement that are concordant with the individuals’ needs for achievement or when an individual with exceptional problem solving skills is in an environment that is in turmoil (Beasley et al., 2012). Piasentin and Chapman (2006) found that only a small portion of the workforce perceive fit due to complementarity while most view fit as supplementary (resulting from being similar to others). Journal of occupational and organizational psychology, 80 (2), 341-354.		Person–environment fit has important implications for organizations because it is critical for them to establish and maintain a “good fit” between people and their jobs. Companies use a substantial amount of resources when recruiting new employees, and it is crucial for them to ensure that these new hires will align with the environment they are thrust into. Furthermore, it has been theorized that person–environment fit can mediate the relation of group-specific workplace experiences with job outcomes (Velez & Moradi, 2012).		Andrews, M. C., Baker, T., & Hunt, T. G. (2011). Values and person–organization fit: Does moral intensity strengthen outcomes?Leadership & Organization Development Journal, 32(1), 5-19. doi:10.1108/01437731111099256		Boon, C., & Den Hartog, D. N. (2011). Human resource management, person–environment fit, and trust. Trust and human resource management, 109-121.		Bretz, Robert D.; Judge, Timothy A. (1994). The role of human resource systems in job applicant decision processes. Journal of Management, 20(3). doi: 531-551, 10.1016/0149-2063(94)90001-9		Cable, D.M., & Judge, T.A. (1996). Person–organization fit, job choice decisions, and organizational entry. Organizational Behavior and Human Decision Processes, 67, 294–311.		Cable, D.M., & Edwards, J.R. (2004). Complementary and supplementary fit: A theoretical and empirical integration. Journal of Applied Psychology, 89, 822–834.		Caplan, R. D. (1987). Person-environment fit theory and organizations: Commensurate dimensions, time perspectives, and mechanisms. Journal of Vocational Behavior, 31, 248-267.		Chatman, J. (1991). Matching people and organizations: Selection and socialization in public accounting firms. Administrative Science Quarterly, 36, 459–484.		Cronbach, L. J. (1958). Proposals leading to analytic treatment of social perception scores. In R. Tagiuri & L. Petrullo (Eds.), Person perception and interpersonal behavior (pp. 353-379). Stanford, CA: Stanford University Press.		Dawis, R. V. (1992). Person-environment fit and job satisfaction. In C. J. Cranny, P. C. Smith, & E. F. Stone (Eds.), Job satisfaction (pp. 69-88). New York: Lexington.		Edwards, J. R. (1991). Person-job fit: A conceptual integration, literature review, and methodological critique. In C. L. Cooper & I. T. Robertson (Eds.), International review of industrial and organizational psychology (vol. 6, pp. 283-357). New York: Wiley.		Edwards, J. R. (1994). The study of congruence in organizational behavior research: Critique and a proposed alternative. Organizational Behavior and Human Decision Processes, 58, 51-100 (erratum, 58, 323-325).		Edwards, J. R. (2002). Alternatives to difference scores: Polynomial regression analysis and response surface methodology. In F. Drasgow & N. W. Schmitt (Eds.), Advances in measurement and data analysis (pp. 350–400). San Francisco: Jossey-Bass.		Edwards, J. R. (2008). Person-environment fit in organizations: An assessment of theoretical progress. The Academy of Management Annals, 2, 167-230.		Edwards, J. R., Cable, D. M., Williamson, I. O., Lambert, L. S., & Shipp, A. J. (2006). The phenomenology of fit: Linking the person and environment to the subjective experience of person-environment fit. Journal of Applied Psychology, 91, 802-827.		Edwards, J. R., & Parry, M. E. (1993). On the use of polynomial regression equations as an alternative to difference scores in organizational research. Academy of Management Journal, 36, 1577-1613.		Edwards, J. R., & Shipp. A. J. (2007). The relationship between person-environment fit and outcomes: An integrative theoretical framework. In C. Ostroff & T. A. Judge (Eds.), Perspectives on organizational fit (pp. 209-258). San Francisco: Jossey-Bass.		French, J. R. P., Jr., Caplan, R. D., & Harrison, R. V. (1982). The mechanisms of job stress and strain. London: Wiley.		French, J. R. P., Jr., Rodgers, W. L., & Cobb, S. (1974). Adjustment as person-environment fit. In G. Coelho, D. Hamburg, & J. Adams (Eds.), Coping and adaptation (pp. 316-333). New York: Basic Books.		Guan, Y., Deng, H., Risavy, S. D., Bond, M. H., & Li, F. (2010). Supplementary fit, complementary fit, and work-related outcomes: The role of self-construal. Journal of Applied Psychology, 60(2), 210-286. doi:10.1111/j.1464- 0597.2010.00436.x		Johns, G. (1981). Difference score measures of organizational behavior variables: A critique. Organizational Behavior and Human Performance, 27, 443-463.		Juntunen, C. L., & Even, C. E. (2012). Theories of Vocational Psychology. APA Handbook of Counseling Psychology, 1, 237-262. Retrieved October 9, 2012, from 10.1037/13754-009		Kristof, A.L. (1996). Person–organization fit: An integrative review of its conceptualizations, measurement, and implications. Personnel Psychology, 49, 1–49.		Kristof-Brown, A.L., Zimmerman, R.D., & Johnson, E.C. (2005). Consequences of individuals' fit at work: A meta-analysis of person–job, person–organization, person–group, and person–supervisor fit. Personnel Psychology, 58, 281–342.		Kristof-Brown, A., & Guay, R. P. (2011). Person–environment fit. APA handbook of industrial and organizational psychology,3, 3-50. doi:10.1037/12171-001		Locke, E. A. (1969). What is job satisfaction? Organizational Behavior and Human Performance, 4, 309-336.		Locke, E. A. (1976). The nature and causes of job satisfaction. In M. Dunnette (Ed.), Handbook of industrial and organizational psychology: 1297-1350. Chicago: Rand McNally.		Muchinsky, P. M., & Monahan, C. J. (1987). What is person-environment congruence? Supplementary versus complementary models of fit. Journal of Vocational Behavior, 31, 268-277.		Piasentin, K.A., & Chapman, D.S. (2007). Perceived similarity and complementarity as predictors of subjective person–organization fit. Journal of Occupational and Organizational Psychology, 80, 341–354.		Velez, B. L., & Moradi, B. (2012). Workplace support, discrimination, and person–organization fit: Tests of the theory of work adjustment with LGB individuals. Journal of Counseling Psychology, 59(3), 399–407. Retrieved October 1, 2012		Van Vianen, A. E. m. (2000). Person–Organization Fit: The Match Between Newcomers’ and Recruiters’ Preferences for Organizational Cultures. Personnel Psychology, 53(1), 113–149. doi:10.1111/j.1744-6570.2000.tb00196.x		
An independent contractor is a natural person, business, or corporation that provides goods or services to another entity under terms specified in a contract or within a verbal agreement. Unlike an employee, an independent contractor does not work regularly for an employer but works as and when required, during which time he or she may be subject to law of agency. Independent contractors are usually paid on a freelance basis. Contractors often work through a limited company or franchise, which they themselves own, or may work through an umbrella company.		In the United States, any company or organization engaged in a trade or business that pays more than $600 to an independent contractor in one year is required to report this to the Internal Revenue Service (IRS) as well as to the contractor, using Form 1099-MISC.[1][2] This form is merely a report of the money paid; independent contractors do not have income taxes withheld like regular employees.						The distinction between independent contractor and employee is an important one in the United States, as the costs for business owners to maintain employees is significantly higher than independent contractors due to the need to pay Social Security, Medicare and unemployment taxes on employees.[3]		In the early 1990s, the IRS methodically began to look for employers who were misclassifying employees as independent contractors, and has since obtained billions of dollars in Social Security back taxes.[3] Recently, worker classification initiatives have been a top priority for the IRS, the Department of Labor, and state agencies. In 2011, the IRS and the Department of Labor entered into a memorandum of understanding in an effort to jointly increase worker misclassification audits.[4]		The United States Supreme Court has offered the following guidelines to distinguish employees from independent contractors:		The IRS, for federal income tax, applies a "right to control test" which considers the nature of the working relationship.[5] They highlight three general aspects of the employment arrangement: financial control, behavioral control, and relationship between the parties. In general, their criteria parallel those of the supreme court in sentiment. They include guidelines such as the amount of instruction, training, integration, use of assistants, length of professional relationship, regularity of work, location of work, payment schedule, source of funds for business expenditures, right to quit, and financial risk more typically seen with each work category. In their framework, independent contractors retain control over schedule and hours worked, jobs accepted, and performance monitoring. They also can have a major investment in equipment, furnish their own supplies, provide their own insurance, repairs, and other expenses related to their business. They may also perform a unique service that is not in the normal course of business of the employer. This contrasts with employees, who usually work at the schedule required by the employer, and whose performance the employer directly supervises. Independent contractors can also work for multiple firms, and offer their services to the general public.[3]		The distinction between independent contractors and employees is not always clear, and continues to evolve. For example, some independent contractors may work for a number of different organizations throughout the year, while others retain independent contractor status although they work for the same organization the entire year.[6] Other companies, for example in the freight transport industry, specify the schedule for the independent contractor, require purchase of vehicles from the company and prohibit work for other companies.[4]		In July 2015, the U.S. Department of Labor issued new guidelines on the misclassification of employees as independent contractors. "A worker who is economically dependent on an employer is suffered or permitted to work by the employer. Thus, applying the economic realities test in view of the expansive definition of "employ" under the Act, most workers are employees under the Fair Labor Standards Act."[7][8]		Examples of occupations where independent contractor arrangements are typical:		Independent contracting has both benefits and drawbacks to contractors.		Firms in the sharing economy such as Uber can gain substantial advantages by classifying associates as independent contractors.[9]		The employer of an independent contractor is generally not held vicariously liable for the tortious acts and omissions of the contractor, because the control and supervision found in an employer–employee or principal–agent relationship is lacking. However, vicarious liability will be imposed in some circumstances:		Due to the higher expense of maintaining employees, many employers needing to transition to independent contractors may find the switch to contracting difficult. There is a transitional status for employees as an alternative to independent contracting known as being a statutory employee. Statutory employees are less expensive to hire than classic employees because the employer does not have to pay unemployment tax. However, they are more expensive than independent contractors because Social Security and Medicare taxes must be paid on wages. Similarly to independent contractors, statutory employees can deduct their trade or business expenses from their W2 earnings.[3]		A growing number of workers do not neatly fit the government's categorizations of independent contractors and statutory employees, and are increasingly being classified as dependent contractors. Some of these contingent workforce—independent contractors, temporary workers, and part-time workers, who work when and for how long they want, such as those who work for such companies as Uber, Handybook, Inc., and CrowdFlower—have filed lawsuits that argue that companies that substantially control workers' work and behaviors while working (such as at Handybook, Inc.: when to knock on customers' doors vs. ring the doorbells, and how to use the customers' bathrooms) should be covered by minimum-wage and overtime rules under the Fair Labor Standards Act, as well as receive other traditional employee protections. Wilma Liebman, former chairperson of the National Labor Relations Board, has observed that Canada and Germany have such protections in place for contingent workers. And UK Prime Minister David Cameron has appointed an overseer for freelance workers in that country, to determine how the government should support freelance workers.[15][16][17]		
The notice period is the time period between the receipt of the letter of dismissal and the end of the last working day. This time period has to be given to an employee by his/her employer before his/her employment ends. It also refers to the period between resignation date and last working day in the company when an employee resigns.		In the United Kingdom the statutory redundancy notice periods are:[1]		These statutory periods constitute the minimum notice period to be given by the employer, however, some employers may opt to give employees longer notice periods, in order to give the employees a better opportunity to find alternative employment.[note 1]				
The personality–job fit theory postulates that a person's personality traits will reveal insight as to adaptability within an organization. The degree of confluence between a person and the organization is expressed as their Person-Organization (P-O) fit.[1] This is also referred to as a person–environment fit.[2][3][4] A common measure of the P-O fit is workplace efficacy; the rate at which workers are able to complete tasks. These tasks are mitigated by workplace environs- for example, a worker who works more efficiently as an individual than in a team will have a higher P-O fit for a workplace that stresses individual tasks (such as accountancy).[1] By matching the right personality with the right job, company workers can achieve a better synergy and avoid pitfalls such as high turnover and low job satisfaction. Employees are more likely to stay committed to organizations if the fit is 'good'.		In practice, P-O fit would be used to gauge integration with organizational competencies. The Individual is assessed on these competencies, which reveals efficacy, motivation, influence, and co-worker respect. Competencies can be assessed using various tools like psychological tests, assessment centres competency based interview, situational analysis, etc.		If the Individual displays a high P-O fit, we can say that the Individual would most likely be able to adjust to the company environment and work culture, and would be able to perform at an optimum level.		In tech, Good&Co uses this theory in determining matches for its users with companies and positions that fit their personality.[5]				
An application for employment, job application, or application form (often simply called an application) usually includes a form or collection of forms that an individual seeking employment, called an applicant, must fill out as part of the process of informing an employer of the applicant's availability and desire to be employed, and persuading the employer to offer the applicant employment.						From the employer's perspective, the application serves a number of purposes. These vary depending on the nature of the job and the preferences of the person responsible for hiring, as "each organization should have an application form that reflects its own environment".[2] At a minimum, an application usually requires the applicant to provide information sufficient to demonstrate that he or she is legally permitted to be employed. The typical application also requires the applicant to provide information regarding relevant skills, education, and experience (previous employment or volunteer work). The application itself is a minor test of the applicant's literacy, penmanship, and communication skills - a careless job applicant might disqualify themselves with a poorly filled-out application.		The application may also require the applicant to disclose any criminal record, and to provide information sufficient to enable the employer to conduct an appropriate background check. For a business that employs workers on a part-time basis, the application may inquire as to the applicant's specific times and days of availability, and preferences in this regard. It is important to note, however, that an employer may be prohibited from asking applicants about characteristics that are not relevant to the job, such as their political view or sexual orientation.[2][3]		For white collar jobs, particularly those requiring communication skills, the employer will typically require applicants to accompany the form with a cover letter and a résumé.[4] However, even employers who accept a cover letter and résumé will frequently also require the applicant to complete a form application, as the other documents may neglect to mention details of importance to the employers.[5][6] In some instances, an application is effectively used to dissuade "walk-in" applicants, serving as a barrier between the applicant and a job interview with the person with the authority to hire.[7]		For many businesses, applications for employment can be filled out online, and do not have to be submitted in person. However, it is still recommended that applicants bring a printed copy of their application to an interview.[8]		Application blanks are the second most common hiring instrument next to personal interviews.[9] Companies will occasionally use two types of application blanks, short and long.[citation needed] They both help companies with initial screening and the longer form can be used for other purposes as well. The answers that applicants choose to submit are helpful to the company because they can become an interview question for that applicant at a future date.		Application blanks can either be done by hand or electronically, depending on the company.[citation needed] When submitting an application blank typically companies will ask you attach a one-page cover letter as well as a resume. Applicants tend to make the mistake of sharing too much information with the company and their application will be immediately overlooked.[citation needed] Offering too much information gives the company a bigger opportunity to find something they do not like. Please ask or find a copy of the job ad before applying for a job to make sure you list your key skills and expertise that matches the job you are applying for first on your job application, resume and cover letter to help you not list too much information. Companies are not allowed to ask certain questions in person or on an application such as age, health status, religion, marital status, about children, race, height, weight, or whom you live with♮. ♮		Applications usually ask the applicant at the minimum for their name, phone number, and address. In addition, applications also ask for previous employment information, educational background, emergency contacts, references, as well as any special skills the applicant might have.		The three categories application fields are very useful for discovering are; physical characteristics, experience, and socio-environmental factors.		If the company has a bona fide occupational qualification (BFOQ) to ask regarding a physical condition, they may ask questions about it,[citation needed] for example:		Experience requirements can be separated into two groups on an application, work experience and educational background.[citation needed] Educational background is important to companies because by evaluating applicants' performance in school tells them what their personality is like as well as their intelligence. Work experience is important to companies because it will inform the company if the applicant meets their requirements. Companies are usually interested when applicants were unemployed and when/why the applicant left their previous job.		Companies are interested in the applicant's social environment because it can inform them of their personality, interests, and qualities.[citation needed] If they are extremely active within an organization, that may demonstrate their ability to communicate well with others. Being in management may demonstrate their leadership ability as well as their determination and so on.		Customs vary internationally when it comes to the inclusion or non-inclusion of a photograph of the applicant. In the English-speaking countries, notably the United States, this is not customary, and books or websites giving recommendations about how to design an application typically advise against it unless explicitly requested by the employer.[10] In other countries, for instance Germany, the inclusion of a photograph of the applicant is still common, and many employers would consider an application incomplete without it.		The job application procedures in Austria are very similar to those in Germany. However, there is more emphasis on academic degrees in Austria than in Germany. Otherwise, the general rules of the German Bewerbung also apply to job applications in Austria.[11]		In France, the 2006 Equal Opportunities Act (fr) requires companies with more than 50 employees to request an anonymous application (CV anonyme).		The job application is called Bewerbung in Germany and usually consists of three parts, such as the Anschreiben, the Lebenslauf and the Zeugnisse. Anschreiben is the German word for Cover Letter and aims at the same goal: convincing the employer to submit an invitation for a job interview.[12] It is essential to work with the paper size DIN A4 and to stay with a length of one single page. The Anschreiben must be signed by hand and accompanied by a Lebenslauf, the Curriculum Vitae, and Zeugnisse as copies of relevant reference documents. The Lebenslauf is of an anti-chronological structure and should give information on work experience, education and professional training as well as on applicant's skills. In Germany, the Lebenslauf – respectively the Curriculum Vitae – usually includes a photograph called Bewerbungsfoto. Some employers, mainly governmental organisations, deliberately neglect the photograph to ensure a higher degree of objectivity in the course of assessment procedures. A length of two pages is to be aimed at when generating the Lebenslauf. In general, there are two options of submitting a job application in Germany: a job application folder (Bewerbungsmappe) or online (Onlinebewerbung). According to a study,[13] the Onlinebewerbung was more favored in Germany than the Bewerbungsmappe by 2012. Presumably, this development will persist.		The CV is the most important part of the application and should not be longer than two to three pages. It is divided into three areas:		in chronological order		in anti-chronological order		The application letter (La Lettera di accompagnamento al curriculum) will be taken relatively short, polite and formal in Italian applications. Long versions and extensively explained motivations, as well as photos and copies of certificates shall be presented only at the interview.		In Spain, the application consists of two parts: the cover letter (Carta de Candidatura) and the CV. No work or training certificates are attached. The cover letter should contain information on the motivation and drafted shortly. The CV should be structured in a tabular form. In Spain, multiple job interviews in the same company are common.		Job applications are known to be used by hackers to get employees to open attachments or links or connect USB sticks with malware. As companies typically have more financial resources than private individuals they are often a target of cyberextortion − so called ransomware.[14][15] Ransomware such as "Petya"[16][17] and "GoldenEye"[18][19][20][21] were found to make use of job applications. Cyberespionage and attacks on critical infrastructure-related companies may be other reasons for such attacks and other than ransomware attacks may leave employees in the dark about their computer or network infection.[22][23][24] The best method for mitigating such risks would be to have the HR department use a separate computer for job applications that is entirely disconnected from the internal network, on which no confidential or valuable information is stored and to which no portable devices such as USB sticks that may get connected to other computers of the company are connected.						
Pink slip refers to the American practice, by a human resources department, of including a discharge notice in an employee's pay envelope to notify the worker of his or her termination of employment or layoff.[1] The "pink slip" has become a metonym for the termination of employment in general. According to an article in The New York Times, the editors of the Random House Dictionary have dated the term to at least as early as 1910.		The phrase most likely originated in vaudeville. When the United Booking Office (established in 1906) would issue a cancellation notice to an act, the notice was on a pink slip ("The Argot of Vaudeville Part I" New York Times, Dec. 16, 1917, p.X7.) Another possible etymology is that many applications (including termination papers) are done in triplicate form, with each copy on a different color of paper, one of which is typically pink.		In the Chilly Willy cartoon, "little Televillian", Mr. Stoop, tells Smedley that if he's disturbed while reading show scripts, he would be forced to pink slip Smedley, (which Smedley doesn't know the meaning of) and yells at him, " It means...you will be fired!".		In the UK and Ireland the equivalent of a pink slip is a P45; in Belgium the equivalent is known as a C4.		
An induction programme is the process used within many businesses to welcome new employees to the company and prepare them for their new role.		Induction training should, according to TPI-theory, include development of theoretical and practical skills, but also meet interaction needs that exist among the new employees.[1]		An Induction Programme can also include the safety training delivered to contractors before they are permitted to enter a site or begin their work. It is usually focused on the particular safety issues of an organisation but will often include much of the general company information delivered to employees.						An induction programme is an important process for bringing staff into an organisation. It provides an introduction to the working environment and the set-up of the employee within the organisation. The process will cover the employer and employee rights and the terms and conditions of employment. As a priority the induction programme must cover any legal and compliance requirements for working at the company and pay attention to the health and safety of the new employee.		An induction programme is part of an organisations knowledge management process and is intended to enable the new starter to become a useful, integrated member of the team, rather than being "thrown in at the deep end" without understanding how to do their job, or how their role fits in with the rest of the company.		Good induction programmes can increase productivity and reduce short-term turnover of staff. These programs can also play a critical role under the socialization to the organization in terms of performance, attitudes and organizational commitment.[2] In addition well designed induction programmes can significantly increase the speed to competency of new employees thus meaning they are more productive in a shorter period of time.		A typical induction programme will include at least some of the following:		In order to fully benefit the company and employee, the induction programme should be planned in advance. The timetable should be prepared, detailing the induction activities for a set period of time (ideally at least a week) for the new employee, including a named member of staff who will be responsible for each activity. This plan should be circulated to everyone involved in the induction process, including the new starter. If possible it should be sent to the new starter in advance, if not co-created with the new starter[3]		It is also considered best practice to assign a buddy to every new starter. If possible this should be a person who the new starter will not be working with directly, but who can undertake some of the tasks on the induction programme, as well as generally make the new employee feel welcome. (For example, by ensuring they are included in any lunchtime social activities.)		
A whistleblower (also written as whistle-blower or whistle blower)[1] is a person who exposes any kind of information or activity that is deemed illegal, unethical, or not correct within an organization that is either private or public.[2] The information of alleged wrongdoing can be classified in many ways: violation of company policy/rules, law, regulation, or threat to public interest/national security, as well as fraud, and corruption.[3] Those who become whistleblowers can choose to bring information or allegations to surface either internally or externally. Internally, a whistleblower can bring his/her accusations to the attention of other people within the accused organization such as an immediate supervisor. Externally, a whistleblower can bring allegations to light by contacting a third party outside of an accused organization such as the media, government, law enforcement, or those who are concerned. Whistleblowers, however, take the risk of facing stiff reprisal and retaliation from those who are accused or alleged of wrongdoing.		Because of this, a number of laws exist to protect whistleblowers. Some third party groups even offer protection to whistleblowers, but that protection can only go so far. Whistleblowers face legal action, criminal charges, social stigma, and termination from any position, office, or job. Two other classifications of whistleblowing are private and public. The classifications relate to the type of organizations someone chooses to whistle-blow on: private sector, or public sector. Depending on many factors, both can have varying results. However, whistleblowing in the public sector organization is more likely to result in criminal charges and possible custodial sentences. A whistleblower who chooses to accuse a private sector organization or agency is more likely to face termination and legal and civil charges.		Deeper questions and theories of whistleblowing and why people choose to do so can be studied through an ethical approach. Whistleblowing is a topic of ongoing ethical debate. Leading arguments in the ideological camp that whistleblowing is ethical maintain that whistleblowing is a form of civil disobedience, and aims to protect the public from government wrongdoing.[4][5] In the opposite camp, some see whistleblowing as unethical for breaching confidentiality, especially in industries that handle sensitive client or patient information.[6] Legal protection can also be granted to protect whistleblowers, but that protection is subject to many stipulations. Hundreds of laws grant protection to whistleblowers, but stipulations can easily cloud that protection and leave whistleblowers vulnerable to retaliation and legal trouble. However, the decision and action has become far more complicated with recent advancements in technology and communication.[7] Whistleblowers frequently face reprisal, sometimes at the hands of the organization or group they have accused, sometimes from related organizations, and sometimes under law. Questions about the legitimacy of whistleblowing, the moral responsibility of whistleblowing, and the appraisal of the institutions of whistleblowing are part of the field of political ethics.						U.S. civic activist Ralph Nader is said to have coined the phrase, but he in fact put a positive spin on the term[8] in the early 1970s to avoid the negative connotations found in other words such as "informer" and "snitch".[9] However, the origins of the word date back to the 19th century.		The word is linked to the use of a whistle to alert the public or a crowd about a bad situation, such as the commission of a crime or the breaking of rules during a game. The phrase whistle blower attached itself to law enforcement officials in the 19th century because they used a whistle to alert the public or fellow police.[10] Sports referees, who use a whistle to indicate an illegal or foul play, also were called whistle blowers.[11][12]		An 1883 story in the Janesville Gazette called a policeman who used his whistle to alert citizens about a riot a whistle blower, without the hyphen. By the year 1963, the phrase had become a hyphenated word, whistle-blower. The word began to be used by journalists in the 1960s for people who revealed wrongdoing, such as Nader. It eventually evolved into the compound word whistleblower.[10]		Most whistleblowers are internal whistleblowers, who report misconduct on a fellow employee or superior within their company through anonymous reporting mechanisms often called hotlines.[13] One of the most interesting questions with respect to internal whistleblowers is why and under what circumstances do people either act on the spot to stop illegal and otherwise unacceptable behavior or report it.[14] There are some reasons to believe that people are more likely to take action with respect to unacceptable behavior, within an organization, if there are complaint systems that offer not just options dictated by the planning and control organization, but a choice of options for absolute confidentiality.[15]		Anonymous reporting mechanisms, as mentioned previously, help foster a climate whereby employees are more likely to report or seek guidance regarding potential or actual wrongdoing without fear of retaliation. The coming ISO 37001 – anti-bribery management systems standard, includes anonymous reporting as one of the criteria for the new standard.		External whistleblowers, however, report misconduct to outside persons or entities. In these cases, depending on the information's severity and nature, whistleblowers may report the misconduct to lawyers, the media, law enforcement or watchdog agencies, or other local, state, or federal agencies. In some cases, external whistleblowing is encouraged by offering monetary reward.		The third party service involves utilizing an external agency to inform the individuals at the top of the organizational pyramid of misconduct, without disclosing the identity of the whistleblower. This is a relatively new phenomenon and has been developed due to whistleblower discrimination. International Whistleblowers is an example of an organization involved in delivering a third party service for whistleblowers.		An increasing number of companies and authorities use third party services in which the whistleblower is anonymous also towards the third party service provider. This is possible via toll free phone numbers configured not to record the whistleblower origin call, and also through web solutions which apply asymmetrical encryption.		Private sector whistleblowing, though not as high profile as public sector whistleblowing, is arguably more prevalent and suppressed in society today.[16] Simply because private corporations usually have stricter regulations that suppress potential whistleblowers. An example of private sector whistleblowing is when an employee reports to someone in a higher position such as a manager, or a third party that is isolated from the individual chapter, such as their lawyer or the police. In the private sector corporate groups can easily hide wrongdoings by individual branches. It is not until these wrongdoings bleed into the top officials that corporate wrongdoings are seen by the public. Situations in which a person may blow the whistle are in cases of violated laws or company policy, such as sexual harassment or theft. These instances, nonetheless, are small compared to money laundering or fraud charges on the stock market. Whistleblowing in the private sector is typically not as high-profile or openly discussed in major news outlets, though occasionally, third parties expose human rights violations and exploitation of workers.[17] While there are organizations such as the United States Department of Labor (DOL), and laws in place such as the Sarbanes-Oxley Act and the United States Federal Sentencing Guidelines for Organizations (FSGO) which protects whistleblowers in the private sector, many employees still fear for their jobs due to direct or indirect threats from their employers or the other parties involved. In an effort to overcome those fears, in 2010 Dodd–Frank Wall Street Reform and Consumer Protection Act was put forth to provide great incentive to whistleblowers. For example, if a whistleblower gave information which could be used to legally recover over one million dollars; then they could receive ten to thirty percent of it.		Despite government efforts to help regulate the private sector, the employees must still weigh their options. They either expose the company and stand the moral and ethical high ground; or expose the company, lose their job, their reputation and potentially the ability to be employed again. According to a study at the University of Pennsylvania, out of three hundred whistleblowers studied, sixty nine percent of them had foregone that exact situation; and they were either fired or were forced to retire after taking the ethical high ground. It is outcomes like that which makes it all that much harder to accurately track how prevalent whistleblowing is in the private sector.[18]		Recognizing the public value of whistleblowing has been increasing over the last 50 years. In the United States, both State and Federal statutes have been put in place to protect whistleblowers from retaliation. The United States Supreme Court ruled that public sector whistleblowers are protected under First Amendment rights from any job retaliation when they raise flags over alleged corruption.[19] Exposing misconduct, illegal, or dishonest activity is a big fear for public employees because they feel they are going against their government and country. Private sector whistleblowing protection laws were in place long before ones for the public sector. After many federal whistleblowers were scrutinized in high-profile media cases, laws were finally introduced to protect government whistleblowers. These laws were enacted to help prevent corruption and encourage people to expose misconduct, illegal, or dishonest activity for the good of society.[20] People who choose to act as a whistleblower often suffer retaliation from their employer. They most likely are fired because they are an at-will employee, which means they can be fired without a reason. There are exceptions in place for whistleblowers who are at-will employees. Even without a statute, numerous decisions encourage and protect whistleblowing on grounds of public policy. Statutes state that an employer shall not take any adverse employment actions any employee in retaliation for a good faith report of a whistleblowing action or cooperating in anyway in an investigation, proceeding, or lawsuit arising under said action.[19] Federal whistleblower legislation includes a statute protecting all government employees. In the federal civil service, the government is prohibited from taking, or threatening to take, any personnel action against an employee because the employee disclosed information that he or she reasonably believed showed a violation of law, gross mismanagement, and gross waste of funds, abuse of authority, or a substantial and specific danger to public safety or health. To prevail on a claim, a federal employee must show that a protected disclosure was made, that the accused official knew of the disclosure, that retaliation resulted, and that there was a genuine connection between the retaliation and the employee's action.[19]		Individual harm, public trust damage, and a threat of national security are three categories of harm that may come to whistleblowers. Revealing whistleblower identities automatically puts their life in harm's way. Especially with media outlets using words like "traitor" and "treason" to associate with whistleblowers. There are many countries around the world that associate treason with the death penalty, even though whoever allegedly committed treason may or may not have caused anyone physical harm. A primary reason for the death penalty would be that they have potentially endangered an entire people, therefore being responsible for any harm to come as a result. US law states,		"...whoever knowingly and willfully communicates, furnishes, transmits, or otherwise makes available to an unauthorized person, or publishes, or uses in any manner prejudicial to the safety or interest of the United States or for the benefit of any foreign government to the detriment of the United States any classified information."[21][22]		Public trust dates back to the days of the Vietnam War. Henry Kissinger once said that the purpose of "those who stole" the Pentagon Papers was to "undermine confidence in their government" and "raise doubts about our reliability in the minds of other governments, friend and foe, and indeed about the stability of our political system."		Emotional strain on the accused from the whistle blower is also unconstrained. When a leader challenges a whistle blower, there is an automatic indictment of the leader's character.[23] Questioning the whistleblower makes the accused guilty until proven innocent.		Whistleblowers are sometimes seen as selfless martyrs for public interest and organizational accountability; others view them as "traitors" or "defectors." Some even accuse them of solely pursuing personal glory and fame, or view their behavior as motivated by greed in qui tam cases. Some academics (such as Thomas Alured Faunce) feel that whistleblowers should at least be entitled to a rebuttable presumption that they are attempting to apply ethical principles in the face of obstacles and that whistleblowing would be more respected in governance systems if it had a firmer academic basis in virtue ethics.[24][25]		It is probable that many people do not even consider blowing the whistle, not only because of fear of retaliation, but also because of fear of losing their relationships at work and outside work.[26]		Persecution of whistleblowers has become a serious issue in many parts of the world:		Employees in academia, business or government might become aware of serious risks to health and the environment, but internal policies might pose threats of retaliation to those who report these early warnings. Private company employees in particular might be at risk of being fired, demoted, denied raises and so on for bringing environmental risks to the attention of appropriate authorities. Government employees could be at a similar risk for bringing threats to health or the environment to public attention, although perhaps this is less likely.[27]		There are examples of "early warning scientists" being harassed for bringing inconvenient truths about impending harm to the notice of the public and authorities. There have also been cases of young scientists being discouraged from entering controversial scientific fields for fear of harassment.[27]		Whistleblowers are often protected under law from employer retaliation, but in many cases punishment has occurred, such as termination, suspension, demotion, wage garnishment, and/or harsh mistreatment by other employees. A 2009 study found that up to 38% of whistleblowers experienced professional retaliation in some form, including wrongful termination.[citation needed] For example, in the United States, most whistleblower protection laws provide for limited "make whole" remedies or damages for employment losses if whistleblower retaliation is proven. However, many whistleblowers report there exists a widespread "shoot the messenger" mentality by corporations or government agencies accused of misconduct and in some cases whistleblowers have been subjected to criminal prosecution in reprisal for reporting wrongdoing.		As a reaction to this many private organizations have formed whistleblower legal defense funds or support groups to assist whistleblowers; three such examples are the National Whistleblowers Center[28] in the United States, and Whistleblowers UK[29] and Public Concern at Work (PCaW)[30] in the United Kingdom. Depending on the circumstances, it is not uncommon for whistleblowers to be ostracized by their co-workers, discriminated against by future potential employers, or even fired from their organization. This campaign directed at whistleblowers with the goal of eliminating them from the organization is referred to as mobbing. It is an extreme form of workplace bullying wherein the group is set against the targeted individual.[31]		There is limited research on the psychological impacts of whistle blowing. However, poor experiences of whistleblowing can cause a prolonged and prominent assault upon staff well being. As workers attempt to address concerns, they are often met with a wall of silence and hostility by management.[32] Some whistleblowers speak of overwhelming and persistent distress, drug and alcohol problems, paranoid behaviour at work, acute anxiety, nightmares, flashbacks and intrusive thoughts.[33] Depression is often reported by whistleblowers, and suicidal thoughts may occur in up to about 10%.[34][35] General deterioration in health and self care has been described.[36] The range of symptomatology shares many of the features of posttraumatic stress disorder, though there is debate about whether the trauma experienced by whistleblowers meets diagnostic thresholds.[37] Increased stress related physical illness has also been described in whistleblowers.[35][38] The stresses involved in whistleblowing can be huge. As such, workers remain afraid to blow the whistle, in fear that they will not be believed or they have lost faith in believing that anything will happen if they do speak out.[39] This fear may indeed be justified, because an individual who feels threatened by whistleblowing, may plan the career destruction of the ‘complainant’ by reporting fictitious errors or rumours.[40] This technique, labelled as ‘gaslighting’ is a common, unconventional approach used by organizations to manage employees who cause difficulty by raising concerns.[41] In extreme cases, this technique involves the organization or manager proposing that the complainant's mental health is unstable.[42] Organizations also often attempt to ostracise and isolate whistleblowers by undermining their concerns by suggesting that these are groundless, carrying out inadequate investigations or by ignoring them altogether. Whistleblowers may also be disciplined, suspended and reported to professional bodies upon manufactured pretexts.[43][44] Where whistleblowers persist in raising their concerns, they increasingly risk detriments such as dismissal.[45] Following dismissal, whistleblowers may struggle to find further employment due to damaged reputations, poor references and blacklisting. The social impact of whistleblowing through loss of livelihood (and sometimes pension), and family strain may also impact on whistleblowers’ psychological well being. Whistleblowers may also experience immense stress as a result of litigation regarding detriments such as unfair dismissal, which they often face with imperfect support or no support at all from unions. Whistleblowers who continue to pursue their concerns may also face long battles with official bodies such as regulators and government departments.[43][44] Such bodies may reproduce the "institutional silence" by employers, adding to whistleblowers’ stress and difficulties.[46] In all, some whistleblowers suffer great injustice, that may never be acknowledged or rectified.[42] Such extreme experiences of threat and loss inevitably cause severe distress and sometimes mental illness, sometimes lasting for years afterwards. This mistreatment also deters others from coming forward with concerns. Thus, poor practices remain hidden behind a wall of silence, and prevent any organization from experiencing the improvements that may be afforded by intelligent failure.[33][46] Some whistleblowers who part ranks with their organizations have had their mental stability questioned, such as Adrian Schoolcraft, the NYPD veteran who alleged falsified crime statistics in his department and was forcibly committed to a mental institution.[47] Conversely, the emotional strain of a whistleblower investigation is devastating to the accused's family.[23]		The definition of ethics is the moral principles that govern a person's or group's behavior. The ethical implications of whistleblowing can be negative as well as positive. However, sometimes employees may blow the whistle as an act of revenge. Rosemary O'Leary explains this in her short volume on a topic called guerrilla government. "Rather than acting openly, guerrillas often choose to remain "in the closet," moving clandestinely behind the scenes, salmon swimming upstream against the current of power. Over the years, I have learned that the motivations driving guerrillas are diverse. The reasons for acting range from the altruistic (doing the right thing) to the seemingly petty (I was passed over for that promotion). Taken as a whole, their acts are as awe inspiring as saving human lives out of a love of humanity and as trifling as slowing the issuance of a report out of spite or anger."[48] For example, of the more than 1,000 whistleblower complaints that are filed each year with the Pentagon's Inspector General, about 97 percent are not substantiated.[49] The negative results of being a whistleblower could be one being seen as a traitor, a hero, or just one of the majority (97 percent) whistleblowers who are simply disgruntled with a perceived but not true unfairness. It is believed throughout the professional world that an individual is bound to secrecy within their work sector. Discussions of whistleblowing and employee loyalty usually assume that the concept of loyalty is irrelevant to the issue or, more commonly, that whistleblowing involves a moral choice that pits the loyalty that an employee owes an employer against the employee's responsibility to serve the public interest.[50] Robert A. Larmer describes the standard view of whistleblowing in the Journal of Business Ethics by explaining that an employee possesses prima facie (based on the first impression; accepted as correct until proved otherwise) duties of loyalty and confidentiality to their employers and that whistleblowing cannot be justified except on the basis of a higher duty to the public good.[50] It is important to recognize that in any relationship which demands loyalty the relationship works both ways and involves mutual enrichment.[51]		The ethics of Edward Snowden's actions have been widely discussed and debated in news media and academia worldwide.[52] Edward Snowden released classified intelligence to the American people in an attempt to allow Americans to see the inner workings of the government. A person is diligently tasked with the conundrum of choosing to be loyal to the company or to blow the whistle on the company's wrongdoing. Discussions on whistleblowing generally revolve around three topics: attempts to define whistleblowing more precisely, debates about whether and when whistleblowing is permissible, and debates about whether and when one has an obligation to blow the whistle.[53]		Many whistleblowers have stated that they were motivated to take action to put an end to unethical practices, after witnessing injustices in their businesses or organizations.[54] A 2009 study found that whistleblowers are often motivated to take action when they notice a sharp decline in ethical practices, as opposed to a gradual worsening.[55] There are generally two metrics by which whistleblowers determine if a practice is unethical. The first metric involves a violation of the organization's bylaws or written ethical policies. These violations allow individuals to concretize and rationalize blowing the whistle.[56] On the other hand, "value-driven" whistleblowers are influenced by their personal codes of ethics. In these cases, whistleblowers have been criticized for being driven by personal biases.[57]		In addition to ethics, social and organizational pressure are a motivating forces. A 2012 study identified that individuals are more likely to blow the whistle when several others know about the wrongdoing, because they would otherwise fear consequences for keeping silent.[58] In cases when one person is causing an injustice, the individual who notices the injustice may file a formal report, rather than confronting the wrongdoer, because confrontation would be more emotionally and psychologically stressful.[59][60][61] Furthermore, individuals may be motivated to report unethical behavior when they believe their organizations will support them.[62] Professionals in management roles may feel responsibility to blow the whistle in order to uphold the values and rules of their organizations.[63]		Legal protection for whistleblowing varies from country to country and may depend on the country of the original activity, where and how secrets were revealed, and how they eventually became published or publicized. Over a dozen countries have now adopted comprehensive whistleblower protection laws that create mechanisms for reporting wrongdoing and provide legal protections to whistleblowers. Over 50 countries have adopted more limited protections as part of their anti-corruption, freedom of information, or employment laws.[64] For purposes of the English Wikipedia, this section emphasizes the English-speaking world and covers other regimes only insofar as they represent exceptionally greater or lesser protections.		To be a whistleblower takes bravery. Barry Adams explains the options as, "The list of negative consequences to whistleblowing seems endless: broken promises to fix the problem, disillusionment, isolation, humiliation, formation of an 'anti-you' group, loss of job, questioning of the whistleblower's mental health, vindictive tactics to make the individual's work more difficult and/or insignificant, assassination of character, formal reprimand, and difficult court proceedings." (Ahern & McDonald, 2002; Brodie, 1998; Fletcher et al., 1998; Wilmot, 2000) It all depends on the level of what information one reveals and to whom. The consequences vary from a corporate whistleblower to federal whistleblower. The 1st and 14th amendment protects to an extent and guarantees protection for practical allegations from retaliation. Lachman, V. D. (2008). Retrieved March 30, 2016, from http://www.medscape.com/viewarticle/582797_6		There are laws in a number of states.[65] The former NSW Police Commissioner Tony Lauer summed up official government and police attitudes as: "Nobody in Australia much likes whistleblowers, particularly in an organization like the police or the government."		Whistleblowers Australia is an association for those who have exposed corruption or any form of malpractice, especially if they were then hindered or abused.[66]		The Office of the Public Sector Integrity Commissioner of Canada (PSIC) provides a safe and confidential mechanism enabling public servants and the general public to disclose wrongdoings committed in the public sector. It also protects from reprisal public servants who have disclosed wrongdoing and those who have cooperated in investigations. The Office's goal is to enhance public confidence in Canada's federal public institutions and in the integrity of public servants.[67]		Mandated by the Public Servants Disclosure Protection Act (The Act), PSIC is a permanent and independent Agent of Parliament. The Act, which came into force on April 15, 2007, applies to most of the federal public sector, approximately 400,000 public servants.[68] This includes government departments and agencies, parent Crown corporations, the Royal Canadian Mounted Police and other federal public sector bodies.		Not all disclosures lead to an investigation as the Act sets out the jurisdiction of the Commissioner and gives the option not to investigate under certain circumstances. On the other hand, if PSIC conducts an investigation and finds no wrongdoing was committed, the Commissioner must report his findings to the discloser and to the organization's chief executive. Also, reports of founded wrongdoing are presented before the House of Commons and the Senate in accordance with The Act. As of June 2014, a total of 9 reports have been tabled in Parliament.[69]		The Act also established the Public Servants Disclosure Protection Tribunal (PSDPT) to protect public servants by hearing reprisal complaints referred by the Public Sector Integrity Commissioner. The Tribunal can grant remedies in favour of complainants and order disciplinary action against persons who take reprisals.		PSIC's current Commissioner is Mr. Mario Dion. Previously, he has served in various senior roles in the public service, including as Associate Deputy Minister of Justice, Executive Director and Deputy Head of the Office of Indian Residential Schools Resolution of Canada, and as Chair of the National Parole Board.		In Jamaica, the Protected Disclosures Act, 2011[70] received assent in March 2011. It creates a comprehensive system for the protection of whistleblowers in the public and private sector. It is based on the Public Interest Disclosure Act 1998.		The Government of India has been considering adopting a whistleblower protection law for several years. In 2003, the Law Commission of India recommended the adoption of the Public Interest Disclosure (Protection of Informers) Act, 2002.[71] In August 2010, the Public Interest Disclosure and Protection of Persons Making the Disclosures Bill, 2010 was introduced into the Lok Sabha, lower house of the Parliament of India.[72] The Bill was approved by the cabinet in June, 2011. The Public Interest Disclosure and Protection of Persons Making the Disclosures Bill, 2010 was renamed as The Whistleblowers' Protection Bill, 2011 by the Standing Committee on Personnel, Public Grievances, Law and Justice.[73] The Whistleblowers' Protection Bill, 2011 was passed by the Lok Sabha on 28 December 2011.[74] and by the Rajyasabha on 21 February 2014. The Whistle Blowers Protection Act, 2011 has received the Presidential assent on May 9, 2014 and the same has been subsequently published in the official gazette of the Government of India on May 9, 2014 by the Ministry of Law and Justice, Government of India.		The government of Ireland committed to adopting a comprehensive whistleblower protection law in January 2012. The bill reportedly covers both the public and private sectors.[75]		The Netherlands has measures in place to mitigate the risks of whistleblowing: the whistleblower advice centre (Adviespunt Klokkenluiders) offers advice to whistleblowers, and the Parliament recently passed a proposal to establish a so-called house for whistleblowers, to protect them from the severe negative consequences that they might endure (Kamerstuk, 2013). Dutch media organizations also provide whistleblower support; on 9 September 2013[76] a number of major Dutch media outlets supported the launch of Publeaks, which provides a secure website for people to leak documents to the media. Publeaks is designed to protect whistleblowers. It operates on the GlobaLeaks software developed by the Hermes Center for Transparency and Digital Human Rights, which supports whistleblower-oriented technologies internationally.[77]		The Swiss Council of States agreed on a draft amendment of the Swiss Code of Obligations in September 2014. The draft introduces articles 321abis to 321asepties, 328(3), 336(2)(d).[78] An amendment of article 362(1) adds articles 321abis to 321asepties to the list of provisions that may not be overruled by labour and bargaining agreements. Article 321ater introduces an obligation on employees to report irregularities to their employer before reporting to an authority. An employee will, however, not breach his duty of good faith if he reports an irregularity to an authority and		Article 321aquarter provides that an employee may exceptionally directly report to an authority. Exceptions apply in cases		The draft does not improve on protection against dismissal for employees who report irregularities to their employer.[79] The amendment does not provide for employees anonymously filing their observations of irregularities.		Whistleblowing in the United Kingdom is subject to the Public Interest Disclosure Act (PIDA) 1998.		The Freedom to Speak Up Review set out 20 principles to bring about improvements to help whistleblowers in the NHS, including:[80]		Monitor produced a whistleblowing policy in November 2015 that all NHS organizations in England are obliged to follow. It explicitly says that anyone bullying or acting against a whistleblower could be potentially liable to disciplinary action.[81]		Whistleblowing tradition in the United States started with Benjamin Franklin leaking a few letters Hutchinson had written to Thomas Whately, resulting in the American Revolution.		To be considered a whistleblower in the United States, most federal whistleblower statutes require that federal employees have reason to believe their employer violated some law, rule, or regulation; testify or commence a legal proceeding on the legally protected matter; or refuse to violate the law.		In cases where whistleblowing on a specified topic is protected by statute, U.S. courts have generally held that such whistleblowers are protected from retaliation.[82] However, a closely divided U.S. Supreme Court decision, Garcetti v. Ceballos (2006) held that the First Amendment free speech guarantees for government employees do not protect disclosures made within the scope of the employees' duties.		In the United States, legal protections vary according to the subject matter of the whistleblowing, and sometimes the state where the case arises.[83] In passing the 2002 Sarbanes–Oxley Act, the Senate Judiciary Committee found that whistleblower protections were dependent on the "patchwork and vagaries" of varying state statutes.[84] Still, a wide variety of federal and state laws protect employees who call attention to violations, help with enforcement proceedings, or refuse to obey unlawful directions.		The first US law adopted specifically to protect whistleblowers was the 1863 United States False Claims Act (revised in 1986), which tried to combat fraud by suppliers of the United States government during the American Civil War. The Act encourages whistleblowers by promising them a percentage of the money recovered by the government and by protecting them from employment retaliation.[85]		Another US law that specifically protects whistleblowers is the Lloyd–La Follette Act of 1912. It guaranteed the right of federal employees to furnish information to the United States Congress. The first US environmental law to include an employee protection was the Clean Water Act of 1972. Similar protections were included in subsequent federal environmental laws, including the Safe Drinking Water Act (1974), Resource Conservation and Recovery Act (1976), Toxic Substances Control Act of 1976, Energy Reorganization Act of 1974 (through 1978 amendment to protect nuclear whistleblowers), Comprehensive Environmental Response, Compensation, and Liability Act (CERCLA, or the Superfund Law) (1980), and the Clean Air Act (1990). Similar employee protections enforced through OSHA are included in the Surface Transportation Assistance Act (1982) to protect truck drivers, the Pipeline Safety Improvement Act (PSIA) of 2002, the Wendell H. Ford Aviation Investment and Reform Act for the 21st Century ("AIR 21"), and the Sarbanes–Oxley Act, enacted on July 30, 2002 (for corporate fraud whistleblowers).		Investigation of retaliation against whistleblowers under 20 federal statutes falls under the jurisdiction of the Office of the Whistleblower Protection Program[86] of the United States Department of Labor's[87] Occupational Safety and Health Administration (OSHA).[88] New whistleblower statutes enacted by Congress, which are to be enforced by the Secretary of Labor, are generally delegated by a Secretary's Order[89] to OSHA's Office of the Whistleblower Protection Program (OWPP).		The patchwork of laws means that victims of retaliation need to be aware of the laws at issue to determine the deadlines and means for making proper complaints. Some deadlines are as short as 10 days (Arizona State Employees have 10 days to file a "Prohibited Personnel Practice" Complaint before the Arizona State Personnel Board), while others are up to 300 days.		Those who report a false claim against the federal government, and suffer adverse employment actions as a result, may have up to six years (depending on state law) to file a civil suit for remedies under the US False Claims Act (FCA).[90] Under a qui tam provision, the "original source" for the report may be entitled to a percentage of what the government recovers from the offenders. However, the "original source" must also be the first to file a federal civil complaint for recovery of the federal funds fraudulently obtained, and must avoid publicizing the claim of fraud until the US Justice Department decides whether to prosecute the claim itself. Such qui tam lawsuits must be filed under seal, using special procedures to keep the claim from becoming public until the federal government makes its decision on direct prosecution.		The Espionage Act of 1917 has been used to prosecute whistleblowers in the United States including Edward Snowden and Chelsea Manning. In 2013, Manning was convicted of violating the Espionage Act and sentenced to 35 years in prison for leaking sensitive military documents to WikiLeaks.[91] The same year, Snowden was charged with violating the Espionage Act for releasing confidential documents belonging to the NSA.[92]		Section 922 of the Dodd–Frank Wall Street Reform and Consumer Protection Act (Dodd-Frank) in the United States incentivizes and protects whistleblowers.[93] By Dodd-Frank, the U.S. Securities and Exchange Commission (SEC) financially rewards whistleblowers for providing original information about violations of federal securities laws that results in sanctions of at least $1M.[94][95][96] Common violations of federal securities laws that could lead to an SEC award include: accounting fraud at public companies,[97] investment and securities fraud,[98] insider trading,[99] foreign bribery and other violations of the Foreign Corrupt Practices Act,[100] EB-5 investment fraud,[101] Ponzi schemes,[102] unregistered securities offerings,[103] and deceptive non-GAAP financial measures in the SEC filings of public companies.[104] As of May 31, 2017, the SEC has issued more than $154 million to whistleblowers.[105]		Additionally, Dodd-Frank offers job security to whistleblowers by illegalizing termination or discrimination due to whistleblowing.[94][106][107] The whistleblower provision has proven successful; after the enactment of Dodd-Frank, the SEC charged KBR (company) and BlueLinx Holdings Inc. (company) with violating the whistleblower protection Rule 21F-17 by having employees sign confidentiality agreements that threatened repercussions for discussing internal matters with outside parties.[108][109][110] As of his recent election, President Donald Trump has announced plans to dismantle Dodd-Frank, which may negatively impact whistleblower protection in the United States.[111]		The federally recognized National Whistleblower Appreciation Day is observed annually on July 30th, on the anniversary of the country's original 1778 whistleblower protection law.		There are comprehensive laws in New Zealand and South Africa. A number of other countries have recently adopted comprehensive whistleblower laws including Ghana, South Korea, and Uganda. They are also being considered in Kenya and Rwanda. The European Court of Human Rights ruled in 2008 that whistleblowing was protected as freedom of expression. And in February 2017, Nigeria also set up the whistleblowing policy against corruption and other ills in the country.[112]		Many NGOs advocate for stronger and more comprehensive legal rights and protections for whistleblowers. Among them are the Government Accountability Project (GAP), Blueprint for Free Speech, Public Concern at Work (PCaW) and the Open Democracy Advice Center (ODAC). Among the more publicly visible whistleblower activists are Thomas M. Devine of GAP, Cathy James of PCaW, Mark Worth of Blueprint for Free Speech.		Whistleblowers that may be at risk of those they are exposing are now using encryption methods and anonymous content sharing software to protect their identity. Tor, a highly accessible anonymity network, is one that is frequently used by whistleblowers around the world.[113] Tor has undergone a number of large security updates to protect the identities of potential whistleblowers who may wish to anonymously leak information.[114]		Recently specialized whistleblowing software like SecureDrop and GlobaLeaks has been built on top of the Tor technology in order to incentivize and simplify its adoption for secure whistleblowing.[115][116]		Many large companies are using OpenBoard to offer an anonymous way for employees, vendors and suppliers to report any questionable activity. The identity and ownership of OpenBoard forks is not obvious and the security of the service is not apparent.[citation needed]		Some examples of companies using OpenBoard are listed below.		In 2016, the rock band Thrice released a song titled "Whistleblower" off of the album To Be Everywhere Is to Be Nowhere. The song is written from the perspective of Snowden.[117]		See also: Daniel Ellsberg-related films		
In Australia and New Zealand, long service leave (LSL) is an employee entitlement to an additional vacation on full pay after an extended period of service with an employer. In Australia, employees are generally entitled to long service leave over and above their annual leave if they work for a particular employer for a certain length of time. A common entitlement in Australia is that employees who remain with the one employer for ten years are entitled to two calendar months (eight and two-thirds weeks) paid LSL, less on a pro rata basis, the longer they stay with that employer. When a worker ceases work with an employer, he or she is usually entitled to be paid the amount of LSL entitlement not taken on termination on a pro rata basis, though usually after a minimum period of service.		It remains one of the great entitlements for working Australians and one that is peculiar to the Australian labour market. The rules governing long service leave entitlements vary for different employees depending on their circumstances and the relevant jurisdiction. Currently, annual leave entitlements are covered by the state or territory law in which the employee is employed.		The Institute of Actuaries of Australia estimated that the total value of long service leave benefits in Australia was around $16.5bn in 2001.[1]		There has been a debate in Australia about the protection of employee entitlements (including long service leave) in the event of employer insolvency, with some high-profile cases involving employees losing benefits that had been accrued.[citation needed]		Nowadays, long service leave is ingrained in Australian culture and is specified by state based and some federal legislation. It is often not taken when it falls due, leading to calls to reduce long-service entitlement in the public sector.[2]						Workers in Australia are entitled to long service leave based on legislation of the relevant state or territory, as follows:		Within a limited number of industries, such as construction, coal-mining, contract-cleaning industries and the public sector, it is possible to transfer long-service leave entitlements from one employer to another, as long as the employee remains in the same state. Known as portable long service leave this is done mostly through specific legislated schemes which employers in those industries pay into, and which administer the funds for employees.[4]		The Australian Senate has recently moved to inquire into portable long service leave schemes. The inquiry will be conducted by the Education and Employment References Committee. The committee will consider how portable schemes might be structured and what role the Australian Federal Government might play in helping to establish a scheme. The committee will also have to evaluate the effect that the differing State long service entitlements will have on a national scheme, as the current state based long service leave provisions are all practically different. As of 11 November 2015, the committee had yet to meet and set dates for submissions and reporting etc., as nearly half of the members have been away on long service leave.		Long service leave is a benefit peculiar to Australia and New Zealand (and possibly some public servants in India) and relates to their colonial heritage. There is a similar system of sabbatical leaves also in Finland.[5] Long service leave developed from the concept of furlough, which stems from the Dutch word verlof (meaning leave) and its usage originates in leave granted from military service.[citation needed]		Long service leave was introduced in Australia in the 1860s. The idea was to allow civil servants the opportunity to sail home to England after 10 years’ service in ‘the colonies’. It was 13 weeks for every ten years of service, composed of five weeks to sail back to England, three weeks of leave and five weeks to sail back.		In the 19th century, furlough as a benefit as it is now known, was a privilege granted by legislation to the colonial and Indian Services. In Australia, the benefits were first granted to Victorian and South Australian civil servants. The nature of the leave allowed civil servants to sail 'home' to England, safe in the knowledge that they were able to return to their positions upon their return to Australia.[citation needed]		The concept spread beyond the public service over the period 1950 to 1975, mainly as a result of pressure from employees seeking comparability with the public service.		
Industrial noise, or occupational noise, is often a term used in relation to environmental health and safety, rather than nuisance, as sustained exposure can cause permanent hearing damage. Industrial noise or occupational noise is the amount of acoustical energy (noise) received by an employees auditory system while they are working.		"Twenty-two million workers are exposed to potentially damaging noise at work each year. Last year, U.S. business paid more than $1.5 million in penalties for not protecting workers from noise." - OSHA[2]		Industrial noise is an occupational hazard linked to traditionally loud industries such as ship-building, Mining, railroad work, Welding and Construction. Industrial noise, if experienced repeatedly, at a high intensity, for an extended period of time, can cause noise-induced hearing loss (NIHL). NIHL caused by industrial noise can be classified as occupational hearing loss.		Modern thinking in occupational safety and health further identifies noise as hazardous to workers' safety and health. This hazard is experienced in various places of employment and through a variety of sources.		Noise, in the context of industrial noise, is hazardous to a persons hearing because of its loud intensity through repeated long-term exposure. In order for Noise to cause Hearing impairment for the worker, the noise has to be close enough, loud enough and the listener has to be exposed for long enough. These factors have been taken into account by the governing occupational health and safety organizations as they determine the unsafe noise exposure levels and durations for their respective industries.		National Institute for Occupational Safety and Health (NIOSH), Occupational Safety and Health Administration (OSHA), Mine Safety and Health Administration (MSHA), Federal Railroad Administration (FRA) have all set standards on hazardous occupational noise in their respective industries.[3] Each industry is different, as workers tasks and equipment differ, but most regulations agree that noise becomes hazardous when it exceeds 85 Decibel, for an 8-hour exposure (typical work shift). This relationship between allotted noise level and exposure time is known as an Exposure action value (EAV) or Permissible exposure limit (PEL). The EAV or PEL can be seen as equations which manipulate the allotted exposure time according to the intensity of the industrial noise. This equation works as an inverse relationship. As the industrial noise intensity increases, the allotted exposure time, to still remain safe, decreases.		These above calculations of PEL and EAV are based on measurements taken to determine the intensity of that particular industrial noise. A-weighted measurements are commonly used to determine noise levels that can cause harm to the human ear. There are also special exposure meters available that integrate noise over a period of time to give an Leq value (equivalent sound pressure level), defined by standards.		Hazardous industrial noise can cause a permanent auditory threshold shift as excessive exposure to loud noises can damage the Hair cells in the ear. Please see Occupational hearing loss or Noise-induced hearing loss for more information regarding the physiology of hearing loss.		Noise can also effect the safety of the employee and the safety of others. Noise can be a causal factor in work accidents, both by masking hazards and warning signals, and by impeding concentration. High intensity noise can interfere with vital workplace communication which increases the chance of accidents and decreases productivity.[4]		Noise acts synergistically with other hazards to increase the risk of harm to workers. In particular, noise and toxic materials (e.g. some solvents, metals, asphyxiants and pesticides) have some ototoxic properties may also affect the hearing function.[5][6]						There are several ways to limit your exposure to hazardous industrial noise. There is a hierarchy of controls[7] which companies have to abide by if their employees are exposed to a hazardous amount of noise. First, the company can eliminate the noise source. If the noise source can not be eliminated or switched out, the company must try to engineer that noise out. This process is called acoustic quieting.		Acoustic quieting is the process of making machinery quieter by damping vibrations to prevent them from reaching the observer. The company can isolate the certain piece of machinery by placing materials on the machine or in between the machine and the worker to decreases the signal intensity that reaches the workers ear.		Noise decreases as distance from its source increases. When two identical noise sources are side by side producing a recorded noise of, say, 100 dB(A) the reduction in noise from removing one of the noise sources is about 3 dB, resulting in 97 dB(A). When the distance to a noise source is doubled the recorded noise level is reduced by 6 dB, sometimes called the Rule of 6.		The noise attenuation in decibels at a distance from the source d {\displaystyle d} , knowing the SPL at distance d 0 {\displaystyle d_{0}} , is 20 l o g 10 ( d d 0 ) {\displaystyle 20log_{10}\left({\frac {d}{d_{0}}}\right)} . If the distance is doubled, i.e. ( d d 0 ) = 2 {\displaystyle \left({\frac {d}{d_{0}}}\right)=2} , the attenuation becomes 6.02 dB (6 for most practical purposes).Experts have developed a number of standards for noise reduction and isolation of its sources from people.[8][9][10][11][12][13]		To decrease the employees exposure to hazardous, the company can take administrative controls limiting the employee's exposure time. This can be done by changing work shifts and switching employees out from the noise exposure area. Lastly, to decrease industrial noise exposure, Personal protective equipment should be used. There are several types of Earplug that can be used to attenuate the noise down to a safe level.		For a more detailed description of the hierarchy of controls, please see Occupational hearing loss.		In the United States, the National Institute for Occupational Safety and Health (NIOSH) and the Occupational Safety and Health Administration (OSHA) work together to provide standards and regulations for noise in the workplace.[14][15] Industrial noise can also be regulated by legislation. A 2012 Cochrane review found low-quality evidence that legislation reduced industrial noise both immediately and in the long-term.[16]		Since the hazards of occupational noise exposure were realized, programs and initiatives such as the US Buy Quiet program have been set up to regulate or discourage noise exposure. The Buy Quiet initiative promotes the purchase of quieter tools and equipment and encourages manufacturers to design quieter equipment.[17] Additionally, the Safe-In-Sound Award was created to recognize successes in hearing loss prevention programs or initiatives.[18]		General:		
The workweek and weekend are those complementary parts of the week devoted to labour and rest, respectively. The legal working week (British English), or workweek (American English), is the part of the seven-day week devoted to labour. In most of the Western world, it is Monday to Friday; the weekend is Saturday and Sunday. A weekday is any day of the working week. Other institutions often follow the same days, such as places of education.		In some Christian traditions, Sunday is the "day of rest and worship". Jewish Shabbat or Biblical Sabbath lasts from sunset on Friday to the fall of full darkness on Saturday; as a result, the weekend in Israel is observed on Friday–Saturday. also Muslim-majority countries have Friday–Saturday weekend		The Christian Sabbath was just one day each week, but the preceding day (the Jewish Sabbath) came to be taken as a holiday as well in the twentieth century. This shift has been accompanied by a reduction in the total number of hours worked per week, following changes in employer expectations. The present-day concept of the 'week-end' first arose in the industrial north of Britain in the early part of nineteenth century[1] and was originally a voluntary arrangement between factory owners and workers allowing Saturday afternoon off from 2pm in agreement that staff would be available for work sober and refreshed on Monday morning.[2] The Amalgamated Clothing Workers of America Union was the first to successfully demand a five-day work week in 1929.		Most countries have adopted a two-day weekend, however, the days of the weekend differ according to religious tradition, i.e. either Thursday–Friday, Friday–Saturday, or Saturday–Sunday. Proposals have continued to be put forward for further reductions in the number of days or hours worked per week, on the basis of predicted social and economic benefits.		A continuous seven-day cycle that runs throughout history paying no attention whatsoever to the phases of the moon, having a fixed day of rest, was probably first practiced in Judaism, dated to the 6th century BC at the latest.[3][4]		In Ancient Rome, every eight days there was a nundinae. It was a market day, during which children were exempted from school[5] and plebs ceased from work in the field and came to the city to sell the produce of their labor[6][7] or practise religious rites.[citation needed].		The French Revolutionary Calendar had ten-day weeks (called décades) and allowed décadi, one out of the ten days, as a leisure day.		In cultures with a four-day week, the three Sabbaths derives from the culture's main religious tradition: Friday (Muslim), Saturday (Jewish), and Sunday (Christian).		The present-day concept of the relatively longer 'week-end' first arose in the industrial north of Britain in the early part of nineteenth century[8] and was originally a voluntary arrangement between factory owners and workers allowing Saturday afternoon off from 2pm in agreement that staff would be available for work sober and refreshed on Monday morning.[9] The Oxford English Dictionary traces the first use of the term weekend to the British magazine Notes and Queries in 1879.[10]		In 1908, the first five-day workweek in the United States was instituted by a New England cotton mill so that Jewish workers would not have to work on the Sabbath from sundown Friday to sundown Saturday.[11] In 1926, Henry Ford began shutting down his automotive factories for all of Saturday and Sunday. In 1929, the Amalgamated Clothing Workers of America Union was the first union to demand a five-day workweek and receive it. After that, the rest of the United States slowly followed, but it was not until 1940, when a provision of the 1938 Fair Labor Standards Act mandating a maximum 40-hour workweek went into effect, that the two-day weekend was adopted nationwide.[11]		Over the succeeding decades, particularly in the 1940s, 1950s, and 1960s, an increasing number of countries adopted either a Friday–Saturday or Saturday–Sunday weekend to harmonize with international markets. A series of workweek reforms in the mid-to-late 2000s and early 2010s brought much of the Arab World in synchronization with the majority of countries around the world, in terms of working hours, the length of the workweek, and the days of the weekend. The International Labour Organization (ILO) currently defines a workweek exceeding 48 hours as excessive. A 2007 study by the ILO found that at least 614.2 million people around the world were working excessive hours.[12]		Actual workweek lengths have been falling in the developed world. Every reduction of the length of the workweek has been accompanied by an increase in real per-capita income.[13][verification needed] In the United States, the workweek length reduced slowly from before the Civil War to the turn of the 20th century. A rapid reduction took place from 1900 to 1920, especially between 1913 and 1919, when weekly hours fell by about eight percent.[14] In 1926, Henry Ford standardized on a five-day workweek, instead of the prevalent six days, without reducing employees' pay.[15] Hours worked stabilized at about 49 per week during the 1920s, and during the Great Depression fell below 40.[14] During the Depression, President Herbert Hoover called for a reduction in work hours in lieu of layoffs. Later, President Franklin Roosevelt signed the Fair Labor Standards Act of 1938, which established a five-day, 40-hour workweek for many workers.[15] The proportion of people working very long weeks has since risen, and the full-time employment of women has increased dramatically.[16]		The New Economics Foundation has recommended moving to a 21-hour standard workweek to address problems with unemployment, high carbon emissions, low well-being, entrenched inequalities, overworking, family care, and the general lack of free time.[15][17][18][19][20][21] The Center for Economic and Policy Research states that reducing the length of the work week would slow climate change and have other environmental benefits.[22]		(Countries listed alphabetically. Some countries have Saturday a normal school day. Some countries appear under the subsections for Muslim countries and the European Union.)		Else: Saturday-Thursday		Monday–Saturday		Else: Monday–Saturday		In Australia the working week begins on Monday and terminates on Friday. An eight-hour working day is the norm. Working three weekdays a fortnight, for example, would therefore be approximately twenty-four hours (including or excluding traditional breaks tallying up to two hours). Some people work overtime with extra pay on offer for those that do, especially for weekend work. Shops open seven days a week in most states with opening hours from 9am to 5.30pm on weekdays, with some states having two "late night trading" nights on Thursday and Friday, when trading ceases at 9pm. Many supermarkets and low end department stores remain open until midnight and some trade 24/7. Restaurants and cinemas can open at all hours, save for some public holidays. Bars generally trade seven days a week but there are local municipal restrictions concerning trading hours. Banks trade on Monday to Friday, with some branches opening on Saturdays (and in some cases Sundays) in high demand areas. The Post Office (Australia Post) trades Monday to Friday as per retail shops but some retail post offices may trade on Saturdays and Sundays in some shopping centres. A notable exception to the above is South Australia whereby retail establishments are restricted to trading between the hours of 11am-5pm on Sundays.		As a general rule, Brazil adopts a 44-hour working week, which typically begins on Monday and ends on Friday, with a Saturday-Sunday weekend. Brazilian Law,[32] however, also allows for shorter Monday-to-Friday working hours so employees can work on Saturdays or Sundays, as long as the weekly 44-hour limit is respected and the employee gets at least one weekend day. This is usually the case for malls, supermarkets and shops. The law also grants labor unions the right to negotiate different work weeks, within certain limits, which then become binding for that union's labor category. Overtime is allowed, limited to two extra hours a day, with an increase in pay.		The working week in Chile averages 45 hours, most often worked on a Monday-Friday schedule, but is not uncommon to work on Saturdays. Retail businesses mostly operate Monday through Saturday, with larger establishments being open seven days a week.		In China, there is a five-day Monday-Friday working week, prior to which work on Saturday was standard. China began the two-day Saturday–Sunday weekend on May 1, 1995. Most government employees work 5 days a week (including officials and industrial management). Most manufacturing facilities operate on Saturdays as well. However, most shops, museums, cinemas and commercial establishments open on Saturdays, Sundays and holidays. Banks are also open throughout the weekend and on most public holidays.		During the period of public holidays, swapped holidays are common between the actual holiday and weekend, so a three-day or seven-day holiday periods are created. The nearby Saturday or Sunday may be changed as a normal working day. For example, on a three-day holiday period, if the actual holiday falls on a Tuesday, Monday will be swapped as a holiday, and citizens are required to work on the previous Saturday.		A number of provinces and municipalities across China, including Hebei, Jiangxi and Chongqing, have issued new policies, calling on companies to create 2.5-day weekends. Under the plan, government institutions, State-owned companies, joint-ventures and privately held companies are to be given incentives to allow their workers to take off at noon on Friday before coming back to the office on Monday.[33]		In Hong Kong, the working week begins on Monday and ends on Saturday, while Saturday is usually a half day, and Sunday is a rest day. Most of the local enterprises operate on Saturday, while manufacturing facilities and construction site premises often run on a full-day. However, the five-day working week has been encouraged by the Government in 2006, most of multinational enterprises and some private companies run a five-day working week as well. Besides, a number of employees also have alternative Saturdays off.		Most commercial establishments, shops, museums, libraries and cinemas open on Saturday, Sunday and most public holidays. On the other hand, banks and mail offices open on Saturday morning in general. For schools, normal lessons are seldom held on Saturdays, but students may be required to go school on Saturdays for extra-curricular activities or make-up classes.		Most office jobs work on Monday to Friday 9:00 a.m. to 6:00 p.m. (1:00 to 2:00 p.m. for lunch break) and Saturday 9:00 a.m. to 1:00 p.m. usually. However, it is noted that employees commonly have to work overtime for several hours per day.		In general, Colombia has a 48-hour working week. Depending on the business, people work five days for max 8 hours per day, typically Monday to Friday, or six days for eight hours a day, Monday to Saturday.[34]		In Europe, the standard full-time working week begins on Monday and ends on Saturday. Most retail shops are open for business on Saturday. In Ireland, Italy, Finland, Sweden, the Netherlands and the former socialist states of Europe, large shopping centres open on Sunday. In European countries such as Germany, there are laws regulating shop hours. With exceptions, shops must be closed on Sundays and from midnight until the early morning hours of every day.		The working week is Monday to Friday 8 hours per day. Shops are open on Saturday. By law, almost no shop is open on Sunday. However, exceptions have been made, for example for bakeries, petrol stations and shops at railway stations, especially in the largest cities (Vienna, Graz, Salzburg, Linz).		The working week is Monday to Friday. Working time must not exceed 8 hours per day and 38 hours per week (on average, annualised).		The working week is Monday to Friday, eight hours per day, forty hours per week. Most pharmacies, shops, bars, cafés, and restaurants will operate on Saturdays and Sundays.		The working week is Monday to Friday, seven and a half hours per day (+ 30 minutes lunch break), 37.5 hours per week (or 40 hours per week if lunch breaks are included as working hours). Most pharmacies, shops, bars, cafés, and restaurants are open on Saturday and Sunday.		In the Czech Republic, full-time employment is usually Monday to Friday, eight hours per day and forty hours per week. Many shops and restaurants are open on Saturday and Sunday, but employees still usually work forty hours per week.		Denmark has an official 37-hour working week, with primary work hours between 6:00 and 18:00, Monday to Friday. In public institutions, a 30-minute lunch break every day is included as per collective agreements, so that the actual required working time is 34.5 hours. In private companies, the 30-minute lunch break is normally not included. The workday is usually 7.5 hours Monday to Thursday and 7 hours on Friday. Some small shops are closed Monday.[35]		In Estonia, the working week begins on Monday and ends on Friday. Usually a working week is forty hours.		In Finland, the working week begins on Monday and ends on Friday. A full-time job is defined by law as being at least 32 and at most forty hours per week. In retail and restaurant occupations, among others, the weekly hours may be calculated as an average over three to ten weeks, depending on the employment contract. Banks and bureaus are closed on weekends. Most shops are open on Saturdays, while some are closed on Sundays.		The standard working week is Monday to Friday. Shops are also open on Saturday. Small shops may close on a weekday (generally Monday) to compensate workers for having worked on Saturday. By law, préfets may authorise a small number of specific shops to open on Sunday such as bars, cafés, restaurants, and bakeries, which are traditionally open every day but only during the morning on Sunday. Workers are not obliged to work on Sunday. School children have traditionally taken Wednesday off, or had only a half day, making up the time either with longer days for the rest of the week or sometimes a half day on Saturday. This practice was made much less common under new legislation rolled out over 2013–14.[36]		The standard working week is Monday to Friday. State jobs are from 07:00 until 15:00. Shops are open generally Mondays-Wednesdays from 09:30–15:00 and then from 17:30–21:00 and Tuesday-Thursday-Fridays 09:30-21:00. Saturdays generally 09:00-15:00. It is very rare for a shop to open on Sunday.		In Hungary the working week begins on Monday and ends on Friday. Full-time employment is usually considered forty hours per week. For office workers, the work day usually begins between 8 and 9 o'clock and ends between 16:00 and 18:00, depending on the contract and lunch time agreements.		The forty-hour workweek of public servants includes lunch time. Their work schedule typically consists of 8.5 hours between Monday and Thursday (from 8:00 to 16:30) and 6 hours on Fridays (8:00–14:00).		Ireland has a working week from Monday to Friday, with core working hours from 09:00 to 17:30. Retail stores are usually open until 21:00 every Thursday. Many grocery stores, especially in urban areas, are open until 21:00 or later, and some supermarkets and convenience stores may open around the clock. Shops are generally open all day Saturday and a shorter day Sunday (usually 10:00–12:00 to 17:00–19:00).		In Italy the 40-hour rule applies: Monday to Friday, 09:00 to 18:00, with a one-hour break for lunch. Sunday is always a holiday; Saturday is seldom a work day at most companies and universities, but it is generally a regular day for elementary, middle, and high schools.		In the past, shops had a break from 13:00 to 16:00 and they were generally open until 19:00/20:00. Working times for shops have been changed recently and now are at the owner's discretion; malls are generally open Tuesday to Sunday 09:00 to 20:00, 15:00 to 20:00 on Monday, with no lunchtime closing.[37]		Latvia has a Monday to Friday working week capped at forty hours.[38] Shops are mostly open on weekends, many large retail chains having full working hours even on Sunday. Private enterprises usually hold hours from 9:00 to 18:00, however government institutions and others may have a shorter working day, ending at 17:00.		The standard working week in Luxembourg is 40 hours per week with 8 hours per day.[39] Monday through Friday is the standard working week, though many shops and businesses open on Saturdays (though for somewhat restricted hours). Trading on Sundays is extremely restricted and generally limited to grocery stores opening on Sunday mornings.[40]		In the Netherlands, the standard working week is Monday to Friday (40 hours).[41] Shops are almost always open on Saturdays and often on Sundays. Coffeeshops have different opening times in every city; for instance, Leiden's coffeeshops are only open from 17:00 until 23:00.		The working week is Monday to Friday; 8 hours per day, 40 hours in total per week. Large malls are open on Saturday and Sunday; many small shops are closed on Sunday.		The working week is Monday to Friday; 8 hours per day, 40 hours in total per week. Street shops are almost always open on Saturday mornings but shopping centres are typically open every day (including Saturdays and Sundays).		The working week is Monday to Friday; 8 hours per day, 40 hours in total per week. Shops are open on Saturday and Sunday. The weekend begins on Friday, and ends on Monday.		The working week is Monday to Friday; 8 hours per day, 40 hours in total per week. The traditional opening hours are 9:00 to 13:00–14:00 and then 15:00–16:00 to 18:00 for most offices and workplaces. Most shops are open on Saturday mornings and many of the larger shopping malls are open all day Saturday and in some cities like Madrid, they are open most Sundays. Some restaurants, bars, and shops are closed Mondays, as Mondays are commonly a slow business day.[42]		In Sweden, the standard working week is Monday to Friday, both for offices and industry workers. The standard workday is eight hours, although it may vary greatly between different fields and businesses. Most office workers have flexible working hours and can largely decide themselves on how to divide these over the week. The working week is regulated by Arbetstidslagen (Work time law) to a maximum of 40 hours per week.[43] The 40-hour-week is however easily bypassed by overtime. The law allows a maximum of 200 hours overtime per year.[44] There is however no overseeing government agency; the law is often cited as toothless.[citation needed]		Shops are almost always open on Saturdays and often on Sundays, supermarkets and shopping centres, so that employees there have to work. Traditionally, restaurants were closed on Mondays if they were opened during the weekend, but this has in recent years largely fallen out of practice. Many museums do however still remain closed on Mondays.		The traditional business working week is from Monday to Friday (35 to 40 hours depending on contract). In retail, and other fields such as healthcare, days off might be taken on any day of the week. Employers can make their employees work every day of a week, although the employer is required to allow each employee breaks of either a continuous period of 24 hours every week or a continuous period of 48 hours every two weeks.		Laws for shop opening hours differ between Scotland and the rest of the UK. In England, Wales, and Northern Ireland, many shops and services are open on Saturdays and increasingly so on Sundays as well. In England and Wales, stores' maximum Sunday opening hours vary according to the total floor space of the store.[45] In Scotland, however, there is no restriction in law on shop opening hours on a Sunday.		Under the EU Working Time Directive, workers cannot be forced to work for more than 48 hours per week on average. However, the UK allows individuals to opt out if they so choose. Individuals can choose to opt in again after opting out, even if opting out was part of their employment contract. It is illegal to dismiss them or treat them unfairly for so doing – but they may be required to give up to 3 months notice to give the employer time to prepare, depending on what their employment contract says.[46]		The minimum holiday entitlement is now 28 days per year, but that can include public holidays, depending on the employee's contract.[47] England & Wales have eight, Scotland has nine, and Northern Ireland has ten permanent public holidays each year.[48][49] The 28 days holiday entitlement means that if the government creates a one-off public holiday in a given year, it is not necessarily a day off and it does not add 1 day to employees' holiday entitlement – unless the employer says otherwise, which some do.		The working week is Monday to Friday. Working time must not exceed 8 hours per day and 40 hours per week (on average, annualised).		The standard working week in India for most office jobs begins on Monday and ends on Saturday. The work schedule is 60 hours per week, Sunday being a rest day. However, most government offices and the software industry follow a 5-day workweek[50]. All major industries along with services like transport, hospitality, healthcare etc. work in shifts.		Central government offices follow a 5-day week. State governments follow half day work on the 1st, 3rd, and 5th Saturdays of each month and rest on the 2nd and 4th Saturdays, except West Bengal's government which follows a Monday–Friday workweek. There is usually no half working day in the private sector and people work in two or three shifts of 8hours each.		Generally establishments other than those having pure desk jobs are open till late evening in most cities, offering more flexibility of time to visitors. Most stores are open for 6 or 7 days a week. Retail shops in malls are open on all days. Doctors are mostly available in morning and evening in their clinics and at hospitals during day. Doctors usually work 12 hours a day, 6 days a week. Senior doctors and surgeons work more. Most visiting doctors attached to hospitals visit on all days.		Many services are open till 8 p.m. or 9 p.m. Most Restaurants are open on all days. Small eateries open early and bigger ones open around 11am. Most eateries close between 9 and 11 p.m. Many highway restaurants called dhabas are open for 24 hours a day. Dhabas are available in large numbers on all major state and national highways; outside city or village limits. Some highway fuel stations are open for 24 hours. Overall India works longer hours in most areas than most of the world and offers more flexibility of time for visitors.		Friday is the Muslim holiday when Jumu'ah prayers take place. Most of the Middle Eastern countries and some other predominantly Muslim countries used to consider Thursday and Friday as their weekend. However, this weekend arrangement is no longer observed (see below). Afghanistan (2015)		Three countries in the Muslim world have Friday as the only weekend day and have a six-day working week.		Following reforms in a number of Arab states in the Persian Gulf in the 2000s and 2010s, the Thursday–Friday weekend was replaced by the Friday–Saturday weekend. This change provided for the Muslim offering of Friday prayers and afforded more work days to coincide with the working calendars of international financial markets.		Other countries with Muslim-majority populations or significant Muslim populations follow the Saturday–Sunday weekend, such as Indonesia, Lebanon (where it is officially unclear whether the population is majority-Muslim or majority-Christian, and in the event large segments of the population are not observant in either religion), Turkey, Tunisia and Morocco. While Friday is a working day, a long midday break is given to allow time for worship.		Brunei Darussalam has a non-contiguous working week, consisting of Monday to Thursday plus Saturday. The days of rest are Friday and Sunday.		Some non-government companies in Brunei adopted the working week of Monday to Friday, while the weekend starts on Saturday until Sunday. Depending on the company rules, employees may be required to work half-day on Saturday.		In Israel, the standard workweek is 43 hours as prescribed by law. The typical workweek is five days, Sunday to Thursday, with 8.6 hours a day as the standard, with anything beyond that considered overtime. A minority of jobs operate on a partial six-day Sunday-Friday workweek, with 8 hours of work per day for the first five days and three hours on Friday as the standard hours before overtime.[62][63][64] A six-day workweek with Friday as a partial workday used to be the standard in Israel.[65][66] Many Israelis work overtime hours, with a maximum of 12 overtime hours a week permitted by law. Most offices and businesses run on a five-day week, though many stores, post offices, banks, and schools are open and public transportation runs six days a week. Almost all businesses are closed during Saturday, and most public services except for emergency services, including almost all public transport, are unavailable on Saturdays. However, some shops, restaurants, cafes, places of entertainment, and factories are open on Saturdays, and a few bus and share taxi lines are active.[67][68][69] Employees who work Saturdays, particularly service industry workers, public sector workers, and pilots, are compensated with alternative days off.[70] In 2014, the average workweek was 45.8 hours for men and 37.1 hours for women.[71] The Israeli standard workweek is expected to be modified in July 2017, with workers allowed to choose between a 42-hour workweek by leaving an hour early one day a week or taking a Sunday off every 4-6 weeks.[72]		The standard business office working week in Japan begins on Monday and ends on Friday, 40 hours per week. This system became common between 1980 and 2000. Before then, most workers in Japan worked full-time from Monday to Friday and a half day on Saturday, 45–48 hours per week. Public schools and facilities (excluding city offices) are generally open on Saturdays for half a day.[73]		Mexico has a 48-hour work week (8 hours × 6 days),[74] it is a custom in most industries and trades to work half day on Saturday. Shops and retailers open on Saturday and Sunday in most large cities.		Mongolia has a Monday to Friday working week, with a normal maximum time of 40 hours. Most shops are also open on weekends, many large retail chains having full opening hours even on Sunday. Private enterprises conduct business from 9:00 to 18:00, and government institutions may have full working hours.		Nepal follows the ancient Vedic calendar, which has the resting day on Saturday and the first day of the working week on Sunday.[75] Schools in Nepal are off on Saturdays, so it is common for pupils to go to school from Sunday to Friday.		In November 2012, the home ministry proposed a two-day holiday per week plan for all government offices except at those providing essential services like electricity, water, and telecommunications.[76] This proposal followed a previous proposal by the Nepali government, i.e. Load-shedding Reduction Work Plan 2069 BS, for a five working day plan for government offices as part of efforts to address the problem of load-shedding. The proposal has been discussed in the Administration Committee; it is not yet clear whether the plan includes private offices and educational institutions.		In New Zealand the working week is typically Monday to Friday 8:30 to 17:00, but it is not uncommon for many industries (especially construction) to work a half day on Saturday, normally from 8:00 or 9:00 to about 13:00. Supermarkets, malls, independent retailers, and increasingly, banks, remain open seven days a week.		In Russia the common working week begins on Monday and ends on Friday with 8 hours per day.		Federal law defines a working week duration of 5 or 6 days with no more than 40 hours worked. In all cases Sunday is a holiday. With a 5-day working week the employer chooses which day of the week will be the second day off. Usually this is a Saturday, but in some organizations (mostly government), it is Monday. Government offices can thereby offer Saturday service to people with a normal working schedule.		There are non-working public holidays in Russia; all of them fall on a fixed date. By law, if such a holiday coincides with an ordinary day off, the next work day becomes a day off. An official public holiday cannot replace a regular day off. Each year the government can modify working weeks near public holidays in order to optimize the labor schedule. For example, if a five-day week has a public holiday on Tuesday or Thursday, the calendar is rearranged to provide a reasonable working week.		Exceptions include occupations such as transit workers, shop assistants, and security guards. In many cases independent schemes are used. For example, the service industry often uses the X-through-Y scheme (Russian: X через Y) when every worker uses X days for work and the next Y days for rest.		In the Soviet Union the standard working week was 41 hours: 8 hours, 12 min. Monday to Friday. Before the mid-1960s there was a 42-hour 6-day standard working week: 7 hours Monday to Friday and 6 hours on Saturday.		In Singapore the common working week is 5-day work week, which runs from Monday to Friday beginning 8:30 a.m. and end at 5 p.m. – 6 p.m. Some companies work a half day on Saturdays. Shops, supermarkets and malls are open seven days a week and on most public holidays.		In South Africa the working week traditionally was Monday to Friday with a half-day on Saturday and Sunday a public holiday. However, since 2013 there have been changes to the working week concept based on more than one variation. The week can be 5 days of work, or more. The maximum number of hours someone can work in a week remains 45.[77]		In Thailand the working week is Monday to Saturday for a maximum of 44 to 48 hours per week (Saturday is usually a half or full day).[citation needed]		However, government offices and some private companies have modernised through enacting the American and European standard of working Monday through Friday.[citation needed]		Currently, 50% of the luxury beach resorts in Phuket have a five-day working week. Of the remaining 50%, 23% have taken steps to reform their 6-day workweek through such measures as reducing the working week from 6 days to 5.5 days.[citation needed]		The standard working week in the United States begins on Monday and ends on Friday, 40 hours per week, with Saturday and Sunday being weekend days. However, in practice, only 42% of employees work 40-hour weeks. The average workweek for full-time employees is 47 hours.[78] Most stores are open for business on Saturday and often on Sunday as well, except in a few places where prohibited by law (see Blue law). Increasingly, employers are offering compressed work schedules to employees. Some government and corporate employees now work a 9/80 work schedule (80 hours over 9 days during a two-week period)—commonly 9 hour days Monday to Thursday, 8 hours on one Friday, and off the following Friday. Jobs in healthcare, law enforcement, transportation, retail, and other service positions commonly require employees to work on the weekend or to do shift work.[79]		Vietnam has a standard 48-hour six-day workweek. Monday to Friday are full workdays and Saturday is a partial day. Work typically begins at 8:00 AM and lasts until 5:00 PM from Monday to Friday and until 12:00 PM on Saturdays. This includes a one-hour lunch break. Government offices and banks follow a five-day workweek and are thus closed on Saturday.[80][81]		
Training is teaching, or developing in oneself or others, any skills and knowledge that relate to specific useful competencies. Training has specific goals of improving one's capability, capacity, productivity and performance. It forms the core of apprenticeships and provides the backbone of content at institutes of technology (also known as technical colleges or polytechnics). In addition to the basic training required for a trade, occupation or profession, observers of the labor-market recognize as of 2008[update] the need to continue training beyond initial qualifications: to maintain, upgrade and update skills throughout working life. People within many professions and occupations may refer to this sort of training as professional development						Physical training concentrates on mechanistic goals: training programs in this area develop specific skills or muscles, often with a view of peaking at a particular time. Some physical training programs focus on raising overall physical fitness.		In military use, training means gaining the physical ability to perform and survive in combat, and learning the many skills needed in a time of war. These include how to use a variety of weapons, outdoor survival skills, and how to survive being captured by the enemy, among many others. See military education and training.		For psychological or physiological reasons, people who believe it may be beneficial to them can choose to practice relaxation training, or autogenic training, in an attempt to increase their ability to relax or deal with stress.[1] While some studies have indicated relaxation training is useful for some medical conditions, autogenic training has limited results or has been the result of few studies.		Some commentators use a similar term for workplace learning to improve performance: "training and development". There are also additional services available online for those who wish to receive training above and beyond that which is offered by their employers. Some examples of these services include career counseling, skill assessment, and supportive services.[2] One can generally categorize such training as on-the-job or off-the-job.		The on-the-job training method takes place in a normal working situation, using the actual tools, equipment, documents or materials that trainees will use when fully trained. On-the-job training has a general reputation as most effective for vocational work[citation needed].It involves employee training at the place of work while he or she is doing the actual job. Usually, a professional trainer (or sometimes an experienced employee) serves as the course instructor using hands-on training often supported by formal classroom training. Sometimes training can occur by using web-based technology or video conferencing tools.		Simulation based training is another method which uses technology to assist in trainee development. This is particularly common in the training of skills requiring a very high degree of practice, and in those which include a significant responsibility for life and property. An advantage is that simulation training allows the trainer to find, study, and remedy skill deficiencies in their trainees in a controlled, virtual environment. This also allows the trainees an opportunity to experience and study events that would otherwise be rare on the job, e.g., in-flight emergencies, system failure, etc., wherein the trainer can run 'scenarios' and study how the trainee reacts, thus assisting in improving his/her skills if the event was to occur in the real world. Examples of skills that commonly include simulator training during stages of development include piloting aircraft, spacecraft, locomotives, and ships, operating air traffic control airspace/sectors, power plant operations training, advanced military/defense system training, and advanced emergency response training.		Off-the-job training method takes place away from normal work situations — implying that the employee does not count as a directly productive worker while such training takes place. Off-the-job training method also involves employee training at a site away from the actual work environment. It often utilizes lectures, case studies, role playing, and simulation, having the advantage of allowing people to get away from work and concentrate more thoroughly on the training itself. This type of training has proven more effective in inculcating concepts and ideas[citation needed]. Many personnel selection companies offer a service which would help to improve employee competencies and change the attitude towards the job. The internal personnel training topics can vary from effective problem-solving skills to leadership training.		In religious and spiritual use, training may refer to the purification of the mind, heart, understanding and actions to obtain a variety of spiritual goals such as (for example) closeness to God or freedom from suffering. Note for example the institutionalised spiritual training of Threefold Training in Buddhism, Meditation in Hinduism or discipleship in Christianity. These aspects of training can be short term or last a lifetime, depending on the context of the training and which religious group it is a part of.		Compare religious ritual.		Instructor Guide (IG), is an important document available to an instructor. Specifically, it is used within a Lesson Plan, as the blueprint that ensures instruction is presented in proper sequence and to the depth required by the objectives. Objectives of a lesson plan:		Parochial schools are a fairly widespread institution in the United States. A parochial school is a primary or secondary school supervised by a religious organization, especially a Roman Catholic day school affiliated with a parish or a holy order. As of 2004, out of the approximately 50 million children who were enrolled in American grade schools, 4.2 million children attend a church-affiliated school, which is approximately 1 in 12 students.[5] Within the Christian religion, for example, one can attend a church-affiliated college with the intent of getting a degree in a field associated with religious studies. Some people may also attend church-affiliated colleges in pursuit of a non-religious degree, and typically do it just to deepen their understanding of the specific religion that the school is associated with.[citation needed] The largest non-public school system in the United States, the Catholic school system, operates 5,744 elementary schools and 1,206 secondary schools.		Researchers have developed training methods for artificial-intelligence devices as well. Evolutionary algorithms, including genetic programming and other methods of machine learning, use a system of feedback based on "fitness functions" to allow computer programs to determine how well an entity performs a task. The methods construct a series of programs, known as a “population” of programs, and then automatically test them for "fitness", observing how well they perform the intended task. The system automatically generates new programs based on members of the population that perform the best. These new members replace programs that perform the worst. The procedure repeats until the achievement of optimum performance.[6] In robotics, such a system can continue to run in real-time after initial training, allowing robots to adapt to new situations and to changes in themselves, for example, due to wear or damage. Researchers have also developed robots that can appear to mimic simple human behavior as a starting point for training.[7]		
Military service National service Conscription crisis Conscientious objector		Conscription, or drafting, is the compulsory enlistment of people in a national service, most often a military service.[5] Conscription dates back to antiquity and continues in some countries to the present day under various names. The modern system of near-universal national conscription for young men dates to the French Revolution in the 1790s, where it became the basis of a very large and powerful military. Most European nations later copied the system in peacetime, so that men at a certain age would serve 1–8 years on active duty and then transfer to the reserve force.		Conscription is controversial for a range of reasons, including conscientious objection to military engagements on religious or philosophical grounds; political objection, for example to service for a disliked government or unpopular war; and ideological objection, for example, to a perceived violation of individual rights. Those conscripted may evade service, sometimes by leaving the country.[6] Some selection systems accommodate these attitudes by providing alternative service outside combat-operations roles or even outside the military, such as 'Siviilipalvelus' (alternative civil service) in Finland, Zivildienst (compulsory community service) in Austria and Switzerland. Most post-Soviet countries conscript soldiers not only for Armed Forces but also for paramilitary organizations which are dedicated to police-like domestic only service (Internal Troops) or non-combat rescue duties (Civil Defence Troops) – none of which is considered alternative to the military conscription.		As of the early 21st century, many states no longer conscript soldiers, relying instead upon professional militaries with volunteers enlisted to meet the demand for troops. The ability to rely on such an arrangement, however, presupposes some degree of predictability with regard to both war-fighting requirements and the scope of hostilities. Many states that have abolished conscription therefore still reserve the power to resume it during wartime or times of crisis.[7] States involved in wars or interstate rivalries are most likely to implement conscription, whereas democracies are less likely than autocracies to implement conscription.[8] Former British colonies are less likely to have conscription, as they are influenced by British anticonscription norms that can be traced back to the English Civil War.[8]						Around the reign of Hammurabi (1791–1750 BC), the Babylonian Empire used a system of conscription called Ilkum. Under that system those eligible were required to serve in the royal army in time of war.[9] During times of peace they were instead required to provide labour for other activities of the state.[9] In return for this service, people subject to it gained the right to hold land.[9] It is possible that this right was not to hold land per se but specific land supplied by the state.[9]		Various forms of avoiding military service are recorded. While it was outlawed by the Code of Hammurabi, the hiring of substitutes appears to have been practiced both before and after the creation of the code.[10] Later records show that Ilkum commitments could become regularly traded.[10] In other places, people simply left their towns to avoid their Ilkum service.[10] Another option was to sell Ilkum lands and the commitments along with them. With the exception of a few exempted classes, this was forbidden by the Code of Hammurabi.[10]		Under the feudal conditions for holding land in the medieval period, most peasants and freemen were liable to provide one man of suitable age per family for military duty when required by either the king or the local lord. The levies raised in this way fought as infantry under local superiors. Although the exact laws varied greatly depending on the country and the period, generally these levies were only obliged to fight for one to three months. Most were subsistence farmers, and it was in everyone's interest to send the men home for harvest-time.		In medieval Scandinavia the leiðangr (Old Norse), leidang (Norwegian), leding, (Danish), ledung (Swedish), lichting (Dutch), expeditio (Latin) or sometimes leþing (Old English), was a levy of free farmers conscripted into coastal fleets for seasonal excursions and in defence of the realm.		The bulk of the Anglo-Saxon English army, called the fyrd, was composed of part-time English soldiers drawn from the landowning minor nobility. These thegns were the land-holding aristocracy of the time and were required to serve with their own armour and weapons for a certain number of days each year. The historian David Sturdy has cautioned about regarding the fyrd as a precursor to a modern national army composed of all ranks of society, describing it as a "ridiculous fantasy":		The persistent old belief that peasants and small farmers gathered to form a national army or fyrd is a strange delusion dreamt up by antiquarians in the late eighteenth or early nineteenth centuries to justify universal military conscription.[11]		Medieval levy in Poland was known as the pospolite ruszenie.		The system of military slaves was widely used in the Middle East, beginning with the creation of the corps of Turkish slave-soldiers (ghulams or mamluks) by the Abbasid caliph al-Mu'tasim in the 820s and 830s. The Turkish troops soon came to dominate the government, establishing a pattern throughout the Islamic world of a ruling military class, often separated by ethnicity, culture and even religion by the mass of the population, a paradigm that found its apogee in the Mamluks of Egypt and the Janissary corps of the Ottoman Empire, institutions that survived until the early 19th century.		In the middle of the 14th century, Ottoman Sultan Murad I developed personal troops to be loyal to him, with a slave army called the Kapıkulu. The new force was built by taking Christian children from newly conquered lands, especially from the far areas of his empire, in a system known as the devşirme (translated "gathering" or "converting"). The captive children were forced to convert to Islam. The Sultans had the young boys trained over several years. Those who showed special promise in fighting skills were trained in advanced warrior skills, put into the sultan's personal service, and turned into the Janissaries, the elite branch of the Kapıkulu. A number of distinguished military commanders of the Ottomans, and most of the imperial administrators and upper-level officials of the Empire, such as Pargalı İbrahim Pasha and Sokollu Mehmet Paşa, were recruited in this way.[12] By 1609, the Sultan's Kapıkulu forces increased to about 100,000.[13]		In later years, Sultans turned to the Barbary Pirates to supply their Jannissaries corps. Their attacks on ships off the coast of Africa or in the Mediterranean, and subsequent capture of able-bodied men for ransom or sale provided some captives for the Sultan's system. Starting in the 17th century, Christian families living under the Ottoman rule began to submit their sons into the Kapikulu system willingly, as they saw this as a potentially invaluable career opportunity for their children. Eventually the Sultan turned to foreign volunteers from the warrior clans of Circassians in southern Russia to fill his Janissary armies. As a whole the system began to break down, the loyalty of the Jannissaries became increasingly suspect. Mahmud II forcibly disbanded the Janissary corps in 1826.[14][15]		Similar to the Janissaries in origin and means of development were the Mamluks of Egypt in the Middle Ages. The Mamluks were usually captive non-Muslim Iranian and Turkish children who had been kidnapped or bought as slaves from the Barbary coasts. The Egyptians assimilated and trained the boys and young men to become Islamic soldiers who served the Muslim caliphs and the Ayyubid sultans during the Middle Ages. The first mamluks served the Abbasid caliphs in 9th century Baghdad. Over time they became a powerful military caste. On more than one occasion, they seized power, for example, ruling Egypt from 1250–1517.		From 1250 Egypt had been ruled by the Bahri dynasty of Kipchak origin. Slaves from the Caucasus served in the army and formed an elite corp of troops. They eventually revolted in Egypt to form the Burgi dynasty. The Mamluks' excellent fighting abilities, massed Islamic armies, and overwhelming numbers succeeded in overcoming the Christian Crusader fortresses in the Holy Land. The Mamluks were the most successful defense against the Mongol Ilkhanate of Persia and Iraq from entering Egypt.[16]		On the western coast of Africa, Berber Muslims captured non-Muslims to put to work as laborers. They generally converted the younger people to Islam and many became quite assimilated. In Morocco, the Berber looked south rather than north. The Moroccan Sultan Moulay Ismail, called "the Bloodthirsty" (1672–1727), employed a corps of 150,000 black slaves, called his Black Guard. He used them to coerce the country into submission.[17]		Modern conscription, the massed military enlistment of national citizens, was devised during the French Revolution, to enable the Republic to defend itself from the attacks of European monarchies. Deputy Jean-Baptiste Jourdan gave its name to the 5 September 1798 Act, whose first article stated: "Any Frenchman is a soldier and owes himself to the defense of the nation." It enabled the creation of the Grande Armée, what Napoleon Bonaparte called "the nation in arms," which overwhelmed European professional armies that often numbered only into the low tens of thousands. More than 2.6 million men were inducted into the French military in this way between the years 1800 and 1813.[18]		The defeat of the Prussian Army in particular shocked the Prussian establishment, which had believed it was invincible after the victories of Frederick the Great. The Prussians were used to relying on superior organization and tactical factors such as order of battle to focus superior troops against inferior ones. Given approximately equivalent forces, as was generally the case with professional armies, these factors showed considerable importance. However, they became considerably less important when the Prussian armies faced forces that outnumbered their own in some cases by more than ten to one. Scharnhorst advocated adopting the levée en masse, the military conscription used by France. The Krümpersystem was the beginning of short-term compulsory service in Prussia, as opposed to the long-term conscription previously used.[19]		In the Russian Empire, the military service time "owed" by serfs was 25 years at the beginning of the 19th century. In 1834 it was decreased to 20 years. The recruits were to be not younger than 17 and not older than 35.[20] In 1874 Russia introduced universal conscription in the modern pattern, an innovation only made possible by the abolition of serfdom in 1861. New military law decreed that all male Russian subjects, when they reached the age of 20, were eligible to serve in the military for six years.[21]		In the decades prior to World War I universal conscription along broadly Prussian lines became the norm for European armies, and those modeled on them. By 1914 the only substantial armies still completely dependent on voluntary enlistment were those of Britain and the United States. Some colonial powers such as France reserved their conscript armies for home service while maintaining professional units for overseas duties.		The range of eligible ages for conscripting was expanded to meet national demand during the World Wars. In the United States, the Selective Service System drafted men for World War I initially in an age range from 21 to 30 but expanded its eligibility in 1918 to an age range of 18 to 45.[22] In the case of a widespread mobilization of forces where service includes homefront defense, ages of conscripts may range much higher, with the oldest conscripts serving in roles requiring lesser mobility. Expanded-age conscription was common during the Second World War: in Britain, it was commonly known as "call-up" and extended to age 51. Nazi Germany termed it Volkssturm ("People's Storm") and included men as young as 16 and as old as 60.[23] During the Second World War, both Britain and the Soviet Union conscripted women. The United States was on the verge of drafting women into the Nurse Corps because it anticipated it would need the extra personnel for its planned invasion of Japan. However, the Japanese surrendered and the idea was abandoned.[24]		Both feminists[25][26][27] and opponents of discrimination against men[28][29]:102 have criticized military conscription, or compulsory military service, as sexist.		Feminists have argued that military conscription is sexist because wars serve the interests of the patriarchy, the military is a sexist institution, conscripts are therefore indoctrinated in sexism, and conscription of men normalizes violence by men as socially acceptable.[30][31] Feminists have been organizers and participants in resistance to conscription in several countries.[32][33][34][35]		Historically, only men have been subjected to conscription,.[29][36][37][38][39] In the second half of the 20th century, women began to be conscripted, primarily in communist/socialist countries. The integration of women into militaries, and especially into combat forces, did not begin on a large scale until the second half of the 20th century. Men who opt out of military service must often perform alternative service, such as Zivildienst in Austria and Switzerland, whereas women do not have even these obligations.		American libertarians oppose conscription and call for the abolition of the Selective Service System, believing that impressment of individuals into the armed forces is "involuntary servitude."[40] Ron Paul, a former presidential nominee of the U.S. Libertarian Party has said that conscription "is wrongly associated with patriotism, when it really represents slavery and involuntary servitude."[41] The philosopher Ayn Rand opposed conscription, suggesting that "of all the statist violations of individual rights in a mixed economy, the military draft is the worst. It is an abrogation of rights. It negates man's fundamental right—the right to life—and establishes the fundamental principle of statism: that a man's life belongs to the state, and the state may claim it by compelling him to sacrifice it in battle."[42]		In 1917, a number of radicals and anarchists, including Emma Goldman, challenged the new draft law in federal court arguing that it was a direct violation of the Thirteenth Amendment's prohibition against slavery and involuntary servitude. However, the Supreme Court unanimously upheld the constitutionality of the draft act in the case of Arver v. United States on 7 January 1918. The decision said the Constitution gave Congress the power to declare war and to raise and support armies. The Court emphasized the principle of the reciprocal rights and duties of citizens:		It can be argued that in a cost-to-benefit ratio, conscription during peace time is not worthwhile.[44] Months or years of service performed by the most fit and capable subtract from the productivity of the economy; add to this the cost of training them, and in some countries paying them. Compared to these extensive costs, some would argue there is very little benefit; if there ever was a war then conscription and basic training could be completed quickly, and in any case there is little threat of a war in most countries with conscription. In the United States, every male resident is required by law to register with the Selective Service System within 30 days following his 18th birthday and be available for a draft; this is often accomplished automatically by a motor vehicle department during licensing or by voter registration).		The cost of conscription can be related to the parable of the broken window in anti-draft arguments. The cost of the work, military service, does not disappear even if no salary is paid. The work effort of the conscripts is effectively wasted, as an unwilling workforce is extremely inefficient. The impact is especially severe in wartime, when civilian professionals are forced to fight as amateur soldiers. Not only is the work effort of the conscripts wasted and productivity lost, but professionally skilled conscripts are also difficult to replace in the civilian workforce. Every soldier conscripted in the army is taken away from his civilian work, and away from contributing to the economy which funds the military. This may be less a problem in an agrarian or pre-industrialized state where the level of education is generally low, and where a worker is easily replaced by another. However, this is potentially more costly in a post-industrial society where educational levels are high and where the workforce is sophisticated and a replacement for a conscripted specialist is difficult to find. Even direr economic consequences result if the professional conscripted as an amateur soldier is killed or maimed for life; his work effort and productivity are lost.[45]		Jean Jacques Rousseau argued vehemently against professional armies, feeling it was the right and privilege of every citizen to participate to the defense of the whole society and a mark of moral decline to leave this business to professionals. He based this view on the development of the Roman republic, which came to an end at the same time as the Roman army changed from a conscript to professional force.[46] Similarly, Aristotle linked the division of armed service among the populace intimately with the political order of the state.[47] Niccolò Machiavelli argued strongly for conscription[citation needed], seeing the professional armies as the cause of the failure of societal unity in Italy.		Other proponents, such as William James, consider both mandatory military and national service as ways of instilling maturity in young adults.[48] Some proponents, such as Jonathan Alter and Mickey Kaus, support a draft in order to reinforce social equality, create social consciousness, break down class divisions and for young adults to immerse themselves in public enterprise.[49][50][51] Charles Rangel called for the reinstatement of the draft during the Iraq conflict.		It is estimated by the British military that in a professional military, a company deployed for active duty in peacekeeping corresponds to three inactive companies at home. Salaries for each are paid from the military budget. In contrast, volunteers from a trained reserve are in their civilian jobs when they are not deployed.[52]		It was more financially beneficial for less-educated young Portuguese men born in 1967 to participate in conscription, as opposed to participating in the highly competitive job market with men of the same age who continued through to higher education.[53]		Traditionally conscription has been limited to the male population of a given body. Women and handicapped males have been exempt from conscription. Many societies have considered, and continue to consider, military service as a test of manhood and a rite of passage from boyhood into manhood.[54][55]		As of 2013[update], countries that were actively drafting women into military service included Bolivia,[56] Chad,[57] Eritrea,[58][59][60] Israel,[58][59][61] Mozambique [62] and North Korea.[63] Israel has universal female conscription, although in practice women can avoid service by claiming a religious exemption and over a third of Israeli women do so.[58][59][64] Sudanese law allows for conscription of women, but this is not implemented in practice.[65] In the United Kingdom during World War II, beginning in 1941, women were brought into the scope of conscription but, as all women with dependent children were exempt and many women were informally left in occupations such as nursing or teaching, the number conscripted was relatively few.[66]		In 2015 Norway introduced female conscription, making it the first NATO member and first European country to have a legally compulsory national service for both men and women.[67] In practice only motivated volunteers are selected to join the army in Norway.[68]		In the USSR, there was no systematic conscription of women for the armed forces, but the severe disruption of normal life and the high proportion of civilians affected by World War II after the German invasion attracted many volunteers for what was termed "The Great Patriotic War".[69] Medical doctors of both sexes could and would be conscripted (as officers). Also, the free Soviet university education system required Department of Chemistry students of both sexes to complete an ROTC course in NBC defense, and such female reservist officers could be conscripted in times of war. The United States came close to drafting women into the Nurse Corps in preparation for a planned invasion of Japan.[70][71]		In 1981 in the United States, several men filed lawsuit in the case Rostker v. Goldberg, alleging that the Selective Service Act of 1948 violates the Due Process Clause of the Fifth Amendment by requiring that only men register with the Selective Service System (SSS). The Supreme Court eventually upheld the Act, stating that "the argument for registering women was based on considerations of equity, but Congress was entitled, in the exercise of its constitutional powers, to focus on the question of military need, rather than 'equity.'"[72]		On October 1, 1999 in the Taiwan Area, the Judicial Yuan of the Republic of China in its Interpretation 490 considered that the physical differences between males and females and the derived role differentiation in their respective social functions and lives would not make drafting only males a violation of the Constitution of the Republic of China.[73][(see discussion) verification needed] Though women are not conscripted in Taiwan, transsexual persons are exempt.[74]		A conscientious objector is an individual whose personal beliefs are incompatible with military service, or, more often, with any role in the armed forces."[75][76] In some countries, conscientious objectors have special legal status, which augments their conscription duties. For example, Sweden used to allow conscientious objectors to choose a service in the "weapons-free" branch, such as an airport fireman, nurse or telecommunications technician.		The reasons for refusing to serve are varied. Some conscientious objectors are so for religious reasons—notably, the members of the historic peace churches, pacifist by doctrine; Jehovah's Witnesses, while not strictly pacifists, refuse to participate in the armed forces on the ground that they believe Christians should be neutral in worldly conflicts.		Universal conscription in China dates back to the State of Qin, which eventually became the Qin Empire of 221 BC. Following unification, historical records show that a total of 300,000 conscript soldiers and 500,000 conscript labourers constructed the Great Wall of China.[150]		In the following dynasties, universal conscription was abolished and reintroduced on numerous occasions.		As of 2011[update], universal military conscription is theoretically mandatory in the People's Republic of China, and reinforced by law. However, due to the large population of China and large pool of candidates available for recruitment, the People's Liberation Army has always had sufficient volunteers, so conscription has not been required in practice at all.[citation needed]		Every male citizen of the Republic of Austria up to the age of 35 can be drafted for a six month long basic military training in the Bundesheer. For men refusing to undergo this training, a nine month lasting community service is mandatory.		Military service in the Cypriot National Guard is mandatory for all male citizens of the Republic of Cyprus, as well as any male non-citizens born of a parent of Greek Cypriot descent, lasting from the January 1 of the year in which they turn 18 years of age to December 31, of the year in which they turn 50.[151] All male residents of Cyprus who are of military age (16 and over) are required to obtain an exit visa from the Ministry of Defense.		Conscription is known in Denmark since the Viking Age, where one man out of every 10 had to serve the king. Frederick IV of Denmark changed the law in 1710 to every 4th man. The men were chosen by the landowner and it was seen as a penalty.		Since 12 February 1849, every physically fit man must do military service. According to §81 in the Constitution of Denmark, which was promulgated in 1849:		Every male person able to carry arms shall be liable with his person to contribute to the defence of his country under such rules as are laid down by Statute. — Constitution of Denmark[152]		The legislation about compulsory military service is articulated in the Danish Law of Conscription.[153] National service takes 4–12 months.[154] It is possible to postpone the duty when one is still in full-time education.[155] Every male turning 18 will be drafted to the 'Day of Defence', where they will be introduced to the Danish military and their health will be tested.[156] Physically unfit persons are not required to do military service.[154][157] It is only compulsory for men, while women are free to choose to join the Danish army.[158] Almost all of the men have been volunteers in recent years,[159] 96.9% of the total amount of recruits having been volunteers in the 2015 draft.[160]		After lottery,[161] one can become a conscientious objector.[162] Total objection (refusal from alternative civilian service) results in up to 4 months jailtime according to the law.[163] However, in 2014 a Danish man, who signed up for the service and objected later, got only 14 days of home arrest.[164] In many countries the act of desertion (objection after signing up) is punished harder than objecting the compulsory service.		Conscription in Finland is part of a general compulsion for national military service for all adult males (Finnish: maanpuolustusvelvollisuus; Swedish: totalförsvarsplikt) defined in the 127§ of the Constitution of Finland.		Conscription can take the form of military or of civilian service. According to Finnish Defence Forces 2011 data slightly under 80% of Finnish males turned 30 had entered and finished the military service. The number of female volunteers to annually enter armed service had stabilised at approximately 300.[165] The service period is 165, 255 or 347 days for the rank and file conscripts and 347 days for conscripts trained as NCOs or reserve officers. The length of civilian service is always twelve months. Those electing to serve unarmed in duties where unarmed service is possible serve either nine or twelve months, depending on their training.[166][167]		Any Finnish citizen who refuses to perform both military and civilian service faces a penalty of 173 days in prison, minus any served days. Such sentences are usually served fully in prison, with no parole.[168][169] Jehovah's Witnesses are exempted in that they may be granted a deferment of service for 3 years upon presentation of a certificate from their congregation's minister showing they are an active member of that religious community. Providing they are still an active member 3 years later, there is nothing to stop them getting a further certificate and deferment.[170] The inhabitants of the demilitarized Åland Islands are exempt from military service. By the Conscription Act of 1951, they are, however, required to serve a time at a local institution, like the coast guard. However, until such service has been arranged, they are freed from service obligation. The non-military service of Åland islands has not been arranged since the introduction of the act, and there are no plans to institute it. The inhabitants of Åland islands can also volunteer for military service on the mainland. As of 1995, women are permitted to serve on a voluntary basis and pursue careers in the military after their initial voluntary military service.		The military service takes place in Finnish Defence Forces or in the Finnish Border Guard. All services of the Finnish Defence Forces train conscripts. However, the Border Guard trains conscripts only in land-based units, not in coast guard detachments or in the Border Guard Air Wing. Civilian service may take place in the Civilian Service Center in Lapinjärvi or in an accepted non-profit organization of educational, social or medical nature.		In both East and West Germany, military service was mandatory for all male citizens. With the end of the Cold War, then unified Germany drastically reduced the size of its armed forces. The low demand for conscripts led to the suspension of compulsory conscription in 2011. Since then only volunteer professionals serve in the Bundeswehr.		Since 1914, Greece (Hellenic Republic) has mandatory military service of 9 months for men between the ages of 16 and 45. Citizens discharged from active service are normally placed in the Reserve and are subject to periodic recall of 1–10 days at irregular intervals.[171]		Universal conscription was introduced in Greece during the military reforms of 1909, although various forms of selective draft had been in place earlier. In more recent years, conscription was associated with the state of general mobilisation declared on July 20, 1974 due to the crisis in Cyprus (the mobilisation was formally ended on December 18, 2002).		The length of a tour has varied historically, between 12–36 months depending on various factors particular to the conscript, and the political situation. Although women are accepted into the Greek army on a voluntary basis, they are not required to enlist, as men are. Soldiers receive no health insurance, but they are provided medical support during their army service, including hospitalization costs.		Since 2009, Greece has mandatory military service of 9 months for male citizens between the ages of 19 and 45. However, as the Armed forces had been gearing towards a complete professional army system, the government had promised that the mandatory military service would be cut to 6 months by 2008 or even abolished completely. However, this timetable is under reconsideration as of April 2006, due to severe manpower shortages. These were caused by a combination of (a) financial difficulties, which meant that professional soldiers could not be hired at the projected rate, and (b) widespread abuse of the deferment process, which meant that 66% of the draftees deferred service in 2005. In August 2009, the mandatory service was reduced to 9 months for the Land Army, while has remained to 12 months for the Navy and the Air Force. The number of conscripts affected to the latter two has been greatly reduced, with an aim towards full professionalisation.		Lithuania abolished its conscription in 2008.[172] In May 2015 the Lithuanian parliament voted to return the conscription and the conscripts started their training in August 2015.[173] In practice there is no conscription in Lithuania, since all recruits have been volunteers.[174]		Conscription, which was called "Service Duty" (Dutch: dienstplicht) in the Netherlands, was first employed in 1810 by French occupying forces. Napoleon's brother Louis Bonaparte, who was King of Holland from 1806 to 1810, had tried to introduce conscription a few years earlier, unsuccessfully. Every man aged 20 years or older had to enlist. By means of drawing lots it was decided who had to undertake service in the French army. It was possible to arrange a substitute against payment.		Later on, conscription was used for all men over the age of 18. Postponement was possible, due to study, for example. Conscientious objectors could perform an alternative civilian service instead of military service. For various reasons, this forced military service was criticized at the end of the twentieth century. Since the Cold War was over, so was the direct threat of a war. Instead, the Dutch army was employed in more and more peacekeeping operations. The complexity and danger of these missions made the use of conscripts controversial. Furthermore, the conscription system was thought to be unfair as only men were drafted.		In the European part of Netherlands, compulsory attendance has been officially suspended since 1 May 1997. Between 1991 and 1996, the Dutch armed forces phased out their conscript personnel and converted to an all-volunteer force. The last conscript troops were inducted in 1995, and demobilized in 1996. The suspension means that citizens are no longer forced to serve in the armed forces, as long as it is not required for the safety of the country. Since then, the Dutch army is an all-volunteer force. However, to this day, every male and female[175] citizen aged 17 gets a letter in which he is told that he has been registered but does not have to present himself for service. The Dutch army allowed its male soldiers to have long hair from the early 1970s to the end of conscription in the mid-1990s.		As of March 2016[update], Norway currently employs a weak form of mandatory military service for men and women. In practice recruits are not forced to serve, instead only those who are motivated are selected.[176] About 60,000 Norwegians are available for conscription every year, but only 8,000 to 10,000 are conscripted.[177] Since 1985, women have been able to enlist for voluntary service as regular recruits. On 14 June 2013 the Norwegian Parliament voted to extend conscription to women, making Norway the first NATO member and first European country to make national service compulsory for both sexes.[178] In earlier times, up until at least the early 2000s, all men aged 19–44 were subject to mandatory service, with good reasons required to avoid becoming drafted. There is a right of conscientious objection.		As of 1 January 2011, Serbia no longer practises mandatory military service. Prior to this, mandatory military service lasted 6 months for men. Conscientious objectors could however opt for 9 months of civil service instead.		On 15 December 2010, the Parliament of Serbia voted to suspend mandatory military service. The decision fully came into force on January 1, 2011.[179]		Sweden had conscription (Swedish: värnplikt) for men between 1901 and 2010.[180] Peace-time conscription was made dormant in 2010, and the law on conscription was simultaneously made gender-neutral.[181]		Due to tensions in the Baltic, the Swedish government has stated it will reintroduce military conscription beginning January 1, 2018, for 4,000 men and women who were born in 1999.[182]		The United Kingdom introduced conscription to full-time military service for the first time in January 1916 (the eighteenth month of World War I) and abolished it in 1920. Ireland, then part of the United Kingdom, was exempted from the original 1916 military service legislation, and although further legislation in 1918 gave power for an extension of conscription to Ireland, the power was never put into effect.		Conscription was reintroduced in 1939, in the lead up to World War II, and continued in force until 1963. Northern Ireland was exempted from conscription legislation throughout the whole period.		In all, 8,000,000 men were conscripted in the Second World War, as well as several hundred thousand younger single women.[183] The introduction of conscription in May 1939, before the war began, was partly due to pressure from the French, who emphasized the need for a large British army to oppose the Germans.[184] From early 1942 unmarried women age 19–30 were conscripted. Most were sent to the factories, but they could volunteer for the Auxiliary Territorial Service (ATS) and other women's services. None was assigned to combat roles unless she volunteered. By 1943 women were liable to some form of directed labour up to age 51. During the Second World War, 1.4 million British men volunteered for service and 3.2 million were conscripted. Conscripts comprised 50% of the Royal Air Force, 60% of the Royal Navy and 80% of the British Army.[185]		Britain and her colonies did not develop such pervasive administrative states, and therefore did not opt out for regulatory solutions, such as conscription, as a reliability.[186]The abolition of conscription in Britain was announced on 4 April 1957, by new prime minister Harold Macmillan, with the last conscripts being recruited three years later.[187]		There is a mandatory service for all male and female who are fit and 18 years old. Men must serve 32 months while women serve 24 months. Yet, some are exempt from mandatory service:		All of the above can choose to volunteer to the IDF. Relatively large numbers of Bedouin choose to volunteer.		Male Druze and Circassian Israeli citizens are liable, by agreement with their community leaders (Female Druze and Circassian are exempt from service).		In the United States, conscription, also called "the draft", ended in 1973, but males aged between 18 and 25 are required to register with the Selective Service System to enable a reintroduction of conscription if necessary. President Gerald Ford suspended mandatory draft registration in 1975, but President Jimmy Carter reinstated that requirement when the Soviet Union intervened in Afghanistan. Selective Service registration is still required of almost all young men, although the draft has not been used since 1973[188] and there have been no prosecutions for violations of the draft registration law since 1986.[189]		
Equal opportunity arises from the similar treatment of all people, unhampered by artificial barriers or prejudices or preferences, except when particular distinctions can be explicitly justified.[1] The aim according to this often complex and contested concept[2] is that important jobs should go to those "most qualified" – persons most likely to perform ably in a given task – and not go to persons for arbitrary or irrelevant reasons, such as circumstances of birth, upbringing, having well-connected relatives or friends,[3] religion, sex,[4] ethnicity,[4] race, caste,[5] or involuntary personal attributes such as disability, age, gender identity, or sexual orientation.[5][6] Chances for advancement should be open to everybody interested[7] such that they have "an equal chance to compete within the framework of goals and the structure of rules established".[8] The idea is to remove arbitrariness from the selection process and base it on some "pre-agreed basis of fairness, with the assessment process being related to the type of position",[3] and emphasizing procedural and legal means.[5][9] Individuals should succeed or fail based on their own efforts and not extraneous circumstances such as having well-connected parents.[10] It is opposed to nepotism[3] and plays a role in whether a social structure is seen as legitimate.[3][5][11] The concept is applicable in areas of public life in which benefits are earned and received such as employment and education, although it can apply to many other areas as well.						People with differing political viewpoints often view the concept differently.[12] The meaning of equal opportunity is debated in fields such as political philosophy, sociology and psychology. It is being applied to increasingly wider areas beyond employment[9][13] including lending,[14] housing, college admissions, voting rights, and elsewhere.[1] In the classical sense, equality of opportunity is closely aligned with the concept of equality before the law and ideas of meritocracy.[15]		Generally the terms "equality of opportunity" and "equal opportunity" are interchangeable, with occasional slight variations: "equality of opportunity" has more of a sense of being an abstract political concept, while "equal opportunity" is sometimes used as an adjective, usually in the context of employment regulations, to identify an employer, a hiring approach, or law. Equal opportunity provisions have been written into regulations and have been debated in courtrooms.[16] It is sometimes conceived as a legal right against discrimination.[5][17][18] It is an ideal which has become increasingly widespread[19] in Western nations during the last several centuries and is intertwined with social mobility, most often with upward mobility and with rags to riches stories:		The coming President of France is the grandson of a shoemaker. The actual President is a peasant's son. His predecessor again began life in a humble way in the shipping business. There is surely equality of opportunity under the new order in the old nation.		According to the Stanford Encyclopedia of Philosophy, the concept assumes that society is stratified with a diverse range of roles, some of which are more desirable than others.[3] And the benefit of equality of opportunity is to bring fairness to the selection process for coveted roles in corporations, associations, nonprofits, universities, and elsewhere.[21] There is no "formal linking" between equality of opportunity and political structure, according to one view, in the sense that there can be equality of opportunity in democracies, autocracies, and in communist nations,[3] although it is primarily associated with a competitive market economy[3] and embedded within the legal frameworks of democratic societies.[22] People with different political perspectives see equality of opportunity differently: liberals disagree about which conditions are needed to ensure it; many "old-style" conservatives see inequality and hierarchy in general as beneficial out of a respect for tradition.[23] It can apply to a specific hiring decision, or to all hiring decisions by a specific company, or rules governing hiring decisions for an entire nation. The scope of equal opportunity has expanded to cover more than issues regarding the rights of minority groups, but covers practices regarding "recruitment, hiring, training, layoffs, discharge, recall, promotions, responsibility, wages, sick leave, vacation, overtime, insurance, retirement, pensions, and various other benefits."[21] The concept has been applied to numerous aspects of public life, including accessibility of polling stations,[24] care provided to HIV patients,[25] whether men and women have equal opportunities to travel on a spaceship,[26] bilingual education,[27] skin color of models in Brazil,[28] television time for political candidates,[29] army promotions,[30] admittance to universities,[31] and ethnicity in the United States.[32] The term is interrelated with and often contrasted with other conceptions of equality such as equality of outcome and equality of autonomy. Equal opportunity emphasizes the personal ambition and talent and abilities of the individual, rather than his or her qualities based on membership in a group, such as a social class or race or extended family.[5] Further, it is seen as unfair if external factors, that are viewed as being beyond the control of a person, significantly influence what happens to him or her.[5] Equal opportunity, then, emphasizes a fair process; in contrast, equality of outcome emphasizes a fair outcome.[5] In sociological analysis, equal opportunity is seen as a factor correlating positively with social mobility, in the sense that it can benefit society overall by maximizing well-being.[5]		There are different concepts lumped under equality of opportunity.[2][19][33][34]		Formal equality of opportunity is a lack of (unfair) direct discrimination. It requires that deliberate discrimination be relevant and meritocratic. For instance, job interviews should only discriminate against applicants for job incompetence. Universities should not accept a less-capable applicant instead of a more-capable applicant who can't pay tuition.		Substantive equality of opportunity is absence of indirect discrimination. It requires that society be fair and meritocratic. For instance, a person should not be more likely to die at work because they were born in a country with corrupt labor law enforcement. No-one should have to drop out of school because their family needs of a full-time carer or wage earner.		Formal equality of opportunity does not imply substantive equality of opportunity. Firing any employee who gets pregnant is formally equal, but substantively it hurts women more.		Substantive inequality is often more difficult to address. A political party that formally allows anyone to join, but meets in a non-wheelchair-accessible building far from public transit, substantively discriminates against both young and old members, as they are less likely to be able-bodied car-owners. But if the party raises membership dues in order to afford a better building, it discourages poor members instead. Grade-cutoff university admission is formally fair, but if in practice it overwhelmingly picks women and graduates of expensive user-fee schools, it is substantively unfair to men and the poor. The unfairness has obviously already taken place; the university can choose to try to counterbalance it, but it likely can't single-handedly make pre-university opportunities equal.		Social mobility and the Great Gatsby Curve are often used as an indicator of substantive equality of opportunity. [35]		Both equality concepts say that it is unfair and inefficient if extraneous factors rule people's lives. Both accept, as fair, inequality based on relevant, meritocratic factors. They differ in the scope of the methods used to promote them.		Formal equality of opportunity[2] is sometimes referred to as the nondiscrimination principle[36] or described as the absence of direct discrimination,[2] or described in the narrow sense as equality of access.[2][37] It is characterized by:		The formal approach is seen as a somewhat basic "no frills" or "narrow"[5] approach to equality of opportunity, a minimal standard of sorts, limited to the public sphere as opposed to private areas such as the family, marriage, or religion.[5] What is considered "fair" and "unfair" is spelled out in advance.[38] An expression of this version appeared in The New York Times:		There should be an equal opportunity for all. Each and every person should have as great or as small an opportunity as the next one. There should not be the unfair, unequal, superior opportunity of one individual over another.		This sense was also expressed by economists Milton and Rose Friedman in their 1980 book Free to Choose.[40] The Friedmans explained that equality of opportunity was "not to be interpreted literally" since some children are born blind while others are born sighted, but that "its real meaning is ... a career open to the talents."[40] This means that there should be "no arbitrary obstacles" blocking a person from realizing their ambitions, and that "Not birth, nationality, color, religion, sex, nor any other irrelevant characteristic should determine the opportunities that are open to a person – only his abilities."[40]		A somewhat different view was expressed by John Roemer who used the term nondiscrimination principle to mean that "all individuals who possess the attributes relevant for the performance of the duties of the position in question be included in the pool of eligible candidates, and that an individual's possible occupancy of the position be judged only with respect to those relevant attributes."[36] Matt Cavanagh argued that race and sex shouldn't matter when getting a job, but that the sense of equality of opportunity should not extend much further than preventing straightforward discrimination.[41]		The ideal of a society in which people do not suffer disadvantage from discrimination on grounds of supposed race, ethnicity, religion, sex, sexual orientation is widely upheld as desirable in itself.		It is a relatively straightforward task for legislators to ban blatant efforts to favor one group over another and encourage equality of opportunity as a result. Japan banned gender-specific job descriptions in advertising as well as sexual discrimination in employment as well as other practices deemed unfair,[42] although a subsequent report suggested that the law was having minimal effect in securing Japanese women high positions in management.[43][needs update] In the United States, the Equal Employment Opportunity Commission sued a private test preparation firm, Kaplan, for unfairly using credit histories to discriminate against African Americans in terms of hiring decisions.[16] According to one analysis, it is possible to imagine a democracy which meets the formal criteria (1 through 3) but which still favors wealthy candidates who are selected in free and fair elections.[44]		"If higher inequality makes intergenerational mobility more difficult, it is likely because opportunities for economic advancement are more unequally distributed among children."[46]		This term, sometimes called fair equality of opportunity,[19] is a somewhat broader[5] and more expansive concept than the more limiting formal equality of opportunity and it deals with what is sometimes described as indirect discrimination.[2] It goes farther, and is more controversial[5] than the formal variant, and has been thought to be much harder to achieve, with greater disagreement about how to achieve greater equality,[5] and has been described as "unstable",[19] particularly if the society in question is unequal to begin with in terms of great disparity of wealth.[47] It has been identified as more of a left-leaning political position[48] but this is not a hard-and-fast rule. The substantive model is advocated by people who see limitations in the formal model:		Therein lies the problem with the idea of equal opportunity for all. Some people are simply better placed to take advantage of opportunity.		There is little income mobility – the notion of America as a land of opportunity is a myth.		In the substantive approach, the starting point before the race begins is unfair, since people have had differing experiences before even approaching the competition. The substantive approach examines the applicants themselves before applying for a position, and judges whether they have equal abilities or talents, and if not, then it suggests that authorities (usually the government) take steps to make applicants more equal before they get to the point where they compete for a position, and fixing the before-the-starting-point issues has sometimes been described as working towards "fair access to qualifications."[19] It seeks to remedy inequalities perhaps because of an "unfair disadvantage" based sometimes on "prejudice in the past."[9] According to John Hills, children of wealthy and well-connected parents usually have a decisive advantage over other types of children, and he notes that "advantage and disadvantage reinforce themselves over the life cycle, and often on to the next generation" so that successful parents pass along their wealth and education to succeeding generations, making it difficult for others to climb up a social ladder.[51] But so-called positive action efforts to bring an underprivileged person up to speed before a competition begins are limited to the period of time before the evaluation begins; at that point, the "final selection for posts must be made according to the principle the best person for the job," that is, a less qualified applicant should not be chosen over a more qualified applicant.[2] And there are nuanced views too: one position suggested that the unequal results following a competition were unjust if caused by bad luck but just if chosen by the individual, and that weighing matters such as personal responsibility was important; this variant of the substantive model has sometimes been called luck egalitarianism.[19] Still, regardless of the nuances, the overall idea is to give children from less fortunate backgrounds more of a chance,[51] or to achieve at the beginning what some theorists call equality of condition.[2] Writer Ha-Joon Chang expressed this view:		We can accept the outcome of a competitive process as fair only when the participants have equality in basic capabilities; the fact that no one is allowed to have a head start does not make the race fair if some contestants have only one leg.		In a sense, substantive equality of opportunity moves the "starting point" further back in time. Sometimes it entails the use of affirmative action policies to help all contenders become equal before they get to the starting point, perhaps with greater training, or sometimes redistributing resources via restitution or taxation to make the contenders more equal. It holds that all who have a "genuine opportunity to become qualified" be given a chance to do so. And it is sometimes based on a recognition that unfairness exists, hindering social mobility, combined with a sense that the unfairness should not exist or should be lessened in some manner.[53] One example postulated was that a warrior society could provide special nutritional supplements to poor children, offer scholarships to military academies, and dispatch "warrior skills coaches" to every village as a way to make opportunity substantively more fair.[3] The idea is to give every ambitious and talented youth a chance to compete for prize positions regardless of their circumstances of birth.[3]		The substantive approach tends to have a broader definition of extraneous circumstances which should be kept out of a hiring decision. One editorial writer suggested that among the many types of extraneous circumstances which should be kept out of hiring decisions was personal beauty, sometimes termed lookism:		Lookism judges individuals by their physical allure rather than abilities or merit. This naturally works to the advantage of people perceived to rank higher in the looks department. They get preferential treatment at the cost of others. Which fair, democratic system can justify this? If anything, lookism is as insidious as any other form of bias based on caste, creed, gender and race that society buys into. It goes against the principle of equality of opportunity.		The substantive position was advocated by Bhikhu Parekh in 2000 in Rethinking Multiculturalism who wrote that "all citizens should enjoy equal opportunities to acquire the capacities and skills needed to function in society and to pursue their self-chosen goals equally effectively" and that "equalising measures are justified on grounds of justice as well as social integration and harmony."[2][55] Parekh argued that equal opportunities included so-called cultural rights which are "ensured by the politics of recognition."[2]		Affirmative action programs usually fall under the substantive category.[5] The idea is to help disadvantaged groups get back to a normal starting position after a long period of discrimination. The programs involve government action, sometimes with resources being transferred from an advantaged group to a disadvantaged one, and these programs have been justified on the grounds that imposing quotas counterbalances the past discrimination[3] as well as being a "compelling state interest" in diversity in society.[5] For example, there was a case in Sao Paulo in Brazil of a quota imposed on the São Paulo Fashion Week to require that "at least 10 percent of the models to be black or indigenous", as a coercive measure to counteract a "longstanding bias towards white models."[56] It does not have to be accomplished via government action; for example, in the 1980s in the United States, President Reagan dismantled parts of affirmative action, but one report in the Chicago Tribune suggested that companies remained committed to the principle of equal opportunity regardless of government requirements.[57] In another instance, upper-middle class students taking the Scholastic Aptitude Test in the United States performed better, since they had had more "economic and educational resources to prepare for these test than others."[5] The test, itself, was seen as fair in a formal sense, but the overall result was seen as nevertheless unfair. In India, the Indian Institutes of Technology found that to achieve substantive equality of opportunity, the school had to reserve 22.5 percent of seats for applicants from "historically disadvantaged schedule castes and tribes."[5][58] Elite universities in France began a special "entrance program" to help applicants from "impoverished suburbs."[5]		Philosopher John Rawls offered this variant of substantive equality of opportunity, and explained that it happens when individuals with the same "native talent and the same ambition" have the same prospects of success in competitions.[3][59][60][61] Gordon Marshall offers a similar view with the words "positions are to be open to all under conditions in which persons of similar abilities have equal access to office."[23] An example was given: If two persons X and Y have identical talent but X is from a poor family while Y is from a rich one, then equality of fair opportunity is in effect when both X and Y have the same chance of winning the job.[3] It suggests the ideal society is "classless" without a social hierarchy being passed from generation to generation, although parents can still pass along advantages to their children by genetics and socialization skills.[3] One view suggests that this approach might advocate "invasive interference in family life."[3] Marshall posed this question:		Does it demand that, however unequal their abilities, people should be equally empowered to achieve their goals? This would imply that the unmusical individual who wants to be a concert pianist should receive more training than the child prodigy.		Economist Paul Krugman agrees mostly with the Rawlsian approach in that he would like to "create the society each of us would want if we didn’t know in advance who we’d be."[62] Krugman elaborated: "If you admit that life is unfair, and that there’s only so much you can do about that at the starting line, then you can try to ameliorate the consequences of that unfairness."[62]		Some theorists have posed a level playing field conception of equality of opportunity,[3][19] similar in many respects to the substantive principle, (although it has been used in different contexts to describe formal equality of opportunity[9]) and it is a core idea regarding the subject of distributive justice espoused by John Roemer[36][63][64] and Ronald Dworkin[65][66] and others. Like the substantive notion, the level playing field conception goes farther than the usual formal approach.[36] The idea is that initial "unchosen inequalities" – prior circumstances over which an individual had no control but which impact his or her success in a given competition for a particular post – these unchosen inequalities should be eliminated as much as possible, according to this conception. According to Roemer, society should "do what it can to level the playing field so that all those with relevant potential will eventually be admissible to pools of candidates competing for positions.[36] Afterwards, when an individual competes for a specific post, he or she might make specific choices which cause future inequalities – and these inequalities are deemed acceptable because of the previous presumption of fairness.[67] And this system helps undergird the legitimacy of a society's divvying up of roles as a result in the sense that it makes certain achieved inequalities "morally acceptable," according to persons who advocate this approach.[3] This conception has been contrasted to the substantive version among some thinkers, and it usually has ramifications for how society treats young persons in such areas as education and socialization and health care. But this conception has been criticized as well.[68][69][70] Rawls postulated the difference principle which argued that "inequalities are justified only if needed to improve the lot of the worst off, for example by giving the talented an incentive to create wealth."[2][23][66]		There is some overlap among these different conceptions with the term meritocracy which describes an administrative system which rewards such factors as individual intelligence, credentials, education, morality, knowledge or other criteria believed to confer merit. Equality of opportunity is often seen as a major aspect of a meritocracy.[2][3] One view was that equality of opportunity was more focused on what happens before the race begins, while meritocracy is more focused on fairness at the competition stage.[71] The term meritocracy can also be used in a negative sense to refer to a system in which an elite hold themselves in power by controlling access to merit (via access to education, experience, or bias in assessment or judgment).		There is general agreement that equality of opportunity is good for society, although there are diverse views about how it is good, since it is a value judgement.[23] It is generally viewed as a positive political ideal in the abstract sense.[3] In nations where equality of opportunity is absent, it can negatively impact economic growth, according to some views; one report in Al Jazeera suggested that Egypt, Tunisia, and other Middle Eastern nations were stagnating economically in part because of a dearth of equal opportunity.[72] The principle of equal opportunity can conflict with notions of meritocracy in circumstances in which individual differences in human abilities are believed to be determined mostly by genetics; in such circumstances, there can be conflict about how to achieve fairness in such situations.[73]		There is general agreement that programs to bring about certain types of equality of opportunity can be difficult, and that efforts to cause one result often have unintended consequences or cause other problems. There is agreement that the formal approach is easier to implement than the others, although there are difficulties there too.		A government policy that requires equal treatment can pose problems for lawmakers. A requirement for government to provide equal health care services for all citizens can be prohibitively expensive. If government seeks equality of opportunity for citizens to get health care by rationing services using a maximization model to try to save money, new difficulties might emerge. For example, trying to ration health care by maximizing the "quality-adjusted years of life" might steer monies away from disabled persons even though they may be more deserving, according to one analysis.[3][74] In another instance, BBC News questioned whether it was wise to ask female army recruits to undergo the same strenuous tests as their male counterparts, since many women were being injured as a result.[75]		Age discrimination can present vexing challenges for policymakers trying to implement equal opportunity.[3][76][77] According to several studies, attempts to be equally fair to both a young and an old person are problematic because the older person has presumably fewer years left to live, and it may make more sense for a society to invest greater resources in a younger person's health.[78][79] Treating both persons equally, while following the letter of the equality of opportunity, seems unfair from a different perspective.		Efforts to achieve equal opportunity along one dimension can exacerbate unfairness in other dimensions. For example, take public bathrooms: if, for the sake of fairness, the physical area of men's and women's bathrooms is equal, the overall result may be unfair since men can use urinals, which require less physical space.[80] In other words, a more fair arrangement may be to allot more physical space for women's restrooms. A researcher explained:		By creating men's and women's rooms of the same size, society guarantees that individual women will be worse off then individual men.		Another difficulty is that it is hard for a society to bring substantive equality of opportunity to every type of position or industry. If a nation focuses efforts on some industries or positions, then people with other talents may be left out. For example, in an example in the Stanford Encyclopedia of Philosophy, a warrior society might provide equal opportunity for all kinds of people to achieve military success through fair competition, but people with non-military skills such as farming may be left out.[3]		Lawmakers have run into problems trying to implement equality of opportunity. In 2010 in Britain, a legal requirement "forcing public bodies to try to reduce inequalities caused by class disadvantage" was scrapped after much debate, and replaced by a hope that organizations would try to focus more on "fairness" than "equality"; fairness is generally seen as a much vaguer concept than equality,[81] but easier for politicians to manage if they are seeking to avoid fractious debate. In New York City, mayor Ed Koch tried to find ways to maintain the "principle of equal treatment" while arguing against more substantive and abrupt transfer payments called minority set-asides.[82]		Many countries have specific bodies tasked with looking at equality of opportunity issues; in the United States, for example, it is the Equal Employment Opportunity Commission;[16][83] in Britain, there is the Equality of opportunity committee[24] as well as the Equality and Human Rights Commission;[38] in Canada, the Royal Commission on the Status of Women has "equal opportunity as its precept."[84] In China, the Equal Opportunities Commission handles matters regarding ethnic prejudice.[85] In addition, there have been political movements pushing for equal treatment, such as the Women's Equal Opportunity League which in the early decades of the twentieth century, pushed for fair treatment by employers in the U.S.[86] One of the group's members explained:		I am not asking for sympathy but for an equal right with men to earn my own living in the best way open and under the most favorable conditions that I could choose for myself.		The consensus view is that trying to measure equality of opportunity is difficult[71] whether examining a single hiring decision or looking at groups over time.		It is difficult to prove unequal treatment although statistical analysis can provide indications of problems, but it is subject to conflicts over interpretation and methodological issues. For example, a study in 2007 by the University of Washington examined its own treatment of women. Researchers collected statistics about female participation in numerous aspects of university life, including percentages of women with full professorships (23%), enrollment in programs such as nursing (90%) and engineering (18%).[91] There is wide variation in how these statistics might be interpreted. For example, the 23% figure for women with full professorships could be compared to the total population of women (presumably 50%) perhaps using census data,[92] or it might be compared to the percentage of women with full professorships at competing universities. It might be used in an analysis of how many women applied for the position of full professor compared to how many women attained this position. Further, the 23% figure could be used as a benchmark or baseline figure as part of an ongoing longitudinal analysis to be compared with future surveys to track progress over time.[90][93] In addition, the strength of the conclusions is subject to statistical issues such as sample size and bias. For reasons such as these, there is considerable difficulty with most forms of statistical interpretation.		Statistical analysis of equal opportunity has been done using sophisticated examinations of computer databases. An analysis in 2011 by University of Chicago researcher Stefano Allesina examined 61,000 names of Italian professors by looking at the "frequency of last names", doing one million random drawings, and he suggested that Italian academia was characterized by violations of equal opportunity practices as a result of these investigations.[94] The last names of Italian professors tended to be similar more often than predicted by random chance.[94] The study suggested that newspaper accounts showing that "nine relatives from three generations of a single family (were) on the economics faculty" at the University of Bari were not aberrations, but indicated a pattern of nepotism throughout Italian academia.[94]		There is support for the view that often equality of opportunity is measured by the criteria of equality of outcome,[95] although with difficulty. In one example, an analysis of relative equality of opportunity was done based on outcomes, such as a case to see whether hiring decisions were fair regarding men versus women; the analysis was done using statistics based on average salaries for different groups.[96][97] In another instance, a cross-sectional statistical analysis was conducted to see whether social class affected participation in the United States Armed Forces during the Vietnam War; a report in Time Magazine by MIT suggested that soldiers came from a variety of social classes, and that the principle of equal opportunity had worked,[98] possibly because soldiers had been chosen by a lottery process for conscription. In college admissions, equality of outcome can be measured directly by comparing offers of admission given to different groups of applicants; for example, there have been reports in newspapers of discrimination against Asian-Americans regarding college admissions in the United States which suggest that Asian-American applicants need higher grades and test scores to win admission to prestigious universities than other ethnic groups.[99][100]		Equal opportunity has been described as a fundamental basic notion in business and commerce, and described by economist Adam Smith as a basic economic precept.[1] There has been research suggesting that "competitive markets will tend to drive out such discrimination" since employers or institutions which hire based on arbitrary criteria will be weaker as a result, and not perform as well as firms which embrace equality of opportunity.[3] Firms competing for overseas contracts have sometimes argued in the press for equal chances during the bidding process, such as when American oil corporations wanted equal shots at developing oil fields in Sumatra;[101] and firms, seeing how fairness is beneficial while competing for contracts, can apply the lesson to other areas such as internal hiring and promotion decisions. A report in USA Today suggested that the goal of equal opportunity was "being achieved throughout most of the business and government labor markets because major employers pay based on potential and actual productivity."[96] Fair opportunity practices include measures taken by an organization to ensure fairness in the employment process. A basic definition of equality is the idea of equal treatment and respect. In job advertisements and descriptions, the fact that the employer is an equal opportunity employer is sometimes indicated by the abbreviations EOE or MFDV which stands for Minority, Female, Disabled, Veteran. Analyst Ross Douthat in The New York Times suggested that equality of opportunity depends on a rising economy which brings new chances for upward mobility, and he suggested that greater equality of opportunity is more easily achieved during "times of plenty."[102] Efforts to achieve equal opportunity can rise and recede, sometimes as a result of economic conditions or political choices.[103]		According to professor David Christian of Macquarie University, an underlying Big History trend has been a shift from seeing people as resources to exploit towards a perspective of seeing people as individuals to empower. According to Christian, in many ancient agrarian civilizations, roughly nine of every ten persons was a peasant exploited by a ruling class. In the past thousand years, there has been a gradual movement in the direction of greater respect for equal opportunity, as political structures based on generational hierarchies and feudalism broke down during the late Middle Ages and new structures emerged during the Renaissance. Monarchies were replaced by democracies; kings were replaced by parliaments and congresses. Slavery was abolished generally. The new entity of the nation state emerged with highly specialized parts, including corporations, laws, and new ideas about citizenship, and values about individual rights found expression in constitutions, laws, and statutes.		In the United States, one legal analyst suggested that the real beginning of the modern sense of equal opportunity was in the Fourteenth Amendment which provided "equal protection under the law."[21] The amendment did not mention equal opportunity directly, but it helped undergird a series of later rulings which dealt with legal struggles, particularly by African Americans and later women, seeking greater political and economic power in the growing republic. In 1933, a congressional "Unemployment Relief Act" forbade discrimination "on the basis of race, color, or creed".[21] The Supreme Court's 1954 Brown v Board of Education decision furthered government initiatives to end discrimination.[21] In 1961, President Kennedy signed Executive Order 10925 which enabled a presidential committee on equal opportunity,[21] which was soon followed by President Johnson's Executive Order 11246.[104] The Civil Rights Act of 1964 became the legal underpinning of equal opportunity in employment.[21] Businesses and other organizations learned to comply with the rulings by specifying fair hiring and promoting practices and posting these policy notices on bulletin boards, employee handbooks, and manuals as well as training sessions and films.[21] Courts dealt with issues about equal opportunity, such as the 1989 Wards Cove decision, the Supreme Court ruled that statistical evidence, by itself, was insufficient to prove racial discrimination. The Equal Employment Opportunity Commission was established, sometimes reviewing charges of discrimination cases which numbered in the tens of thousands annually during the 1990s.[21] Some law practices specialized in employment law. Conflict between formal and substantive approaches manifested itself in backlashes, sometimes described as reverse discrimination, such as the Bakke case when a white male applicant to medical school sued on the basis of being denied admission because of a quota system preferring minority applicants.[5][105] In 1990, the Americans with Disabilities Act prohibited discrimination against disabled persons, including cases of equal opportunity.[106][107] In 2008, the Genetic Information Nondiscrimination Act prevents employers from using genetic information when hiring, firing, or promoting employees.[108]		Many economists measure the degree of equal opportunity with measures of Economic mobility. For instance, Joseph Stiglitz asserts that with five economic divisions and full equality of opportunity, "20 percent of those in the bottom fifth would see their children in the bottom fifth. Denmark almost achieves that – 25 percent are stuck there. Britain, supposedly notorious for its class divisions, does only a little worse (30 percent). That means they have a 70 percent chance of moving up. The chances of moving up in America, though, are markedly smaller (only 58 percent of children born to the bottom group make it out), and when they do move up, they tend to move up only a little." Similar analyses can be performed for each economic division and overall. They all show how far from the ideal all industrialized nations are, and how correlated measures of equal opportunity are with income inequality and wealth inequality.[109]		There is agreement that the concept of equal opportunity lacks a precise definition.[3][110] While it generally describes "open and fair competition" with equal chances for achieving sought-after jobs or positions[5] as well as an absence of discrimination,[5][13][111] the concept is elusive with a "wide range of meanings".[41] It is hard to measure, and implementation poses problems[3] as well as disagreements about what to do.[19]		There have been various criticisms directed at both the substantive and formal approach. One account suggests that left-leaning thinkers who advocate equality of outcome fault even formal equality of opportunity on the grounds that it "legitimates inequalities of wealth and income."[19] John William Gardner suggested several views: (1) that inequalities will always exist regardless of trying to erase them (2) that bringing everyone "fairly to the starting line" without dealing with the "destructive competitiveness that follows" (3) any equalities achieved will entail future inequalities.[112] Substantive equality of opportunity has led to concerns that efforts to improve fairness "ultimately collapses into the different one of equality of outcome or condition."[19]		Economist Larry Summers advocated an approach of focusing on equality of opportunity and not equality of outcomes and that the way to strengthen equal opportunity was to bolster public education.[113] A contrasting report in The Economist criticized efforts to contrast equality of opportunity and equality of outcome as being opposite poles on a hypothetical ethical scale, such that equality of opportunity should be the "highest ideal" while equality of outcome was "evil".[114] Rather, the report argued that any difference between the two types of equality was illusory and that both terms were highly interconnected.[114] According to this argument, wealthier people have greater opportunities––wealth itself can be considered as "distilled opportunity"––and children of wealthier parents have access to better schools, health care, nutrition and so forth.[114] Accordingly, people who endorse equality of opportunity may like the idea of it in principle, yet at the same time they would be unwilling to take the extreme steps or "titanic interventions" necessary to achieve real intergenerational equality.[114] A slightly different view in The Guardian suggested that equality of opportunity was merely a "buzzword" to sidestep the thornier political question of income inequality.[115]		There is speculation that since equality of opportunity is only one of sometimes competing "justice norms", there is a risk that following equality of opportunity too strictly might cause problems in other areas.[3][116] A hypothetical example was suggested: suppose wealthier people gave excessive amounts of campaign contributions; suppose, further, that these contributions resulted in better regulations; then, laws limiting such contributions on the basis of equal opportunity for all political participants may have the unintended long term consequence of making political decision-making lackluster and possibly hurting the groups that it was trying to protect.[3] Philosopher John Kekes makes a similar point in his book The Art of Politics: The New Betrayal of America and How to Resist It in which he suggests that there is a danger to elevating any one particular political good – including equality of opportunity – without balancing competing goods such as justice, property rights, and others.[117] Kekes advocated having a balanced perspective, including a continuing dialog between cautionary elements and reform elements.[117] A similar view was expressed in The Economist:		It strikes us as wrong – or not obviously right – that some people starve while others have private jets. We are uncomfortable when university professors earn less, for example, than junior lawyers. But equality appears to pull against other important ideals such as liberty and efficiency.		Economist Paul Krugman sees equality of opportunity as a "non-Utopian compromise" which works and is a "pretty decent arrangement" which varies from country to country.[62] But there are differing views, such as by Matt Cavanagh, who criticised equality of opportunity in his 2002 book Against Equality of Opportunity.[41] Cavanath favored a limited approach of opposing specific kinds of discrimination as steps to help people get greater control over their lives.[118]		Conservative thinker Dinesh D'Souza criticized equality of opportunity on the basis that "it is an ideal that cannot and should not be realized through the actions of the government" and added that "for the state to enforce equal opportunity would be to contravene the true meaning of the Declaration and to subvert the principle of a free society."[119] D'Souza described how his parenting undermined equality of opportunity:		I have a five-year-old daughter. Since she was born ... my wife and I have gone to great lengths in the Great Yuppie Parenting Race. ... My wife goes over her workbooks. I am teaching her chess. Why are we doing these things? We are, of course, trying to develop her abilities so that she can get the most out of life. The practical effect of our actions, however, is that we are working to give our daughter an edge – that is, a better chance to succeed than everybody else's children. Even though we might be embarrassed to think of it this way, we are doing our utmost to undermine equal opportunity. So are all the other parents who are trying to get their children into the best schools...		D'Souza argued that it was wrong for government to try to bring his daughter down, or to force him to raise up other people's children.[119] But a counterargument is that there is a benefit to everybody, including D'Souza's daughter, to have a society with less anxiety about downward mobility, less class resentment, and less possible violence.[119]		An argument similar to D'Souza's was raised by Nozick in Anarchy, State, and Utopia, who wrote that the only way to achieve equality of opportunity was "directly worsening the situations of those more favored with opportunity, or by improving the situation of those less well-favored."[120] Nozick gave an argument of two suitors competing to marry one "fair lady": X was plain; Y was better looking and more intelligent. If Y didn't exist, then "fair lady" would have married X; but Y exists, so she marries Y. Nozick asks: Does suitor X have a legitimate complaint against Y on the basis of unfairness since Y didn't earn his good looks or intelligence?[121] Nozick suggests that there is no grounds for complaint. Nozick argued against equality of opportunity on the grounds that it violates the rights of property, since the equal opportunity maxim interferes with an owner's right to do what he or she pleases with a property.[3] Property rights were a major component of the philosophy of John Locke, and are sometimes referred to as Lockean rights.[3] The sense of the argument is along these lines: equal opportunity rules regarding, say, a hiring decision within a factory, made to bring about greater fairness, violate a factory owner's rights to run the factory as he or she sees best; it has been argued that a factory owner's right to property encompasses all decision-making within the factory as being part of those property rights. That some people's "natural assets" were unearned is irrelevant to the equation, according to Nozick, and he argued that people are nevertheless entitled to enjoy these assets and other things freely given by others.[23]		Friedrich Hayek felt that luck was too much of a variable in economics, such that one can not devise a system with any kind of fairness when many market outcomes are unintended.[23] By sheer chance or random circumstances, a person may become wealthy just by being in the right place and time, and Hayek argued that it is impossible to devise a system to make opportunities equal without knowing how such interactions may play out.[23] Hayek saw not only equality of opportunity but all of social justice as a "mirage".[23]		Some conceptions of equality of opportunity, particularly the substantive and level playing field variants, have been criticized on the basis that they make assumptions to the effect that people have similar genetic makeups.[3] Other critics have suggested that social justice is more complex than mere equality of opportunity.[3] Robert Nozick made the point that what happens in society can not always be reduced to competitions for a coveted position; in 1974, Nozick wrote that "life is not a race in which we all compete for a prize which someone has established" and that there is "no unified race" and there is not some one person "judging swiftness."[121]		
Frictional unemployment is the unemployment that results from time spent between jobs when a worker is searching for, or transitioning from one job to another.[1] It is sometimes called search unemployment and can be based on the circumstances of the individual.						Frictional unemployment exists because both jobs and workers are heterogeneous, and a mismatch can result between the characteristics of supply and demand. Such a mismatch can be related to skills, payment, worktime, location, attitude, taste, and a multitude of other factors. New entrants (such as graduating students) and re-entrants (such as former homemakers) can also suffer a spell of frictional unemployment. Workers as well as employers accept a certain level of imperfection, risk or compromise, but usually not right away; they will invest some time and effort to find a match. This is in fact beneficial to the economy since it results in a better allocation of resources. However, if the search takes too long and mismatches are too frequent, the economy suffers, since some work will not get done. Therefore, governments will seek ways to reduce unnecessary frictional unemployment.		Frictional unemployment is related to and compatible with the concept of full employment because both suggest reasons why full employment is never reached.		The frictions in the labor market are sometimes illustrated graphically with a Beveridge curve, a downward-sloping, convex curve that shows a fixed relationship between the unemployment rate on one axis and the vacancy rate on the other. Changes in the supply of or demand for labor cause movements along this curve. An increase in labor market frictions will shift the curve outwards, and vice versa. A longer term form of frictional unemployment is structural unemployment which is very similar.		One kind of frictional unemployment is called wait unemployment: it refers to the effects of the existence of some sectors where employed workers are paid more than the market-clearing equilibrium wage. Not only does this restrict the amount of employment in the high-wage sector, but it attracts workers from other sectors who wait to try to get jobs there. The main problem with this theory is that such workers will likely "wait" while having jobs, so that they are not counted as unemployed. In Hollywood, for example, those who are waiting for acting jobs also wait on tables in restaurants for pay (while acting in "Equity Waiver" plays at night for no pay). However, these workers might be seen as underemployed (definition 1).		Policies to reduce frictional unemployment include:		
At-will employment is a term used in U.S. labor law for contractual relationships in which an employee can be dismissed by an employer for any reason (that is, without having to establish "just cause" for termination), and without warning.[1] When an employee is acknowledged as being hired "at will", courts deny the employee any claim for loss resulting from the dismissal. The rule is justified by its proponents on the basis that an employee may be similarly entitled to leave his or her job without reason or warning.[2] In contrast, the practice is seen as unjust by those who view the employment relationship as characterized by inequality of bargaining power.[3]		At-will employment gradually became the default rule under the common law of the employment contract in most states during the late 19th century, and was endorsed by the U.S. Supreme Court during the Lochner era, when members of the U.S. judiciary consciously sought to prevent government regulation of labor markets.[4] Over the 20th century, many states modified the rule by adding an increasing number of exceptions, or by changing the default expectations in the employment contract altogether. In workplaces with a trade union recognized for purposes of collective bargaining, and in many public sector jobs, the normal standard for dismissal is that the employer must have a "just cause". Otherwise, subject to statutory rights (particularly the discrimination prohibitions under the Civil Rights Act), most states adhere to the general principle that employer and employee may contract for the dismissal protection they choose.[5] At-will employment remains controversial, and remains a central topic of debate in the study of law and economics, especially with regard to the macroeconomic efficiency of allowing employers to summarily and arbitrarily terminate employees.						At-will employment is generally described as follows: "any hiring is presumed to be 'at will'; that is, the employer is free to discharge individuals 'for good cause, or bad cause, or no cause at all,' and the employee is equally free to quit, strike, or otherwise cease work."[6] In an October 2000 decision largely reaffirming employers' rights under the at-will doctrine, the Supreme Court of California explained:		[A]n employer may terminate its employees at will, for any or no reason ... the employer may act peremptorily, arbitrarily, or inconsistently, without providing specific protections such as prior warning, fair procedures, objective evaluation, or preferential reassignment ... The mere existence of an employment relationship affords no expectation, protectable by law, that employment will continue, or will end only on certain conditions, unless the parties have actually adopted such terms.[7]		At-will employment disclaimers are a staple of employee handbooks in the United States. It is common for employers to define what at-will employment means, explain that an employee’s at-will status cannot be changed except in a writing signed by the company president (or chief executive), and require that an employee sign an acknowledgment of his or her at-will status.[8] However, the National Labor Relations Board has opposed as unlawful the practice of including in such disclaimers language declaring that the at-will nature of the employment cannot be changed without the written consent of senior management.[note 1][9]		The original common law rule for dismissal of employees according to William Blackstone envisaged that, unless another practice was agreed, employees would be deemed to be hired for a fixed term of one year.[10] Over the 19th century, most states in the North adhered to the rule that the period by which an employee was paid (a week, a month or a year) determined the period of notice that should be given before a dismissal was effective. For instance, in 1870 in Massachusetts, Tatterson v. Suffolk Mfg Co[11] held that an employee's term of hiring dictated the default period of notice.[12] By contrast, in Tennessee, a court stated in 1884 that an employer should be allowed to dismiss any worker, or any number of workers, for any reason at all.[13] An individual, or a collective agreement, according to the general doctrine of freedom of contract could always stipulate that an employee should only be dismissed for a good reason, or a "just cause", or that elected employee representatives would have a say on whether a dismissal should take effect. However, the position of the typical 19th-century worker meant that this was rare.		The at-will practice is typically traced to a treatise published by Horace Gray Wood in 1877, called Master and Servant.[14] Wood cited four U.S. cases as authority for his rule that when a hiring was indefinite, the burden of proof was on the servant to prove that an indefinite employment term was for one year.[15] In Toussaint v. Blue Cross & Blue Shield of Michigan, the Court noted that "Wood’s rule was quickly cited as authority for another proposition."[16] Wood, however, misinterpreted two of the cases which in fact showed that in Massachusetts and Michigan, at least, the rule was that employees should have notice before dismissal according to the periods of their contract.[17]		In New York, the first case to adopt Wood's rule was Martin v New York Life Ins Co[18] in 1895. Bartlett J asserted that New York law now followed Wood's treatise, which meant that an employee who received $10,000, paid in a salary over a year, could be dismissed immediately. The case did not make reference to the previous authority. Four years earlier, in 1891, Adams v Fitzpatrick[19] had held that New York law followed the general practice of requiring notice similar to pay periods. However, subsequent New York cases continued to follow the at-will rule into the early 20th century.[20]		Some courts saw the rule as requiring the employee to prove an express contract for a definite term in order to maintain an action based on termination of the employment.[21] Thus was born the U.S. at-will employment rule, which allowed discharge for no reason. This rule was adopted by all U.S. states. In 1959, the first judicial exception to the at-will rule was created by one of the California Courts of Appeal.[22] Later, in a 1980 landmark case involving ARCO, the Supreme Court of California endorsed the rule first articulated by the Court of Appeal.[23] The resulting civil actions by employees are now known in California as Tameny actions for wrongful termination in violation of public policy.[24]		Since 1959, several common law and statutory exceptions to at-will employment have been created.		Common law protects an employee from retaliation if the employee disobeys an employer on the grounds that the employer ordered him or her to do something illegal or immoral. However, in the majority of cases, the burden of proof remains upon the discharged employee. No U.S. state but Montana has chosen to statutorily modify the employment at-will rule.[25] In 1987, the Montana legislature passed the Wrongful Discharge from Employment Act (WDEA). The WDEA is unique in that, although it purports to preserve the at-will concept in employment law, it also expressly enumerates the legal bases for a wrongful discharge action.[16] Under the WDEA, a discharge is wrongful only if: "it was in retaliation for the employee's refusal to violate public policy or for reporting a violation of public policy; the discharge was not for good cause and the employee had completed the employer's probationary period of employment; or the employer violated the express provisions of its own written personnel policy."[26]		The doctrine of at-will employment can be overridden by an express contract or civil service statutes (in the case of government employees). As many as 34% of all U.S. employees apparently enjoy the protection of some kind of "just cause" or objectively reasonable requirement for termination that takes them out of the pure "at-will" category, including the 7.5% of unionized private-sector workers, the 0.8% of nonunion private-sector workers protected by union contracts, the 15% of nonunion private-sector workers with individual express contracts that override the at-will doctrine, and the 16% of the total workforce who enjoy civil service protections as public-sector employees.[27]		Under the public policy exception, an employer may not fire an employee if it would violate the state's public policy doctrine or a state or federal statute.		This includes retaliating against an employee for performing an action that complies with public policy (such as repeatedly warning that the employer is shipping defective airplane parts in violation of safety regulations promulgated pursuant to the Federal Aviation Act of 1958[28]), as well as refusing to perform an action that would violate public policy. In this diagram, the pink states have the 'exception', which protects the employee.		As of October 2000,[29] forty-two U.S. states and the District of Columbia recognize public policy as an exception to the at-will rule.[30]		The 8 states which do not have the exception are:		Thirty-six U.S. states (and the District of Columbia) also recognize an implied contract as an exception to at-will employment.[29] Under the implied contract exception, an employer may not fire an employee "when an implied contract is formed between an employer and employee, even though no express, written instrument regarding the employment relationship exists."[29] Proving the terms of an implied contract is often difficult, and the burden of proof is on the fired employee. Implied employment contracts are most often found when an employer's personnel policies or handbooks indicate that an employee will not be fired except for good cause or specify a process for firing. If the employer fires the employee in violation of an implied employment contract, the employer may be found liable for breach of contract.		Thirty-six U.S. states have an implied-contract exception. The fourteen states having no such exception are:		The implied-contract theory to circumvent at-will employment must be treated with caution. In 2006, the Texas Court of Civil Appeals in Matagorda County Hospital District v. Burwell[33] held that a provision in an employee handbook stating that dismissal may be for cause, and requiring employee records to specify the reason for termination, did not modify an employee's at-will employment. The New York Court of Appeals, that state’s highest court, also rejected the implied-contract theory to circumvent employment at will. In Anthony Lobosco, Appellant v. New York Telephone Company/NYNEX, Respondent,[34] the court restated the prevailing rule that an employee could not maintain an action for wrongful discharge where state law recognized neither the tort of wrongful discharge, nor exceptions for firings that violate public policy, and an employee's explicit employee handbook disclaimer preserved the at-will employment relationship. And in the same 2000 decision mentioned above, the Supreme Court of California held that the length of an employee's long and successful service, standing alone, is not evidence in and of itself of an implied-in-fact contract not to terminate except for cause.[7]		Eleven US states have recognized a breach of an implied covenant of good faith and fair dealing as an exception to at-will employment.[29][35] The states are:		Court interpretations of this have varied from requiring "just cause" to denial of terminations made for malicious reasons, such as terminating a long-tenured employee solely to avoid the obligation of paying the employee's accrued retirement benefits. Other court rulings have denied the exception, holding that it is too burdensome upon the court for it to have to determine an employer's true motivation for terminating an employee.[29]		Although all U.S. states have a number of statutory protections for employees, most wrongful termination suits brought under statutory causes of action use the federal anti-discrimination statutes which prohibit firing or refusing to hire an employee because of race, color, religion, sex, national origin, age, or handicap status. Other reasons an employer may not use to fire an at-will employee are:		Examples of federal statutes include:		On the one hand, the doctrine of at-will employment has been heavily criticized for its severe harshness upon employees.[40] It has also been criticized as predicated upon flawed assumptions about the inherent distribution of power and information in the employee-employer relationship.[41] On the other hand, conservative scholars in the field of law and economics such as Professors Richard A. Epstein[42] and Richard Posner[43] credit employment at will as a major factor underlying the strength of the U.S. economy.		At-will employment has also been identified as a reason for the success of Silicon Valley as an entrepreneur-friendly environment.[44]		In a 2009 article surveying the academic literature from both U.S. and international sources, University of Virginia law professor J.H. Verkerke explained that "although everyone agrees that raising firing costs must necessarily deter both discharges and new hiring, predictions for all other variables depend heavily on the structure of the model and assumptions about crucial parameters."[27] The effect of raising firing costs is generally accepted in mainstream economics (particularly neoclassical economics); for example, professors Tyler Cowen and Alex Tabarrok explain in their macroeconomics textbook that employers become more reluctant to hire employees if they are uncertain about their ability to immediately fire them.[45]		The first major empirical study on the impact of exceptions to at-will employment was published in 1992 by James N. Dertouzos and Lynn A. Karoly of the RAND Corporation,[46] which found that recognizing tort exceptions to at-will could cause up to a 2.9% decline in aggregate employment and recognizing contract exceptions could cause an additional decline of 1.8%. According to Verkerke, the RAND paper received "considerable attention and publicity."[27] Indeed, it was favorably cited in a 2010 book published by the libertarian Cato Institute.[47]		However, a 2000 paper by Thomas Miles found no effect upon aggregate employment but found that adopting the implied contract exception causes use of temporary employment to rise as much as 15%.[27] Later work by David Autor in the mid-2000s identified multiple flaws in Miles' methodology, found that the implied contract exception decreased aggregate employment 0.8 to 1.6%, and confirmed the outsourcing phenomenon identified by Miles, but also found that the tort exceptions to at-will had no statistically significant influence.[27] Autor and colleagues later found in 2007 that the good faith exception does reduce job flows, and seems to cause labor productivity to rise but total factor productivity to drop.[27] In other words, employers forced to find a "good faith" reason to fire an employee tend to automate operations to avoid hiring new employees, but also suffer an impact on total productivity because of the increased difficulty in discharging unproductive employees. Other researchers have found that at-will exceptions have a negative effect on reemployment of unemployed (but not already employed) workers, and that hedonic regressions on at-will exceptions show large negative effects on individual welfare with regard to home values, rents, and wages.[27] Verkerke also explains that several international comparative studies have found that "job security has a large negative effect on employment rates."[27]		 This article incorporates public domain material from the United States Government document "The employment-at-will doctrine: three major exceptions" by Charles J. Muhl, U.S. Bureau of Labor Statistics (retrieved on February 6, 2010).		
The working poor are working people whose incomes fall below a given poverty line. Depending on how one defines "working" and "poverty," someone may or may not be counted as part of the working poor.		While poverty is often associated with joblessness, a significant proportion of the poor are actually employed.[1][2] Largely because they are earning such low wages, the working poor face numerous obstacles that make it difficult for many of them to find and keep a job, save up money, and maintain a sense of self-worth.[3]		The official working poverty rate in the US has remained somewhat static over the past four decades, but many social scientists argue that the official rate is set too low, and that the proportion of workers facing significant financial hardship has instead increased over the years. Changes in the economy, especially the shift from a manufacturing-based to a service-based economy, have resulted in the polarization of the labor market. This means that there are more jobs at the top and the bottom of the income spectrum, but fewer jobs in the middle.[4]		There are a wide range of anti-poverty policies that have been shown to improve the situation of the working poor. Research suggests that increasing welfare state generosity is the most effective way to reduce poverty and working poverty.[5][6] Other tools available to governments are increasing minimum wages across a nation, and absorbing educational and health care costs for children of the working poor.						In the United States, the issue of working poverty was initially brought to the public's attention during the Progressive Era (1890s–1920s). Progressive Era thinkers like Robert Hunter, Jane Addams, and W.E.B. Du Bois saw society's unequal opportunity structure as the root cause of poverty and working poverty, but they also saw a link between moral factors and poverty. In his study of Philadelphia's African American neighborhoods, W.E.B. Du Bois draws a distinction between "hardworking" poor people who fail to escape poverty due to racial discrimination and those who are poor due to moral deficiencies such as laziness or lack of perseverance.[7]		After the Great Depression, the New Deal, and World War II, the United States experienced an era of prosperity during which most workers experienced significant gains in wages and working conditions. During this period (1930s–1950s), scholars shifted their attention away from poverty and working poverty. However, in the late 1950s and early 1960s American scholars and policymakers began to revisit the problem. Influential books like John Kenneth Galbraith's The Affluent Society (1958)[8] and Michael Harrington's The Other America (1962)[9] reinvigorated the discussions on poverty and working poverty in the United States.		Since the start of the War on Poverty in the 1960s, scholars and policymakers on both ends of the political spectrum have paid an increasing amount of attention to working poverty. One of the key ongoing debates concerns the distinction between the working and the nonworking (unemployed) poor. Conservative scholars tend to see nonworking poverty as a more urgent problem than working poverty because they believe that non-work is a moral hazard that leads to welfare dependency and laziness, whereas work, even poorly paid work, is morally beneficial. In order to solve the problem of nonworking poverty, some conservative scholars argue that the government must stop "coddling" the poor with welfare benefits like AFDC/TANF.[10]		On the other hand, liberal scholars and policymakers often argue that most working and nonworking poor people are quite similar. Studies comparing single mothers on and off welfare show that receiving welfare payments does not degrade a person's desire to find a job and get off of welfare.[11] The main difference between the working and the nonworking poor, they argue, is that the nonworking poor have a more difficult time overcoming basic barriers to entry into the labor market, such as arranging for affordable childcare, finding housing near potential jobs, or arranging for transportation to and from work. In order to help the nonworking poor gain entry into the labor market, liberal scholars and policymakers argue that the government should provide more housing assistance, childcare, and other kinds of aid to poor families.[12]		Discussions about the alleviation of working poverty are also politically charged. Conservative scholars and policymakers often attribute the prevalence of inequality and working poverty to overregulation and overtaxation, which they claim constricts job growth. In order to lower the rate of working poverty, conservatives advocate reducing welfare benefits and enacting less stringent labor laws.[10][13] On the other hand, many liberals argue that working poverty can only be solved through increased, not decreased, government intervention. This government intervention could include workplace reforms (such as higher minimum wages, living wage laws, job training programs, etc.) and an increase in government transfers (such as housing, food, childcare, and healthcare subsidies).[3][5][6][11]		According to the US Department of Labor, the working poor "are persons who spent at least 27 weeks [in the past year] in the labor force (that is, working or looking for work), but whose incomes fell below the official poverty level."[1] In other words, if someone spent more than half of the past year in the labor force without earning more than the official poverty threshold, the US Department of Labor would classify them as "working poor." (Note: The official poverty threshold, which is set by the US Census Bureau, varies depending on the size of a family and the age of the family members. The US Bureau of Labor Statistics calculates working poverty rates for all working individuals, all families with at least one worker, and all "unrelated individuals." The individual-level working poverty rate calculates the percentage of all workers whose incomes fall below the poverty threshold. In 2009, the individual-level working poverty rate in the US was 7%, compared to 4.7% in 2000. The family-level working poverty rate only includes families of two or more people who are related by birth, marriage, or adoption. According to the Bureau of Labor Statistics' definition of family-level working poverty, a family is working poor if the combined cash income of the family falls below the poverty threshold for a family of their size. In 2009, the family-level working poverty rate in the US was 7.9%, compared to 5.6% in 2000. Finally, the unrelated individual working poverty rate measures working poverty among those who do not currently live with any family members. In 2009, 11.7% of employed unrelated individuals were poor, compared to 7.6% in 2000.[1][14]		In Europe and other non-US, high-income countries, poverty and working poverty are defined in relative terms. A relative measure of poverty is based on a country's income distribution rather than an absolute amount of money. Eurostat, the statistical office of the European Union, classifies a household as poor if its income is less than 60 percent of the country's median household income. According to Eurostat, a relative measure of poverty is appropriate because "minimal acceptable standards usually differ between societies according to their general level of prosperity: someone regarded as poor in a rich developed country might be regarded as rich in a poor developing country."[15]		When conducting cross-national research on working poverty, scholars tend to use a relative measure of poverty. In these studies, to be classified as "working poor," a household must satisfy the following two conditions: 1) at least one member of the household must be "working" (which can be defined in various ways), and 2) the total household income must be less than 60% (or 50%, or 40%) of the median income for that country.[5][6] Brady, Fullerton, and Cross's 2010 cross-national study on working poverty in high-income countries defines a household as working poor if 1) it has at least one employed person and 2) the total household income falls below 50% of the median income for that country. According to this relative definition, the US's working poverty rate was 11% in the year 2000, nearly double the rate produced by the US government's absolute definition. For the same year, Canada's working poverty rate was 7.8%, the UK's was 4%, and Germany's was 3.8%.[6]		Poverty is often associated with joblessness, but a large proportion of poor people are actually working or looking for work. In 2009, according to the US Census Bureau's official definition of poverty, 8.8 million US families were below the poverty line (11.1% of all families). Of these families, 5.19 million, or 58.9%, had at least one person who was classified as working. In the same year, there were 11.7 million unrelated individuals (people who do not live with family members) whose incomes fell below the official poverty line (22% of all unrelated individuals). 3.9 million of these poor individuals, or 33%, were part of the working poor.[1][2] The cost of raising a child from birth to age 18 for a middle-income, two-parent family averaged $226,920 last year (not including college), according to the U.S. Department of Agriculture. That's up nearly 40% -- or more than $60,000 -- from 10 years ago. Just one year of spending on a child can cost up to $13,830 in 2010, compared to $9,860 a decade ago.		Using the US Census Bureau's definition of poverty, the working poverty rate seems to have remained relatively stable since 1978.[1] However, many scholars have argued that the official poverty threshold is too low, and that real wages and working conditions have actually declined for many workers over the past three or four decades. Social scientists like Arne Kalleberg have found that the decline in US manufacturing and the subsequent polarization of the labor market has led to an overall worsening of wages, job stability, and working conditions for people with lower skill levels and less formal education. From the mid-1940s to the mid-1970s, manufacturing jobs offered many low-skilled and medium-skilled workers stable, well-paying jobs. Due to global competition, technological advances, and other factors, US manufacturing jobs have been disappearing for decades. (From 1970 to 2008, the percentage of the labor force employed in the manufacturing sector shrank from 23.4% to 9.1%.[16][17]) During this period of decline, job growth became polarized on either end of the labor market. That is, the jobs that replaced medium-pay, low- to medium-skill manufacturing jobs were high-paying, high-skill jobs and low-paying, low-skill jobs. Therefore, many low- to medium-skilled workers who would have been able to work in the manufacturing sector in 1970 must now take low-paying, precarious jobs in the service sector.[4]		Other high-income countries have also experienced declining manufacturing sectors over the past four decades, but most of them have not experienced as much labor market polarization as the United States. Labor market polarization has been the most severe in liberal market economies like the US, Britain, and Australia. Countries like Denmark and France have been subject to the same economic pressures, but due to their more "inclusive" (or "egalitarian") labor market institutions, such as centralized and solidaristic collective bargaining and strong minimum wage laws, they have experienced less polarization.[4]		Cross-national studies have found that European countries' working poverty rates are much lower than the US's. Most of this difference can be explained by the fact that European countries' welfare states are more generous than the US's.[5][6] The relationship between generous welfare states and low rates of working poverty is elaborated upon in the "Risk Factors" and "Anti-Poverty Policies" sections.		The following graph uses data from Brady, Fullerton, and Cross (2010) to show the working poverty rates for a small sample of countries. Brady, Fullerton, and Cross (2010) accessed this data through the Luxembourg Income Study. This graph measures household, rather than person-level, poverty rates. A household is coded as "poor" if its income is less than 50% of its country's median income. This is a relative, rather than absolute, measure of poverty. A household is classified as "working" if at least one member of the household was employed at the time of the survey. The most important insight contained in this graph is that the US has strikingly higher working poverty rates than European countries.				There are five major categories of risk factors that increase a person's likelihood of experiencing working poverty: sectoral factors, demographic factors, economic factors, labor market institutions, and welfare generosity. Working poverty is a phenomenon that affects a very wide range of people, but there are some employment sectors, demographic groups, political factors, and economic factors that are correlated with higher rates of working poverty than others. Sectoral and demographic factors help explain why certain people within a given country are more likely than others to be working poor. Political and economic factors can explain why different countries have different working poverty rates.		Sectoral tendencies Working poverty is not distributed equally among employment sectors. The service sector has the highest rate of working poverty. In fact, 13.3% of US service sector workers found themselves below the poverty line in 2009.[1] Examples of low-wage service sector workers include fast-food workers, home health aids, waiters/waitresses, and retail workers.		Although the service sector has the highest rate of working poverty, a significant portion of the working poor are blue-collar workers in the manufacturing, agriculture, and construction industries. Most manufacturing jobs used to offer generous wages and benefits, but manufacturing job quality has declined over the years. Nowadays, many US manufacturing jobs are located in right-to-work states, where it is nearly impossible for workers to form a union. This means that manufacturing employers are able to pay lower wages and offer fewer benefits than they used to.		Demographic factors In her book, No Shame in My Game, Katherine Newman finds that "[t]he nation's young, its single parents, the poorly educated, and minorities are more likely than other workers to be poor" (p. 42).[18] These factors, in addition to being part of a large household, being part of a single-earner household, being female, and having a part-time (instead of a full-time) job have been found to be important working poverty "risk factors." Immigrant workers and self-employed workers are also more likely to be working poor than other kinds of workers.[5][6]		Economic factors There is a widespread assumption that economic growth leads to tighter labor markets and higher wages. However, the evidence suggests that economic growth does not always benefit each part of the population equally. For example, the 1980s was a period of economic growth and prosperity in the United States, but most of the economic gains were concentrated at the top of the income spectrum. This means that people near the bottom of the labor market did not benefit from the economic gains of the 1980s. In fact, many have argued that low-skilled workers experienced declining prosperity in the 1980s.[19] Therefore, changing economic conditions do not have as large of an impact on working poverty rates as one might expect.		Labor market institutions Labor markets can be egalitarian, efficient, or somewhere in the middle. According to Brady, Fullerton, and Cross (2010), "[e]fficient labor markets typically feature flexibility, low unemployment, and higher economic growth, and facilitate the rapid hiring and firing of workers. Egalitarian labor markets are bolstered by strong labor market institutions, higher wages, and greater security" (p562). The United States has an efficient labor market, whereas most European countries have egalitarian labor markets. Each system has its drawbacks, but the egalitarian labor market model is typically associated with lower rates of working poverty. One tradeoff to this is that the "lowest skilled and least employable" people are sometimes excluded from an egalitarian labor market, and must instead rely on government aid in order to survive (p. 563).[6] If the United States switched from an efficient to an egalitarian labor market, it might have to increase its welfare state generosity in order to cope with a higher unemployment rate.		Centralized wage bargaining is a key component of egalitarian labor markets. In a country with centralized wage bargaining institutions, wages for entire industries are negotiated at the regional or national level. This means that similar workers earn similar wages, which reduces income inequality. Lohmann (2009) finds that countries with centralized wage bargaining institutions have lower rates of "pre-transfer" working poverty.[5] The "pre-transfer" working poverty rate is the percentage of workers who fall below the poverty threshold based on their earned wages (not counting government transfers).		Welfare state generosity Cross-national studies are in agreement that the most important factor affecting working poverty rates is welfare state generosity. A generous welfare state spends a higher proportion of its GDP on things like unemployment insurance, social security, family assistance, childcare subsidies, healthcare subsidies, housing subsidies, transportation subsidies, and food subsidies. Studies on working poverty have found that these kinds of government spending can pull a significant number of people out of poverty, even if they earn low wages. Lohmann's 2009 study shows that welfare state generosity has a significant impact on the "post-transfer" rate of working poverty.[5] The "post-transfer" rate of working poverty is the percentage of working households that fall below the poverty threshold after government aid has been taken into account.		Different types of transfers benefit different kinds of low-wage families. Family benefits will benefit households with children and unemployment benefits will benefit households that include workers with significant work experience. Transfers such as old-age benefits are unlikely to benefit low-wage households unless the elderly are living in the same household. Sometimes, even when benefits are available, those who qualify do not take advantage of them. Migrants in particular are less likely to take advantage of the available benefits.[5]		The working poor face many of the same daily life struggles as the nonworking poor, but they also face some unique obstacles. Some studies, many of them qualitative, provide detailed insights into the obstacles that hinder workers' ability to find jobs, keep jobs, and make ends meet. Some of the most common struggles faced by the working poor are finding affordable housing, arranging transportation to and from work, buying basic necessities, arranging childcare, having unpredictable work schedules, juggling two or more jobs, and coping with low-status work.		Housing Working poor people who do not have friends or relatives with whom they can live often find themselves unable to rent an apartment of their own. Although the working poor are employed at least some of the time, they often find it difficult to save enough money for a deposit on a rental property. As a result, many working poor people end up in living situations that are actually more costly than a month-to-month rental. For instance, many working poor people, especially those who are in some kind of transitional phase, rent rooms in week-to-week motels. These motel rooms tend to cost much more than a traditional rental, but they are accessible to the working poor because they do not require a large deposit. If someone is unable or unwilling to pay for a room in a motel, they might live in his/her car, in a homeless shelter, or on the street. This is not a marginal phenomenon; in fact, according to the 2008 US Conference of Mayors, one in five homeless people are currently employed.[20]		Of course, some working poor people are able to access housing subsidies (such as a Section 8 Housing Choice Voucher) to help cover their housing expenses. However, these housing subsidies are not available to everyone who meets the Section 8 income specifications. In fact, less than 25% of people who qualify for a housing subsidy receive one.[12]		Education The issue with education starts many times with the working poor from childhood and follows them into their struggle for a substantial income. Children growing up in families of the working poor are not provided the same educational opportunities as their middle-class counterpart. In many cases the low income community is filled with school that are lacking necessities and support needed to form a solid education.[21] This follows students as they continue in education. In many cases this hinders the possibility for America's youth to continue on to higher education. The grades and credits just are not attained in many cases, and the lack of guidance in the schools leaves the children of the working poor with no degree. Also, the lack of funds for continuing education causes these children to fall behind. In many cases, their parents did not continue on into higher education and because of this have a difficult time finding jobs with salaries that can support a family. Today a college degree is a requirement for many jobs, and it is the low skill jobs that usually only require a high school degree or GED. The inequality in available education continues the vicious cycle of families entering into the working poor.		Transportation Given the fact that many working poor people do not own a car or cannot afford to drive their car, where they live can significantly limit where they are able to work, and vice versa.[3] Given the fact that public transportation in many US cities is sparse, expensive, or non-existent, this is a particularly salient obstacle. Some working poor people are able to use their social networks—if they have them—to meet their transportation needs. In a study on low-income single mothers, Edin and Lein found that single mothers who had someone to drive them to and from work were much more likely to be able to support themselves without relying on government aid.[11]		Basic necessities Like the unemployed poor, the working poor struggle to pay for basic necessities like food, clothing, housing, and transportation. In some cases, however, the working poor's basic expenses can be higher than the unemployed poor's. For instance, the working poor's clothing expenses may be higher than the unemployed poor's because they must purchase specific clothes or uniforms for their jobs.[3] Also, because the working poor are spending much of their time at work, they may not have the time to prepare their own food. In this case, they may frequently resort to eating fast food, which is less healthful and more expensive than home-prepared food.[3]		Childcare Working poor parents with young children, especially single parents, face significantly more childcare-related obstacles than other people. Often, childcare costs can exceed a low-wage earners' income, making work, especially in a job with no potential for advancement, an economically illogical activity.[11][12] However, some single parents are able to rely on their social networks to provide free or below-market-cost childcare.[11] There are also some free childcare options provided by the government, such as the Head Start Program. However, these free options are only available during certain hours, which may limit parents' ability to take jobs that require late-night shifts.The U.S. "average" seems to suggest that for one toddler, in full-time day care, on weekdays, the cost is approximately $600.00 per month. But, that figure can rise to well over $1000.00 per month in major metro areas, and fall to less than $350 in rural areas.The average cost of center-based daycare in the United States is $11,666 per year ($972 a month), but prices range from $3,582 to $18,773 a year ($300 to $1,564 monthly), according to the National Association of Child Care Resource & Referral Agencies (NACCRRA).[22]		Work schedules Many low-wage jobs force workers to accept irregular schedules. In fact, some employers will not hire someone unless they have "open availability," which means being available to work any time, any day.[3] This makes it difficult for workers to arrange for childcare and to take on a second job. In addition, working poor people's working hours can fluctuate wildly from one week to the next, making it difficult for them to budget effectively and save up money.[3]		Multiple jobs Many low-wage workers have to work multiple jobs in order to make ends meet. In 1996, 6.2 percent of the workforce held two or more full- or part-time jobs. Most of these people held two part-time jobs or one part-time job and one full-time job, but 4% of men and 2% of women held two full-time jobs at the same time.[23] This can be physically exhausting and can often lead to short and long-term health problems.[3]		Low-Status Work Many low-wage service sector jobs require a great deal of customer service work. Although not all customer service jobs are low-wage or low-status,[24] many of them are. Some argue[who?] that the low status nature of some jobs can have negative psychological effects on workers,[3] but others argue that low status workers come up with coping mechanisms that allow them to maintain a strong sense of self-worth.[18][25] One of these coping mechanisms is called boundary work. Boundary work happens when one group of people valorize their own social position by comparing themselves to another group, who they perceive to be inferior in some way. For example, Newman (1999) found that fast food workers in New York City cope with the low-status nature of their job by comparing themselves to the unemployed, who they perceive to be even lower-status than themselves.[26] Thus, although the low-status nature of working poor people's jobs may have some negative psychological effects, some, but probably not all, of these negative effects can be counteracted through coping mechanisms such as boundary work.		Scholars, policymakers, and others have come up with a variety of proposals for how to reduce or eliminate working poverty. Most of these proposals are directed toward the United States, but they might also be relevant to other countries. The remainder of the section outlines the pros and cons of some of the most commonly proposed solutions.		Cross-national studies like Lohmann (2009) and Brady, Fullerton, and Cross (2010) clearly show that countries with generous welfare states have lower levels of working poverty than countries with less-generous welfare states, even when factors like demography, economic performance, and labor market institutions are taken into account. Having a generous welfare state does two key things to reduce working poverty: it raises the minimum level of wages that people are willing to accept, and it pulls a large portion of low-wage workers out of poverty by providing them with an array of cash and non-cash government benefits.[5] Many think that increasing the United States' welfare state generosity would lower the working poverty rate. A common critique of this proposal is that a generous welfare state would not work because it would stagnate the economy, raise unemployment, and degrade people's work ethic.[10] However, as of 2011[update], most European countries have a lower unemployment rate than the US. Furthermore, although Western European economies' growth rates can be lower than the US's from time to time, their growth rates tend to be more stable, whereas the US's tends to fluctuate relatively severely. Individual states offer financial assistance for child care, but the aid varies widely. Most assistance is administered through the Child Care and Development Block Grants. Check here to find the contact information for your state. Many subsidies have strict income guidelines and are generally for families with children under 13 (the age limit is often extended if the child has a disability). Many subsidies permit home-based care, but some only accept a day care center, so check the requirements. If you need to use an authorized provider, ask if they will put you in touch with an agency that can help you find one.		Some states distribute funds through social or health departments or agencies (like this one in Washington State). For example, the Children's Cabinet in Nevada can refer families to providers, help them apply for subsidies and can even help families who want to pay a relative for care. North Carolina's Smart Start is a public/private partnership that offers funding for child care. Check the National Women's Law Center for each state's child care assistance policy.[27]		In the conclusion of her book, Nickel and Dimed (2001), Barbara Ehrenreich argues that Americans need to pressure employers to improve worker compensation.[3] Generally speaking, this implies a need to strengthen the labor movement. Interestingly, cross-national statistical studies on working poverty suggest that generous welfare states have a larger impact on working poverty than strong labor movements. The labor movements in various countries have accomplished this through political parties of their own (labor parties) or strategic alliances with non-labor parties, for instance, when striving to put a meaningful minimum wage in place. The federal government offers a Flexible Spending Account (FSA) that's administered through workplaces.		If your job offers an FSA (also known as a Dependent Care Account), you can put aside up to $5,000 in per-tax dollars to pay for child care expenses. If both you and your spouse have an FSA, the family limit is $5,000 -- but you could get as much as $2,000 in tax savings if your combined contributions reach the maximum. [28]		Some argue[who?] that more vocational training, especially in growth industries like healthcare and renewable energy, is the solution to working poverty. To be sure, wider availability of vocational training could pull some people out of working poverty, but the fact remains that the low-wage service sector is a rapidly growing part of the US economy. Even if more nursing and clean energy jobs were added to the economy, there would still be a huge portion of the workforce in low-wage service sector jobs like retail, food service, and cleaning. Therefore, it seems clear that any significant reduction in the working poverty rate will have to come from offering higher wages and more benefits to the current, and future, population of service sector workers.		Given the fact that such a large proportion of working poor households are headed by a single mother, one clear way to reduce working poverty would be to make sure that children's fathers share the cost of child rearing. In cases where the father cannot provide child support, scholars like Irwin Garfinkel advocate for the implementation of a child support guarantee, whereby the government pays childcare costs if the father cannot. Child support is not always a guarantee if the father or mother does not work. For example, if the parent without custody is not working then the parent with custody does not receive any child support unless the non working parent is employed at their job longer than 90 days, excluding if the non begins to work for its a city or government. Also, the government does not pay for childcare cost if you make more than the cut off range (your gross, per county or state.)		Households with two wage-earners have a significantly lower rate of working poverty than households with only one wage-earner. Also, households with two adults, but only one wage-earner, have lower working poverty rates than households with only one adult. Therefore, it seems clear that having two adults in a household, especially if there are children present, is more likely to keep a household out of poverty than having just one adult in a household. Many scholars and policymakers have used this fact to argue that encouraging people to get married and stay married is an effective way to reduce working poverty (and poverty in general). However, this is easier said than done. Research has shown that low-income people marry less often than higher-income people because they have a more difficult time finding a partner who is employed, which is often seen as a prerequisite for marriage.[29] Therefore, unless the employment opportunity structure is improved, simply increasing the number of marriages among low-income people would be unlikely to lower working poverty rates.		Ultimately, effective solutions to working poverty are multifaceted. Each of the aforementioned proposals could help reduce working poverty in the United States, but they might have a greater impact if at least a few of them were pursued simultaneously.		
Slavery is, in the strictest sense of the term, any system in which principles of property law are applied to people, allowing individuals to own, buy and sell other individuals, as a de jure form of property.[1] A slave is unable to withdraw unilaterally from such an arrangement and works without remuneration. Many scholars now use the term chattel slavery to refer to this specific sense of legalised, de jure slavery. In a broader sense, however, the word slavery may also refer to any situation in which an individual is de facto forced to work against their own will. Scholars also use the more generic terms such as unfree labour or forced labour, to refer to such situations.[2] However, and especially under slavery in broader senses of the word, slaves may have some rights and protections according to laws or customs.		Slavery began to exist before written history, in many cultures.[3] A person could become a slave from the time of their birth, capture, or purchase.		While slavery was institutionally recognized by most societies, it has now been outlawed in all recognized countries,[4][5] the last being Mauritania in 2007. Nevertheless, there are still more slaves today than at any previous point in history:[6] An estimated 45 million people remain enslaved worldwide.[7] The most common form of the slave trade is now commonly referred to as human trafficking. Chattel slavery is also still practiced by the Islamic State of Iraq and the Levant. In other areas, slavery (or unfree labour) continues through practices such as debt bondage, serfdom, domestic servants kept in captivity, certain adoptions in which children are forced to work as slaves, child soldiers, and forced marriage.[8]		The English word slave comes from Old French sclave, from the Medieval Latin sclavus, from the Byzantine Greek σκλάβος, which, in turn, comes from the ethnonym Slav, because in some early Medieval wars many Slavs were captured and enslaved.[9][10] An older interpretation connected it to the Greek verb skyleúo 'to strip a slain enemy'.[11]		There is a dispute among historians about whether terms such as "unfree labourer" or "enslaved person", rather than "slave", should be used when describing the victims of slavery. According to those proposing a change in terminology, "slave" perpetuates the crime of slavery in language, by reducing its victims to a nonhuman noun instead of, according to Andi Cumbo-Floyd, "carry[ing] them forward as people, not the property that they were". Other historians prefer "slave" because the term is familiar and shorter, or because it accurately reflects the inhumanity of slavery, with "person" implying a degree of autonomy that slavery does not allow for.[12]		Chattel slavery, also called traditional slavery, is so named because people are treated as the chattel (personal property) of the owner and are bought and sold as commodities. Although it dominated many societies in the past, this form of slavery has been formally abolished and is very rare today. Even when it can be said to survive, it is not upheld by the legal system of any internationally recognized government.[13]		Indenture, otherwise known as bonded labour or debt bondage is a form of unfree labour under which a person pledges himself or herself against a loan.[14] The services required to repay the debt, and their duration, may be undefined.[14] Debt bondage can be passed on from generation to generation, with children required to pay off their parents' debt.[14] It is the most widespread form of slavery today.[2] Debt bondage is most prevalent in South Asia.[15]		Forced labour, or unfree labour, is sometimes used to refer to when an individual is forced to work against their own will, under threat of violence or other punishment[2] but the generic term unfree labour is also used to describe chattel slavery, as well as any other situation in which a person is obliged to work against their own will and a person's ability to work productively is under the complete control of another person. This may also include institutions not commonly classified as slavery, such as serfdom, conscription and penal labour. While some unfree labourers, such as serfs, have substantive, de jure legal or traditional rights, they also have no ability to terminate the arrangements under which they work, are frequently subject to forms of coercion, such as threats of violence, and experience restrictions on their activities and movement outside their place of work.		Human trafficking primarily involves women and children forced into prostitution[16] and is the fastest growing form of forced labour,[2] with Thailand, Cambodia, India, Brazil and Mexico having been identified as leading hotspots of commercial sexual exploitation of children.[17]		In 2007, Human Rights Watch estimated that 200,000 to 300,000 children served as soldiers in current conflicts.[18]		A forced marriage may be regarded as a form of slavery by one or more of the parties involved in the marriage, as well as by people observing the marriage. People forced into marriage can be required to engage in sexual activity or to perform domestic duties or other work without any personal control. The customs of bride price and dowry that exist in many parts of the world can lead to buying and selling people into marriage.[19][20] Forced marriage continues to be practiced in parts of the world including some parts of Asia and Africa. Forced marriages may also occur in immigrant communities in Europe, the United States, Canada and Australia.[21][22][23][24] Marriage by abduction occurs in many places in the world today, with a national average of 69% of marriages in Ethiopia being through abduction.[25]		The International Labour Organization defines child and forced marriage as forms of modern-day slavery.[26]		The word "slave" has also been used to refer to a legal state of dependency to somebody else.[27][28] In many cases, such as in ancient Persia, the situation and lives of such slaves could be better than those of other common citizens.[29]		Even though slavery is now outlawed in every country,[31] the number of slaves today is estimated as between 12 million[32] and 29.8 million.[33] Several estimates of the number of slaves in the world have been provided.[34] According to a broad definition of slavery used by Kevin Bales of Free the Slaves (FTS), an advocacy group linked with Anti-Slavery International, there were 27 million people in slavery in 1999, spread all over the world.[35] In 2005, the International Labour Organization provided an estimate of 12.3 million forced labourers.[36] Siddharth Kara has also provided an estimate of 28.4 million slaves at the end of 2006 divided into three categories: bonded labour/debt bondage (18.1 million), forced labour (7.6 million), and trafficked slaves (2.7 million).[37] Kara provides a dynamic model to calculate the number of slaves in the world each year, with an estimated 29.2 million at the end of 2009. According to a 2003 report by Human Rights Watch, an estimated 15 million children in debt bondage in India work in slavery-like conditions to pay off their family's debts.[38][39]		A report by the Walk Free Foundation in 2013,[40] found India had the highest number of slaves, nearly 14 million, followed by China (2.9 million), Pakistan (2.1 million), Nigeria, Ethiopia, Russia, Thailand, Democratic Republic of Congo, Myanmar and Bangladesh; while the countries with the highest of proportion of slaves were Mauritania, Haiti, Pakistan, India and Nepal.[41]		In June 2013, U.S. State Department released a report on slavery, it placed Russia, China, Uzbekistan in the worst offenders category, Cuba, Iran, North Korea, Sudan, Syria, and Zimbabwe were also at the lowest level. The list also included Algeria, Libya, Saudi Arabia and Kuwait among a total of 21 countries.[42][43]		While American slaves in 1809 were sold for around $40,000 (in inflation adjusted dollars), a slave nowadays can be bought for just $90, making replacement more economical than providing long term care.[44] Slavery is a multibillion-dollar industry with estimates of up to $35 billion generated annually.[45]		Trafficking in human beings (also called human trafficking) is one method of obtaining slaves.[46] Victims are typically recruited through deceit or trickery (such as a false job offer, false migration offer, or false marriage offer), sale by family members, recruitment by former slaves, or outright abduction. Victims are forced into a "debt slavery" situation by coercion, deception, fraud, intimidation, isolation, threat, physical force, debt bondage or even force-feeding with drugs of abuse to control their victims.[47] "Annually, according to U.S. government-sponsored research completed in 2006, approximately 800,000 people are trafficked across national borders, which does not include millions trafficked within their own countries. Approximately 80 percent of transnational victims are women and girls and up to 50 percent are minors, reports the U.S. State Department in a 2008 study.[48]		While the majority of trafficking victims are women, and sometimes children, who are forced into prostitution (in which case the practice is called sex trafficking), victims also include men, women and children who are forced into manual labour.[49] Due to the illegal nature of human trafficking, its exact extent is unknown. A U.S. government report published in 2005, estimates that 600,000 to 800,000 people worldwide are trafficked across borders each year. This figure does not include those who are trafficked internally.[49] Another research effort revealed that between 1.5 million and 1.8 million individuals are trafficked either internally or internationally each year, 500,000 to 600,000 of whom are sex trafficking victims.[37]		Examples of modern slavery are numerous. Child slavery has commonly been used in the production of cash crops and mining.		In 2008, the Nepalese government abolished the Haliya system, under which 20,000 people were forced to provide free farm labour.[50]		Though slavery was officially abolished in Qing China in 1910,[51] the practice continues unofficially in some regions of the country.[52][53][54] In June and July 2007, 550 people who had been enslaved by brick manufacturers in Shanxi and Henan were freed by the Chinese government.[55] Among those rescued were 69 children.[56] In response, the Chinese government assembled a force of 35,000 police to check northern Chinese brick kilns for slaves, sent dozens of kiln supervisors to prison, punished 95 officials in Shanxi province for dereliction of duty, and sentenced one kiln foreman to death for killing an enslaved worker.[55]		The North Korean government[57] operates six large political prison camps,[58] where political prisoners and their families (around 200,000 people)[59] in lifelong detention[60] are subjected to hard slave labour,[61] torture and inhumane treatment.[62]		In November 2006, the International Labour Organization announced it will be seeking "to prosecute members of the ruling Myanmar junta for crimes against humanity" over the continuous unfree labour of its citizens by the military at the International Court of Justice.[63][64] According to the International Labor Organization (ILO), an estimated 800,000 people are subject to forced labour in Myanmar.[65]		In 2008, in Brazil about 5,000 slaves were rescued by government authorities as part of an initiative to eradicate slavery, which was reported as ongoing in 2010.[66] Poverty has forced at least 225,000 Haitian children to work as restavecs (unpaid household servants); the United Nations considers this to be a form of slavery.[67]		Some tribal sheiks in Iraq still keep blacks, called Abd, which means servant or slave in Arabic, as slaves.[68]		According to media reports from late 2014 the Islamic State of Iraq and the Levant (ISIL) was selling Yazidi and Christian women as slaves.[69][70][71] According to Haleh Esfandiari of the Woodrow Wilson International Center for Scholars, after ISIL militants have captured an area "[t]hey usually take the older women to a makeshift slave market and try to sell them."[72] In mid-October 2014, the UN estimated that 5,000 to 7,000 Yazidi women and children were abducted by ISIL and sold into slavery.[73][74] In the digital magazine Dabiq, ISIL claimed religious justification for enslaving Yazidi women whom they consider to be from a heretical sect. ISIL claimed that the Yazidi are idol worshipers and their enslavement part of the old shariah practice of spoils of war.[75][76][77][78][79] According to The Wall Street Journal, ISIL appeals to apocalyptic beliefs and claims "justification by a Hadith that they interpret as portraying the revival of slavery as a precursor to the end of the world".[80]		In Mauritania, the last country to abolish slavery (in 1981),[82] it is estimated that up to 600,000 men, women and children, or 20% of the population, are enslaved with many used as bonded labour.[83][84][85] Slavery in Mauritania was criminalized in August 2007.[86] (although slavery as a practice was legally banned in 1981, it was not a crime to own a slave until 2007).[87] Although many slaves have escaped or have been freed since 2007, as of 2012[update], only one slave-owner had been sentenced to serve time in prison.[88]		An article in the Middle East Quarterly in 1999 reported that slavery is endemic in Sudan.[89] Estimates of abductions during the Second Sudanese Civil War range from 14,000 to 200,000 people.[90]		In Niger, slavery is also a current phenomenon. A Nigerien study has found that more than 800,000 people are enslaved, almost 8% of the population.[91][92][93] Niger installed anti slavery provision in 2003.[94][95] In a landmark ruling in 2008, the ECOWAS Community Court of Justice declared that the Republic of Niger failed to protect Hadijatou Mani Koraou from slavery, and awarded Mani CFA 10,000,000 (approximately US$20,000) in reparations.[96]		Many pygmies in the Republic of Congo and Democratic Republic of Congo belong from birth to Bantus in a system of slavery.[97][98]		According to the U.S. State Department, more than 109,000 children were working on cocoa farms alone in Ivory Coast in "the worst forms of child labour" in 2002.[99]		On the night of 14–15 April 2014, a group of militants attacked the Government Girls Secondary School in Chibok, Nigeria. They broke into the school, pretending to be guards,[100] telling the girls to get out and come with them.[101] A large number of students were taken away in trucks, possibly into the Konduga area of the Sambisa Forest where Boko Haram were known to have fortified camps.[101] Houses in Chibok were also burned down in the incident.[102] According to police, approximately 276 children were taken in the attack, of whom 53 had escaped as of 2 May.[103] Other reports said that 329 girls were kidnapped, 53 had escaped and 276 were still missing.[104][105][106] The students have been forced to convert to Islam[107] and into marriage with members of Boko Haram, with a reputed "bride price" of ₦2,000 each ($12.50/£7.50).[108][109] Many of the students were taken to the neighbouring countries of Chad and Cameroon, with sightings reported of the students crossing borders with the militants, and sightings of the students by villagers living in the Sambisa Forest, which is considered a refuge for Boko Haram.[109][110]		On May 5, 2014 a video in which Boko Haram leader Abubakar Shekau claimed responsibility for the kidnappings emerged. Shekau claimed that "Allah instructed me to sell them...I will carry out his instructions"[111] and "[s]lavery is allowed in my religion, and I shall capture people and make them slaves."[112] He said the girls should not have been in school and instead should have been married since girls as young as nine are suitable for marriage.[111][112]		Evidence of slavery predates written records, and has existed in many cultures.[3] Slavery is rare among hunter-gatherer populations because it requires economic surpluses and a high population density to be viable. This, although it has existed among unusually resource-rich hunter gatherers, such as the American Indian peoples of the salmon-rich rivers of the Pacific Northwest Coast, slavery became widespread only with the invention of agriculture during the Neolithic Revolution about 11,000 years ago.[113]		In the earliest known records, slavery is treated as an established institution. The Code of Hammurabi (c. 1760 BC), for example, prescribed death for anyone who helped a slave escape or who sheltered a fugitive.[114] The Bible mentions slavery as an established institution.[3]		Slavery was known in almost every ancient civilization and society including Sumer, Ancient Egypt, Ancient China, the Akkadian Empire, Assyria, Ancient India, Ancient Greece, the Roman Empire, the Hebrew kingdoms of the ancient Levant, and the pre-Columbian civilizations of the Americas.[3] Such institutions included debt-slavery, punishment for crime, the enslavement of prisoners of war, child abandonment, and the birth of slave children to slaves.[115]		Records of slavery in Ancient Greece date as far back as Mycenaean Greece. It is certain that Classical Athens had the largest slave population, with as many as 80,000 in the 6th and 5th centuries BC;[116] two to four-fifths of the population were slaves.[117] As the Roman Republic expanded outward, entire populations were enslaved, thus creating an ample supply from all over Europe and the Mediterranean. Greeks, Illyrians, Berbers, Germans, Britons, Thracians, Gauls, Jews, Arabs, and many more were slaves used not only for labour, but also for amusement (e.g. gladiators and sex slaves). This oppression by an elite minority eventually led to slave revolts (see Roman Servile Wars); the Third Servile War led by Spartacus (a Thracian) being the most famous and bitter.		By the late Republican era, slavery had become a vital economic pillar in the wealth of Rome, as well as a very significant part of Roman society.[118] It is estimated that 25% or more of the population of Ancient Rome was enslaved, although the actual percentage is debated by scholars, and varied from region to region.[119][120] Slaves represented 15–25% of Italy's population,[121] mostly captives in war[121] especially from Gaul[122] and Epirus. Estimates of the number of slaves in the Roman Empire suggest that the majority of slaves were scattered throughout the provinces outside of Italy.[121] Generally, slaves in Italy were indigenous Italians,[123] with a minority of foreigners (including both slaves and freedmen) born outside of Italy estimated at 5% of the total in the capital at its peak, where their number was largest. Those from outside of Europe were predominantly of Greek descent, while the Jewish ones never fully assimilated into Roman society, remaining an identifiable minority. These slaves (especially the foreigners) had higher death rates and lower birth rates than natives, and were sometimes even subjected to mass expulsions.[124] The average recorded age at death for the slaves of the city of Rome was extraordinarily low: seventeen and a half years (17.2 for males; 17.9 for females).[125][page needed]		Large-scale trading in slaves was mainly confined to the South and East of early medieval Europe: the Byzantine Empire and the Muslim world were the destinations, while pagan Central and Eastern Europe (along with the Caucasus and Tartary) were important sources. Viking, Arab, Greek, and Radhanite Jewish merchants were all involved in the slave trade during the Early Middle Ages.[126][127][128] The trade in European slaves reached a peak in the 10th century following the Zanj rebellion which dampened the use of African slaves in the Arab world.[129][130]		Medieval Spain and Portugal were the scene of almost constant Muslim invasion of the predominantly Christian area. Periodic raiding expeditions were sent from Al-Andalus to ravage the Iberian Christian kingdoms, bringing back booty and slaves. In raid against Lisbon, Portugal in 1189, for example, the Almohad caliph Yaqub al-Mansur took 3,000 female and child captives, while his governor of Córdoba, in a subsequent attack upon Silves, Portugal in 1191, took 3,000 Christian slaves.[131] From the 11th to the 19th century, North African Barbary Pirates engaged in Razzias, raids on European coastal towns, to capture Christian slaves to sell at slave markets in places such as Algeria and Morocco.[132][133]		In Britain, slavery continued to be practiced following the fall of Rome and sections of Hywel the Good's laws dealt with slaves in medieval Wales. The trade particularly picked up after the Viking invasions, with major markets at Chester[134] and Bristol[135] supplied by Danish, Mercian, and Welsh raiding of one another's borderlands. At the time of the Domesday Book, nearly 10% of the English population were slaves.[136] Slavery in early medieval Europe was so common that the Roman Catholic Church repeatedly prohibited it — or at least the export of Christian slaves to non-Christian lands was prohibited at e.g. the Council of Koblenz (922), the Council of London (1102) aimed mainly at the sale of English slaves to Ireland[137] and having no legal standing), and the Council of Armagh (1171). In 1452, Pope Nicholas V issued the papal bull Dum Diversas, granting the kings of Spain and Portugal the right to reduce any "Saracens (antiquated term referring to Muslims), pagans and any other unbelievers" to perpetual slavery, legitimizing the slave trade as a result of war.[138] The approval of slavery under these conditions was reaffirmed and extended in his Romanus Pontifex bull of 1455. However, Pope Paul III forbade enslavement of the Native Americans in 1537 in his papal bull Sublimus Dei.[139] Dominican friars who arrived at the Spanish settlement at Santo Domingo strongly denounced the enslavement of the local Native Americans. Along with other priests, they opposed their treatment as unjust and illegal in an audience with the Spanish king and in the subsequent royal commission.[140]		The Byzantine-Ottoman wars and the Ottoman wars in Europe brought large numbers of slaves into the Islamic world.[142] To staff its bureaucracy, the Ottoman Empire established a janissary system which seized hundreds of thousands of Christian boys through the devşirme system. They were well cared for but were legally slaves owned by the government and were not allowed to marry. They were never bought or sold. The Empire gave them significant administrative and military roles. The system began about 1365; there were 135,000 janissaries in 1826, when the system ended.[143] After the Battle of Lepanto, 12,000 Christian galley slaves were recaptured and freed from the Ottoman fleet.[144] Eastern Europe suffered a series of Tatar invasions, the goal of which was to loot and capture slaves into jasyr.[145] Seventy-five Crimean Tatar raids were recorded into Poland–Lithuania between 1474 and 1569.[146]		Approximately 10–20% of the rural population of Carolingian Europe consisted of slaves.[147] Slavery largely disappeared from Western Europe by the later Middle Ages.[148] The slave trade became illegal in England in 1102,[149] but England went on to become very active in the lucrative Atlantic slave trade from the seventeenth to the early nineteenth century. In Scandinavia, thralldom was abolished in the mid-14th century.[150] Slavery persisted longer in Eastern Europe. Slavery in Poland was forbidden in the 15th century; in Lithuania, slavery was formally abolished in 1588; they were replaced by the second serfdom. In Kievan Rus and Muscovy, slaves were usually classified as kholops.		In the process of the Mongols invasion of China proper, many Han Chinese were enslaved by the Mongols. According to Japanese historian Sugiyama Masaaki (杉山正明) and Funada Yoshiyuki (舩田善之), there were also certain number of Mongolian slaves owned by Han Chinese during the Yuan dynasty. Moreover, there is no evidence that Han Chinese, who were considered people of the bottom of Yuan society by some research, were suffered a particularly cruel abuse. In the early Qing dynasty, many Han Chinese were enslaved by the Manchurian rulers, some of them found themselves in positions of power and influence in Manchu administrations and owned their own Han Chinese slaves.[151][152][153]		In early Islamic states of the Western Sudan (present-day West Africa), including Ghana (750–1076), Mali (1235–1645), Segou (1712–1861), and Songhai (1275–1591), about a third of the population was enslaved.[155]		Slaves were purchased or captured on the frontiers of the Islamic world and then imported to the major centres, where there were slave markets from which they were widely distributed.[156][157][158] In the 9th and 10th centuries, the black Zanj slaves may have constituted at least a half of the total population of lower Iraq.[155] At the same time, many slaves in the region were also imported from Central Asia and the Caucasus.[155] Many slaves were taken in the wars with the Christian nations of medieval Europe.		Slavery was also widespread in Africa, with both internal and external slave trade.[159]		Until the late 18th century, the Crimean Khanate (a Muslim Tatar state) maintained a massive slave trade with the Ottoman Empire and the Middle East,[145] exporting about 2 million slaves from Poland-Lithuania and Russia over the period 1500–1700.[160]		During the Second World War (1939–1945) Nazi Germany effectively enslaved about 12 million people, both those considered undesirable and citizens of countries they conquered, with the avowed intention of treating these untermenschen as a permanent slave class of inferior beings who could be worked until they died but who possessed neither the rights nor the legal status of members of the Aryan race.[161]		The Ottoman Empire owned and traded slaves on a massive scale. Many slaves were the created by conquest and the suppression of rebellions, in the aftermath of which, entire populations were sometimes enslaved and sold across the Empire, reducing the risk of future rebellion. The Ottomans also purchased slaves from traders who brought slaves into the Empire from Europe and Africa.		In Algiers, the capital of Algeria, captured Christians and Europeans were forced into slavery. Raids by Barbary pirates on coastal villages and ships extending from Italy to Iceland, enslaved an estimated 1 million to 1¼ million Europeans between the 16th and 19th centuries.[162] This eventually led to the bombardment of Algiers by an Anglo-Dutch fleet in 1816.[163][164]		Half the population of the Sokoto caliphate of the 19th century (based in the future northern Nigeria) were slaves.[155] The Swahili-Arab slave trade reached its height about 160 years ago, when, for example, approximately 20,000 slaves were considered to be carried yearly from Nkhotakota on Lake Malawi to Kilwa.[165] Roughly half the population of Madagascar was enslaved.[155][166]		According to the Encyclopedia of African History, "It is estimated that by the 1890s the largest slave population of the world, about 2 million people, was concentrated in the territories of the Sokoto Caliphate. The use of slave labour was extensive, especially in agriculture."[167][168] The Anti-Slavery Society estimated there were 2 million slaves in Ethiopia in the early 1930s out of an estimated population of 8 to 16 million.[169]		Hugh Clapperton in 1824 believed that half the population of Kano were enslaved people.[170] W. A. Veenhoven wrote: "The German doctor, Gustav Nachtigal, an eye-witness, believed that for every slave who arrived at a market three or four died on the way ... Keltie (The Partition of Africa, London, 1920) believes that for every slave the Arabs brought to the coast at least six died on the way or during the slavers' raid. Livingstone puts the figure as high as ten to one."[171]		One of the most famous slave traders on the eastern Zanj (Bantu) coast was Tippu Tip, himself the grandson of a slave. The prazeros were slave-traders along the Zambezi. North of the Zambezi, the waYao and Makua people played a similar role as professional slave-raiders and -traders. Still further north were the Nyamwezi slave-traders.[172]		In Constantinople, about one-fifth of the population consisted of slaves.[155] The city was a major centre of the slave trade in the 15th and later centuries. By 1475 most of the slaves were provided by Tatar raids on Slavic villages.[173] It has been estimated that some 200,000 slaves—mainly Circassians—were imported into the Ottoman Empire between 1800 and 1909.[174] As late as 1908, women slaves were still sold in the Ottoman Empire.[175] A slave market for captured Russian and Persian slaves was centred in the Central Asian khanate of Khiva.[176] In the early 1840s, the population of the Uzbek states of Bukhara and Khiva included about 900,000 slaves.[174] Darrel P. Kaiser wrote, "Kazakh-Kirghiz tribesmen kidnapped 1573 settlers from colonies [German settlements in Russia] in 1774 alone and only half were successfully ransomed. The rest were killed or enslaved."[177]		According to Sir Henry Bartle Frere (who sat on the Viceroy's Council), there were an estimated 8 or 9 million slaves in India in 1841. About 15% of the population of Malabar were slaves. Slavery was abolished in British India by the Indian Slavery Act V. of 1843.[3]		In East Asia, the Imperial government formally abolished slavery in China in 1906, and the law became effective in 1910.[178] The Nangzan in Tibetan history were, according to Chinese sources, hereditary household slaves.[179]		In the Joseon period of Korea, members of the slave class were known as nobi. The nobi were socially indistinct from freemen other than the ruling yangban class, and some possessed property rights, legal entities and civil rights. Hence, some scholars argue that it's inappropriate to call them "slaves",[180] while some scholars describe them as serfs.[181][182] The nobi population could fluctuate up to about one-third of the population, but on average the nobi made up about 10% of the total population.[183] In 1801, the vast majority of government nobi were emancipated,[184] and by 1858 the nobi population stood at about 1.5 percent of the total population of Korea.[185] The hereditary nobi system was officially abolished around 1886–87 and the rest of the nobi system was abolished with the Gabo Reform of 1894,[185] but traces remained until 1930.		In late 16th century Japan, slavery as such was officially banned, but forms of contract and indentured labour persisted alongside the period penal codes' forced labour.[186]		The hill tribe people in Indochina were "hunted incessantly and carried off as slaves by the Siamese (Thai), the Anamites (Vietnamese), and the Cambodians".[187] A Siamese military campaign in Laos in 1876 was described by a British observer as having been "transformed into slave-hunting raids on a large scale".[188] The census, taken in 1879, showed that 6% of the population in the Malay sultanate of Perak were slaves.[174] Enslaved people made up about two-thirds of the population in part of North Borneo in the 1880s.[174]		Throughout the 1930s and 1940s the Yi people (also known as Nuosu) of China terrorized Sichuan to rob and enslave non-Nuosu including Han people. The descendants of the Han Chinese slaves are the White Yi (白彝) and they outnumber the Black Yi (黑彝) aristocracy by ten to one.[189] As much as tens of thousands of Han slaves were incorporated into Nuosu society every year. The Han slaves and their offspring were used for manual labor.[190] There is a saying goes like: "the worst insult to a Nuosu is to call him a "Han" (with the implication being that "your ancestors were slaves")".[191]		Slavery in the Americas had a contentious history, and played a major role in the history and evolution of some countries, triggering at least one revolution and one civil war, as well as numerous rebellions. The Aztecs had slaves.[192] Other Amerindians, such as the Inca of the Andes, the Tupinambá of Brazil, the Creek of Georgia, and the Comanche of Texas, also owned slaves.[3]		The maritime town of Lagos was the first slave market created in Portugal (one of the earliest colonizers of the Americas) for the sale of imported African slaves—the Mercado de Escravos, opened in 1444.[193][194] In 1441, the first slaves were brought to Portugal from northern Mauritania.[194]		In 1519, Mexico's first Afro-Mexican slave was brought by Hernán Cortés.		By 1552, black African slaves made up 10% of the population of Lisbon.[195][196] In the second half of the 16th century, the Crown gave up the monopoly on slave trade and the focus of European trade in African slaves shifted from import to Europe to slave transports directly to tropical colonies in the Americas—in the case of Portugal, especially Brazil.[194] In the 15th century one-third of the slaves were resold to the African market in exchange of gold.[197]		In order to establish itself as an American empire, Spain had to fight against the relatively powerful civilizations of the New World. The Spanish conquest of the indigenous peoples in the Americas included using the Natives as forced labour. The Spanish colonies were the first Europeans to use African slaves in the New World on islands such as Cuba and Hispaniola, see Atlantic slave trade.[198]		Bartolomé de Las Casas a 16th-century Dominican friar and Spanish historian participated in campaigns in Cuba (at Bayamo and Camagüey) and was present at the massacre of Hatuey; his observation of that massacre led him to fight for a social movement away from the use of natives as slaves and towards the importation of African Blacks as slaves. Also, the alarming decline in the native population had spurred the first royal laws protecting the native population (Laws of Burgos, 1512–1513).		The first African slaves arrived in Hispaniola in 1501.[199] In 1518, Charles I of Spain agreed to ship slaves directly from Africa. England played a prominent role in the Atlantic slave trade. The "slave triangle" was pioneered by Francis Drake and his associates. In 1640 a Virginia court sentenced John Punch to slavery, forcing him to serve his master, Hugh Gwyn, for the remainder of his life. This was the first legal sanctioning of slavery in the English colonies.[200][201] In 1655, A black man, Anthony Johnson of Virginia, was granted ownership of John Casor as the result of a civil case.[202]		The Henrietta Marie was probably built in France sometime in the 17th century and carried a crew of about eighteen men. The ship came into English possession late in the 17th century, possibly as a war prize during the War of the Grand Alliance. It was put to use in the Atlantic slave trade, making at least two voyages carrying Africans to slavery in the West Indies. On its first voyage, in 1697–1698, the ship carried more than 200 people from Africa that were sold as slaves in Barbados. In 1699 the Henrietta Marie sailed from England on the first leg of the triangular trade route with a load of trade goods, including iron and copper bars, pewter utensils, glass beads, cloth and brandy. The ship sailed under license from the Royal African Company (which held a monopoly on English trade with Africa), in exchange for ten percent of the profits of the voyage. It is known to have traded for African captives at New Calabar on the Guinea Coast. The ship then sailed on the second leg of its voyage, from Africa to the West Indies, and in May 1701 landed 191 Africans for sale in Port Royal, Jamaica. The Henrietta Marie then loaded a cargo of sugar, cotton, dyewoods (indigo) and ginger to take back to England on the third leg of the triangular route. After leaving Port Royal on 18 May 1701, the ship headed for the Yucatán Channel to pass around the western end of Cuba (thus avoiding the pirates infesting the passage between Cuba and Hispaniola) and catch the Gulf Stream, the preferred route for all ships leaving the Caribbean to return to Europe. A month later, the Henrietta Marie wrecked on New Ground Reef near the Marquesas Keys, approximately 35 miles (56 kilometres) west of Key West. All aboard were lost.[203]		Pirates often targeted slavers. For example, the 300 ton English frigate Concord launched in 1710 but was captured by the French one year later. She was modified to hold more cargo, including slaves, and renamed La Concorde de Nantes. Sailing as a slave ship, she was captured by the pirate Captain Benjamin Hornigold on November 28, 1717, near the island of Martinique. Hornigold turned her over to one of his men, Edward Teach (later known as Blackbeard), and made him her captain. Teach then renamed her the Queen Anne's Revenge.[204] By 1750, slavery was a legal institution in all of the 13 American colonies,[205][206] and the profits of the slave trade and of West Indian plantations amounted to 5% of the British economy at the time of the Industrial Revolution.[207]		The trans-Atlantic slave trade peaked in the late 18th century, when the largest number of slaves were captured on raiding expeditions into the interior of West Africa. These expeditions were typically carried out by African kingdoms, such as the Oyo empire (Yoruba), the Ashanti Empire,[208] the kingdom of Dahomey,[209] and the Aro Confederacy.[210] Europeans rarely entered the interior of Africa, due to fierce African resistance. The slaves were brought to coastal outposts where they were traded for goods. A significant portion of African Americans in North America are descended from Mandinka people.[211] Through a series of conflicts, primarily with the Fulani Jihad States, about half of the Senegambian Mandinka were converted to Islam while as many as a third were sold into slavery to the Americas through capture in conflict.[211]		An estimated 12 million Africans arrived in the Americas from the 16th to the 19th centuries.[212] Of these, an estimated 645,000 were brought to what is now the United States. The usual estimate is that about 15% of slaves died during the voyage, with mortality rates considerably higher in Africa itself in the process of capturing and transporting indigenous peoples to the ships.[213]		Many Europeans who arrived in North America during the 17th and 18th centuries came under contract as indentured servants.[214] The transformation from indentured servitude to slavery was a gradual process in Virginia. The earliest legal documentation of such a shift was in 1640 where a negro, John Punch, was sentenced to lifetime slavery for attempting to run away. This case also marked the disparate treatment of Africans as held by the Virginia County Court, as two white runaways received far lesser sentences.[215] After 1640, planters started to ignore the expiration of indentured contracts and kept their servants as slaves for life. This was demonstrated by the case Johnson v. Parker, where the court ruled that John Casor, an indentured servant, be returned to Johnson who claimed that Casor belonged to him for his life.[216][217] According to the 1860 U. S. census, 393,975 individuals, representing 8% of all US families, owned 3,950,528 slaves.[218] One-third of Southern families owned slaves.[219]		The largest number of slaves were shipped to Brazil.[220] In the Spanish viceroyalty of New Granada, corresponding mainly to modern Panama, Colombia, and Venezuela, the free black population in 1789 was 420,000, whereas African slaves numbered only 20,000. Free blacks also outnumbered slaves in Brazil. By contrast, in Cuba, free blacks made up only 15% in 1827; and in the French colony of Saint-Domingue (present-day Haiti) it was a mere 5% in 1789.[221]		Author Charles Rappleye argued that:		In the West Indies in particular, but also in North and South America, slavery was the engine that drove the mercantile empires of Europe. It appeared, in the eighteenth century, as universal and immutable as human nature.[222]		Although the trans-Atlantic slave trade ended shortly after the American Revolution, slavery remained a central economic institution in the Southern states of the United States, from where slavery expanded with the westward movement of population.[223] Historian Peter Kolchin wrote, "By breaking up existing families and forcing slaves to relocate far from everyone and everything they knew" this migration "replicated (if on a reduced level) many of [the] horrors" of the Atlantic slave trade.[224]		Historian Ira Berlin called this forced migration the Second Middle Passage. Characterizing it as the "central event" in the life of a slave between the American Revolution and the Civil War, Berlin wrote that whether they were uprooted themselves or simply lived in fear that they or their families would be involuntarily moved, "the massive deportation traumatized black people, both slave and free.."[225]		By 1860, 500,000 slaves had grown to 4 million. As long as slavery expanded, it remained profitable and powerful and was unlikely to disappear. Although complete statistics are lacking, it is estimated that 1,000,000 slaves moved west from the Old South between 1790 and 1860.[226]		Most of the slaves were moved from Maryland, Virginia, and the Carolinas. Michael Tadman, in a 1989 book Speculators and Slaves: Masters, Traders, and Slaves in the Old South, indicates that 60–70% of interregional migrations were the result of the sale of slaves. In 1820, a child in the Upper South had a 30% chance to be sold south by 1860.[226]		In Puerto Rico, African slavery was finally abolished on March 22, 1873.		According to Robert Davis, between 1 million and 1.25 million Europeans were captured by Barbary pirates and sold as slaves in North Africa and Ottoman Empire between the 16th and 19th centuries.[227][228] There was also an extensive trade in Christian slaves in the Black Sea region for several centuries until the Crimean Khanate was destroyed by the Russian Empire in 1783.[229] In the 1570s close to 20,000 slaves a year were being sold in the Crimean port of Kaffa.[230] The slaves were captured in southern Russia, Poland-Lithuania, Moldavia, Wallachia, and Circassia by Tatar horsemen.[231] Some researchers estimate that altogether more than 3 million people were captured and enslaved during the time of the Crimean Khanate.[232][233]		The Arab slave trade lasted more than a millennium.[234] As recently as the early 1960s, Saudi Arabia's slave population was estimated at 300,000.[235] Along with Yemen, the Saudis abolished slavery only in 1962.[236] Historically, slaves in the Arab World came from many different regions, including Sub-Saharan Africa (mainly Zanj),[237] the Caucasus (mainly Circassians),[238] Central Asia (mainly Tartars), and Central and Eastern Europe (mainly Saqaliba).[239]		Under Omani Arabs Zanzibar became East Africa's main slave port, with as many as 50,000 enslaved Africans passing through every year during the 19th century.[240][241] Some historians estimate that between 11 and 18 million African slaves crossed the Red Sea, Indian Ocean, and Sahara Desert from 650 to 1900 AD.[3][242] Eduard Rüppell described the losses of Sudanese slaves being transported on foot to Egypt: "after the Daftardar bey's 1822 campaign in the southern Nuba mountains, nearly 40,000 slaves were captured. However, through bad treatment, disease and desert travel barely 5000 made it to Egypt.."[243]		The Moors, starting in the 8th century, also raided coastal areas around the Mediterranean and Atlantic Ocean, and became known as the Barbary pirates.[244] It is estimated that they captured 1.25 million white slaves from Western Europe and North America between the 16th and 19th centuries.[245][246] The mortality rate was very high. For instance, plague killed a third to two-thirds of the 30,000 occupants of the slave pens in Algiers in 1662.[227]		Slavery has existed, in one form or another, through recorded human history—as have, in various periods, movements to free large or distinct groups of slaves.		Ashoka, who ruled the Maurya Empire from 269–232 BCE, abolished the slave trade but not slavery.[248] The Qin dynasty, which ruled China from 221 to 206 BC, abolished slavery and discouraged serfdom. However, many of its laws were overturned when the dynasty was overthrown.[249] Slavery was again abolished, by Wang Mang, in China in 17 C.E but was reinstituted after his assassination.[250]		The Spanish colonization of the Americas sparked a discussion about the right to enslave Native Americans. A prominent critic of slavery in the Spanish New World colonies was Bartolomé de las Casas, who opposed the enslavement of Native Americans, and later also of Africans in America.		One of the first protests against slavery came from German and Dutch Quakers in Pennsylvania in 1688.[251] One of the most significant milestones in the campaign to abolish slavery throughout the world occurred in England in 1772, with British judge Lord Mansfield, whose opinion in Somersett's Case was widely taken to have held that slavery was illegal in England. This judgement also laid down the principle that slavery contracted in other jurisdictions could not be enforced in England.[252] In 1777, Vermont, at the time an independent nation, became the first portion of what would become the United States to abolish slavery.[251] France abolished slavery in 1794.[251] All the Northern states abolished slavery; New Jersey in 1804 was the last to act. None of the Southern or border states abolished slavery before the American Civil War.[253]		British Member of Parliament William Wilberforce led the anti-slavery movement in the United Kingdom, although the groundwork was an anti-slavery essay by Thomas Clarkson. Wilberforce was also urged by his close friend, Prime Minister William Pitt the Younger, to make the issue his own, and was also given support by reformed Evangelical John Newton. The Slave Trade Act was passed by the British Parliament on March 25, 1807, making the slave trade illegal throughout the British Empire,[254] Wilberforce also campaigned for abolition of slavery in the British Empire, which he lived to see in the Slavery Abolition Act 1833. After the 1807 act abolishing the slave trade was passed, these campaigners switched to encouraging other countries to follow suit, notably France and the British colonies. Between 1808 and 1860, the British West Africa Squadron seized approximately 1,600 slave ships and freed 150,000 Africans who were aboard.[255] Action was also taken against African leaders who refused to agree to British treaties to outlaw the trade, for example against "the usurping King of Lagos", deposed in 1851. Anti-slavery treaties were signed with over 50 African rulers.[256]		In 1839, the world's oldest international human rights organization, Anti-Slavery International, was formed in Britain by Joseph Sturge, which campaigned to outlaw slavery in other countries.[257] There were celebrations in 2007 to commemorate the 200th anniversary of the abolition of the slave trade in the United Kingdom through the work of the British Anti-Slavery Society.		In the United States, abolitionist pressure produced a series of small steps towards emancipation. After January 1, 1808, the importation of slaves into the United States was prohibited,[258] but not the internal slave trade, nor involvement in the international slave trade externally. Legal slavery persisted; and those slaves already in the U.S. were legally emancipated only in 1863. Many American abolitionists took an active role in opposing slavery by supporting the Underground Railroad. Violent clashes between anti-slavery and pro-slavery Americans included Bleeding Kansas, a series of political and armed disputes in 1854–1861 as to whether Kansas would join the United States as a slave or free state. By 1860, the total number of slaves reached almost four million, and the American Civil War, beginning in 1861, led to the end of slavery in the United States.[259] In 1863, Lincoln issued the Emancipation Proclamation, which freed slaves held in the Confederate States; the 13th Amendment to the U. S. Constitution prohibited most forms of slavery throughout the country.		In the case of freed slaves of the United States, many became sharecroppers and indentured servants. In this manner, some became tied to the very parcel of land into which they had been born a slave having little freedom or economic opportunity due to Jim Crow laws which perpetuated discrimination, limited education, promoted persecution without due process and resulted in continued poverty. Fear of reprisals such as unjust incarcerations and lynchings deterred upward mobility further.		In the 1860s, David Livingstone's reports of atrocities within the Arab slave trade in Africa stirred up the interest of the British public, reviving the flagging abolitionist movement. The Royal Navy throughout the 1870s attempted to suppress "this abominable Eastern trade", at Zanzibar in particular. In 1905, the French abolished indigenous slavery in most of French West Africa.[262]		On December 10, 1948, the United Nations General Assembly adopted the Universal Declaration of Human Rights, which declared freedom from slavery is an internationally recognized human right. Article 4 of the Universal Declaration of Human Rights states:		No one shall be held in slavery or servitude; slavery and the slave trade shall be prohibited in all their forms.[263]		In 2014, for the first time in history, major leaders of many religions, Buddhist, Anglican, Catholic, and Orthodox Christian, Hindu, Jewish, and Muslim, met to sign a shared commitment against modern-day slavery; the declaration they signed calls for the elimination of slavery and human trafficking by the year 2020.[264] The signatories were: Pope Francis, Mātā Amṛtānandamayī, Bhikkhuni Thich Nu Chân Không (representing Zen Master Thích Nhất Hạnh), Datuk K Sri Dhammaratana, Chief High Priest of Malaysia, Rabbi Abraham Skorka, Rabbi David Rosen, Abbas Abdalla Abbas Soliman, Undersecretary of State of Al Azhar Alsharif (representing Mohamed Ahmed El-Tayeb, Grand Imam of Al-Azhar), Grand Ayatollah Mohammad Taqi al-Modarresi, Sheikh Naziyah Razzaq Jaafar, Special advisor of Grand Ayatollah (representing Grand Ayatollah Sheikh Basheer Hussain al Najafi), Sheikh Omar Abboud, Justin Welby, Archbishop of Canterbury, and Metropolitan Emmanuel of France (representing Ecumenical Patriarch Bartholomew.)[264]		Groups such as the American Anti-Slavery Group, Anti-Slavery International, Free the Slaves, the Anti-Slavery Society, and the Norwegian Anti-Slavery Society continue to campaign to eliminate slavery.		Economists have attempted to model the circumstances under which slavery (and variants such as serfdom) appear and disappear. One observation is that slavery becomes more desirable for landowners where land is abundant but labour is scarce, such that rent is depressed and paid workers can demand high wages. If the opposite holds true, then it becomes more costly for landowners to have guards for the slaves than to employ paid workers who can only demand low wages due to the amount of competition.[265] Thus, first slavery and then serfdom gradually decreased in Europe as the population grew, but were reintroduced in the Americas and in Russia as large areas of new land with few people became available.[266] In his books, Time on the Cross and Without Consent or Contract: the Rise and Fall of American Slavery, Robert Fogel maintains that slavery was in fact a profitable method of production, especially on bigger plantations growing cotton that fetched high prices in the world market. It gave whites in the South higher average incomes than those in the North, but most of the money was spent on buying slaves and plantations.		Slavery is more common when the labour done is relatively simple and thus easy to supervise, such as large-scale growing of a single crop. It is much more difficult and costly to check that slaves are doing their best and with good quality when they are doing complex tasks. Therefore, slavery was seen as the most efficient method of production for large-scale crops like sugar and cotton, whose output was based on economies of scale. This enabled a gang system of labour to be prominent on large plantations where field hands were monitored and worked with factory-like precision. Each work gang was based on an internal division of labour that not only assigned every member of the gang to a precise task but simultaneously made their own performance dependent on the actions of the others. The hoe hands chopped out the weeds that surrounded the cotton plants as well as excessive sprouts. The plow gangs followed behind, stirring the soil near the rows of cotton plants and tossing it back around the plants. Thus, the gang system worked like an early version of the assembly line later to be found in factories.[267]		Critics since the 18th century have argued that slavery tends to retard technological advancement, since the focus is on increasing the number of slaves doing simple tasks rather than upgrading the efficiency of labour. Because of this, theoretical knowledge and learning in Greece—and later in Rome—was not applied to ease physical labour or improve manufacturing.[268]		Adam Smith made the argument that free labour was economically better than slave labour, and argued further that slavery in Europe ended during the Middle Ages, and then only after both the church and state were separate, independent and strong institutions,[269] that it is nearly impossible to end slavery in a free, democratic and republican forms of governments since many of its legislators or political figures were slave owners, and would not punish themselves, and that slaves would be better able to gain their freedom when there was centralized government, or a central authority like a king or the church.[270] Similar arguments appear later in the works of Auguste Comte, especially when it comes to Adam Smith's belief in the separation of powers or what Comte called the "separation of the spiritual and the temporal" during the Middle Ages and the end of slavery, and Smith's criticism of masters, past and present. As Smith stated in the Lectures on Jurisprudence, "The great power of the clergy thus concurring with that of the king set the slaves at liberty. But it was absolutely necessary both that the authority of the king and of the clergy should be great. Where ever any one of these was wanting, slavery still continues.."		Slaves can be an attractive investment because the slave-owner only needs to pay for sustenance and enforcement. This is sometimes lower than the wage-cost of free labourers, because free workers earn more than sustenance, in these cases slaves have positive price. When the cost of sustenance and enforcement exceeds the wage rate, slave-owning would no longer be profitable, and owners would simply release their slaves. Slaves are thus a more attractive investment in high-wage environments, and environments where enforcement is cheap, and less attractive in environments where the wage-rate is low and enforcement is expensive.[271]		Free workers also earn compensating differentials, whereby they are paid more for doing unpleasant work. Neither sustenance nor enforcement costs rise with the unpleasantness of the work, however, so slaves' costs do not rise by the same amount. As such, slaves are more attractive for unpleasant work, and less for pleasant work. Because the unpleasantness of the work is not internalised, being born by the slave rather than the owner, it is a negative externality and leads to over-use of slaves in these situations.[271]		The weighted average global sales price of a slave is calculated to be approximately $340, with a high of $1,895 for the average trafficked sex slave, and a low of $40 to $50 for debt bondage slaves in part of Asia and Africa.[37] Worldwide slavery is a criminal offense but slave owners can get very high returns for their risk. According to researcher Siddharth Kara, the profits generated worldwide by all forms of slavery in 2007 were $91.2 billion. That is second only to drug trafficking in terms of global criminal enterprises. The weighted average annual profits generated by a slave in 2007 was $3,175, with a low of an average $950 for bonded labour and $29,210 for a trafficked sex slave.[37] Approximately 40% of slave profits each year are generated by trafficked sex slaves, representing slightly more than 4% of the world's 29 million slaves.[37]		Robert E. Wright has developed a model that helps to predict when firms (individuals, companies) will be more likely to use slaves rather than wage workers, indentured servants, family members, or other types of labourers.[272]		Throughout history, slaves were clothed in a distinctive fashion, particularly with respect to footwear or rather the lack thereof. This was both due to economic reasons as well as a distinguishing feature, especially in South Africa and South America. For example, the Cape Town slave code stated that "Slaves must go barefoot and must carry passes."[273] This was the case in the majority of states that abolished slavery later in history, as most images from the respective historical period suggest that slaves were barefoot.[274] To quote Brother Riemer (1779): "[the slaves] are, even in their most beautiful suit, obliged to go barefoot. Slaves were forbidden to wear shoes. This was a prime mark of distinction between the free and the bonded and no exceptions were permitted."[275]		As shoes have been considered badges of freedom since biblical times "But the father said to his servants, Bring forth the best robe, and put [it] on him; and put a ring on his hand, and shoes on [his] feet (Luke 15:22)" this aspect has been an informal law wherever slavery existed. A barefoot person could therefore be clearly identified as a slave upon first sight. In certain societies this rule is valid to this day, as with the Tuareg slavery which is still unofficially practiced, and their slaves have to go barefoot.[276]		On May 21, 2001, the National Assembly of France passed the Taubira law, recognizing slavery as a crime against humanity. Apologies on behalf of African nations, for their role in trading their countrymen into slavery, remain an open issue since slavery was practiced in Africa even before the first Europeans arrived and the Atlantic slave trade was performed with a high degree of involvement of several African societies. The black slave market was supplied by well-established slave trade networks controlled by local African societies and individuals.[277] Indeed, as already mentioned in this article, slavery persists in several areas of West Africa until the present day.		There is adequate evidence citing case after case of African control of segments of the trade. Several African nations such as the Calabar and other southern parts of Nigeria had economies depended solely on the trade. African peoples such as the Imbangala of Angola and the Nyamwezi of Tanzania would serve as middlemen or roving bands warring with other African nations to capture Africans for Europeans.[278]		Several historians have made important contributions to the global understanding of the African side of the Atlantic slave trade. By arguing that African merchants determined the assemblage of trade goods accepted in exchange for slaves, many historians argue for African agency and ultimately a shared responsibility for the slave trade.[279]		In 1999, President Mathieu Kerekou of Benin (formerly the Kingdom of Dahomey) issued a national apology for the central role Africans played in the Atlantic slave trade.[280] Luc Gnacadja, minister of environment and housing for Benin, later said: "The slave trade is a shame, and we do repent for it."[281] Researchers estimate that 3 million slaves were exported out of the Slave Coast bordering the Bight of Benin.[281] President Jerry Rawlings of Ghana also apologized for his country's involvement in the slave trade.[280]		The issue of an apology is linked to reparations for slavery and is still being pursued by a number of entities across the world. For example, the Jamaican Reparations Movement approved its declaration and action Plan.		In September 2006, it was reported that the UK government might issue a "statement of regret" over slavery.[282] This was followed by a "public statement of sorrow" from Tony Blair on November 27, 2006,[283] and a formal apology on March 14, 2007.[284]		On February 25, 2007, the Commonwealth of Virginia resolved to 'profoundly regret' and apologize for its role in the institution of slavery. Unique and the first of its kind in the U. S., the apology was unanimously passed in both Houses as Virginia approached the 400th anniversary of the founding of Jamestown, where the first slaves were imported into North America in 1619.[285]		Liverpool, which was a large slave trading port, apologized in 1999. On August 24, 2007, Mayor Ken Livingstone of London, United Kingdom, apologized publicly for Britain's role in colonial slave trade. "You can look across there to see the institutions that still have the benefit of the wealth they created from slavery," he said, pointing towards the financial district. He claimed that London was still tainted by the horrors of slavery. Specifically, London outfitted, financed, and insured many of the ships, which helped fund the building of London's docks. Jesse Jackson praised Livingstone, and added that reparations should be made, one of his common arguments.[286]		On July 30, 2008, the United States House of Representatives passed a resolution apologizing for American slavery and subsequent discriminatory laws.[287] In June 2009, the US Senate passed a resolution apologizing to African-Americans for the "fundamental injustice, cruelty, brutality, and inhumanity of slavery".[288] The news was welcomed by President Barack Obama, the nation's first President of African descent.[289] Some of President Obama's ancestors may have been slave owners.[290]		In 2010, Libyan leader Muammar Gaddafi apologized for Arab involvement in the slave trade, saying: "I regret the behavior of the Arabs… They brought African children to North Africa, they made them slaves, they sold them like animals, and they took them as slaves and traded them in a shameful way."[291]		There have been movements to achieve reparations for those formerly held as slaves, or sometimes their descendants. Claims for reparations for being held in slavery are handled as a civil law matter in almost every country. This is often decried as a serious problem, since former slaves' relative lack of money means they often have limited access to a potentially expensive and futile legal process. Mandatory systems of fines and reparations paid to an as yet undetermined group of claimants from fines, paid by unspecified parties, and collected by authorities have been proposed by advocates to alleviate this "civil court problem.."Since in almost all cases there are no living ex-slaves or living ex-slave owners these movements have gained little traction. In nearly all cases the judicial system has ruled that the statute of limitations on these possible claims has long since expired.		The word slavery is often used as a pejorative to describe any activity in which one is coerced into performing.		Proponents of animal rights apply the term slavery to the condition of some or all human-owned animals, arguing that their status is comparable to that of human slaves.[292]		Some argue that military drafts and other forms of coerced government labour constitute state-operated slavery.[293][294] Some libertarians and anarcho-capitalists view government taxation as a form of slavery.[295]		Some Antipsychiatry proponents apply the term slavery to the involuntary psychiatric patient. There are no unbiased physical tests for mental illness, and the psychiatric patient must follow the orders of his/her psychiatrist. Drapetomania was a psychiatric diagnosis for a slave who did not want to be a slave. Thomas Szasz wrote a book titled "Psychiatric Slavery",[296] published in 1998 and a book titled " Liberation by Oppression: A Comparative Study of Slavery and Psychiatry",[297] published in 2003.		Some socialists, view total and immediate wage dependence as a form of slavery.[298] The labour market, as institutionalised under today's market economic systems, has been criticised,[299] especially by both mainstream socialists and anarcho-syndicalists,[300][301][302] who utilise the term wage slavery[303][304] as a pejorative for wage labour. Socialists draw parallels between the trade of labour as a commodity and slavery. Cicero is also known to have suggested such parallels.[305]		For Marxists, labour-as-commodity, which is how they regard wage labour,[306] provides an absolutely fundamental point of attack against capitalism.[307] "It can be persuasively argued," noted one concerned philosopher[who?], "that the conception of the worker's labour as a commodity confirms Marx's stigmatization of the wage system of private capitalism as 'wage-slavery;' that is, as an instrument of the capitalist's for reducing the worker's condition to that of a slave, if not below it."[308][citation needed]		Film has been the most influential medium in the presentation of the history of slavery to the general public around the world.[309] The American film industry has had a complex relationship with slavery and until recent decades often avoided the topic. Films such as Birth of a Nation (1915)[310] and Gone with the Wind (1939) became controversial because they gave a favourable depiction. The last favourable treatment was Song of the South from Disney in 1946. In 1940 The Santa Fe Trail gave a liberal but ambiguous interpretation of John Brown's attacks on slavery—the film does not know what to do with slavery.[311] The Civil Rights Movement in the 1950s made defiant slaves into heroes.[312] The question of slavery in American memory necessarily involves its depictions in feature films.[313]		Most Hollywood films used American settings, although Spartacus (1960), dealt with an actual revolt in the Roman Empire known as the Third Servile War. It failed and all the rebels were executed, but their spirit lived on according to the film.[314] The Last Supper (La última cena in Spanish) was a 1976 film directed by Cuban Tomás Gutiérrez Alea about the teaching of Christianity to slaves in Cuba, and emphasizes the role of ritual and revolt. Burn! takes place on the imaginary Portuguese island of Queimada (where the locals speak Spanish) and it merges historical events that took place in Brazil, Cuba, Santo Domingo, Jamaica, and elsewhere. Spartacus stays surprisingly close to the historical record.[315]		Historians agree that films have largely shaped historical memories, but they debate issues of accuracy, plausibility, moralism, sensationalism, how facts are stretched in search of broader truths, and suitability for the classroom.[316][317] Berlin argues that critics complain if the treatment emphasizes historical brutality, or if it glosses over the harshness to highlight the emotional impact of slavery.[318]		
The unemployment rate is a measure of the prevalence of unemployment and it is calculated as a percentage by dividing the number of unemployed individuals by all individuals currently in the labor force. During periods of recession, an economy usually experiences a relatively high unemployment rate.[1] According to International Labour Organization report, more than 200 million people globally or 6% of the world's workforce were without a job in 2012.[2]		There remains considerable theoretical debate regarding the causes, consequences and solutions for unemployment. Classical economics, new classical economics, and the Austrian School of economics argue that market mechanisms are reliable means of resolving unemployment. These theories argue against interventions imposed on the labor market from the outside such, as unionization, bureaucratic work rules, minimum wage laws, taxes, and other regulations that they claim discourage the hiring of workers.		Keynesian economics emphasizes the cyclical nature of unemployment and recommends government interventions in the economy that it claims will reduce unemployment during recessions. This theory focuses on recurrent shocks that suddenly reduce aggregate demand for goods and services and thus reduce demand for workers. Keynesian models recommend government interventions designed to increase demand for workers; these can include financial stimuli, publicly funded job creation, and expansionist monetary policies. Its namesake economist John Maynard Keynes, believed that the root cause of unemployment is the desire of investors to receive more money rather than produce more products, which is not possible without public bodies producing new money.[3]		In addition to these comprehensive theories of unemployment, there are a few categorizations of unemployment that are used to more precisely model the effects of unemployment within the economic system. The main types of unemployment include structural unemployment which focuses on structural problems in the economy and inefficiencies inherent in labour markets, including a mismatch between the supply and demand of laborers with necessary skill sets. Structural arguments emphasize causes and solutions related to disruptive technologies and globalization. Discussions of frictional unemployment focus on voluntary decisions to work based on each individuals' valuation of their own work and how that compares to current wage rates plus the time and effort required to find a job. Causes and solutions for frictional unemployment often address job entry threshold and wage rates. Behavioral economists highlight individual biases in decision making, and often involve problems and solutions concerning sticky wages and efficiency wages.		For centuries, experts have predicted that machines would make workers obsolete and increase unemployment.[4][5]						The state of being without any work both for an educated & uneducated person, for earning one's livelihood is meant by unemployment. Economists distinguish between various overlapping types of and theories of unemployment, including cyclical or Keynesian unemployment, frictional unemployment, structural unemployment and classical unemployment. Some additional types of unemployment that are occasionally mentioned are seasonal unemployment, hardcore unemployment, and hidden unemployment.		Though there have been several definitions of "voluntary" and "involuntary unemployment" in the economics literature, a simple distinction is often applied. Voluntary unemployment is attributed to the individual's decisions, whereas involuntary unemployment exists because of the socio-economic environment (including the market structure, government intervention, and the level of aggregate demand) in which individuals operate. In these terms, much or most of frictional unemployment is voluntary, since it reflects individual search behavior. Voluntary unemployment includes workers who reject low wage jobs whereas involuntary unemployment includes workers fired due to an economic crisis, industrial decline, company bankruptcy, or organizational restructuring.		On the other hand, cyclical unemployment, structural unemployment, and classical unemployment are largely involuntary in nature. However, the existence of structural unemployment may reflect choices made by the unemployed in the past, while classical (natural) unemployment may result from the legislative and economic choices made by labour unions or political parties. So, in practice, the distinction between voluntary and involuntary unemployment is hard to draw.		The clearest cases of involuntary unemployment are those where there are fewer job vacancies than unemployed workers even when wages are allowed to adjust, so that even if all vacancies were to be filled, some unemployed workers would still remain. This happens with cyclical unemployment, as macroeconomic forces cause microeconomic unemployment which can boomerang back and exacerbate these macroeconomic forces.		Classical, or real-wage unemployment, occurs when real wages for a job are set above the market-clearing level causing the number of job-seekers to exceed the number of vacancies. On the other hand, some economists argue that as wages fall below a livable wage many choose to drop out of the labor market and no longer seek employment. This is especially true in countries where low-income families are supported through public welfare systems. In such cases, wages would have to be high enough to motivate people to choose employment over what they receive through public welfare. Wages below a livable wage are likely to result in lower labor market participation in above stated scenario. In addition it must be noted that consumption of goods and services is the primary driver of increased need for labor. Higher wages leads to workers having more income available to consume goods and services. Therefore, higher wages increase general consumption and as a result need for labor increases and unemployment decreases in the economy.		Many economists have argued that unemployment increases with increased governmental regulation. For example, minimum wage laws raise the cost of some low-skill laborers above market equilibrium, resulting in increased unemployment as people who wish to work at the going rate cannot (as the new and higher enforced wage is now greater than the value of their labor).[6][7] Laws restricting layoffs may make businesses less likely to hire in the first place, as hiring becomes more risky.[7]		However, this argument overly simplifies the relationship between wage rates and unemployment, ignoring numerous factors, which contribute to unemployment.[8][9][10][11][12] Some, such as Murray Rothbard, suggest that even social taboos can prevent wages from falling to the market-clearing level.[13]		In Out of Work: Unemployment and Government in the Twentieth-Century America, economists Richard Vedder and Lowell Gallaway argue that the empirical record of wages rates, productivity, and unemployment in American validates classical unemployment theory. Their data shows a strong correlation between adjusted real wage and unemployment in the United States from 1900 to 1990. However, they maintain that their data does not take into account exogenous events.[14]		Cyclical, deficient-demand, or Keynesian unemployment, occurs when there is not enough aggregate demand in the economy to provide jobs for everyone who wants to work. Demand for most goods and services falls, less production is needed and consequently fewer workers are needed, wages are sticky and do not fall to meet the equilibrium level, and mass unemployment results.[15] Its name is derived from the frequent shifts in the business cycle although unemployment can also be persistent as occurred during the Great Depression of the 1930s.		With cyclical unemployment, the number of unemployed workers exceeds the number of job vacancies, so that even if full employment were attained and all open jobs were filled, some workers would still remain unemployed. Some associate cyclical unemployment with frictional unemployment because the factors that cause the friction are partially caused by cyclical variables. For example, a surprise decrease in the money supply may shock rational economic factors and suddenly inhibit aggregate demand.		Keynesian economists on the other hand see the lack of supply for jobs as potentially resolvable by government intervention. One suggested interventions involves deficit spending to boost employment and demand. Another intervention involves an expansionary monetary policy that increases the supply of money which should reduce interest rates which should lead to an increase in non-governmental spending.[16]		It is in the very nature of the capitalist mode of production to overwork some workers while keeping the rest as a reserve army of unemployed paupers.		Marxists share the Keynesian viewpoint of the relationship between economic demand and employment, but with the caveat that the market system's propensity to slash wages and reduce labor participation on an enterprise level causes a requisite decrease in aggregate demand in the economy as a whole, causing crises of unemployment and periods of low economic activity before the capital accumulation (investment) phase of economic growth can continue.		According to Karl Marx, unemployment is inherent within the unstable capitalist system and periodic crises of mass unemployment are to be expected. The function of the proletariat within the capitalist system is to provide a "reserve army of labour" that creates downward pressure on wages. This is accomplished by dividing the proletariat into surplus labour (employees) and under-employment (unemployed).[18] This reserve army of labour fight among themselves for scarce jobs at lower and lower wages.		At first glance, unemployment seems inefficient since unemployed workers do not increase profits, but unemployment is profitable within the global capitalist system because unemployment lowers wages which are costs from the perspective of the owners. From this perspective low wages benefit the system by reducing economic rents. Yet, it does not benefit workers. Capitalist systems unfairly manipulate the market for labour by perpetuating unemployment which lowers laborers' demands for fair wages. Workers are pitted against one another at the service of increasing profits for owners.		According to Marx, the only way to permanently eliminate unemployment would be to abolish capitalism and the system of forced competition for wages and then shift to a socialist or communist economic system. For contemporary Marxists, the existence of persistent unemployment is proof of the inability of capitalism to ensure full employment.[19]		In demand-based theory, it is possible to abolish cyclical unemployment by increasing the aggregate demand for products and workers. However, eventually the economy hits an "inflation barrier" imposed by the four other kinds of unemployment to the extent that they exist. Historical experience suggests that low unemployment affects inflation in the short term but not the long term.[20] In the long term, the velocity of money supply measures such as the MZM ("money zero maturity", representing cash and equivalent demand deposits) velocity is far more predictive of inflation than low unemployment.[21][22]		Some demand theory economists see the inflation barrier as corresponding to the natural rate of unemployment. The "natural" rate of unemployment is defined as the rate of unemployment that exists when the labour market is in equilibrium and there is pressure for neither rising inflation rates nor falling inflation rates. An alternative technical term for this rate is the NAIRU, or the Non-Accelerating Inflation Rate of Unemployment. No matter what its name, demand theory holds that this means that if the unemployment rate gets "too low," inflation will accelerate in the absence of wage and price controls (incomes policies).		One of the major problems with the NAIRU theory is that no one knows exactly what the NAIRU is (while it clearly changes over time).[20] The margin of error can be quite high relative to the actual unemployment rate, making it hard to use the NAIRU in policy-making.[21]		Another, normative, definition of full employment might be called the ideal unemployment rate. It would exclude all types of unemployment that represent forms of inefficiency. This type of "full employment" unemployment would correspond to only frictional unemployment (excluding that part encouraging the McJobs management strategy) and would thus be very low. However, it would be impossible to attain this full-employment target using only demand-side Keynesian stimulus without getting below the NAIRU and causing accelerating inflation (absent incomes policies). Training programs aimed at fighting structural unemployment would help here.		To the extent that hidden unemployment exists, it implies that official unemployment statistics provide a poor guide to what unemployment rate coincides with "full employment".[20]		Structural unemployment occurs when a labour market is unable to provide jobs for everyone who wants one because there is a mismatch between the skills of the unemployed workers and the skills needed for the available jobs. Structural unemployment is hard to separate empirically from frictional unemployment, except to say that it lasts longer. As with frictional unemployment, simple demand-side stimulus will not work to easily abolish this type of unemployment.		Structural unemployment may also be encouraged to rise by persistent cyclical unemployment: if an economy suffers from long-lasting low aggregate demand, it means that many of the unemployed become disheartened, while their skills (including job-searching skills) become "rusty" and obsolete. Problems with debt may lead to homelessness and a fall into the vicious circle of poverty.		This means that they may not fit the job vacancies that are created when the economy recovers. The implication is that sustained high demand may lower structural unemployment. This theory of persistence in structural unemployment has been referred to as an example of path dependence or "hysteresis".		Much technological unemployment,[23] due to the replacement of workers by machines, might be counted as structural unemployment. Alternatively, technological unemployment might refer to the way in which steady increases in labour productivity mean that fewer workers are needed to produce the same level of output every year. The fact that aggregate demand can be raised to deal with this problem suggests that this problem is instead one of cyclical unemployment. As indicated by Okun's Law, the demand side must grow sufficiently quickly to absorb not only the growing labour force but also the workers made redundant by increased labour productivity.		Seasonal unemployment may be seen as a kind of structural unemployment, since it is a type of unemployment that is linked to certain kinds of jobs (construction work, migratory farm work). The most-cited official unemployment measures erase this kind of unemployment from the statistics using "seasonal adjustment" techniques. This results in substantial, permanent structural unemployment.		Frictional unemployment is the time period between jobs when a worker is searching for, or transitioning from one job to another. It is sometimes called search unemployment and can be voluntary based on the circumstances of the unemployed individual.		Frictional unemployment exists because both jobs and workers are heterogeneous, and a mismatch can result between the characteristics of supply and demand. Such a mismatch can be related to skills, payment, work-time, location, seasonal industries, attitude, taste, and a multitude of other factors. New entrants (such as graduating students) and re-entrants (such as former homemakers) can also suffer a spell of frictional unemployment.		Workers as well as employers accept a certain level of imperfection, risk or compromise, but usually not right away; they will invest some time and effort to find a better match. This is in fact beneficial to the economy since it results in a better allocation of resources. However, if the search takes too long and mismatches are too frequent, the economy suffers, since some work will not get done. Therefore, governments will seek ways to reduce unnecessary frictional unemployment through multiple means including providing education, advice, training, and assistance such as daycare centers.		The frictions in the labour market are sometimes illustrated graphically with a Beveridge curve, a downward-sloping, convex curve that shows a correlation between the unemployment rate on one axis and the vacancy rate on the other. Changes in the supply of or demand for labour cause movements along this curve. An increase (decrease) in labour market frictions will shift the curve outwards (inwards).		Hidden, or covered, unemployment is the unemployment of potential workers that is not reflected in official unemployment statistics, due to the way the statistics are collected. In many countries only those who have no work but are actively looking for work (and/or qualifying for social security benefits) are counted as unemployed. Those who have given up looking for work (and sometimes those who are on Government "retraining" programs) are not officially counted among the unemployed, even though they are not employed.		The statistic also does not count the "underemployed"–those working fewer hours than they would prefer or in a job that doesn't make good use of their capabilities. In addition, those who are of working age but are currently in full-time education are usually not considered unemployed in government statistics. Traditional unemployed native societies who survive by gathering, hunting, herding, and farming in wilderness areas, may or may not be counted in unemployment statistics. Official statistics often underestimate unemployment rates because of hidden unemployment.		This is defined in European Union statistics, as unemployment lasting for longer than one year. The United States Bureau of Labor Statistics (BLS), which reports current long-term unemployment rate at 1.9 percent, defines this as unemployment lasting 27 weeks or longer. Long-term unemployment is a component of structural unemployment, which results in long-term unemployment existing in every social group, industry, occupation, and all levels of education.[24]		There are also different ways national statistical agencies measure unemployment. These differences may limit the validity of international comparisons of unemployment data.[25] To some degree these differences remain despite national statistical agencies increasingly adopting the definition of unemployment by the International Labour Organization.[26] To facilitate international comparisons, some organizations, such as the OECD, Eurostat, and International Labor Comparisons Program, adjust data on unemployment for comparability across countries.		Though many people care about the number of unemployed individuals, economists typically focus on the unemployment rate. This corrects for the normal increase in the number of people employed due to increases in population and increases in the labour force relative to the population. The unemployment rate is expressed as a percentage, and is calculated as follows:		As defined by the International Labour Organization, "unemployed workers" are those who are currently not working but are willing and able to work for pay, currently available to work, and have actively searched for work.[27] Individuals who are actively seeking job placement must make the effort to: be in contact with an employer, have job interviews, contact job placement agencies, send out resumes, submit applications, respond to advertisements, or some other means of active job searching within the prior four weeks. Simply looking at advertisements and not responding will not count as actively seeking job placement. Since not all unemployment may be "open" and counted by government agencies, official statistics on unemployment may not be accurate.[28] In the United States, for example, the unemployment rate does not take into consideration those individuals who are not actively looking for employment, such as those still attending college.[29]		The ILO describes 4 different methods to calculate the unemployment rate:[30]		The primary measure of unemployment, U3, allows for comparisons between countries. Unemployment differs from country to country and across different time periods. For example, during the 1990s and 2000s, the United States had lower unemployment levels than many countries in the European Union,[31] which had significant internal variation, with countries like the UK and Denmark outperforming Italy and France. However, large economic events such as the Great Depression can lead to similar unemployment rates across the globe.		Causes of unemployment		(i) Caste System: In many cases, the work is not given to the deserving candidates but given to the person belonging to a particular community. So this gives rise to unemployment.		(ii) Slow Economic Growth: Indian economy is underdeveloped and role of economic growth is very slow. This slow growth fails to provide enough unemployment opportunities to the increasing population.		(iii) Increase in Population: Constant increase in population has been a big problem in India. It is one of the main causes of unemployment. The rate of unemployment is 11.1% in 10th Plan. (iv) Agriculture is a Seasonal Occupation:		Agriculture is underdeveloped in India. It provides seasonal employment. Large part of population is dependent on agriculture. But agriculture being seasonal provides work for a few months. So this gives rise to unemployment. (v) Joint Family System:		In big families having big business, many such persons will be available who do not do any work and depend on the joint income of the family.		Many of them seem to be working but they do not add anything to production. So they encourage disguised unemployment. (vi) Fall of Cottage and Small industries:		The industrial development had adverse effect on cottage and small industries. The production of cottage industries began to fall and many artisans became unemployed.		Eurostat, the statistical office of the European Union, defines unemployed as those persons age 15 to 74 who are not working, have looked for work in the last four weeks, and ready to start work within two weeks, which conform to ILO standards. Both the actual count and rate of unemployment are reported. Statistical data are available by member state, for the European Union as a whole (EU28) as well as for the euro area (EA19). Eurostat also includes a long-term unemployment rate. This is defined as part of the unemployed who have been unemployed for an excess of 1 year.[32]		The main source used is the European Union Labour Force Survey (EU-LFS). The EU-LFS collects data on all member states each quarter. For monthly calculations, national surveys or national registers from employment offices are used in conjunction with quarterly EU-LFS data. The exact calculation for individual countries, resulting in harmonized monthly data, depend on the availability of the data.[33]		The Bureau of Labor Statistics measures employment and unemployment (of those over 15 years of age) using two different labor force surveys[35] conducted by the United States Census Bureau (within the United States Department of Commerce) and/or the Bureau of Labor Statistics (within the United States Department of Labor) that gather employment statistics monthly. The Current Population Survey (CPS), or "Household Survey", conducts a survey based on a sample of 60,000 households. This Survey measures the unemployment rate based on the ILO definition.[36]		The Current Employment Statistics survey (CES), or "Payroll Survey", conducts a survey based on a sample of 160,000 businesses and government agencies that represent 400,000 individual employers.[37] This survey measures only civilian nonagricultural employment; thus, it does not calculate an unemployment rate, and it differs from the ILO unemployment rate definition. These two sources have different classification criteria, and usually produce differing results. Additional data are also available from the government, such as the unemployment insurance weekly claims report available from the Office of Workforce Security, within the U.S. Department of Labor Employment & Training Administration.[38] The Bureau of Labor Statistics provides up-to-date numbers via a PDF linked here.[39] The BLS also provides a readable concise current Employment Situation Summary, updated monthly.[40]		The Bureau of Labor Statistics also calculates six alternate measures of unemployment, U1 through U6, that measure different aspects of unemployment:[41]		Note: "Marginally attached workers" are added to the total labour force for unemployment rate calculation for U4, U5, and U6. The BLS revised the CPS in 1994 and among the changes the measure representing the official unemployment rate was renamed U3 instead of U5.[44] In 2013, Representative Hunter proposed that the Bureau of Labor Statistics use the U5 rate instead of the current U3 rate.[45]		Statistics for the U.S. economy as a whole hide variations among groups. For example, in January 2008 U.S. unemployment rates were 4.4% for adult men, 4.2% for adult women, 4.4% for Caucasians, 6.3% for Hispanics or Latinos (all races), 9.2% for African Americans, 3.2% for Asian Americans, and 18.0% for teenagers.[37] Also, the U.S. unemployment rate would be at least 2% higher if prisoners and jail inmates were counted.[46][47]		The unemployment rate is included in a number of major economic indexes including the United States' Conference Board's Index of Leading Indicators a macroeconomic measure of the state of the economy.		Some critics believe that current methods of measuring unemployment are inaccurate in terms of the impact of unemployment on people as these methods do not take into account the 1.5% of the available working population incarcerated in U.S. prisons (who may or may not be working while incarcerated), those who have lost their jobs and have become discouraged over time from actively looking for work, those who are self-employed or wish to become self-employed, such as tradesmen or building contractors or IT consultants, those who have retired before the official retirement age but would still like to work (involuntary early retirees), those on disability pensions who, while not possessing full health, still wish to work in occupations suitable for their medical conditions, those who work for payment for as little as one hour per week but would like to work full-time.[53]		These people are "involuntary part-time" workers, those who are underemployed, e.g., a computer programmer who is working in a retail store until he can find a permanent job, involuntary stay-at-home mothers who would prefer to work, and graduate and Professional school students who were unable to find worthwhile jobs after they graduated with their bachelor's degrees.		Internationally, some nations' unemployment rates are sometimes muted or appear less severe due to the number of self-employed individuals working in agriculture.[48] Small independent farmers are often considered self-employed; so, they cannot be unemployed. The impact of this is that in non-industrialized economies, such as the United States and Europe during the early 19th century, overall unemployment was approximately 3% because so many individuals were self-employed, independent farmers; yet, unemployment outside of agriculture was as high as 80%.[48]		Many economies industrialize and experience increasing numbers of non-agricultural workers. For example, the United States' non-agricultural labour force increased from 20% in 1800, to 50% in 1850, to 97% in 2000.[48] The shift away from self-employment increases the percentage of the population who are included in unemployment rates. When comparing unemployment rates between countries or time periods, it is best to consider differences in their levels of industrialization and self-employment.		Additionally, the measures of employment and unemployment may be "too high". In some countries, the availability of unemployment benefits can inflate statistics since they give an incentive to register as unemployed. People who do not seek work may choose to declare themselves unemployed so as to get benefits; people with undeclared paid occupations may try to get unemployment benefits in addition to the money they earn from their work.[54]		However, in countries such as the United States, Canada, Mexico, Australia, Japan and the European Union, unemployment is measured using a sample survey (akin to a Gallup poll).[26] According to the BLS, a number of Eastern European nations have instituted labour force surveys as well. The sample survey has its own problems because the total number of workers in the economy is calculated based on a sample rather than a census.		It is possible to be neither employed nor unemployed by ILO definitions, i.e., to be outside of the "labour force".[28] These are people who have no job and are not looking for one. Many of these people are going to school or are retired. Family responsibilities keep others out of the labour force. Still others have a physical or mental disability which prevents them from participating in labour force activities. And of course, some people simply elect not to work preferring to be dependent on others for sustenance.		Typically, employment and the labour force include only work done for monetary gain. Hence, a homemaker is neither part of the labour force nor unemployed. Nor are full-time students nor prisoners considered to be part of the labour force or unemployment.[53] The latter can be important. In 1999, economists Lawrence F. Katz and Alan B. Krueger estimated that increased incarceration lowered measured unemployment in the United States by 0.17% between 1985 and the late 1990s.[53]		In particular, as of 2005, roughly 0.7% of the U.S. population is incarcerated (1.5% of the available working population). Additionally, children, the elderly, and some individuals with disabilities are typically not counted as part of the labour force in and are correspondingly not included in the unemployment statistics. However, some elderly and many disabled individuals are active in the labour market		In the early stages of an economic boom, unemployment often rises.[15] This is because people join the labour market (give up studying, start a job hunt, etc.) as a result of the improving job market, but until they have actually found a position they are counted as unemployed. Similarly, during a recession, the increase in the unemployment rate is moderated by people leaving the labour force or being otherwise discounted from the labour force, such as with the self-employed.		For the fourth quarter of 2004, according to OECD, (source Employment Outlook 2005 ISBN 92-64-01045-9), normalized unemployment for men aged 25 to 54 was 4.6% in the U.S. and 7.4% in France. At the same time and for the same population the employment rate (number of workers divided by population) was 86.3% in the U.S. and 86.7% in France. This example shows that the unemployment rate is 60% higher in France than in the U.S., yet more people in this demographic are working in France than in the U.S., which is counterintuitive if it is expected that the unemployment rate reflects the health of the labour market.[55][56]		Due to these deficiencies, many labour market economists prefer to look at a range of economic statistics such as labour market participation rate, the percentage of people aged between 15 and 64 who are currently employed or searching for employment, the total number of full-time jobs in an economy, the number of people seeking work as a raw number and not a percentage, and the total number of person-hours worked in a month compared to the total number of person-hours people would like to work. In particular the NBER does not use the unemployment rate but prefer various employment rates to date recessions.[57]		The labor force participation rate is the ratio between the labor force and the overall size of their cohort (national population of the same age range). In the West during the later half of the 20th century, the labor force participation rate increased significantly, due to an increase in the number of women who entered the workplace.		In the United States, there have been four significant stages of women's participation in the labor force—increases in the 20th century and decreases in the 21st century. Male labor force participation decreased from 1953 until 2013. Since October 2013 men have been increasingly joining the labor force.		During the late 19th century through the 1920s, very few women worked outside the home. They were young single women who typically withdrew from the labor force at marriage unless family needed two incomes. These women worked primarily in the textile manufacturing industry or as domestic workers. This profession empowered women and allowed them to earn a living wage. At times, they were a financial help to their families.		Between 1930 and 1950, female labor force participation increased primarily due to the increased demand for office workers, women's participation in the high school movement, and due to electrification which reduced the time spent on household chores. Between the 1950s to the early 1970s, most women were secondary earners working mainly as secretaries, teachers, nurses, and librarians (pink-collar jobs).		Between the mid 1970s to the late 1990s there was a period of revolution of women in the labor force brought on by a source of different factors. Women more accurately planned for their future in the work force, investing in more applicable majors in college that prepared them to enter and compete in the labor market. In the United States, the female labor force participation rate rose from approximately 33% in 1948 to a peak of 60.3% in 2000. As of April 2015 the female labor force participation is at 56.6%, the male labor force participation rate is at 69.4% and the total is 62.8%.[58]		A common theory in modern economics claims that the rise of women participating in the U.S. labor force in the 1950s through to the 1990s was due to the introduction of a new contraceptive technology, birth control pills, and the adjustment of age of majority laws. The use of birth control gave women the flexibility of opting to invest and advance their career while maintaining a relationship. By having control over the timing of their fertility, they were not running a risk of thwarting their career choices. However, only 40% of the population actually used the birth control pill.		This implies that other factors may have contributed to women choosing to invest in advancing their careers. One factor may be that more and more men delayed the age of marriage, allowing women to marry later in life without worrying about the quality of older men. Other factors include the changing nature of work, with machines replacing physical labor, eliminating many traditional male occupations, and the rise of the service sector, where many jobs are gender neutral.		Another factor that may have contributed to the trend was The Equal Pay Act of 1963, which aimed at abolishing wage disparity based on sex. Such legislation diminished sexual discrimination and encouraged more women to enter the labor market by receiving fair remuneration to help raising families and children.		At the turn of the 21st century the labor force participation began to reverse its long period of increase. Reasons for this change include a rising share of older workers, an increase in school enrollment rates among young workers and a decrease in female labor force participation.[59]		The labor force participation rate can decrease when the rate of growth of the population outweighs that of the employed and unemployed together. The labor force participation rate is a key component in long-term economic growth, almost as important as productivity.		A historic shift began around the end of the great recession as women began leaving the labor force in the United States and other developed countries.[60] The female labor force participation rate in the United States has steadily decreased since 2009 and as of April 2015 the female labor force participation rate has gone back down to 1988 levels of 56.6%.[58]		Participation rates are defined as follows:		The labor force participation rate explains how an increase in the unemployment rate can occur simultaneously with an increase in employment. If a large amount of new workers enter the labor force but only a small fraction become employed, then the increase in the number of unemployed workers can outpace the growth in employment.[61]		The unemployment ratio calculates the share of unemployed for the whole population. Particularly many young people between 15 and 24 are studying full-time and are therefore neither working nor looking for a job. This means they are not part of the labour force which is used as the denominator for calculating the unemployment rate.[62] The youth unemployment ratios in the European Union range from 5.2 (Austria) to 20.6 percent (Spain). These are considerably lower than the standard youth unemployment rates, ranging from 7.9 (Germany) to 57.9 percent (Greece).[63]		High and persistent unemployment, in which economic inequality increases, has a negative effect on subsequent long-run economic growth. Unemployment can harm growth not only because it is a waste of resources, but also because it generates redistributive pressures and subsequent distortions, drives people to poverty, constrains liquidity limiting labor mobility, and erodes self-esteem promoting social dislocation, unrest and conflict.[64] 2013 Economics Nobel prize winner Robert J. Shiller said that rising inequality in the United States and elsewhere is the most important problem.[65]		Unemployed individuals are unable to earn money to meet financial obligations. Failure to pay mortgage payments or to pay rent may lead to homelessness through foreclosure or eviction.[66] Across the United States the growing ranks of people made homeless in the foreclosure crisis are generating tent cities.[67]		Unemployment increases susceptibility to cardiovascular disease, somatization, anxiety disorders, depression, and suicide. In addition, unemployed people have higher rates of medication use, poor diet, physician visits, tobacco smoking, alcoholic beverage consumption, drug use, and lower rates of exercise.[68] According to a study published in Social Indicator Research, even those who tend to be optimistic find it difficult to look on the bright side of things when unemployed. Using interviews and data from German participants aged 16 to 94—including individuals coping with the stresses of real life and not just a volunteering student population—the researchers determined that even optimists struggled with being unemployed.[69]		In 1979, Brenner found that for every 10% increase in the number of unemployed there is an increase of 1.2% in total mortality, a 1.7% increase in cardiovascular disease, 1.3% more cirrhosis cases, 1.7% more suicides, 4.0% more arrests, and 0.8% more assaults reported to the police.[70][71]		A study by Ruhm, in 2000, on the effect of recessions on health found that several measures of health actually improve during recessions.[72] As for the impact of an economic downturn on crime, during the Great Depression the crime rate did not decrease. The unemployed in the U.S. often use welfare programs such as Food Stamps or accumulating debt because unemployment insurance in the U.S. generally does not replace a majority of the income one received on the job (and one cannot receive such aid indefinitely).		Not everyone suffers equally from unemployment. In a prospective study of 9570 individuals over four years, highly conscientious people suffered more than twice as much if they became unemployed.[73] The authors suggested this may be due to conscientious people making different attributions about why they became unemployed, or through experiencing stronger reactions following failure. There is also possibility of reverse causality from poor health to unemployment.[74]		Some researchers hold that many of the low-income jobs are not really a better option than unemployment with a welfare state (with its unemployment insurance benefits). But since it is difficult or impossible to get unemployment insurance benefits without having worked in the past, these jobs and unemployment are more complementary than they are substitutes. (These jobs are often held short-term, either by students or by those trying to gain experience; turnover in most low-paying jobs is high.)		Another cost for the unemployed is that the combination of unemployment, lack of financial resources, and social responsibilities may push unemployed workers to take jobs that do not fit their skills or allow them to use their talents. Unemployment can cause underemployment, and fear of job loss can spur psychological anxiety. As well as anxiety, it can cause depression, lack of confidence, and huge amounts of stress. This stress is increased when the unemployed are faced with health issues, poverty, and lack of relational support.[75]		Another personal cost of unemployment is its impact on relationships. A 2008 study from Covizzi, which examines the relationship between unemployment and divorce, found that the rate of divorce is greater for couples when one partner is unemployed.[76] However, a more recent study has found that some couples often stick together in “unhappy” or “unhealthy” marriages when unemployed to buffer financial costs.[77] A 2014 study by Van der Meer found that the stigma that comes from being unemployed affects personal well-being, especially for men, who often feel as though their masculine identities are threatened by unemployment.[78]		Unemployment can also bring personal costs in relation to gender. One study found that women are more likely to experience unemployment than men and that they are less likely to move from temporary positions to permanent positions.[79] Another study on gender and unemployment found that men, however, are more likely to experience greater stress, depression, and adverse effects from unemployment, largely stemming from the perceived threat to their role as breadwinner.[80] This study found that men expect themselves to be viewed as “less manly” after a job loss than they actually are, and as a result they engage in compensating behaviors, such as financial risk-taking and increased assertiveness, because of it.		Costs of unemployment also vary depending on age. The young and the old are the two largest age groups currently experiencing unemployment.[81] A 2007 study from Jacob and Kleinert found that young people (ages 18 to 24) who have fewer resources and limited work experiences are more likely to be unemployed.[82] Other researchers have found that today’s high school seniors place a lower value on work than those in the past, and this is likely because they recognize the limited availability of jobs.[83] At the other end of the age spectrum, studies have found that older individuals have more barriers than younger workers to employment, require stronger social networks to acquire work, and are also less likely to move from temporary to permanent positions.[79][79][81] Additionally, some older people see age discrimination as the reason they are not getting hired.[84]		An economy with high unemployment is not using all of the resources, specifically labour, available to it. Since it is operating below its production possibility frontier, it could have higher output if all the workforce were usefully employed. However, there is a trade-off between economic efficiency and unemployment: if the frictionally unemployed accepted the first job they were offered, they would be likely to be operating at below their skill level, reducing the economy's efficiency.[85]		During a long period of unemployment, workers can lose their skills, causing a loss of human capital. Being unemployed can also reduce the life expectancy of workers by about seven years.[7]		High unemployment can encourage xenophobia and protectionism as workers fear that foreigners are stealing their jobs.[86] Efforts to preserve existing jobs of domestic and native workers include legal barriers against "outsiders" who want jobs, obstacles to immigration, and/or tariffs and similar trade barriers against foreign competitors.		High unemployment can also cause social problems such as crime; if people have less disposable income than before, it is very likely that crime levels within the economy will increase.		A 2015 study published in The Lancet estimates that unemployment causes 45,000 suicides a year globally.[87]		High levels of unemployment can be causes of civil unrest,[88] in some cases leading to revolution, and particularly totalitarianism. The fall of the Weimar Republic in 1933 and Adolf Hitler's rise to power, which culminated in World War II and the deaths of tens of millions and the destruction of much of the physical capital of Europe, is attributed to the poor economic conditions in Germany at the time, notably a high unemployment rate[89] of above 20%; see Great Depression in Central Europe for details.		Note that the hyperinflation in the Weimar Republic is not directly blamed for the Nazi rise—the Hyperinflation in the Weimar Republic occurred primarily in the period 1921–23, which was contemporary with Hitler's Beer Hall Putsch of 1923, and is blamed for damaging the credibility of democratic institutions, but the Nazis did not assume government until 1933, ten years after the hyperinflation but in the midst of high unemployment.		Rising unemployment has traditionally been regarded by the public and media in any country as a key guarantor of electoral defeat for any government which oversees it. This was very much the consensus in the United Kingdom until 1983, when Margaret Thatcher's Conservative government won a landslide in the general election, despite overseeing a rise in unemployment from 1,500,000 to 3,200,000 since its election four years earlier.[90]		The primary benefit of unemployment is that people are available for hire, without being headhunted away from their existing employers. This permits new and old businesses to take on staff.		Unemployment is argued to be "beneficial" to the people who are not unemployed in the sense that it averts inflation,[citation needed] which itself has damaging effects, by providing (in Marxian terms) a reserve army of labour, that keeps wages in check. However, the direct connection between full local employment and local inflation has been disputed by some due to the recent increase in international trade that supplies low-priced goods even while local employment rates rise to full employment.[91]		Full employment cannot be achieved because workers would shirk[citation needed] if they were not threatened with the possibility of unemployment. The curve for the no-shirking condition (labeled NSC) goes to infinity at full employment as a result. The inflation-fighting benefits to the entire economy arising from a presumed optimum level of unemployment has been studied extensively.[92] The Shapiro–Stiglitz model suggests that wages are not bid down sufficiently to ever reach 0% unemployment.[93] This occurs because employers know that when wages decrease, workers will shirk and expend less effort. Employers avoid shirking by preventing wages from decreasing so low that workers give up and become unproductive. These higher wages perpetuate unemployment while the threat of unemployment reduces shirking.		Before current levels of world trade were developed, unemployment was demonstrated to reduce inflation, following the Phillips curve, or to decelerate inflation, following the NAIRU/natural rate of unemployment theory, since it is relatively easy to seek a new job without losing one's current one. And when more jobs are available for fewer workers (lower unemployment), it may allow workers to find the jobs that better fit their tastes, talents, and needs.		As in the Marxian theory of unemployment, special interests may also benefit: some employers may expect that employees with no fear of losing their jobs will not work as hard, or will demand increased wages and benefit. According to this theory, unemployment may promote general labour productivity and profitability by increasing employers' rationale for their monopsony-like power (and profits).[17]		Optimal unemployment has also been defended as an environmental tool to brake the constantly accelerated growth of the GDP to maintain levels sustainable in the context of resource constraints and environmental impacts.[94] However the tool of denying jobs to willing workers seems a blunt instrument for conserving resources and the environment—it reduces the consumption of the unemployed across the board, and only in the short term. Full employment of the unemployed workforce, all focused toward the goal of developing more environmentally efficient methods for production and consumption might provide a more significant and lasting cumulative environmental benefit and reduced resource consumption.[95] If so the future economy and workforce would benefit from the resultant structural increases in the sustainable level of GDP growth.		Some critics of the "culture of work" such as anarchist Bob Black see employment as overemphasized culturally in modern countries. Such critics often propose quitting jobs when possible, working less, reassessing the cost of living to this end, creation of jobs which are "fun" as opposed to "work," and creating cultural norms where work is seen as unhealthy. These people advocate an "anti-work" ethic for life.[96]		As a result of productivity the work week declined considerably over the 19th century.[97][98] By the 1920s in the U.S. the average work week was 49 hours, but the work week was reduced to 40 hours (after which overtime premium was applied) as part of the National Industrial Recovery Act of 1933. At the time of the Great Depression of the 1930s it was believed that due to the enormous productivity gains due to electrification, mass production and agricultural mechanization, there was no need for a large number of previously employed workers.[23][99]		Societies try a number of different measures to get as many people as possible into work, and various societies have experienced close to full employment for extended periods, particularly during the Post-World War II economic expansion. The United Kingdom in the 1950s and 60s averaged 1.6% unemployment,[101] while in Australia the 1945 White Paper on Full Employment in Australia established a government policy of full employment, which policy lasted until the 1970s when the government ran out of money.		However, mainstream economic discussions of full employment since the 1970s suggest that attempts to reduce the level of unemployment below the natural rate of unemployment will fail, resulting only in less output and more inflation.		Increases in the demand for labour will move the economy along the demand curve, increasing wages and employment. The demand for labour in an economy is derived from the demand for goods and services. As such, if the demand for goods and services in the economy increases, the demand for labour will increase, increasing employment and wages.		There are many ways to stimulate demand for goods and services. Increasing wages to the working class (those more likely to spend the increased funds on goods and services, rather than various types of savings, or commodity purchases) is one theory proposed. Increased wages is believed to be more effective in boosting demand for goods and services than central banking strategies that put the increased money supply mostly into the hands of wealthy persons and institutions. Monetarists suggest that increasing money supply in general will increase short-term demand. Long-term the increased demand will be negated by inflation. A rise in fiscal expenditures is another strategy for boosting aggregate demand.		Providing aid to the unemployed is a strategy used to prevent cutbacks in consumption of goods and services which can lead to a vicious cycle of further job losses and further decreases in consumption/demand. Many countries aid the unemployed through social welfare programs. These unemployment benefits include unemployment insurance, unemployment compensation, welfare and subsidies to aid in retraining. The main goal of these programs is to alleviate short-term hardships and, more importantly, to allow workers more time to search for a job.		A direct demand-side solution to unemployment is government-funded employment of the able-bodied poor. This was notably implemented in Britain from the 17th century until 1948 in the institution of the workhouse, which provided jobs for the unemployed with harsh conditions and poor wages to dissuade their use. A modern alternative is a job guarantee, where the government guarantees work at a living wage.		Temporary measures can include public works programs such as the Works Progress Administration. Government-funded employment is not widely advocated as a solution to unemployment, except in times of crisis; this is attributed to the public sector jobs' existence depending directly on the tax receipts from private sector employment.		In the U.S., the unemployment insurance allowance one receives is based solely on previous income (not time worked, family size, etc.) and usually compensates for one-third of one's previous income. To qualify, one must reside in their respective state for at least a year and, of course, work. The system was established by the Social Security Act of 1935. Although 90% of citizens are covered by unemployment insurance, less than 40% apply for and receive benefits.[102] However, the number applying for and receiving benefits increases during recessions. In cases of highly seasonal industries the system provides income to workers during the off seasons, thus encouraging them to stay attached to the industry.		According to classical economic theory, markets reach equilibrium where supply equals demand; everyone who wants to sell at the market price can. Those who do not want to sell at this price do not; in the labour market this is classical unemployment. Monetary policy and fiscal policy can both be used to increase short-term growth in the economy, increasing the demand for labour and decreasing unemployment.		However, the labor market is not 100% efficient, although it may be more efficient than the bureaucracy. Some argue that minimum wages and union activity keep wages from falling, which means too many people want to sell their labour at the going price but cannot. This assumes perfect competition exists in the labour market, specifically that no single entity is large enough to affect wage levels and that employees are similar in ability.		Advocates of supply-side policies believe those policies can solve this by making the labour market more flexible. These include removing the minimum wage and reducing the power of unions. Supply-siders argue the reforms increase long-term growth by reducing labour costs. This increased supply of goods and services requires more workers, increasing employment. It is argued that supply-side policies, which include cutting taxes on businesses and reducing regulation, create jobs, reduce unemployment and decrease labour's share of national income. Other supply-side policies include education to make workers more attractive to employers.		There are relatively limited historical records on unemployment because it has not always been acknowledged or measured systematically. Industrialization involves economies of scale that often prevent individuals from having the capital to create their own jobs to be self-employed. An individual who cannot either join an enterprise or create a job is unemployed. As individual farmers, ranchers, spinners, doctors and merchants are organized into large enterprises, those who cannot join or compete become unemployed.		Recognition of unemployment occurred slowly as economies across the world industrialized and bureaucratized. Before this, traditional self sufficient native societies have no concept of unemployment. The recognition of the concept of "unemployment" is best exemplified through the well documented historical records in England. For example, in 16th century England no distinction was made between vagrants and the jobless; both were simply categorized as "sturdy beggars", to be punished and moved on.[104]		The closing of the monasteries in the 1530s increased poverty, as the church had helped the poor. In addition, there was a significant rise in enclosure during the Tudor period. Also the population was rising. Those unable to find work had a stark choice: starve or break the law. In 1535, a bill was drawn up calling for the creation of a system of public works to deal with the problem of unemployment, to be funded by a tax on income and capital. A law passed a year later allowed vagabonds to be whipped and hanged.[105]		In 1547, a bill was passed that subjected vagrants to some of the more extreme provisions of the criminal law, namely two years servitude and branding with a "V" as the penalty for the first offense and death for the second.[106] During the reign of Henry VIII, as many as 72,000 people are estimated to have been executed.[107] In the 1576 Act each town was required to provide work for the unemployed.[108]		The Elizabethan Poor Law of 1601, one of the world's first government-sponsored welfare programs, made a clear distinction between those who were unable to work and those able-bodied people who refused employment.[109] Under the Poor Law systems of England and Wales, Scotland and Ireland a workhouse was a place where people who were unable to support themselves, could go to live and work.[110]		Poverty was a highly visible problem in the eighteenth century, both in cities and in the countryside. In France and Britain by the end of the century, an estimated 10 percent of the people depended on charity or begging for their food.		By 1776 some 1,912 parish and corporation workhouses had been established in England and Wales, housing almost 100,000 paupers.		A description of the miserable living standards of the mill workers in England in 1844 was given by Fredrick Engels in The Condition of the Working-Class in England in 1844.[111] In the preface to the 1892 edition Engels notes that the extreme poverty he wrote about in 1844 had largely disappeared. David Ames Wells also noted that living conditions in England had improved near the end of the 19th century and that unemployment was low.		The scarcity and high price of labor in the U.S. during the 19th century was well documented by contemporary accounts, as in the following:		"The laboring classes are comparatively few in number, but this is counterbalanced by, and indeed, may be one of the causes of the eagerness by which they call in the use of machinery in almost every department of industry. Wherever it can be applied as a substitute for manual labor, it is universally and willingly resorted to ....It is this condition of the labor market, and this eager resort to machinery wherever it can be applied, to which, under the guidance of superior education and intelligence, the remarkable prosperity of the United States is due."[112] Joseph Whitworth, 1854		Scarcity of labor was a factor in the economics of slavery in the United States.		As new territories were opened and Federal land sales conducted, land had to be cleared and new homesteads established. Hundreds of thousands of immigrants annually came to the U.S. and found jobs digging canals and building railroads. Almost all work during most of the 19th century was done by hand or with horses, mules, or oxen, because there was very little mechanization. The workweek during most of the 19th century was 60 hours. Unemployment at times was between one and two percent.		The tight labor market was a factor in productivity gains allowing workers to maintain or increase their nominal wages during the secular deflation that caused real wages to rise at various times in the 19th century, especially in the final decades.[113]		There were labor shortages during WW I.[23] Ford Motor Co. doubled wages to reduce turnover. After 1925 unemployment began to gradually rise.[114]		The decade of the 1930s saw the Great Depression impact unemployment across the globe. One Soviet trading corporation in New York averaged 350 applications a day from Americans seeking jobs in the Soviet Union.[115] In Germany the unemployment rate reached nearly 25% in 1932.[116]		In some towns and cities in the north east of England, unemployment reached as high as 70%; the national unemployment level peaked at more than 22% in 1932.[117] Unemployment in Canada reached 27% at the depth of the Depression in 1933.[118] In 1929, the U.S. unemployment rate averaged 3%.[119]		In the U.S., the Works Progress Administration (1935–43) was the largest make-work program. It hired men (and some women) off the relief roles ("dole") typically for unskilled labor.[120]		In Cleveland, Ohio, the unemployment rate was 60%; in Toledo, Ohio, 80%.[121] There were two million homeless people migrating across the United States.[121] Over 3 million unemployed young men were taken out of the cities and placed into 2600+ work camps managed by the Civilian Conservation Corps.[122]		Unemployment in the United Kingdom fell later in the 1930s as the depression eased, and remained low (in six figures) after World War II.		Fredrick Mills found that in the U.S., 51% of the decline in work hours was due to the fall in production and 49% was from increased productivity.[123] By 1972 unemployment in the UK had crept back up above 1,000,000, and was even higher by the end of the decade, with inflation also being high. Although the monetarist economic policies of Margaret Thatcher's Conservative government saw inflation reduced after 1979, unemployment soared in the early 1980s, exceeding 3,000,000 – a level not seen for some 50 years – by 1982. This represented one in eight of the workforce, with unemployment exceeding 20% in some parts of the United Kingdom which had relied on the now-declining industries such as coal mining.[124]		However, this was a time of high unemployment in all major industrialised nations.[125] By the spring of 1983, unemployment in the United Kingdom had risen by 6% in the previous 12 months; compared to 10% in Japan, 23% in the United States of America and 34% in West Germany (seven years before reunification).[126]		Unemployment in the United Kingdom remained above 3,000,000 until the spring of 1987, by which time the economy was enjoying a boom.[124] By the end of 1989, unemployment had fallen to 1,600,000. However, inflation had reached 7.8% and the following year it reached a nine-year high of 9.5%; leading to increased interest rates.[127]		Another recession began during 1990 and lasted until 1992. Unemployment began to increase and by the end of 1992 nearly 3,000,000 in the United Kingdom were unemployed. Then came a strong economic recovery.[124] With inflation down to 1.6% by 1993, unemployment then began to fall rapidly, standing at 1,800,000 by early 1997.[128]		The official unemployment rate in the 16 EU countries that use the euro rose to 10% in December 2009 as a result of another recession.[129] Latvia had the highest unemployment rate in EU at 22.3% for November 2009.[130] Europe's young workers have been especially hard hit.[131] In November 2009, the unemployment rate in the EU27 for those aged 15–24 was 18.3%. For those under 25, the unemployment rate in Spain was 43.8%.[132] Unemployment has risen in two-thirds of European countries since 2010.[133]		Into the 21st century, unemployment in the United Kingdom remained low and the economy remaining strong, while at this time several other European economies – namely, France and Germany (reunified a decade earlier) – experienced a minor recession and a substantial rise in unemployment.[134]		In 2008, when the recession brought on another increase in the United Kingdom, after 15 years of economic growth and no major rises in unemployment.[135] Early in 2009, unemployment passed the 2,000,000 mark, by which time economists were predicting it would soon reach 3,000,000.[136] However, the end of the recession was declared in January 2010[137] and unemployment peaked at nearly 2,700,000 in 2011,[138] appearing to ease fears of unemployment reaching 3,000,000.[139] The unemployment rate of Britain's young black people was 47.4% in 2011.[140] 2013/2014 has seen the employment rate increase from 1,935,836 to 2,173,012 as supported by[141] showing the UK is creating more job opportunities and forecasts the rate of increase in 2014/2015 will be another 7.2%.[142]		An 26 April 2005 Asia Times article notes that, "In regional giant South Africa, some 300,000 textile workers have lost their jobs in the past two years due to the influx of Chinese goods".[143] The increasing U.S. trade deficit with China cost 2.4 million American jobs between 2001 and 2008, according to a study by the Economic Policy Institute (EPI).[144] From 2000 to 2007, the United States lost a total of 3.2 million manufacturing jobs.[145] 12.1% of US military veterans who had served after the September 11 attacks in 2001 were unemployed as of 2011; 29.1% of male veterans aged 18–24 were unemployed.[68]		About 25 million people in the world's 30 richest countries will have lost their jobs between the end of 2007 and the end of 2010 as the economic downturn pushes most countries into recession.[146] In April 2010, the U.S. unemployment rate was 9.9%, but the government's broader U-6 unemployment rate was 17.1%.[147] In April 2012, the unemployment rate was 4.6% in Japan.[148] In a 2012 news story, the Financial Post reported, "Nearly 75 million youth are unemployed around the world, an increase of more than 4 million since 2007. In the European Union, where a debt crisis followed the financial crisis, the youth unemployment rate rose to 18% last year from 12.5% in 2007, the ILO report shows."[149]		http://www.economicshelp.org/blog/2247/unemployment/definition-of-unemployment/ http://mawdoo3.com/%D8%AA%D8%B9%D8%B1%D9%8A%D9%81_%D8%A7%D9%84%D8%A8%D8%B7%D8%A7%D9%84%D8%A9 http://www.economicsdiscussion.net/articles/main-causes-of-unemployment-in-india/2281		
The unemployment rate is a measure of the prevalence of unemployment and it is calculated as a percentage by dividing the number of unemployed individuals by all individuals currently in the labor force. During periods of recession, an economy usually experiences a relatively high unemployment rate.[1] According to International Labour Organization report, more than 200 million people globally or 6% of the world's workforce were without a job in 2012.[2]		There remains considerable theoretical debate regarding the causes, consequences and solutions for unemployment. Classical economics, new classical economics, and the Austrian School of economics argue that market mechanisms are reliable means of resolving unemployment. These theories argue against interventions imposed on the labor market from the outside such, as unionization, bureaucratic work rules, minimum wage laws, taxes, and other regulations that they claim discourage the hiring of workers.		Keynesian economics emphasizes the cyclical nature of unemployment and recommends government interventions in the economy that it claims will reduce unemployment during recessions. This theory focuses on recurrent shocks that suddenly reduce aggregate demand for goods and services and thus reduce demand for workers. Keynesian models recommend government interventions designed to increase demand for workers; these can include financial stimuli, publicly funded job creation, and expansionist monetary policies. Its namesake economist John Maynard Keynes, believed that the root cause of unemployment is the desire of investors to receive more money rather than produce more products, which is not possible without public bodies producing new money.[3]		In addition to these comprehensive theories of unemployment, there are a few categorizations of unemployment that are used to more precisely model the effects of unemployment within the economic system. The main types of unemployment include structural unemployment which focuses on structural problems in the economy and inefficiencies inherent in labour markets, including a mismatch between the supply and demand of laborers with necessary skill sets. Structural arguments emphasize causes and solutions related to disruptive technologies and globalization. Discussions of frictional unemployment focus on voluntary decisions to work based on each individuals' valuation of their own work and how that compares to current wage rates plus the time and effort required to find a job. Causes and solutions for frictional unemployment often address job entry threshold and wage rates. Behavioral economists highlight individual biases in decision making, and often involve problems and solutions concerning sticky wages and efficiency wages.		For centuries, experts have predicted that machines would make workers obsolete and increase unemployment.[4][5]						The state of being without any work both for an educated & uneducated person, for earning one's livelihood is meant by unemployment. Economists distinguish between various overlapping types of and theories of unemployment, including cyclical or Keynesian unemployment, frictional unemployment, structural unemployment and classical unemployment. Some additional types of unemployment that are occasionally mentioned are seasonal unemployment, hardcore unemployment, and hidden unemployment.		Though there have been several definitions of "voluntary" and "involuntary unemployment" in the economics literature, a simple distinction is often applied. Voluntary unemployment is attributed to the individual's decisions, whereas involuntary unemployment exists because of the socio-economic environment (including the market structure, government intervention, and the level of aggregate demand) in which individuals operate. In these terms, much or most of frictional unemployment is voluntary, since it reflects individual search behavior. Voluntary unemployment includes workers who reject low wage jobs whereas involuntary unemployment includes workers fired due to an economic crisis, industrial decline, company bankruptcy, or organizational restructuring.		On the other hand, cyclical unemployment, structural unemployment, and classical unemployment are largely involuntary in nature. However, the existence of structural unemployment may reflect choices made by the unemployed in the past, while classical (natural) unemployment may result from the legislative and economic choices made by labour unions or political parties. So, in practice, the distinction between voluntary and involuntary unemployment is hard to draw.		The clearest cases of involuntary unemployment are those where there are fewer job vacancies than unemployed workers even when wages are allowed to adjust, so that even if all vacancies were to be filled, some unemployed workers would still remain. This happens with cyclical unemployment, as macroeconomic forces cause microeconomic unemployment which can boomerang back and exacerbate these macroeconomic forces.		Classical, or real-wage unemployment, occurs when real wages for a job are set above the market-clearing level causing the number of job-seekers to exceed the number of vacancies. On the other hand, some economists argue that as wages fall below a livable wage many choose to drop out of the labor market and no longer seek employment. This is especially true in countries where low-income families are supported through public welfare systems. In such cases, wages would have to be high enough to motivate people to choose employment over what they receive through public welfare. Wages below a livable wage are likely to result in lower labor market participation in above stated scenario. In addition it must be noted that consumption of goods and services is the primary driver of increased need for labor. Higher wages leads to workers having more income available to consume goods and services. Therefore, higher wages increase general consumption and as a result need for labor increases and unemployment decreases in the economy.		Many economists have argued that unemployment increases with increased governmental regulation. For example, minimum wage laws raise the cost of some low-skill laborers above market equilibrium, resulting in increased unemployment as people who wish to work at the going rate cannot (as the new and higher enforced wage is now greater than the value of their labor).[6][7] Laws restricting layoffs may make businesses less likely to hire in the first place, as hiring becomes more risky.[7]		However, this argument overly simplifies the relationship between wage rates and unemployment, ignoring numerous factors, which contribute to unemployment.[8][9][10][11][12] Some, such as Murray Rothbard, suggest that even social taboos can prevent wages from falling to the market-clearing level.[13]		In Out of Work: Unemployment and Government in the Twentieth-Century America, economists Richard Vedder and Lowell Gallaway argue that the empirical record of wages rates, productivity, and unemployment in American validates classical unemployment theory. Their data shows a strong correlation between adjusted real wage and unemployment in the United States from 1900 to 1990. However, they maintain that their data does not take into account exogenous events.[14]		Cyclical, deficient-demand, or Keynesian unemployment, occurs when there is not enough aggregate demand in the economy to provide jobs for everyone who wants to work. Demand for most goods and services falls, less production is needed and consequently fewer workers are needed, wages are sticky and do not fall to meet the equilibrium level, and mass unemployment results.[15] Its name is derived from the frequent shifts in the business cycle although unemployment can also be persistent as occurred during the Great Depression of the 1930s.		With cyclical unemployment, the number of unemployed workers exceeds the number of job vacancies, so that even if full employment were attained and all open jobs were filled, some workers would still remain unemployed. Some associate cyclical unemployment with frictional unemployment because the factors that cause the friction are partially caused by cyclical variables. For example, a surprise decrease in the money supply may shock rational economic factors and suddenly inhibit aggregate demand.		Keynesian economists on the other hand see the lack of supply for jobs as potentially resolvable by government intervention. One suggested interventions involves deficit spending to boost employment and demand. Another intervention involves an expansionary monetary policy that increases the supply of money which should reduce interest rates which should lead to an increase in non-governmental spending.[16]		It is in the very nature of the capitalist mode of production to overwork some workers while keeping the rest as a reserve army of unemployed paupers.		Marxists share the Keynesian viewpoint of the relationship between economic demand and employment, but with the caveat that the market system's propensity to slash wages and reduce labor participation on an enterprise level causes a requisite decrease in aggregate demand in the economy as a whole, causing crises of unemployment and periods of low economic activity before the capital accumulation (investment) phase of economic growth can continue.		According to Karl Marx, unemployment is inherent within the unstable capitalist system and periodic crises of mass unemployment are to be expected. The function of the proletariat within the capitalist system is to provide a "reserve army of labour" that creates downward pressure on wages. This is accomplished by dividing the proletariat into surplus labour (employees) and under-employment (unemployed).[18] This reserve army of labour fight among themselves for scarce jobs at lower and lower wages.		At first glance, unemployment seems inefficient since unemployed workers do not increase profits, but unemployment is profitable within the global capitalist system because unemployment lowers wages which are costs from the perspective of the owners. From this perspective low wages benefit the system by reducing economic rents. Yet, it does not benefit workers. Capitalist systems unfairly manipulate the market for labour by perpetuating unemployment which lowers laborers' demands for fair wages. Workers are pitted against one another at the service of increasing profits for owners.		According to Marx, the only way to permanently eliminate unemployment would be to abolish capitalism and the system of forced competition for wages and then shift to a socialist or communist economic system. For contemporary Marxists, the existence of persistent unemployment is proof of the inability of capitalism to ensure full employment.[19]		In demand-based theory, it is possible to abolish cyclical unemployment by increasing the aggregate demand for products and workers. However, eventually the economy hits an "inflation barrier" imposed by the four other kinds of unemployment to the extent that they exist. Historical experience suggests that low unemployment affects inflation in the short term but not the long term.[20] In the long term, the velocity of money supply measures such as the MZM ("money zero maturity", representing cash and equivalent demand deposits) velocity is far more predictive of inflation than low unemployment.[21][22]		Some demand theory economists see the inflation barrier as corresponding to the natural rate of unemployment. The "natural" rate of unemployment is defined as the rate of unemployment that exists when the labour market is in equilibrium and there is pressure for neither rising inflation rates nor falling inflation rates. An alternative technical term for this rate is the NAIRU, or the Non-Accelerating Inflation Rate of Unemployment. No matter what its name, demand theory holds that this means that if the unemployment rate gets "too low," inflation will accelerate in the absence of wage and price controls (incomes policies).		One of the major problems with the NAIRU theory is that no one knows exactly what the NAIRU is (while it clearly changes over time).[20] The margin of error can be quite high relative to the actual unemployment rate, making it hard to use the NAIRU in policy-making.[21]		Another, normative, definition of full employment might be called the ideal unemployment rate. It would exclude all types of unemployment that represent forms of inefficiency. This type of "full employment" unemployment would correspond to only frictional unemployment (excluding that part encouraging the McJobs management strategy) and would thus be very low. However, it would be impossible to attain this full-employment target using only demand-side Keynesian stimulus without getting below the NAIRU and causing accelerating inflation (absent incomes policies). Training programs aimed at fighting structural unemployment would help here.		To the extent that hidden unemployment exists, it implies that official unemployment statistics provide a poor guide to what unemployment rate coincides with "full employment".[20]		Structural unemployment occurs when a labour market is unable to provide jobs for everyone who wants one because there is a mismatch between the skills of the unemployed workers and the skills needed for the available jobs. Structural unemployment is hard to separate empirically from frictional unemployment, except to say that it lasts longer. As with frictional unemployment, simple demand-side stimulus will not work to easily abolish this type of unemployment.		Structural unemployment may also be encouraged to rise by persistent cyclical unemployment: if an economy suffers from long-lasting low aggregate demand, it means that many of the unemployed become disheartened, while their skills (including job-searching skills) become "rusty" and obsolete. Problems with debt may lead to homelessness and a fall into the vicious circle of poverty.		This means that they may not fit the job vacancies that are created when the economy recovers. The implication is that sustained high demand may lower structural unemployment. This theory of persistence in structural unemployment has been referred to as an example of path dependence or "hysteresis".		Much technological unemployment,[23] due to the replacement of workers by machines, might be counted as structural unemployment. Alternatively, technological unemployment might refer to the way in which steady increases in labour productivity mean that fewer workers are needed to produce the same level of output every year. The fact that aggregate demand can be raised to deal with this problem suggests that this problem is instead one of cyclical unemployment. As indicated by Okun's Law, the demand side must grow sufficiently quickly to absorb not only the growing labour force but also the workers made redundant by increased labour productivity.		Seasonal unemployment may be seen as a kind of structural unemployment, since it is a type of unemployment that is linked to certain kinds of jobs (construction work, migratory farm work). The most-cited official unemployment measures erase this kind of unemployment from the statistics using "seasonal adjustment" techniques. This results in substantial, permanent structural unemployment.		Frictional unemployment is the time period between jobs when a worker is searching for, or transitioning from one job to another. It is sometimes called search unemployment and can be voluntary based on the circumstances of the unemployed individual.		Frictional unemployment exists because both jobs and workers are heterogeneous, and a mismatch can result between the characteristics of supply and demand. Such a mismatch can be related to skills, payment, work-time, location, seasonal industries, attitude, taste, and a multitude of other factors. New entrants (such as graduating students) and re-entrants (such as former homemakers) can also suffer a spell of frictional unemployment.		Workers as well as employers accept a certain level of imperfection, risk or compromise, but usually not right away; they will invest some time and effort to find a better match. This is in fact beneficial to the economy since it results in a better allocation of resources. However, if the search takes too long and mismatches are too frequent, the economy suffers, since some work will not get done. Therefore, governments will seek ways to reduce unnecessary frictional unemployment through multiple means including providing education, advice, training, and assistance such as daycare centers.		The frictions in the labour market are sometimes illustrated graphically with a Beveridge curve, a downward-sloping, convex curve that shows a correlation between the unemployment rate on one axis and the vacancy rate on the other. Changes in the supply of or demand for labour cause movements along this curve. An increase (decrease) in labour market frictions will shift the curve outwards (inwards).		Hidden, or covered, unemployment is the unemployment of potential workers that is not reflected in official unemployment statistics, due to the way the statistics are collected. In many countries only those who have no work but are actively looking for work (and/or qualifying for social security benefits) are counted as unemployed. Those who have given up looking for work (and sometimes those who are on Government "retraining" programs) are not officially counted among the unemployed, even though they are not employed.		The statistic also does not count the "underemployed"–those working fewer hours than they would prefer or in a job that doesn't make good use of their capabilities. In addition, those who are of working age but are currently in full-time education are usually not considered unemployed in government statistics. Traditional unemployed native societies who survive by gathering, hunting, herding, and farming in wilderness areas, may or may not be counted in unemployment statistics. Official statistics often underestimate unemployment rates because of hidden unemployment.		This is defined in European Union statistics, as unemployment lasting for longer than one year. The United States Bureau of Labor Statistics (BLS), which reports current long-term unemployment rate at 1.9 percent, defines this as unemployment lasting 27 weeks or longer. Long-term unemployment is a component of structural unemployment, which results in long-term unemployment existing in every social group, industry, occupation, and all levels of education.[24]		There are also different ways national statistical agencies measure unemployment. These differences may limit the validity of international comparisons of unemployment data.[25] To some degree these differences remain despite national statistical agencies increasingly adopting the definition of unemployment by the International Labour Organization.[26] To facilitate international comparisons, some organizations, such as the OECD, Eurostat, and International Labor Comparisons Program, adjust data on unemployment for comparability across countries.		Though many people care about the number of unemployed individuals, economists typically focus on the unemployment rate. This corrects for the normal increase in the number of people employed due to increases in population and increases in the labour force relative to the population. The unemployment rate is expressed as a percentage, and is calculated as follows:		As defined by the International Labour Organization, "unemployed workers" are those who are currently not working but are willing and able to work for pay, currently available to work, and have actively searched for work.[27] Individuals who are actively seeking job placement must make the effort to: be in contact with an employer, have job interviews, contact job placement agencies, send out resumes, submit applications, respond to advertisements, or some other means of active job searching within the prior four weeks. Simply looking at advertisements and not responding will not count as actively seeking job placement. Since not all unemployment may be "open" and counted by government agencies, official statistics on unemployment may not be accurate.[28] In the United States, for example, the unemployment rate does not take into consideration those individuals who are not actively looking for employment, such as those still attending college.[29]		The ILO describes 4 different methods to calculate the unemployment rate:[30]		The primary measure of unemployment, U3, allows for comparisons between countries. Unemployment differs from country to country and across different time periods. For example, during the 1990s and 2000s, the United States had lower unemployment levels than many countries in the European Union,[31] which had significant internal variation, with countries like the UK and Denmark outperforming Italy and France. However, large economic events such as the Great Depression can lead to similar unemployment rates across the globe.		Causes of unemployment		(i) Caste System: In many cases, the work is not given to the deserving candidates but given to the person belonging to a particular community. So this gives rise to unemployment.		(ii) Slow Economic Growth: Indian economy is underdeveloped and role of economic growth is very slow. This slow growth fails to provide enough unemployment opportunities to the increasing population.		(iii) Increase in Population: Constant increase in population has been a big problem in India. It is one of the main causes of unemployment. The rate of unemployment is 11.1% in 10th Plan. (iv) Agriculture is a Seasonal Occupation:		Agriculture is underdeveloped in India. It provides seasonal employment. Large part of population is dependent on agriculture. But agriculture being seasonal provides work for a few months. So this gives rise to unemployment. (v) Joint Family System:		In big families having big business, many such persons will be available who do not do any work and depend on the joint income of the family.		Many of them seem to be working but they do not add anything to production. So they encourage disguised unemployment. (vi) Fall of Cottage and Small industries:		The industrial development had adverse effect on cottage and small industries. The production of cottage industries began to fall and many artisans became unemployed.		Eurostat, the statistical office of the European Union, defines unemployed as those persons age 15 to 74 who are not working, have looked for work in the last four weeks, and ready to start work within two weeks, which conform to ILO standards. Both the actual count and rate of unemployment are reported. Statistical data are available by member state, for the European Union as a whole (EU28) as well as for the euro area (EA19). Eurostat also includes a long-term unemployment rate. This is defined as part of the unemployed who have been unemployed for an excess of 1 year.[32]		The main source used is the European Union Labour Force Survey (EU-LFS). The EU-LFS collects data on all member states each quarter. For monthly calculations, national surveys or national registers from employment offices are used in conjunction with quarterly EU-LFS data. The exact calculation for individual countries, resulting in harmonized monthly data, depend on the availability of the data.[33]		The Bureau of Labor Statistics measures employment and unemployment (of those over 15 years of age) using two different labor force surveys[35] conducted by the United States Census Bureau (within the United States Department of Commerce) and/or the Bureau of Labor Statistics (within the United States Department of Labor) that gather employment statistics monthly. The Current Population Survey (CPS), or "Household Survey", conducts a survey based on a sample of 60,000 households. This Survey measures the unemployment rate based on the ILO definition.[36]		The Current Employment Statistics survey (CES), or "Payroll Survey", conducts a survey based on a sample of 160,000 businesses and government agencies that represent 400,000 individual employers.[37] This survey measures only civilian nonagricultural employment; thus, it does not calculate an unemployment rate, and it differs from the ILO unemployment rate definition. These two sources have different classification criteria, and usually produce differing results. Additional data are also available from the government, such as the unemployment insurance weekly claims report available from the Office of Workforce Security, within the U.S. Department of Labor Employment & Training Administration.[38] The Bureau of Labor Statistics provides up-to-date numbers via a PDF linked here.[39] The BLS also provides a readable concise current Employment Situation Summary, updated monthly.[40]		The Bureau of Labor Statistics also calculates six alternate measures of unemployment, U1 through U6, that measure different aspects of unemployment:[41]		Note: "Marginally attached workers" are added to the total labour force for unemployment rate calculation for U4, U5, and U6. The BLS revised the CPS in 1994 and among the changes the measure representing the official unemployment rate was renamed U3 instead of U5.[44] In 2013, Representative Hunter proposed that the Bureau of Labor Statistics use the U5 rate instead of the current U3 rate.[45]		Statistics for the U.S. economy as a whole hide variations among groups. For example, in January 2008 U.S. unemployment rates were 4.4% for adult men, 4.2% for adult women, 4.4% for Caucasians, 6.3% for Hispanics or Latinos (all races), 9.2% for African Americans, 3.2% for Asian Americans, and 18.0% for teenagers.[37] Also, the U.S. unemployment rate would be at least 2% higher if prisoners and jail inmates were counted.[46][47]		The unemployment rate is included in a number of major economic indexes including the United States' Conference Board's Index of Leading Indicators a macroeconomic measure of the state of the economy.		Some critics believe that current methods of measuring unemployment are inaccurate in terms of the impact of unemployment on people as these methods do not take into account the 1.5% of the available working population incarcerated in U.S. prisons (who may or may not be working while incarcerated), those who have lost their jobs and have become discouraged over time from actively looking for work, those who are self-employed or wish to become self-employed, such as tradesmen or building contractors or IT consultants, those who have retired before the official retirement age but would still like to work (involuntary early retirees), those on disability pensions who, while not possessing full health, still wish to work in occupations suitable for their medical conditions, those who work for payment for as little as one hour per week but would like to work full-time.[53]		These people are "involuntary part-time" workers, those who are underemployed, e.g., a computer programmer who is working in a retail store until he can find a permanent job, involuntary stay-at-home mothers who would prefer to work, and graduate and Professional school students who were unable to find worthwhile jobs after they graduated with their bachelor's degrees.		Internationally, some nations' unemployment rates are sometimes muted or appear less severe due to the number of self-employed individuals working in agriculture.[48] Small independent farmers are often considered self-employed; so, they cannot be unemployed. The impact of this is that in non-industrialized economies, such as the United States and Europe during the early 19th century, overall unemployment was approximately 3% because so many individuals were self-employed, independent farmers; yet, unemployment outside of agriculture was as high as 80%.[48]		Many economies industrialize and experience increasing numbers of non-agricultural workers. For example, the United States' non-agricultural labour force increased from 20% in 1800, to 50% in 1850, to 97% in 2000.[48] The shift away from self-employment increases the percentage of the population who are included in unemployment rates. When comparing unemployment rates between countries or time periods, it is best to consider differences in their levels of industrialization and self-employment.		Additionally, the measures of employment and unemployment may be "too high". In some countries, the availability of unemployment benefits can inflate statistics since they give an incentive to register as unemployed. People who do not seek work may choose to declare themselves unemployed so as to get benefits; people with undeclared paid occupations may try to get unemployment benefits in addition to the money they earn from their work.[54]		However, in countries such as the United States, Canada, Mexico, Australia, Japan and the European Union, unemployment is measured using a sample survey (akin to a Gallup poll).[26] According to the BLS, a number of Eastern European nations have instituted labour force surveys as well. The sample survey has its own problems because the total number of workers in the economy is calculated based on a sample rather than a census.		It is possible to be neither employed nor unemployed by ILO definitions, i.e., to be outside of the "labour force".[28] These are people who have no job and are not looking for one. Many of these people are going to school or are retired. Family responsibilities keep others out of the labour force. Still others have a physical or mental disability which prevents them from participating in labour force activities. And of course, some people simply elect not to work preferring to be dependent on others for sustenance.		Typically, employment and the labour force include only work done for monetary gain. Hence, a homemaker is neither part of the labour force nor unemployed. Nor are full-time students nor prisoners considered to be part of the labour force or unemployment.[53] The latter can be important. In 1999, economists Lawrence F. Katz and Alan B. Krueger estimated that increased incarceration lowered measured unemployment in the United States by 0.17% between 1985 and the late 1990s.[53]		In particular, as of 2005, roughly 0.7% of the U.S. population is incarcerated (1.5% of the available working population). Additionally, children, the elderly, and some individuals with disabilities are typically not counted as part of the labour force in and are correspondingly not included in the unemployment statistics. However, some elderly and many disabled individuals are active in the labour market		In the early stages of an economic boom, unemployment often rises.[15] This is because people join the labour market (give up studying, start a job hunt, etc.) as a result of the improving job market, but until they have actually found a position they are counted as unemployed. Similarly, during a recession, the increase in the unemployment rate is moderated by people leaving the labour force or being otherwise discounted from the labour force, such as with the self-employed.		For the fourth quarter of 2004, according to OECD, (source Employment Outlook 2005 ISBN 92-64-01045-9), normalized unemployment for men aged 25 to 54 was 4.6% in the U.S. and 7.4% in France. At the same time and for the same population the employment rate (number of workers divided by population) was 86.3% in the U.S. and 86.7% in France. This example shows that the unemployment rate is 60% higher in France than in the U.S., yet more people in this demographic are working in France than in the U.S., which is counterintuitive if it is expected that the unemployment rate reflects the health of the labour market.[55][56]		Due to these deficiencies, many labour market economists prefer to look at a range of economic statistics such as labour market participation rate, the percentage of people aged between 15 and 64 who are currently employed or searching for employment, the total number of full-time jobs in an economy, the number of people seeking work as a raw number and not a percentage, and the total number of person-hours worked in a month compared to the total number of person-hours people would like to work. In particular the NBER does not use the unemployment rate but prefer various employment rates to date recessions.[57]		The labor force participation rate is the ratio between the labor force and the overall size of their cohort (national population of the same age range). In the West during the later half of the 20th century, the labor force participation rate increased significantly, due to an increase in the number of women who entered the workplace.		In the United States, there have been four significant stages of women's participation in the labor force—increases in the 20th century and decreases in the 21st century. Male labor force participation decreased from 1953 until 2013. Since October 2013 men have been increasingly joining the labor force.		During the late 19th century through the 1920s, very few women worked outside the home. They were young single women who typically withdrew from the labor force at marriage unless family needed two incomes. These women worked primarily in the textile manufacturing industry or as domestic workers. This profession empowered women and allowed them to earn a living wage. At times, they were a financial help to their families.		Between 1930 and 1950, female labor force participation increased primarily due to the increased demand for office workers, women's participation in the high school movement, and due to electrification which reduced the time spent on household chores. Between the 1950s to the early 1970s, most women were secondary earners working mainly as secretaries, teachers, nurses, and librarians (pink-collar jobs).		Between the mid 1970s to the late 1990s there was a period of revolution of women in the labor force brought on by a source of different factors. Women more accurately planned for their future in the work force, investing in more applicable majors in college that prepared them to enter and compete in the labor market. In the United States, the female labor force participation rate rose from approximately 33% in 1948 to a peak of 60.3% in 2000. As of April 2015 the female labor force participation is at 56.6%, the male labor force participation rate is at 69.4% and the total is 62.8%.[58]		A common theory in modern economics claims that the rise of women participating in the U.S. labor force in the 1950s through to the 1990s was due to the introduction of a new contraceptive technology, birth control pills, and the adjustment of age of majority laws. The use of birth control gave women the flexibility of opting to invest and advance their career while maintaining a relationship. By having control over the timing of their fertility, they were not running a risk of thwarting their career choices. However, only 40% of the population actually used the birth control pill.		This implies that other factors may have contributed to women choosing to invest in advancing their careers. One factor may be that more and more men delayed the age of marriage, allowing women to marry later in life without worrying about the quality of older men. Other factors include the changing nature of work, with machines replacing physical labor, eliminating many traditional male occupations, and the rise of the service sector, where many jobs are gender neutral.		Another factor that may have contributed to the trend was The Equal Pay Act of 1963, which aimed at abolishing wage disparity based on sex. Such legislation diminished sexual discrimination and encouraged more women to enter the labor market by receiving fair remuneration to help raising families and children.		At the turn of the 21st century the labor force participation began to reverse its long period of increase. Reasons for this change include a rising share of older workers, an increase in school enrollment rates among young workers and a decrease in female labor force participation.[59]		The labor force participation rate can decrease when the rate of growth of the population outweighs that of the employed and unemployed together. The labor force participation rate is a key component in long-term economic growth, almost as important as productivity.		A historic shift began around the end of the great recession as women began leaving the labor force in the United States and other developed countries.[60] The female labor force participation rate in the United States has steadily decreased since 2009 and as of April 2015 the female labor force participation rate has gone back down to 1988 levels of 56.6%.[58]		Participation rates are defined as follows:		The labor force participation rate explains how an increase in the unemployment rate can occur simultaneously with an increase in employment. If a large amount of new workers enter the labor force but only a small fraction become employed, then the increase in the number of unemployed workers can outpace the growth in employment.[61]		The unemployment ratio calculates the share of unemployed for the whole population. Particularly many young people between 15 and 24 are studying full-time and are therefore neither working nor looking for a job. This means they are not part of the labour force which is used as the denominator for calculating the unemployment rate.[62] The youth unemployment ratios in the European Union range from 5.2 (Austria) to 20.6 percent (Spain). These are considerably lower than the standard youth unemployment rates, ranging from 7.9 (Germany) to 57.9 percent (Greece).[63]		High and persistent unemployment, in which economic inequality increases, has a negative effect on subsequent long-run economic growth. Unemployment can harm growth not only because it is a waste of resources, but also because it generates redistributive pressures and subsequent distortions, drives people to poverty, constrains liquidity limiting labor mobility, and erodes self-esteem promoting social dislocation, unrest and conflict.[64] 2013 Economics Nobel prize winner Robert J. Shiller said that rising inequality in the United States and elsewhere is the most important problem.[65]		Unemployed individuals are unable to earn money to meet financial obligations. Failure to pay mortgage payments or to pay rent may lead to homelessness through foreclosure or eviction.[66] Across the United States the growing ranks of people made homeless in the foreclosure crisis are generating tent cities.[67]		Unemployment increases susceptibility to cardiovascular disease, somatization, anxiety disorders, depression, and suicide. In addition, unemployed people have higher rates of medication use, poor diet, physician visits, tobacco smoking, alcoholic beverage consumption, drug use, and lower rates of exercise.[68] According to a study published in Social Indicator Research, even those who tend to be optimistic find it difficult to look on the bright side of things when unemployed. Using interviews and data from German participants aged 16 to 94—including individuals coping with the stresses of real life and not just a volunteering student population—the researchers determined that even optimists struggled with being unemployed.[69]		In 1979, Brenner found that for every 10% increase in the number of unemployed there is an increase of 1.2% in total mortality, a 1.7% increase in cardiovascular disease, 1.3% more cirrhosis cases, 1.7% more suicides, 4.0% more arrests, and 0.8% more assaults reported to the police.[70][71]		A study by Ruhm, in 2000, on the effect of recessions on health found that several measures of health actually improve during recessions.[72] As for the impact of an economic downturn on crime, during the Great Depression the crime rate did not decrease. The unemployed in the U.S. often use welfare programs such as Food Stamps or accumulating debt because unemployment insurance in the U.S. generally does not replace a majority of the income one received on the job (and one cannot receive such aid indefinitely).		Not everyone suffers equally from unemployment. In a prospective study of 9570 individuals over four years, highly conscientious people suffered more than twice as much if they became unemployed.[73] The authors suggested this may be due to conscientious people making different attributions about why they became unemployed, or through experiencing stronger reactions following failure. There is also possibility of reverse causality from poor health to unemployment.[74]		Some researchers hold that many of the low-income jobs are not really a better option than unemployment with a welfare state (with its unemployment insurance benefits). But since it is difficult or impossible to get unemployment insurance benefits without having worked in the past, these jobs and unemployment are more complementary than they are substitutes. (These jobs are often held short-term, either by students or by those trying to gain experience; turnover in most low-paying jobs is high.)		Another cost for the unemployed is that the combination of unemployment, lack of financial resources, and social responsibilities may push unemployed workers to take jobs that do not fit their skills or allow them to use their talents. Unemployment can cause underemployment, and fear of job loss can spur psychological anxiety. As well as anxiety, it can cause depression, lack of confidence, and huge amounts of stress. This stress is increased when the unemployed are faced with health issues, poverty, and lack of relational support.[75]		Another personal cost of unemployment is its impact on relationships. A 2008 study from Covizzi, which examines the relationship between unemployment and divorce, found that the rate of divorce is greater for couples when one partner is unemployed.[76] However, a more recent study has found that some couples often stick together in “unhappy” or “unhealthy” marriages when unemployed to buffer financial costs.[77] A 2014 study by Van der Meer found that the stigma that comes from being unemployed affects personal well-being, especially for men, who often feel as though their masculine identities are threatened by unemployment.[78]		Unemployment can also bring personal costs in relation to gender. One study found that women are more likely to experience unemployment than men and that they are less likely to move from temporary positions to permanent positions.[79] Another study on gender and unemployment found that men, however, are more likely to experience greater stress, depression, and adverse effects from unemployment, largely stemming from the perceived threat to their role as breadwinner.[80] This study found that men expect themselves to be viewed as “less manly” after a job loss than they actually are, and as a result they engage in compensating behaviors, such as financial risk-taking and increased assertiveness, because of it.		Costs of unemployment also vary depending on age. The young and the old are the two largest age groups currently experiencing unemployment.[81] A 2007 study from Jacob and Kleinert found that young people (ages 18 to 24) who have fewer resources and limited work experiences are more likely to be unemployed.[82] Other researchers have found that today’s high school seniors place a lower value on work than those in the past, and this is likely because they recognize the limited availability of jobs.[83] At the other end of the age spectrum, studies have found that older individuals have more barriers than younger workers to employment, require stronger social networks to acquire work, and are also less likely to move from temporary to permanent positions.[79][79][81] Additionally, some older people see age discrimination as the reason they are not getting hired.[84]		An economy with high unemployment is not using all of the resources, specifically labour, available to it. Since it is operating below its production possibility frontier, it could have higher output if all the workforce were usefully employed. However, there is a trade-off between economic efficiency and unemployment: if the frictionally unemployed accepted the first job they were offered, they would be likely to be operating at below their skill level, reducing the economy's efficiency.[85]		During a long period of unemployment, workers can lose their skills, causing a loss of human capital. Being unemployed can also reduce the life expectancy of workers by about seven years.[7]		High unemployment can encourage xenophobia and protectionism as workers fear that foreigners are stealing their jobs.[86] Efforts to preserve existing jobs of domestic and native workers include legal barriers against "outsiders" who want jobs, obstacles to immigration, and/or tariffs and similar trade barriers against foreign competitors.		High unemployment can also cause social problems such as crime; if people have less disposable income than before, it is very likely that crime levels within the economy will increase.		A 2015 study published in The Lancet estimates that unemployment causes 45,000 suicides a year globally.[87]		High levels of unemployment can be causes of civil unrest,[88] in some cases leading to revolution, and particularly totalitarianism. The fall of the Weimar Republic in 1933 and Adolf Hitler's rise to power, which culminated in World War II and the deaths of tens of millions and the destruction of much of the physical capital of Europe, is attributed to the poor economic conditions in Germany at the time, notably a high unemployment rate[89] of above 20%; see Great Depression in Central Europe for details.		Note that the hyperinflation in the Weimar Republic is not directly blamed for the Nazi rise—the Hyperinflation in the Weimar Republic occurred primarily in the period 1921–23, which was contemporary with Hitler's Beer Hall Putsch of 1923, and is blamed for damaging the credibility of democratic institutions, but the Nazis did not assume government until 1933, ten years after the hyperinflation but in the midst of high unemployment.		Rising unemployment has traditionally been regarded by the public and media in any country as a key guarantor of electoral defeat for any government which oversees it. This was very much the consensus in the United Kingdom until 1983, when Margaret Thatcher's Conservative government won a landslide in the general election, despite overseeing a rise in unemployment from 1,500,000 to 3,200,000 since its election four years earlier.[90]		The primary benefit of unemployment is that people are available for hire, without being headhunted away from their existing employers. This permits new and old businesses to take on staff.		Unemployment is argued to be "beneficial" to the people who are not unemployed in the sense that it averts inflation,[citation needed] which itself has damaging effects, by providing (in Marxian terms) a reserve army of labour, that keeps wages in check. However, the direct connection between full local employment and local inflation has been disputed by some due to the recent increase in international trade that supplies low-priced goods even while local employment rates rise to full employment.[91]		Full employment cannot be achieved because workers would shirk[citation needed] if they were not threatened with the possibility of unemployment. The curve for the no-shirking condition (labeled NSC) goes to infinity at full employment as a result. The inflation-fighting benefits to the entire economy arising from a presumed optimum level of unemployment has been studied extensively.[92] The Shapiro–Stiglitz model suggests that wages are not bid down sufficiently to ever reach 0% unemployment.[93] This occurs because employers know that when wages decrease, workers will shirk and expend less effort. Employers avoid shirking by preventing wages from decreasing so low that workers give up and become unproductive. These higher wages perpetuate unemployment while the threat of unemployment reduces shirking.		Before current levels of world trade were developed, unemployment was demonstrated to reduce inflation, following the Phillips curve, or to decelerate inflation, following the NAIRU/natural rate of unemployment theory, since it is relatively easy to seek a new job without losing one's current one. And when more jobs are available for fewer workers (lower unemployment), it may allow workers to find the jobs that better fit their tastes, talents, and needs.		As in the Marxian theory of unemployment, special interests may also benefit: some employers may expect that employees with no fear of losing their jobs will not work as hard, or will demand increased wages and benefit. According to this theory, unemployment may promote general labour productivity and profitability by increasing employers' rationale for their monopsony-like power (and profits).[17]		Optimal unemployment has also been defended as an environmental tool to brake the constantly accelerated growth of the GDP to maintain levels sustainable in the context of resource constraints and environmental impacts.[94] However the tool of denying jobs to willing workers seems a blunt instrument for conserving resources and the environment—it reduces the consumption of the unemployed across the board, and only in the short term. Full employment of the unemployed workforce, all focused toward the goal of developing more environmentally efficient methods for production and consumption might provide a more significant and lasting cumulative environmental benefit and reduced resource consumption.[95] If so the future economy and workforce would benefit from the resultant structural increases in the sustainable level of GDP growth.		Some critics of the "culture of work" such as anarchist Bob Black see employment as overemphasized culturally in modern countries. Such critics often propose quitting jobs when possible, working less, reassessing the cost of living to this end, creation of jobs which are "fun" as opposed to "work," and creating cultural norms where work is seen as unhealthy. These people advocate an "anti-work" ethic for life.[96]		As a result of productivity the work week declined considerably over the 19th century.[97][98] By the 1920s in the U.S. the average work week was 49 hours, but the work week was reduced to 40 hours (after which overtime premium was applied) as part of the National Industrial Recovery Act of 1933. At the time of the Great Depression of the 1930s it was believed that due to the enormous productivity gains due to electrification, mass production and agricultural mechanization, there was no need for a large number of previously employed workers.[23][99]		Societies try a number of different measures to get as many people as possible into work, and various societies have experienced close to full employment for extended periods, particularly during the Post-World War II economic expansion. The United Kingdom in the 1950s and 60s averaged 1.6% unemployment,[101] while in Australia the 1945 White Paper on Full Employment in Australia established a government policy of full employment, which policy lasted until the 1970s when the government ran out of money.		However, mainstream economic discussions of full employment since the 1970s suggest that attempts to reduce the level of unemployment below the natural rate of unemployment will fail, resulting only in less output and more inflation.		Increases in the demand for labour will move the economy along the demand curve, increasing wages and employment. The demand for labour in an economy is derived from the demand for goods and services. As such, if the demand for goods and services in the economy increases, the demand for labour will increase, increasing employment and wages.		There are many ways to stimulate demand for goods and services. Increasing wages to the working class (those more likely to spend the increased funds on goods and services, rather than various types of savings, or commodity purchases) is one theory proposed. Increased wages is believed to be more effective in boosting demand for goods and services than central banking strategies that put the increased money supply mostly into the hands of wealthy persons and institutions. Monetarists suggest that increasing money supply in general will increase short-term demand. Long-term the increased demand will be negated by inflation. A rise in fiscal expenditures is another strategy for boosting aggregate demand.		Providing aid to the unemployed is a strategy used to prevent cutbacks in consumption of goods and services which can lead to a vicious cycle of further job losses and further decreases in consumption/demand. Many countries aid the unemployed through social welfare programs. These unemployment benefits include unemployment insurance, unemployment compensation, welfare and subsidies to aid in retraining. The main goal of these programs is to alleviate short-term hardships and, more importantly, to allow workers more time to search for a job.		A direct demand-side solution to unemployment is government-funded employment of the able-bodied poor. This was notably implemented in Britain from the 17th century until 1948 in the institution of the workhouse, which provided jobs for the unemployed with harsh conditions and poor wages to dissuade their use. A modern alternative is a job guarantee, where the government guarantees work at a living wage.		Temporary measures can include public works programs such as the Works Progress Administration. Government-funded employment is not widely advocated as a solution to unemployment, except in times of crisis; this is attributed to the public sector jobs' existence depending directly on the tax receipts from private sector employment.		In the U.S., the unemployment insurance allowance one receives is based solely on previous income (not time worked, family size, etc.) and usually compensates for one-third of one's previous income. To qualify, one must reside in their respective state for at least a year and, of course, work. The system was established by the Social Security Act of 1935. Although 90% of citizens are covered by unemployment insurance, less than 40% apply for and receive benefits.[102] However, the number applying for and receiving benefits increases during recessions. In cases of highly seasonal industries the system provides income to workers during the off seasons, thus encouraging them to stay attached to the industry.		According to classical economic theory, markets reach equilibrium where supply equals demand; everyone who wants to sell at the market price can. Those who do not want to sell at this price do not; in the labour market this is classical unemployment. Monetary policy and fiscal policy can both be used to increase short-term growth in the economy, increasing the demand for labour and decreasing unemployment.		However, the labor market is not 100% efficient, although it may be more efficient than the bureaucracy. Some argue that minimum wages and union activity keep wages from falling, which means too many people want to sell their labour at the going price but cannot. This assumes perfect competition exists in the labour market, specifically that no single entity is large enough to affect wage levels and that employees are similar in ability.		Advocates of supply-side policies believe those policies can solve this by making the labour market more flexible. These include removing the minimum wage and reducing the power of unions. Supply-siders argue the reforms increase long-term growth by reducing labour costs. This increased supply of goods and services requires more workers, increasing employment. It is argued that supply-side policies, which include cutting taxes on businesses and reducing regulation, create jobs, reduce unemployment and decrease labour's share of national income. Other supply-side policies include education to make workers more attractive to employers.		There are relatively limited historical records on unemployment because it has not always been acknowledged or measured systematically. Industrialization involves economies of scale that often prevent individuals from having the capital to create their own jobs to be self-employed. An individual who cannot either join an enterprise or create a job is unemployed. As individual farmers, ranchers, spinners, doctors and merchants are organized into large enterprises, those who cannot join or compete become unemployed.		Recognition of unemployment occurred slowly as economies across the world industrialized and bureaucratized. Before this, traditional self sufficient native societies have no concept of unemployment. The recognition of the concept of "unemployment" is best exemplified through the well documented historical records in England. For example, in 16th century England no distinction was made between vagrants and the jobless; both were simply categorized as "sturdy beggars", to be punished and moved on.[104]		The closing of the monasteries in the 1530s increased poverty, as the church had helped the poor. In addition, there was a significant rise in enclosure during the Tudor period. Also the population was rising. Those unable to find work had a stark choice: starve or break the law. In 1535, a bill was drawn up calling for the creation of a system of public works to deal with the problem of unemployment, to be funded by a tax on income and capital. A law passed a year later allowed vagabonds to be whipped and hanged.[105]		In 1547, a bill was passed that subjected vagrants to some of the more extreme provisions of the criminal law, namely two years servitude and branding with a "V" as the penalty for the first offense and death for the second.[106] During the reign of Henry VIII, as many as 72,000 people are estimated to have been executed.[107] In the 1576 Act each town was required to provide work for the unemployed.[108]		The Elizabethan Poor Law of 1601, one of the world's first government-sponsored welfare programs, made a clear distinction between those who were unable to work and those able-bodied people who refused employment.[109] Under the Poor Law systems of England and Wales, Scotland and Ireland a workhouse was a place where people who were unable to support themselves, could go to live and work.[110]		Poverty was a highly visible problem in the eighteenth century, both in cities and in the countryside. In France and Britain by the end of the century, an estimated 10 percent of the people depended on charity or begging for their food.		By 1776 some 1,912 parish and corporation workhouses had been established in England and Wales, housing almost 100,000 paupers.		A description of the miserable living standards of the mill workers in England in 1844 was given by Fredrick Engels in The Condition of the Working-Class in England in 1844.[111] In the preface to the 1892 edition Engels notes that the extreme poverty he wrote about in 1844 had largely disappeared. David Ames Wells also noted that living conditions in England had improved near the end of the 19th century and that unemployment was low.		The scarcity and high price of labor in the U.S. during the 19th century was well documented by contemporary accounts, as in the following:		"The laboring classes are comparatively few in number, but this is counterbalanced by, and indeed, may be one of the causes of the eagerness by which they call in the use of machinery in almost every department of industry. Wherever it can be applied as a substitute for manual labor, it is universally and willingly resorted to ....It is this condition of the labor market, and this eager resort to machinery wherever it can be applied, to which, under the guidance of superior education and intelligence, the remarkable prosperity of the United States is due."[112] Joseph Whitworth, 1854		Scarcity of labor was a factor in the economics of slavery in the United States.		As new territories were opened and Federal land sales conducted, land had to be cleared and new homesteads established. Hundreds of thousands of immigrants annually came to the U.S. and found jobs digging canals and building railroads. Almost all work during most of the 19th century was done by hand or with horses, mules, or oxen, because there was very little mechanization. The workweek during most of the 19th century was 60 hours. Unemployment at times was between one and two percent.		The tight labor market was a factor in productivity gains allowing workers to maintain or increase their nominal wages during the secular deflation that caused real wages to rise at various times in the 19th century, especially in the final decades.[113]		There were labor shortages during WW I.[23] Ford Motor Co. doubled wages to reduce turnover. After 1925 unemployment began to gradually rise.[114]		The decade of the 1930s saw the Great Depression impact unemployment across the globe. One Soviet trading corporation in New York averaged 350 applications a day from Americans seeking jobs in the Soviet Union.[115] In Germany the unemployment rate reached nearly 25% in 1932.[116]		In some towns and cities in the north east of England, unemployment reached as high as 70%; the national unemployment level peaked at more than 22% in 1932.[117] Unemployment in Canada reached 27% at the depth of the Depression in 1933.[118] In 1929, the U.S. unemployment rate averaged 3%.[119]		In the U.S., the Works Progress Administration (1935–43) was the largest make-work program. It hired men (and some women) off the relief roles ("dole") typically for unskilled labor.[120]		In Cleveland, Ohio, the unemployment rate was 60%; in Toledo, Ohio, 80%.[121] There were two million homeless people migrating across the United States.[121] Over 3 million unemployed young men were taken out of the cities and placed into 2600+ work camps managed by the Civilian Conservation Corps.[122]		Unemployment in the United Kingdom fell later in the 1930s as the depression eased, and remained low (in six figures) after World War II.		Fredrick Mills found that in the U.S., 51% of the decline in work hours was due to the fall in production and 49% was from increased productivity.[123] By 1972 unemployment in the UK had crept back up above 1,000,000, and was even higher by the end of the decade, with inflation also being high. Although the monetarist economic policies of Margaret Thatcher's Conservative government saw inflation reduced after 1979, unemployment soared in the early 1980s, exceeding 3,000,000 – a level not seen for some 50 years – by 1982. This represented one in eight of the workforce, with unemployment exceeding 20% in some parts of the United Kingdom which had relied on the now-declining industries such as coal mining.[124]		However, this was a time of high unemployment in all major industrialised nations.[125] By the spring of 1983, unemployment in the United Kingdom had risen by 6% in the previous 12 months; compared to 10% in Japan, 23% in the United States of America and 34% in West Germany (seven years before reunification).[126]		Unemployment in the United Kingdom remained above 3,000,000 until the spring of 1987, by which time the economy was enjoying a boom.[124] By the end of 1989, unemployment had fallen to 1,600,000. However, inflation had reached 7.8% and the following year it reached a nine-year high of 9.5%; leading to increased interest rates.[127]		Another recession began during 1990 and lasted until 1992. Unemployment began to increase and by the end of 1992 nearly 3,000,000 in the United Kingdom were unemployed. Then came a strong economic recovery.[124] With inflation down to 1.6% by 1993, unemployment then began to fall rapidly, standing at 1,800,000 by early 1997.[128]		The official unemployment rate in the 16 EU countries that use the euro rose to 10% in December 2009 as a result of another recession.[129] Latvia had the highest unemployment rate in EU at 22.3% for November 2009.[130] Europe's young workers have been especially hard hit.[131] In November 2009, the unemployment rate in the EU27 for those aged 15–24 was 18.3%. For those under 25, the unemployment rate in Spain was 43.8%.[132] Unemployment has risen in two-thirds of European countries since 2010.[133]		Into the 21st century, unemployment in the United Kingdom remained low and the economy remaining strong, while at this time several other European economies – namely, France and Germany (reunified a decade earlier) – experienced a minor recession and a substantial rise in unemployment.[134]		In 2008, when the recession brought on another increase in the United Kingdom, after 15 years of economic growth and no major rises in unemployment.[135] Early in 2009, unemployment passed the 2,000,000 mark, by which time economists were predicting it would soon reach 3,000,000.[136] However, the end of the recession was declared in January 2010[137] and unemployment peaked at nearly 2,700,000 in 2011,[138] appearing to ease fears of unemployment reaching 3,000,000.[139] The unemployment rate of Britain's young black people was 47.4% in 2011.[140] 2013/2014 has seen the employment rate increase from 1,935,836 to 2,173,012 as supported by[141] showing the UK is creating more job opportunities and forecasts the rate of increase in 2014/2015 will be another 7.2%.[142]		An 26 April 2005 Asia Times article notes that, "In regional giant South Africa, some 300,000 textile workers have lost their jobs in the past two years due to the influx of Chinese goods".[143] The increasing U.S. trade deficit with China cost 2.4 million American jobs between 2001 and 2008, according to a study by the Economic Policy Institute (EPI).[144] From 2000 to 2007, the United States lost a total of 3.2 million manufacturing jobs.[145] 12.1% of US military veterans who had served after the September 11 attacks in 2001 were unemployed as of 2011; 29.1% of male veterans aged 18–24 were unemployed.[68]		About 25 million people in the world's 30 richest countries will have lost their jobs between the end of 2007 and the end of 2010 as the economic downturn pushes most countries into recession.[146] In April 2010, the U.S. unemployment rate was 9.9%, but the government's broader U-6 unemployment rate was 17.1%.[147] In April 2012, the unemployment rate was 4.6% in Japan.[148] In a 2012 news story, the Financial Post reported, "Nearly 75 million youth are unemployed around the world, an increase of more than 4 million since 2007. In the European Union, where a debt crisis followed the financial crisis, the youth unemployment rate rose to 18% last year from 12.5% in 2007, the ILO report shows."[149]		http://www.economicshelp.org/blog/2247/unemployment/definition-of-unemployment/ http://mawdoo3.com/%D8%AA%D8%B9%D8%B1%D9%8A%D9%81_%D8%A7%D9%84%D8%A8%D8%B7%D8%A7%D9%84%D8%A9 http://www.economicsdiscussion.net/articles/main-causes-of-unemployment-in-india/2281		
Workers' compensation is a form of insurance providing wage replacement and medical benefits to employees injured in the course of employment in exchange for mandatory relinquishment of the employee's right to sue their employer for the tort of negligence. The trade-off between assured, limited coverage and lack of recourse outside the worker compensation system is known as "the compensation bargain". One of the problems that the compensation bargain solved is the problem of employers becoming insolvent as a result of high damage awards. The system of collective liability was created to prevent that, and thus to ensure security of compensation to the workers. Individual immunity is the necessary corollary to collective liability.		While plans differ among jurisdictions, provision can be made for weekly payments in place of wages (functioning in this case as a form of disability insurance), compensation for economic loss (past and future), reimbursement or payment of medical and like expenses (functioning in this case as a form of health insurance), and benefits payable to the dependents of workers killed during employment (functioning in this case as a form of life insurance).		General damage for pain and suffering, and punitive damages for employer negligence, are generally not available in workers' compensation plans, and negligence is generally not an issue in the case. These laws were first enacted in Europe and Oceania, with the United States following shortly thereafter.						Common law imposes obligations on employers to: provide a safe workplace; provide safe tools; give warnings of dangers; provide adequate co-worker assistance (fit, trained, suitable "fellow servants") so worker is not overburdened; promulgate and enforce safe work rules.[1] K Claims under the common law for worker injury are limited by three defenses afforded employers:		Workers' compensation statutes are intended to eliminate the need for litigation (and the limitations of common law remedies) by having employees give up the potential for pain- and suffering-related awards in exchange for not being required to prove tort (legal fault) on the part of their employer. The laws are designed to ensure employees who are injured or disabled on the job are not required to cover medical bills related to their on-the-job injury, and are provided with monetary awards to cover loss of wages directly related to the accident, as well as to compensate for permanent physical impairments.		These laws also provide benefits for dependents of those workers who are killed in work-related accidents or illnesses. Some laws also protect employers and fellow workers by limiting the amount an injured employee can recover from an employer and by eliminating the liability of co-workers in most accidents. State statutes [in the United States] establish this framework for most employment. Federal statutes [in the United States] are limited to federal employees or those workers employed in some significant aspect of interstate commerce.[2]		As Australia experienced a relatively influential labour movement in the late 19th and early 20th century, statutory compensation was implemented very early in Australia. Each territory has its own legislation and its own governing body.		A typical example is Work Safe Victoria, which manages Victoria's workplace safety system. Its responsibilities include helping employees avoid workplace injuries occurring, enforcement of Victoria's occupational and safety laws, provision of reasonably priced workplace injury insurance for employers, assisting injured workers back into the workforce, and managing the workers' compensation scheme by ensuring the prompt delivery of appropriate services and adopting prudent financial practices.[3]		Compensation law in New South Wales has recently (2013) been overhauled by the state government. In a push to speed up the process of claims and to reduce the amount of claims, a threshold of 11% WPI (whole person impairment) was implemented.		Workers' compensation regulators for each of the states and territories are as follows:[4]		Every employer must comply with the state, territory or commonwealth legislation, as listed below, which applies to them:		The National Social Insurance Institute (in Portuguese, Instituto Nacional do Seguro Social – INSS) provides insurance for those who contribute. It is a public institution that aims to recognize and grant rights to its policyholders. The amount transferred by the INSS is used to replace the income of the worker taxpayer, when he or she loses the ability to work, due to sickness, disability, age, death, involuntary unemployment, or even pregnancy and imprisonment. During the first 15 days the worker's salary is paid by the employer and after that by the INSS, as long as the inability to work lasts. Although the worker's income is guaranteed by the INSS, the employer is still responsible for any loss of working capacity, temporary or permanent, when found negligent or when its economic activity involves risk of accidents or developing labour related diseases.		Workers' compensation was Canada's first social program to be introduced as it was favored by both workers' groups and employers hoping to avoid lawsuits. The system arose after an inquiry by Ontario Chief Justice William Meredith who outlined a system in which workers were to be compensated for workplace injuries, but must give up their right to sue their employers. It was introduced in the various provinces at different dates. Ontario was first in 1915, Manitoba in 1916, and British Columbia in 1917. It remains a provincial responsibility and thus the rules vary from province to province. In some provinces, such as Ontario's Workplace Safety and Insurance Board, the program also has a preventative role ensuring workplace safety. In British Columbia, the occupational health and safety mandate (including the powers to make regulation, inspect and assess administrative penalties) is legislatively assigned to the Workers' Compensation Board of British Columbia WorkSafeBC. In most provinces the workers' compensation board or commission remains concerned solely with insurance. The workers' compensation insurance system in every province is funded by employers based on their payroll, industry sector and history of injuries (or lack thereof) in their workplace (usually referred to as "experience rating").		The German worker's compensation law of 6 July 1884,[15] initiated by Chancellor Otto von Bismarck,[16][17] was passed only after three attempts and was the first of its kind in the world.[18] Similar laws passed in Austria in 1887, Norway in 1894, and Finland in 1895.[19]		The law paid indemnity to all private wage earners and apprentices, including those who work in the agricultural and horticultural sectors and marine industries, family helpers and students with work-related injuries, for up to 13 weeks. Workers who are totally disabled get continued benefits at 67 percent after 13 weeks, paid by the accident funds, financed entirely by employers.		The German compensation system has been taken as a model for many nations.		Main article : The Workmen's Compensation Act 1923		The Indian worker's compensation law 1923 was introduced on 5 March 1923. It includes Employer's liability compensation, amount of compensation.		Workers' accident compensation insurance is paired with unemployment insurance and referred to collectively as labour insurance.[20][21] Workers' accident compensation insurance is managed by the Labor Standards Office.[22]		The Workmen's Compensation Act 1952[23] is modelled on the United Kingdom's Workmen's Compensation Act 1906. Adopted before Malaysia's independence from the UK, it is now used only by non-Malaysian workers, since citizens are covered by the national social security scheme.		The Mexican Constitution of 1917 defined the obligation of employers to pay for illnesses or accidents related to the workplace. It also defined social security as the institution to administer the right of workers, but only until 1943 was the Mexican Social Security Institute created (IMSS). Since then, IMSS manages the Work Risks Insurance in a vertically integrated fashion: registration of workers and firms, collection, classification of risks and events, and medical and rehabilitation services. A reform in 1997 defined that contributions are related to the experience of each employer. Public sector workers are covered by social security agencies with corporate and operative structures similar to those of IMSS.		In New Zealand, all companies that employ staff and in some cases others, must pay a levy to the Accident Compensation Corporation, a Crown Entity, which administers New Zealand's universal no-fault accidental injury scheme. The scheme provides financial compensation and support to citizens, residents, and temporary visitors who have suffered personal injuries.		The United Kingdom was one of the first countries to have a workmen's insurance scheme, which operated from 1897 to 1946. This was created by the Workmen's Compensation Act 1897, expanded to include industrial diseases by the Workmen's Compensation Act 1906 and replaced by a state compensation scheme under the National Insurance (Industrial Injuries) Act 1946. Since 1976, this state scheme has been set out in the UK's Social Security Acts.		Work related safety issues in the UK are supervised by the Health & Safety Executive (HSE) who provide the framework by which employers and employees are able to comply with statutory rules and regulations.[24]		With the exception of the following, all employers are obliged to purchase compulsory Employers Liability Insurance in accordance with the Employers Liability (Compulsory Insurance) Act of 1969. The current minimum Limit of Indemnity required is £5,000,000 per occurrence. Market practice is to usually provide a minimum £10,000,000 with inner limits to £5,000,000 for certain risks e.g. workers on oil rigs and acts of terrorism.		These employers do not require Employers Liability Compulsory Insurance:		"Employees" are defined as anyone who has entered into or works under a contract of service or apprenticeship with an employer. The contract may be for manual labour, clerical work or otherwise, it may be written or verbal and it may be for full-time or part-time work.		These persons are not classed as employees and, therefore, are exempt:		Employees need to establish that their employer has a legal liability to pay compensation. This will principally be a breach of a statutory duty or under the tort of negligence. In the event that the employer is insolvent or no longer in existence Compensation can be sought directly from the insurer under the terms of the Third Party Rights Against Insurers Act of 1930.		History: see: Workmen's Compensation Act 1897 & following		In 1855, Georgia and Alabama passed Employer Liability Acts; 26 other states passed similar acts between 1855 and 1907. Early laws permitted injured employees to sue the employer and then prove a negligent act or omission.[26][27] (A similar scheme was set forth in Britain's 1880 Act.[28])		The first statewide worker's compensation law was passed in Maryland in 1902, and the first law covering federal employees was passed in 1906.[29] (See: FELA, 1908; FECA, 1916; Kern, 1918.) By 1949, every state had enacted a workers' compensation program.[30]		At the turn of the 20th century workers' compensation laws were voluntary.[citation needed] An elective law made passage easier and some argued that compulsory workers' compensation laws would violate the 14th amendment due process clause of the U.S. Constitution.[citation needed] Since workers' compensation mandated benefits without regard to fault or negligence, many felt that compulsory participation would deprive the employer of property without due process.[citation needed] The issue of due process was resolved by the United States Supreme Court in 1917 in New York Central Railway Co. v. White which held that an employer's due process rights were not impeded by mandatory workers' compensation.[31] After the ruling many states enacted new compulsory workers' compensation laws.[citation needed]		In the United States, most employees who are injured on the job receive medical care responsive to the work-place injury, and, in some cases, payment to compensate for resulting disabilities.[citation needed] Generally, an injury that occurs when an employee is on his or her way to or from work does not qualify for worker's compensation benefits; however, there are some exceptions if your responsibilities demand that you be in multiple locations, or stay in the course of your employment after work hours.[32]		Insurance policies are available to employers through commercial insurance companies: if the employer is deemed an excessive risk to insure at market rates, it can obtain coverage through an assigned-risk program.[citation needed] In many states, there are public uninsured employer funds to pay benefits to workers employed by companies who illegally fail to purchase insurance.[citation needed]		Various organisations focus resources on providing education and guidance to workers' compensation administrators and adjudicators in various state and national workers' compensation systems. These include the American Bar Association (ABA), the International Association of Industrial Accident Boards and Commissions (IAIABC),[33] the National Association of Workers' Compensation Judiciary (NAWCJ),[34] and the Workers Compensation Research Institute.[35]		In the United States, according to the Bureau of Labor Statistics' 2010 National Compensation Survey, workers' compensation costs represented 1.6% of employer spending overall, although rates varied significantly across industry sectors. For instance, workers' compensation accounted for 4.4% of employer spending in the construction industry, 1.8% in manufacturing and 1.3% in services.[36]		Clinical outcomes for patients with workers' compensation tend to be worse compared to those non-workers' compensation patients among those undergoing upper extremity surgeries, and have found they tend to take longer to return to their jobs and tend to return to work at lower rates. Factors that might explain this outcome include this patient population having strenuous upper extremity physical demands, and a possible financial gain from reporting significant post-operative disability.[37]		As each state within the United States has its own workers' compensation laws, the circumstances under which workers' compensation is available to workers, the amount of benefits that a worker may receive, and the duration of the benefits paid to an injured worker, vary by state.[38] The workers' compensation system is administered on a state-by-state basis, with a state governing board overseeing varying public/private combinations of workers' compensation systems.[citation needed] The names of such governing boards, or "quasi-judicial agencies," vary from state to state, many being designated as "workers' compensation commissions". In North Carolina, the state entity responsible for administering the workers' compensation system is referred to as the North Carolina Industrial Commission[39]		In a majority of states, workers' compensation is solely provided by private insurance companies.[40] Twelve states operate state funds (that serve as models to private insurers and insures state employees), and a handful of states have state-owned monopoly insurance providers.[41] To keep state funds from crowding out private insurers, the state funds may be required to act as assigned-risk programs or insurers of last resort for businesses that cannot obtain coverage from a private insurer.[42] In contrast, private insurers can turn away the worst risks and may also write comprehensive insurance packages covering general liability, natural disasters, and other forms of insurance coverage.[43] Of the twelve state funds, the largest is California's State Compensation Insurance Fund.		Underreporting of injuries is a significant problem in the workers' compensation system.[44] Workers, fearing retaliation from their employers, may avoid reporting injuries incurred on the job and instead seek treatment privately, bearing the cost themselves or passing these costs on to their health insurance provider – an element in the increasing cost of health insurance nationwide.[45]		In all states except Georgia and Mississippi, it is illegal for an employer to terminate or refuse to hire an employee for having reported a workplace injury or filed a workers' compensation claim.[46] However, it is often not easy to prove discrimination on the basis of the employee's claims history.[citation needed] To abate discrimination of this type, some states have created a "subsequent injury trust fund" which will reimburse insurers for benefits paid to workers who suffer aggravation or recurrence of a compensable injury.[citation needed] It is also suggested that laws should be made to prohibit inclusion of claims history in databases or to make it anonymous.[citation needed] (See privacy laws.)		Although workers' compensation statutes generally make the employer completely immune from any liability (such as for negligence) above the amount provided by the workers' compensation statutory framework, there are exceptions. In some states, like New Jersey, an employer can still be held liable for larger amounts if the employee proves the employer intentionally or recklessly caused the harm, while in other states, like Pennsylvania, the employer is immune in all circumstances, but other entities involved in causing the injury, like subcontractors or product manufacturers, may still be held liable.[47]		Some employers vigorously contest employee claims for workers' compensation payments.[citation needed] Injured workers may be able to get help with their claims from state agencies or by retaining a workers' compensation lawyer.[citation needed] Laws in many states limit a claimant's legal expenses to a certain fraction of an award; such "contingency fees" are payable only if the recovery is successful. In some states this fee can be as high as 40% or as little as 11% of the monetary award recovered, if any.[38]		In the vast majority of states, original jurisdiction over workers' compensation disputes has been transferred by statute from the trial courts to special administrative agencies.[48] Within such agencies, disputes are usually handled informally by administrative law judges. Appeals may be taken to an appeals board and from there into the state court system. However, such appeals are difficult and are regarded skeptically by most state appellate courts, because the point of workers' compensation was to reduce litigation. A few states still allow the employee to initiate a lawsuit in a trial court against the employer. For example, Ohio allows appeals to go before a jury.[49]		The California Constitution, Article XIV section 4, sets forth the intent of the people to establish a system of workers' compensation.[50] This section provides the Legislature with the power to create and enforce a complete system of workers' compensation and, in that behalf, create and enforce a liability on the part of any or all employers to compensate any or all of their employees for injury or disability, and their dependents, for death incurred or sustained by said employees in the course of their employment, irrespective of the fault of any employee. Further, the Constitution provides that the system must accomplish substantial justice in all cases expeditiously, inexpensively, and without incumbrance of any character. It was the intent of the people of California when they voted to amend the state constitution in 1918, to require the Legislature to establish a simple system that guaranteed full provision for adequate insurance coverage against liability to pay or furnish compensation. Providing a full provision for regulating such insurance coverage in all its aspects, including the establishment and management of a State compensation insurance fund; full provision for otherwise securing the payment of compensation; and full provision for vesting power, authority and jurisdiction in an administrative body with all the requisite governmental functions to determine any dispute or matter arising under such legislation, in that the administration of such legislation accomplish substantial justice in all cases expeditiously, inexpensively, and without encumbrance of any character. All of which matters is the people expressly declared to be the social public policy of this State, binding upon all departments of the State government.[51]		Texas allows employers to opt out of the workers' compensation system, with those employers who do not purchase workers' compensation insurance being called nonsubscribers.[52] However, those employers, known as non-subscribers, are exposed to legal liability in the event of employee injury. The employee must demonstrate that employer negligence caused the injury; if the employer does not subscribe to workers' compensation, the employer loses their common law defense of contributory negligence, assumption of the risk, and the fellow employee doctrine.[53] If successful, the employee can recover their full common law damages, which are more generous than workers' compensation benefits.		In 1995, 44% of Texas employers were nonsubscribers, while in 2001 the percentage was estimated to be 35%.[52] The industry advocacy group Texas Association of Business Nonsubscription claims that nonsubscribing employers have had greater satisfaction ratings and reduced expenses when compared to employers enrolled in the workers' compensation system.[54] A research survey by Texas's Research and Oversight Council on Workers' Compensation found that 68% of non-subscribing employers and 60% of subscribing employers—a majority in both cases—were satisfied with their experiences in the system, and that satisfaction with nonsubscription increased with the size of the firm; but it stated that further research was needed to gauge satisfaction among employees and to determine the adequacy of compensation under nonsubscription compared to subscription.[52] In recent years, the Texas Supreme Court has been limiting employer duties to maintain employee safety, limiting the remedies received by injured workers.		In recent years, workers' compensation programs in West Virginia and Nevada were successfully privatised, through mutualisation, in part to resolve situations in which the programs in those states had significantly underfunded their liabilities.[citation needed] Only four states rely on entirely state-run programs for workers' compensation: North Dakota, Ohio, Washington, and Wyoming.[citation needed] Many other states maintain state-run funds but also allow private insurance companies to insure employers and their employees, as well.[citation needed]		The federal government has its own workers' compensation program, subject to its own requirements and statutory parameters for federal employees.[citation needed] The federal government pays its workers' compensation obligations for its own employees through regular appropriations.[citation needed]		Employees of common carriers by rail have a statutory remedy under the Federal Employers' Liability Act, 45 U.S.C. sec. 51, which provides that a carrier "shall be liable" to an employee who is injured by the negligence of the employer. To enforce his compensation rights, the employee may file suit in United States district court or in a state court. The FELA remedy is based on tort principles of ordinary negligence and differs significantly from most state workers' compensation benefit schedules.		Seafarers employed on United States vessels who are injured because of the owner's or the operator's negligence can sue their employers under the Jones Act, 46 U.S.C. App. 688., essentially a remedy very similar to the FELA one.		Dock workers and other maritime workers, who are not seafarers working aboard navigating vessels, are covered by the Federal Longshore and Harbor Workers' Compensation Act, known as US L&H.		Workers' compensation fraud can be committed by doctors, lawyers, employers, insurance company employees and claimants, and may occur in both the private and public sectors.[55]		The topic of workers' compensation fraud is highly controversial, with claimant supporters arguing that fraud by claimants is rare—as low as one-third of one percent,[56] others focusing on the widely reported National Insurance Crime Bureau statistic that workers' compensation fraud accounts for $7.2 billion in unnecessary costs,[57] and government entities acknowledging that "there is no generally accepted method or standard for measuring the extent of workers' compensation fraud ... as a consequence, there are widely divergent opinions about the size of the problem and the relative importance of the issue."[58]		According to the Coalition Against Insurance Fraud, tens of billions of dollars in false claims and unpaid premiums are stolen in the U.S. alone every year.[59]		The most common forms of workers' compensation fraud by workers are:		The most common forms of workers' compensation fraud by employers are:		
Epilepsy can affect employment for a variety of reasons. Many employers are reluctant to hire a person they know has epilepsy, even if the seizures are controlled by medication. If the employee suffers a seizure while at work, they could harm themselves (but rarely others, contrary to popular belief) depending on the nature of the work. Employers are often unwilling to bear any financial costs that may come from employing a person with epilepsy, i.e. insurance costs, paid sick leave etc. Many people whose seizures are successfully controlled by a medication suffer from a variety of side effects, most notably drowsiness, which may affect job performance. Many laws prohibit or restrict people with epilepsy from performing certain duties, most notably driving or operating dangerous machinery, thereby lowering the pool of jobs available to people with epilepsy. Epilepsy sufferers are also prohibited from joining the armed forces, though they may work in certain civilian military positions.		Employment issues are responsible for 85% of the cost of epilepsy on society.[1] In the United States, the median income for persons with epilepsy is 93% that of all persons. The unemployment rate for persons with epilepsy has been reported to be between 25% and 69%. The high school graduation rate has been reported at 64%, compared with an overall national average of 82%.[2]						The following issues exist for people with epilepsy in their quest for and performance of employment:		People with epilepsy may be barred from various types of employment, either by law, by company regulations, or by common sense, thereby lowering the pool of jobs available to the job seeker.[3]		Those barred from driving by the laws of the land in which they reside cannot perform any jobs that involve operating a motor vehicle. Even if the patients are permitted by law to drive their own vehicle, they may be barred by local and national laws from driving a vehicle for the purpose of certain types of employment, such as getting a Commercial Driver's License or driving a school bus or being the engineer of a train (even if a person who has not had a seizure in a certain time period is not permanently banned, they may still have to be able to stay seizure-free for a year or more even without medication).[4]		Most countries bar those who have ever had a seizure from flying an aircraft, except perhaps for a private craft, especially if the aircraft in question is a commercial or military aircraft or is any type of jet, making a career in aviation or in space extremely unlikely (again, some people who have been seizure-free without medication for a considerable time period, usually at least a year, are allowed to fly- even jets- in some cases, if they can remain seizure-free for at least a year without using anticonvulsant drugs).[4]		Jobs involving the operation of dangerous machinery may pose a problem to people with epilepsy, including construction and industrial work.[4]		Many places have laws barring those with epilepsy whose seizures are not entirely controlled from working in positions that involve a high degree of responsibility to the well-being of others. This includes police officers, teachers, and health care workers.[4]		There are many hazards that people with epilepsy, including those whose seizures are fully controlled by medication, face in the workplace. Those with active seizures face the obvious risk of loss of consciousness or muscle control, and those with side effects face diminished concentration or physical strength. Some of the hazards include:[5]		Even if a person with epilepsy is able to safely perform job duties him/herself, many are limited where they can work if they cannot provide their own transportation to the job site. Since some cannot drive themselves to work, they cannot travel to a place of employment.[6][7]		Some people with epilepsy who cannot drive may also be unable to safely walk, use public transportation, or otherwise independently travel safely due to their seizure risk, further preventing them from reaching a place of employment. Such persons may be at risk for suffering seizures while on or waiting for a public transport vehicle or while crossing the street.[1]		Stigma alone can make it more difficult for people with epilepsy to find jobs. Even if one's seizures are fully controlled by medication, or if the condition has been completely cured by surgery, many employers are reluctant to hire a person with epilepsy.[8]		United States law does not require an applicant for a job to disclose one's condition to the employer. If an applicant voluntarily reveals one's condition, the employer is only allowed to ask whether the employee requires any special accommodations, and if so, what types.[9]		If an employer learns of an epileptic condition after making a decision to hire an employee, the employer is not legally permitted to withdraw the decision to hire as a result of this information unless the employee's duties will pose a risk to public safety. If this is the case, the employer is permitted to require the employee to obtain information from a physician regarding this.		Federal law in the United States requires that federal government agencies and employers receiving federal funding cannot discriminate in hiring against a prospective employee with epilepsy unless the duties one would be performing can be unsafe with a seizure disorder.[10]		Employees who have epilepsy may require special accommodations from their employers. Although against the law, some employers may feel reluctant to provide these accommodations. Some special needs include:[11]		According to the law of the United States, an employer is permitted to inquire into an employee's epileptic condition if the employee suffers one or more seizures while on duty only if they affect safety or job performance.[9]		The employer is permitted to require the employee to take a leave of absence or reassign the employee until the issues are resolved if the seizures pose a threat to the safety of others.		Depending on the severity, epilepsy can be considered a disability which makes employment very difficult or even impossible for many sufferers for a variety of reasons. Those with seizures that cannot be controlled may find themselves unable to perform job duties of any type because their consciousness is constantly interrupted by the seizures. The aftermath of an often unpredictable seizure may leave a patient too fatigued to work for a period of time, or may temporarily impair the patient's memory. Seizures may pose a hazard to the employee or others in the event the employee loses consciousness while performing certain duties. Even if the seizures are completely controlled by a medication, side effects, such as drowsiness or fatigue, may make the performance of duties impossible or more difficult.		In the United States, while the Americans with Disabilities Act does not fully protect persons with epilepsy from discrimination in hiring practices, the Social Security Administration only considers people with epilepsy "disabled" and thereby eligible to receive benefits if the condition severely limits one or more major life activities.[12] Employment may be hard to find or perform for many people with epilepsy, but not all are eligible for government-sponsored disability payments.		To qualify, documentation to the Social Security Administration of an EEG detailing a typical seizure is required, though a normal diagnosis does not rule out benefits. A seizure diary, including times and dates of seizures, and the effects the seizures have had is required. A person may qualify either if the seizures themselves have debilitating effects, or if the drugs used to treat the disorder have side effects that make employment impossible or difficult.[13]		Many countries restrict people with epilepsy from joining their armed forces.		In the United States, in order to enroll in military service in a combat role, one must be seizure-free since age 5 and off all medications.[14]		In the United Kingdom people with epilepsy are automatically barred for life from joining the military.		
Wage theft is the denial of wages or employee benefits that are rightfully owed to an employee. Wage theft can be conducted through various means such as: failure to pay overtime, minimum wage violations, employee misclassification, illegal deductions in pay, working off the clock, or not being paid at all.		According to some studies, wage theft is common in the United States, particularly from low wage legal or illegal immigrant workers.[1][2] The Economic Policy Institute reported in 2014 that survey evidence suggests wage theft costs US workers billions of dollars a year.[3] Some rights violated by wage theft have been guaranteed to workers in the United States in the 1938 Fair Labor Standards Act (FLSA).[4]						According to the FLSA, unless exempt, employees are entitled to receive overtime pay calculated at least time and one-half times pay for all time worked past forty hours a week. Some exemptions to this rule apply to public service agencies or to employees who meet certain requirements in accordance to their job duties along with a salary of no less than $455 a week. Despite regulations, there are many employees today who are not paid overtime due them. A 2009 study of workers in the United States found that in 12 occupations more than half of surveyed workers reported being denied overtime pay: child care (90.2 percent denial), stock and office clerks (86 percent), home health care (82.7 percent), beauty/dry cleaning and general repair workers (81.9 percent), car wash workers and parking attendants (77.9 percent), waiters, cafeteria workers and bartenders (77.9 percent), retail salespersons (76.2 percent), janitors and grounds workers (71.2 percent), garment workers (69.9 percent), cooks and dishwashers (67.8 percent), construction workers (66.1 percent) and cashiers (58.8 percent).[7][8]		In 2009, reform placed the new federal minimum wage at $7.25. Some states have legislation that sets a state minimum wage. In the case an employee is subject to both federal and state minimum wage acts, the employee is entitled to the higher standard of compensation. For tipped employees, the employer is only required to compensate the employee $2.13 an hour as long as the fixed wage and the tips add up to be at or above the federal minimum wage. Minimum wage is enforced by the Wage and Hour Division (WHD). WHD is generally contacted by 25,000 people a year in regards to concerns and violations of minimum wage pay.[7][9] A common form of wage theft for tipped employees is to receive no standard pay ($2.13 an hour) along with tips.[8]		Misclassification of employees is a violation that leaves employees very vulnerable to other forms of wage theft. Under the FLSA, independent contractors do not receive the same protection as an employee for certain benefits. The difference between the two classifications depends on the permanency of the employment, opportunity for profit and loss, the worker's level of self-employment along with their degree of control. An independent contractor is not entitled to minimum wage, overtime, insurance, protection, or other employee rights. Attempts are sometimes made to define ordinary employees as independent contractors.[7][8]		Misclassification in the United States is extensive. In New York state, for example, it was found in a 2007 study that approximately 704,785 workers, or 10.3% of the state's private sector workforce, was misclassiﬁed each year. For the industries covered in the study, average unemployment insurance taxable wages underreported due to misclassiﬁcation was on average $4.3 billion for the year and unemployment insurance tax underreported in these industries was $176 million.[10][11]		Employees are subject to forms of wage theft through illegal deductions. Trivial to sometimes fabricated violations in the workplace are used to validate deductions. Any deduction that brings an employee to a level of compensation lower than minimum wage is also illegal. In many states, employers are required to issue employees documentation of deductions along with earnings. Failure to issue this documentation is generally prevalent in working places subject to wage theft.[7][8]		The most blatant form of wage theft is for an employee to not be paid for work done. An employee being asked to work overtime, working through breaks, or being asked to report early and/or leave late without pay is being subjected to wage theft. This is sometimes justified as displacing a paid meal break without guaranteeing meal break time. In the most extreme cases, employees report receiving nothing.[7][8] In some cases, the legal status of the workers can enable employers to withhold pay without fear of facing any consequences.[12]		Putting the pressure on injured workers to not file for workers' compensation is frequently successful.[1] Employees are often confronted with threats of firing or calls to immigration services if they complain or seek redress. Workers are often denied time off or vacation time that they have acquired or denied pay for sick leave or vacation time.		In Australia, another form of wage theft is the failure of employers to pay the mandatory minimum contribution to employee's superannuation fund. Between 2009 and 2013 the Australian Tax Office recovered A$1.3 billion in unpaid superannuation which is estimated to be only a small portion of total unpaid superannuation.[13] 		A 2009 study based on 2008 interviews of over 4,000 low wage workers in Chicago, Los Angeles, and New York City found that wage theft from low wage workers in large cities in the United States was severe and widespread. Incidents varied with the type of job and employee. Sixty-eight percent of the surveyed workers experienced at least one pay-related violation in the week prior to the survey. On average the workers in the three cities lost a total of $2,634 annually due to workplace violations, out of an average income of $17,616, which translates into wage theft of fifteen percent of income. Extrapolating from these figures, low wage workers in Chicago, Los Angeles, and New York City lost more than $2.9 billion due to employment and labor law violations.[7] Nationally it is estimated that workers are not paid at least $19 billion every year in overtime[14] and that in the US $40 billion to $60 billion in total are lost annually due to all forms of wage theft.[15] This compares to national annual losses of $340 million due to robbery, $4.1 billion due to burglary, $5.3 billion due to larceny, and $3.8 billion due to auto theft in 2012.[5]		Studies have found inflated rates of wage theft violations in markets employing women and foreign-born populations. Within the foreign-born population, women were at a much greater risk for wage violations than their male counterparts. Undocumented workers or unauthorized immigrants stood at the highest risk levels. Education, longer tenured employment, and English proficiency proved to be influential factors in employee populations. All three variables reduced the probability of wage theft for the aforementioned demographics. Workplaces where the compensation was paid in one weekly flat rate or in cash saw a higher instance rate of wage theft. Smaller businesses with less than 100 employees also saw a higher instance rate of violations than larger business. In one study, the manufacturing industry, repair services, and private home employment were at the highest risk for violations at the workplace. Home health care, education, and construction saw the lowest levels of wage theft. Restaurants, grocery stores, retail, and warehousing fell around the median.[7][16]		In November 2011, Warehouse Workers helped Wal-Mart warehouse employees file their fourth class action lawsuit against the warehouse companies. Without Wal-Mart being a direct defendant, the argument was made that Wal-Mart has created this culture amongst the companies it works with. The first lawsuit filed was in 2009. The workers argued that poor record keeping and broken promises have led to workers receiving less than minimum wage. Walmart also denied workers paid vacations that they were promised upon contracting.[17] In a report released on November 26, 2011, a Palm Beach County organization, People Engaged in Active Community Efforts (PEACE), sent postcards to Macy's and Bealls executives as a form of protest. The Florida Retail Federation had recently proposed a bill to block a wage theft ordinance in their county, which was intended to create a system that would speed the investigation and processing of wage theft reports.[18]		A 2012 study by the Iowa Policy Project calculated that dishonest employers defraud Iowa workers out of about $600 million annually in wages. State Senator Tony Bisignano, Democrat from Des Moines and Senator William Dotzler, a Democrat from Waterloo, Iowa, proposed a bill to strengthen wage law enforcement on January 28, 2015, "since Iowa's wage theft laws are so weak they are impossible to enforce". The Iowa Association of Business and Industry opposed the bill, saying that resources for enforcement should be the focus instead.[19]		In the United States the Fair Labor Standards Act (FLSA) requires employers to keep detailed records regarding the identity of workers and hours worked for all workers who are protected under federal minimum wage laws.[20][21] Most states require that employers also provide each worker with documentation every pay period detailing that worker's hours, wages and deductions.[22][better source needed] As of September 2011 Arkansas, Florida, Louisiana, Mississippi, Nebraska, South Dakota, Tennessee and Virginia did not require this documentation.[23][better source needed] A 2008 survey of wage theft from workers in Illinois, New York, and California found that 57% of low wage workers did not receive this required documentation and that workers who were paid in cash or on a weekly rate were more likely to experience wage theft.[7] Anecdotal evidence suggests that tip theft, which is a legally complex issue distinct from wage theft and not necessarily under the control of the same laws governing the payment of wages,[24][better source needed] may also be common in instances where employer record keeping does not comply with the law.[25]		When the Wage and Hour Division (WHD) receives reports of violations, it works to ensure that employers change their work practices and pay back missed wages to the employees. Willful violators can face fines up to $10,000 upon their first conviction with imprisonment resulting from future convictions. In regards to child labor laws, an employer can face a fine of up to $11,000 per minor. In 2012 the Wage and Hour Division collected $280 million in back wages for 308,000 workers.[26] As of 2014, there are 1,100 federal investigators for 135 million workers in more than 7 million businesses.[27] The ratio of labor enforcement agents to U.S. workers has decreased tenfold since the inception of the FLSA from one for every 11,000 workers in 1941[28] to one for every 123,000 workers in 2014.[27]		In February 2010 Miami-Dade County, Florida became the first jurisdiction in America to ban wage theft with an ordinance passed unanimously by the county commission. Prior to the ordinance, wage theft was called "the crime wave that almost no one talks about".[29]		
The following articles contain lists of recessions:		
The average wage is a measure for the financial well-being of a country's inhabitants. The average wages are sometimes, like in this article, adjusted to living expenses ("purchasing power parity"). The wage distribution is right-skewed; the majority of people earn less than the average wage. For an alternative measure, the Median household income uses median instead of average.						The OECD (Organization for Economic Co-operation and Development) dataset contains data on average annual wages for full-time and full-year equivalent employees in the total economy. Average annual wages per full-time equivalent dependent employee are obtained by dividing the national-accounts-based total wage bill by the average number of employees in the total economy, which is then multiplied by the ratio of average usual weekly hours per full-time employee to average usually weekly hours for all employees.		Average wages are converted in US dollar nominal using 2013 US dollar nominal for private consumption and are deflated by a price deflator for private final consumption expenditures in 2013 prices. The OECD is a weighted average based on dependent employment weights in 2013 for the countries shown.[1]		Gross average monthly wage estimates for 2015 are computed by converting national currency figures from the UNECE (United Nations Economic Commission for Europe) Statistical Database, compiled from national and international (OECD, EUROSTAT, CIS) official sources. Wages in US dollars are computed by the UNECE Secretariat using nominal exchange rates.		Gross average monthly wages cover total wages and salaries in cash and in kind, before any tax deduction and before social security contributions. They include wages and salaries, remuneration for time not worked, bonuses and gratuities paid by the employer to the employee. Wages cover the total economy and are expressed per full-time equivalent employee.[3]		
A background check or background investigation is the process of looking up and compiling criminal records, commercial records, and financial records of an individual or an organization.[1]						Background checks are often requested by employers on job candidates for employment screening, especially on candidates seeking a position that requires high security or a position of trust, such as in a school, courthouse, hospital, financial institution, airport, and government. These checks are traditionally administered by a government agency for a nominal fee, but can also be administered by private companies. Background checks can be expensive depending on the information requested. Results of a background check typically include past employment verification, credit history, and criminal history. The objective of backround checks is to ensure the safety and security of the employees in the organisation.[2]		These checks are often used by employers as a means of judging a job candidate's past mistakes, character, and fitness, and to identify potential hiring risks for safety and security reasons. Background checks are also used to thoroughly investigate potential government employees in order to be given a security clearance. However, these checks may sometimes be used for illegal purposes, such as unlawful discrimination (or employment discrimination), identity theft, and violation of privacy.		Checks are frequently conducted to confirm information found on an employment application or résumé/curriculum vitae. One study showed that half of all reference checks done on prospective employees differed between what the job applicant provided and what the source reported.[3] They may also be conducted as a way to further differentiate potential employees and pick the one the employer feels is best suited for the position. Employers have an obligation to make sure their work environment is safe for all employees and helps prevent other employment problems in the workplace.[4]		In the United States, the Brady Bill requires criminal checks for those wishing to purchase handguns from licensed firearms dealers. Restricted firearms (like machine guns), suppressors, explosives or large quantities of precursor chemicals, and concealed weapons permits also require criminal checks.[5]		Checks are also required for those working in positions with special security concerns, such as trucking, ports of entry, and airports (including airline transportation). Other laws exist to prevent those who do not pass a criminal check from working in careers involving the elderly, disabled, or children.		Pre-employment screening refers to the process of investigating the backgrounds of potential employees and is commonly used to verify the accuracy of an applicant's claims as well as to discover any possible criminal history, workers compensation claims, or employer sanctions. For example, CBC News of Canada reported that fraud in the workplace cost Canadian Businesses over $3.2 Billion in 2011.[6]		A number of annual reports, including BDO Stoy Hayward's Fraudtrack 4[7] and CIFAS's [8] (The UK's fraud prevention service) 'The Enemy Within' have showed a rising level of major discrepancies and embellishments on CVs over previous years.[citation needed]		Almost half (48%) of organizations with fewer than 100 staff experienced problems with vetted employees.[citation needed]		Thirty-nine percent of UK organizations have experienced a situation where their vetting procedures have allowed an employee to be hired who was later found to have lied or misrepresented themselves in their application.[citation needed]		Since the onset of the financial crisis of 2007–2010, the level of fraud has almost doubled and some experts have predicted that it will escalate further.[9] Background-checking firm Powerchex has claimed the number of applicants lying on their applications has been increasing since the summer of 2007 when the financial crisis began.[10] In 2009, Powerchex claimed that nearly one in 5 applicants has a major lie or discrepancy on his or her application.[11]		The first Polish research on the issue of pre-employment screening shows that 81% of recruiters have come across the phenomenon of lies in the CVs of candidates for the job. The survey was conducted by IBBC Group and Background Screening Service, which deal with outsourcing and conducting background screenings in Central - Eastern Europe.		The research shows how many failures occurred in the relationship between employer and employee over the years and what dangers it brings. Applicants usually lie about additional skills (85%), dates of employment (58%), responsibilities (53%) or positions (28%).		Due to bad hire, employers lose in a multitude of ways: financial-wise, reputation-wise and time-wise. The research shows that as many as 77% of them have lost significant sums of money to carry out the re-recruitment. Moreover, the companies have suffered losses after hiring the dishonest employee due to his low competences (57%), lack of diligence at work (28%), spending on additional training (28%) or problems with attendance (15%) and employee theft (7%). The demand for pre-employment screenings is constantly growing among Polish entrepreneurs.		The possibilities of background checks in Poland		Background screening can be conducted in Poland on the grounds of written consent of the person. Depending on the degree to which recruitment is carried out, verifications can include: identity, history of education, employment history, reason for leaving, additional qualifications, employee opinion, criminal record, indebtedness, references, business connections, presence on sanction lists, presence in the media.		While background screening and verification is well established in Australia and New Zealand,[12] it is in its infancy in Asia and regulations vary widely across the region.[13] Leading background screening companies in the region must navigate different regulatory frameworks and market leaders offer a full complement of services including financial probity, education, criminal background checking where legal, directorships.		Larger companies are more likely to outsource than their smaller counterparts – the average staff size of the companies who outsource is 3,313 compared to 2,162 for those who carry out in-house checks.		Financial services firms had the highest proportion of respondents who outsource the service, with over a quarter (26%) doing so, compared to an overall average of 16% who outsource vetting to a third party provider.		The construction and property industry showed the lowest level of outsourcing, with 89% of such firms in the sample carrying out checks in-house, making the overall average 16%. This can increase over the years.		Companies that choose to outsource must be sure to use companies that are Fair Credit Reporting Act (FCRA) compliant. Companies that fail to use an FCRA compliant company may face legal issues due to the complexity of the law.		The Financial Services Authority states in their Training & Competence guidance that regulated firms should have:		The Financial Services Authority’s statutory objectives:		Due to the sensitivity of the information contained in consumer reports and certain records, there are a variety of important laws regulating the dissemination and legal use of this information. Most notably, the Fair Credit Reporting Act (FCRA) regulates the use of consumer reports (which it defines as information collected and reported by third party agencies) as it pertains to adverse decisions, notification to the applicant, and destruction and safekeeping of records.		If a consumer report is used as a factor in an adverse hiring decision, the applicant must be presented with a "pre-adverse action disclosure," a copy of the FCRA summary of rights, and a "notification of adverse action letter." Individuals are entitled to know the source of any information used against them including a credit reporting company. Individuals must also consent in order for the employer to obtain a credit report. Pre-employment credit reports do not include a credit score. A pre-employment credit report will show up on an individuals credit report as a "soft inquiry" and do not affect the individual's credit score.[citation needed]		Title XLV, section 768.095, of the Florida Statutes protects employers from negligent hiring liabilities, provided they attempt to conduct certain screening procedures. Employers who follow these steps will be presumed not to have been negligent when hiring if a background check fails to reveal any records on an applicant.[14]		There are a variety of types of investigative searches that can be used by potential employers. Many commercial sites will offer specific searches to employers for a fee. Services like these will actually perform the checks, supply the company with adverse action letters, and ensure compliance throughout the process. It is important to be selective about which pre-employment screening agency one uses. A legitimate company will be happy to explain the process. Many employers choose to search the most common records such as criminal records, driving records, and education verification. Other searches such as sex offender registry, credential verification, skills assessment, reference checks, credit reports and Patriot Act searches are becoming increasingly common. Employers should consider the position in question when determining which types of searches to include, and should always use the same searches for every applicant being considered for one.		The amount of information included on a background check depends to a large degree on the sensitivity of the reason for which it is conducted—e.g., somebody seeking employment at a minimum wage job would be subject to far fewer requirements than somebody applying to work for a law enforcement agency such as the FBI or jobs related to national security.		Drug tests and credit checks for employment are highly controversial practices. According to the Privacy Rights Clearinghouse, a project of the Utility Consumers' Action Network (UCAN): "While some people are not concerned about background investigations, others are uncomfortable with the idea of investigators poking around in their personal histories. In-depth checks could unearth information that is irrelevant, taken out of context, or just plain wrong. A further concern is that the report might include information that is illegal to use for hiring purposes or which comes from questionable sources."		In May 2002, allegedly improper post-hire checks conducted by Northwest Airlines were the subject of a civil lawsuit between Northwest and 10,000 of their mechanics.		In the case of an arrest that did not lead to a conviction, employment checks can continue including the arrest record for up to seven years, per § 605 of the Fair Credit Reporting Act:		Subsection (b) provides for an exception if the report is in connection with "the employment of any individual at an annual salary which equals, or which may reasonably be expected to equal $75,000, or more".[16]		Some proposals for decreasing potential harm to innocent applicants include:		In New Zealand, criminal checks have been affected by the Criminal Records (Clean Slate) Act 2004, which allows individuals to legally conceal "less serious" convictions from their records provided they had been conviction-free for at least seven years.		In Michigan, the system of criminal checks has been criticized in a recent case where a shooting suspect was able to pass an FBI check to purchase a shotgun although he had failed the check for a state handgun permit. According to the spokesman of the local police department,		The Brady Campaign to Prevent Gun Violence has criticized the federal policy, which denies constitutional rights based on a criminal check only if the subject has been accused of a crime.		Taking advantage of public records availability in the United States, a number of Web-based companies began purchasing U.S. public records data and selling it online, primarily to assist the general public in locating people. Many of these sites advertise background research and provide employers and/or landlords with fee-based checks.		There has been a growing movement on the web[citation needed] to use advertising-based models to subsidize these checks. These companies display targeted ads next to the reports delivered to landlords or employers. Some of the reports provided by these pay sites are only expanded versions of a basic people search providing a 20-year history of addresses, phone numbers, marriages and divorces, businesses owned and property ownership. Usually, these sites will also provide a nationwide criminal report for an added charge.		As a general rule, employers may not take adverse action against an applicant or employee (not hiring or terminating them), solely on the basis of results obtained through a database search. Database searches, as opposed to source records searches (search of actual county courthouse records), are notoriously inaccurate, contain incomplete or outdated information, and should only be used as an added safety net when conducting a background check. Failure by employers to follow FCRA guidelines can result in hefty penalties.[18]		
Richard L. Florida (born November 26, 1957, in Newark, New Jersey) is an American urban studies theorist. Florida's focus is on social and economic theory. He is currently a professor and head of the Martin Prosperity Institute at the Rotman School of Management at the University of Toronto.[1]		Prof. Florida received a PhD from Columbia University in 1986. Prior to joining George Mason University's School of Public Policy, where he spent two years, he taught at Carnegie Mellon University's Heinz College in Pittsburgh from 1987 to 2005. He was named a Senior Editor at The Atlantic in March 2011 after serving as a correspondent for TheAtlantic.com for a year.[2]						Florida lives in Toronto and Miami and is married to Rana Florida.[3][4]		Florida is best known for his concept of the creative class and its implications for urban regeneration. This idea was expressed in Florida's best-selling books The Rise of the Creative Class (2002), Cities and the Creative Class, and The Flight of the Creative Class, and later published a book focusing on the issues surrounding urban renewal and talent migration, titled Who's Your City?		Florida's theory asserts that metropolitan regions with high concentrations of technology workers, artists, musicians, lesbians and gay men, and a group he describes as "high bohemians", exhibit a higher level of economic development. Florida refers to these groups collectively as the "creative class." He posits that the creative class fosters an open, dynamic, personal and professional urban environment. This environment, in turn, attracts more creative people, as well as businesses and capital. He suggests that attracting and retaining high-quality talent versus a singular focus on projects such as sports stadiums, iconic buildings, and shopping centers, would be a better primary use of a city's regeneration of resources for long-term prosperity. He has devised his own ranking systems that rate cities by a "Bohemian index," a "Gay index," a "diversity index" and similar criteria.[5]		Florida's earlier work focused on innovation by manufacturers, including the continuous-improvement systems implemented by such automakers as Toyota.[citation needed]		Florida's ideas have been criticized from a variety of political perspectives and by both academics and journalists. His theories have been criticized as being elitist, and his conclusions have been questioned.[6] Researchers have also criticized Florida's work for its methodology. Terry Nichols Clark of the University of Chicago used Florida's own data to question the correlation between the presence of significant numbers of gay men in a city and the presence of high-technology knowledge industries. [7] Harvard economist Edward Glaeser analyzed Florida's data and concluded that educational levels, rather than the presence of bohemians or gay people, is correlated with metropolitan economic development.[8] Other critics have said that the conditions it describes may no longer exist, and that his theories may be better suited to politics, rather than economics.[9] Florida has gone on to directly reply to a number of these objections.[10]		Florida's book, The Rise of the Creative Class, came at the end of the dot-com boom in 2002. It was followed by a "prequel", Cities and the Creative Class, which provided more in-depth data to support his findings.		With the rise of Google, the gurus of Web 2.0, and the call from business leaders (often seen in publications such as Business 2.0) for a more creative, as well as skilled, workforce, Florida asserts that the contemporary relevance of his research is easy to see.[10] One author characterizes him as an influence on radical centrist political thought.[11]		Some scholars have voiced concern over Florida's influence on urban planners throughout the United States. A 2010 book, Weird City, examines Florida's influence on planning policy in Austin, Texas. The main body of the book treats Florida's creative class theory in an introductory and neutral tone, but in a theoretical "postscript" chapter, the author criticizes Florida's tendency to "whitewash" the negative externalities associated with creative city development.[12]		
A job interview is a one-on-one interview consisting of a conversation between a job applicant and a representative of an employer which is conducted to assess whether the applicant should be hired.[1] Interviews are one of the most popularly used devices for employee selection.[2] Interviews vary in the extent to which the questions are structured, from a totally unstructured and free-wheeling conversation, to a structured interview in which an applicant is asked a predetermined list of questions in a specified order;[3] structured interviews are usually more accurate predictors of which applicants will make good employees, according to research studies.[4]		A job interview typically precedes the hiring decision. The interview is usually preceded by the evaluation of submitted résumés from interested candidates, possibly by examining job applications or reading many resumes. Next, after this screening, a small number of candidates for interviews is selected.		Potential job interview opportunities also include networking events and career fairs. The job interview is considered one of the most useful tools for evaluating potential employees.[5] It also demands significant resources from the employer, yet has been demonstrated to be notoriously unreliable in identifying the optimal person for the job.[5] An interview also allows the candidate to assess the corporate culture and demands of the job.		Multiple rounds of job interviews and/or other candidate selection methods may be used where there are many candidates or the job is particularly challenging or desirable. Earlier rounds sometimes called 'screening interviews' may involve fewer staff from the employers and will typically be much shorter and less in-depth. An increasingly common initial interview approach is the telephone interview. This is especially common when the candidates do not live near the employer and has the advantage of keeping costs low for both sides. Since 2003, interviews have been held through video conferencing software, such as Skype.[6] Once all candidates have been interviewed, the employer typically selects the most desirable candidate(s) and begins the negotiation of a job offer.						Researchers have attempted to identify which interview strategies or "constructs" can help interviewers choose the best candidate. Research suggests that interviews capture a wide variety of applicant attributes.[7][8][9] Constructs can be classified into three categories: job-relevant content, interviewee performance (behavior unrelated to the job but which influences the evaluation), and job-irrelevant interviewer biases.[10]		Job-relevant interview content Interview questions are generally designed to tap applicant attributes that are specifically relevant to the job for which the person is applying. The job-relevant applicant attributes that the questions purportedly assess are thought to be necessary for one to successfully perform on the job. The job-relevant constructs that have been assessed in the interview can be classified into three categories: general traits, experiential factors, and core job elements. The first category refers to relatively stable applicant traits. The second category refers to job knowledge that the applicant has acquired over time. The third category refers to the knowledge, skills, and abilities associated with the job.		General traits:		Experiential factors:		Core job elements:		Interviewee performance Interviewer evaluations of applicant responses also tend to be colored by how an applicant behaves in the interview. These behaviors may not be directly related to the constructs the interview questions were designed to assess, but can be related to aspects of the job for which they are applying. Applicants without realizing it may engage in a number of behaviors that influence ratings of their performance. The applicant may have acquired these behaviors during training or from previous interview experience. These interviewee performance constructs can also be classified into three categories: social effectiveness skills, interpersonal presentation, and personal/contextual factors.		Social effectiveness skills:		Interpersonal presentation:		Personal/contextual factors:		Job-irrelevant interviewer biases The following are personal and demographic characteristics that can potentially influence interviewer evaluations of interviewee responses. These factors are typically not relevant to whether the individual can do the job (that is, not related to job performance), thus, their influence on interview ratings should be minimized or excluded. In fact, there are laws in many countries that prohibit consideration of many of these protected classes of people when making selection decisions. Using structured interviews with multiple interviewers coupled with training may help reduce the effect of the following characteristics on interview ratings.[24] The list of job-irrelevant interviewer biases is presented below.		The extent to which ratings of interviewee performance reflect certain constructs varies widely depending on the level of structure of the interview, the kind of questions asked, interviewer or applicant biases, applicant professional dress or nonverbal behavior, and a host of other factors. For example, some research suggests that applicant's cognitive ability, education, training, and work experiences may be better captured in unstructured interviews, whereas applicant's job knowledge, organizational fit, interpersonal skills, and applied knowledge may be better captured in a structured interview.[8]		Further, interviews are typically designed to assess a number of constructs. Given the social nature of the interview, applicant responses to interview questions and interviewer evaluations of those responses are sometimes influenced by constructs beyond those the questions were intended to assess, making it extremely difficult to tease out the specific constructs measured during the interview.[29] Reducing the number of constructs the interview is intended to assess may help mitigate this issue. Moreover, of practical importance is whether the interview is a better measure of some constructs in comparison to paper and pencil tests of the same constructs. Indeed, certain constructs (mental ability and skills, experience) may be better measured with paper and pencil tests than during the interview, whereas personality-related constructs seem to be better measured during the interview in comparison to paper and pencil tests of the same personality constructs.[30] In sum, the following is recommended: Interviews should be developed to assess the job relevant constructs identified in the job analysis.[31][32]		Person-environment fit is often measured by organizations when hiring new employees. There are many types of Person-environment fit with the two most relevant for interviews being Person-job and Person-organization fit.[33][34] Interviewers usually emphasis Person-job fit and ask twice as many questions about Person-job fit compared to Person-organization fit.[35] Interviewers are more likely to give applicants with good Person-job fit a hiring recommendation compared to an applicant with good Person-organization fit.[34]		An applicant’s knowledge, skills, abilities, and other attributes (KSAOs) are the most commonly measured variables when interviewers assess Person-job fit.[34] In one survey, all interviewers reported that their organization measures KSAOs to determine Person-job fit.[34] The same study found that all interviewers used personality traits and 65% of the interviewers used personal values to measure Person-organization fit.[34]		Despite fit being a concern among organizations, how to determine fit and the types of questions to use varies. When interview fit questions were examined, only 4% of the questions used in interviews were similar across the majority of organizations. 22% of questions were commonly used by recruiters in some organizations. In contrast, 74% of the questions had no commonality between organizations.[34] Although the idea of fit is similar in many organizations, the questions used and how that information is judged may be very different.[34]		Person-job fit and Person-organization fit have different levels of importance at different stages of a multi stage interview proves. Despite this, Person-job fit is considered of highest importance throughout the entire process. Organizations focus more on job related skills early on to screen out potential unqualified candidates. Thus, more questions are devoted to Person-job fit during the initial interview stages.[35][34] Once applicants have passed the initial stages, more questions are used for Person-organization fit in the final interview stages. Although there is more focus on Person-organization fit in these later stages, Person-job fit is still considered to be of greater importance.[35]		In a single stage interview, both fits are assessed during a single interview.[35] Interviewers still put more weight on Person-job fit questions over the Person-organization questions in these situations as well. Again, Person-job fit questions are used to screen out and reduce the number of applicants.[35][34]		Potential applicants also use job interviews to assess their fit within an organization. This can determine if an applicant will take a job offer when one is offered. When applicants assess their fit with an organization the experience they have during the job interview is the most influential.[36]		Applicants felt that they had highest fit with an organization when they could add information not covered during the interview that they wanted to share. Applicants also liked when they could ask questions about the organization. They also when they could ask follow up questions to ensure they answered the interviewer’s questions to the level the interviewer wanted.[36]  Interviewer behaviors that encourage fit perceptions in applicants include complimenting applicants on their resume and thanking them for traveling to the interview.[36] Applicants like to be given contact information if follow up information is needed, the interviewer making eye contact, and asking if the applicant was comfortable.[36]		The Interviewer can discourage fit perceptions by how they act during an interview as well. the biggest negative behavior for applicants was the interviewer not knowing information about their organization. Without information about the organization applicants cannot judge how well they fit. Another negative behavior is not knowing applicants’ background information during the interview. Interviewers can also hurt fit perception by being inattentive during the interview and not greeting the applicant.[36]		There are some issues with fit perceptions in interviews. Applicants’ Person-organization fit scores can be altered by the amount of ingratiation done by the applicants.[37] Interviewers skew their Person-organization fit scores the more ingratiation applicants do during an interview. By applicants emphasizing similarities between them and the interviewer this leads to a higher Person-organization fit perceptions by the interviewer.[37] This higher perception of fit leads to a greater likelihood of the candidate being hired.[38][37][34]		One way to think about the interview process is as three separate, albeit related, phases: (1) the preinterview phase which occurs before the interviewer and candidate meet, (2) the interview phase where the interview is conducted, and (3) the postinterview phase where the interviewer forms judgments of candidate qualifications and makes final decisions.[39] Although separate, these three phases are related. That is, impressions interviewers form early on may affect how they view the person in a later phase.		Preinterview phase: The preinterview phase encompasses the information available to the interviewer beforehand (e.g., resumes, test scores, social networking site information) and the perceptions interviewers form about applicants from this information prior to the actual face-to-face interaction between the two individuals. In this phase, interviewers are likely to already have ideas about the characteristics that would make a person ideal or qualified for the position.[40] Interviewers also have information about the applicant usually in the form of a resume, test scores, or prior contacts with the applicant.[39] Interviewers then often integrate information that they have on an applicant with their ideas about the ideal employee to form a preinterview evaluation of the candidate. In this way, interviewers typically have an impression of you even before the actual face-to-face interview interaction. Nowadays with recent technological advancements, we must be aware that interviewers have an even larger amount of information available on some candidates. For example, interviewers can obtain information from search engines (e.g. Google, Bing, Yahoo), blogs, and even social networks (e.g. Linkedin, Facebook, Twitter). While some of this information may be job-related, some of it may not be. In some cases, a review of Facebook may reveal undesirable behaviors such as drunkenness or drug use. Despite the relevance of the information, any information interviewers obtain about the applicant before the interview is likely to influence their preinterview impression of the candidate. And, why is all this important? It is important because what interviewers think about you before they meet you, can have an effect on how they might treat you in the interview and what they remember about you.[39][41] Furthermore, researchers have found that what interviewers think about the applicant before the interview (preinterview phase) is related to how they evaluate the candidate after the interview, despite how the candidate may have performed during the interview.[42]		Interview phase: The interview phase entails the actual conduct of the interview, the interaction between the interviewer and the applicant. Initial interviewer impressions about the applicant before the interview may influence the amount of time an interviewer spends in the interview with the applicant, the interviewer’s behavior and questioning of the applicant,[43] and the interviewer’s postinterview evaluations.[42] Preinterview impressions also can affect what the interviewer notices about the interviewee, recalls from the interview, and how an interviewer interprets what the applicant says and does in the interview.[41]		As interviews are typically conducted face-to-face, over the phone, or through video conferencing[44] (e.g. Skype), they are a social interaction between at least two individuals. Thus, the behavior of the interviewer during the interview likely "leaks" information to the interviewee. That is, you can sometimes tell during the interview whether the interviewer thinks positively or negatively about you.[39] Knowing this information can actually affect how the applicant behaves, resulting in a self-fulfilling prophecy effect.[43][45] For example, interviewees who feel the interviewer does not think they are qualified may be more anxious and feel they need to prove they are qualified. Such anxiety may hamper how well they actually perform and present themselves during the interview, fulfilling the original thoughts of the interviewer. Alternatively, interviewees who perceive an interviewer believes they are qualified for the job may feel more at ease and comfortable during the exchange, and consequently actually perform better in the interview. It should be noted again, that because of the dynamic nature of the interview, the interaction between the behaviors and thoughts of both parties is a continuous process whereby information is processed and informs subsequent behavior, thoughts, and evaluations.		Postinterview phase: After the interview is conducted, the interviewer must form an evaluation of the interviewee’s qualifications for the position. The interviewer most likely takes into consideration all the information, even from the preinterview phase, and integrates it to form a postinterview evaluation of the applicant. In the final stage of the interview process, the interviewer uses his/her evaluation of the candidate (i.e., in the form of interview ratings or judgment) to make a final decision. Sometimes other selection tools (e.g., work samples, cognitive ability tests, personality tests) are used in combination with the interview to make final hiring decisions; however, interviews remain the most commonly used selection device in North America.[46]		For interviewees: Although the description of the interview process above focuses on the perspective of the interviewer, job applicants also gather information on the job and/or organization and form impressions prior to the interview.[40] The interview is a two-way exchange and applicants are also making decisions about whether the company is a good fit for them. Essentially, the process model illustrates that the interview is not an isolated interaction, but rather a complex process that begins with two parties forming judgments and gathering information, and ends with a final interviewer decision.		There are many types of interviews that organizations can conduct. What is the same across all interview types, however, is the idea of interview structure. How much an interview is structured, or developed and conducted the same way across all applicants, depends on the number of certain elements included in that interview. Overall, the interview can be standardized both with regard to the content (i.e., what questions are asked) and to the evaluative process (i.e., how the applicants’ responses to the questions are scored). When an interview is standardized, it increases the likelihood that an interviewee’s ratings are due to the quality of his/her responses instead of non-job-related and often distracting factors, such as appearance. Interview structure is more appropriately thought to be on a continuum, ranging from completely unstructured to fully structured.[47] However, structure is often treated as a having only two categories (that is, structured vs. unstructured), which many researchers believe to be too simple of an approach.[47][48]		Unstructured		The unstructured interview, or one that does not include a good number of standardization elements, is the most common form of interview today.[49] Unstructured interviews are typically seen as free-flowing; the interviewer can swap out or change questions as he/she feels is best, and different interviewers may not rate or score applicant responses in the same way. There are also no directions put in place regarding how the interviewer and the interviewee should interact before, during, or after the interview. Unstructured interviews essentially allow the interviewer to conduct the interview however he or she thinks is best.		Given unstructured interviews can change based on who the interviewer might be, it is not surprising that unstructured interviews are typically preferred by interviewers.[50] Interviewers tend to develop confidence in their ability to accurately rate interviewees,[51] detect whether applicants are faking their answers,[52] and trust their judgment about whether the person is a good candidate for the job.[53] Unstructured interviews allow interviewers to do so more freely. Research suggests, however, that unstructured interviews are actually highly unreliable, or inconsistent between interviews. That means that two interviewers who conduct an interview with the same person may not agree and see the candidate the same way even if they were in the same interview with that applicant. Often interviewers who conduct unstructured interviews fail to identify the high-quality candidates for the job.[54] See the section on interview structure issues for a more in-depth discussion.		Structured		Interview structure is the degree to which interviews are identical and conducted the same across applicants.[48] Also known as guided, systematic, or patterned interviews, structured interviews aim to make both the content (the information addressed as well as the administration of the interaction) and the evaluation (how the applicant is scored) the same no matter what applicant is being interviewed. Specifically, researchers commonly address 15 elements[55] that can be used to make the interview’s content and evaluation process similar. An interview’s degree of structure is often thought of as the extent to which these elements are included when conducting interviews.		Content structure:		Evaluation structure:		Multiple research studies have shown that using these elements to design the interview increases the interview’s ability to identify high-performing individuals. As mentioned, the structure of an interview is on a scale that ranges from unstructured to structured, but it remains unclear which or how many structure elements must be included before the interview can be considered ‘structured.’ Some researchers argue that including at least some, but not all, elements into the interview should be considered “semi-structured.”[56] Others have attempted to create levels of structure, such as Huffcutt, Culbertson, and Weyhrauch’s[57] four levels of structure, which point to varying degrees of standardization in each level. Despite being difficult to say exactly what a structured interview is, structured interviews are widely seen as more preferred over unstructured interviews by organizations if an accurate and consistent measure of an applicant is desired.[57]		Types of questions		Regardless of interview structure, there are several types of questions interviewers ask applicants. Two major types that are used frequently and that have extensive empirical support are situational questions[58] and behavioral questions (also known as patterned behavioral description interviews).[59] Best practices include basing both types of questions on "critical incidents" that are required to perform the job[60] but they differ in their focus (see below for descriptions). Critical incidents are relevant tasks that are required for the job and can be collected through interviews or surveys with current employees, managers, or subject matter experts.[61][55] One of the first critical incidents techniques ever used in the United States Army asked combat veterans to report specific incidents of effective or ineffective behavior of a leader. The question posed to veterans was "Describe the officer’s actions. What did he do?" Their responses were compiled to create a factual definition or "critical requirements" of what an effective combat leader is.[60]		Previous research has found mixed results regarding whether behavioral or situational questions will best predict future job performance of an applicant.[62][63] It is likely that variables unique to each situation, such as the specific criteria being examined,[7] the applicant’s work experience,[9] or the interviewee’s nonverbal behavior[64] make a difference with regard to which question type is the best. It is recommended to incorporate both situational and behavioral questions into the interview to get the best of both question types.[65] The use of high-quality questions represents an element of structure, and is essential to ensure that candidates provide meaningful responses reflective of their capability to perform on the job.[66]		Situational interview questions[58] ask job applicants to imagine a set of circumstances and then indicate how they would respond in that situation; hence, the questions are future oriented. One advantage of situational questions is that all interviewees respond to the same hypothetical situation rather than describe experiences unique to them from their past. Another advantage is that situational questions allow respondents who have had no direct job experience relevant to a particular question to provide a hypothetical response.[67] Two core aspects of the SI are the development of situational dilemmas that employees encounter on the job, and a scoring guide to evaluate responses to each dilemma.[68]		Behavioral (experience-based or patterned behavioral) interviews are past-oriented in that they ask respondents to relate what they did in past jobs or life situations that are relevant to the particular job relevant knowledge, skills, and abilities required for success.[59][69] The idea is that past behavior is the best predictor of future performance in similar situations. By asking questions about how job applicants have handled situations in the past that are similar to those they will face on the job, employers can gauge how they might perform in future situations.[67]		Behavioral interview questions include:[70]		Examples include the STAR and SOARA techniques.		Other possible types of questions that may be asked alongside structured interview questions or in a separate interview include: background questions, job knowledge questions, and puzzle type questions. A brief explanation of each follows.		A case interview is an interview form used mostly by management consulting firms and investment banks in which the job applicant is given a question, situation, problem or challenge and asked to resolve the situation. The case problem is often a business situation or a business case that the interviewer has worked on in real life. In recent years, company in other sectors like Design, Architecture, Marketing, Advertising, Finance and Strategy have adopted a similar approach to interviewing candidates. Technology has transformed the Case-based and Technical interview process from a purely private in-person experience to an online exchange of job skills and endorsements.		Another type of job interview found throughout the professional and academic ranks is the panel interview. In this type of interview the candidate is interviewed by a group of panelists representing the various stakeholders in the hiring process. Within this format there are several approaches to conducting the interview. Example formats include;		The benefits of the panel approach to interviewing include: time savings over serial interviewing, more focused interviews as there is often less time spend building rapport with small talk, and "apples to apples" comparison because each stake holder/interviewer/panelist gets to hear the answers to the same questions.[74]		In the group interview, multiple applicants are interviewed at one time by one or more interviewers. This type of interview can be used for selection, promotion, or assessment of team skills. Interviewers may also use a group interview to assess an applicant’s stress management skills or assertiveness because in such a group setting the applicant will be surrounded by other applicants who also want to get the job. Group interviews can be less costly than one-on-one or panel interviews, especially when many applicants need to be interviewed in a short amount of time. In addition, because fewer interviewers are needed, fewer interviewers need to be trained.[75] These positive qualities of the group interview have made them more popular.[76]		Despite the potential benefits to the group interview, there are problems with this interview format. In group interviews the interviewer has to multitask more than when interviewing one applicant at a time. Interviewers in one-on-one interviews are already busy doing many things. These include attending to what applicants are saying and how they are acting, taking notes, rating applicant responses to questions, and managing what they say and how they act. Interviewing more than one applicant at a time makes it more challenging for the interviewer. This can negatively affect that interviewer and his/her job as interviewer.[77] Another problem with group interviews is that applicants who get questioned later in the interview have more of a chance to think about how to answer the questions already asked by the interviewer. This can give applicants questioned later in the interview an advantage over the earlier-questioned applicants. These problems can make it less likely for group interviews to accurately predict who will perform well on the job.		Group interviews haven’t been studied as much as one-on-one interviews, but the research that has been done suggests that in the field of education group interviews can be an effective method of selection.[78] For example, a 2016 study found that applicants for teaching jobs thought that the group interview was fair.[75] A 2006 study found conflicting findings.[76] These include that applicants in a group interview who were questioned later in the interview gave more complete and higher quality responses and that group interviews were seen as not fair. They also found that group interviews were not as effective as one-on-one interviews. Further research needs to be conducted to more extensively evaluate the group interview’s usefulness for various purposes. This research needs to be done across various domains outside of the education sector. Research also needs to clarify conflicting findings by determining in which situations study results can be applied.		Stress interviews are still in common use. One type of stress interview is where the employer uses a succession of interviewers (one at a time or en masse) whose mission is to intimidate the candidate and keep him/her off-balance. The ostensible purpose of this interview: to find out how the candidate handles stress. Stress interviews might involve testing an applicant's behavior in a busy environment. Questions about handling work overload, dealing with multiple projects, and handling conflict are typical.[79]		Another type of stress interview may involve only a single interviewer who behaves in an uninterested or hostile manner. For example, the interviewer may not make eye contact, may roll his eyes or sigh at the candidate's answers, interrupt, turn his back, take phone calls during the interview, or ask questions in a demeaning or challenging style. The goal is to assess how the interviewee handles pressure or to purposely evoke emotional responses. This technique was also used in research protocols studying stress and type A (coronary-prone) behavior because it would evoke hostility and even changes in blood pressure and heart rate in study subjects. The key to success for the candidate is to de-personalize the process. The interviewer is acting a role, deliberately and calculatedly trying to "rattle the cage". Once the candidate realizes that there is nothing personal behind the interviewer's approach, it is easier to handle the questions with aplomb.		Example stress interview questions:		Candidates may also be asked to deliver a presentation as part of the selection process. One stress technique is to tell the applicant that they have 20 minutes to prepare a presentation, and then come back to room five minutes later and demand that the presentation be given immediately. The "Platform Test" method involves having the candidate make a presentation to both the selection panel and other candidates for the same job. This is obviously highly stressful and is therefore useful as a predictor of how the candidate will perform under similar circumstances on the job. Selection processes in academic, training, airline, legal and teaching circles frequently involve presentations of this sort.		This kind of interview focuses on problem solving and creativity. The questions aim at the interviewee's problem-solving skills and likely show their ability in solving the challenges faced in the job through creativity. Technical interviews are being conducted online at progressive companies before in-person talks as a way to screen job applicants.		Advancements in technology along with increased usage has led to interviews becoming more common through a telephone interview and through videoconferencing than face-to-face. Companies utilize technology in interviews due to their cheap costs, time-saving benefits, and their ease of use.[48] Also, technology allows for a company to recruit more applicants from further away.[80] Although they are being utilized more, it is still not fully understood how technology may affect how well interviewers select the best person for the job when compared to in-person interviews.[81]		Media richness theory states that more detailed forms of communication will be able to better convey complex information. The ability to convey this complexity allows more media rich forms of communication to better handle uncertainty (like what can occur in an interview) than shallower and less detailed communication mediums.[82] Thus, in the job interview context, a face-to-face interview would be more media rich than a video interview due to the amount of data that can be more easily communicated. Verbal and nonverbal cues are read more in the moment and in relation to what else is happening in the interview. A video interview may have a lag between the two participants. Poor latency can influence the understanding of verbal and nonverbal behaviors, as small differences in the timing of behaviors can change their perception. Likewise, behaviors such as eye contact may not work as well. A video interview would be more media rich than a telephone interview due to the inclusion of both visual and audio data. Thus, in a more media rich interview, interviewers have more ways to gather, remember, and interpret the data they gain about the applicants.		So are these new types of technology interviews better? Research on different interview methods has examined this question using media richness theory. According to the theory, interviews with more richness are expected to result in a better outcome. In general, studies have found results are consistent with media richness theory. Applicants’ interview scores and hiring ratings have been found to be worse in phone and video interviews than in face-to-face interviews.[83] Applicants are also seen as less likeable and were less likely to be endorsed for jobs in interviews using video.[84] Applicants have had a say too. They think that interviews using technology are less fair and less job-related.[85] From the interviewers’ view, there are difficulties for the interviewer as well. Interviewers are seen as less friendly in video interviews.[48] Furthermore, applicants are more likely to accept a job after a face-to-face interview than after a telephone or video interview.[80] Due to these findings, companies should weigh the costs and benefits of using technology over face-to-face interviews when deciding on selection methods.		While preparing for an interview, prospective employees usually look at what the job posting or job description says in order to get a better understanding of what is expected of them should they get hired. Exceptionally good interviewees look at the wants and needs of a job posting and show off how good they are at those abilities during the interview to impress the interviewer and increase their chances of getting a job.		Researching the company itself is also a good way for interviewees to impress lots of people during an interview. It shows the interviewer that the interviewee is not only knowledgeable about the company's goals and objectives, but also that the interviewee has done their homework and that they make a great effort when they are given an assignment. Researching about the company makes sure that employees are not entirely clueless about the company they are applying for, and at the end of the interview, the interviewee might ask some questions to the interviewer about the company, either to learn more information or to clarify on some points that they might have found during their research. In any case, it impresses the interviewer and it shows that the interviewee is willing to learn more about the company.		Most interviewees also find that practising answering the most common questions asked in interviews helps them prepare for the real one. It minimizes the chance of their being caught off-guard regarding certain questions, prepares their minds to convey the right information in the hopes of impressing the interviewer, and also makes sure that they do not accidentally say something that might not be suitable in an interview situation.		Interviewees are generally dressed properly in business attire for the interview, so as to look professional in the eyes of the interviewer. They also bring their résumé, cover letter and references to the interview to supply the interviewer the information they need, and to also cover them in case they forgot to bring any of the papers. Items like cellphones, a cup of coffee and chewing gum are not recommended to bring to an interview, as it can lead to the interviewer perceiving the interviewee as unprofessional and in some cases, even rude.		Above all, interviewees should be confident and courteous to the interviewer, as they are taking their time off work to participate in the interview. An interview is often the first time an interviewer looks at the interviewee first hand, so it is important to make a good first impression.[86]		It may not only be what you say in an interview that matters, but also how you say it (e.g., how fast you speak) and how you behave during the interview (e.g., hand gestures, eye contact). In other words, although applicants’ responses to interview questions influence interview ratings,[87] their nonverbal behaviors may also affect interviewer judgments.[88] Nonverbal behaviors can be divided into two main categories: vocal cues (e.g., articulation, pitch, fluency, frequency of pauses, speed, etc.) and visual cues (e.g., smiling, eye contact, body orientation and lean, hand movement, posture, etc.).[89] Oftentimes physical attractiveness is included as part of nonverbal behavior as well.[89] There is some debate about how large a role nonverbal behaviors may play in the interview. Some researchers maintain that nonverbal behaviors affect interview ratings a great deal,[87] while others have found that they have a relatively small impact on interview outcomes, especially when considered with applicant qualifications presented in résumés.[90] The relationship between nonverbal behavior and interview outcomes is also stronger in structured interviews than unstructured,[91] and stronger when interviewees’ answers are of high quality.[90]		Applicants’ nonverbal behaviors may sway interview ratings through the inferences interviewers make about the applicant based on their behavior. For instance, applicants who engage in positive nonverbal behaviors such as smiling and leaning forward are perceived as more likable, trustworthy, credible,[89] warmer, successful, qualified, motivated, competent,[92] and social skills.[93] These applicants are also predicted to be better accepted and more satisfied with the organization if hired.[92]		Applicants’ verbal responses and their nonverbal behavior may convey some of the same information about the applicant.[88] However, despite any shared information between content and nonverbal behavior, it is clear that nonverbal behaviors do predict interview ratings to an extent beyond the content of what was said, and thus it is essential that applicants and interviewers alike are aware of their impact. You may want to be careful of what you may be communicating through the nonverbal behaviors you display.		To hire the best applicants for the job, interviewers form judgments, sometimes using applicants’ physical attractiveness. That is, physical attractiveness is usually not necessarily related to how well one can do the job, yet has been found to influence interviewer evaluations and judgments about how suitable an applicant is for the job. Once individuals are categorized as attractive or unattractive, interviewers may have expectations about physically attractive and physically unattractive individuals and then judge applicants based on how well they fit those expectations.[94] As a result, it typically turns out that interviewers will judge attractive individuals more favorably on job-related factors than they judge unattractive individuals. People generally agree on who is and who is not attractive and attractive individuals are judged and treated more positively than unattractive individuals.[95] For example, people who think another is physically attractive tend to have positive initial impressions of that person (even before formally meeting them), perceive the person to be smart, socially competent, and have good social skills and general mental health.[94]		Within the business domain, physically attractive individuals have been shown to have an advantage over unattractive individuals in numerous ways, that include, but are not limited to, perceived job qualifications, hiring recommendations, predicted job success, and compensation levels.[94] As noted by several researchers, attractiveness may not be the most influential determinant of personnel decisions, but may be a deciding factor when applicants possess similar levels of qualifications.[94] In addition, attractiveness does not provide an advantage if the applicants in the pool are of high quality, but it does provide an advantage in increased hiring rates and more positive job-related outcomes for attractive individuals when applicant quality is low and average.[96]		Vocal Attractiveness Just as physical attractiveness is a visual cue, vocal attractiveness is an auditory cue and can lead to differing interviewer evaluations in the interview as well. Vocal attractiveness, defined as an appealing mix of speech rate, loudness, pitch, and variability, has been found to be favorably related to interview ratings and job performance.[97][98] In addition, the personality traits of agreeableness and conscientiousness predict performance more strongly for people with more attractive voices compared to those with less attractive voices.[97]		As important as it is to understand how physical attractiveness can influence the judgments, behaviors, and final decisions of interviewers, it is equally important to find ways to decrease potential bias in the job interview. Conducting an interview with elements of structure is a one possible way to decrease bias.[99]		An abundance of information is available to instruct interviewees on strategies for improving their performance in a job interview. Information used by interviewees comes from a variety of sources ranging from popular how-to books to formal coaching programs, sometimes even provided by the hiring organization. Within the more formal coaching programs, there are two general types of coaching. One type of coaching is designed to teach interviewees how to perform better in the interview by focusing on how to behave and present oneself. This type of coaching is focused on improving aspects of the interview that are not necessarily related to the specific elements of performing the job tasks. This type of coaching could include how to dress, how to display nonverbal behaviors (head nods, smiling, eye contact), verbal cues (how fast to speak, speech volume, articulation, pitch), and impression management tactics. Another type of coaching is designed to focus interviewees on the content specifically relevant to describing one’s qualifications for the job, in order to help improve their answers to interview questions. This coaching, therefore, focuses on improving the interviewee’s understanding of the skills, abilities, and traits the interviewer is attempting to assess, and responding with relevant experience that demonstrates these skills.[100] For example, this type of coaching might teach an interviewee to use the STAR approach for answering behavioral interview questions.[101]		A coaching program might include several sections focusing on various aspects of the interview. It could include a section designed to introduce interviewees to the interview process, and explain how this process works (e.g., administration of interview, interview day logistics, different types of interviews, advantages of structured interviews). It could also include a section designed to provide feedback to help the interviewee to improve their performance in the interview, as well as a section involving practice answering example interview questions. An additional section providing general interview tips about how to behave and present oneself could also be included.[102]		It is useful to consider coaching in the context of the competing goals of the interviewer and interviewee. The interviewee’s goal is typically to perform well (i.e. obtain high interview ratings), in order to get hired. On the other hand, the interviewer’s goal is to obtain job-relevant information, in order to determine whether the applicant has the skills, abilities, and traits believed by the organization to be indicators of successful job performance.[100] Research has shown that how well an applicant does in the interview can be enhanced with coaching.[100][103][104][105] The effectiveness of coaching is due, in part, to increasing the interviewee’s knowledge, which in turn results in better interview performance. Interviewee knowledge refers to knowledge about the interview, such as the types of questions that will be asked, and the content that the interviewer is attempting to assess.[106] Research has also shown that coaching can increase the likelihood that interviewers using a structured interview will accurately choose those individuals who will ultimately be most successful on the job (i.e., increase reliability and validity of the structured interview).[100] Additionally, research has shown that interviewees tend to have positive reactions to coaching, which is often an underlying goal of an interview.[102] Based on research thus far, the effects of coaching tend to be positive for both interviewees and interviewers.		Interviewers should be aware that applicants can fake their responses during the job interview. Such applicant faking can influence interview outcomes when present. One concept related to faking is impression management (IM; when you intend or do not intend to influence how favorably you are seen during interactions[107]). Impression management can be either honest or deceptive.[108] Honest IM tactics are used to frankly describe favorable experiences, achievements and job-related abilities. Deceptive IM tactics are used to embellish or create an ideal image for the job in question.[109] Honest IM tactics such as self-promotion (positively highlighting past achievements and experiences) may be considered necessary by interviewers in the interview context. Consequently, candidates who do not use these tactics may be viewed as disinterested in the job. This can lead to less favorable ratings.[110] Faking can then be defined as "deceptive impression management or the intentional distortion of answers in the interview in order to get better interview ratings and/or otherwise create favorable perceptions".[108] Thus, faking in the employment interview is intentional, deceptive, and aimed at improving perceptions of performance.		Faking in the employment interview can be broken down into four elements.[108] The first involves the interviewee portraying him or herself as an ideal job candidate by exaggerating true skills, tailoring answers to better fit the job, and/or creating the impression that personal beliefs, values, and attitudes are similar to those of the organization.		The second aspect of faking is inventing or completely fabricating one’s image by piecing distinct work experiences together to create better answers, inventing untrue experiences or skills, and portraying others’ experiences or accomplishments as one's own.		Thirdly, faking might also be aimed at protecting the applicant’s image. This can be accomplished through omitting certain negative experiences, concealing negatively perceived aspects of the applicant’s background, and by separating oneself from negative experiences.		The fourth and final component of faking involves ingratiating oneself to the interviewer by conforming personal opinions to align with those of the organization, as well as insincerely praising or complimenting the interviewer or organization.		Of all of the various faking behaviors listed, ingratiation tactics were found to be the most prevalent in the employment interview, while flat out making up answers or claiming others’ experiences as one’s own is the least common.[108] However, fabricating true skills appears to be at least somewhat prevalent in employment interviews. One study found that over 80% of participants lied about job-related skills in the interview,[111] presumably to compensate for a lack of job-required skills/traits and further their chances for employment.		Most importantly, faking behaviors have been shown to affect outcomes of employment interviews. For example, the probability of getting another interview or job offer increases when interviewees make up answers.[108]		Different interview characteristics also seem to impact the likelihood of faking. Faking behavior is less prevalent, for instance, in past behavioral interviews than in situational interviews, although follow-up questions increased faking behaviors in both types of interviews. Therefore, if practitioners are interested in decreasing faking behaviors among job candidates in employment interview settings, they should utilize structured, past behavioral interviews and avoid the use of probes or follow-up questions.[108]		The interview is a social process and individual differences of the interviewer and applicant may impact the result of the interview.		Interviewees may differ on any number of dimensions commonly assessed by job interviews and evidence suggests that these differences affect interview ratings. Many interviews are designed to measure some specific differences between applicants, or individual difference variables, such as Knowledge, Skills, and Abilities needed to do the job well. Other individual differences can affect how interviewers rate the applicants even if that characteristic is not meant to be assessed by the interview questions.[112] For instance, General Mental Ability G factor (psychometrics) is moderately related to structured interview ratings and strongly related to structured interviews using behavioral description and situational judgment interview questions, because they are more cognitively intensive interview types.[113][114] Other individual differences between people, such as extraversion and emotional intelligence, are also commonly measured during a job interview because they are related to verbal ability, which may be useful for jobs that involve interacting with people.[113] Many individual difference variables may be linked to interview performance because they reflect applicants’ genuine ability to perform better in cognitively and socially demanding situations. For instance, someone with high general mental ability may perform better in a cognitively demanding situation, such as a job interview, which requires quick thinking and responding. Similarly, someone with strong social skills may perform better in a job interview, as well as other social situations, because they understand how to act correctly. Thus, when an applicant performs well in an interview due to higher general mental abilities or better social skills, it is not necessarily undesirable, because they may also perform better when they are faced with situations on the job in which those skills would be valuable. On the other hand, not all individual difference variables that lead to higher interview performance would be desirable on the job. Some individual difference variables, such as those that are part of the dark triad, can lead to increased interview ratings, initially, but may not be reflective of actual KSAOs that would help the individual to perform better once hired.		Individuals who are high in Machiavellianism may be more willing and more skilled at faking and less likely to give honest answers during interviews.[115][116][117] Individuals high in Machiavellianism have stronger intentions to use faking in interviews compared to psychopaths or narcissists and are also more likely to see the use of faking in interviews as fair.[118][119] Men and women high in Machiavellianism may use different tactics to influence interviewers. In one study, which examined the how much applicants allowed the interviewers to direct the topics covered during the interview, women high Machiavellianism tended to allow interviewers more freedom to direct the content of the interview. Men high in Machiavellianism, on the other hand, gave interviewers the least amount of freedom in directing the content of the interview.[120] Men high in Machiavellianism were also more likely to make-up information about themselves or their experiences during job interviews.[121] Thus, while individuals high in Machiavellianism may appear to do well in interviews, this seems to be largely because they give untrue responses and because they want to control interpersonal interactions.		Narcissists typically perform well at job interviews, with narcissists receiving more favorable hiring ratings from interviewers than individuals who are not narcissists.[122] Even more experienced and trained raters evaluate narcissists more favorably.[123][124] This is perhaps because interviews are one of the few social situations where narcissistic behaviors, such as boasting actually create a positive impression, though favorable impressions of narcissists are often short-lived.[125] Interviewers’ initial impressions of narcissistic applicants are formed primarily on the basis of highly visible cues, which makes them susceptible to biases.[126] Narcissists are more skilled at displaying likable cues, which lead to more positive first impressions, regardless of their long-term likability or job performance. Upon first meeting narcissists, people often rate them as more agreeable, competent, open, entertaining, and well-adjusted. Narcissists also tend to be neater and flashier dressers, display friendlier facial expressions, and exhibit more self-assured body movements.[127] Importantly, while narcissistic individuals may rate their own job performance more favorably, studies show that narcissism is not related to job performance.[128] Thus, while narcissists may seem to perform better and even be rated as performing better in interviews, these more favorable interview ratings are not predictive of favorable job performance, as narcissists do not actually perform better in their jobs than non-narcissists.		Corporate psychopaths are readily recruited into organizations because they make a distinctly positive impression at interviews.[129] They appear to be alert, friendly and easy to get along with and talk to. They look like they are of good ability, emotionally well adjusted and reasonable, and these traits make them attractive to those in charge of hiring staff within organizations. Unlike narcissists, psychopaths are better able to create long-lasting favorable first impressions, though people may still eventually see through their facades.[130] Psychopaths’ undesirable personality traits may be easily misperceived by even skilled interviewers. For instance, their irresponsibility may be misconstrued by psychopaths as risk-taking or entrepreneurial spirit. Their thrill-seeking tendencies may be conveyed as high energy and enthusiasm for the job or work. Their superficial charm may be misinterpreted by interviewers as charisma.[130][131] It is worth noting that psychopaths are not only accomplished liars, they are also more likely to lie in interviews.[117] For instance, psychopaths may create fictitious work experiences or resumes.[130] They may also fabricate credentials such as diplomas, certifications, or awards.[130] Thus, in addition to seeming competent and likable in interviews, psychopaths are also more likely to outright make-up information during interviews than non-psychopaths.		There are many differences about interviewers that may affect how well they conduct an interview and make decisions about applicants. A few of them are how much experience they have as an interviewer, their personality, and intelligence.[132] To date, it is not clear how experience effects the results of interviews. In some cases, prior experience as an interviewer leads them to use more of the information provided by the applicant to decide if an applicant is right for the job intelligence.[132] In other cases, the experience of the interviewer did not help them make more accurate decisions.[133] One reason for the different results could be the type of experience the interviewer had.[134] Also, other differences in the interviewer, such as personality or intelligence, could be a reason why results vary.[134]		The mental ability of interviewers may play a role in how good they are as interviewers. Higher mental ability is important because during the interview, a lot of information needs to be processed – what the applicant said, what they meant, what it means for how they can do the job, etc. Research has shown that those higher in general mental ability were more accurate when judging the personality of others.[135] Also, interviewers who have higher social intelligence and emotional intelligence seem to do a better job of understanding how an applicant behaves in an interview and what that means for how they will act once on the job.[136] These abilities do not appear to be enough on their own to make accurate judgements.[137]		The personality of the interviewer may also affect the ratings they give applicants. There are many ways that personality and social skills can impact one’s ability to be a good judge or interviewer. Some of the specific social skills good judges display are warmth, interest in engaging with others, and eye contact.[136] Interviewers who display warm behaviors, such as smiling and leaning toward the applicant, are rated more positively than those who don’t act this way or show cold behaviors.[138] Interviewers who prefer to engage with others also tend to judge applicants more accurately.[139] It is likely that these people are using information from their own personalities as well as how they see people in general to help them be more accurate.[139]		There is extant data which puts into question the value of job interviews as a tool for selecting employees. Where the aim of a job interview is ostensibly to choose a candidate who will perform well in the job role, other methods of selection provide greater predictive power and often lower costs.[140]		As discussed previously, interviews with more structure are considered best practice, as they tend to result in much better decisions about who will be a good performing employee than interviews with less structure.[141] Structure in an interview can be compared to the standardization of a typical paper and pencil test: It would be considered unfair if every test taker were given different questions and a different number of questions on an exam, or if their answers were each graded differently. Yet this is exactly what occurs in an unstructured interview; interviewers decide the number and content of questions, rate responses using whatever strategy they want (e.g., relying on intuition, or using overall ratings at the end of the interview rather than after each time the applicant responds), and may score some applicants more harshly than others. Thus, interviewers who don’t consider at least a moderate amount of structure may make it hard for an organization’s interview to effectively select candidates that best fit the work needs of the organization.		In terms of reliability, meta-analytic results provided evidence that interviews can have acceptable levels of interrater reliability, or consistent ratings across interviewers interrater reliability (i.e. .75 or above), when a structured panel interview is used.[142] In terms of criterion-related validity, or how well the interview predicts later job performance criterion validity, meta-analytic results have shown that when compared to unstructured interviews, structured interviews have higher validities, with values ranging from .20-.57 (on a scale from 0 to 1), with validity coefficients increasing with higher degrees of structure.[141][143][144] That is, as the degree of structure in an interview increases, the more likely interviewers can successfully predict how well the person will do on the job, especially when compared to unstructured interviews. In fact, one structured interview that included a) a predetermined set of questions that interviewers were able to choose from, and b) interviewer scoring of applicant answers after each individual question using previously created benchmark answers, showed validity levels comparable to cognitive ability tests (traditionally one of the best predictors of job performance) for entry level jobs.[141]		Honesty and integrity are attributes that can be very hard to determine using a formal job interview process: the competitive environment of the job interview may in fact promote dishonesty. Some experts on job interviews express a degree of cynicism towards the process.[who?]		Applicant reactions to the interview process include specific factors such as; fairness, emotional responses, and attitudes toward the interviewer or the organization.[145] Though the applicant's perception of the interview process may not influence the interviewer(s) ability to distinguish between individuals' suitability, applicants reactions are important as those who react negatively to the selection process are more likely to withdraw from the selection process.[146][147][148] They are less likely to accept a job offer, apply on future occasions,[149] or to speak highly of the organization to others and to be a customer of that business.[146][147][150] Compared to other selection methods, such as personality or cognitive ability tests, applicants, from different cultures may have positive opinions about interviews.[146][151]		Interview design can influence applicants' positive and negative reactions, though research findings on applicants preferences for structured compared to unstructured interviews appear contradictory.[48][152] Applicants' negative reactions to structured interviews may be reduced by providing information about the job and organization.[153] Providing interview questions to applicants before the interview, or telling them how their answers will be evaluated, are also received positively.[154]		The type of questions asked can affect applicant reactions. General questions are viewed more positively than situational or behavioral questions[155] and 'puzzle' interview questions may be perceived as negative being perceived unrelated to the job, unfair, or unclear how to answer.[156] Using questions that discriminating unfairly in law unsurprisingly are viewed negatively with applicants less likely to accept a job offer, or to recommend the organization to others.[157]		Some of the questions and concerns on the mind of the hiring manager include:		A sample of intention behind questions asked for understanding observable responses, displayed character, and underlying motivation:		The 'friendliness' of the interviewer may be equated to fairness of the process and improve the liklihood of accepting a job offer,[158] and face-to-face interviews compared to video conferencing and telephone interviews.[159] In video conferencing interviews the perception of the interviewer may be viewed as less personable, trustworthy, and competent.[160]		Interview anxiety refers to having unpleasant feelings before or during a job interview.[161] It also reflects the fear of partaking in an interview.[162] Job candidates may feel this increased sense of anxiety because they have little to no control over the interview process.[163] It could also be because they have to speak with a stranger.[164] Due to this fear, anxious candidates display certain behaviors or traits that signal to the interviewer that they are anxious. Examples of such behaviors include frequent pauses, speaking more slowly than usual, and biting or licking of lips.[165]		Research has identified five dimensions of interview anxiety: communication anxiety, social anxiety, performance anxiety, behavioral anxiety and appearance anxiety.[166] Further research shows that both the interviewer and applicant agree that speaking slowly is a clear sign of interview anxiety. However, they do not agree on other anxiety indicators such as frequent pauses and biting or licking of lips.[165] Trait judgments are also related to interview anxiety and can affect interviewer perceptions of anxiety. Low assertiveness has been identified as the key trait related to interview anxiety. Thus, the most important indicators of interview anxiety are slow speech rate and low assertiveness.[165]		Another issue in interview anxiety is gender differences. Although females report being more anxious than males in interviews, their anxiety is not as readily detected as that for males. This can be explained by the Sex-Linked Anxiety Coping Theory (SCT). This theory suggests that females cope better than males when they are anxious in interviews.[167]		Whether anxieties come from individual differences or from the interview setting, they have important costs for job candidates. These include: limiting effective communication and display of future potential,[168] reducing interview performance and evaluation despite potential fit for the job,[161] and reducing the chance of a second interview compared to less anxious individuals.[169] Speaking slowly and low assertiveness have the strongest negative impact on perceptions of interview anxiety. Thus, candidates who experience anxiety in interviews should try to display assertive behaviors such as being dominant, professional, optimistic, attentive and confident[165] In addition, they should speak at a consistent pace that is not unusually slow.		Applicants who view the selection process more favorably tend to be more positive about the organization, and are likely to influence an organization’s reputation.[161][170] whereas, in contrast, anxious or uncomfortable during their interview may view an organization less favorably, causing the otherwise qualified candidates not accepting a job offer.[161] If an applicant is nervous, they might not act the same way they would on the job, making it harder for organizations to use the interview for predicting someone’s future job performance.[161]		In many countries laws are put into place to prevent organizations from engaging in discriminatory practices against protected classes when selecting individuals for jobs.[171] In the United States, it is unlawful for private employers with 15 or more employees along with state and local government employers to discriminate against applicants based on the following: race, color, sex (including pregnancy), national origin, age (40 or over), disability, or genetic information (note: additional classes may be protected depending on state or local law). More specifically, an employer cannot legally "fail or refuse to hire or to discharge any individual, or otherwise discriminate against any individual with respect to his compensation, terms, conditions, or privilege of employment" or "to limit, segregate, or classify his employees or applicants for employment in any way which would deprive or tend to deprive any individual of employment opportunities or otherwise adversely affect his status as an employee."[172][173]		The Civil Rights Act of 1964 and 1991 (Title VII) were passed into law to prevent the discrimination of individuals due to race, color, religion, sex, or national origin. The Pregnancy Discrimination Act was added as an amendment and protects women if they are pregnant or have a pregnancy-related condition.[174]		The Age Discrimination in Employment Act of 1967 prohibits discriminatory practice directed against individuals who are 40 years of age and older. Although some states (e.g. New York) do have laws preventing the discrimination of individuals younger than 40, no federal law exists.[175]		The Americans with Disabilities Act of 1990 protects qualified individuals who currently have or in the past have had a physical or mental disability (current users of illegal drugs are not covered under this Act). A person is a cripple if he has a disability that substantially limits a major life activity, has a history of a disability, is regarded by others as being disabled, or has a physical or mental impairment that is not transitory (lasting or expected to last six months or less) and minor. In order to be covered under this Act, the individual must be qualified for the job. A qualified individual is "an individual with a disability who, with or without reasonable accommodation, can perform the essential functions of the employment position that such individual holds or desires."[176] Unless the disability poses an "undue hardship," reasonable accommodations must be made by the organization. "In general, an accommodation is any change in the work environment or in the way things are customarily done that enables an individual with a disability to enjoy equal employment opportunities."[176] Examples of reasonable accommodations are changing the workspace of an individual in a wheelchair to make it more wheelchair accessible, modifying work schedules, and/or modifying equipment.[177] Employees are responsible for asking for accommodations to be made by their employer.[174]		The most recent law to be passed is Title II of the Genetic Information Nondiscrimination Act of 2008. In essence, this law prohibits the discrimination of employees or applicants due to an individual’s genetic information and family medical history information.		In rare circumstances, it is lawful for employers to base hiring decisions on protected class information if it is considered a Bona Fide Occupational Qualification, that is, if it is a "qualification reasonably necessary to the normal operation of the particular business." For example, a movie studio may base a hiring decision on age if the actor they are hiring will play a youthful character in a film.[178]		Given these laws, organizations are limited in the types of questions they legally are allowed to ask applicants in a job interview. Asking these questions may cause discrimination against protected classes, unless the information is considered a Bona Fide Occupational Qualification. For example, in the majority of situations it is illegal to ask the following questions in an interview as a condition of employment:		Applicants with disabilities may be concerned with the effect that their disability has on both interview and employment outcomes. Research has concentrated on four key issues: how interviewers rate applicants with disabilities, the reactions of applicants with disabilities to the interview, the effects of disclosing a disability during the interview, and the perceptions different kinds of applicant disabilities may have on interviewer ratings.		The job interview is a tool used to measure constructs or overall characteristics that are relevant for the job. Oftentimes, applicants will receive a score based on their performance during the interview. Research has found different findings based on interviewers’ perceptions of the disability. For example, some research has found a leniency effect (i.e., applicants with disabilities receive higher ratings than equally qualified non-disabled applicants) in ratings of applicants with disabilities[181][182] Other research, however, has found there is a disconnect between the interview score and the hiring recommendation for applicants with disabilities. That is, even though applicants with disabilities may have received a high interview score, they are still not recommended for employment.[183][184] The difference between ratings and hiring could be detrimental to a company because they may be missing an opportunity to hire a qualified applicant.		A second issue in interview research deals with the applicants’ with disabilities reactions to the interview and applicant perceptions of the interviewers. Applicants with disabilities and able-bodied applicants report similar feelings of anxiety towards an interview.[185] Applicants with disabilities often report that interviewers react nervously and insecurely, which leads such applicants to experience anxiety and tension themselves. The interview is felt to be the part of the selection process where covert discrimination against applicants with disabilities can occur.[185] Many applicants with disabilities feel they cannot disclose (i.e., inform potential employer of disability) or discuss their disability because they want to demonstrate their abilities. If the disability is visible, then disclosure will inevitably occur when the applicant meets the interviewer, so the applicant can decide if they want to discuss their disability. If an applicant has a non-visible disability, however, then that applicant has more of a choice in disclosing and discussing. In addition, applicants who were aware that the recruiting employer already had employed people with disabilities felt they had a more positive interview experience.[185] Applicants should consider if they are comfortable with talking about and answering questions about their disability before deciding how to approach the interview.		Research has also demonstrated that different types of disabilities have different effects on interview outcomes. Disabilities with a negative stigma and that are perceived as resulting from the actions of the person (e.g., HIV-Positive, substance abuse) result in lower interview scores than disabilities for which the causes are perceived to be out of the individual’s control (e.g., physical birth defect).[184] A physical disability often results in higher interviewer ratings than psychological (e.g., mental illness) or sensory conditions (e.g., Tourette Syndrome).[182][186] In addition, there are differences between the effects of disclosing disabilities that are visible (e.g., wheelchair bound) and non-visible (e.g., Epilepsy) during the interview. When applicants had a non-visible disability and disclosed their disability early in the interview they were not rated more negatively than applicants who did not disclose. In fact, they were liked more than the applicants who did not disclose their disability and were presumed not disabled.[187] Interviewers tend to be impressed by the honesty of the disclosure.[186] Strong caution needs to be taken with applying results from studies about specific disabilities, as these results may not apply to other types of disabilities. Not all disabilities are the same and more research is needed to find whether these results are relevant for other types of disabilities.		Some practical implications for job interviews for applicants with disabilities include research findings that show there are no differences in interviewer responses to a brief, shorter discussion or a detailed, longer discussion about the disability during the interview.[186] Applicants, however, should note that when a non-visible disability is disclosed near the end of the interview, applicants were rated more negatively than early disclosing and non-disclosing applicants. Therefore, it is possible that interviewers feel individuals who delay disclosure may do so out of shame or embarrassment. In addition, if the disability is disclosed after being hired, employers may feel deceived by the new hire and reactions could be less positive than would have been in the interview.[188] If applicants want to disclose their disability during the interview, research shows that a disclosure and/or discussion earlier in the interview approach may afford them some positive interview effects.[189] The positive effects, however, are preceded by the interviewers perception of the applicants’ psychological well-being. That is, when the interviewer perceives the applicant is psychologically well and/or comfortable with his or her disability, there can be positive interviewer effects. In contrast, if the interviewer perceives the applicant as uncomfortable or anxious discussing the disability, this may either fail to garner positive effect or result in more negative interview ratings for the candidate. Caution must again be taken when applying these research findings to other types of disabilities not investigated in the studies discussed above. There are many factors that can influence the interview of an applicant with a disability, such as whether the disability is physical or psychological, visible or non-visible, or whether the applicant is perceived as responsible for the disability or not. Therefore, applicants should make their own conclusions about how to proceed in the interview after comparing their situations with those examined in the research discussed here.		Although it is illegal for employers to ask about applicants’ arrest record during an interview as a deciding factor in applicant hiring decisions, employers do have the right to obtain information about applicants’ criminal convictions before hiring, including during the interview phase.[190] Many companies consider hiring applicants with criminal history a liability. For instance, if a company hired someone with an assault charge and that person later assaulted another employee or vendor, some people would say that the company was liable or legally responsible for not maintaining a safe work environment. Although the legalities are more complex, this potential responsibility an organization may carry often is a reason why many companies conduct criminal background checks. When making hiring decisions that somewhat depend on one’s criminal background, employers must consider the following:		Although not much research has been conducted to examine whether applicants should talk about their criminal histories or not, a 2012 study[195] found that employers were more likely to hire someone with a criminal record if the applicant made face-to-face contact with the employer and was prepared and willing to discuss his/her job related knowledge. Applicants also had an increased chance of being hired if they discussed what they learned from their experience in the justice system, as well as how they were rehabilitated, during the interview. This study found that employers preferred applicants that revealed their criminal records upfront and were willing to take responsibility for their actions.[195]		Ban the Box is a campaign to remove the question about criminal history from job applications as an opportunity to give people with criminal histories a reasonable chance in the employment selection process. By allowing applicants to be interviewed before disclosing their criminal histories, this campaign seeks to increase the number of applicants with criminal histories in the workplace.[196] The campaign focuses on how discrimination in the recruiting phase of selection makes it harder for people with criminal convictions to obtain employment. Not having employment makes it harder for people with criminal histories to support their families, and a lack of a job can lead to an increased chance of the person becoming a repeat offender.[197]		Job applicants who are underweight (to the point of emaciation), overweight or obese may face discrimination in the interview.[198][199] The negative treatment of overweight and obese individuals may stem from beliefs that weight is controllable and those who fail to control their weight are lazy, unmotivated, and lack self-discipline.[200][201] Underweight individuals may also be subject to appearance-related negative treatment.[199] Underweight, overweight and obese applicants are not protected from discrimination by any current United States laws.[198] However, some individuals who are morbidly obese and whose obesity is due to a physiological disorder may be protected against discrimination under the Americans with Disabilities Act.[202]		Discrimination against pregnant applicants is illegal under the Pregnancy Discrimination Act of 1978, which views pregnancy as a temporary disability and requires employers to treat pregnant applicants the same as all other applicants.[203] Yet, discrimination against pregnant applicants continues both in the United States and internationally.[203][204] Research shows that pregnant applicants compared to non-pregnant applicants are less likely to be recommended for hire.[205][206] Interviewers appear concerned that pregnant applicants are more likely than non-pregnant applicants to miss work and even quit.[206] Organizations who wish to reduce potential discrimination against pregnant applicants should consider implementing structured interviews, although some theoretical work suggests interviewers may still show biases even in these types of interviews.[205][207]		Employers are using social networking sites like Facebook and LinkedIn to obtain additional information about job applicants.[208][209][210] While these sites may be useful to verify resume information, profiles with pictures also may reveal much more information about the applicant, including issues pertaining to applicant weight and pregnancy.[211] Some employers are also asking potential job candidates for their social media logins which has alarmed many privacy watch dogs and regulators.[212]		As with the common comparisons between Eastern and Western cultures, interviews and the constructs assessed by the interview have been found to differ across the world. For example, studies of the United States of America (USA) to Canada have found conflicting results in average levels of agreeableness in each country.[213] People tend to use social comparison when reporting their own level of agreeableness.[213] Even though Canadians are likely to be more agreeable, they might score similarly to those individuals from the USA.[213] In situations where social comparison is a factor, an honest answer could result in under- or over-estimation.		Because of these cultural differences, more businesses are adding cross-cultural training to their HR training.[214][215] The goal of cross-cultural training is to improve one’s ability to adapt and judge people from other cultures. This training is a first step in ensuring the process of using the job interview to decide whom to hire works the same in a selection situation where there are cross-cultural factors.		One cultural difference in the job interview is in the type of questions applicants will expect and not expect to be asked.[216] Interviewers outside the USA often ask about family, marital status and children.[216] These types of questions are usually not allowed by USA job laws but acceptable in other countries. Applicants can be surprised by questions interviewers ask them that are not appropriate or consistent with their own cultures. For example, in Belgium and Russia interviewers are unlikely to ask about an applicant’s personal values, opinions and beliefs.[216] Thus, USA interviewers who do ask applicants about their values can make non-USA applicants uneasy or misinterpret the reason they are not prepared.		Another difference is in the consistency with which common constructs, even those that generalize across cultures, predict across different countries and cultures. For example, those who seem high in Agreeableness can do less well on the job in European workplaces.[214] But those high in Agreeableness in the USA or Japan will do better on the job as measured on the same criteria.[214] In some cases the structured Behavior Description Interview (BDI) that predicts who will do well on the job in some countries, from their interview scores, fails to predict accurately which applicants to hire in other countries.[214]		There are a few ways that cross-cultural differences can mess up the results of our attempts to predict job performance.[217] The first source of error is construct bias, the possibility that the construct being measured is viewed differently by those from another culture, if it exists at all. One way this could happen is if the behaviors a person displays, that go with that construct, are viewed differently in different cultures. It could also be the extent to which the construct even exists in their country. For example, the Multidimensional Work Ethic Profile (MWEP), is a scale demonstrated to work across many countries.[218][219][220] However, in China the MWEP concept/dimension of Leisure has been shown to have poor equivalence with other countries, and may be a culturally inappropriate assessment due to the Confucian concept of hard work without leisure.[221] Research has shown that differences in the levels of established cross-cultural constructs such as Cultural Tightness-Looseness increase or decrease the effect of the Five Factor Model personality traits.[222] Tight cultures have strong social norms and adherence coupled with low tolerance for behavior that deviates from those norms, and loose cultures are the opposite with weak norms and high tolerance for deviance.[223] An interviewer from a tight culture might view the normal behaviors of a loose cultured interviewee as signs of a poor moral character despite the behavior being normal. As such, differences between the tightness-looseness of the interviewer’s and interviewee’s home countries can introduce method bias, negatively affecting the interviewer’s assessment of interviewee answers and behaviors. First construct bias must be measured by comparing groups of persons from distinct cultures and comparing if any real differences are discovered. Then information on those differences can be used to make the adjustments needed to allow the construct to measure what it is intended to measure in people from a different culture.		Response bias is another cross-cultural difference that has been shown to affect how we measure constructs and interpret the results.[224] Social desirability bias is a tendency to give a socially acceptable answer, even if it is a lie, because we want to look good. Giving socially acceptable, but part or completely false, answers can inflate interview scores.[225] One simple example of socially acceptable answers is called acquiescence bias, which is a tendency to agree with all questions with positive meaning.[225] People also have been found to show different attitudes towards answers on the extreme high and low end of a set of options (extremely agree or extremely disagree).[225] In some cases, people from a different cultures may just be unfamiliar with a word (term, concept, context) or with a type of question.[225] Another research study found that self and other reports of conscientiousness failed to relate with expected job behaviors across cultures, demonstrating that one of the most predictive constructs in the USA is tied to aspects of USA culture that may not be present in a different type of culture.[213]		For example, in the West, applicants prefer to eliminate details and focus on the larger issue, tending towards a comprehensive evaluation starting from individual elements then moving towards the whole.[226] In Japan, a respondent would go from the general to the specific in answering, preferring to divide a problem and analyze it piece by piece. Likewise, there are differences between individualist and collectivist cultures in the types of answers they chose. When given a series of options, individualists tend to choose the task oriented option that involves direct communication with others.[226] Yet collectivists choose the option that sees group harmony and protecting or saving face for others as more important.[226] These differences can introduce method bias when interviewers evaluate or score how the applicant did in the interview. This is why it is important to understand how and why the best answer in one culture is not the best elsewhere. It might even be completely wrong.		There is also item bias introduced by the actual items or questions in an interview. Poor item translation can be a problem.[217] This might be incorrectly translating the same item to another language such as in an organization that hires both English and Spanish speaking employees. Or it might be in someone not understanding the wording of an item because they are not native to that country’s language. Similar to construct bias, the wording of an item can result in measuring different traits because of different meanings in the two different cultures.		
Permanent employees, regular employees or the directly employed work for an employer and are paid directly by that employer. Permanent (regular) employees do not have a predetermined end date to employment. In addition to their wages, they often receive benefits like subsidized health care, paid vacations, holidays, sick time, or contributions to a retirement plan. Permanent employees are often eligible to switch job positions within their companies. Even when employment is "at will", permanent employees of large companies are generally protected from abrupt job termination by severance policies, like advance notice in case of layoffs, or formal discipline procedures. They may be eligible to join a union, and may enjoy both social and financial benefits of their employment.		With exception of South Korea where extensive laws and regulations make firing of permanent employees nearly impossible, rarely permanent employment means employment of an individual that is guaranteed throughout the employee's working life. In the private sector, such jobs are rare; permanent employment is far more common in the public sector, where profit and loss is not as important.						In Japan, the concept of lifetime employment (終身雇用, shūshin koyō) originated in large companies around 1910 but became widespread in its strongest form during the economic growth period following World War II, beginning around 1955. Labor unions had reacted strongly to mass dismissals in the late 1940s and early 1950s, and court precedents restricted employers' rights to dismiss employees due to business difficulty.[1]		Large-scale layoffs remain legally restricted and socially taboo in Japan; many large companies such as Sony, Panasonic, NEC and Toshiba deal with excess labor capacity by offering voluntary retirement programs and by sending unnecessary workers to "banishment rooms" where they are held with minimal work responsibilities until they decide to resign. The Japanese government under Prime Minister Shinzō Abe began examining possible loosening of the restrictions on layoffs in 2013.[2]				
Day care, daycare,[1][2] child day care, or childcare is the care of a child during the day by a person other than the child's legal guardians, typically performed by someone outside the child's immediate family. Day care is typically an ongoing service during specific periods, such as the parents' time at work.		Day care can also refer to daytime care for disabled or elderly people in both UK and US English,[3][4] so child day care is often preferable at first mention.		The service is known as day care[5][4] or childcare[6][7][8] in the United Kingdom, North America, and Australia and as crèche in Ireland and New Zealand. According to Oxford Living Dictionaries, child care in two words can in addition have the broader meaning of the care of a child by anyone, including the parents,[9] but US dictionaries do not record that spelling or meaning.[7][10][8] In English-speaking and other conservative countries, the vast majority of childcare is still performed by the parents, in-house nannies or through informal arrangements with relatives, neighbors or friends, but most children are in daycare centers for most of the day in Nordic Countries, for example. Child care in the child's own home is traditionally provided by a nanny or au pair, or by extended family members including grandparents, aunts and uncles. Child care is provided in nurseries or crèches or by a nanny or family child care provider caring for children in their own homes. It can also take on a more formal structure, with education, child development, discipline and even preschool education falling into the fold of services.		The day care industry is a continuum from personal parental care to large, regulated institutions. Some childminders care for children from several families at the same time, either in their own home (commonly known as "family day care" in Australia) or in a specialized child care facility. Some employers provide nursery provisions for their employees at or near the place of employment. For-profit day care corporations often exist where the market is sufficiently large or there are government subsidies. Research shows that not-for-profits are much more likely to produce the high quality environments in which children thrive."[11] Local governments, often municipalities, may operate non-profit day care centers. For all providers, the largest expense is labor. Local legislation may regulate the operation of daycare centers, affecting staffing requirements. In Canada, the workforce is predominantly female (95%) and low paid, averaging only 60% of average workforce wage. Some jurisdictions require licensing or certification. Legislation may specify details of the physical facilities (washroom, eating, sleeping, lighting levels, etc.).		Independent studies suggest that good daycare is not harmful.[12] In some cases, good daycare can provide different experiences than parental care does, especially when children reach two and are ready to interact with other children. Children in higher quality childcare had somewhat better language and cognitive development during the first 4½ years of life than those in lower quality care.						Day care appeared in France about 1840, and the Société des Crèches was recognized by the French government in 1869. Originating in Europe in the late 18th and early 19th century, day cares were established in the United States by private charities in the 1850s, such as the Charity Organization Society founded by Ansley Wilcox. The Fitch Creche in Buffalo, New York was known as the first day center for working mothers in the United States. Another at that time was the New York Day Nursery in 1854.		More contemporary proposals for government advancement of day care in the United States have experienced a checkered path, for example, in 1971, the Comprehensive Child Development Act was passed by Congress, but was vetoed by Richard Nixon. It "would have created nationally funded child care centers providing early childhood services and after-school care, as well as nutrition, counseling, and even medical and dental care. The centers would charge parents on a sliding scale."[13] Various proposals have been considered, but to date, none leading to legislation that would establish a national policy supporting day care in the United States.		The day care industry is a continuum from personal parental care to large, regulated institutions.		The vast majority of childcare is still performed by the parents, in-house nanny or through informal arrangements with relatives, neighbors or friends. For example, in Canada, among two parent families with at least one working parent, 62% of parents handle the childcare themselves, 32% have other in-home care (nannies, relatives, neighbours or friends) and only 6.5% use a formal day care center.[14]		However, for-profit day care corporations often exist where the market is sufficiently large or there are government subsidies. For instance, in North America, KinderCare Learning Centers, one of the largest of such companies, has approximately 1,600 centers located in 39 states and the District of Columbia.[15] Bright Horizons Family Solutions another of the largest has over 600 daycare centers.[16] Similarly the Australian government's childcare subsidy has allowed the creation of a large private-sector industry in that country.[17]		Another factor favoring large corporate daycares is the existence of childcare facilities in the workplace. Large corporations will not handle this employee benefit directly themselves and will seek out large corporate providers to manage their corporate daycares. Most smaller, for-profit daycares operate out of a single location.		In general, the geographic limitations and the diversity in type of daycare providers make child daycare a highly fragmented industry. The largest providers own only a very small share of the market. This leads to frustration for parents who are attempting to find quality child daycare, with 87% of them describing the traditional search for child daycare as "difficult and frustrating".[citation needed]		"Considerable research has accumulated showing that not-for-profits are much more likely to produce the high quality environments in which children thrive."[11] Not-for-profit organizations are more likely to provide good services to a vulnerable population under conditions that are very hard to monitor or measure.		Local governments, often municipalities, may operate non-profit day care centers. In non-profits, the title of the most senior supervisor is typically "executive director", following the convention of most non-profit organizations.		Family child care homes can be operated by a single individual out of their home. In most states, the legal age of 18 is only required. There may be occasions when more than one individual cares for children in a family childcare home. This can be a stay-at-home parent who seeks supplemental income while caring for their own child. There are also many family childcare providers who have chosen this field as a profession. Both state and county agency legislation regulate the ratios (number and ages of children) allowed per family child care home. Some counties have more stringent quality standards that require licensing for family child care homes while other counties require little or no regulations for childcare in individuals' homes. Some family child care homes operate illegally with respect to tax legislation where the care provider does not report fees as income and the parent does not receive a receipt to qualify for childcare tax deductions. However, licensing a family child care home is beneficial for family child care home providers so that they can have access to financial benefits from their state government, or the federal government where they are allowed to accept children from parents who meet the criterion to benefit from the government childcare subsidy funding. Examples of such benefits are: free Professional Development and training courses, Child And Adult Care Food Program (which allows eligible childcare and family childcare home providers to claim a portion of costs relating to nutritious meals served to children), and more;.[18]		Family childcare may be less expensive than center-based care because of the lower overhead (lower ratios mean less staff are required to maintain regulated ratios. Many family childcare home providers may be certified with the same credentials as center based staff potentially leading to higher level of care.		Franchising of family child care home facilities attempts to bring economies of scale to home daycare. A central operator handles marketing, administration and perhaps some central purchasing while the actual care occurs in individual homes. The central operator may provide training to the individual care providers. Some providers even offer enrichment programs to take the daycare experience to a more educational and professional level.		For all providers, the largest expense is labor. In a 1999 Canadian survey of formal child care centers, labor accounted for 63% of costs and the industry had an average profit of 5.3%.[19] Given the labor-intensive nature of the industry, it is not surprising that the same survey showed little economies of scale between larger and smaller operators.		Local legislation may regulate the operation of daycare centers, affecting staffing requirements. Laws may mandate staffing ratios (for example 6 weeks to 12 months, 1:4; 12 months to 18 months, 1:5; 18 months to 24 months, 1:9; et and even higher ratios for older children). Legislation may mandate qualifications of supervisors. Staff typically do not require any qualifications but staff under the age of eighteen may require supervision. Typically, once the child reaches the age of twelve, they are no longer covered by daycare legislation and programs for older children may not be regulated.		In Canada, the workforce is predominantly female (95%) and low paid, averaging only 60% of average workforce wage. Many employees are at local minimum wage and are typically paid by the hour rather than salaried. In the United States, "child care worker" is the fifth most female-dominated occupation (95.5% female in 1999).[20] In the US, staffing requirements vary from state to state.		Some jurisdictions require licensing or certification. Parents may also turn to independent rating services, or rely on recommendations and referrals. Some places develop voluntary quality networks, for example in Australia most childcare services are part of a national Quality Assurance system. Most countries have laws relating to childcare, which seek to keep children safe and prevent and punish child abuse. Such laws may add cost and complexity to childcare provision and may provide tools to help ensure quality childcare.		Additionally, legislation typically defines what constitutes daycare (e.g., so as to not regulate individual babysitters). It may specify details of the physical facilities (washroom, eating, sleeping, lighting levels, etc.). The minimum window space may be such that it precludes day cares from being in a basement. It may specify the minimum floor space per child (for example 2.8 square metres) and the maximum number of children per room (for example 24). It may mandate minimum outdoor time (for example 2 hours for programs 6 hours or longer). Legislation may mandate qualifications of supervisors. Staff typically do not require any qualifications but staff under the age of eighteen may require supervision. Some legislation also establishes rating systems, the number and condition of various toys, and documents to be maintained.[21] Typically[citation needed], once children reach the age of twelve, they are no longer covered by daycare legislation and programs for older children may not be regulated.		Legislation may mandate staffing ratios (for example, 6 weeks to 12 months, 1:4; 12 months to 18 months, 1:5; 18 months to 24 months, 1:9; etc.). The caregiver-to-child ratio is one factor indicative of quality of care. Ratios vary greatly by location and by daycare center. Potential consequences of a caregiver:child ratio which is too high could be very serious[citation needed]. However, many states allow a higher numbers of toddlers to caregivers and some centers do not comply consistently. For example, within the US: Pennsylvania, ages 1–3, 1 teacher to 5 children;[22] Missouri: age 2, 1 teacher to 8 children;[23] North Carolina: 1 teacher to 10 children.[21]		Many organizations in the developed world campaign for free or subsidized childcare for all. Others campaign for tax breaks or allowances to provide parents a non-finance driven choice. Many of the free or subsidized childcare programs in the United States are also Child Development programs, or afterschool programs which hire certified teachers to teach the children while they are in their care. There are often local industry associations that lobby governments on childcare policy, promote the industry to the public[24] or help parents choose the right daycare provider.[25]		In the United States, childcare in regulated commercial or family childcare home setting is administered or led by teachers who may have a Child Development Associate or higher credentials. These higher credentials include Associate, Bachelor, and even master's degrees in the field of Early Childhood Education (ECE). Although childcare professionals may obtain a degree, many states require that they attend workshops yearly to upgrade their knowledge and skill levels. Many day cares require a teacher to obtain a certain amount of training. For example, Texas requires a minimum of 25 hours a year, and the first year as a teacher, you are required to have 50 hours.		Childcare infection is the spread of infection during childcare, typically because of contact among children in daycare or school.[26] This happens when groups of children meet in a childcare environment, and there any individual with an infectious disease may spread it to the entire group. Commonly spread diseases include influenza-like illness and enteric illnesses, such as diarrhea among babies using diapers. It is uncertain how these diseases spread, but hand washing reduces some risk of transmission and increasing hygiene in other ways also reduces risk of infection.[27]		Due to social pressure, parents of sick children in childcare may be willing to give unnecessary medical care to their children when advised to do so by childcare workers and even if it is against the advice of health care providers.[28] In particular, children in childcare are more likely to take antibiotics than children outside of childcare.[28]		Australia has a large child care industry,[29] however in many locations (especially in inner-city suburbs of large cities and in rural areas) the availability is limited and the waiting periods can be up to several years.[30] The Australian government's Child Care Benefit[31] scheme provides generous assistance with child care costs, but this still leaves many families with a large out of pocket expense. Most families are eligible for Child Care Rebate, which provides a rebate of 50% of fees, up to a yearly cap of AUD$7500 per child. The median weekly cost of centre-based long day care in 2013 was approximately A$364[32] which puts it out of the reach of lower income earners.[33]		Regulation is governed by the ACECQA,[34] a federal government body, which acts as a central body for the state bodies.[35]		Ratios are:		All childcare workers must have, or be undertaking, the minimum "Certificate III in Children's Services" in order to work in a centre (Recognition of Prior Learning is available to help qualify staff with many years experience, but no qualifications). (Common more advanced qualifications are "Diploma of Children's Services" and an Early Childhood Education degree).		Rules differ between states regarding family day care in Australia. To start a Family Day Care business in Victoria, an educator should be either having "Certificate III in Children's Services" or be actively working towards the same. Additionally, Current Police check, Current First Aid training, Insurance (specifically for family day care) is necessary for starting a family day care. The house should be safe for children. A group of 15 educators works under one Supervisor who must have a "Diploma in Children's Services".		Canada offers both private and subsidized daycare centers. Some shortages of subsidized openings can lengthen the time needed to find a suitable childcare provider. To counter this, government or private enterprise sometimes enable parents to look for available spaces online.[36][37]		In Denmark day-cares accept children ranging from 6 months old to 3 years old. 91.2% of 1-2 year old children are enrolled in different types of day-care institutions. Most of these are managed by a municipality and mostly government funded. The different types of institutions ranges from separate day-care institutions (Vuggestue), kindergartens with a day-care department (Integrerede institutioner) and in-home day-care (Dagpleje).[38]		The day-cares are play-based focusing on the children’s perspective and involvement in the day-to-day life. The day-cares are staffed by trained social educators or pedagogues (pædagog)[39]		In Germany, preschool education is the domain of the Kindertagesstätte (literally "children's day site", often shortened to Kita or KITA), which is usually divided into the Kinderkrippe (crèche) for toddlers (age up to 3 years), and the Kindergarten for children who are older than three years and before school. Children in their last Kindergarten year may be grouped into a Vorschule ("preschool") and given special pedagogic attention; special preschool institutions comparable to the US-American kindergarten are the exception.		Kitas are typically run by public (i. e. communal) and "free" carriers (such as the churches, other religious organizations, social organizations with a background in the trade unions and profit-orientated corporations), and subsidized by the states (Länder). In this case, the care is open to the general public—e. g. a Protestant or Muslim child may claim a place in a Kita run by the catholic church.		Preschool education, unlike school and university, is not in the exclusive domain of the states. The federal government regulates daycare through the Kinder- und Jugendhilfegesetz (KJHG), which stipulates a legal claim to daycare:		Alternative daycare can be provided through Tagespflegepersonen (usually Tagesmütter, "day mothers"), i. e. stay-at-home parents which provide commercial day care to other children. This form of daycare is also federally regulated through the KJHG.		Preschool education (Frühpädagogik) is increasingly seen as an integral part of education as a whole; several states such as Bavaria have released detailed educational plans for daycare carriers who claim state subsidies. "Early pedagogics" has increasingly moved into the academic domain, with an increasing number of staff being trained at universities of applied science (Fachhochschulen) and regular universities. Non-academic personnel in daycare facilities have usually attended specialized schools for several years. In the state of Bavaria for example, daycare assistants (Kinderpfleger) will have attended school for two years, daycare teachers (Erzieher) for three years with an additional two-year internship.		In Japan, the child care industry is worth trillions of yen, and is expanding due to rising work force participation by mothers. In 2004 nearly 2 million children were in some form of day care. (Jetro)		In Mexico, President Felipe Calderon Hinojosa created a Social Program named "Programa de Estancias Infantiles" that included more than 8,000 daycare spaces for children between 1 and 3.11 years old. This program subsidizes mothers that work and study and also single fathers in a vulnerable situation. It has a great success having more than 125,000 children over the country. This is regulated by the Social Development Minister (Secretaría de Desarrollo Social).[6]		Most children in Norway start daycare between 10 months and 3 years old. Funded parental leave for working parents is either 44 weeks with full pay, or 54 weeks with 80% pay (both up to a certain level only). The government guarantees daycare for all children that are at least 1 year old by 1 August.[40] Coverage is still not 100%, but most regions are getting close (2011). There's a maximum price to enable all families to afford it.		Spain provides paid maternity leave of 16 weeks with 30-50% of mothers returning to work (most full-time) after this[citation needed], thus babies 4 months of age tend to be placed in daycare centers. Adult-infant ratios are about 1:7-8 first year and 1:16-18 second year.[citation needed] Public preschool education is provided for most children aged 3–5 years in "Infantil" schools which also provide primary school education.[citation needed]		The UK has a wide range of childcare options, including childminders, day nurseries, playgroups and pre-school education at school. It is regulated by OFSTED (CSSIW in Wales), which operates the application and inspection process for the sector.		Childcare is primarily funded by parents, however the Single Funding Formula (pre-school funding) can be used at some day nurseries, playgroups and schools for a maximum of 5 sessions per week, after a child reaches 3 years. The government introduced a childcare allowance (vouchers) by which employers could make payments for childcare, prior to tax, on employees' wages.		Median rates (2011) are approximately £4.50 per hour for childminders, £7:5-£10 net per hour for nannies, £60-100 per week for au pairs and £35-£50 per day for day nurseries.		State legislation may regulate the number and ages of children allowed before the home is considered an official daycare program and subject to more stringent safety regulations. Often the nationally recognized Child Development Associate credential is the minimum standard for the individual leading this home care program.[citation needed] Each state has different regulations for teacher requirements. In some states, teachers must have an associate degree in child development. States with quality standards built into their licensing programs may have higher requirements for support staff such as teacher assistants. And in Head Start programs, by 2012, all lead teachers must have a bachelor's degree in Early Childhood Education. States vary in the standards set for daycare providers, such as teacher to child ratios.		Family childcare can also be nationally accredited by the National Association of Family Childcare if the provider chooses to go through the process. National accreditation is only awarded to those programs who demonstrate the quality standards set forth by the NAFCC.		According to the 1995 U.S. Census Bureau Survey of Income and Program Participation (SIPP), over thirty-six percent of families of preschoolers with working mothers primarily relied on childcare in the home of a relative, family daycare provider or other non-relative. Almost twenty-six percent of families used organized childcare facilities as their primary arrangement.[41]		Child care can cost up to $15,000 for one year in the United States. The average annual cost of full-time care for an infant in center-based care ranges from $4,863 in Mississippi to $16,430 in Massachusetts.[42]		Independent studies suggest that good daycare for non-infants is not harmful.[12] In some cases, good daycare can provide different experiences than parental care does, especially when children reach two and are ready to interact with other children. Bad daycare puts the child at physical, emotional and attachment risk. Higher quality care was associated with better outcomes. Children in higher quality childcare had somewhat better language and cognitive development during the first 4½ years of life than those in lower quality care. They were also somewhat more cooperative than those who experienced lower quality care during the first 3 years of life.		The National Institute of Health released a study in March, 2007 after following a group of children through early childhood to the 6th grade.[43] The study found that the children who received a higher quality of childcare scored higher on 5th grade vocabulary tests than the children who had attended childcare of a lower quality. The study also reported that teachers found children from childcare to be "disobedient", fight more frequently, and more argumentative. The study reported the increases in both aggression and vocabulary were small. "The researchers emphasized that the children’s behavior was within the normal range and were not considered clinically disordered."		As a matter of social policy, consistent, good daycare, may ensure adequate early childhood education for children of less skilled parents. From a parental perspective, good daycare can complement good parenting.		A 2001 report showed that children in high-quality care scored higher on tests of language, memory and other skills than did children of stay-at-home mothers or children in lower-quality day care.[44]		A study appearing in Child Development in July/August 2003 found that the amount of time spent in daycare before four-and-a-half tended to correspond with the child's tendency to be less likely to get along with others, to be disobedient, and to be aggressive, although still within the normal range.[45][46]		
An apprenticeship is a system of training a new generation of practitioners of a trade or profession with on-the-job training and often some accompanying study (classroom work and reading). Apprenticeship also enables practitioners to gain a license to practice in a regulated profession. Most of their training is done while working for an employer who helps the apprentices learn their trade or profession, in exchange for their continued labor for an agreed period after they have achieved measurable competencies. Apprenticeships typically last 3 to 7 years. People who successfully complete an apprenticeship reach the "journeyman" or professional certification level of competence.		Although the formal boundaries and terminology of the apprentice/journeyman/master system often do not extend outside of guilds and trade unions, the concept of on-the-job training leading to competence over a period of years is found in any field of skilled labor.		In early modern usage, the clipped form prentice was common.						The system of apprenticeship first developed in the later Middle Ages and came to be supervised by craft guilds and town governments. A master craftsman was entitled to employ young people as an inexpensive form of labour in exchange for providing food, lodging and formal training in the craft. Most apprentices were males, but female apprentices were found in crafts such as seamstress,[1] tailor, cordwainer, baker and stationer.[2] Apprentices usually began at ten to fifteen years of age, and would live in the master craftsman's household. Most apprentices aspired to becoming master craftsmen themselves on completion of their contract (usually a term of seven years), but some would spend time as a journeyman and a significant proportion would never acquire their own workshop.		In Coventry those completing seven-year apprenticeships with stuff merchants were entitled to become freemen of the city.[3]		Subsequently, governmental regulation and the licensing of technical colleges and vocational education formalized and bureaucratized the details of apprenticeship.[citation needed]		Australian Apprenticeships encompass all apprenticeships and traineeships. They cover all industry sectors in Australia and are used to achieve both 'entry-level' and career 'upskilling' objectives. There were 475,000 Australian Apprentices in-training as at 31 March 2012, an increase of 2.4% from the previous year. Australian Government employer and employee incentives may be applicable, while State and Territory Governments may provide public funding support for the training element of the initiative. Australian Apprenticeships combine time at work with formal training and can be full-time, part-time or school-based.[4]		Australian Apprentice and Traineeship services are dedicated to promoting retention, therefore much effort is made to match applicants with the right apprenticeship or traineeship. This is done with the aid of aptitude tests, tips, and information on 'how to retain an apprentice or apprenticeship'.[5]		Information and resources on potential apprenticeship and traineeship occupations are available in over sixty industries.[6]		The distinction between the terms apprentices and trainees lies mainly around traditional trades and the time it takes to gain a qualification. The Australian government uses Australian Apprenticeships Centres to administer and facilitate Australian Apprenticeships so that funding can be disseminated to eligible businesses and apprentices and trainees and to support the whole process as it underpins the future skills of Australian industry. Australia also has a fairly unusual safety net in place for businesses and Australian Apprentices with its Group Training scheme. This is where businesses that are not able to employ the Australian Apprentice for the full period until they qualify, are able to lease or hire the Australian Apprentice from a Group Training Organisation. It is a safety net, because the Group Training Organisation is the employer and provides continuity of employment and training for the Australian Apprentice.[7][8]		In addition to a safety net, Group Training Organisations (GTO) have other benefits such as additional support for both the Host employer and the trainee/apprentice through an industry consultant who visits regularly to make sure that the trainee/apprentice are fulfilling their work and training obligations with their Host employer. There is the additional benefit of the trainee/apprentice being employed by the GTO reducing the Payroll/Superannuation and other legislative requirements on the Host employer who pays as invoiced per agreement.[citation needed]		Apprenticeship training in Austria is organized in a dual education system: company-based training of apprentices is complemented by compulsory attendance of a part-time vocational school for apprentices (Berufsschule).[9] It lasts two to four years – the duration varies among the 250 legally recognized apprenticeship trades.		About 40 percent of all Austrian teenagers enter apprenticeship training upon completion of compulsory education (at age 15). This number has been stable since the 1950s.[10]		The five most popular trades are: Retail Salesperson (5,000 people complete this apprenticeship per year), Clerk (3,500 / year), Car Mechanic (2,000 / year), Hairdresser (1,700 / year), Cook (1,600 / year).[11] There are many smaller trades with small numbers of apprentices, like "EDV-Systemtechniker" (Sysadmin) which is completed by fewer than 100 people a year.[12]		The Apprenticeship Leave Certificate provides the apprentice with access to two different vocational careers. On the one hand, it is a prerequisite for the admission to the Master Craftsman Exam and for qualification tests, and on the other hand it gives access to higher education via the TVE-Exam or the Higher Education Entrance Exam which are prerequisites for taking up studies at colleges, universities, "Fachhochschulen", post-secondary courses and post-secondary colleges.[9]		The person responsible for overseeing the training inside the company is called "Lehrherr" or "Ausbilder". An Ausbilder must prove he has the professional qualifications needed to educate another person. The "Ausbilder" must also prove he does not have a criminal record and is an otherwise respectable person. According to the laws: the person wanting to educate a young apprentice must prove that he has an ethical way of living and the civic qualities of a good citizen.[13]		In the Czech Republic, the term "vocational school" (učiliště) can refer to the two, three or four years of secondary practical education. Apprenticeship Training is implemented under Education Act (školský zákon). Apprentices spend about 30-60% of their time in companies (sociální partneři školy) and the rest in formal education. Depending on the profession, they may work for two to three days a week in the company and then spend two or three days at a vocational school.		Switzerland has an apprenticeship similarly to Germany and Austria. The educational system is ternar, which is basically dual education system with mandatory practical courses. The length of an apprenticeship can be 2, 3 or 4 years.		Apprenticeships with a length of 2 years are for persons with weaker school results. The certificate awarded after successfully completing a 2-year apprenticeship is called "Eidgenössisches Berufsattest" (EBA) in German and "Certificato federale di formazione pratica" (CFP) in Italian.		Apprenticeship with a length of 3 or 4 years are the most common ones. The certificate awarded after successfully completing a 3 or 4-year apprenticeship is called "Certificat Fédérale de Capacité" (CFC), "Eidgenössisches Fähigkeitszeugnis" (EFZ) or "Attestato federale di capacità" (AFC).		Some crafts like Electrician are educated in lengths of 3 and 4 years. In this case, an Electrician with 4 years apprenticeship gets more theoretical background than one with 3 years apprenticeship. Also, but that is easily lost in translation, the profession has a different name.		Each of the over 300 nationwide defined vocational profiles has defined framework - conditions as length of education, theoretical and practical learning goals and certification conditions.		Typically an apprenticeship is started at age of 15 and 18 after finishing general education. Some apprenticeships have a recommend or required age of 18, which obviously leads to a higher average age. There is formally no maximum age, however, for persons above 21 it is hard to find a company due to companies preferring younger ages due to the lower cost of labour.		In Canada, each province has its own apprenticeship program. In Canada, apprenticeships tend to be formalized for craft trades and technician level qualifications. At the completion of the provincial exam, they may write the Interprovincal Standard exam. British Columbia is one province that uses these exams as the provincial exam. This means a qualification for the province will satisfy the whole country. The interprovincial exam questions are agreed upon by all provinces.		In France, apprenticeships also developed between the ninth and thirteenth centuries, with guilds structured around apprentices, journeymen and master craftsmen, continuing in this way until 1791, when the guilds were suppressed.		The first laws regarding apprenticeships were passed in 1851. From 1919, young people had to take 150 hours of theory and general lessons in their subject a year. This minimum training time rose to 360 hours a year in 1961, then 400 in 1986.		The first training centres for apprentices (centres de formation d'apprentis, CFAs) appeared in 1961, and in 1971 apprenticeships were legally made part of professional training. In 1986 the age limit for beginning an apprenticeship was raised from 20 to 25. From 1987 the range of qualifications achieveable through an apprenticeship was widened to include the brevet professionnel (certificate of vocational aptitude), the bac professionnel (vocational baccalaureate diploma), the brevet de technicien supérieur (advanced technician's certificate), engineering diplomas, master's degree and more.		On January 18, 2005, President Jacques Chirac announced the introduction of a law on a programme for social cohesion comprising the three pillars of employment, housing and equal opportunities. The French government pledged to further develop apprenticeship as a path to success at school and to employment, based on its success: in 2005, 80% of young French people who had completed an apprenticeship entered employment. In France, the term apprenticeship often denotes manual labor but it also includes other jobs like secretary, manager, engineer, shop assistant... The plan aimed to raise the number of apprentices from 365,000 in 2005 to 500,000 in 2009. To achieve this aim, the government is, for example, granting tax relief for companies when they take on apprentices. (Since 1925 a tax has been levied to pay for apprenticeships.) The minister in charge of the campaign, Jean-Louis Borloo, also hoped to improve the image of apprenticeships with an information campaign, as they are often connected with academic failure at school and an ability to grasp only practical skills and not theory. After the civil unrest end of 2005, the government, led by prime minister Dominique de Villepin, announced a new law. Dubbed "law on equality of chances", it created the First Employment Contract as well as manual apprenticeship from as early as 14 years of age. From this age, students are allowed to quit the compulsory school system in order to quickly learn a vocation. This measure has long been a policy of conservative French political parties, and was met by tough opposition from trade unions and students.		Apprenticeships are part of Germany's dual education system, and as such form an integral part of many people's working life. Finding employment without having completed an apprenticeship is almost impossible. For some particular technical university professions, such as food technology, a completed apprenticeship is often recommended; for some, such as marine engineering it may even be mandatory.		In Germany, there are 342 recognized trades (Ausbildungsberufe) where an apprenticeship can be completed. They include for example doctor's assistant, banker, dispensing optician, plumber or oven builder.[14] The dual system means that apprentices spend about 50-70% of their time in companies and the rest in formal education. Depending on the profession, they may work for three to four days a week in the company and then spend one or two days at a vocational school (Berufsschule). This is usually the case for trade and craftspeople. For other professions, usually which require more theoretical learning, the working and school times take place blockwise e.g., in a 12–18 weeks interval. These Berufsschulen have been part of the education system since the 19th century.		In 2001, two-thirds of young people aged under 22 began an apprenticeship, and 78% of them completed it, meaning that approximately 51% of all young people under 22 have completed an apprenticeship.[citation needed] One in three companies offered apprenticeships in 2003,[citation needed] in 2004 the government signed a pledge with industrial unions that all companies except very small ones must take on apprentices.		The latent decrease of the German population due to low birth rates is now causing a lack of young people available to start an apprenticeship.[citation needed]		After graduation from school at the age of fifteen to nineteen (depending on type of school), students start an apprenticeship in their chosen professions. Realschule and Gymnasium graduates usually have better chances for being accepted as an apprentice for sophisticated craft professions or apprenticeships in white-collar jobs in finance or administration. An apprenticeship takes between 2.5 and 3.5 years. Originally, at the beginning of the 20th century, less than 1% of German students attended the Gymnasium (the 8-9 year university-preparatory school) to obtain the Abitur graduation which was the only way to university back then. In the 1950's still only 5% of German youngsters entered university and in 1960 only 6% did. Due to the risen social wealth and the increased demand for academic professionals in Germany, about 24% of the youngsters entered college/university in 2000.[15] Of those, who did not enter university many started an apprenticeship. The apprenticeships usually end a person's education by age 18-20, but also older apprentices are accepted by the employers under certain conditions. This is frequently the case for immigrants from countries without a compatible professional training system.		In 1969, a law (the Berufsbildungsgesetz) was passed which regulated and unified the vocational training system and codified the shared responsibility of the state, the unions, associations and the chambers of trade and industry. The dual system was successful in both parts of the divided Germany. In the GDR, three-quarters of the working population had completed apprenticeships.		The precise skills and theory taught on German apprenticeships are strictly regulated. The employer is responsible for the entire education programme coordinated by the German chamber of commerce. Apprentices obtain a special apprenticeship contract until the end of the education programme. During the programme it is not allowed to assign the apprentice to regular employment and he is well protected from abrupt dismissal until the programme ends. The defined content and skill set of the apprentice profession must be fully provided and taught by the employer. The time taken is also regulated. Each profession takes a different time, usually between 24 and 36 months.		Thus, everyone who had completed an apprenticeship e.g., as an industrial manager (Industriekaufmann) has learned the same skills and has attended the same courses in procurement and stocking up, controlling, staffing, accounting procedures, production planning, terms of trade and transport logistics and various other subjects. Someone who has not taken this apprenticeship or did not pass the final examinations at the chamber of industry and commerce is not allowed to call himself an Industriekaufmann. Most job titles are legally standardized and restricted. An employment in such function in any company would require this completed degree.		The rules and laws for the trade and craftwork apprentices such as mechanics, bakers, joiners, etc. are as strict as and even broader than for the business professions. The involved procedures, titles and traditions still strongly reflect the medieval origin of the system. Here, the average duration is about 36 months, some specialized crafts even take up to 42 months.		After completion of the dual education, e.g., a baker is allowed to call himself a bakery journeyman (Bäckergeselle). After the apprenticeship the journeyman can enter the master's school (Meisterschule) and continue his education at evening courses for 3–4 years or full-time for about one year. The graduation from the master's school leads to the title of a master craftsman (Meister) of his profession, so e.g., a bakery master is entitled as Bäckermeister. A master is officially entered in the local trade register, the craftspeople's roll (Handwerksrolle). A master craftsman is allowed to employ and to train new apprentices. In some mostly safety-related professions, e.g., that of electricians only a master is allowed to found his own company.		To employ and to educate apprentices requires a specific license. The AdA - Ausbildung der Ausbilder - "Education of the Educators" license needs to be acquired by a training at the chamber of industry and commerce.		The masters complete this license course within their own master's coursework. The training and examination of new masters is only possible for masters who have been working several years in their profession and who have been accepted by the chambers as a trainer and examiner.		Academic professionals, e.g., engineers, seeking this license need to complete the AdA during or after their university studies, usually by a one-year evening course.		The holder of the license is only allowed to train apprentices within his own field of expertise. For example, a mechanical engineer would be able to educate industrial mechanics, but not e.g., laboratory assistants or civil builders.		When the apprenticeship is ended, the former apprentice now is considered a journeyman. He may choose to go on his journeyman years-travels.		In India, the Apprentices Act was enacted in 1961.[16] It regulates the programme of training of apprentices in the industry so as to conform to the syllabi, period of training etc. as laid down by the Central Apprenticeship Council and to utilise fully the facilities available in industry for imparting practical training with a view to meeting the requirements of skilled manpower for industry.		The Apprentices Act enacted in 1961 and was implemented effectively in 1962. Initially, the Act envisaged training of trade apprentices. The Act was amended in 1973 to include training of graduate and diploma engineers as "Graduate" & "Technician" Apprentices. The Act was further amended in 1986 to bring within its purview the training of the 10+2 vocational stream as "Technician (Vocational)" Apprentices.		Overall responsibility is with the Directorate General of Employment & Training (DGE&T) in the Union Ministry of Skill Development and Entrepreneurship.[17]		In Pakistan, the Apprenticeship Training is implemented under a National Apprenticeship Ordinance 1962 and Apprenticeship Rules 1966. It regulates apprenticeship programs in industry and a TVET institute for theoretical instructions. It is obligatory for industry having fifty or more workers in an apprenticeable trade to operate apprenticeship training in the industry. Entire cost of training is borne by industry including wages to apprentices. The provincial governments through Technical Education & Vocational Training Authorities (Punjab TEVTA, Sindh TEVTA, KP TEVTA, Balochistan TEVTA and AJK TEVTA) enforce implementation of apprenticeship.		The training period varies for different trades ranging from 1–4 years. As of 2015, more than 30,000 apprentices are being trained in 2,751 industries in 276 trades across Pakistan. This figure constitutes less than 10% of institution based Vocational Training i.e. more than 350 thousand annually.		Recently, Government of Pakistan through National Vocational & Technical Training Commission (NAVTTC) has initiated to reform existing system of apprenticeship. Highlights of the modern apprenticeship system are:		- Inclusion of services, agriculture and mining sector - Cost sharing by Industry and Government - Regulating and formalizing Informal Apprenticeships - Mainstream Apprenticeship Qualifications with National Vocational Qualifications Framework (Pakistan NVQF) - Increased participation of Female - Training Cost reimbursement (for those industries training more number of apprentices than the required) - Assessment and Certification of apprentices jointly by Industry - Chamber of Commerce & Industry - Government - Apprenticeship Management Committee (having representation of 40% employers, 20% workers and 40% Government officials)		In Turkey, apprenticeship has been part of the small business culture for centuries since the time of Seljuk Turks who claimed Anatolia as their homeland in the 11th century.		There are three levels of apprenticeship. The first level is the apprentice, i.e., the "çırak" in Turkish. The second level is pre-master which is called, "kalfa" in Turkish. The mastery level is called as "usta" and is the highest level of achievement. An 'usta' is eligible to take in and accept new 'ciraks' to train and bring them up. The training process usually starts when the small boy is of age 10-11 and becomes a full grown master at the age of 20-25. Many years of hard work and disciplining under the authority of the master is the key to the young apprentice's education and learning process.		In Turkey today there are many vocational schools that train children to gain skills to learn a new profession. The student after graduation looks for a job at the nearest local marketplace usually under the authority of a master.		Apprenticeships have a long tradition in the United Kingdom, dating back to around the 12th century and flourishing by the 14th century. The parents or guardians of a minor would agree with a Guild's Master craftsman the conditions for an apprenticeship which would bind the minor for 5–9 years (e.g., from age 14 to 21). They would pay a "premium" to the craftsman and the contract would be recorded in an indenture.[19] Modern apprenticeships range from craft to high status in professional practice in engineering, law, accounting, architecture, management consulting, and others.		In the 16th century, the payment of a "premium" to the master was not at all common, but such fees became usual in the 17th century, though they varied greatly from trade to trade. The payment of a one-off fee could be very difficult for some parents and in the 18th-century payment by instalment became frequent, this actually being required by law in 1768.		In theory no wage had to be paid to an apprentice since the technical training was provided in return for the labour given. However, it was usual to pay small sums to apprentices, sometimes with which to buy, or instead of, new clothes. By the 18th century regular payments, at least in the last two or three years of the apprentice's term, became usual and those who lived apart from their masters were frequently paid a regular wage. This was sometimes called the "half-pay" system or "colting", payments being made weekly or monthly to the apprentice or to his parents. In these cases, the apprentice often went home from Saturday night to Monday morning. This was the norm in the 19th century but this system had existed in some trades since the 16th century.[20]		In 1563, the Statute of Artificers and Apprentices was passed to regulate and protect the apprenticeship system, forbidding anyone from practising a trade or craft without first serving a 7-year period as an apprentice to a master[21] (though in practice Freemen's sons could negotiate shorter terms).[22]		From 1601, 'parish' apprenticeships under the Elizabethan Poor Law came to be used as a way of providing for poor, illegitimate and orphaned children of both sexes alongside the regular system of skilled apprenticeships, which tended to provide for boys from slightly more affluent backgrounds. These parish apprenticeships, which could be created with the assent of two Justices of the Peace, supplied apprentices for occupations of lower status such as farm labouring, brickmaking and menial household service.[19]		In the early years of the Industrial Revolution entrepreneurs began to resist the restrictions of the apprenticeship system,[23] and a legal ruling established that the Statute of Apprentices did not apply to trades that were not in existence when it was passed in 1563, thus excluding many new 18th century industries.[20][21] In 1814 compulsory apprenticeship by indenture was abolished.		The mainstay of training in industry has been the apprenticeship system (combining academic and practice), and the main concern has been to avoid skill shortages in traditionally skilled occupations and higher technician and engineering professionals, e.g., through the UK Industry Training Boards (ITBs) set up under the 1964 Act. The aims were to ensure an adequate supply of training at all levels; to improve the quality and quantity of training; and to share the costs of training among employers. The ITBs were empowered to publish training recommendations, which contained full details of the tasks to be learned, the syllabus to be followed, the standards to be reached and vocational courses to be followed. These were often accompanied by training manuals, which were in effect practitioners' guides to apprentice training, and some ITBs provide training in their own centers. The ITBs did much to formalise what could have been a haphazard training experience and greatly improved its quality. The years from the mid-1960s to the mid-1970s saw the highest levels of apprentice recruitment, yet even so, out of a school leaving cohort of about 750,000, only about 110,000 (mostly boys) became apprentices. The apprenticeship system aimed at highly developed craft and higher technician skills for an elite minority of the workforce, the majority of whom were trained in industries that declined rapidly from 1973 onwards, and by the 1980s it was clear that in manufacturing this decline was permanent.[24]		Since the 1950s the UK high technology industry (Aerospace, Nuclear, Oil & Gas, Automotive, Telecommunications, Power Generation and Distribution etc.) trained its higher technicians and professional engineers via the traditional indentured apprenticeship system of learning - usually a 4 - 6-year process from age 16–21. There were 4 types of traditional apprenticeship; craft, technician, higher technician, and graduate. Craft, technician and higher technician apprenticeships usually took 4 to 5 years while a graduate apprenticeship was a short 2-year experience usually while at university or post graduate experience. Non graduate technician apprenticeships were often referred to as technical apprenticeships. The traditional Apprenticeship Framework in the 1950s, 1960s and 1970s was designed to allow young people (16 years old) an alternative path to GCE A Levels to achieve both an academic qualification at level 4 or 5 NVQ along with competency based skills for knowledge work. Often referred to as the "Golden Age" of work and employment for bright young people, the traditional technical apprenticeship framework was open to young people who had a minimum of 4 GCE "O" Levels to enroll in an Ordinary National Certificate or Diploma or a City & Guilds engineering technician course. Apprentices could progress to the Higher National Certificate, Higher National Diploma or advanced City and Guilds course such as Full Technological Certification. Apprenticeship positions at elite companies often had hundreds of applications for a placement. Academic learning during an apprenticeship was achieved either via block release or day release at a local technical institute. An OND or HND were usually obtained via the block release approach whereby an apprentice would be released for periods of up to 3 months to study academic courses full-time and then return to the employer for applied work experience.For entrance into the higher technical engineering apprenticeships "O"Levels had to include Mathematics, Physics, and English language. The academic level of subjects such as mathematics, physics, chemistry on ONC / OND and some City & Guilds advanced technicians courses was equivalent to A level mathematics, physics and chemistry. The academic science subjects were based on applied science in subjects such as thermodynamics, fluid mechanics, mechanics of machines, dynamics and statics, electrical science and electronics. These are often referred to as the engineering sciences. HNC and HND were broadly equivalent to subjects in the first year of a bachelor's degree in engineering but not studied to the same intensity or mathematical depth. HNC was accepted as entrance into the first year of an engineering degree and high performance on an HND course could allow a student direct entry into the second year of a degree. Few apprentices followed this path since it would have meant 10 –12 years in further and higher education. For the few that did follow this path they accomplished a solid foundation of competency-based work training via apprenticeship and attained a higher academic qualification at a university or Polytechnic combining both forms of education; vocational plus academic. During the 1970s City and Guilds assumed responsibility for the administration of HNC and HND courses.		The City and Guilds of London Institute the forerunner of Imperial College engineering school has been offering vocational education through apprenticeships since the 1870s from basic craft skills (mechanic, hairdresser, chef, plumbing, carpentry, bricklaying etc.) all the way up to qualifications equivalent to university master's degrees and doctorates. The City and Guilds diploma of fellowship is awarded to individuals who are nationally recognized through peer review as having achieved the very highest level in competency-based achievement. The first award of FCGI was approved by Council in December 1892 and awarded in 1893 to Mr H A Humphrey, Engineering Manager of the Refined Bicarbonate and Crystal Plant Departments of Messrs Brunner, Mond & Co. His award was for material improvements in the manufacture of bicarbonate of soda. The system of nomination was administered within Imperial College, with recommendations being passed to the Council of the Institute for approval. Approximately 500 plus people have been awarded Fellowship since its inception.		The traditional apprenticeship framework's purpose was to provide a supply of young people seeking to enter work-based learning via apprenticeships by offering structured high-value learning and transferable skills and knowledge. Apprenticeship training was enabled by linking industry with local technical colleges and professional engineering institutions. The apprenticeship framework offered a clear pathway and competency outcomes that addressed the issues facing the industry sector and specific companies. This system was in place since the 1950s. The system provided young people with an alternative to staying in full-time education post- 16/18 to gain purely academic qualifications without work-based learning. The apprenticeship system of the 1950s, 1960s and 1970s provided the necessary preparation for young people to qualify as a Craft trade (Machinist, Toolmaker, Fitter, Plumber, Welder, Mechanic, Millwright etc.), or Technician (quality inspector, draughtsman, designer, planner, work study, programmer), or Technician Engineer (tool design, product design, methods, stress and structural analysis, machine design etc.) and even enabled a path to a fully qualified Chartered Engineer in a specific discipline (Mechanical, Electrical, Civil, Aeronautical, Chemical, Building, Structural, Manufacturing etc.). The Chartered Engineer qualification was usually achieved aged 28 and above. Apprentices undertook a variety of job roles in numerous shop floor and office technical functions to assist the work of master craftsmen, technicians, engineers, and managers in the design, development, manufacture and maintenance of products and production systems.		It was possible for apprentices to progress from national certificates and diplomas to engineering degrees if they had the aptitude.[25] The system allowed young people to find their level and still achieve milestones along the path from apprenticeship into higher education via a polytechnic or university. Though rare, it was possible for an apprentice to advance from vocational studies, to undergraduate degree, to graduate study and earn a master's degree or a PhD. The system was effective; industry was assured of a supply of well educated and fit for work staff, local technical colleges offered industry relevant courses that had a high measure of academic content and an apprentice was prepared for professional life or higher education by the age of 21. With the exception of advanced technology companies particularly in aerospace (BAE systems, Rolls-Royce, Bombardier) this system declined with the decline of general manufacturing industry in the UK.		Traditional apprenticeships reached their lowest point in the 1980s: by that time, training programmes declined. The exception to this was in the high technology engineering areas of aerospace, chemicals, nuclear, automotive, power and energy systems where apprentices continued to served the structured four- to five-year programmes of both practical and academic study to qualify as engineering technician or Incorporate Engineer (engineering technologist) and even go on to earn a master of engineering degree and qualify as a Chartered Engineer (UK); the UK gold standard engineering qualification. Engineering technicians and technologists continued in the traditional approach from the golden age attended the local technical college (1 day and 2 evenings per week) on a City & Guilds programme or Ordinary National Certificate / Higher National Certificate course. In effect becoming a chartered engineer via the apprenticeship route involved 10 – 12 years of both academic and vocational training at an employer, college of further education and university. In 1986 National Vocational Qualifications (NVQs) were introduced, in an attempt to revitalize vocational training. Still, by 1990, apprenticeship took up only two-thirds of one percent of total employment.[citation needed]		In 1994, the Government introduced Modern Apprenticeships (renamed Apprenticeships in England, Wales and Northern Ireland), based on frameworks today of the Sector Skills Councils. In 2009, the National Apprenticeship Service was founded to coordinate apprenticeships in England. Apprenticeship frameworks contain a number of separately certified elements:		In Scotland, Modern Apprenticeship Frameworks are approved by the Modern Apprenticeship Group (MAG) and it, with the support of the Scottish Government, has determined that from January 2010, all Frameworks submitted to it for approval, must have the mandatory elements credit rated for the Scottish Credit and Qualifications Framework (SCQF).		As of 2009 there are over 180 apprenticeship frameworks.[26] The current scheme extends beyond manufacturing and high technology industry to parts of the service sector with no apprenticeship tradition. In 2008 Creative & Cultural Skills, the Sector Skills Council, introduced a set of Creative Apprenticeships awarded by EDI.[27] A freelance apprenticeship framework was also approved and uses freelance professionals to mentor freelance apprentices. The Freelance Apprenticeship was first written and proposed by Karen Akroyd (Access To Music) in 2008. In 2011 Freelance Music Apprenticeships are available in music colleges in Birmingham, Manchester and London. The Department of Education under its 2007-2010 name stated its intention to make apprenticeships a "mainstream part of England's education system".[28]		Employers who offer apprenticeship places have an employment contract with their apprentices, but off-the-job training and assessment is wholly funded by the state for apprentices aged between 16 and 18. In England, Government only contributes 50% of the cost of training for apprentices aged 19 – 24. Apprenticeships at Level 3 or above for those aged 24 or over no longer attract State funding, although there is a State loan facility in place by which individuals or companies can cover the cost of study and assessment and repay the State by instalments over an extended period at preferential rates of interest.		Government funding agencies (in England, the Skills Funding Agency) contract with 'learning providers' to deliver apprenticeships, and may accredit them as a National Skills Academy. These organisations provide off-the-job tuition and manage the bureaucratic workload associated with the apprenticeships. Providers are usually private training companies but might also be Further Education colleges, voluntary sector organisations, Chambers of Commerce or employers themselves.		The UK government has implemented a rigorous apprenticeship structure which in many ways resembles the traditional architecture of the 1950s, 1960s and 1970s. There are three levels of apprenticeship available spanning 2–6 years of progression. It is possible for ambitious apprentices to progress from level 2 (intermediate) to level 7 ( master's degree) over many years of training and education. Learners start at a level which reflects their current qualifications and the opportunities available in the sector of interest:		Intermediate Apprenticeship (Level 2; equivalent to five good GCSE passes): provides learners with the skills and qualifications for their chosen career and allow entry (if desired) to an Advanced Apprenticeship. To be accepted learners need to be enthusiastic, keen to learn and have a reasonable standard of education; most employers require applicants to have two or more GCSEs (A*-C), including English and Maths.[29]		Advanced Apprenticeship (Level 3; equivalent to two A-level passes): to start this programme, learners should have five GCSEs (grade A*-C) or have completed an Intermediate Apprenticeship. This will provide them with the skills and qualifications needed for their career and allow entry (if desired) to a Higher Apprenticeship or degree level qualification. Advanced apprenticeships can last between two and four years.[30]		Higher Apprenticeship (Level 4/5; equivalent to a Foundation Degree): to start this programme, learners should have a Level 3 qualification (A-Levels, Advanced Diploma or International Baccalaureate) or have completed an Advanced Apprenticeship. Higher apprenticeships are designed for students who are aged 18 or over.[31]		Degree Apprenticeship (Level 5/6; achieve bachelor's degree) and (Level 7 Masters): to start this programme, learners should have a level 3/4 qualification (A-Levels, Advanced Diploma or International Baccalaureate) relevant to occupation or have completed an Advanced Apprenticeship also relevant to occupation. It differs from a 'Higher Apprenticeship' due to graduating with a bachelor's degree at an accredited university. Degree apprenticeships can last between two and four years.[32]		Under the current UK system, commencing from 2013, groups of employers ('trailblazers') develop new apprenticeships, working together to design apprenticeship standards and assessment approaches.[33] As at July 2015, there were 140 Trailblazer employer groups which had so far collectively delivered or were in the process of delivering over 350 apprenticeship standards.[34]		From April 2017 an Apprenticeship Levy has been in place to fund apprenticeships. Many UK public bodies are subject to a statutory target to employ an average of at least 2.3% of their staff as new start apprentices over the period from 1 April 2017 to 31 March 2021, and to "have regard" to this target when planning their recruitment and career development activities.[35]		Apprenticeship programs in the United States are regulated by the Smith-Hughes Act (1917), The National Industrial Recovery Act (1933), and National Apprenticeship Act, also known as the "Fitzgerald Act."[36]		The number of American apprentices has increased from 375,000 in 2014 to 500,000 in 2016, while the federal government intends to see 750,000 by 2019, particularly by expanding the apprenticeship model to include white-collar occupations such as information technology.[37][38]		In the United States, education officials and nonprofit organizations who seek to emulate the apprenticeship system in other nations have created school to work education reforms. They seek to link academic education to careers. Some programs include job shadowing, watching a real worker for a short period of time, or actually spending significant time at a job at no or reduced pay that would otherwise be spent in academic classes or working at a local business. Some legislators raised the issue of child labor laws for unpaid labor or jobs with hazards.		In the United States, school to work programs usually occur only in high school. American high schools were introduced in the early 20th century to educate students of all ability and interests in one learning community rather than prepare a small number for college. Traditionally, American students are tracked within a wide choice of courses based on ability, with vocational courses (such as auto repair and carpentry) tending to be at the lower end of academic ability and trigonometry and pre-calculus at the upper end.		American education reformers have sought to end such tracking, which is seen as a barrier to opportunity. By contrast, the system studied by the NCEE (National Center on Education and the Economy) actually relies much more heavily on tracking. Education officials in the U.S., based largely on school redesign proposals by NCEE and other organizations, have chosen to use criterion-referenced tests that define one high standard that must be achieved by all students to receive a uniform diploma. American education policy under the "No Child Left Behind Act" has as an official goal the elimination of the achievement gap between populations. This has often led to the need for remedial classes in college.[39]		Many U.S. states now require passing a high school graduation examination to ensure that students across all ethnic, gender and income groups possess the same skills. In states such as Washington, critics have questioned whether this ensures success for all or just creates massive failure (as only half of all 10th graders have demonstrated they can meet the standards).[40]		The construction industry is perhaps the heaviest user of apprenticeship programs in the United States, with the US Department of Labor reporting 74,164 new apprentices accepted in 2007 at the height of the construction boom. Most of these apprentices participated in what are called "joint" apprenticeship programs, administered jointly by construction employers and construction labor unions.[41] For example, the International Union of Painters and Allied Trades (IUPAT) has opened the Finishing Trades Institute (FTI). The FTI is working towards national accreditation so that it may offer associate and bachelor's degrees that integrate academics with a more traditional apprentice programs. The IUPAT has joined forces with the Professional Decorative Painters Association (PDPA) to build educational standards using a model of apprenticeship created by the PDPA.		Persons interested in learning to become electricians can join one of several apprenticeship programs offered jointly by the International Brotherhood of Electrical Workers and the National Electrical Contractors Association. No background in electrical work is required. A minimum age of 18 is required. There is no maximum age. Men and women are equally invited to participate. The organization in charge of the program is called the National Joint Apprenticeship and Training Committee [1].		Apprentice electricians work 32 to 40+ hours per week at the trade under the supervision of a journeyman wireman and receive pay and benefits. They spend an additional 8 hours every other week in classroom training. At the conclusion of training (five years for inside wireman and outside lineman, less for telecommunications), apprentices reach the level of journeyman wireman. All of this is offered at no charge, except for the cost of books (which is approximately $200–600 per year), depending on grades. Persons completing this program are considered highly skilled by employers and command high pay and benefits. Other unions such as the United Brotherhood of Carpenters and Joiners of America, United Association of Plumbers, Fitters, Welders and HVAC Service Techs, Operating Engineers, Ironworkers, Sheet Metal Workers, Plasterers, Bricklayers and others offer similar programs.		Trade associations such as the Independent Electrical Contractors and Associated Builders and Contractors also offer a variety of apprentice training programs. Registered programs also are offered by the Aerospace Joint Apprenticeship Committee (AJAC) to fill a shortage of aerospace and advanced manufacturing workers in Washington State.		For FDA-regulated industries such as food, pharmaceuticals, medical devices, nutraceuticals and cosemecuticals, companies may offer apprenticeships in Quality Assurance, Quality Control, Medical Affairs (MSLs), Clinical Trials, or Regulatory Affairs. Apprentices may be placed at a host company and must continue to work toward an industry certification such as those offered by ASQ or RAPS while they remain in the apprenticeship. The costs of training and mentorship can be covered by the program and apprentices receive full pay and benefits.		A modified form of apprenticeship is required for before an engineer is licensed as a Professional Engineer in any of the states of the United States. In the United States, regulation of professional engineering licenses is the right and responsibility of the federated state. That is, each of the 50 states sets its own licensing requirements and issues (and, if needed, revokes) licenses to practice engineering in that state.		Although the requirements can vary slightly from state to state, in general to obtain a Professional Engineering License in a given state, one must graduate with Bachelor of Science in Engineering from an accredited college or university, pass the Fundamentals of Engineering (FE) Exam, which designates the title of Engineer in Training (EIT), work in that discipline for at least four years under a licensed Professional Engineer (PE), and then pass the Principles and Practice of Engineering Exam. One and two years of experience credit is given for those with qualifying master’s and doctoral degrees, respectively.[42]		In most cases the states have reciprocity agreements so that once an individual becomes licensed in one state can also become licensed in other states with relative ease.		The modern concept of an internship is similar to an apprenticeship but not as rigorous. Universities still use apprenticeship schemes in their production of scholars: bachelors are promoted to masters and then produce a thesis under the oversight of a supervisor before the corporate body of the university recognises the achievement of the standard of a doctorate. Another view of this system is of graduate students in the role of apprentices, post-doctoral fellows as journeymen, and professors as masters .[citation needed] In the "Wealth of Nations" Adam Smith states that		Seven years seem anciently to have been, all over Europe, the usual term established for the duration of apprenticeships in the greater part of incorporated trades. All such incorporations were anciently called universities, which indeed is the proper Latin name for any incorporation whatever. The university of smiths, the university of tailors, etc., are expressions which we commonly meet with in the old charters of ancient towns [...] As to have wrought seven years under a master properly qualified was necessary in order to entitle any person to become a master, and to have himself apprenticed in a common trade; so to have studied seven years under a master properly qualified was necessary to entitle him to become a master, teacher, or doctor (words anciently synonymous) in the liberal arts, and to have scholars or apprentices (words likewise originally synonymous) to study under him.[43]		Also similar to apprenticeships are the professional development arrangements for new graduates in the professions of accountancy, engineering, management consulting, and the law. A British example was training contracts known as 'articles of clerkship'. The learning curve in modern professional service firms, such as law firms, consultancies or accountancies, generally resembles the traditional master-apprentice model: the newcomer to the firm is assigned to one or several more experienced colleagues (ideally partners in the firm) and learns his skills on the job.		
Unemployment extension occurs when regular unemployment benefits are exhausted and extended for additional weeks. Unemployment extensions are created by passing new legislation at the federal level, often referred to as an "unemployment extension bill." This new legislation is introduced and passed during times of high or above average unemployment rates. Unemployment extensions are set during a date range in order to estimate their federal cost. After expiration, the unemployment data is re-evaluated, and new legislation may be proposed and passed to further extend them.						In the United States, there is a standard of 26 weeks of unemployment benefits, known as "regular unemployment insurance (UI) benefits." There are two programs for extending unemployment insurance (UI) benefits:[1] Emergency Unemployment Compensation (EUC) and Extended Benefits (EB).		EUC is a 100% federally funded program that provides benefits to individuals who have exhausted regular state benefits. The EUC program was created on June 30, 2008, and has been modified several times. Most recently, the American Taxpayer Relief Act of 2012 (P.L. 112-240) extended the expiration date of the EUC program to January 1, 2014. To date, Congress has not passed any further extensions.[2]		EUC has four levels: Tiers 1, 2, 3 and 4.[3]		EUC08, or Emergency Unemployment Compensation 2008, is an extension of unemployment benefits authorized under Federal law. The Middle Class Tax Relief and Job Creation Act of 2012 (enacted on Feb 22, 2012) modified EUC08.[4][5]		Claimants who filed an initial claim effective on or after May 7, 2006 are potentially eligible for EUC08. Individuals who are monetarily ineligible when a new benefit year is filed may qualify for EUC08 on the basis of a previous claim.								Extended Benefits are available to workers who have exhausted regular unemployment insurance benefits during periods of high unemployment. The basic Extended Benefits program provides up to 13 additional weeks of benefits when a State is experiencing high unemployment. Some States have also enacted a voluntary program to pay up to 7 additional weeks (20 weeks maximum) of Extended Benefits during periods of extremely high unemployment.[6]		In addition to individuals who exhausted all rights to benefits after the original date of enactment, the EUC law includes what is commonly referred to as a reachback provision. A reachback provision makes eligible those individuals who exhausted all rights to benefits prior to the original date of enactment (back to a specified date). The EUC law makes eligible all individuals whose benefit year ending date is on or after May 1, 2007. Practically speaking, this means some individuals receiving EUC may have become unemployed as early as May 2006.[7]		
A career is an individual's journey through learning, work and other aspects of life. There are a number of ways to define a career and the term is used in a variety of ways.						The word career is defined by the Oxford English Dictionary as a person's "course or progress through life (or a distinct portion of life)". In this definition career is understood to relate to a range of aspects of an individual's life, learning and work. Career is also frequently understood to relate to the working aspects of an individual's life e.g. as in career woman. A third way in which the term career is used to describe an occupation or a profession that usually involves special training or formal education,[1] and is considered to be a person’s lifework.[2] In this case "a career" is seen as a sequence of related jobs usually pursued within a single industry or sector e.g. "a career in education" or "a career in the building trade".		For a pre-modernist notion of "career", compare cursus honorum.		By the late 20th century, a wide range of choices (especially in the range of potential professions) and more widespread education had allowed it to become possible to plan (or design) a career: in this respect the careers of the career counselor and of the career advisor have grown up. It is also not uncommon for adults in the late 20th/early 21st centuries to have dual or multiple careers, either sequentially or concurrently. Thus, professional identities have become hyphenated or hybridized to reflect this shift in work ethic. Economist Richard Florida notes this trend generally and more specifically among the "creative class".		Career management describes the active and purposeful management of a career by an individual. Ideas of what comprise "career management skills" are described by the Blueprint model (in the United States, Canada, Australia, Scotland, and England[3])[4] and the Seven C's of Digital Career Literacy (specifically relating to the Internet skills).[5]		Key skills include the ability to reflect on one's current career, research the labour market, determine whether education is necessary, find openings, and make career changes.		According to Behling and others, an individual's decision to join a firm may depend on any of the three factors viz. objective factor, subjective factor and critical contact.[6]		These theories assume that candidates have a free choice of employers and careers. In reality the scarcity of jobs and strong competition for desirable jobs severely skews the decision making process. In many markets employees work particular careers simply because they were forced to accept whatever work was available to them. Additionally, Ott-Holland and colleagues found that culture can have a major influence on career choice, depending on the type of culture.[7]		When choosing a career that's best for you, according to US News, there are multiple things to consider. Some of those include: natural talents, work style, social interaction, work-life balance, whether or not you are looking to give back, whether you are comfortable in the public eye, dealing with stress or not, and finally, how much money you want to make. If choosing a career feels like too much pressure, here's another option: pick a path that feels right today by making the best decision you can, and know that you can change your mind in the future. In today's workplace, choosing a career doesn't necessarily mean you have to stick with that line of work for your entire life. Make a smart decision, and plan to re-evaluate down the line based on your long-term objectives.[8]		Changing occupation is an important aspect of career and career management. Over a lifetime, both the individual and the labour market will change; it is to be expected that many people will change occupations during their lives. Data collected by the U.S. Bureau of Labor Statistics through the National Longitudinal Survey of Youth in 1979 showed that individuals between the ages of 18 and 38 will hold more than 10 jobs.[9]		A survey conducted by Right Management[10] suggests the following reasons for career changing.		According to an article on Time.com, one out of three people currently employed (as of 2008) spends about an hour per day searching for another position.[10]		Career success is a term used frequently in academic and popular writing about career. It refers to the extent and ways in which an individual can be described as successful in his or her working life so far.[11]		Traditionally, career success has often been thought of in terms of earnings and/or status within an occupation or organisation. This can be expressed either in absolute terms (e.g. the amount a person earns) or in relative terms (e.g. the amount a person earns compared with their starting salary). Earnings and status are examples of objective criteria of success, where "objective" means that they can be factually verified, and are not purely a matter of opinion.		Many observers argue that careers are less predictable than they once were, due to the fast pace of economic and technological change.[12] This means that career management is more obviously the responsibility of the individual rather than his or her employing organisation, because a "job for life" is a thing of the past. This has put more emphasis on subjective criteria of career success.[13] These include job satisfaction, career satisfaction, work-life balance, a sense of personal achievement, and attaining work that is consistent with one's personal values. A person's assessment of his or her career success is likely to be influenced by social comparisons, such as how well family members, friends, or contemporaries at school or college have done.[14]		The amount and type of career success a person achieves is affected by several forms of career capital.[15] These include social capital (the extent and depth of personal contacts a person can draw upon), human capital (demonstrable abilities, experiences and qualifications), economic capital (money and other material resources which permit access to career-related resources), and cultural capital (having skills, attitudes or general know-how to operate effectively in a particular social context).[16]		There are a range of different educational, counseling, and human resource management interventions that can support individuals to develop and manage their careers. Career support is commonly offered while people are in education, when they are transitioning to the labour market, when they are changing career, during periods of unemployment, and during transition to retirement. Support may be offered by career professionals, other professionals or by non-professionals such as family and friends. Professional career support is sometimes known as "career guidance" as in the OECD definition of career guidance:		The activities may take place on an individual or group basis, and may be face-to-face or at a distance (including helplines and web-based services). They include career information provision (in print, ICT-based and other forms), assessment and self-assessment tools, counselling interviews, career education programmes (to help individuals develop their self-awareness, opportunity awareness, and career management skills), taster programmes (to sample options before choosing them), work search programmes, and transition services."[17]		However this use of the term "career guidance" can be confusing as the term is also commonly used to describe the activities of career counselors.		Career support is offered by a range of different mechanisms. Much career support is informal and provided through personal networks or existing relationships such as management. There is a market for private career support however the bulk of career support that exists as a professionalised activity is provided by the public sector.[citation needed]		Key types of career support include:		Some research shows adding one year of schooling beyond high school creates an increase of wages 17.8% per worker. However, additional years of schooling, beyond 9 or 10 years, have little effect on worker's wages. In summary, better educated, bigger benefits. In 2010, 90% of the U.S. Workforce had a high school diploma, 64% had some college, and 34% had at least a bachelor's degree.[23]		The common problem that people may encounter when trying to achieve an education for a career is the cost. The career that comes with the education must pay well enough to be able to pay off the schooling. The benefits of schooling can differ greatly depending on the degree (or certification) obtained, the programs the school may offer, and the ranking of the school. Sometimes, colleges provide students more with just education to prepare for careers. It is not uncommon for colleges to provide pathways and support straight into the workforce the students may desire.[24]		Much career support is delivered face-to-face, but an increasing amount of career support is delivered online.[5]		
This article lists the statutory retirement age in different countries. In some contexts, the retirement age is the age at which a person is expected or required to cease work and is usually the age at which they may be entitled to receive superannuation or other government benefits, like a state pension Policy makers usually consider the demography, fiscal cost of ageing, health, life expectancy, nature of profession, supply of labour force etc. while deciding the retirement age.[1]						Many of the countries listed in the table below are in the process of reforming the ages (see the notes in the table for details). The ages in the table show when an individual retires if they retire/have retired in the year given in the table; the trend in some countries is that in the future the age will increase gradually (where available, explanations are given in the section on notes), therefore one's year of birth determines when one has the age of retirement (e.g. in Romania women born in January 1955 had the retirement age in January 2015 at age 60; those born in January 1958 will retire in January 2019 at age 61; those born in January 1961 will retire in January 2023 at age 62; those born in January 1967 will retire in January 2030 at age 63).[2]		Retiring age will be standardised to 66 year and 7 months for both men and women from January 1, 2018.		The average of statutory retirement age in the 34 countries of the Organisation for Economic Co-operation and Development (OECD) in 2014 was males 65 years and females 63.5 years, but the tendency all over the world is to increase the retirement age.[63] This is also reflected by the findings that just over half the Asian investors surveyed region-wide said they agreed with raising the retirement age, with a quarter disagreeing and the remainder undecided.[64]		Men either retire later than women or at the same time. This inequality is being addressed in some countries where the retirement ages are being equalised.		Reforms tend to be phased-in slowly when the retirement age (or pension age) is increased, with grandfathering ensuring a gradual change. In contrast, when the age of retirement is decreased, changes are often brought about rapidly.[65]		One such example of grandfathering are the transitional pension rules which were applied for staff aged 54 years or older, and to some extent for all staff in place, when in 2014 the retirement age of European civil servants was increased to 66 years of age.[66]		
A professional association (also called a professional body, professional organization, or professional society) is usually a nonprofit organization seeking to further a particular profession, the interests of individuals engaged in that profession and the public interest.		The roles of these professional associations have been variously defined: "A group of people in a learned occupation who are entrusted with maintaining control or oversight of the legitimate practice of the occupation;"[1] also a body acting "to safeguard the public interest;"[2] organizations which "represent the interest of the professional practitioners," and so "act to maintain their own privileged and powerful position as a controlling body."[2]		Many professional bodies are involved in the development and monitoring of professional educational programs, and the updating of skills, and thus perform professional certification to indicate that a person possesses qualifications in the subject area. Sometimes membership of a professional body is synonymous with certification, though not always. Membership of a professional body, as a legal requirement, can in some professions form the primary formal basis for gaining entry to and setting up practice within the profession; see licensure.		Many professional bodies also act as learned societies for the academic disciplines underlying their professions.		As a practical matter, most professional organizations of global scope (see List of professional organizations) are located in the United States. The U.S. has often led the transformation of various occupations into professions, a process described in the academic literature as professionalization.						In the United States, PA (Professional Association), used in conjunction with a business name is a corporation formed by professionals such as barristers, engineers, dentists, and medical doctors. In the past, the so-called "learned professions" were not allowed to operate as corporations. A PA is attractive to professionals because it provides some of the tax advantages and liability protections of a business corporation.[3]		
Temporary work or temporary employment refers to an employment situation where the working arrangement is limited to a certain period of time based on the needs of the employing organization. Temporary employees are sometimes called "contractual", "seasonal", "interim", "casual staff", "outsourcing", "freelance"; or the word may be shortened to "temps". In some instances, temporary, highly skilled professionals (particularly in the white-collar worker fields, such as law, engineering, and accounting) refer to themselves as consultants.		Temporary workers may work full-time or part-time depending on the individual situation. In some instances, temporary workers receive benefits (such as health insurance), but usually benefits are only given to permanent employees as a cost-cutting measure by the employer to save money. Not all temporary employees find jobs through a temporary employment agency. With the rise of the Internet and 'gig economy'[definition needed], many workers are now finding short-term jobs through freelance marketplaces: a situation that brings into being a global market for work[1].		A temporary work agency, temp agency or temporary staffing firm finds and retains workers. Other companies, in need of short-term workers, contract with the temporary work agency to send temporary workers, or temps, on assignments to work at the other companies. Temporary employees are also used in work that has a cyclical nature, requiring frequent adjustments to staffing levels.						The staffing industry in the United States began after World War II with small agencies in urban areas employing housewives for part-time work as office workers. Over the years the advantages of having workers who could be hired and fired on short notice and were exempt from paperwork and regulatory requirements resulted in a gradual but substantial increase in the use of temporary workers, with over 3.5 million temporary workers employed in the United States by 2000.[2]		There has indeed been a great paradigm shift since the 1940s in the way firms utilize the temporary worker. Throughout the Fordist era, temporary workers made up a rather marginal proportion of the total labor force in North America. Typically, temporary workers were white women in pink collar, clerical positions who provided companies with a stop-gap solution for permanent workers who needed a leave of absence, when on vacation or in illness.[3] In contrast, in the Post-Fordist period, characterized by neoliberalism, deindustrialization and the dismantling of the welfare state, these understandings of temporary labor began to shift.[4] In this paradigm, the idea of the temporary worker as a stopgap solution to permanent labor became an entirely normative employment alternative to permanent work.[5]		Therefore, temporary workers no longer represented a substitute for permanent workers on leave but became semi-permanent, precarious positions routinely subject to the threat of elimination because of fluctuations in a company's products. In the context of today's temporary labor force, both people and positions have become temporary, and temporary agencies use the temporary worker in a systematic and planned, as opposed to impromptu manner.[3]		As the market began to transform from Fordism to a post-Order regime of capital accumulation, the social regulation of labor markets and the very nature of work began to shift.[6] This transformation has been characterized by an economic restructuring that emphasized flexibility within spaces of work, labor markets, employment relationships, wages and benefits. Most governments in Western Europe started to deregulate temporary work.[7] And indeed, global processes of neoliberalism and market rule contributed greatly to this increasing pressure put on local labor markets towards flexibility.[8] This greater flexibility within labor markets is important at the global level, particularly within OECD countries and liberal market economies (see liberal market economy).		The temporary labor industry is worth over €157 billion per year, and the largest agencies are spread across over 60 nations. The biggest temporary work agencies are most profitable in emerging economies of the Global North, and those that have undergone market liberalization, deregulation and (re)regulation.[9]		The desire to market flexible, adaptable temporary workers has become a driving, monetary oriented objective of the temporary work industry. This has caused individual agencies to adopt practices that focus on competition with other firms, that promote “try before you buy” practices and that maximize their ability to produce a product: the temporary worker. Through this process, the ideal temporary worker has today become largely imagined, produced and marketed by temporary agencies.[10]		The role of a temp agency is as a third party between client employer and client employee. This third party, handles remuneration, work scheduling, complaints, taxes, etc. created by the relationship between a client employer and a client employee. Client firms request the type of job that is to be done, and the skills required to do it. Client firms can also terminate an assignment and are able to file a complaint about the temp.[11][12] Work schedules are determined by assignment, which is determined by the agency and can last for an indeterminate period of time, extended to any point and cut short.[11] Because the assignments are temporary, there is little incentive to provide benefits and the pay is low in situations where there is a lot of labor flexibility. (Nurses are an exception to this as there is currently a shortage).[11][12][13] Workers can refuse assignment but risk going through an indeterminate period of downtime since work is based on availability of assignments, which the agency cannot “create” only fill.[11]		Whether the work comes through an independent gig economy source or a temp agency, when a temporary employee[14] agrees to an assignment, they receive instructions pertaining to the job. The agency also provides information on correct work attire, work hours, wages, and whom to report to. If a temporary employee arrives at a job assignment and is asked to perform duties not described when they accepted the job, they may call an agency representative for clarification. If they choose not to continue on the assignment based on these discrepancies, they will most likely lose pay and may undermine chances at other job opportunities. However, some agencies guarantee an employee a certain number of hours pay if, once the temporary employee arrives, there is no work or the work isn't as described. Most agencies do not require an employee to continue work if the discrepancies are enough to make it difficult for the employee to actually do the work.[15]		A temporary work agency may have a standard set of tests to judge the competence of the secretarial or clerical skills of an applicant. An applicant is hired based on their scores on these tests, and is placed into a database. Companies or individuals looking to hire someone temporarily contact the agency and describe the skill set they are seeking. A temporary employee is then found in the database and is contacted to see if they would be interested in taking the assignment.[15]		It is up to the temporary employee to keep in constant contact with the agency when not currently working on an assignment; by letting the agency know that they are available to work they are given priority over those who may be in the agency database who have not made it clear that they are ready and willing to take an assignment. A temp agency employee is the exclusive employee of the agency, not of the company in which they are placed (although subject to legal dispute). The temporary employee is bound by the rules and regulations of the client firm, even if they contrast with those of the company in which they are placed.		There are a number of reasons as to why a firm utilizes temp agencies. They provide employers a way to add more workers for a short term increase in the workforce. Using temps allows firms to replace a missing regular employee. A temp worker’s competency and value can be determined without the inflexibility of hiring a new person and seeing how they work out. Utilizing temp workers can also be a way of not having to pay benefits and the increased salaries of a regular employees. A firm can also use temp workers to vary compensation in what would normally be an illegal or impossible manner. The role of temp workers in the work space can also have the effects of coercing regular employees into taking more work for less pay. Additionally, temp workers are less likely to sue over mistreatment, which allows firms to reduce the costs of employment in high-stress, regulated jobs.[11][12][13][16]		Temp agencies are a growing part of industrial economies. From 1961-1971 the number of employees sent out by temporary staffing agencies increased by 16 percent. Temporary staffing industry payrolls increased by 166 percent from 1971 to 1981, and 206 percent from 1981 to 1991, and 278 percent from 1991 to 1999. The temporary staffing sector accounted for 1 out of 12 new jobs in the 90’s.[16] In 1996, $105 billion, worldwide,in staffing agency revenues. By 2008, $300 billion was generated, worldwide, in revenues for staffing agencies.[17] The Temporary Staffing Industry accounts for 16% of job growth in the U.S. since the great recession ended, even though it only accounts for 2% of all-farm jobs.[18] This growth has occurred for a number of reasons. Demand in temporary employment can be primarily attributed to demand by employers and not employees [13][19] A large driver of demand was in European labor market. Previously, temporary employment agencies were considered quasi-legal entities. This reputation shied potential client employers away. However, in the later half of the 20th century, there would be shift predominated by legal protections and closer relationships with primary employers. This combined with the tendency for growth of the TSI in countries where there are strict regulations on dismissal of hired employees but loose regulations on temporary work, growth is much faster compared to industrialized nations without these labor conditions.[19][20]		Staffing agencies are prone to improper behavior just like any other employer.[11][17] There have been cases of some temp agencies that have created and reinforced an ethnic hierarchy that determines who gets what jobs.[12]		Temps have been told to be a “guest” and not a worker, which can lead to worker exploitation. One ramification is that temps have to deal with sexual harassment and are sometimes encouraged not to report it, and in some rare cases encouraged to make themselves “sexually available”.		An additional ramification of temp workers “guest” status is being at the bottom of the workplace hierarchy, which is visually identifiable on ID cards, in different colored uniforms, as well as the encouragement of more “provocative dress”.[12] Their “guest” status often means, temp Workers are unable to access on-site workplace accommodations and aren’t included in meetings despite the length of their time working at the client firm.[11][12][21]		This is all compounded by a work system in which temps must file complaints about clients through the temp agencies, which, often enough, not only disqualifies them from another assignment at that firm, it disqualifies them from receiving an assignment from that temporary agency upon review.[11] Since a client firm is harder to replace than a client employee and there is no disincentive to not giving a complaining employee, an assignment; there is an incentive for agencies to find employees who are willing to go along with the conditions for client firms, as opposed to severing ties with firms that routinely violate the law.[11]		Temporary workers are at a high risk of being injured or killed on the job. In 2015, 829 fatal injuries (17% of all occupational fatalities) occurred among contract workers.[22] Studies have also shown a higher burden of non-fatal occupational injuries and illnesses among temporary workers compared to those in standard employment arrangements.[23][24] There are many possible contributing factors to the high rates of injuries and illnesses among temporary workers. They are often inexperienced and assigned to hazardous jobs and tasks,[25][24][26][27] may be reluctant to object to unsafe working conditions or to advocate for safety measures due to fear of job loss or other repercussions,[26] and they may lack basic knowledge and skills to protect themselves from workplace hazards due to insufficient safety training.[28] According to a joint guidance document released by the Occupational Safety and Health Administration (OSHA) and the National Institute for Occupational Safety and Health (NIOSH), both staffing agencies and host employers (i.e., the clients of staffing agencies) are responsible for providing and maintaining a safe and healthy work environment for temporary workers.[29] Collaborative and interdisciplinary (e.g., epidemiology, occupational psychology, organizational science, economics, law, management, sociology, labor health and safety) research and intervention efforts are needed to protect and promote the occupational safety and health of temporary workers.[30]		Workers, scholars, union organisers and activists have identified many cons associated with temporary work, and more recently the gig economy.[32] These include:		Scholars have argued that neoliberal policies have been a prominent component in the erosion of the standard employment relationship. This precarious new model of employment has greatly reduced the worker’s ability to negotiate and, in particular, with the introduction of advanced technology (that can easily replace the worker), reduced the temp’s bargaining power.[34] Internet of Things-based companies such as Uber have come into conflict with authorities and workers for circumventing labour and social security obligations.[35][36] It has been suggested that labour regulations in North America do little in addressing labour market insecurities and the precarious nature of temporary labour. In many cases, legislation has done little to acknowledge or adapt to the growth of non-standard employment in Canada.[37]		In the European Union, temporary work is regulated by the Temporary Agency Work Directive and the Member States' laws implementing that directive.[38]		
A tenured appointment is an indefinite appointment that can be terminated only for cause or under extraordinary circumstances such as financial exigency or program discontinuation. Tenure defends the principle of academic freedom, which holds that it is beneficial for society in the long run if scholars are free to hold and examine a variety of views. Elementary, middle, and secondary school teachers can also be granted tenure in some places.		Since 1915, the American Association of University Professors[1] has developed standards to guide higher education in service of the common good. The modern conception of tenure in US higher education originated with the AAUP's 1940 Statement of Principles on Academic Freedom and Tenure.[2] Jointly formulated and endorsed by the AAUP and the Association of American Colleges and Universities (AAC&U), the 1940 Statement is endorsed by over 250 scholarly and higher education organizations and is widely adopted into faculty handbooks and collective bargaining agreements at institutions of higher education throughout the United States.[3] This statement holds that, "The common good depends upon the free search for truth and its free exposition" and stresses that academic freedom is essential in teaching and research in this regard.		Some have argued that modern tenure systems diminish academic freedom, forcing those seeking tenured positions to profess conformance to the level of mediocrity as those awarding the tenured professorships. For example, according to physicist Lee Smolin, "...it is practically career suicide for a young theoretical physicist not to join the field of string theory."[4] Some U.S. states considered legislation to remove tenure at public universities.[5] However, many argue, among other things, that the job security granted by tenure is necessary to recruit talented individuals into professorships, because in many fields private industry jobs pay significantly more. Economist Steven Levitt has suggested that if tenure is removed, universities will need to raise faculty salaries to compensate for the lost job security. He also states that it would attract those who teach for the love of teaching rather than those looking to retire.[6]				
In organizational behavior and industrial and organizational psychology, organizational commitment is the individual's psychological attachment to the organization. The basis behind many of these studies was to find ways to improve how workers feel about their jobs so that these workers would become more committed to their organizations. Organizational commitment predicts work variables such as turnover, organizational citizenship behavior, and job performance. Some of the factors such as role stress, empowerment, job insecurity and employability, and distribution of leadership have been shown to be connected to a worker's sense of organizational commitment. employee experiences a 'sense of oneness' with their organization.		Organizational scientists have also developed many nuanced definitions of organizational commitment, and numerous scales to measure them. Exemplary of this work is Meyer and Allen's model of commitment, which was developed to integrate numerous definitions of commitment that had been proliferated in the literature. Meyer and Allen's model has also been critiqued because the model is not consistent with empirical findings. It may also not be fully applicable in domains such as customer behavior. There has also been debate surrounding what Meyers and Allen's model was trying to achieve.						Meyer and Allen's (1991) three-component model of commitment was created to argue that commitment has three different components that correspond with different psychological states. Meyer and Allen created this model for two reasons: first "aid in the interpretation of existing research" and second "to serve as a framework for future research".[1] Their study was based mainly around previous studies of organizational commitment. Meyer and Allen's research indicated that there are three "mind sets" which can characterize an employee's commitment to the organization. Mercurio (2015) extended this model by reviewing the empirical and theoretical studies on organizational commitment. Mercurio posits that emotional, or affective commitment is the core essence of organizational commitment.[2]		AC is defined as the employee's positive emotional attachment to the organization. Meyer and Allen pegged AC as the "desire" component of organizational commitment. An employee who is affectively committed strongly identifies with the goals of the organization and desires to remain a part of the organization. This employee commits to the organization because he/she "wants to". This commitment can be influenced by many different demographic characteristics: age, tenure, sex, and education but these influences are neither strong nor consistent. The problem with these characteristics is that while they can be seen, they cannot be clearly defined. Meyer and Allen gave this example that "positive relationships between tenure and commitment maybe due to tenure-related differences in job status and quality"[1] In developing this concept, Meyer and Allen drew largely on Mowday, Porter, and Steers's (2006)[3] concept of commitment, which in turn drew on earlier work by Kanter (1968)[4] Mercurio (2015) stated that..."affective commitment was found to be an enduring, demonstrably indispensable, and central characteristic of organizational commitment".[5]		Continuance commitment is the "need" component or the gains versus losses of working in an organization. "Side bets", or investments, are the gains and losses that may occur should an individual stay or leave an organization. An individual may commit to the organization because he/she perceives a high cost of losing organizational membership (cf. Becker's 1960 "side bet theory"[6] Things like economic costs (such as pension accruals) and social costs (friendship ties with co-workers) would be costs of losing organizational membership. But an individual doesn't see the positive costs as enough to stay with an organization they must also take into account the availability of alternatives (such as another organization), disrupt personal relationships, and other "side bets" that would be incurred from leaving their organization. The problem with this is that these "side bets" don't occur at once but that they "accumulate with age and tenure".[1]		The individual commits to and remains with an organization because of feelings of obligation, the last component of organizational commitment. These feelings may derive from a strain on an individual before and after joining an organization. For example, the organization may have invested resources in training an employee who then feels a 'moral' obligation to put forth effort on the job and stay with the organization to 'repay the debt.' It may also reflect an internalized norm, developed before the person joins the organization through family or other socialization processes, that one should be loyal to one's organization. The employee stays with the organization because he/she "ought to". But generally if an individual invest a great deal they will receive "advanced rewards". Normative commitment is higher in organizations that value loyalty and systematically communicate the fact to employees with rewards, incentives and other strategies. Normative commitment in employees is also high where employees regularly see visible examples of the employer being committed to employee well-being. An employee with greater organizational commitment has a greater chance of contributing to organizational success and will also experience higher levels of job satisfaction. High levels of job satisfaction, in turn, reduces employee turnover and increases the organization's ability to recruit and retain talent. Meyer and Allen based their research in this area more on theoretical evidence rather than empirical, which may explain the lack of depth in this section of their study compared to the others. They drew off Wiener's (2005) [7] research for this commitment component.		Since the model was made, there has been conceptual critique to what the model is trying to achieve. Specifically from three psychologists, Omar Solinger, Woody Olffen, and Robert Roe. To date, the three-component conceptual model has been regarded as the leading model for organizational commitment because it ties together three aspects of earlier commitment research (Becker, 2005; Buchanan, 2005; Kanter, 1968; Mathieu & Zajac, 1990; Mowday, Porter, & Steers, 1982; Salancik, 2004; Weiner, 2004; Weiner & Vardi, 2005). However, a collection of studies have shown that the model is not consistent with empirical findings. Solinger, Olffen, and Roe use a later model by Alice Eagly and Shelly Chaiken, Attitude-behavior Model (2004), to present that TCM combines different attitude phenomena. They have come to the conclusion that TCM is a model for predicting turnover. In a sense the model describes why people should stay with the organization whether it is because they want to, need to, or ought to. The model appears to mix together an attitude toward a target, that being the organization, with an attitude toward a behavior, which is leaving or staying. They believe the studies should return to the original understanding of organizational commitment as an attitude toward the organization and measure it accordingly. Although the TCM is a good way to predict turnover, these psychologists do not believe it should be the general model. Because Eagly and Chaiken's model is so general, it seems that the TCM can be described as a specific subdivision of their model when looking at a general sense of organizational commitment. It becomes clear that affective commitment equals an attitude toward a target, while continuance and normative commitment are representing different concepts referring to anticipated behavioral outcomes, specifically staying or leaving. This observation backs up their conclusion that organizational commitment is perceived by TCM as combining different target attitudes and behavioral attitudes, which they believe to be both confusing and logically incorrect. The attitude-behavioral model can demonstrate explanations for something that would seem contradictory in the TCM. That is that affective commitment has stronger associations with relevant behavior and a wider range of behaviors, compared to normative and continuance commitment. Attitude toward a target (the organization) is obviously applicable to a wider range of behaviors than an attitude toward a specific behavior (staying). After their research, Sollinger, Olffen, and Roe believe Eagly and Chaiken's attitude-behavior model from 1993 would be a good alternative model to look at as a general organizational commitment predictor because of its approach at organizational commitment as a singular construct, which in turn would help predicting various behaviors beyond turnover.[8]		More recently, scholars have proposed a five component model of commitment, though it has been developed in the context of product and service consumption. This model proposes habitual and forced commitment as two additional dimensions which are very germane in consumption settings. It seems, however, that habitual commitment or inertial may also become relevant in many job settings. People get habituated to a job—the routine, the processes, the cognitive schemas associated with a job can make people develop a latent commitment to the job—just as it may occur in a consumption setting. The paper—by Keiningham and colleagues also compared applications of the TCM in job settings and in consumption settings to develop additional insights.		Job satisfaction is commonly defined as the extent to which employees like their work. Researchers have examined Job satisfaction for the past several decades. Studies have been devoted to figuring out the dimensions of job satisfaction, antecedents of job satisfaction, and the relationship between satisfaction and commitment. Satisfaction has also been examined under various demographics of gender, age, race, education, and work experience. Most research on job satisfaction has been aimed towards the person-environment fit paradigm. Job satisfaction has been found to be an important area of research because one of the top reasons individuals give for leaving a job is dissatisfaction.[9]		Much of the literature on the relationship between commitment and satisfaction with one's job indicates that if employees are satisfied they develop stronger commitment to their work. Kalleberg (1990) studied work attitudes of workers in the USA and Japan and found a correlation of 0.73 between job satisfaction and organizational commitment of workers in Japan and a higher significant correlation of 0.81 among Americans. A study conducted by Dirani and Kuchinke produced results indicating a strong correlation between job commitment and job satisfaction and that satisfaction was a reliable predictor of commitment.[9][10] Job satisfaction among employees—at least in retail settings—can also strengthen the association between customer satisfaction and customer loyalty.		A study conducted by Hulpia et al. focused on the impact of the distribution of leadership and leadership support among teachers and how that affected job satisfaction and commitment. The study found that there was a strong relationship between organizational commitment and the cohesion of the leadership team and the amount of leadership support. Previously held beliefs about job satisfaction and commitment among teachers was that they were negatively correlated with absenteeism and turnover and positively correlated with job effort and job performance. This study examined how one leader (usually a principal) effected the job satisfaction and commitment of teachers. The study found that when leadership was distributed by the 'leader' out to the teachers as well workers reported higher job satisfaction and organizational commitment than when most of the leadership fell to one person. Even when it was only the perception of distributed leadership roles workers still reported high levels of job satisfaction/commitment.[15]		By the end of the 1990s, leaders did not find the value in understanding whether or not their people were more or less committed to the organization. It was particularly frustrating that leaders could see that people committed to the organization were not as committed to strategic change initiatives, the majority of which failed to live up to expectations. John Meyer responded to this gap by proposing a model of organizational change commitment.[16] The new model includes the same 3-components, but also includes a behavioral commitment scale: resistance, passive resistance, compliance, cooperation, and championing.[17] Though Meyer does not cite him, a peer reviewed source for behavioral commitment comes from Leon Coetsee in South Africa.[18] Coetsee brought the resistance-to-commitment model[19] of Harvard consultant Arnold Judson[20] to academic research and has continued developing the model as late as 2011.[21]		Five rules help to enhance organizational commitment:[22]		
An employee handbook, sometimes also known as an employee manual, staff handbook, or company policy manual, is a book given to employees by an employer. Usually, the employee handbook contains several key sections and includes information about company culture, policies, and procedures.[1]		The employee handbook can be used to bring together employment and job-related information which employees need to know. It typically has three types of content:[2]		The employee handbook is almost always a part of a company's onboarding or induction process for new staff. A written employee handbook gives clear advice to employees and creates a culture where issues are dealt with fairly and consistently.						While it often varies from business to business, specific areas that an employee handbook may address include:		Revisions to an employee handbook vary from company to company. At many larger companies, a revised handbook comes out annually or at other regular intervals. It is recommended that handbooks be updated regularly as laws and regulations change.		New employees are often required to sign an acknowledgement form stating they have received, read and understand the information within the employee handbook and accept its terms.[3]		Acknowledgement forms typically have additional content:		Failure of an employee to sign the acknowledgement form within a timely manner may prevent them from being hired or may result in termination.		In 2009, Netflix publicly released a presentation entitled Netflix Culture: Freedom & Responsibility that described their movement away from a company culture based on command and control and towards one centered around freedom and responsibility. The presentation went viral and as of March 2017 had nearly 16 million views.		Because the presentation also discusses other common company policies, it is often referred to as Netflix's employee handbook. It has been very influential, with many companies copying their "unlimited" vacation policy[4] and their simple "Act in Netflix’s best interests" expenses policy.		Alongside the Valve Handbook for New Employees and the Zappos Culture Book, the presentation has influenced many companies to create culture-first employee handbooks that highlight their company culture instead of or before policies that would be found in a traditional employee handbook.[5] Even the policies are rewritten in a much different way than tradition would dictate, or include informal summaries or introductions in plain language.		Federal and state laws and the growing number of cases of employee-related litigation against management strongly suggests that a written statement of company policy is a business necessity for firms of any size.		For example, the United States Equal Employment Opportunity Commission reported that in 2005, companies paid out more than $378 million in discrimination non-litigated settlements. In 2014, the EEOC received a total of 88,778 discrimination charges filed against private businesses.[6]		Other examples of litigation against a company stemming from employee actions are the release of a customer's private information and, of course, the actions of one employee against another; sexual harassment being one type of offensive employee conduct.		That said, some legal counsels recommend very small companies (less than 10 or 15 employees) to not have a handbook at all because most very small companies find it difficult to dedicate time or resources to maintaining an up-to-date employee handbook. It may be less risky for very small companies to not have documented policies at all rather than having misleading, out-of-date, or incomplete policies.[7]		An effective employee handbook is an obvious, simple and inexpensive answer to the question, "How does a business protect itself against lawsuits based on employee behavior?" There are several key sections that should be included in an employee handbook if a business decides to implement one.[8] There are also several key elements that businesses should consider before implementing an employee handbook.		One of the most important aspects of any employee handbook is that the handbook is kept current as laws change. If a company chooses to publish its handbook in multiple languages, each version should be updated concurrently.		Employee handbooks should be reviewed by an attorney for consistency and compliance with current federal and state or provincial laws. As an example, many US states have specific laws that go above and beyond federal laws. Because of this, a New Mexico employee handbook should not be used in California. In the US, California is the state with the highest number of regulations that go above and beyond federal law. Companies operating there usually have special content for California employees. Other states that will typically require special content are New York, Massachusetts, Illinois, Connecticut, Washington DC, and Texas.		In the United Kingdom, the employee handbook may also form part of an employee’s terms and conditions of employment. If five or more people are employed, it is a requirement of the Health and Safety at Work Act to have a written statement of the company's health and safety policy.[9]		
The Organization for Economic Co-operation and Development defines the employment rate as the employment-to-population ratio. This is a statistical ratio that measures the proportion of the country's working age population (statistics are often given for ages 15 to 64[1][2]) that is employed. This includes people that have stopped looking for work.[3] The International Labour Organization states that a person is considered employed if they have worked at least 1 hour in "gainful" employment in the most recent week.[4]						The employment-population ratio has not always been looked at for labor statistics and where specific areas are economically, but after the recent recession it has been given more attention worldwide, especially by economists. The National Bureau Of Economic Research (NBER) states that the Great Recession ended in June 2009.[5] During 2009 and 2010, however, many areas were still struggling economically, which is the reason the employment-population ratio is still used by both Americans and people around the world.		Key terms that explain the use of the ratio follow:		Employed persons. All those who, (1) do any work at all as paid employees, work in their own business or profession or on their own farm, or work 15 hours or more as unpaid workers in a family-operated enterprise; and (2) all those who do not work but had jobs or businesses from which they were temporarily absent due to illness, bad weather, vacation, childcare problems, labor dispute, maternity or paternity leave, or other family or personal obligations — whether or not they were paid by their employers for the time off and whether or not they were seeking other jobs.		Unemployed persons. All those who, (1) have no employment during the reference week; (2) are available for work, except for temporary illness; and (3) have made specific efforts, such as contacting employers, to find employment sometime during the past 4-week period.		Participant rate This represents the proportion of the population that is in the labor force.		Not in the labor force. Included in this group are all persons in the civilian noninstitutional population who are neither employed nor unemployed. Information is collected on their desire for and availability to take a job at the time of the CPS interview, jobsearch activity in the prior year, and reason for not looking for work in past 4-week period.		Multiple jobholders. These are employed persons who, have two or more jobs as a wage and salary worker, are self-employed and also held a wage and salary job, or work as an unpaid family worker and also hold a wage and salary job.		[6]		The ratio is used to evaluate the ability of the economy to create jobs and therefore is used in conjunction with the unemployment rate for a general evaluation of the labour market stance. Having a high ratio means that an important proportion of the population in working age is employed, which in general will have positive effects on the GDP per capita. Nevertheless, the ratio does not give an indication of working conditions, number of hours worked per person, earnings or the size of the black market. Therefore, the analysis of the labour market must be done in conjunction with other statistics.		This measure comes from dividing the civilian noninstitutionalized population who are employed by the total noninstitutionalized population and multiplying by 100.[7]		In general, a high ratio is considered to be above 70 percent of the working-age population whereas a ratio below 50 percent is considered to be low. The economies with low ratios are generally situated in the Middle East and North Africa.		Employment-to-population ratios are typically higher for men than for women. Nevertheless, in the past decades, the ratios tended to fall for men and increase in the case of women, which made the differences between both to be reduced.		Source: OECD.StatExtracts[1], except as noted		
A social class (or, simply, class), as in class society, is a set of subjectively defined concepts in the social sciences and political theory centered on models of social stratification in which people are grouped into a set of hierarchical social categories,[1] the most common being the upper, middle, and lower classes.		Class is a subject of analysis for sociologists, political scientists, anthropologists, and social historians. However, there is not a consensus on a definition of "class", and the term has a wide range of sometimes conflicting meanings. In common parlance, the term "social class" is usually synonymous with "socio-economic class", defined as "people having the same social, economic, cultural, political or educational status", e.g., "the working class"; "an emerging professional class".[2] However, academics distinguish social class and socioeconomic status, with the former referring to one’s relatively stable sociocultural background and the latter referring to one’s current social and economic situation and, consequently, being more changeable over time.[3]		The precise measurements of what determines social class in society has varied over time. Karl Marx thought "class" was defined by one's relationship to the means of production (their relations of production). His simple understanding of classes in modern capitalist society, are the proletariat, those who work but do not own the means of production; and the bourgeoisie, those who invest and live off of the surplus generated by the former. This contrasts with the view of the sociologist Max Weber, who argued "class" is determined by economic position, in contrast to "social status" or "Stand" which is determined by social prestige rather than simply just relations of production.[4]		The term "class" is etymologically derived from the Latin classis, which was used by census takers to categorize citizens by wealth, in order to determine military service obligations.[5]		In the late 18th century, the term "class" began to replace classifications such as estates, rank, and orders as the primary means of organizing society into hierarchical divisions. This corresponded to a general decrease in significance ascribed to hereditary characteristics, and increase in the significance of wealth and income as indicators of position in the social hierarchy.[6][7]						Historically social class and behavior was sometimes laid down in law. For example, permitted mode of dress in some times and places was strictly regulated, with sumptuous dressing only for the high ranks of society and aristocracy; sumptuary laws stipulated the dress and jewelry appropriate for a person's social rank and station.		Definitions of social classes reflect a number of sociological perspectives, informed by anthropology, economics, psychology, and sociology. The major perspectives historically have been Marxism and Structural functionalism. The common stratum model of class divides society into a simple hierarchy of working class, middle class and upper class. Within academia, two broad schools of definitions emerge: those aligned with 20th-century sociological stratum models of class society, and those aligned with the 19th-century historical materialist economic models of the Marxists and anarchists.[8][9][10]		Another distinction can be drawn between analytical concepts of social class, such as the Marxist and Weberian traditions, and the more empirical traditions such as socio-economic status approach, which notes the correlation of income, education and wealth with social outcomes without necessarily implying a particular theory of social structure.[11]		For Marx, class is a combination of objective and subjective factors. Objectively, a class shares a common relationship to the means of production. Subjectively, the members will necessarily have some perception ("class consciousness") of their similarity and common interest. Class consciousness is not simply an awareness of one's own class interest but is also a set of shared views regarding how society should be organized legally, culturally, socially and politically. These class relations are reproduced through time.		In Marxist theory, the class structure of the capitalist mode of production is characterized by the conflict between two main classes: the bourgeoisie, the capitalists who own the means of production, and the much larger proletariat (or 'working class') who must sell their own labour power (See also: wage labour). This is the fundamental economic structure of work and property, a state of inequality that is normalized and reproduced through cultural ideology.		Marxists explain the history of "civilized" societies in terms of a war of classes between those who control production and those who produce the goods or services in society. In the Marxist view of capitalism, this is a conflict between capitalists (bourgeoisie) and wage-workers (the proletariat). For Marxists, class antagonism is rooted in the situation that control over social production necessarily entails control over the class which produces goods—in capitalism this is the exploitation of workers by the bourgeoisie.[12]		Furthermore, "in countries where modern civilisation has become fully developed, a new class of petty bourgeois has been formed".[13] "An industrial army of workmen, under the command of a capitalist, requires, like a real army, officers (managers) and sergeants (foremen, over-lookers) who, while the work is being done, command in the name of the capitalist".[14]		Marx makes the argument that, as the bourgeoisie reach a point of wealth accumulation, they hold enough power as the dominant class to shape political institutions and society according to their own interests. Marx then goes on to claim that the non-elite class, owing to their large numbers, have the power to overthrow the elite and create an equal society.[15]		In The Communist Manifesto, Marx himself argued that it was the goal of the proletariat itself to displace the capitalist system with socialism, changing the social relationships underpinning the class system and then developing into a future communist society in which: "..the free development of each is the condition for the free development of all." This would mark the beginning of a classless society in which human needs rather than profit would be motive for production. In a society with democratic control and production for use, there would be no class, no state and no need for financial and banking institutions and money.[16][17]		Max Weber formulated a three-component theory of stratification, that saw social class as emerging from an interplay between "class", "status" and "power". Weber believed that class position was determined by a person's relationship to the means of production, while status or "Stand" emerged from estimations of honor or prestige.[18]		Weber derived many of his key concepts on social stratification by examining the social structure of many countries. He noted that contrary to Marx's theories, stratification was based on more than simply ownership of capital. Weber pointed out that some members of the aristocracy lack economic wealth yet might nevertheless have political power. Likewise in Europe, many wealthy Jewish families in lack prestige and honor, because they were a member of a "pariah group" like the Jews.		On April 2, 2013 the results of a survey[19] conducted by BBC Lab UK developed in collaboration with academic experts and slated to be published in the journal Sociology were published online.[20][21][22][23][24] The results released were based on a survey of 160,000 residents of the United Kingdom most of whom lived in England and described themselves as "white". Class was defined and measured according to the amount and kind of economic, cultural, and social resources reported. Economic capital was defined as income and assets; cultural capital as amount and type of cultural interests and activities, and social capital as the quantity and social status of their friends, family and personal and business contacts.[23] This theoretical framework was developed by Pierre Bourdieu who first published his theory of social distinction in 1979.		Today, concepts of social class often assume three general categories: a very wealthy and powerful upper class that owns and controls the means of production; a middle class of professional workers, small business owners, and low-level managers; and a lower class, who rely on low-paying wage jobs for their livelihood and often experience poverty.		The upper class[25] is the social class composed of those who are rich, well-born, powerful, or a combination of those. They usually wield the greatest political power. In some countries, wealth alone is sufficient to allow entry into the upper class. In others, only people who are born or marry into certain aristocratic bloodlines are considered members of the upper class, and those who gain great wealth through commercial activity are looked down upon by the aristocracy as nouveau riche.[26] In the United Kingdom, for example, the upper classes are the aristocracy and royalty, with wealth playing a less important role in class status. Many aristocratic peerages or titles have 'seats' attached to them, with the holder of the title (e.g. Earl of Bristol) and his family being the custodians of the house, but not the owners. Many of these require high expenditures, so wealth is typically needed. Many aristocratic peerages and their homes are parts of estates, owned and run by the title holder with moneys generated by the land, rents, or other sources of wealth. In America, however, where there is no aristocracy or royalty, the upper class status belongs to the extremely wealthy, the so-called 'super-rich', though there is some tendency even in America for those with old family wealth to look down on those who have earned their money in business, the struggle between New Money and Old Money.		The upper class is generally contained within the richest one or two percent of the population. Members of the upper class are often born into it, and are distinguished by immense wealth which is passed from generation to generation in the form of estates.[27]		The middle class is the most contested of the three categories, the broad group of people in contemporary society who fall socio-economically between the lower and upper classes.[28] One example of the contest of this term is that in the United States "middle class" is applied very broadly and includes people who would elsewhere be considered working class. Middle-class workers are sometimes called "white-collar workers".		Theorists such as Ralf Dahrendorf have noted the tendency toward an enlarged middle class in modern Western societies, particularly in relation to the necessity of an educated work force in technological economies.[29] Perspectives concerning globalization and neocolonialism, such as dependency theory, suggest this is due to the shift of low-level labour to developing nations and the Third World.[30]		Lower class (occasionally described as working class) are those employed in low-paying wage jobs with very little economic security. The term "lower class" also refers to persons with low income.		The working class is sometimes separated into those who are employed but lacking financial security, and an underclass—those who are long-term unemployed and/or homeless, especially those receiving welfare from the state. The latter is analogous to the Marxist term "lumpenproletariat".[25] Members of the working class are sometimes called blue-collar workers.		There are usually four social classes that are described in America: the upper class, the middle class, the working class, and the lower class. The upper class typically earns above $250,000 per year; the middle class earns between $48,000 and $249,000 per year; the working class up to $48,000; and the lower class up generally receives a minimal income that is not enough to sustain themselves. These large income gaps are thought to be one of several root causes of class warfare.[31] While income is a large indicator of class, general wealth and accumulated assets plays a large role in class position because those things have value that can be exchanged for money and thus grant power.		A person's socioeconomic class has wide-ranging effects. It can impact the schools they are able to attend, their health, the jobs open to them, who they may marry, and their treatment by police and the courts.[32][33][34][35][36][37][38][39]		Angus Deaton and Anne Case have analyzed the mortality rates related to the group of white, middle-aged Americans between the ages of 45 and 54 and its relation to class. There has been a growing number of suicides and deaths by substance abuse in this particular group of middle-class Americans. This group also has been recorded to have an increase in reports of chronic pain and poor general health. Deaton and Case came to the conclusion from these observation that because of the constant stress that these white, middle aged Americans feel fighting poverty and wavering between the lower and working class, these strains have taken a toll on these people and affected their whole bodies.[40]		Social classifications can also determine the sporting activities that such classes take part in. It is suggested that those of an upper social class are more likely to take part in sporting activities, whereas those of a lower social background are less likely to participate in sport. However upper class people tend to not take part in certain sports that have been commonly known to be linked with the lower class.[41]		A person's social class has a significant impact on their educational opportunities. Not only are upper-class parents able to send their children to exclusive schools that are perceived to be better, but in many places state-supported schools for children of the upper class are of a much higher quality than those the state provides for children of the lower classes.[42][43][44][45][46][47] This lack of good schools is one factor that perpetuates the class divide across generations.		In 1977, British cultural theorist Paul Willis published a study titled "Learning to Labour", in which he investigated the connection between social class and education. In his study, he found that a group of working-class schoolchildren had developed an antipathy towards the acquisition of knowledge as being outside their class, and therefore undesirable, perpetuating their presence in the working class.[48]		A person's social class has a significant impact on their physical health, their ability to receive adequate medical care and nutrition, and their life expectancy.[49][50][51]		Lower-class people experience a wide array of health problems as a result of their economic status. They are unable to use health care as often, and when they do it is of lower quality, even though they generally tend to experience a much higher rate of health issues. Lower-class families have higher rates of infant mortality, cancer, cardiovascular disease, and disabling physical injuries. Additionally, poor people tend to work in much more hazardous conditions, yet generally have much less (if any) health insurance provided for them, as compared to middle- and upper-class workers.[52]		The conditions at a person's job vary greatly depending on class. Those in the upper-middle class and middle class enjoy greater freedoms in their occupations. They are usually more respected, enjoy more diversity, and are able to exhibit some authority.[53] Those in lower classes tend to feel more alienated and have lower work satisfaction overall. The physical conditions of the workplace differ greatly between classes. While middle-class workers may "suffer alienating conditions" or "lack of job satisfaction", blue-collar workers are more apt to suffer alienating, often routine, work with obvious physical health hazards, injury, and even death.[54]		A recent UK government study has suggested that a 'glass floor' exists in British society which prevents those who are less able, but whom come from wealthier backgrounds, from slipping down the social ladder. This is due to the fact that those from wealthier backgrounds have more opportunities available to them. In fact, the article shows that less able, better-off kids are 35% more likely to become high earners than bright poor kids.[55]		Class conflict, frequently referred to as "class warfare" or "class struggle", is the tension or antagonism which exists in society due to competing socioeconomic interests and desires between people of different classes.		For Marx, the history of class society was a history of class conflict. He pointed to the successful rise of the bourgeoisie, and the necessity of revolutionary violence—a heightened form of class conflict—in securing the bourgeoisie rights that supported the capitalist economy.		Marx believed that the exploitation and poverty inherent in capitalism were a pre-existing form of class conflict. Marx believed that wage labourers would need to revolt to bring about a more equitable distribution of wealth and political power.[56][57]		"Classless society" refers to a society in which no one is born into a social class. Distinctions of wealth, income, education, culture, or social network might arise and would only be determined by individual experience and achievement in such a society.		Since these distinctions are difficult to avoid, advocates of a classless society (such as anarchists and communists) propose various means to achieve and maintain it and attach varying degrees of importance to it as an end in their overall programs/philosophy.		Race and other large-scale groupings can also influence class standing. The association of particular ethnic groups with class statuses is common in many societies. As a result of conquest or internal ethnic differentiation, a ruling class is often ethnically homogenous and particular races or ethnic groups in some societies are legally or customarily restricted to occupying particular class positions. Which ethnicities are considered as belonging to high or low classes varies from society to society.		In modern societies, strict legal links between ethnicity and class have been drawn, such as in apartheid, the caste system in Africa, the position of the Burakumin in Japanese society, and the Casta system in Latin America.[citation needed]		
Refresher / Re-training is the process of learning a new or the same old skill or trade for the same group of personnel. Refresher/Re-training is required to be provided on regular basis to avoid personnel obsolescence due to technological changes & the tendency to forget. This short term instruction course shall serve to re-acquaint personnel with skills previously learnt (recall to retain the potentials) or to bring one's knowledge or skills up-to-date (latest) so that skills stay sharp. This kind of training could be provided annually or more frequently as maybe required, based on the importance of consistency of the task of which the skill is involved. Examples of refresher/re-training are cGMP, GDP, HSE trainings. Re-training (repetition of a training conducted earlier) shall also be conducted for an employee, when the employee is rated as ‘not qualified’ for a skill or knowledge, as determined based on the assessment of answers in the training questionnaire of the employee.						The need to retrain workers is often thought to apply to older members of the workforce, many of whom saw their occupations disappear and their skills lose value as technology, outsourcing and a weak economy combined to erode their ability to make a living. While older Americans do not face as high a rate of unemployment as the country’s teenagers and young adults, when they do find themselves unemployed, they remain unemployed for more than twice as long as teenagers.[1]		While the stereotype for retraining needs is the older worker, youth in the United States and across the European Community (OECD) and Africa suffer from the same problem. The gap between the skills they possess and those that employers are actively seeking is significant and stagnating to their employment prospects. Currently in the United States, psychology, history and the performing arts make up 22% of college degrees earned. Demand for skilled employees, however, is in the areas of technology and engineering, currently at 5% of conferred degrees.[2] “In both Britain and the United States, many people with expensive liberal arts degrees are finding it impossible to get decent jobs,” reports the Economist in its April 27, 2013 issue,[3] adding that in northern Africa, job applicants with degrees face an unemployment level twice that of non-degreed candidates.		While technology anxiety and a nervousness about learning new processes and acquiring new skill sets has impacted older workers,[4] younger job seekers are also facing a deficit of “applied soft skills” such as work ethic, social skills, communication and leadership.[5]		The need for greater partnership and transfer of information between institutions of higher education is essential in reducing the skills gap for old and young people alike.[3] Expanded internships and post-hiring training can help from the employers’ perspective [5] and upgraded and more authentic technical training will help close the gap on the side of educators.[3]		There is some controversy surrounding the use of retraining to offset economic changes caused by free trade and automation. For example, most studies show that displaced factory workers in the United States on the average have lower wages after retraining to other positions when a factory is closed due to offshoring. A similar issue surrounds movement from technical jobs to liaison jobs due to offshore outsourcing. Such changes may also favor certain personality types over others, due to the changing tasks and skills required.[citation needed] Other research estimates that one academic year of such retraining at a community college increases the long-term earnings by about 8 percent for older males and by about 10 percent for older females.[6]		Government policy may make a difference in employability and motivation for retraining and re-entry into the workforce for older workers. In economies with greater regulations surrounding the hiring, termination and wages, reductions in unemployment were difficult to achieve. The very groups harmed with continued higher unemployment were those that the regulations sought to protect.[7]		Retraining is sometimes offered as part of workfare programs, which may include support for transportation, childcare, or an internship.[8]		As difficult and controversial as it may seem, retraining older and younger workers alike to prepare them to be part of a changing workforce will have a lasting impact on workers across the globe. Unemployed workers are at significantly greater risk for poor physical health, greater stress, alcoholism, marital problems and even suicide.[9] Among young workers, beginning their careers with extended bouts of joblessness results in lower overall earnings and more unemployment throughout their careers.[3]		
A professional school is a graduate school level institution that prepares students for careers in specific fields. Some of the schools also offer undergraduate degrees in specific professions. Examples of this type of school include:		
An employment counsellor, also known as a career development professional, advises, coaches, provides information to, and supports people who are planning, seeking and managing their life/work direction.						Career development professionals help clients of all ages:		Working with clients individually or in groups, career development professionals may:		Career development professionals may work in a variety of settings but usually work in offices where they can conduct private interviews with clients and in classrooms or boardrooms where they conduct group sessions. Depending on the organization, their hours of work may include some evening and weekend work.		Career development professionals need the following characteristics:		They should enjoy consulting with people, compiling information and working with clients to develop innovative solutions to problems.		Most career development professionals have post-secondary education in a related discipline such as psychology, education, social work or human resources development. Increasingly, employers are looking for applicants who have a certificate, diploma or degree in career development, or an equivalent combination of education and experience. One of the certifications that is available is the National Certified Counselor (NCC) credential.		
A labor camp (or labour, see spelling differences) or work camp is a simplified detention facility where inmates are forced to engage in penal labor as a form of punishment under the criminal code. Labor camps have many common aspects with slavery and with prisons (especially prison farms). Conditions at labor camps vary widely depending on the operators.		In the 20th century, a new category of labor camps developed for the imprisonment of millions of people who were not criminals per se, but political opponents (real or imagined) and various so-called undesirables under the totalitarian, both communist and fascist regimes. Some of those camps were dubbed "reeducation facilities" for political coercion, but most others served as backbone of industry and agriculture for the benefit of the state especially in times of war. Labor camps of forced labor were abolished by Convention no. 105 of the United Nations International Labour Organization (ILO), adopted internationally on 27 June 1957.[1]		[14][15][16][17][18]		
In economics, a recession is a business cycle contraction which results in a general slowdown in economic activity.[1][2] Macroeconomic indicators such as GDP (gross domestic product), investment spending, capacity utilization, household income, business profits, and inflation fall, while bankruptcies and the unemployment rate rise. In the United Kingdom, it is defined as a negative economic growth for two consecutive quarters.[3][4]		Recessions generally occur when there is a widespread drop in spending (an adverse demand shock). This may be triggered by various events, such as a financial crisis, an external trade shock, an adverse supply shock or the bursting of an economic bubble. Governments usually respond to recessions by adopting expansionary macroeconomic policies, such as increasing money supply, increasing government spending and decreasing taxation.						In a 1979 New York Times article, economic statistician Julius Shiskin suggested several rules of thumb for defining a recession, one of which was two down consecutive quarters of GDP.[5] In time, the other rules of thumb were forgotten. Some economists prefer a definition of a 1.5-2 percentage points rise in unemployment within 12 months.[6]		In the United States, the Business Cycle Dating Committee of the National Bureau of Economic Research (NBER) is generally seen as the authority for dating US recessions. The NBER defines an economic recession as: "a significant decline in economic activity spread across the economy, lasting more than a few months, normally visible in real GDP, real income, employment, industrial production, and wholesale-retail sales."[7] Almost universally, academics, economists, policy makers, and businesses defer to the determination by the NBER for the precise dating of a recession's onset and end.		In the United Kingdom, recessions are generally defined as two consecutive quarters of negative economic growth, as measured by the seasonal adjusted quarter-on-quarter figures for real GDP.[3][4] The exact same recession definition applies for all member states of the European Union. [8]		A recession has many attributes that can occur simultaneously and includes declines in component measures of economic activity (GDP) such as consumption, investment, government spending, and net export activity. These summary measures reflect underlying drivers such as employment levels and skills, household savings rates, corporate investment decisions, interest rates, demographics, and government policies.		Economist Richard C. Koo wrote that under ideal conditions, a country's economy should have the household sector as net savers and the corporate sector as net borrowers, with the government budget nearly balanced and net exports near zero.[9][10] When these relationships become imbalanced, recession can develop within the country or create pressure for recession in another country. Policy responses are often designed to drive the economy back towards this ideal state of balance.		A severe (GDP down by 10%) or prolonged (three or four years) recession is referred to as an economic depression, although some argue that their causes and cures can be different.[6] As an informal shorthand, economists sometimes refer to different recession shapes, such as V-shaped, U-shaped, L-shaped and W-shaped recessions.		The type and shape of recessions are distinctive. In the US, V-shaped, or short-and-sharp contractions followed by rapid and sustained recovery, occurred in 1954 and 1990–91; U-shaped (prolonged slump) in 1974–75, and W-shaped, or double-dip recessions in 1949 and 1980–82. Japan’s 1993–94 recession was U-shaped and its 8-out-of-9 quarters of contraction in 1997–99 can be described as L-shaped. Korea, Hong Kong and South-east Asia experienced U-shaped recessions in 1997–98, although Thailand’s eight consecutive quarters of decline should be termed L-shaped.[11]		Recessions have psychological and confidence aspects. For example, if companies expect economic activity to slow, they may reduce employment levels and save money rather than invest. Such expectations can create a self-reinforcing downward cycle, bringing about or worsening a recession.[12] Consumer confidence is one measure used to evaluate economic sentiment.[13] The term animal spirits has been used to describe the psychological factors underlying economic activity. Economist Robert J. Shiller wrote that the term "...refers also to the sense of trust we have in each other, our sense of fairness in economic dealings, and our sense of the extent of corruption and bad faith. When animal spirits are on ebb, consumers do not want to spend and businesses do not want to make capital expenditures or hire people."[14]		High levels of indebtedness or the bursting of a real estate or financial asset price bubble can cause what is called a "balance sheet recession." This is when large numbers of consumers or corporations pay down debt (i.e., save) rather than spend or invest, which slows the economy. The term balance sheet derives from an accounting identity that holds that assets must always equal the sum of liabilities plus equity. If asset prices fall below the value of the debt incurred to purchase them, then the equity must be negative, meaning the consumer or corporation is insolvent. Economist Paul Krugman wrote in 2014 that "the best working hypothesis seems to be that the financial crisis was only one manifestation of a broader problem of excessive debt--that it was a so-called "balance sheet recession." In Krugman's view, such crises require debt reduction strategies combined with higher government spending to offset declines from the private sector as it pays down its debt.[15]		For example, economist Richard Koo wrote that Japan's "Great Recession" that began in 1990 was a "balance sheet recession." It was triggered by a collapse in land and stock prices, which caused Japanese firms to have negative equity, meaning their assets were worth less than their liabilities. Despite zero interest rates and expansion of the money supply to encourage borrowing, Japanese corporations in aggregate opted to pay down their debts from their own business earnings rather than borrow to invest as firms typically do. Corporate investment, a key demand component of GDP, fell enormously (22% of GDP) between 1990 and its peak decline in 2003. Japanese firms overall became net savers after 1998, as opposed to borrowers. Koo argues that it was massive fiscal stimulus (borrowing and spending by the government) that offset this decline and enabled Japan to maintain its level of GDP. In his view, this avoided a U.S. type Great Depression, in which U.S. GDP fell by 46%. He argued that monetary policy was ineffective because there was limited demand for funds while firms paid down their liabilities. In a balance sheet recession, GDP declines by the amount of debt repayment and un-borrowed individual savings, leaving government stimulus spending as the primary remedy.[9][10][16][17]		Krugman discussed the balance sheet recession concept during 2010, agreeing with Koo's situation assessment and view that sustained deficit spending when faced with a balance sheet recession would be appropriate. However, Krugman argued that monetary policy could also affect savings behavior, as inflation or credible promises of future inflation (generating negative real interest rates) would encourage less savings. In other words, people would tend to spend more rather than save if they believe inflation is on the horizon. In more technical terms, Krugman argues that the private sector savings curve is elastic even during a balance sheet recession (responsive to changes in real interest rates) disagreeing with Koo's view that it is inelastic (non-responsive to changes in real interest rates).[18][19]		A July 2012 survey of balance sheet recession research reported that consumer demand and employment are affected by household leverage levels. Both durable and non-durable goods consumption declined as households moved from low to high leverage with the decline in property values experienced during the subprime mortgage crisis. Further, reduced consumption due to higher household leverage can account for a significant decline in employment levels. Policies that help reduce mortgage debt or household leverage could therefore have stimulative effects.[20][21]		A liquidity trap is a Keynesian theory that a situation can develop in which interest rates reach near zero (zero interest-rate policy) yet do not effectively stimulate the economy. In theory, near-zero interest rates should encourage firms and consumers to borrow and spend. However, if too many individuals or corporations focus on saving or paying down debt rather than spending, lower interest rates have less effect on investment and consumption behavior; the lower interest rates are like "pushing on a string." Economist Paul Krugman described the U.S. 2009 recession and Japan's lost decade as liquidity traps. One remedy to a liquidity trap is expanding the money supply via quantitative easing or other techniques in which money is effectively printed to purchase assets, thereby creating inflationary expectations that cause savers to begin spending again. Government stimulus spending and mercantilist policies to stimulate exports and reduce imports are other techniques to stimulate demand.[22] He estimated in March 2010 that developed countries representing 70% of the world's GDP were caught in a liquidity trap.[23]		Behavior that may be optimal for an individual (e.g., saving more during adverse economic conditions) can be detrimental if too many individuals pursue the same behavior, as ultimately one person's consumption is another person's income. Too many consumers attempting to save (or pay down debt) simultaneously is called the paradox of thrift and can cause or deepen a recession. Economist Hyman Minsky also described a "paradox of deleveraging" as financial institutions that have too much leverage (debt relative to equity) cannot all de-leverage simultaneously without significant declines in the value of their assets.[24]		During April 2009, U.S. Federal Reserve Vice Chair Janet Yellen discussed these paradoxes: "Once this massive credit crunch hit, it didn’t take long before we were in a recession. The recession, in turn, deepened the credit crunch as demand and employment fell, and credit losses of financial institutions surged. Indeed, we have been in the grips of precisely this adverse feedback loop for more than a year. A process of balance sheet deleveraging has spread to nearly every corner of the economy. Consumers are pulling back on purchases, especially on durable goods, to build their savings. Businesses are cancelling planned investments and laying off workers to preserve cash. And, financial institutions are shrinking assets to bolster capital and improve their chances of weathering the current storm. Once again, Minsky understood this dynamic. He spoke of the paradox of deleveraging, in which precautions that may be smart for individuals and firms—and indeed essential to return the economy to a normal state—nevertheless magnify the distress of the economy as a whole."[24]		There are no known completely reliable predictors, but the following are considered possible predictors.[25]		Most mainstream economists believe that recessions are caused by inadequate aggregate demand in the economy, and favor the use of expansionary macroeconomic policy during recessions. Strategies favored for moving an economy out of a recession vary depending on which economic school the policymakers follow. Monetarists would favor the use of expansionary monetary policy, while Keynesian economists may advocate increased government spending to spark economic growth. Supply-side economists may suggest tax cuts to promote business capital investment. When interest rates reach the boundary of an interest rate of zero percent (zero interest-rate policy) conventional monetary policy can no longer be used and government must use other measures to stimulate recovery. Keynesians argue that fiscal policy—tax cuts or increased government spending—works when monetary policy fails. Spending is more effective because of its larger multiplier but tax cuts take effect faster.		For example, Paul Krugman wrote in December 2010 that significant, sustained government spending was necessary because indebted households were paying down debts and unable to carry the U.S. economy as they had previously: "The root of our current troubles lies in the debt American families ran up during the Bush-era housing bubble...highly indebted Americans not only can’t spend the way they used to, they’re having to pay down the debts they ran up in the bubble years. This would be fine if someone else were taking up the slack. But what’s actually happening is that some people are spending much less while nobody is spending more — and this translates into a depressed economy and high unemployment. What the government should be doing in this situation is spending more while the private sector is spending less, supporting employment while those debts are paid down. And this government spending needs to be sustained..."[31]		Some recessions have been anticipated by stock market declines. In Stocks for the Long Run, Siegel mentions that since 1948, ten recessions were preceded by a stock market decline, by a lead time of 0 to 13 months (average 5.7 months), while ten stock market declines of greater than 10% in the Dow Jones Industrial Average were not followed by a recession.[32]		The real-estate market also usually weakens before a recession.[33] However real-estate declines can last much longer than recessions.[34]		Since the business cycle is very hard to predict, Siegel argues that it is not possible to take advantage of economic cycles for timing investments. Even the National Bureau of Economic Research (NBER) takes a few months to determine if a peak or trough has occurred in the US.[35]		During an economic decline, high yield stocks such as fast-moving consumer goods, pharmaceuticals, and tobacco tend to hold up better.[36] However, when the economy starts to recover and the bottom of the market has passed (sometimes identified on charts as a MACD[37]), growth stocks tend to recover faster. There is significant disagreement about how health care and utilities tend to recover.[38] Diversifying one's portfolio into international stocks may provide some safety; however, economies that are closely correlated with that of the U.S. may also be affected by a recession in the U.S.[39]		There is a view termed the halfway rule[40] according to which investors start discounting an economic recovery about halfway through a recession. In the 16 U.S. recessions since 1919, the average length has been 13 months, although the recent recessions have been shorter. Thus if the 2008 recession followed the average, the downturn in the stock market would have bottomed around November 2008. The actual US stock market bottom of the 2008 recession was in March 2009.		Generally an administration gets credit or blame for the state of economy during its time.[41] This has caused disagreements about on actually started.[42] In an economic cycle, a downturn can be considered a consequence of an expansion reaching an unsustainable state, and is corrected by a brief decline. Thus it is not easy to isolate the causes of specific phases of the cycle.		The 1981 recession is thought to have been caused by the tight-money policy adopted by Paul Volcker, chairman of the Federal Reserve Board, before Ronald Reagan took office. Reagan supported that policy. Economist Walter Heller, chairman of the Council of Economic Advisers in the 1960s, said that "I call it a Reagan-Volcker-Carter recession.[43] The resulting taming of inflation did, however, set the stage for a robust growth period during Reagan's administration.		Economists usually teach that to some degree recession is unavoidable, and its causes are not well understood. Consequently, modern government administrations attempt to take steps, also not agreed upon, to soften a recession.		Unemployment is particularly high during a recession. Many economists working within the neoclassical paradigm argue that there is a natural rate of unemployment which, when subtracted from the actual rate of unemployment, can be used to calculate the negative GDP gap during a recession. In other words, unemployment never reaches 0 percent, and thus is not a negative indicator of the health of an economy unless above the "natural rate," in which case it corresponds directly to a loss in gross domestic product, or GDP.[44]		The full impact of a recession on employment may not be felt for several quarters. Research in Britain shows that low-skilled, low-educated workers and the young are most vulnerable to unemployment[45] in a downturn. After recessions in Britain in the 1980s and 1990s, it took five years for unemployment to fall back to its original levels.[46] Many companies often expect employment discrimination claims to rise during a recession.[47]		Productivity tends to fall in the early stages of a recession, then rises again as weaker firms close. The variation in profitability between firms rises sharply. Recessions have also provided opportunities for anti-competitive mergers, with a negative impact on the wider economy: the suspension of competition policy in the United States in the 1930s may have extended the Great Depression.[46]		The living standards of people dependent on wages and salaries are not more affected by recessions than those who rely on fixed incomes or welfare benefits. The loss of a job is known to have a negative impact on the stability of families, and individuals' health and well-being. Fixed income benefits receive small cuts which make it tougher to survive.[46]		According to the International Monetary Fund (IMF), "Global recessions seem to occur over a cycle lasting between eight and 10 years."[48] The IMF takes many factors into account when defining a global recession. Until April 2009, IMF several times communicated to the press, that a global annual real GDP growth of 3.0 percent or less in their view was "...equivalent to a global recession."[49][50] By this measure, six periods since 1970 qualify: 1974–1975,[51] 1980–1983,[51] 1990–1993,[51][52] 1998,[51][52] 2001–2002,[51][52] and 2008–2009.[53] During what IMF in April 2002 termed the past three global recessions of the last three decades, global per capita output growth was zero or negative, and IMF argued—at that time—that because of the opposite being found for 2001, the economic state in this year by itself did not qualify as a global recession.[48]		In April 2009, IMF had changed their Global recession definition to:		By this new definition, a total of four global recessions took place since World War II: 1975, 1982, 1991 and 2009. All of them only lasted one year, although the third would have lasted three years (1991–93) if IMF as criteria had used the normal exchange rate weighted per‑capita real World GDP rather than the purchase power parity weighted per‑capita real World GDP.[54][55]		The worst recession Australia has ever suffered happened in the beginning of the 1930s. As a result of late 1920s profit issues in agriculture and cutbacks, 1931-1932 saw Australia’s biggest recession in its entire history. It fared better than other nations, that underwent depressions, but their poor economic states influenced Australia’s as well, that depended on them for export, as well as foreign investments. The nation also benefited from bigger productivity in manufacturing, facilitated by trade protection, which also helped with feeling the effects less.		Due to a credit squeeze, the economy had gone into a brief recession in 1961 Australia was facing a rising level of inflation in 1973, caused partially by the oil crisis happening in that same year, which brought inflation at a 13% increase. Economic recession hit by the middle of the year 1974, with no change in policy enacted by the government as a measure to counter the economic situation of the country. Consequently, the unemployment level rose and the trade deficit increased significantly.[56]		Another recession – the most recent one to date – came in the 1990s, at the beginning of the decade. It was the result of a major stock collapse in 1987, in October,[57] referred to now as Black Monday. Although the collapse was larger than the one in 1929, the global economy recovered quickly, but North America still suffered a decline in lumbering savings and loans, which led to a crisis. The recession wasn’t limited to only America, but it also affected partnering nations, such as Australia. The unemployment level increased to 10.8%, employment declined by 3.4% and the GDP also decreased as much as 1.7%. Inflation, however, was successfully reduced.		The most recent recession to affect the United Kingdom was the late-2000s recession.		According to economists, since 1854, the U.S. has encountered 32 cycles of expansions and contractions, with an average of 17 months of contraction and 38 months of expansion.[7] However, since 1980 there have been only eight periods of negative economic growth over one fiscal quarter or more,[58] and four periods considered recessions:		For the past three recessions, the NBER decision has approximately conformed with the definition involving two consecutive quarters of decline. While the 2001 recession did not involve two consecutive quarters of decline, it was preceded by two quarters of alternating decline and weak growth.[58]		Official economic data shows that a substantial number of nations were in recession as of early 2009. The US entered a recession at the end of 2007,[61] and 2008 saw many other nations follow suit. The US recession of 2007 ended in June 2009[62] as the nation entered the current economic recovery.		The United States housing market correction (a possible consequence of United States housing bubble) and subprime mortgage crisis significantly contributed to a recession.		The 2007–2009 recession saw private consumption fall for the first time in nearly 20 years. This indicates the depth and severity of the current recession. With consumer confidence so low, recovery takes a long time. Consumers in the U.S. have been hard hit by the current recession, with the value of their houses dropping and their pension savings decimated on the stock market. Not only have consumers watched their wealth being eroded – they are now fearing for their jobs as unemployment rises.[63]		U.S. employers shed 63,000 jobs in February 2008,[64] the most in five years. Former Federal Reserve chairman Alan Greenspan said on 6 April 2008 that "There is more than a 50 percent chance the United States could go into recession."[65] On 1 October, the Bureau of Economic Analysis reported that an additional 156,000 jobs had been lost in September. On 29 April 2008, Moody's declared that nine US states were in a recession. In November 2008, employers eliminated 533,000 jobs, the largest single month loss in 34 years.[66] For 2008, an estimated 2.6 million U.S. jobs were eliminated.[67]		The unemployment rate in the US grew to 8.5 percent in March 2009, and there were 5.1 million job losses until March 2009 since the recession began in December 2007.[68] That was about five million more people unemployed compared to just a year prior,[69] which was the largest annual jump in the number of unemployed persons since the 1940s.[70]		Although the US Economy grew in the first quarter by 1%,[71][72] by June 2008 some analysts stated that due to a protracted credit crisis and "...rampant inflation in commodities such as oil, food, and steel," the country was nonetheless in a recession.[73] The third quarter of 2008 brought on a GDP retraction of 0.5%[74] the biggest decline since 2001. The 6.4% decline in spending during Q3 on non-durable goods, like clothing and food, was the largest since 1950.[75]		A 17 November 2008 report from the Federal Reserve Bank of Philadelphia based on the survey of 51 forecasters, suggested that the recession started in April 2008 and would last 14 months.[76] They project real GDP declining at an annual rate of 2.9% in the fourth quarter and 1.1% in the first quarter of 2009. These forecasts represent significant downward revisions from the forecasts of three months ago.		A 1 December 2008, report from the National Bureau of Economic Research stated that the U.S. has been in a recession since December 2007 (when economic activity peaked), based on a number of measures including job losses, declines in personal income, and declines in real GDP.[77] By July 2009 a growing number of economists believed that the recession may have ended.[78][79] The National Bureau of Economic Research announced on 20 September 2010 that the 2008/2009 recession ended in June 2009, making it the longest recession since World War II.[80]		Many other countries, particularly in Europe, have undergone decreasing rates of GDP growth. Some countries have been able to avoid a recession but have still experienced slower economic activity, such as China. India and Australia were able to maintain positive growth throughout the late-2000s recession.		China had their stock market crash, which began with the popping of the stock market bubble on 12 July 2015.		Canada officially declared a recession in 2015 after two quarters of shrinking GDP.		
A salary is a form of periodic payment from an employer to an employee, which may be specified in an employment contract. It is contrasted with piece wages, where each job, hour or other unit is paid separately, rather than on a periodic basis. From the point of view of running a business, salary can also be viewed as the cost of acquiring and retaining human resources for running operations, and is then termed personnel expense or salary expense. In accounting, salaries are recorded in payroll accounts.[1]		Salary is a fixed amount of money or compensation paid to an employee by an employer in return for work performed. Salary is commonly paid in fixed intervals, for example, monthly payments of one-twelfth of the annual salary.		Salary is typically determined by comparing market pay rates for people performing similar work in similar industries in the same region. Salary is also determined by leveling the pay rates and salary ranges established by an individual employer. Salary is also affected by the number of people available to perform the specific job in the employer's employment locale.[2]						While there is no first pay stub for the first work-for-pay exchange, the first salaried work would have required a society advanced enough to have a barter system which allowed for the even exchange of goods or services between tradesmen. More significantly, it presupposes the existence of organized employers—perhaps a government or a religious body—that would facilitate work-for-hire exchanges on a regular enough basis to constitute salaried work. From this, most infer that the first salary would have been paid in a village or city during the Neolithic Revolution, sometime between 10,000 BCE and 6000 BCE.[1]		A cuneiform inscribed clay tablet dated about 3100 BCE provides a record of the daily beer rations for workers in Mesopotamia. The beer is represented by an upright jar with a pointed base. The symbol for rations is a human head eating from a bowl. Round and semicircular impressions represent the measurements.[3]		By the time of the Hebrew Book of Ezra (550 to 450 BCE), salt from a person was synonymous with drawing sustenance, taking pay, or being in that person's service. At that time, salt production was strictly controlled by the monarchy or ruling elite. Depending on the translation of Ezra 4:14, the servants of King Artaxerxes I of Persia explain their loyalty variously as "because we are salted with the salt of the palace" or "because we have maintenance from the king" or "because we are responsible to the king".[1]		The Latin word salarium linked employment and salt, but the exact link is not very clear. Modern sources maintain that the word salarium is derived from the word sal (salt) because at some point a soldier's salary may have been an allowance for the purchase of salt[4] or the price of having soldiers conquer salt supplies and guard the Salt Roads (Via Salaria) that led to Rome.[5][6] But there is no evidence for this assertion at all.[7] Some people even claim that the word soldier itself comes from the Latin sal dare (to give salt),[8] but mainstream sources disagree, noting that the word soldier more likely derives from the gold solidus introduced by Diocletian in 301 CE.[9]		Regardless of the exact connection, the salarium paid to Roman soldiers has defined a form of work-for-hire ever since in the Western world, and gave rise to such expressions as "being worth one's salt".[1]		Within the Roman Empire or (later) medieval and pre-industrial Europe and its mercantile colonies, salaried employment appears to have been relatively rare and mostly limited to servants and higher status roles, especially in government service. Such roles were largely remunerated by the provision of lodging, food, and livery clothes (i.e., "food, clothing, and shelter" in modern idiom). Many courtiers, such as valets de chambre, in late medieval courts were paid annual amounts, sometimes supplemented by large if unpredictable extra payments. At the other end of the social scale, those in many forms of employment either received no pay, as with slavery (although many slaves were paid some money at least), serfdom, and indentured servitude, or received only a fraction of what was produced, as with sharecropping. Other common alternative models of work included self- or co-operative employment, as with masters in artisan guilds, who often had salaried assistants, or corporate work and ownership, as with medieval universities and monasteries.[1]		Even many of the jobs initially created by the Commercial Revolution in the years from 1520 to 1650 and later during Industrialisation in the 18th and 19th centuries would not have been salaried, but, to the extent they were paid as employees, probably paid an hourly or daily wage or paid per unit produced (also called piece work).[1]		In corporations of this time, such as the several East India Companies, many managers would have been remunerated as owner-shareholders. Such a remuneration scheme is still common today in accounting, investment, and law firm partnerships where the leading professionals are equity partners, and do not technically receive a salary, but rather make a periodic "draw" against their share of annual earnings.[1]		From 1870 to 1930, the Second Industrial Revolution gave rise to the modern business corporation powered by railroads, electricity and the telegraph and telephone. This era saw the widespread emergence of a class of salaried executives and administrators who served the new, large-scale enterprises being created.		New managerial jobs lent themselves to salaried employment, in part because the effort and output of "office work" were hard to measure hourly or piecewise, and in part because they did not necessarily draw remuneration from share ownership.[1]		As Japan rapidly industrialized in the 20th century, the idea of office work was novel enough that a new Japanese word (salaryman) was coined to describe those who performed it, as well as referencing their remuneration.[1]		In the 20th century, the rise of the service economy made salaried employment even more common in developed countries, where the relative share of industrial production jobs declined, and the share of executive, administrative, computer, marketing, and creative jobs—all of which tended to be salaried—increased.[1]		Today, the concept of a salary continues to evolve as part of a system of the total compensation that employers offer to employees. Salary (also now known as fixed pay) is coming to be seen as part of a "total rewards" system which includes bonuses, incentive pay, commissions, benefits and perquisites (or perks), and various other tools which help employers link rewards to an employee's measured performance.[1]		Compensation has evolved considerably. Consider the change from the days of and before the industrial evolution, when a job was held for a lifetime, to the fact that, from 1978 to 2008, individuals who aged from 18 to 44, held an average number of 11 jobs.[10] Compensation has evolved gradually moving away from fixed short-term immediate compensation towards fixed + variable outcomes-based compensation.[citation needed] An increase in knowledge-based work has also led to pursuit of partner (as opposed to employee) like engagement.		In Botswana, salaries are almost entirely paid on a monthly basis with pay dates falling on different dates of the second half of the month. Pay day usually ranges from the 15th of the month to the last day. The date of disbursement of the salary is usually determined by the company and in some cases in conjunction with the recognized Workers Union.		The Botswana Employment Act Cap 47:01 Chapter VII regulates the aspect of protection of wages in the contracts of employment. The minimum and maximum wage payment period with the exception of casual employees should not be less than one week or more than a month, and where not expressly stipulated a month is the default wage period per section 75 of the Act payable before the third working day after the wage period. The wages are to be paid during working hours at the place of employment, or in any other way, such as through a bank account with the consent of the employee. Salaries should be made in legal tender, however, part payment in kind is not prohibited provided it is appropriate for the personal use and benefit of employee and his family, and the value attributable to such payment in kind is fair and reasonable. The payment in kind should not exceed forty per cent of the total amount paid out to the employee.		The minimum wage is set, adjusted and can even be abolished by the Minister on the advice of the Minimum Wages Advisory Board for specified trade categories. The stipulated categories include building, construction, hotel, catering, wholesale, watchmen, the domestic service sector, the agricultural sector etc. The current minimum wages set for these sectors are set out in the Subsidiary legislation in the Act.		Women on maternity leave are entitled to 25% of their salaries as stipulated by the Employment Act but the majority of the companies pay out at about 50% for the period.[11]		By working for the Danish Government, it has been agreed under political agreements, that the salary is dependent on the seniority, education, and of a qualification allowance.		According to European law, the movement of capital, services and (human) resources is unlimited between member states. Salary determination, such as minimum wage, is still the prerogative of each member state. Other social benefits, associated with salaries are also determined on member-state level.[12]		In India, salaries are generally paid on the last working day of the month (Government, Public sector departments, Multi-national organisations as well as majority of other private sector companies). According to the Payment of Wages Act, if a company has less than 1,000 Employees, salary is paid by the 7th of every month. If a company has more than 1,000 Employees, salary is paid by the 10th of every month.[13]		Minimum wages in India are governed by the Minimum Wages Act, 1948.[14] Employees in India are notified of their salary being increased through a hard copy letter given to them.[15]		In Italy, the Constitution guarantees a minimum wage, as stated in Article 36, Paragraph 1[16]		This constitutional guarantee is implemented not through a specific legislation, but rather through collective bargaining which sets minimum wage standards in a sector by sector basis. Collective bargaining is protected by trade unions, which have constitutional rights such as legal personality. The Constitution also guarantees equal pay for women, as stated in Article 37, Paragraph 1[16]		In Japan, owners would notify employees of salary increases through "jirei". The concept still exists and has been replaced with an electronic form, or E-mail in larger companies.[17] The position and world of "salarymen" is open to only one third of Japanese men. From school age these young potentials are groomed and pre-selected to one day join a company as a "salaryman". The selection process is rigorous and thereafter the process initiation speaks of total dedication to the company.[18]		Minimum wages are used widely in developing countries to protect vulnerable workers, reduce wage inequality, and lift the working poor out of poverty. The political popularity of minimum wages stems in part from the fact that the policy offers a means for redistributing income without having to increase government spending or establish formal transfer mechanisms.[19] The challenge to policymakers is to find that wage level that is considered fair given workers' needs and the cost of living, but does not harm employment or a country's global competitiveness.[20]		South African median employee earning is R2800 a month (USD 190.35) and the average earning is around R8500. These figures are found in SA statistics. Indeed, they reflect the huge gap in the South African society with a large proportion of the population under poverty line that does not have the same opportunities for employment.[21]		Median monthly earnings of white (R9500) and Indian/Asian (R6000) population were substantially higher than the median monthly earnings of their coloured (R2652) and black African (R2167) counterparts. Black Africans earned 22,% of what the white population earned; 36,1% of what Indians/Asians earned; and 81,7% of what the coloured population earned. In the bottom 5%, black Africans earned R500 or less per month while the white population earned R2 000 or less, while in the top 5% they earned R12 567 or more compared to the white population who earned R34000 or more per month.[22]		In the Netherlands the salary which occurs most frequently is referred to as Jan Modaal. The term "modaal" is derived from the statistical term Modus. If the government's macro economic policy negatively affects this "Modaal" income or salary-group often the policy is adjusted in order to protect this group of income earners.[23] The Dutch word "soldij" can be directly linked to the word "soldaat" or soldier, which finds its origin in the word for the gold coin solidus, with which soldiers were paid during the Roman Empire.		The Netherlands is in the top 5 of the highest salary-paying countries in the EU. The focus has been on the salary levels and accompanying bonuses whereas secondary benefits, though present, has been downplayed yet that is changing. The Netherlands claims a 36th position when it comes to secondary benefits when compared to other countries in Europe.[24]		The minimum wage is determined through collective labor negotiations (CAOs). The minimum wage is age dependent; the legal minimum wage for a 16-year-old is lower than, for instance, a 23-year-old (full minimum wage). Adjustments to the minimum wage are made twice a year; on January 1 and on July 1. The minimum wage for a 21-year-old on January 1, 2013 is 1,065.30 Euro netto per month and on July 1, 2013 this minimum wage is 1,071.40 Euro netto per month.[25] For a 23 year old on 1 January 2014 is 1485,60 Euro gross salary / month plus 8% holiday subsidy so 1604,45 Euro gross salary / month		In the United States, the distinction between periodic salaries (which are normally paid regardless of hours worked) and hourly wages (meeting a minimum wage test and providing for overtime) was first codified by the Fair Labor Standards Act of 1938. At that time, five categories were identified as being "exempt" from minimum wage and overtime protections, and therefore salariable. In 1991, some computer workers were added as a sixth category but effective August 23, 2004 the categories were revised and reduced back down to five (executive, administrative, professional, computer, and outside sales employees).		In June 2015 the Department of Labor proposed raising "the salary threshold from $455 a week (the equivalent of $23,660 a year) to about $970 a week ($50,440 a year) in 2016"[26] On May 18, 2016 the Final rule updating the overtime regulations was announced. Effective December 1, 2016 it says:		"The Final Rule sets the standard salary level at the 40th percentile of weekly earnings of full-time salaried workers in the lowest-wage Census Region, currently the South ($913 per week, equivalent to $47,476 per year for a full-year worker)."[27]		"The Final Rule sets the HCE total annual compensation level equal to the 90th percentile of earnings of full-time salaried workers nationally ($134,004 annually). To be exempt as an HCE, an employee must also receive at least the new standard salary amount of $913 per week on a salary or fee basis and pass a minimal duties test."[27]		"Although the FLSA ensures minimum wage and overtime pay protections for most employees covered by the Act, some workers, including bona fide EAP employees, are exempt from those protections. Since 1940, the Department’s regulations have generally required each of three tests to be met for the FLSA’s EAP exemption to apply:		"The Final Rule includes a mechanism to automatically update the standard salary level requirement every three years to ensure that it remains a meaningful test for distinguishing between overtime-protected white collar workers and bona fide EAP workers who may not be entitled to overtime pay and to provide predictability and more graduated salary changes for employers. Specifically, the standard salary level will be updated to maintain a threshold equal to the 40th percentile of weekly earnings of full-time salaried workers in the lowest-wage Census Region." [27]		"For the first time, employers will be able to use nondiscretionary bonuses and incentive payments (including commissions) to satisfy up to 10 percent of the standard salary level. Such payments may include, for example, nondiscretionary incentive bonuses tied to productivity and profitability."[27]		A general rule for comparing periodic salaries to hourly wages is based on a standard 40-hour work week with 50 weeks per year (minus two weeks for vacation). (Example: $40,000/year periodic salary divided by 50 weeks equals $800/week. Divide $800/week by 40 standard hours equals $20/hour).		Zimbabwe operates on a two tier system being wages and salaries. Wages are managed by the National Employment Council (NEC). Each sector has its own NEC; i.e. agriculture, communications, mining, catering, educational institutions, etc. On the council are representatives from the unions and the employers. The public sector is under the Public Service Commission and wages and salaries are negotiated there.		Wages are negotiated annually or biennially for minimum wages, basic working conditions and remunerations. If there is a stalemate it goes for arbitration with the Ministry of labour. The ruling will become binding on all companies in that industry. Industries often then use their associations to negotiate and air their views. For example, the mining industry nominates an employee within the chamber of mines to attend all meetings and subcommittee with industry players is a forum for discussions.		Salaries are negotiated by the respective employees. However, NEC obviously affects the relativity and almost acts as a barometer for salaried staff. Salaries and wages in Zimbabwe are normally paid monthly. Most companies' pay around the 20th does allow various statutory payments and processing for the month end. Government employees are also staggered to ease the cash flow though teachers are paid around mid-month being 16th. Agricultural workers are normally paid on the very last day of the month as they are contract employees.		Zimbabwe is a highly banked society with most salaries being banked. All government employees are paid through the bank. Since "dollarisation" (movement from the Zimbabwean dollar to USD) Zimbabwe has been moving toward a more informal sector and these are paid in 'brown envelopes'.		PAYE (Pay As You Earn) is a significant contributor to tax being 45%.[28] Given the high unemployment rate the tax is quite heavy. This of course captures those that pay and keep records properly. The average salary is probably $250. This is skewed downwards by the large number of government employees whose average salary is around there. At the top end salaries are quite competitive and this is to be able to attract the right skills though the cost of living is high so it balances this out. A top-earning Zimbabwean spends a lot more money on necessities than say a South African top earner. This is more evident when a comparison with the United States or England is done. The need to have a generator, borehole or buy water or take care of the extended family since there is no welfare given the government's financial position.		In the hyperinflation days salaries was the cheapest factor of production given that it was paid so irregularly though it went to twice monthly. As workers could not withdraw their money, remuneration was often in the following forms:		• Fuel coupons were most popular and individuals were paid in liters of fuel • The product that the company is selling; e.g. pork/meat for the abattoirs • Foreign currency payment was illegal and one had to seek special dispensation or had to show that their revenue/funding was received in foreign currency like NGOs or exporters • Shares for the listed companies on the stock market (not in the traditional option scheme but just getting shares)		Prices were price controlled. By remunerating in the product it basically allowed the employees to side sell for real value.		Zimbabwe traditionally had a competitive advantage in its cost of labor. With "dollarisation" and higher cost of living this is slowly being eroded. For example, an average farm employee probably earned the equivalent of $20 but could buy a basket of goods currently worth $500. Now, the average farm worker earns $80 and that basket of goods is, as mentioned, $500, the basket being soap, meal, school fees, protein foods, etc.		Prior to the acceptance of an employment offer, the prospective employee usually has the opportunity to negotiate the terms of the offer. This primarily focuses on salary, but extends to benefits, work arrangements, and other amenities as well. Negotiating salary can potentially lead the prospective employee to a higher salary. In fact, a 2009 study of employees indicated that those who negotiated salary saw an average increase of $4,913 from their original salary offer.[29] In addition, the employer is able to feel more confident that they have hired an employee with strong interpersonal skills and the ability to deal with conflict. Negotiating salary will thus likely yield an overall positive outcome for both sides of the bargaining table.		Perhaps the most important aspect of salary negotiation is the level of preparation put in by the prospective employee. Background research on comparable salaries will help the prospective employee understand the appropriate range for that position. Assessment of alternative offers that the prospective employee has already received can help in the negotiation process. Research on the actual company itself will help identify where concessions can be made by the company and what may potentially be considered off-limits. These items, and more, can be organized into a negotiations planning document that can be used in the evaluation of the offers received from the employer.		The same 2009 study highlighted the personality differences and negotiation mind-sets that contributed to successful outcomes. Overall, individuals who are risk-averse (e.g., worried about appearing ungrateful for the job offer) tended to avoid salary negotiations or use very weak approaches to the negotiation process. On the contrary, those who were more risk-tolerant engaged in negotiations more frequently and demonstrated superior outcomes. Individuals who approached the negotiation as a distributive problem (i.e. viewing a higher salary as a win for him/her and a loss to the employer) ended up with an increased salary, but lower rate of satisfaction upon completion. Those who approached the negotiation as an integrative problem (i.e. viewing the negotiation process an opportunity to expand the realm of possibilities and help both parties achieve a “win” outcome) were able to both secure an increased salary and an outcome they were truly satisfied with.[29]		Salary disparities between men and women may partially be explained by differences in negotiation tactics used by men and women. Although men and women are equally likely to initiate in a salary negotiation with employers, men will achieve higher outcomes than women by about 2% of starting salary[30] Studies have indicated that men tend to use active negotiation tactics of directly asking for a higher salary, while women tend to use more of an indirect approach by emphasizing self-promotion tactics (e.g. explaining the motivation to be a good employee).[31] Other research indicates that early-childhood play patterns may influence the way men and women negotiate. Men and women tend to view salary differently in terms of relative importance. Overall level of confidence in a negotiation may also be a determinant of why men tend to achieve higher outcomes in salary negotiations.[32] Finally, the awareness of this stereotype alone may directly cause women to achieve lower outcomes as one study indicates.[33] Regardless of the cause, the outcome yields a disparity between men and women that contributes to the overall wage gap observed in many nations.		The Constitution of the Republic of South Africa 239 provides for the right to fair labour practices in terms of article 23. article 9 of the Constitution makes provision for equality in the Bill of Rights, which an employee may raise in the event of an equal pay dispute. In terms of article 9(1) “everyone is equal before the law and has the right to equal protection and benefit of the law'” Furthermore, “the state may not unfairly discriminate directly or indirectly against anyone on one or more grounds, including race, gender, sex, pregnancy, marital status, ethnic or social origin, colour, sexual orientation, age, disability, religion, conscience, belief, culture, language, and birth.”[34] South African employees who were in paid employment had median monthly earnings of R2 800. The median monthly earnings for men (R3 033) were higher than that for women (R2 340) - women in paid employment earned 77,1% of what men did.[22]		Research done in 2011 showed that the “weight double standard” may be more complex that what past research has suggested. This is not only relevant to women, but also to men. The smallest income gap differences occur at thin weights (where men are penalized and women are rewarded) and the opposite happens at heavier weights, where the women are affected more negatively.[35]		
In the United States, a pink-collar worker performs jobs in the service industry. In contrast, blue-collar workers are working-class people who perform skilled or unskilled manual labor, and white-collar workers typically perform professional, managerial, or administrative work in an office environment.		Companies may sometimes blend blue, white, and pink industry categorizations.						The term "pink-collar" was popularized in the late 1970s by writer and social critic Louise Kapp Howe to denote women working as nurses, secretaries, and elementary school teachers.[1] Its origins, however, go back to the early 1970s, to when the equal rights amendment, ERA, was placed before the states for ratification (March 1972). At that time, the term was used to denote secretarial and steno-pool staff as well as non-professional office staff, all of which were largely held by women. De rigueur, these positions were not white-collar jobs, but neither were they blue-collar manual labor. Hence, the creation of the term "pink collar," which indicated it was not white-collar but was nonetheless an office job, one that was overwhelmingly filled by women.		Pink-collar occupations tend to be personal-service-oriented worker working in retail, nursing, and teaching (depending on the level), are part of the service sector, and are among the most common occupations in the United States. The Bureau of Labor Statistics estimates that, as of May 2008, there were over 2.2 million persons employed as servers in the United States.[2] Furthermore, the World Health Organization's 2011 World Health Statistics Report states that there are 19.3 million nurses in the world today.[3] In the United States, women comprise 92.1% of the registered nurses that are currently employed.[4]		Pink-collar occupations include:[5][6]		Historically, women were responsible for the running of a household.[7] Their financial security was often dependent upon a male patriarch. Widowed or divorced women struggled to support themselves and their children.[8]		Women began to develop more opportunities when they moved into the paid workplace, formerly of the male domain. In the 20th century women aimed to be treated like the equals of their male counterparts. In 1920 American women won the right to vote, marking a turning point in their roles in life.[9]		Many single women traveled to cities like New York where they found work in factories and sweatshops, working for low pay operating sewing machines, sorting feathers, rolling tobacco and so on.[10]		These factories were dirty, noisy, dark and dangerous. Workers frequently breathed dangerous fumes and worked with flammable materials.[11] Women lost fingers and hands in accidents because in order to save money they were required to clean and adjust the machines while they were running.[11] Unfortunately, most women who worked in the factories did not earn enough money to live on and lived in poverty.		Throughout the 20th century certain women helped change women's roles in America. Emily Balch, Jane Addams, and Lillian Wald are among the most notable.[12] They created settlement houses and launched missions in crowded, unsanitary neighborhoods where immigrants lived.[12] Balch, Addams, and Wald offered social services to the women and children, often inviting them into their homes and classrooms.[12]		Women took on leadership roles starting in the church. Women became involved with the church activities, a few went on to become president of the societies. The women who joined these societies worked with their members some of whom were full-time teachers, nurses, missionaries, and social workers to accomplish their leadership tasks and make a difference.[13] The Association for the Sociology of Religion was the first to elect a woman president in 1938.[13]		World War I was the beginning of "pink-collar jobs" as the military needed personnel to type letters, answer phones, and perform other tasks. One thousand women worked for the U.S. Navy as stenographers, clerks, and telephone operators.[14]		The field of nursing also became "feminized" and was an accepted profession for women. In 1917, Louisa Lee Schuyler opened the Bellevue Hospital School of Nursing, the first to train women as professional nurses.[15] After completing training, some female nurses worked in hospitals, but most worked in field tents.		World War II marked the first time women began working in high-paying industrial jobs. They worked in factories and some even joined the armed forces. These women were segregated from men in separate groups. Although women joined the workforce they still encountered discrimination in and out of the workplace, which persisted despite anti-discrimination laws passed in the 1960s.[16]		Women who joined the armed forces participated in every military field except combat. One thousand female pilots joined the Women Airforce Service Pilots, one hundred and forty thousand women joined the Women's Army Corps and one hundred thousand women joined the U.S. Navy as nurses and administrative staff.[17]		Two million women held office jobs during World War II, which offered job security because they had become "feminized".[18]		A typical job sought by working women was that of a telephone operator or Hello Girl. The workers would sit on stools facing a wall with hundreds of outlets and tiny blinking lights; they had to work quickly when a light flashed plugging the cord into the proper outlet. Despite the hassles many women wanted this job because it paid five dollars a week and provided a rest lounge for the employees to take a break.[19]		Female secretaries were also popular; they were instructed to be efficient, tough and hardworking but to appear soft, accommodating and subservient.[20] They were instructed to be the protector and partner to their boss behind closed doors and a helpmate in public. These women were encouraged to go to charm schools and express their personality through fashion instead of furthering their education.[20]		Social work became a female-dominated profession in the 1930s, emphasizing a group professional identity and the casework method.[21] Social workers gave crucial expertise for the expansion of federal, state and local government, as well as services to meet the needs of the Depression.[21]		Teachers in primary and secondary schools remained female, although as the war progressed women began to move on to better employment and higher salaries.[22] In 1940 teaching positions paid less than $1,500 a year and fell to $800 in rural areas.[22]		Women scientists found it hard to gain appointments at universities; they were forced to take positions in high schools, state or women's colleges, governmental agencies and alternative institutions such as libraries or museums.[23] Women who took jobs at such places often did clerical duties and though some held professional positions, these boundaries were blurred.[23]		Women were hired as librarians who had been professionalized and feminized, in 1920 women accounted for 88% of librarians in the United States.[23]		Two-thirds of the American Geographical Society (AGS)'s employees were women, who served as librarians, editorial personal in the publishing programs, secretaries, research editors, copy editors, proofreaders, research assistants and sales staff. These women came with credentials from well-known colleges and universities and many were overqualified for their positions, but later were promoted to more prestigious positions.		Although female employees did not receive equal pay, they did get sabbaticals to attend university and to travel for their professions all at the cost of the AGS.[23] Male co-workers portrayed their women counterparts as dedicated and self-effacing.[23] Those women working managerial and library or museums positions made an impact on women in the work force, but still encountered discrimination when they tried to advance.		In the 1940s clerical work expanded to occupy the largest number of women employees, this field diversified as it moved into commercial service.[24] The average worker in the 1940s was over 35 years old, married, and needed to work to keep their families afloat.[25]		During the 1950s women were taught that marriage and domesticity were more important than a career. Most women followed this path because of the uncertainty of the post war years.[26] The suburban housewife was encouraged to have hobbies like bread making and sewing. The 1950s housewife was in conflict between being "just a housewife" because their upbringing taught them competition and achievement, many had furthered their education deriving a sense of self-worth.[27]		A single woman working in a factory in the early 20th century earned less than $8 a week, and if the woman was absent from work or late, their employer penalized them by subtracting a few cents or sometimes paying them nothing.[19] These women would live in boarding houses costing $1.50 a week, waking at 5:30 a.m. to start their ten-hour work day.		When women entered the paid workforce in the 1920s they were paid less than men because employers thought the women's jobs were temporary. Employers also paid women less than men because they believed in the "Pin Money Theory", which said that women's earnings were secondary to that of their male counterparts.		Women took typical jobs that were "considerably less substantial than their husbands' in terms of both the average number of hours worked per week as well as continuity over time".[28] However, working women still experienced stress and overload because they were still responsible for the majority of the housework and taking care of the children. This left women isolated and subjected them to their husband's control.[28]		In the early 1900s women's pay was one to three dollars a week and much of that went to living expenses.[29] In the 1900s female tobacco strippers earned five dollars a week, half of what their male coworkers made and seamstresses made six to seven dollars a week compared to a cutter's salary of $16.[30]		The early 1900s women working in factories were paid by the piece, not receiving a fixed weekly wage.[31] Those that were pinching pennies pushed themselves to produce more product so that they earned more money.[31]		The women who earned enough to live on found it impossible to keep their salary rate from being reduced because bosses often made "mistakes" in computing a worker's piece rate.[32] Women who received this kind of treatment did not disagree for fear of losing their jobs. Employers would frequently deduct pay for work they deemed imperfect and for simply trying to lighten the mood by laughing or talking while they worked.[32]		In 1937 a woman's average yearly salary was $525 compared to a man's salary of $1,027.[30] Women in 1991 only earned seventy percent of what men earned regardless of their education.[16]		In the 1940s two-thirds of the women who were in the labor force suffered a decrease in earnings; the average weekly paychecks fell from $50 to $37.[16]		During the 1970s and 1980s women began to fight for equality, they fought against discrimination in jobs where women worked and the educational institutions that would lead to those jobs.[16]		In 1973 the average salaries for women were 57% compared to those of men, but this gender earnings gap was especially noticeable in pink-collar jobs where the largest number of women were employed.[33] Women were given routine, less responsible jobs available and often with a lower pay than men. These jobs were monotonous and mechanical often with assembly-line procedures.[34]		Women entering the workforce had difficulty finding a satisfactory job without references or an education.[35] However, opportunities for higher education expanded as women were admitted to all-male schools like the United States service academies and Ivy League strongholds.[36] Education became a way for society to shape women into its ideal housewife, in the 1950s authorities and educators encouraged college because they found new value in vocational training for domesticity.[37] College prepared women for future roles, while men and women were taught together they were groomed for different paths after they graduated.[38] Education started out as a way to teach women how to be a good wife, but it also allowed them to broaden their minds because of it women earned better jobs and salaries.		Being educated was an expectation for women entering the paying workforce even though male equivalents did not need a high school diploma.[39] While in college a woman would experience extracurricular activities like a sorority that offered a separate space for the woman to practice types of social service work that was expected from her.[40]		Not all of a woman's education was done in the classroom, but rather among their peers through "dating". No longer did men and women have to be supervised when alone together. Dating allowed men and women to practice the paired activities that would later become a way of life.[40]		New women's organizations sprouted up working to reform and protect women in the workplace. The General Federation of Women's Clubs (GFWC) was the largest and most prestigious organization; the members were conservative middle-class housewives.[41] The International Ladies Garment Workers Union (ILGWU) was formed after women shirtwaist makers went on strike in New York City in 1909.[42] It started as a small walkout, with a handful of members from one shop and grew to a force of ten of thousands, changing the course of the labor movement forever.[42] In 1910 women allied themselves with the Progressive Party who sought to reform social issues.		Another organization that grew out of women in the workforce was the Women's Bureau of the Department of Labor. The Women's Bureau regulated conditions for women employees. As female labor became a crucial part of the economy, efforts by the Women's Bureau increased. The Bureau pushed for employers to take advantage of "women-power" and persuaded women to enter the employment market.[43]		In 1913 the ILGWU signed the well-known "protocol in the Dress and Waist Industry" which was the first contract between labor and management settled by outside negotiators.[44] The contract formalized the trade's division of labor by gender.[44]		Another win for women came in 1921 when congress passed the Sheppard-Towner Act, a welfare measure intended to reduce infant and maternal mortality; it was the first federally funded healthcare act.[45] The act provided federal funds to establish health centers for prenatal and child care.[45] Expectant mothers and children could receive health checkups and health advice.[45]		In 1963 the Equal Pay Act was passed making it the first federal law against sex discrimination, equal pay for equal work, and made employers hire women workers if they qualified from the start.[46]		Unions also became a major outlet for women to fight against the unfair treatment they experienced. Women who joined these types of unions stayed before and after work to talk about the benefits of the union, collect dues, obtain charters, and form bargaining committees.[42]		The National Recovery Administration (NRA) was approved in May 1933. The NRA negotiated codes designed to rekindle production.[47] It raised wages, shortened workers' hours, and increased employment for the first time maximizing hour and minimizing wage provisions benefiting female workers.[47] The NRA had its flaws however, it only covered half of the women in the workforce particularly manufacturing and trade.[47] The NRA regulated working conditions only for women with a job and did not offer any relief for the two million unemployed women who desperately needed it.[48]		The 1930s proved successful for women in the workplace thanks to federal relief programs and the growth of unions. For the first time women were not completely dependent on themselves, in 1933 the federal government expanded in its responsibility to female workers. In 1938 the Fair Labor Standards Act grew out of several successful strikes. Two million women joined the workforce during the Great Depression despite negative public opinion.[49]		Pink ghetto is a term used to refer to jobs dominated by women. The term was coined in 1983 to describe the limits women have in furthering their careers, since the jobs are often dead-end, stressful and underpaid.		Pink ghetto can also describe the placement of female managers into positions that will not lead them to the board room, thus perpetuating the "glass ceiling". This includes managing areas such as human resources, customer service, and other areas that do not contribute to the corporate "bottom line". While this allows women to rise in ranks as a manager, their career eventually stalls out and they're excluded from the upper echelons.[50][51][52]		Notes		Bibliography		
In human resources context, turnover is the act of replacing an employee with a new employee. Partings between organizations and employees may consist of termination, retirement, death, interagency transfers, and resignations.[1] An organization’s turnover is measured as a percentage rate, which is referred to as its turnover rate. Turnover rate is the percentage of employees in a workforce that leave during a certain period of time. Organizations and industries as a whole measure their turnover rate during a fiscal or calendar year.[1]		If an employer is said to have a high turnover rate relative to its competitors, it means that employees of that company have a shorter average tenure than those of other companies in the same industry. High turnover may be harmful to a company's productivity if skilled workers are often leaving and the worker population contains a high percentage of novices. Companies will often track turnover internally across departments, divisions, or other demographic groups, such as turnover of women versus men. Most companies allow managers to terminate employees at any time, for any reason, or for no reason at all, even if the employee is in good standing. Additionally, companies track voluntary turnover more accurately by presenting parting employees with surveys, thus identifying specific reasons as to why they may be choosing to resign. Many organizations have discovered that turnover is reduced significantly when issues affecting employees are addressed immediately and professionally. Companies try to reduce employee turnover rates by offering benefits such as paid sick days, paid holidays and flexible schedules.[2] In the United States, the average total of non-farm seasonally adjusted monthly turnover was 3.3% for the period from December, 2000 to November, 2008.[3] However, rates vary widely when compared over different periods of time and with different job sectors. For example, during the 2001-2006 period, the annual turnover rate for all industry sectors averaged 39.6% prior to seasonal adjustments,[4] while the Leisure and Hospitality sector experienced an average annual rate of 74.6% during this same period.[5]						There are four types of turnovers: Voluntary is the first type of turnover, which occurs when an employee voluntarily chooses to resign from the organization. Voluntary turnover could be the result of a more appealing job offer, staff conflict, or lack of advancement opportunities.[1]		The second type of turnover is involuntary, which occurs when the employer makes the decision to discharge an employee and the employee unwillingly leaves his or her position.[1] Involuntary turnover could be a result of poor performance, staff conflict, the at-will employment clause, etc.		The third type of turnover is functional, which occurs when a low-performing employee leaves the organization.[1] Functional turnover reduces the amount of paperwork that a company must file in order to rid itself of a low-performing employee. Rather than having to go through the potentially difficult process of proving that an employee is inadequate, the company simply respects his or her own decision to leave.		The fourth type of turnover is dysfunctional, which occurs when a high-performing employee leaves the organization.[1] Dysfunctional turnover can be potentially costly to an organization, and could be the result of a more appealing job offer or lack of opportunities in career advancement. Too much turnover is not only costly, but it can also give an organization a bad reputation. However, there is also good turnover, which occurs when an organization finds a better fit with a new employee in a certain position. Good turnover can also transpire when an employee has outgrown opportunities within a certain organization and must move forward with his or her career in a new organization.[6]		When accounting for the costs (both real costs, such as time taken to select and recruit a replacement, and also opportunity costs, such as lost productivity), the cost of employee turnover to for-profit organizations has been estimated to be between 30% (the figure used by the American Management Association) to upwards of 150% of the employees' remuneration package.[7] There are both direct and indirect costs. Direct costs relate to the leaving costs, replacement costs and transitions costs, and indirect costs relate to the loss of production, reduced performance levels, unnecessary overtime and low morale. The true cost of turnover is going to depend on a number of variables including ease or difficulty in filling the position and the nature of the job itself. Estimating the costs of turnover within an organization can be a worthwhile exercise, especially since “turnover costs” are unlikely to appear in an organization’s balance sheets. Some of the direct costs can be readily calculated, while the indirect costs can often be more difficult to determine and may require “educated guesses.” Nevertheless, calculating even a rough idea of the total expenses relating to turnover can spur action planning within an organization to improve the work environment and reduce turnover. Surveying employees at the time they leave an organization can also be an effective approach to understanding the drivers of turnover within a particular organization.[8]		In a healthcare context, staff turnover has been associated with worse patient outcomes.[9]		Like recruitment, turnover can be classified as "internal" or "external".[10] Internal turnover involves employees leaving their current positions and taking new positions within the same organization. Both positive (such as increased morale from the change of task and supervisor) and negative (such as project/relational disruption, or the Peter Principle) effects of internal turnover exist, and therefore, it may be equally important to monitor this form of turnover as it is to monitor its external counterpart. Internal turnover might be moderated and controlled by typical HR mechanisms, such as an internal recruitment policy or formal succession planning.		Internal turnover, called internal transfers, is generally considered an opportunity to help employees in their career growth while minimizing the more costly external turnover. A large amount of internal transfers leaving a particular department or division may signal problems in that area unless the position is a designated stepping stone position.		Unskilled positions often have high turnover, and employees can generally be replaced without the organization or business incurring any loss of performance.[citation needed] The ease of replacing these employees provides little incentive to employers to offer generous employment contracts; conversely, contracts may strongly favour the employer and lead to increased turnover as employees seek, and eventually find, more favorable employment.		Practitioners can differentiate between instances of voluntary turnover, initiated at the choice of the employee, and involuntary turnover initiated by the employer due to poor performance or reduction in force (RIF).		The US Bureau of Labor Statistics uses the term "Quits" to mean voluntary turnover and "Total Separations" for the combination of voluntary and involuntary turnover.		High turnover often means that employees are dissatisfied with their jobs, especially when it is relatively easy to find a new one.[11] It can also indicate unsafe or unhealthy conditions, or that too few employees give satisfactory performance (due to unrealistic expectations, inappropriate processes or tools, or poor candidate screening). The lack of career opportunities and challenges, dissatisfaction with the job-scope or conflict with the management have been cited as predictors of high turnover.		Each company has its own unique turnover drivers so companies must continually work to identify the issues that cause turnover in their company. Further the causes of attrition vary within a company such that causes for turnover in one department might be very different from the causes of turnover in another department. Companies can use exit interviews to find out why employees are leaving and the problems they encountered in the workplace.		Low turnover indicates that none of the above is true: employees are satisfied, healthy and safe, and their performance is satisfactory to the employer. However, the predictors of low turnover may sometimes differ than those of high turnover. Aside from the fore-mentioned career opportunities, salary, corporate culture, management's recognition, and a comfortable workplace seem to impact employees' decision to stay with their employer.		Many psychological and management theories exist regarding the types of job content which is intrinsically satisfying to employees and which, in turn, should minimise external voluntary turnover. Examples include Herzberg's two factor theory, McClelland's Theory of Needs, and Hackman and Oldham's Job Characteristics Model.[12]		Evidence suggests that distress is the major cause of turnover in organizations.[13]		A number of studies report a positive relationship between bullying, intention to leave and high turnover. In some cases, the number people who actually leave is a “tip of the iceberg”. Many more who remain have considered leaving. In O’Connell et al.’s (2007) Irish study, 60% of respondents considered leaving whilst 15% actually left the organisation.[14] In a study of public-sector union members, approximately one in five workers reported having considered leaving the workplace as a result of witnessing bullying taking place. Rayner explained these figures by pointing to the presence of a climate of fear in which employees considered reporting to be unsafe, where bullies had “got away with it” previously despite management knowing of the presence of bullying.[14]		One can rather easily spot an office with a bullying problem - there is an exceptionally high rate of turnover. While not all places with high personnel turnover are sites of workplace bullying, nearly every place that has a bully in charge will have elevated staff turnover and absenteeism.[15]		According to Thomas, there tends to be a higher level of stress with people who work or interact with a narcissist, which in turn increases absenteeism and staff turnover.[16] Boddy finds the same dynamic where there is a corporate psychopath in the organisation.[17]		Low turnover may indicate the presence of employee "investments" (also known "side bets")[18] in their position: certain benefits may be enjoyed while the employee remains employed with the organization, which would be lost upon resignation (e.g., health insurance, discounted home loans, redundancy packages). Such employees would be expected to demonstrate lower intent to leave than if such "side bets" were not present.		Research suggests that organizational justice plays a significant role in an employee’s intention to exit an organization. Perceptions of fairness are antecedents and determinants of turnover intention, especially in how employees are treated, outcomes are distributed fairly, and processes and procedures are consistently followed.[19]		Employees are important in any running of a business; without them the business would be unsuccessful. However, more and more employers today are finding that employees remain for approximately 23 to 24 months, according to the 2006 Bureau of Labor Statistics[citation needed]. The Employment Policy Foundation states that it costs a company an average of $15,000 per employee, which includes separation costs, including paperwork, unemployment; vacancy costs, including overtime or temporary employees; and replacement costs including advertisement, interview time, relocation, training, and decreased productivity when colleagues depart. Providing a stimulating workplace environment, which fosters happy, motivated and empowered individuals, lowers employee turnover and absentee rates.[20] Promoting a work environment that fosters personal and professional growth promotes harmony and encouragement on all levels, so the effects are felt company wide.[20]		Continual training and reinforcement develops a work force that is competent, consistent, competitive, effective and efficient.[20] Beginning on the first day of work, providing the individual with the necessary skills to perform their job is important.[21] Before the first day, it is important the interview and hiring process expose new hires to an explanation of the company, so individuals know whether the job is their best choice.[22] Networking and strategizing within the company provides ongoing performance management and helps build relationships among co-workers.[22] It is also important to motivate employees to focus on customer success, profitable growth and the company well-being .[22] Employers can keep their employees informed and involved by including them in future plans, new purchases, policy changes, as well as introducing new employees to the employees who have gone above and beyond in meetings.[22] Early engagement and engagement along the way, shows employees they are valuable through information or recognition rewards, making them feel included.[22]		When companies hire the best people, new talent hired and veterans are enabled to reach company goals, maximizing the investment of each employee.[22] Taking the time to listen to employees and making them feel involved will create loyalty, in turn reducing turnover allowing for growth.[23]		Labour turnover is equal to the number of employees leaving, divided by the average total number of employees (in order to give a percentage value). The number of employees leaving and the total number of employees are measured over one calendar year.		Labour Turnover = NELDY (NEBY + NEEY)/2 {\displaystyle {\mbox{Labour Turnover}}={\frac {\mbox{NELDY}}{\mbox{(NEBY + NEEY)/2}}}}		Where: NELDY = Number of Employees who Left During the Year NEBY = Number of Employees at the Beginning of the Year NEEY = Number of Employees at the End of the Year		For example, at the start of the year a business had 40 employees, but during the year 9 staff resigned with 2 new hires, thus leaving 33 staff members at the end of the year. Hence this year's turnover is 25%. This is derived from, (9/((40+33)/2)) = 25%. However the above formula should be applied with caution if data is grouped. For example, if attrition rate is calculated for Employees with tenure 1 to 4 years, above formula may result artificially inflated attrition rate as employees with tenure more than 4 years are not counted in the denominator.		Over the years there have been thousands of research articles exploring the various aspects of turnover,[24] and in due course several models of employee turnover have been promulgated. The first model, and by far the one attaining most attention from researchers, was put forward in 1958 by March & Simon. After this model there have been several efforts to extend the concept. Since 1958 the following models of employee turnover have been published.		
Youth unemployment is the unemployment of young people, defined by the United Nations as 15–24 years old. An unemployed person is defined as someone who does not have a job but is actively seeking work. In order to qualify as unemployed for official and statistical measurement, the individual must be without employment, willing and able to work, of the officially designated 'working age' and actively searching for a position. Youth unemployment rates are historically four to five times more than the adult rates in every country in the world.						There are 1.2 billion youth in the world aged between 15 and 24, accounting for 17% of the world's population.[1] 87% of them live in developing countries.[1] The age range defined by the United Nations[1] addresses the period when mandatory schooling ends until the age of 24.[2] This definition remains controversial as it not only impacts unemployment statistics but also plays an important role in the targeted solutions designed by policy makers in the world.		Two main debates are ongoing today. First, defining the age range of youth is not as obvious as it seems. Two theoretical perspectives have dominated this debate. Youth can be seen as a stage in life between adolescence and adulthood[3] or as a socially constructed group with its own sub-culture, making it difficult to establish a comparable age range between countries.[4] Second, the definition of unemployment itself leads to the possibility of not accounting for a number of young people left out of work. Those who do not have a job and are not actively seeking work – oftentimes women[5] – are considered inactive and are therefore excluded in unemployment statistics. Their inclusion would substantially increase the unemployment rate.[5]		There are multiple and complex causes behind youth unemployment. Among them, the quality and relevance of education, inflexible labour market and regulations, which in turn create a situation of assistance and dependency, are the main causes discussed today.		The quality and relevance of education is often considered as the first root cause of youth unemployment.[6] In 2010, in 25 out of 27 developed countries, the highest unemployment rate was among people with primary education or less[7] Yet, high education does not guarantee a decent job. For example, in Tunisia, 40% of university graduates are unemployed against 24% of non-graduates.[6] This affects highly educated young females in particular. "In Turkey, the unemployment rate among university educated women is more than 3 times higher than that of university educated men; in Iran and the United Arab Emirates, it is nearly 3 times; and in Saudi Arabia, it is 8 times".[6]		Beyond the necessity to ensure its access to all, education is not adequately tailored to the needs of the labour market, which in turns leads to two consequences: the inability for young people to find jobs and the inability for employers to hire the skills they need. Combined with the economic crisis and the lack of sufficient job creation in many countries, it has resulted in high unemployment rates around the world and the development of a skills crisis. Surveys suggest that up to half of all businesses have open positions for which they are struggling to find suitably qualified people.[8] One global survey found that more than 55% of employers worldwide believe there is a "skill crisis"[8] as businesses witness a growing mismatch between the skills students learn in the education system and those required in the workplace. For many governments, a key question is how they can bridge this gap and ensure that young people are equipped with the skills employers are looking for.		First, a high level of employment protection regulations causes employers to be cautious about hiring more than a minimum number of workers, since they cannot easily be laid off during a downturn, or fired if a new employee should turn out to be unmotivated or incompetent.[9] Second, the development of temporary forms of work such as internships, seasonal jobs and short term contracts have left young workers in precarious situations. Because their jobs are temporary contracts, youth are often the first to be laid off when a company downsizes.[10][2][11] If they are laid off, youth are typically not eligible for redundancy payments because they only worked with the company for a short period of time.[12] Once this work ends, many find themselves unemployed and disadvantaged in the job search. However, some youth are entering work on a part-time basis during tertiary education. This rate is low in countries like Italy, Spain and France but in the United States almost one-third of students combine education and work.[2]		The legitimacy of internships has begun to be questioned. The purpose of internships is to allow students or recent graduates to acquire work experience and a recommendation letter to add to their curriculum vitae. However, many interns have complained that they are simply performing basic grunt-work, rather than learning important knowledge and skills. Whether or not these internship positions are now violating the federal rules that are in place to govern programs such as internships remains to be seen. The internship however, seems to be the only viable alternative to job placement for the young individual. With little to no job growth occurring, the unemployment rate among those fresh out of college and at the later end of the 15-24 aged youth spectrum is approximately 13.2% as of April 2012.[13]		Many countries around the world provide income assistance to support unemployed youth until labour market and economic conditions improve.[14] Although this support is strictly related to obligations in terms of active job search and training, it has led to an emerging debate on whether or not it creates dependency among the youth and has a detrimental effect on them.[15] In September 2014, David Cameron announced that he would cut housing and employment benefits for 18- to 21-year-olds by £3,000 to £23,000[16] to reduce dependency on government assistance and redirect funding to targeted programs for increased learning and training opportunities.		The individual experiences of youth unemployment vary from country to country. Definitions of youth can also vary from country to country so examination of particular countries gives a greater insight into the causes and consequences of youth unemployment.		African countries define youth as someone from as young as 15 to someone well into their mid thirties, which varies from the standardized definition of the United Nations.[4] Africa has the youngest population of any continent which means that the problem of youth unemployment there is particularly relevant. Approximately 200 million people in Africa are between the ages of 15 and 24. This number is expected to double in size in the next 30 years.[4] Between 2001 and 2010, countries in Africa reported some of the world's fasted growing economies.[4] In Africa, the message the youth are receiving from schools and adults is to become job creators rather than job-seekers, which encourages them to become entrepreneurs.[4]		Canada's economy has braved the global recession better than many others. But last year, 14.3 percent of Canadian youth were unemployed, up from 11.2 percent in 2007 and double the current national jobless rate of 7.2 percent, according to Statistics Canada. That amounts to the biggest gap between youth and adult unemployment rates since 1977.[17] The average post-secondary graduate carries $28,000 in student debt.[18] The unemployment rate for Canadian young people is about double that of the rest of the population.[19] In Canada's largest province, Ontario, joblessness rates are the highest. The rate of unemployment for Ontarians between the ages of 15–24 is hovering between 16 and 17 per cent, double that of the normal provincial rate and higher than the national youth unemployment rate of 13.5–14.5 per cent. The percentage of youth in Ontario who actually have a job hasn't climbed above 52 per cent this year. Toronto's youth unemployment rate is at 18 per cent, but only 43 per cent of the area's youth are employed, the lowest rate in the province.[20]		The growth of youth unemployment, which reached new heights of 22.5% across the European Union, as well as the precarisation of labor market conditions reveals that the gap between labor market 'outsiders' and 'insiders' is widening. One of the most dramatic possible consequences of this growing divergence could arguably be the disenfranchisement of labour market outsiders, especially young people, from social and political participation (Ferragina et al. 2016 [21]). If the objective of policymakers is to revive social and political participation in a period of great disenchantment and declining legitimacy for our democracies, there is definitely scope for further enquiry and action into the effects of youth outsiderness on social and political participation.		Due to the great recession in Europe, in 2009, only 15 per cent of males and 10 per cent of females between ages 16–19 in were employed full-time. The youth employment rate in the European Union reached an all-time low of 32.9 percent in the first half of 2011.[22] Of the countries in the European Union Germany sticks outs with its low rate of 7.9%.[23] Some critics argue that the decrease of the youth unemployment began even before the economic downturn, countries such as Greece and Spain.		The United Kingdom has experienced increased youth unemployment in the past few years, with rates reaching over 20 percent in 2009.[24] The term NEET originated here, meaning youth that are not in education, employment or training.[2]		The youth unemployment rate was around 10 percent in 2005, but they haven't reliably reported statistics to the United Nations over the years.[24] However, there has been an increase in young adults remaining in school and getting additional degrees simply because there aren't opportunities for employment. These youth are typically of a lower class, but it can represent a wide variety of individuals across races and classes. They call the phenomenon timepass because the youth are simply passing time in college while waiting for a paid employment opportunity. In India, the employment system is reliant on connections or government opportunities.[25]		Within the Eurozone, only Greece and Spain display higher rates of youth unemployment than Italy.[26] Similarly to Spain, the percentage of people aged 15–24 excluded from the labour market saw a dramatic rise in the aftermath of the financial crisis of 2007–2008. Between 2008 and 2014, youth unemployment rose by 21.5%. By that year, almost 43% of the young were excluded from the labour market in Italy.[27] Furthermore, youth unemployment is unequally distributed throughout the country. In the third quarter of 2014, only 29.7% of the young were unemployed in the North. This number increases to an alarming 51.5% when looking at the South of Italy.[28]		There are 15 million unemployed young men in Arab communities.[29] The youth unemployment rate in Jordan has traditionally been much higher than other countries. In the past ten years, the rate has stayed around 23 percent.[24] There has been a recent increase in the popular belief that unemployment is the fault of the individual and not a societal problem.[29] However, youth unemployment has also been attributed to increased pressure on service sectors that typically employ more youth in Jordan.[29] Youth unemployment has led to later and later ages of marriage in Jordan, which some view as one of the most important consequences of the phenomenon.[29] Another consequence experienced in Jordan is increased mental health problems.[29]		Youth unemployment in Russia was over 18 percent in 2010.[24] However, there was a wide variance in levels of unemployment in Russia just a few years earlier, that continued through the 2008 economic crisis. In 2005, the area around Moscow had an unemployment rate of just 1 percent while the Dagestan region had a rate over 22 percent. This may be partially attributed to the differences in levels of development in the region. It has been found that the higher the level of development in a region, the lower the level of both overall and youth-specific unemployment.[11][30] In Russia, the main cause of youth unemployment has been attributed to lower levels of human capital.[11]		Starting in the 1970s, youth unemployment has been rising at a steady rate in South Africa.[31] Today, South Africa is ranked as the fourth country with the highest percentage of unemployed youth in the world. As of 2014, 52.6 percent of the people aged 15–24 actively looking for a job were unemployed.[32] Furthermore, youth unemployment is unequally distributed throughout different segments of the population. While unemployment between young whites amounts to 12%, this number skyrockets to a troubling 70% between young blacks.[33] It may be that remnant effects of the apartheid era has led to jobs centres being located farther away from typical homes of black communities compared to white communities. This, lingering discrimination, and unequal backgrounds are among the many reasons for the lopsided distribution of unemployment among young white and black South Africans.[34]		Many of the unemployed youth have never worked before. A proposed reason for this is that South Africa's social pension program is relatively generous compared to other middle-income countries. Some senior South Africans (mostly applicable to the white population) are paid almost twice the per capita income. This has led to many of the unemployed youth surviving off of their elders' support, thereby reducing incentives to look for employment.[34] In addition, the reservation wages of many young Africans are prohibitively high. Around 60% of males and 40% of females have reservation wages that are higher than they could expect from smaller sized firms.[35] Some overestimate their ability to obtain jobs from competitive, high-paying, larger sized firms and thus remain unemployed. The higher pay of larger firms, in addition to the costs of employment (such as transport or housing costs), make it almost unfeasible for some youth to accept lower paying jobs from smaller firms. Thus, many of the youth in South Africa choose to remain unemployed until they are able to find a job at a larger firm.[35] South African youth also face problems of education. Many exit the schooling system early. Others face a lack of skill recognition from employers, "even if they have qualifications in the fields that are considered to be in high demand."[36]		In recent decades, the issue of youth unemployment has assumed alarming proportions in Spain. The country was dramatically hit by the Financial crisis of 2007–2008 and the number of young unemployed skyrocketed during this period. Within OECD countries, Spain displayed the most significant increases in terms of job losses within those aged 15–24.[37] By 2014, 57.9 percent of the youth in Spain was unemployed.[26] The failed implementation of effective employment policies and the increased segmentation of the labour marked during the economic recession are thought to be the main causes behind such an alarming situation.[36]		Youth unemployment in the United Kingdom is the level of unemployment among young people, typically defined as those aged 18–25. A related concept is graduate unemployment which is the level of unemployment among university graduates. Statistics for June 2010 show that there are 926,000 young people under the age of 25 who are unemployed which equates to an unemployment rate of 19.6% among young people.[38] This is the highest youth unemployment rate in 17 years.[39] In November 2011 youth unemployment hit 1.02 million,[40] but had fallen to 767,000 by August 2014.[41] The high levels of youth unemployment in the United Kingdom have led some politicians and media commentators to talk of a "lost generation".[42][43][44][45]		The general unemployment rate in the United States has increased in the last 5 years, but the youth unemployment rate has jumped almost 10 percentage points.[24] In 2007, before the most recent recession began, youth unemployment was already at 13 percent. By 2008, this rate had jumped to 18 percent and in 2010 it had climbed to just under 21 percent.[10][24] The length of time the youth are unemployed has expanded as well, with many youth in the United States remaining unemployed after more than a year of searching for a job.[10] This has caused the creation of a scarred generation, as discussed below.		Youth unemployment levels in Greece remain one of the highest in the world. According to one source, between 2000 and 2008, youth inactivity increased from 63 percent to 72 percent.[46] A different source using the harmonized definition of unemployment lists the unemployment rate of youth up to 24 years of age as 24.2% in Greece during 2009.[47] To put this into perspective, the EU-27 average at the time was 18.3%.[47] Youth unemployment rose to 40.1% in May 2011 and then again to about 55% in November 2012.[48]		In addition to youth unemployment (namely those up to 25 years of age), Greece also faced severe graduate unemployment of those 25–29 years of age. In 1998, Greece had the highest level of unemployment of higher education graduates in the 25-29 year old age group. This was due to a lack of demand for highly educated personnel at the time.[49] This trend of low employment among those with higher educational qualifications continues on today. As recently as 2009, "one in three higher education graduates, two in three secondary graduates, and one in three compulsory education graduates have not found some form of stable employment."[50] This lack of employment is thought to have contributed to the feelings of frustration among youth that eventually led to the 2008 Greek riots.		These high levels of unemployment are exacerbated by the failure of unions to attract young workers. GSEE's Young Workers Committee revealed in a 2008 presentation that almost two-thirds of young workers did not joined their workplace unions.[51] Although unions like GSEE and ADEDY actively promote wage increases through collective bargaining efforts and have contributed to obtaining higher wages for young workers, the wages of young workers remained much lower than almost all other countries in EU-15.[51]		Unemployed youth has been called "a lost generation": not only because of productivity loss but also because of the long-term direct and indirect impact unemployment has on young people and their families. Unemployment has been said to affect earnings for about 20 years. Because they aren't able to build up skills or experience during their first years in the workforce, unemployed youth see a decrease in lifetime earnings when compared to those who had steady work or those who were unemployed as an adult. A lower salary can persist for 20 years following the unemployed period before the individual begins earning competitively to their peers.[10] Widespread youth unemployment also leads to a socially excluded generation at great risk for poverty. For example, Spain saw an 18% increase in income inequality.[10]		The lost generation effect impacts also their families. Youth in many countries now live with their parents into their late twenties.[2] This contributes to what is called the "full-nest syndrome". In 2008, 46% of 18- to 34-year-olds in the European Union lived with at least one parent; in most countries the stay-at-homes were more likely to be unemployed than those who had moved out.[52] In families, it is common that when one person becomes unemployed, other members of the family begin looking for or securing employment.[53] This is called the added worker effect. This can sometimes take the form of employment in the informal sector when necessary.[53] Alongside the shift in youth living situations, the impact of returning to live with parents as well as difficulty finding a fulfilling job lead to mental health risks. Being unemployed for a long period of time in youth has been correlated to decreased happiness, job satisfaction and other mental health issues.[10] Unemployed youth also report more isolation from their community.[29] Youth who are neither working nor studying do not have the opportunity to learn and improve their skills. They are progressively marginalised from the labour market and in turn can develop an anti-social behaviour.		The rise of political unrest and anti-social behaviour in the world has been recently attributed to youth unemployment. During the course of 2011 it became a key factor in fuelling protests around the globe. Within twelve months, four regimes (Tunisia, Egypt, Libya, Yemen) in the Arab World fell in the wake of the protests led by young people. Riots and protests similarly engulfed a number of European and North American cities (Spain, France, United Kingdom between 2008 and 2011 for example). The lack of productive engagement of young people in wider society, underlined by high levels of unemployment and under-employment, only serves to add to this feeling of disenfranchisement.		Youth unemployment also dramatically increases public spending at times when economies are struggling to remain competitive and social benefits increase along with an aging population. Youth unemployment has direct costs such as increased benefit payments, lost income-tax revenues and wasted capacity.[52] "In Britain a report by the London School of Economics (LSE), the Royal Bank of Scotland and the Prince's Trust puts the cost of the country's 744,000 unemployed youngsters at £155m ($247m) a week in benefits and lost productivity".[52] Similarly, the economic loss from youth unemployment in Europe is estimated at €153 billion or 1.2% of GDP in 2011.[54]		Youth unemployment has indirect costs too, including emigration. Young people leave their countries in hope to find employment elsewhere.[52] This brain drain has contributed to deteriorating countries' competitiveness, especially in Europe.		The economic crisis has led to a global decrease in competitiveness. "There is a risk of loss of talent and skills since a great amount of university graduates are unable to find a job and to put their knowledge and capabilities into producing innovation and contributing to economic growth".[55] Excluding young people from the labour market means lacking the divergent thinking, creativity and innovation that they naturally offer. This fresh thinking is necessary for employers to foster new designs and innovative ideas.[54] Fighting youth unemployment is therefore key to maintaining the economic performance of a country.		A 2015 study showed that New York City's 'Summer Youth Employment Program' decreased the participants' probability of incarceration and probability of mortality.[56]		The role of labour market policy and institutions varies a lot from countries to countries. Here is a brief account of key propositions recently elaborated to facilitate access to employment for youth. First, a more balanced employment protection for permanent and temporary workers is needed. It will ensure that young people who lack work experience can prove their abilities and skills to then progressively transition to regular employment.[14] It will also encourage a more equal treatment between permanent and temporary workers and help combat informal employment. This proposition has led to multiple discussions on flexible contracts to be designed and offered to youth. Second, discussions are focused on the level and spread of income support provided to unemployed youth.[14] While some countries consider shifting their support from direct financial assistance to funding apprenticeship, others are increasing their support tying it back to stricter obligations of active search and training. Third, Governments are progressively involving employers and trainers to create a holistic approach to youth unemployment and provide intensive programmes with focus on remedial education, work experience and adult mentoring.[57] Some economists argue that high values of minimum wage can be a factor that increases youth unemployment.[58] One Active Labor Market Policy (ALMP) that many governments have put emphasis on in an attempt to tackle unemployment is to directly help unemployed individuals transition to self-employment. Various pan-European studies have shown great success of these programs with regards to job creation and overall well-being.[59]		The case has been made the past few years on the need to provide technical training to youth to prepare them specifically for a job. TVET and Vocational education would help address the skills crisis. Some countries – among them Switzerland, The Netherlands, Singapore, Austria, Norway and Germany – have been remarkably successful in developing vocational education – and have reduced youth unemployment to as little as half the OECD average.[57]		Three main reasons are usually presented for why vocational education should be a part of political programmes to combat youth unemployment:		Foundational skills have also been identified as key to a successful transition to work. "Across OECD countries, PISA results indicate that almost one in five students do not reach a basic minimum level of skills to function in today's societies".[14] On average, 20% of young adults drop out before completing upper secondary education level.[14] Vocational education is dedicated to teaching foundational skills in addition to providing an alternative to general education pathways with on-the-job training.		Many countries offer programmes to improve youth skills and employability. Turkey, among others, has focused on equipping students with skills that are useful for running one’s own business and becoming an entrepreneur. The United Kingdom and Australia have attempted to modernize apprenticeships and use them to provide training for youths in non-traditional occupations. Other measures for youth employment have focused on easing transitions from school or training to work and jobs, such as careers information, advice and guidance services.[63] 		The education system plays a central role in the debate about the youth labor market crisis. What has become evident is that there need to be major changes in what we teach and in the way we teach. One prominent approach taken by various educators is to shift teaching from knowledge-centered teaching to skills-centered teaching.[64][65] "In order to materialize the shift from exclusively content-based to a balanced content-and-skill-based curricula, education providers should make it their goal to establish a guiding skills framework which allows teachers and professors to see the types of skills and applied content they should be transmitting to their students. All educational institutions should work towards adopting or creating a suitable skills framework that aligns with the labor market, which is flexible enough for educators to adapt their subject or grade level. Moreover, this framework should act as a living document that schools and universities can modify to fit their communities or to accommodate changes in the market."[59]		When taking into consideration the need to foster competitiveness through innovation and creativity, recent studies have advocated for entrepreneurship as a viable a solution to youth unemployment.[66] With the right structure and facilitated administrative processes, young people could create enterprises as means to find and create new jobs.[67] According to the OECD, Small and Medium Enterprises are today's main employers with 33% of jobs created over the last ten years.[68] It shows that big companies no longer represent the main sources of employment and that there is a necessity to prepare young people for an entrepreneurship culture. This alternative is often regarded as a way to empower young people to take their future into their hands: it means investing in teaching them the leadership and management skills they need to become innovators and entrepreneurs.[69] These skills also include: communication, teamwork, decision-making, organisational skills and self-confidence.		This solution ties back with labour market and regulations as many reforms are yet to be implemented to ensure that the market is flexible enough to incentivize young people to create enterprises. Target tax and business incentives are key to support young entrepreneurs in creating and scaling their businesses.[70]		A number of studies have shown that young people are not sufficiently advised on work related opportunities, necessary skills and career pathways. Before they leave education, it appears critical that they have access to this information to be better prepared for what to expect and what is expected of them. Good quality career guidance along with labour market prospects should help young people make better career choices.[14] Too many young people choose to study a field that leads to little if no jobs. Governments, employers and trainers should work together to provide clearer pathways to youth. Similarly, programmes should be developed to better transition young people to the world of work. Here, vocational education and apprenticeship systems have shown that practice and on-the-job training had a positive effect.[57]		Awareness has been raised around youth unemployment and it appears clearly that cross-sector collaboration is needed to tackle this issue. Policy makers but also entrepreneurs are trying to address the causes listed below. Best practices and key success factors are now identified and discussed on many forums, such as Decent Work 4 youth, an initiative by the International Labour Organization. Social entrepreneurs have also invested the field with the creation of new online platforms and applications.		Internet has been seen as a new world of opportunities for youth unemployment. With the use of social networks such as Facebook, Aboutme, LinkedIn, Twitter, young people are actively building their informal networks. New web applications are being designed today to use these networks to better match job seekers with employers, training volunteers and other forms of placement or mentoring. The Internet has contributed to redefining traditional forms of communication and young social entrepreneurs are now thinking about designing a job application that fits more with today's online presence and use of new technology. For example, the introduction of 1-minute videos to send to potential employers is being tested. Serious games to mimic the world of work or provide an online "smart" coach are also being developed.[71]		To learn how to add open-license text to Wikipedia articles, please see Wikipedia:Adding open license text to Wikipedia.		
Casual Friday (also known as dress-down Friday or casual day) is a Western trend, which has spread to other parts of the world, in which some business offices relax their dress code on Fridays. Some businesses that usually require employees to wear suits, dress shirts, neckties, and dress shoes allow more casual attire on Fridays. Casual Friday had its origin from Hawaii's custom of Aloha Friday which slowly spread east to California, continuing around the globe until the 1990s, when it became known as Casual Friday. Today, in Hawaii Aloha Wear is worn as business attire for any day of the week, and Aloha Friday is generally used to refer to the last day of the work week.		
Sleeping while on duty or sleeping on the job refers to falling asleep while on the time clock or equivalent, or else while responsible for performing some active or passive job duty. While in some jobs, this is a minor transgression or not even worthy of sanctioning, in other workplaces, this is considered gross misconduct and may be grounds for disciplinary action, including possible termination of employment.[1][2] In other types of work, such as firefighting or live-in caregiving, sleeping at least part of the shift may be a part of the paid work time. While some employees who sleep while on duty in violation do so intentionally and hope not to get caught, others intend in good faith to stay awake, and accidentally doze.		Sleeping while on duty is such an important issue that it is addressed in the employee handbook in some workplaces.[3] Concerns that employers have may include the lack of productivity, the unprofessional appearance, and danger that may occur when the employee's duties involve watching to prevent a hazardous situation.[4] In some occupations, such as pilots, truck and bus drivers, or those operating heavy machinery, falling asleep while on duty puts lives in danger.						The frequency of sleeping while on duty that occurs varies depending on the time of day. Daytime employees are more likely to take short naps, while graveyard shift workers have a higher likelihood of sleeping for a large portion of their shift, sometimes intentionally.		A survey by the National Sleep Foundation has found that 30% of participants have admitted to sleeping while on duty.[5][6] More than 90% of Americans have experienced a problem at work because of a poor night's sleep. One in four admit to shirking duties on the job for the same reason, either calling in sick or napping during work hours.[7]		Employers have varying views of sleeping while on duty. Some companies have instituted policies to allow employees to take napping breaks during the workday in order to improve productivity[8] while others are strict when dealing with employees who sleep while on duty and use high-tech means, such as video surveillance, to catch their employees who may be sleeping on the job. Those who are caught in violation may face disciplinary action such as suspension or firing.		Some employees sleep, nap, or take a power-nap only during their allotted break time at work. This may or may not be permitted, depending on the employer's policies. Some employers may prohibit sleeping, even during unpaid break time, for various reasons, such as the unprofessional appearance of a sleeping employee, the need for an employee to be available during an emergency, or legal regulations. Employees who may endanger others by sleeping on the job may face more serious consequences, such as legal sanctions. For example, airline pilots risk loss of their licenses.		In some industries and work cultures sleeping at work is permitted and even encouraged. Such work cultures typically have flexible schedules, and variant work loads with extremely demanding periods where employees feel unable to spend time commuting. In such environments it is common for employers to provide makeshift sleeping materials for employees, such as a couch and/or inflatable mattress and blankets. This practice is particularly common in start-ups.[9] and during political campaigns. In those work cultures sleeping in the office is seen as evidence of dedication.		In 1968, New York police officers admitted that sleeping while on duty was customary.[10]		In Japan, the practice of napping in public, called inemuri (居眠り, lit. "present while sleeping"), may occur in work meetings or classes. Brigitte Steger, a scholar who focuses on Japanese culture, writes that sleeping at work is considered a sign of dedication to the job, such that one has stayed up late doing work or worked to the point of complete exhaustion, and may therefore be excusable.[11][12]		
Methods of calculation and presentation of unemployment rate vary from country to country. Some countries count insured unemployed only, some count those in receipt of welfare benefit only, some count the disabled and other permanently unemployable people, some countries count those who choose (and are financially able) not to work, supported by their spouses and caring for a family, some count students at college and so on.		For purposes of comparison, harmonized values are published by International Labour Organization (ILO) and by OECD. The ILO harmonized unemployment rate refers to those who are currently not working but are willing and able to work for pay, currently available to work, and have actively searched for work. The OECD harmonized unemployment rate gives the number of unemployed persons as a percentage of the labour force.		
Disability Insurance, often called DI or disability income insurance, or income protection, is a form of insurance that insures the beneficiary's earned income against the risk that a disability creates a barrier for a worker to complete the core functions of their work. For example, the worker may suffer from an inability to maintain composure in the case of psychological disorders or an injury, illness or condition that causes physical impairment or incapacity to work. It encompasses paid sick leave, short-term disability benefits (STD), and long-term disability benefits (LTD).[1] Statistics show that in the US a disabling accident occurs, on average, once every second.[2] In fact, nearly 18.5% of Americans are currently living with a disability,[citation needed] and 1 out of every 4 persons in the US workforce will suffer a disabling injury before retirement.[3]						If one were to get sick or injured and unable to work, disability insurance can provide a source of income while the beneficiary is unable to work. If someone were not able to work ever again in their field disability insurance can provide a source of income for a majority of their life.[4]		In the late 19th century, modern disability insurance began to become available. It was originally known as "accident insurance".[5][6] The first company to offer accident insurance was the Railway Passengers Assurance Company, formed in 1848 in England to insure against the rising number of fatalities on the nascent railway system. It was registered as the Universal Casualty Compensation Company to:		The company was able to reach an agreement with the railway companies, whereby basic accident insurance would be sold as a package deal along with travel tickets to customers. The company charged higher premiums for second and third class travel due to the higher risk of injury in the roofless carriages.[7][8]		Those whose employers do not provide benefits, and self-employed individuals who desire disability coverage, may purchase policies. Premiums and available benefits for individual coverage vary considerably between companies, occupations, states and countries. In general, premiums are higher for policies that provide more monthly benefits, offer benefits for longer periods of time, and start payments of benefits more quickly following a disability claim. Premiums also tend to be higher for policies that define disability in broader terms, meaning the policy would pay benefits in a wider variety of circumstances thus covering more insurances that the individual was going to purchase. Web-based disability insurance calculators assist in determining the disability insurance needed.[9]		High-limit disability insurance is designed to keep individual disability benefits at 65% of income regardless of income level. Coverage is typically issued supplemental to standard coverage. With high-limit disability insurance, benefits can be anywhere from an additional $2,000 to $100,000 per month. Single policy issue and participation (individual or group long-term disability) coverage has gone up to $30,000 with some hospitals.		Business Overhead Expense (BOE) coverage reimburses a business for overhead expenses should the owner experience a disability. Eligible benefits include: rent or mortgage payments, utilities, leasing costs, laundry/maintenance, accounting/billing and collection service fees, business insurance premiums, employee salaries, employee benefits, property tax, and other regular monthly expenses.		In most developed countries, the single most important form of disability insurance is that provided by the national government for all citizens. For example, the UK's version is part of National Insurance; the U.S.'s version is Social Security (SS)—specifically, several parts of SS including Social Security Disability Insurance (SSDI) and Supplemental Security Income (SSI). These programs provide a floor beneath all other disability insurance. In other words, they are the safety net that catches everyone who was otherwise (a) uninsured or (b) underinsured. As such, they are large programs with many beneficiaries. The general theory of the benefit formula is that the benefit is enough to prevent abject poverty.[citation needed]		In addition to federally funded programs, there are five states which currently offer state funded Disability Insurance programs. These programs are designed for short term disabilities only. The coverage amount is determined by the applicant's level of income over the previous 12 months. The states which currently fund disability insurance programs are California, New York, New Jersey, Rhode Island, and Hawaii.[10]		One of the most common reasons for disability is on-the-job injury, which explains why the second largest form of disability insurance is that provided by employers to cover their employees. There are several subtypes that may or may not be separate parts of the benefits package: workers' compensation and more general disability insurance policies.		Workers' compensation (also known by variations of that name, e.g., workman's comp, workmen's comp, worker's comp, compo) offers payments to employees who are (usually temporarily, rarely permanently) unable to work because of a job-related injury. However, workers' compensation is in fact more than just income insurance, because it compensates for economic loss (past and future), reimbursement or payment of medical and life expenses (functioning in this case as a form of health insurance), and benefits payable to the dependents of workers killed during employment (offering a form of life insurance). Workers compensation provides no coverage to those not working. Statistics have shown that the majority of disabilities occur while the injured person is not working and therefore is not covered by workers' compensation.[11]		Newsweek magazine's cover story for March 5, 2007 discussed the problems that American veterans of Afghanistan and Iraq wars have faced in receiving VA benefits. The article describes one veteran who waited 17 months to start receiving payments. Another article, in The New York Times, points out that besides long waits, there is also variation based on the veteran's state of residence and whether he/she is a veteran of the Army, National Guard, or Reserves.[12] The Newsweek article says that it can be difficult for a veteran to get his or her claim approved; Newsweek described the benefits thus:		The 2007 figures cited above correspond in 2012 to $2,673 a month (more with children) and, for the 50% rating, $797 a month for a single veteran.		According to a sidebar in the same Newsweek article,[14] the Americans injured in these wars, for all the obstacles to proper care, will probably receive much better compensation and health care than equally injured Afghan or Iraqi soldiers.		
A drug test is a technical analysis of a biological specimen, for example urine, hair, blood, breath, sweat, and/or oral fluid/saliva — to determine the presence or absence of specified parent drugs or their metabolites. Major applications of drug testing include detection of the presence of performance enhancing steroids in sport, employers screening for drugs prohibited by law (such as cannabis, cocaine and heroin) and police officers testing for the presence and concentration of alcohol (ethanol) in the blood commonly referred to as BAC (blood alcohol content). BAC tests are typically administered via a breathalyzer while urinalysis is used for the vast majority of drug testing in sports and the workplace.		A drug test may also refer to a test that provides quantitative chemical analysis of an illegal drug, typically intended to help with responsible drug use.[1]						The following chart gives approximate detection periods for each substance by test type.[2]		The detection windows depend upon multiple factors: drug class, amount and frequency of use, metabolic rate, body mass, age, overall health, and urine pH. For ease of use, the detection times of metabolites have been incorporated into each parent drug. For example, heroin and cocaine can only be detected for a few hours after use, but their metabolites can be detected for several days in urine. The chart depicts the longer detection times of the metabolites.		Oral fluid or saliva testing results for the most part mimic that of blood. The only exceptions are THC (tetrahydrocannabinol) and benzodiazepines. Oral fluid will likely detect THC from ingestion up to a maximum period of 6–12 hours. This continues to cause difficulty in oral fluid detection of THC and benzodiazepines.[3]		Breath air for the most part mimics blood tests as well. Due to the very low levels of substances in the breath air, liquid chromatography—mass spectrometry has to be used to analyze the sample according to a recent publication wherein 12 analytes were investigated.		Rapid oral fluid products are not approved for use in workplace drug testing programs and are not FDA cleared. Using rapid oral fluid drug tests in the workplace is prohibited in only:[4]		K2, also known as synthetic cannabinoids, is detectable for up to 3 days after single use or up to 30 days for chronic users. It wasn't tested for in the past but is now detectable in modern tests.		Urine analysis is primarily used because of its low cost. Urine drug testing is one of the most common testing methods used. The enzyme-multiplied immune test is the most frequently used urinalysis. Complaints have been made about the relatively high rates of false positives using this test.[18]		Urine drug tests screen the urine for the presence of a parent drug or its metabolites. The level of drug or its metabolites is not predictive of when the drug was taken or how much the patient used. Rather, it is simply a confirmatory report indicating the presence of the parent drug or its metabolites.[19]		Urine drug testing is an immunoassay based on the principle of competitive binding. Drugs which may be present in the urine specimen compete against their respective drug conjugate for binding sites on their specific antibody. During testing, a urine specimen migrates upward by capillary action. A drug, if present in the urine specimen below its cut-off concentration, will not saturate the binding sites of its specific antibody. The antibody will then react with the drug-protein conjugate and a visible colored line will show up in the test line region of the specific drug strip [20]		When an employer requests a drug test from an employee, or a physician requests a drug test from a patient, the employee or patient is typically instructed to go to a collection site or their home. The urine sample goes through a specified 'chain of custody' to ensure that it is not tampered with or invalidated through lab or employee error. The patient or employee’s urine is collected at a remote location in a specially designed secure cup, sealed with tamper-resistant tape, and sent to a testing laboratory to be screened for drugs (typically the Substance Abuse and Mental Health Services Administration 5 panel). The first step at the testing site is to split the urine into two aliquots. One aliquot is first screened for drugs using an analyzer that performs immunoassay as the initial screen. To ensure the specimen integrity and detecting possible adulterant, some other parameters such as, urine creatinine, pH, and specific gravity are tested along in this initial test. If the urine screen is positive then another aliquot of the sample is used to confirm the findings by gas chromatography—mass spectrometry (GC-MS) or liquid chromatography - mass spectrometry methodology. If requested by the physician or employer, certain drugs are screened for individually; these are generally drugs part of a chemical class that are, for one of many reasons, considered more abuse-prone or of concern. For instance, oxycodone and diamorphine may be tested, both sedative analgesics. If such a test is not requested specifically, the more general test (in the preceding case, the test for opiates) will detect the drugs, but the employer or physician will not have the benefit of the identity of the drug.		Employment-related test results are relayed to a medical review office (MRO) where a medical physician reviews the results. If the result of the screen is negative, the MRO informs the employer that the employee has no detectable drug in the urine, typically within 24 hours. However, if the test result of the immunoassay and GC-MS are non-negative and show a concentration level of parent drug or metabolite above the established limit, the MRO contacts the employee to determine if there is any legitimate reason—such as a medical treatment or prescription.[21][22]		On-site instant drug testing is a more cost-efficient method of effectively detecting drug abuse amongst employees, as well as in rehabilitation programs to monitor patient progress.[citation needed] These instant tests can be used for both urine and saliva testing. Although the accuracy of such tests varies with the manufacturer, some kits boast extremely high rates of accuracy, correlating closely with laboratory test results.[citation needed]		Breath test is a widespread method for quickly determining alcohol intoxication. A breath test measures the alcohol concentration in the body by a deep-lung breath. There are different instruments used for measuring the alcohol content of an individual though their breath. Breathalyzer is a widely known instrument which was developed in 1954 and contained chemicals unlike other breath-testing instruments.[23] More modernly used instruments are the infrared light-absorption devices and fuel cell detectors, these two testers are microprocessor controlled meaning the operator only has to press the start button.		To get accurate readings on a breath-testing device the individual must blow for approximately 6 seconds and need to contain roughly 1.1 to 1.5 liters of breath. For a breath-test to result accurately and truly an operator must take steps such as avoiding measuring “mouth alcohol” which is a result from regurgitation, belching, or recent intake of an alcoholic beverage.[24] To avoid measuring “mouth alcohol” the operator must not allow the individual that’s taking the test to consume any materials for at least fifteen minutes before the breath test. When pulled over for a driving violation if an individual in the United States refuses to take a breath test that individual's driver's license can be suspend for a 6 to 12 months time period.		Hair analysis to detect drugs of abuse has been used by court systems in the United States, United Kingdom, Canada, and other countries worldwide. In the United States, hair testing has been accepted in court cases as forensic evidence following the Frye Rule, the Federal Rules of Evidence, and the Daubert Rule. As such, hair testing results are legally and scientifically recognized as admissible evidence.[citation needed]		Although some lower courts may have accepted hair test evidence, there is no controlling judicial ruling in either the federal or any state system declaring any type of hair test as reliable.		Hair testing is now recognized in both the UK and US judicial systems. There are guidelines for hair testing that have been published by the Society of Hair Testing (a private company in France) that specify the markers to be tested for and the cutoff concentrations that need to be tested. Drugs of abuse that can be detected include Cannabis, Cocaine, Amphetamines and drugs new to the UK such as Mephedrone.		In contrast to other drugs consumed, alcohol is deposited directly in the hair. For this reason the investigation procedure looks for direct products of ethanol metabolism. The main part of alcohol is oxidized in the human body. This means it is released as water and carbon dioxide. One part of the alcohol reacts with fatty acids to produce esters. The sum of the concentrations of four of these fatty acid ethyl esters (FAEEs: ethyl myristate, ethyl palmitate, ethyl oleate and ethyl stearate) are used as indicators of the alcohol consumption. The amounts found in hair are measured in nanograms (one nanogram equals only one billionth of a gram), however with the benefit of modern technology, it is possible to detect such small amounts. In the detection of ethyl glucuronide, or EtG, testing can detect amounts in picograms (one picogram equals 0.001 nanograms).		However, there is one major difference between most drugs and alcohol metabolites in the way in which they enter into the hair: on the one hand like other drugs FAEEs enter into the hair via the keratinocytes, the cells responsible for hair growth. These cells form the hair in the root and then grow through the skin surface taking any substances with them. On the other hand, the sebaceous glands produce FAEEs in the scalp and these migrate together with the sebum along the hair shaft (Auwärter et al., 2001, Pragst et al., 2004). So these glands lubricate not only the part of the hair that is just growing at 0.3 mm per day on the skin surface, but also the more mature hair growth, providing it with a protective layer of fat.		FAEEs (nanogram = one billionth of a gram) appear in hair in almost one order of magnitude lower than (the relevant order of magnitude of) EtG (picogram = one trillionth of a gram). It has been technically possible to measure FAEEs since 1993, and the first study reporting the detection of EtG in hair was done by Sachs in 1993.[25]		In practice, most hair which is sent for analysis has been cosmetically treated in some way (bleached, permed etc.). It has been proven that FAEEs are not significantly affected by such treatments (Hartwig et al., 2003a). FAEE concentrations in hair from other body sites can be interpreted in a similar fashion as scalp hair (Hartwig et al., 2003b).		Presumptive substance tests identify a suspicious substance, material or surface where traces of drugs are thought to be, instead of testing individuals through biological methods such as urine or hair testing. The test involves mixing the suspicious material with a chemical in order to trigger a color change to indicate if a drug is present. Most are now available over-the-counter, and do not require a lab to read results.		Benefits to this method include that the person who is suspected of drug use does not need to be confronted or aware of testing. Only a very small amount of material is needed to obtain results, and can be used to test powder, pills, capsules, crystals, or organic material. There is also the ability to detect illicit material when mixed with other non-illicit materials. The tests are used for general screening purposes, offering a generic result for the presence of a wide range of drugs, including Heroin, Cocaine, Methamphetamine, Amphetamine, Ecstasy/MDMA, Methadone, Ketamine, PCP, PMA, DMT, MDPV, and may detect rapidly evolving synthetic designer drugs. Separate tests for Marijuana/Hashish are also available.[26]		There are five primary color-tests reagents used for general screening purposes. The Marquis reagent turns into a variety of colors when in the presence of different substances. Dille-Koppanyi reagent uses two chemical solutions which turns a violet-blue color in the presence of barbiturates. Duquenois-Levine reagent is a series of chemical solutions that turn to the color of purple when the vegetation of marijuana is added. Van Urk reagent turns blue-purple when in the presence of LSD. Scott Test's chemical solution shows up as a faint blue for cocaine base.[27]		Saliva / oral fluid-based drug tests can generally detect use during the previous few days. Is better at detecting very recent use of a substance. THC may only be detectable for 2–24 hours in most cases. On site drug tests are allowed per the Department of Labor.[citation needed]		Detection in saliva tests begins almost immediately upon use of the following substances, and lasts for approximately the following times:		A disadvantage of saliva based drug testing is that it is not approved by FDA or SAMHSA for use with DOT / Federal Mandated Drug Testing. Oral fluid is not considered a bio-hazard unless there is visible blood; however, it should be treated with care.		Sweat patches are attached to the skin to collect sweat over a long period of time (up to 14 days).[29] These are used by child protective services, parole departments, and other government institutions concerned with drug use over long periods, when urine testing is not practical.[30] There are also surface drug tests that test for the metabolite of parent drug groups in the residue of drugs left in sweat.		Drug-testing a blood sample measures whether or not a drug or a metabolite is in the body at a particular time. These types of tests are considered to be the most accurate way of telling if a person is intoxicated. Blood drug tests are not used very often because they need specialized equipment and medically trained administrators.		Depending on how much marijuana was consumed, it can usually be detected in blood tests within six hours of consumption. After six hours has passed, the concentration of marijuana in the blood decreases significantly. It generally disappears completely within 30 days.		Anabolic steroids are used to enhance performance in sports and as they are prohibited in most high-level competitions drug testing is used extensively in order to enforce this prohibition. This is particularly so in individual (rather than team) sports such as athletics and cycling.		Can occur at any time, usually when the investigator has reason to believe that a substance is possibly being abused by the subject by behavior or immediately after an employee-related incident occurs during work hours. Testing protocol typically conforms to the national medical standard, candidates are given up to 120 minutes to reasonably produce a urine sample from the time of commencement (in some instances this time frame may be extended at the examiners discretion).		In the case of life-threatening symptoms, unconsciousness, or bizarre behavior in an emergency situation, screening for common drugs and toxins may help find the cause, called a toxicology test or tox screen to denote the broader area of possible substances beyond just self-administered drugs. These tests can also be done post-mortem during an autopsy in cases where a death was not expected. The test is usually done within 96 hours (4 days) after the desire for the test is realized. Both a urine sample and a blood sample may be tested.[31] A blood sample is routinely used to detect ethanol/methanol and ASA/paracetamol intoxication. Various panels are used for screening urine samples for common substances, e.g. triage 8 that detects amphetamines, benzodiazepines, cocaine, methadone, opiates, cannabis, barbiturates and tricyclic antidepressants.[32] Results are given in 10–15 min.		Similar screenings may be used to evaluate the possible use of date rape drugs. This is usually done on a urine sample.[31]		Before testing samples, the tamper-evident seal is checked for integrity. If it appears to have been tampered with or damaged, the laboratory rejects the sample and does not test it.		Next, the sample must be made testable. Urine and oral fluid can be used "as is" for some tests, but other tests require the drugs to be extracted from urine. Strands of hair, patches, and blood must be prepared before testing. Hair is washed in order to eliminate second-hand sources of drugs on the surface of the hair, then the keratin is broken down using enzymes. Blood plasma may need to be separated by centrifuge from blood cells prior to testing. Sweat patches are opened and the sweat collection component is removed and soaked in a solvent to dissolve any drugs present.		Laboratory-based drug testing is done in two steps. The first step is the screening test, which is an immunoassay based test applied to all samples. The second step, known as the confirmation test, is usually undertaken by a laboratory using highly specific chromatographic techniques and only applied to samples that test positive during the screening test.[33] Screening tests are usually done by immunoassay (EMIT, ELISA, and RIA are the most common). A "dipstick" drug testing method which could provide screening test capabilities to field investigators has been developed at the University of Illinois.[34]		After a suspected positive sample is detected during screening, the sample is tested using a confirmation test. Samples that are negative on the screening test are discarded and reported as negative. The confirmation test in most laboratories (and all SAMHSA certified labs) is performed using mass spectrometry, and is precise but expensive. False positive samples from the screening test will almost always be negative on the confirmation test. Samples testing positive during both screening and confirmation tests are reported as positive to the entity that ordered the test. Most laboratories save positive samples for some period of months or years in the event of a disputed result or lawsuit. For workplace drug testing, a positive result is generally not confirmed without a review by a Medical Review Officer who will normally interview the subject of the drug test.		Urine drug test kits are available as on-site tests, or laboratory analysis. Urinalysis is the most common test type and used by federally mandated drug testing programs and is considered the Gold Standard of drug testing. Urine based tests have been upheld in most courts for more than 30 years. However, urinalysis conducted by the Department of Defense has been challenged for reliability of testing the metabolite of cocaine. There are two associated metabolites of cocaine, benzoylecgonine (BZ) and ecgonine methyl ester (EME), the first (BZ) is created by the presence of cocaine in an aqeous solution with a pH greater than 7.0, while the second (EME) results from the actual human metabolic process. The presence of EME confirms actual ingestion of cocaine by a human being, while the presence of BZ is indicative only. BZ without EME is evidence of sample contamination, however, the US Department of Defense has chosen not to test for EME in its urinalysis program.[35][relevant? – discuss]		A number of different analyses (defined as the unknown substance being tested for) are available on Urine Drug Screens.		Spray (sweat) drug test kits are non-invasive. It is a simple process to collect the required specimen, no bathroom is needed, no laboratory is required for analysis, and the tests themselves are difficult to manipulate and relatively tamper-resistant. The detection window is long and can detect recent drug use within several hours.		There are also some disadvantages to spray or sweat testing. There is not much variety in these drug tests, only a limited number of drugs can be detected, prices tend to be higher, and inconclusive results can be produced by variations in sweat production rates in donors. They also have a relatively long specimen collection period and are more vulnerable to contamination than other common forms of testing.[29]		Hair drug testing is a method that can detect drug use over a much longer period of time,[36] and is often used for highly safety-critical positions where there is zero tolerance of illegal drug use.[37] Standard hair follicle screen covers a period of 30 to 90 days. The growth of hair is usually at the rate of 0.5 inches per month. The hair sample is cut close to the scalp and 80 to 120 strands of hair are needed for the test. In the absence of hair on the head, body hair can be used as an acceptable substitute.[36] This includes facial hair, the underarms, arms, and legs or even pubic hair. Because body hair grows at a different rate than head hair, the timeframe changes, with scientists estimating that drug use can be detected in body hair for up to 12 months. Currently, most entities that use hair testing have prescribed consequences for individuals removing hair to avoid a hair drug test.		The claim that a hair test cannot be tampered with has been shown to be debatable. One study has shown that THC does not readily deposit inside epithelial cells so it is possible for cosmetic and other forms of adulteration to reduce the amount of testable cannabinoids within a hair sample.[25]		The results of federally mandating drug testing were similar to the effects of simply extending to the trucking industry the right to perform drug tests, and it has been argued that the latter approach would have been as effective at lower cost.[38]		Psychologist Tony Buon has criticized the use of workplace drug testing on a number of grounds, including:		Tony Buon has also reported by the CIPD as stating that "drug testing captures the stupid—experienced drug users know how to beat the tests".[42]		A study in 2004 by the Independent Inquiry into Drug Testing at Work found that attempts by employers to force employees to take drug tests could potentially be challenged as a violation of privacy under the Human Rights Act 1998 and Article 8 of the European Convention of Human Rights.[43] However, this does not apply to industries where drug testing is a matter of personal and public safety or security rather than productivity.		In consultation with Dr. Carlton Turner, President Ronald Reagan issued Executive Order 12564. In doing so, he instituted mandatory drug-testing for all safety-sensitive executive-level and civil-service Federal employees. This was challenged in the courts by the National Treasury Employees Union. In 1988, this challenge was considered by the US Supreme Court.[44] A similar challenge resulted in the Court extending the drug-free workplace concept to the private sector.[45] These decisions were then incorporated into the White House Drug Control Strategy directive issued by President George H.W. Bush in 1989.[46] All defendants serving on federal probation or federal supervised release are required to submit to at least three drug tests. Failing a drug test can be construed as possession of a controlled substance, resulting in mandatory revocation and imprisonment.[47]		There have been inconsistent evaluation results as to whether continued pretrial drug testing has beneficial effects.[48]		Testing positive can lead to bail not being granted, or if bail has already been granted, to bail revocation or other sanctions. Arizona also adopted a law in 1987 authorizing mandatory drug testing of felony arrestees for the purpose of informing the pretrial release decision, and the District of Columbia has had a similar law since the 1970s. It has been argued that one of the problems with such testing is that there is often not enough time between the arrest and the bail decision to confirm positive results using GC/MS technology. It has also been argued that such testing potentially implicates the Fifth Amendment privilege against self-incrimination, the right to due process (including the prohibition against gathering evidence in a manner that shocks the conscience or constitutes outrageous government conduct), and the prohibition against unreasonable searches and seizures contained in the Fourth Amendment.[49]		According to Henriksson, the anti-drug appeals of the Reagan administration "created an environment in which many employers felt compelled to implement drug testing programs because failure to do so might be perceived as condoning drug use. This fear was easily exploited by aggressive marketing and sales forces, who often overstated the value of testing and painted a bleak picture of the consequences of failing to use the drug testing product or service being offered."[50] On March 10, 1986, the Commission on Organized Crime asked all U.S. companies to test employees for drug use. By 1987, nearly 25% of the Fortune 500 companies used drug tests.[51]		According to an uncontrolled self-report study done by DATIA and Society for Human Resource Management in 2012 (sample of 6,000 randomly selected human resource professionals), human resource professionals reported the following results after implementing a drug testing program: 19% of companies reported a subjective increase in employee productivity, 16% reported a decrease in employee turnover (8% reported an increase), and unspecified percentages reported decreases in absenteeism and improvement of workers' compensation incidence rates.[52]		According to US Chamber of Commerce 70% of all illicit drug users are employed.[53] Some industries have high rates of employee drug use such as construction (12.8%), repair (11.1%), and hospitality (7.9-16.3%).[54]		A person conducting a business or undertaking (PCBU—the new term that includes employers) has duties under the work health and safety (WHS) legislation to ensure a worker affected by alcohol or other drugs does not place themselves or other persons at risk of injury while at work. Workplace policies and prevention programs can help change the norms and culture around substance abuse.		All organisations—large and small—can benefit from an agreed policy on alcohol and drug misuse that applies to all workers. Such a policy should form part of an organisations overall health and safety management system. PCBUs are encouraged to establish a policy and procedure, in consultation with workers, to constructively manage alcohol and other drug related hazards in their workplace. A comprehensive workplace alcohol and other drug policy should apply to everyone in the workplace and include prevention, education, counselling and rehabilitation arrangements. In addition, the roles and responsibilities of managers and supervisors should be clearly outlined.[20]		All Australian workplace drug testing must comply with Australian standard AS/NZS4308:2008.[citation needed]		In Victoria, roadside saliva tests detect drugs that contain:[55]		In February 2016 a New South Wales magistrate "acquitted a man who tested positive for cannabis". He had been arrested and charged after testing positive during a roadside drug test, despite not having smoked for nine days. He was relying on advice previously given to him by police.[56]		In the United States federal criminal system, refusing to take a drug test triggers an automatic revocation of probation or supervised release.[57][58]		In Victoria, Australia the driver of the car has the option to refuse the drug test. Refusing to undergo a drug test or refusing to undergo a secondary drug test after the first one, triggers an automatic suspension and disqualification for a period of 2 years and a fine of AUD$1000. The second refusal triggers an automatic suspension and disqualification for a period of 4 years and an even larger fine.		
Sick leave (or paid sick days or sick pay) is time off from work that workers can use to stay home to address their health and safety needs without losing pay. Paid sick leave is a statutory requirement in many nations. Most European, many Latin American, a few African and a few Asian countries have legal requirements for paid sick leave. Already in 1500 BCE, at least some of the workers who built the tombs of Egyptian pharaohs received paid sick leave as well as state-supported health care.[1]		In nations without laws mandating paid sick leave, some employers choose to offer it. Those that do offer sick leave do so as a matter of workplace policy or because it is in some or all of the employees' employment contracts or required by a collective bargaining agreement. Currently, five US states (California, Connecticut, Massachusetts, Oregon, Vermont); 23 US cities; and two US counties (Montgomery County, MD and San Francisco County, CA) have laws mandating paid sick leave. Currently, there are additional US city, county and state jurisdictions with proposed legislation to enact paid sick leave. Three cities, one county and one state have sick leaves that become effective July 1, 2017 [2].						Paid sick days (also referred to as sick leave or paid sick leave) guarantee workers paid time off to stay home when they are sick. Some policies also allow paid sick time to be used to care for sick family members, to attend routine doctor or medical appointments, or to address health and safety needs related to domestic violence or sexual assault.		At least 145 countries ensure access to paid sick days for short- or long-term illnesses, with 127 providing a week or more annually.[3]		An analysis from the Bureau of Labor Statistics (BLS) finds that around 39 percent of American workers in the private sector do not have paid sick leave.[4] Around 79 percent of workers in low-wage industries do not have paid sick time.[5] Most food service and hotel workers (78 percent) lack paid sick days.[6]		One survey reports that 77 percent of Americans believe that having paid sick days is "very important" for workers.[7] Some workers report that they or a family member have been fired or suspended for missing work due to illness.[8]		There is also the controversial issue of some employees taking a paid day off as a Mental health day.		Paid sick leave advocates assert that providing paid sick time can reduce turnover, increase productivity, and reduce the spread of contamination in the workplace.[9]		Some research has shown that parents who have access to paid sick leave are more likely to take time away from work to care for their sick kids,[10] and other research finds that most children recover faster from illness when cared for by their parents.[11] However, 53 percent of working mothers and 48 percent of working fathers don’t have paid sick days to care for children.[12] Without paid time off, workers may be forced to send sick children to school where they spread illness and experience negative short- and long-term health outcomes.[13] The Centers for Disease Control (CDC) asks workers to stay at home if they are sick and to keep sick kids out of school.[14] During the 2009 H1N1 crisis, the CDC recommended that anyone with flu-like symptoms remain at home.[15] According to a report from the Institute for Women's Policy Research, more than 8 million workers went to their jobs while sick between September and November 2009 during the H1N1 pandemic.[16]		Nearly seven in ten workers (68 percent) report they have gone to work with the stomach flu or other contagious disease.[8] Nearly half reported that they went to work sick because they could not afford to lose the pay.[17] Thirty percent of workers report they contracted the flu from a colleague.[18]		In addition to their colleagues, workers who choose to go to work sick may risk passing the illness to customers. The Food and Drug Administration guidelines recommend that workers with norovirus-related illnesses work on a restricted basis until 24 hours after symptoms subside.[19] Nearly half of outbreaks caused by the stomach flu are linked to ill food-service workers.[20] In 2008, health officials said a sick employee at a Chipotle restaurant in Kent, Ohio might have caused an outbreak resulting in over 500 people becoming violently ill.[21] The outbreak cost the Kent community between $130,233 and $305,337 in lost wages, lost productivity, and health care costs.[22]		Paid sick leave can also reduce the risk of occupational injuries, especially in high-risk industries such as construction, manufacturing, agriculture and health care. One study found that workers with access to paid sick leave were 28% less likely than those without to experience workplace injuries.[23]		Some studies show that the cost of losing an employee (which can include advertising for, interviewing, and training a replacement) is often greater than the cost of providing sick days to retain existing employees. One brief suggests the average cost of turnover is 25 percent of an employee's total annual compensation.[24]		Presenteeism costs the U.S. economy $180 billion annually in lost productivity. For employers, this costs an average of $255 per employee per year and exceeds the cost of absenteeism and medical and disability benefits.[25] For workers in the foodservice industry, one analysis found that foodborne illness outbreak for a chain restaurant – including negative public opinion, which affects other operations in a metropolitan area – can be up to $7 million.[26]		The United States Bureau of Labor Statistics has said that the average cost of sick leave per employee hour worked is 23 cents and the cost per service worker is 8 cents.[27] Additional research by advocates for a policy has suggested that paid sick days could lead to savings of $1.17 per worker per week for employers.[28]		Many studies have looked at the economic well-being of the city of San Francisco after they passed their sick leave policy in 2004.[29] The research finds that (including other external factors) San Francisco outperformed its neighboring counties in terms of job growth in the years after its sick leave program went into effect – even during the recent recession.[30]		Opponents of a workplace mandate assert that employers should offer paid sick days at their own discretion. They say employers best understand the benefit preferences of their employees and must maintain flexibility to meet the unique needs of their workforce.		Research out of Cleveland State University found that the costs incurred by a paid sick leave policy would include "lost wages for new users of paid sick leave policies and administration expenses incurred to operate sick leave accountability systems." The study concluded that a paid sick leave mandate would have been harmful to the state's employees and employers (noting that the costs of a mandate would outweigh the benefits) by imposing a net cost on the state and resulting in lost jobs.[31]		A survey of New York City employers in 2010 by the Partnership for New York City found that a paid sick leave mandate would cost city employers $789 million per year. The survey also found that small businesses and nonprofit organizations would be faced with almost 20 percent of the cost of the citywide mandate.[32]		A 2013 study by the Employment Policies Institute found that many businesses responded to a paid sick leave mandate in Connecticut by reducing paid leave, scaling back employee benefits, cutting back on hours, reducing wages, or raising prices.[33] About 24% of employers that responded to the survey said they'd hire fewer employees as a consequence of the law and 10% admitted that the law had caused them to limit or restrict their expansion within the state.[33]		The United States does not currently require that employees have access to paid sick days to address their own short-term illnesses or the short-term illness of a family member. The U.S. does guarantee unpaid leave for serious illnesses through the Family and Medical Leave Act (FMLA). This law requires employers with 50 workers working within a 75-mile radius to comply and, within those businesses, covers employees who have worked for their employer for at least 12 months prior to taking the leave.[34] In January 2015, President Barack Obama asked Congress to pass the Healthy Families act under which employees could earn one hour of paid sick time for every 30 hours they work up to seven days or 56 hours of paid sick leave annually. The bill as proposed, would apply to employers with 15 or more employees, for employees as defined in the Fair Labor Standards Act.[35]		Since 2006, an increasing number of cities and two states have implemented some form of paid sick leave:		In November 2006, the voters of San Francisco passed a ballot initiative making the city the first in the country to guarantee paid sick days to all workers. The measure received the support of 61 percent of voters.[36] Under San Francisco's law, workers earn one hour of paid sick time for every 30 hours worked. Workers in businesses with 10 or fewer employees earn up to five days per year, while workers at larger businesses earn nine days per year. Workers use paid sick time to recover from illness, attend doctor visits or care for a sick child, partner, or designated loved one.[37] Two studies demonstrate that employment rates in San Francisco have not suffered in the wake of the paid sick days law.[38] However, the research also found that over 28 percent of employees in the "bottom wage quartile" faced layoffs or total hours reduced as a result of the paid sick leave mandate.[39]		In March 2008, the Washington, D.C. Council voted unanimously to pass legislation guaranteeing workers paid sick time. Under the Accrued Sick and Safe Leave Act, workers in businesses with 100 or more workers earn up to seven days of paid sick leave each year, workers in businesses with 25-99 workers earn five days, and workers in businesses with 24 or fewer workers earn three days. This paid time off can be used to recover from illnesses, care for sick family members, seek routine or preventative medical care, or obtain assistance related to domestic violence or sexual assault. The law exempts tipped restaurant workers as well as workers in the first year of employment.[40] The D.C. law was also the first in the United States to include paid "safe" days for victims of domestic violence, sexual assault, or stalking.		In November 2008, paid sick days were put to a vote on the Milwaukee ballot, and voters passed the measure with 69 percent of the vote, enacting a law that guarantees paid sick and safe days for all workers in the city. The ordinance was immediately challenged by the Chamber of Commerce and has not been implemented. Recently, a Milwaukee County Circuit Court judge ruled the city ordinance invalid under a Wisconsin state law that bans local paid sick leave ordinances.[41]		On July 1, 2011, Connecticut Governor Dannel P. Malloy signed into law Public Act No. 11-52 which will make Connecticut the first state to mandate paid sick leave. The Act, which only narrowly passed through Connecticut’s Senate (18-17) and House of Representatives (76-65), took effect on January 1, 2012, and requires employers to allow their “service workers” to earn one hour of paid sick leave for every 40 hours worked, capped at a maximum of 40 hours per year. The Act applies to the “service workers” of employers with 50 or more employees in Connecticut during any single quarter in the previous year. It is estimated that 200,000 to 300,000 workers will be eligible for the paid leave. The Act defines “service worker” as an hourly or non-exempt employee who is primarily engaged in one of 68 occupations including: Food Service Manager, Medical and Health Services Manager, Librarian, Pharmacist, Physician Assistant, Registered Nurse, Home Health Aide, Nursing Aide, Orderly and Attendant, Security Guard, Cook, Food Preparation Worker, Bartender, Fast Food and Counter Worker, Waiter, Waitress, Dishwasher, Host, Hostess, Janitor, Usher, Lobby Attendant, Ticket Taker, Barber, Baggage Porter, Bellhop, Concierge, Cashier, Retail Salesperson, Courier, Secretary, Administrative Assistant, Computer Operator, Data Entry and Information Processing Worker, Bus Driver, Taxi Driver, and Chauffeur. Day laborers and temporary workers are not included within the definition of a “service worker.”[42]		New York City's paid sick leave law requires workers in the city earn paid sick time at the rate of one hour for every 30 hours worked. Workers at businesses with more than 15 employees are eligible to accrue up to 40 hours (or five days) of paid sick leave per year. Additionally at companies of any size, no one can be fired for taking an unpaid sick day. Workers may use their accrued time to care for their own illness, that of a family member or to seek preventive medical care for themselves or a family member.[43]		On September 8, 2014, California Governor Jerry Brown announced that he would sign a bill requiring employers to offer paid sick leave to those workers, who would accrue the time off at a rate of one hour per 30 hours worked. California would become the second state after Connecticut to require paid days off for ill employees.[44] The Healthy Workplaces, Healthy Families Act of 2014 was signed into law. Under the new law, California employees accrue sick pay at one hour for every 30 hours worked and may begin using accrued paid sick days on or after their 90th day of employment.[45]		On November 4, 2014, Massachusetts voters approved "Question 4", a ballot measure mandating sick pay for all part-time and full-time workers at firms with more than 11 employees. The law was passed 59-41 and comes into effect July 1, 2015. Sick leave may be used for personal physical or mental illness, the care of a sick child, spouse, parent, or parent of a spouse, routine health checks, or to address the physical, psychological, and legal effects of domestic abuse. Leave must accrue at least 1 hour per 30 hours worked. The maximum accrual can be limited to 40 hours, but hours must carry forward between calendar years. Employers may only require a medical note if a minimum of 24 consecutive work hours have been missed.[46]		On June 12, 2015 the Oregon legislature passed OL 537, 2015 mandating sick pay for all workers at businesses with at least 10 employees (or 6 or more for cities with more than 500,000 inhabitants—i.e. Portland, Oregon), effective January 1, 2016; this replaced existing sick pay laws passed in Portland and Eugene. Permitted uses for sick leave include the illness or injury of the employee or a family member, any reason for leave under Oregon’s Domestic Violence leave statute, and certain public health emergencies. Leave accrues at 1 hour per 30 hours worked, and a maximum of 80 hours may be accrued. Businesses with too few employees to qualify for paid sick leave must provide unpaid protected sick leave.[47]		Sick leave has its origins in trade union campaigns for its inclusion in industrial agreements. In Australia, it was introduced into "industrial awards" in 1922 [2]		Under the Federal Government's industrial relations legislation (Fair Work) eligible employees are entitled to 10 days of paid personal leave (sick/carer's leave) per year, which also carries over to subsequent years if not used.		In addition, Australian workers may be entitled to two days of compassionate leave for each permissible occasion where a member of their family or household contracts or develops a personal illness or sustains a personal injury that poses a threat to his or her life, or dies.		Sweden has paid sick leave.[48] The first sick day is usually not paid. After that day 80% of the income is paid for 364 days and 75% for a further maximum 550 days. A medical doctor must certify the illness no later than one week after the first sick day. A parent of a less than 12-year-old sick child can get paid leave to care for the child (termed "temporary parental leave"). In that case the first day is also paid. The state pays all these benefits, except for the first two weeks of sick leave for employees, which is paid by the employer.		According to Chinese Labor Law, the sick leave system is established for employees who are suffering from illness or non-work related injuries. During the medical treatment period, an employer cannot terminate the labor contract and have to pay for the sick-leave wage.[49] Generally, an employee is compensated at 60 to 100 percent of his regular wage during the sick-leave period, depending on the employee's seniority.[50] The minimum sick leave is 3 months long for employees with less than a 10-year cumulative work history, and less than 5 year’s seniority with their current employer. Sick leave for workers with 20 years of work history and 15 years with their current employer is an unlimited paid leave.[49]		In France paid sick leave is paid partly by social security (Sécurité sociale) and partly by the employer. It requires a medical justification no later than 48 hours after the first sick day. Social security only pays one part of the treatment, starting at the fourth day, and can make controls. The employer pays an additional part depending on collective agreement and legislation. Basic legislation requires that an employee working for more than one year, starting at eighth sick day social security and employer together provides 90% of salary for at least 30 days. Ratio and number of days are computed according to the number of years worked in the company.		Other legislation and agreement are applicable in other contexts such as sick child, pregnancy, paternity leave.		Since 2011, civil servants on sick leave were not paid during the first day of sick leave ("jour de carence"), but this is not the case anymore since 2014.[51]		In Germany, employers are legally required to provide at least six weeks of sick leave per illness at full salary if the employee can present a medical certificate of being ill (which is issued on a standard form).[52]		After these six weeks, an employee who is insured in the statutory health insurance (Gesetzliche Krankenversicherung) receives about 70% of their last salary, paid by the insurance. According to § 48 SGB V (social code 5) the health insurance pays for a maximum of 78 weeks in case of a specific illness within a period of 3 years. In case another illness appears during the time when the employee is already on sick leave then the new diagnosed illness will have no effect on the maximum duration of the payment. Only if the patient returns to work and falls sick again with a new diagnosis / illness, then the payment will be extended.		Fathers and mothers who are insured in the statutory health insurance and are raising a child younger than 12 years also have the right to paid leave if the child is sick (Kinderkrankengeld). The insurance pays for a maximum of 10 days per parent and per child (20 days for a single parent), limited to 25 days per year per parent (50 for a single parent).[53][54]		The regulations for patients with a private health insurance depend on the insurance contract.		At least 145 countries provide paid sick days for short- or long-term illnesses, with 127 providing a week or more annually. 98 countries guarantee one month or more of paid sick days.[55]		Many high-income economies require employers to provide paid sick days upwards of 10 days, including: the Netherlands, Switzerland, Sweden, Denmark, Finland, and Singapore.[56]		In August 2008, the National Opinion Research Center at the University of Chicago released their findings from a national public opinion poll on paid sick days.[57]		The Healthy Families Act (HR 2460 / S 1152) would establish a basic workplace mandate of paid sick days so workers can take paid sick days to care for their health or the health of their families.		The bill creates a minimum requirement that allows workers to earn up to seven days per year of paid leave to recover from illness, to care for a sick family member, or to seek preventative health care. It enables victims of domestic violence, stalking, and sexual assault to take paid time off to recover from incidents and seek assistance from the police or court. It also allows people to take time off to care for ill parents and elderly relatives, or to attend diagnostic or routine medical appointments. Employers with fewer than 15 workers would be exempt from the law.		The Healthy Families Act would allow an additional 30 million workers to have access to paid sick leave from their jobs, including 15 million low-wage workers and 13 million women workers. If the bill were to become law, 90 percent of all American workers would have access to paid sick days (up from 61 percent currently).[58]		A version of the bill was first introduced in 2004. Each session, it has gained support inside and outside of Congress.[citation needed] Congresswoman Rosa DeLauro and Senator Edward Kennedy reintroduced the Healthy Families Act in the 111th Congress in May 2009.[59] After Senator Kennedy’s death, Senator Chris Dodd became the lead Senate sponsor of the Healthy Families Act. The bill currently has 125 co-sponsors in the House and 24 in the Senate.		The Healthy Families Act was the subject of three hearings in the 111th United States Congress:		The Obama Administration has testified in support of the bill.[60] First Lady Michelle Obama has spoken out on numerous occasions about the need for a paid sick day mandate.[61]		The U.S. government guarantees federal employees 13 paid sick days a year.[62]		In recent years, advocates in states and cities across the country have created campaigns for paid sick days laws.		Alabama State Rep. Merika Coleman introduced paid sick and safe time legislation in February 2010. The paid sick time would have allowed employees to take time off for illness; to seek medical diagnosis or treatment; care for a sick family member; take time away when a school has been closed due to a public health emergency; or to seek services related to domestic violence, sexual assault, or stalking.[63] The legislation was not considered by the legislature and has since died.		A coalition led by Alaska PIRG advocated for a paid sick days mandate in Alaska in 2009 that would have provided one hour of paid sick time for every 40 hours worked by an employee. The legislation would apply to all businesses with 15 or more employees. The paid sick time could have been used to recover from illness, care for a sick family member, or seek domestic violence recovery services.[64]		Arizona State Rep. Kyrsten Sinema, with the support of several of her colleagues in the Arizona House of Representatives, introduced paid sick and safe time legislation in 2010. The paid sick days could have been used by employees to recover from an illness, seek medical diagnosis or treatment, care for a sick family member, in the event of a public health emergency, or to seek services related to domestic violence, sexual assault, or stalking.[65] The legislation was not considered by the legislature and has since died.		In 2006, San Francisco voters passed the first paid sick leave law in the country with 61 percent of the city vote. Young Workers United; other San Francisco community organizations led the grassroots campaign for the law.[66]		California's paid sick days campaign, coordinated by the California Work and Family Coalition and sponsored by the California Labor Federation, is working to bring paid sick days to all California workers. The California coalition includes advocates working on behalf of workers, women, children, people of color, and the state's public health interest.		Paid sick day legislation was last considered in California in May 2011. The legislation would have allowed workers to earn one hour of paid sick time for every 30 hours worked. Workers would have been able to use the paid sick days to recover from illness, to care for an ill family member, or to seek services related to sexual assault, or domestic violence. The bill would have guaranteed employees of businesses with 10 or more employees the right to use a minimum of nine paid sick days annually. Employees of smaller businesses would be guaranteed a minimum of five paid sick days annually.		The legislation failed in the legislature, mainly over concerns regarding how to pay for the mandate—which would have applied to state workers and government employees as well.[67]		On September 10, 2014, the Healthy Workplaces, Healthy Families Act of 2014 was signed by Governor Jerry Brown, which applies to employers regardless of size, with only a few enumerated categories of employees ineligible for leave. Authored by San Diego Assembly Lorena Gonzalez, the bill is expected to affect more than 6.5 million employees who have no paid sick days. That’s roughly 40 percent of the workforce in the state. Under the law, employees accrue sick pay at no less than one hour for every 30 hours worked and may begin using accrued paid sick days on their 90th day of employment. This new law will take into effect beginning on July 1, 2015.[68][69]		The Colorado Paid Sick Days Coalition is led by 9to5, National Association of Working Women, and includes many partner organizations working on behalf of workers, women, children, people of color, and the state's public health interest. Colorado's paid sick days bill in 2010 would have allowed employees to take time away from work to recover from illness, receive medical treatment, care for a sick family member, or seek services related to domestic violence.[70] Though the legislation was introduced in 2010, it was pulled almost immediately after. The bill sponsors acknowledged that the legislation needed more work before it could be considered.[71]		Connecticut's campaign is led by Working Families. Connecticut's bill was signed by Governor Dan Malloy on July 1, and enables workers to accrue one hour of paid sick time for every 40 hours worked, capped at 40 hours per year. In previous years, similar proposals had failed in the state legislature over concerns of how the mandate would negatively impact businesses in the state.[72] In 2011, the legislation was approved in the state Senate by one vote (18-17). Under the paid sick day mandate, Connecticut workers employed by businesses with 50 or more employees would be able to take a paid sick day to recover from illness, seek preventive care, care for a sick child, or seek assistance related to family violence, sexual assault, or violence.[73]		Legislation introduced in Hawaii in 2009 would have allowed workers to earn paid sick time to recover from their own illness, seek medical diagnosis or treatment, or to care for a sick family member. Employees would earn one hour of paid sick time for every 30 hours worked, up to a maximum of 40 hours for those employed by smaller businesses (defined as having fewer than 50 employees) or a maximum of 72 hours for those employed by larger businesses (defined as having 50 or more employees).[74] The legislation was not approved the legislature and has since died.		Paid sick days legislation in Illinois would allow workers to earn one hour of paid sick time per 30 hours worked, up to a maximum of seven days per year. Under the Healthy Workplace Act, paid sick days could be used to recover from an illness, care for a sick family member, or seek medical diagnosis or treatment.[75] In February 2015, Chicago voters overwhelmingly approved a non-binding sick leave referendum to require employers in the city to provide employees with paid leave in the event of a personal or family illness, an incident of domestic or sexual violence, or a school or building closure due to a public health emergency.[76]		Iowa's paid sick and safe days proposal, introduced by State Sen. Thomas Courtney (on behalf of the Senate Committee on Labor and Business Relations) in 2010, would have allowed workers to earn 5.54 hours of job-protected, paid sick time per 40 hours worked, up to a maximum of 18 days per year. Leave could be used to recover from illness, to seek diagnosis or treatment, to care for a sick family member, to seek services related to domestic assault, sexual abuse, or stalking, or in the event of a public health emergency.[77]		The Maine Women's Lobby partnered with the Maine Work and Family Coalition to introduced a paid sick days bill in Maine in 2010. The bill would have guaranteed workers at larger businesses up to about six paid sick days per year, while workers at smaller businesses could have earned approximately three paid sick days per year. The earned paid sick time could have been used for routine illness, to care for a family member during a public health emergency, to receive preventive care, or to be used in relation to domestic violence, sexual assault, or stalking.[78] The legislation was rejected by the state House in March 2010.[79]		The Massachusetts Paid Leave Coalition, directed by Greater Boston Legal Services in collaboration with the Massachusetts AFL-CIO and the Jewish Alliance for Law and Social Action are advocating for a paid sick days bill in the state legislature. The coalition includes advocates for workers, seniors, children, and people of color, and is supporting a bill that would provide all workers with one hour of paid sick time per 30 hours worked, up to a maximum of seven days per year. Workers could use the paid sick days to recover from illness, to care for a sick family member, or to seek assistance related to domestic violence.[80]		The Minnesota Healthy Families, Healthy Workplace Act from 2009 would have provided all workers with paid sick days to be used to recover from their own illness, to care for a sick family member, or for absence related to domestic violence. Workers would have earned one hour of paid sick time per 40 hours worked, capped at 52 hours (or 6.5 days) per year. Smaller businesses would have provided one hour of paid sick time for every 80 hours worked, capped at 26 hours (or 3.25 days) per year. Businesses with fewer than 15 employees would have been exempt from the law.[81] The proposal was not considered by the state legislature and has since died.		Montana State Rep. Mary Caferro introduced paid sick days legislation in 2009. The legislation was tabled in committee and was not considered further in the 2009 legislative session.[82]		New Hampshire's campaign, led by the New Hampshire Women's Lobby & Alliance, is partnering with a large coalition that includes women's rights and public health advocates to advance a paid sick days bill in the state's legislature. The legislation from 2009, which is now dead, would have provided up to five paid sick and safe days for New Hampshire workers, allowing them to take paid time to recover from illness, to care for a sick family member, or for absence necessary due to domestic violence, sexual assault, or stalking.[citation needed]		New Jersey's campaign for paid sick leave is being led by the New Jersey Time to Care Coalition and the New Jersey Working Families Alliance. On September 25, 2013, the Jersey City council passed an ordinance regarding sick leave, and it became the first city in New Jersey mandating that private employers provide paid sick leave to their employees.[83] Over the next year, Passaic, Newark, East Orange, and Paterson passed similar ordinances.[84] As of December 2014, Paid Sick Leave ordinances have been passed in New Jersey in Passaic, Newark, East Orange, Jersey City, Irvington, Paterson, Montclair, and Trenton.[85]		In February 2014, Assembly Bill 2354 was introduced in the New Jersey General Assembly by Pamela Lampitt and Raj Mukherji. Testimony was heard by the Assembly Labor Committee, and on October 27 the bill was released from the Labor Committee by a vote of 6-3. The bill is currently awaiting further action in the legislature.[86]		New York City's campaign, which is led by A Better Balance: The Work and Family Legal Center, the New York State Paid Family Leave Coalition, and the Working Families Party, is working with a broad coalition to raise awareness and advance the paid sick and safe days mandate. The coalition includes active participation by Make the Road New York and the Restaurant Opportunities Center of New York as well as involvement by public health leaders of the city and advocates working on behalf of workers, children, and women.		New York City's law requires workers in the city earn paid sick time at the rate of one hour for every 30 hours worked. Workers at businesses with more than 15 employees are eligible to accrue up to 40 hours (or five days) of paid sick leave per year. Additionally at companies of any size, no one can be fired for taking an unpaid sick day. Workers may use their accrued time to care for their own illness, that of a family member or to seek preventive medical care for themselves or a family member.[43] City Coucilmember Gale Brewer's legislation passed in the spring of 2013, requiring paid sick days for nearly 1 million New York City workers. Gale's bill was opposed by some business interests, Mayor Bloomberg, and the City Council Speaker Christine Quinn, but she was able to build overwhelming public support and convinced the Speaker to change her mind. The bill passed with a veto-proof majority.[87]		New York State Assemblyman Karim Camara and State Senator Kevin Parker introduced legislation in the New York legislature that would require employers to provide one hour of paid sick time for every 20 hours worked. Workers at larger businesses (defined as having 10 or more employees) would be able to earn up to 80 hours (or ten days) of paid sick time per year, and workers at smaller businesses could earn up to 40 hours (or five days). The time could be used to recover from illness or care for a sick family member.[88]		North Carolina's Paid Sick Days Campaign, led by the North Carolina Justice Center supported legislation that would have guaranteed one hour of paid sick time for every 30 hours worked by employees, up to an annual maximum of 56 hours (7 days) for larger businesses (defined as having 10 or more employees) or 32 hours (4 days) for smaller businesses (defined as having fewer than 10 employees). The paid sick days would have been used for an employee's own illness, to care for a sick family member, or to recover from incidents of domestic violence and stalking. Certain workers who are exempt from minimum wage laws would not be covered by the law.[89] The legislation was not considered by the legislature this year and is now dead.		Portland only: Effective January 1, 2014 all employers with one or more employee must comply with Portland's Sick Leave Law. Employees earn one hour of sick leave for every thirty hours after a minimum of 240 hours worked allowing them to earn a maximum of 40 hours of sick time per year. The leave may be unpaid for employers with less than 6 employees but must be paid for employers with 6 or more. Up to 40 unused hours may rollover into the upcoming year. The illness may be of the employee or a qualified family member including spouse, same-sex domestic partner, child, grandparent, grandchild, parent, or parent-in-law. Qualified illnesses need not be substantiated with a doctor's note and if required, all out-of-pocket expenses for the note must be paid by the employer. Rather than pay for the note, an employer may choose to allow employees to sign a statement acknowledging the time out was for a qualified reason. Adverse employment decisions may not be made based upon a request to use sick leave or actual use of the time.[90]		PathWays PA is working with a large coalition to support paid sick days for all Pennsylvania workers. The Healthy Families, Healthy Workplaces Act would allow workers to earn one hour of paid sick time for every 40 hours worked, capped at 52 hours (or 6.5 days) per year. Businesses with fewer than 10 employees would be required to offer workers one hour of paid sick time for every 80 hours worked, capped at 26 hours (or 3.25 days) per year. Workers may use paid sick days to recover from their own illness, to care for a sick family member, or to recover from or seek services related to incidents of domestic violence.[91]		The coalition advocating for paid sick days in the city of Philadelphia, led by PathWays PA, is supporting the Promoting Healthy Families and Workplaces ordinance. Under this measure, workers would be able to earn one hour of sick time for every 30 hours worked. Workers in larger businesses could earn up to 72 hours (or 9 days) of paid sick time, and workers in smaller businesses could earn up to 40 hours (or five days) of paid sick time. Workers would be able to use their earned sick days to recover from illness or to care for a sick family member.[92] Mayor Michael Nutter vetoed the legislation in June 2011.[93] The Mayor had long said that the issue was best addressed at the federal level, and that a city mandate would make Philadelphia less competitive with neighboring cities that did not impose a paid sick leave mandate.[94] Eventually, a limited version of the plan was approved. Effective May 13, 2015, the Promoting Healthy Families and Workplaces Ordinance requires employers with 10 or more employees in the City of Philadelphia to provide paid and unpaid sick leave to eligible employees. The Ordinance covers full- and part-time employees who work at least 40 hours per year within the City of Philadelphia. Eligible employees will accrue paid sick leave at the rate of one hour for every 40 hours worked, up to a maximum of 40 hours per year.[95]		Introduced by State Reps. Messier, Carnevale, Handy, Naughton, and Ferri, Rhode Island's proposed paid sick time legislation would allow workers to earn job-protected paid time off to be used to recover from an illness, seek medical diagnosis or treatment, care for a sick family member, take time away when a school has been closed due to a public health emergency, or seek services related to domestic violence, sexual assault, or stalking. The bill would allow workers to earn one hour of paid sick time for every 30 hours worked, up to seven days per year for larger businesses and up to four days per year for small businesses.[96]		The Coalition for a Healthy Tacoma, a group of labor, human services, faith, seniors', and women's organizations, is spearheading a new citywide initiative in the city of Tacoma, Washington.[citation needed] The Tacoma campaign is advocating for one hour of paid sick time for every 30 hours an employee works, up to 40 hours per year for workers in smaller businesses and 72 hours per year for workers in larger businesses. Washington already has a statewide law called the Family Care Act that requires businesses that offer paid sick time to permit a worker to use that time to care for an ill family member, so the Tacoma ordinance would only address paid sick time for workers themselves.[97]		Voices for Vermont's Children, the Vermont Livable Wage Campaign, and their coalition partners supported legislation in Vermont from 2009 that would have allowed employee to earn one hour of paid sick time for every 30 hours worked, up to a maximum of 56 hours (or seven days) each year. Workers would have been able to use their days to recover from their own illness, care for a sick family member, or seek preventive or routine health care. The legislation would have also created a safe day mandate that survivors of domestic or sexual assault could use for legal or health issues.[98] The legislation was not considered by the legislature and has since died.[99]		The Economic Opportunity Institute, at the helm of the Washington Family Leave Coalition, is building support for a paid sick day mandate in Washington State. Washington was among the first states in the nation to consider paid sick days legislation, and advocates have built a strong movement committed to improving standards to better meet the needs of both families and businesses.[100]		The Wisconsin Paid Sick Days Coalition, led by 9to5, National Association of Working Women, is working to build upon momentum created by the Milwaukee Paid Sick Days Campaign and to generate support for statewide legislation. Their proposal would provide access paid sick days for all Wisconsin workers, as well as access to paid safe days for victims of domestic violence, sexual assault, and stalking.[citation needed] On May 5, 2011 Governor Scott Walker signed legislation that prohibits local paid sick leave ordinances from being enacted in Wisconsin.[101]		
Self-employment is earning a person living through doing something by oneself. In case of business, self-employment is the process of earning living through using own capital or borrowed fund and also using one's own knowledge, intelligence, efficiency and taking a risk.		Generally, tax authorities will view a person as self-employed if the person chooses to be recognized as such, or is generating income such that the person is required to file a tax return under legislation in the relevant jurisdiction. In the real world, the critical issue for the taxing authorities is not that the person is trading but is whether the person is profitable and hence potentially taxable. In other words, the activity of trading is likely to be ignored if no profit is present, so occasional and hobby- or enthusiast-based economic activity is generally ignored by authorities.		Self-employed people generally find their own work rather than being provided with work by an employer, earning income from a trade or business that they operate.		In some countries governments (the United States and United Kingdom, for example) are placing more emphasis on clarifying whether an individual is self-employed or engaged in disguised employment, often described as the pretense of a contractual intra-business relationship to hide what is otherwise a simple employer-employee relationship.						Although the common perception is that self-employment is concentrated in a few service sector industries, like sales people and insurance agents, research by the Small Business Administration has shown that self-employment occurs across a wide segment of the U.S. economy.[1] Furthermore, industries that are not commonly associated as a natural fit for self-employment, such as manufacturing, have in fact been shown to have a large proportion of self-employed individuals and home-based businesses.[2]		In the United States, any person is considered self-employed for tax purposes if that person is running a business as a sole proprietorship, independent contractor, as a member of a partnership, or as a member of a limited liability company that does not elect to be treated as a corporation. In addition to income taxes, these individuals must pay Social Security and Medicare taxes in the form of a SECA (Self-Employment Contributions Act) tax.		There are many different ways in which one can be self-employed in the United States. Self-employment is a specific form of labor market activity with a particular tax classification spanning hundreds of different occupations and industries. So artists, musicians, accountants, doctors, mechanics, real estate agents, consultants, lawyers, IT software developers, etc. can all be classified as self-employed. Many self-employed individuals have employees who work for them as in the case of small business owners. One ways to differentiate self-employment is by industry-sector. So, one can be self-employed in manufacturing, retail trade, professional services, personal services, or finance. While all forms of self-employment offer independence and autonomy for self-employed individuals, the rewards and income generation vary dramatically by industry [3]		Self-employment is relatively common among new immigrants and ethnic minorities in the United States. In the United States, immigrants tend to have higher rates of self-employment than native-born Americans regardless of race or ethnicity. But, self-employment in the United States is unevenly distributed across racial/ethnic lines. Immigrants and their children who self-identify as White have the highest probability of self-employment in lucrative industries such as professional services and finance .In contrast, racial and ethnic minorities are less likely than native-born Whites to be self-employed, with the exception of Asian immigrants who have a high rates of self-employment in low prestige industries such as retail trade and personal services. Much like the regular labor market, self-employment in the United States is stratified across racial lines.[3] In general, self-employment is more common among immigrants than their second-generation children born in the United States. However, the second-generation children of Asian immigrants may continue to seek self-employment in a variety of industries and occupations [3]		The self-employment tax in the United States is typically set at 15.30%, which is roughly the equivalent of the combined contributions of the employee and employer under the FICA tax. The rate consists of two parts: 12.4% for social security and 2.9% for Medicare. The social security portion of the self-employment tax only applies to the first $110,100 of income for the 2012 tax year. There is no limit to the amount that is taxable under the 2.9% Medicare portion of the self-employment tax.		Generally, only 92.35% of the self-employment income is taxable at the above rates. Additionally, half of the self-employment tax, i.e., the employer-equivalent portion, is allowed as a deduction against income.		The 2010 Tax Relief Act reduced the self-employment tax by 2% for self-employment income earned in calendar year 2011,[4] for a total of 13.3%. This rate will continue for income earned in calendar year 2012, due to the Temporary Payroll Tax Cut Continuation Act of 2011.[5] Self-employed persons sometimes declare more deductions than an ordinary employee. Travel, uniforms, computer equipment, cell phones, etc., can be deducted as legitimate business expenses.		Self-employed persons report their business income or loss on Schedule C of IRS Form 1040 and calculate the self-employment tax on Schedule SE of IRS Form 1040. Estimated taxes must be paid quarterly using form 1040-ES if estimated tax liability exceeds $1,000.		Self-employed workers cannot contribute to a company-run 401k plan of the type with which most people are familiar. However, there are various vehicles available to self-employed individuals to save for retirement. Many set up a Simplified Employee Pension Plan (SEP) IRA, which allows them to contribute up to 25% of their income, up to $51,000 (2013) per year. There is also a vehicle called the Self-Employed 401k (or SE 401k) for self-employed people. The contribution limits vary slightly depending on how your business is organized but are generally higher than the other types of plans.		Research has shown that levels of self-employment in the United States are increasing, and that under certain circumstances this can have positive effects on per capita income and job creation. A 2011 study from the Federal Reserve Bank of Atlanta and Pennsylvania State University looked at U.S. self-employment levels from 1970 to 2000. According to data from the U.S. Bureau of Economic Analysis, the absolute number of people registered as non-farm proprietors (NFPs) or self-employed in metropolitan counties grew by 244% between 1969 and 2006, and by 93% in non-metropolitan counties. In relative terms, the share of self-employed within the labor force grew from 14% in 1969 to 21% in 2006 in metropolitan counties, and from 11% to 19% in non-metropolitan counties.[6][7]		In non-metropolitan counties, the study found that increased levels of self-employment were associated with strong increases in per capita income and job creation and significant reductions in family poverty levels. In 1969, the average income of non-farm proprietors was $6,758 compared to $6,507 earned by salaried employees; by 2006 the difference in earnings widened to $12,041 in favor of salaried employees. The study notes that the gap could be due to underreporting of income by the self-employed. Alternatively, low-productivity workers could be losing their jobs and are forced to be self-employed.[6][7] Further, some research shows that higher local unemployment rates lead workers to self-select into self-employment, as does past unemployment experience.[8]		A self-employed person in the United Kingdom can operate as a sole trader or as a partner in a partnership (including a limited liability partnership or "LLP") but not through an incorporated limited (or unlimited) liability company. It is also possible for someone to form a business that is run only part-time or concurrently while holding down a full-time job. This form of employment, while popular, does come with several legal responsibilities. When working from home, clearance may sometimes be required from the local authority to use part of the home as business premises. Should the self-employed person hold records of customers or suppliers in any electronic form[9] they will be required to register with the Information Commissioner's Office. Other legal responsibilities include statutory public liability insurance cover, modifying premises to be disabled-friendly, and the proper recording and accounting of financial transactions. Free advice on the range of responsibilities is available from government operated Business Link centres.		Many people living with disabilities choose to be self-employed.[10]		The European Commission defines a self-employed person as someone: “pursuing a gainful activity for their own account, under the conditions laid down by national law”. In the exercise of such an activity, the personal element is of special importance and such exercise always involves a large measure of independence in the accomplishment of the professional activities. This definition comes from Directive (2010/41/EU) on the application of the principle of equal treatment between men and women engaged in an activity in a self-employed capacity. This is in contrast to an employee, who is subordinate to and dependent on an employer.		In addition, Article 53 of the Treaty on the Functioning of the European Union (TFEU) provides for the free movement of those taking up and pursuing activities as self-employed people. It stipulates: “In order to make it easier for persons to take up and pursue activities as self-employed persons, the Council shall… issue Directives for the mutual recognition of diplomas, certificates and other evidence of formal qualifications”.		The self-employment form of work does not group homogenous workers. As indicated by the European Commission in 2010, there are “different understandings and definitions of the term self-employment across the countries, with a number of different subcategories defined: for instance, according to the legal status of the enterprise, whether the business has employees or not (employers versus own-account workers) and/or the sector in which the business operates. Some countries also make the distinction between self-employed status and the status of ‘dependent self-employed’ (e.g. Spain, Italy), where the self-employed person works for only one client. Others distinguish self-employment which is carried out in addition to paid employment (e.g. Belgium)”.		The European Parliament Resolution on Social Protection for All has stated that: “the absence of a clear national definition of self-employment increases the risk of false self-employment” and the European Parliament Resolution on the Renewed Social Agenda invites Member States to take initiatives that would “lead to a clear distinction between employers, genuine self-employed and small entrepreneurs on the one hand and employees on the other”.		Self-employment is mostly regulated at national level only. Each authority and individual body applies its own legal and regulatory framework provisions, which may vary depending on their remit or policy area (tax law, social security, business law, employment market, insurance). The provisions related to self-employment vary therefore widely between the countries. As indicated by Eurofound in 2014, the diversity of the self-employed has attracted diverse forms of regulation, mainly decided at national level: “EU employment law addresses the self-employed mainly in narrowly specific areas such as free movement and equal treatment”.		As recommended by the European Forum of Independent Professionals (EFIP), the EU, employers’, employees’ and self-employment representatives should adopt a Europe-wide joint recognition of genuine self-employment and a common definition that includes a shared terminology for the various sectors.		A list of public policy instruments and good practices to support self-employment in the member states is available here.		
Involuntary unemployment occurs when a person is willing to work at the prevailing wage yet is unemployed. Involuntary unemployment is distinguished from voluntary unemployment, where workers choose not to work because their reservation wage is higher than the prevailing wage. In an economy with involuntary unemployment there is a surplus of labor at the current real wage. Involuntary unemployment cannot be represented with a basic supply and demand model at a competitive equilibrium: All workers on the labor supply curve above the market wage would voluntarily choose not to work, and all those below the market wage would be employed. Given the basic supply and demand model, involuntarily unemployed workers lie somewhere off of the labor supply curve.[1] Economists have several theories explaining the possibility of involuntary unemployment including implicit contract theory, disequilibrium theory, staggered wage setting, and efficiency wages.[1]						Models based on implicit contract theory, like that of Azariadis (1975), are based on the hypothesis that labor contracts make it difficult for employers to cut wages. Employers often resort to layoffs rather than implement wage reductions. Azariadis showed that given risk-averse workers and risk-neutral employers, contracts with the possibility of layoff would be the optimal outcome.[2]		Efficiency wage models suggest that employers pay their workers above market clearing wages in order to enhance their productivity.[1] In efficiency wage models based on shirking, employers are worried that workers may shirk knowing that they can simply move to another job if they are caught. Employers make shirking costly by paying workers more than the wages they would receive elsewhere. This gives workers an incentive not to shirk.[1] When all firms behave this way, an equilibrium is reached where there are unemployed workers willing to work at prevailing wages.[3]		Following earlier disequilibrium research including that of Robert Barro and Herschel Grossman, work by Edmond Malinvaud clarified the distinction between classical unemployment, where real wages are too high for markets to clear, and Keynesian unemployment, involuntary unemployment due to inadequate aggregate demand.[1] In Malinvaud's model, classical unemployment is remedied by cutting the real wage while Keynesian unemployment requires an exogenous stimulus in demand.[5] Unlike implicit contrary theory and efficiency wages, this line of research does not rely on a higher than market-clearing wage level. This type of involuntary unemployment is consistent with Keynes's definition while efficiency wages and implicit contract theory do not fit well with Keynes's focus on demand deficiency.[6]		For many economists, involuntary unemployment is a real-world phenomenon of central importance to economics. Many economic theories have been motivated by the desire to understand and control involuntary unemployment.[7] However, acceptance of the concept of involuntary unemployment isn't universal among economists. Some do not accept it as a real or coherent aspect of economic theory.		Shapiro and Stiglitz, developers of an influential shirking model, stated "To us, involuntary unemployment is a real and important phenomenon with grave social consequences that needs to be explained and understood."[8]		Mancur Olson argued that real world events like the Great Depression could not be understood without the concept of involuntary unemployment. He argued against economists who denied involuntary unemployment and put their theories ahead of "common sense and the observations and experiences of literally hundreds of millions of people... that there is also involuntary unemployment and that it is by no means an isolated or rare phenomenon."[9]		Other economists do not believe that true involuntary unemployment exists[10] or question its relevance to economic theory. Robert Lucas claims "...there is an involuntary element in all unemployment in the sense that no one chooses bad luck over good; there is also a voluntary element in all unemployment, in the sense that, however miserable one's current work options, one can always choose to accept them"[11] and "the unemployed worker at any time can always find some job at once."[11] Lucas dismissed the need for theorists to explain involuntary unemployment since it is "not a fact or a phenomenon which it is the task of theorists to explain. It is, on the contrary, a theoretical construct which Keynes introduced in the hope it would be helpful in discovering a correct explanation for a genuine phenomenon: large-scale ﬂuctuations in measured, total unemployment."[12] Along those lines real business cycle and other models from Lucas's new classical school explain fluctuations in employment by shifts in labor supply driven by changes in workers' productivity and preferences for leisure.[1]		Involuntary unemployment is also conceptually problematic with search and matching theories of unemployment. In these models, unemployment is voluntary in the sense that a worker might choose to endure unemployment during a long search for a higher paying job than those immediately available; however, there is an involuntary element in the sense that a worker does not have control of the economic circumstances that force them to look for new work in the first place.[13]		
Downshifting is a social behavior or trend in which individuals live simpler lives to escape from what critics call the rat race of obsessive materialism and to reduce the “stress, overtime, and psychological expense that may accompany it”.[1] It emphasizes finding an improved balance between leisure and work[2] and focusing life goals on personal fulfilment and building relationships instead of the all-consuming pursuit of economic success. The term also refers to the act of reducing the gear of a motor vehicle while driving a manual transmission.		Downshifting, as a concept, shares many characteristics with simple living, but is distinguished, as an alternative form by its focus on moderate change and concentration on an individual comfort level, a gradual approach.[3] In the 1990s this new form of simple living began appearing in the mainstream media and has continually grown in popularity among populations living in industrial societies, especially the United States, the United Kingdom, New Zealand, and Australia.[4]						Slowing down the pace of life and spending time meaningfully while not spending money wastefully are principle values of downshifting. Another main tenet is enjoying leisure time in the company of others, especially loved ones, and shunning self-absorption because it resists the normality of individualism and isolation of post-modern society.		The primary motivations for downshifting are gaining leisure time, escaping from work-and-spend cycle, and removing the clutter of unnecessary possessions that are accrued while existing in those societies with the highest standards of living and levels of production. The personal goals of downshifting are simple: To reach a holistic self-understanding and satisfying meaning in life.[5]		Because of its personalized nature and emphasis on many minor changes rather than complete lifestyle overhaul, it attracts downshifters or participants across the socioeconomic spectrum.[6] An intrinsic consequence of downshifting is increased time for non-work-related activities which, combined with the diverse demographics of downshifters, cultivates higher levels of civic engagement and social interaction.[7]		The scope of participation is limitless because all members of society, adults, children, businesses, institutions, organizations, and governments are able to downshift.[3]		In practice, downshifting involves a variety of behavioral and lifestyle changes. The majority of these downshifts are voluntary choices, but natural, life course events, such as the loss of a job or birth of a child, can prompt involuntary downshifting. There is also a temporal dimension because a downshift could be either temporary or permanent.[8]		Because downshifting is fundamentally based on dissatisfaction with the conditions and consequences of the workplace environment,[9] the most common form of downshifting is work (or income) downshifting. The philosophy of work-to-live replaces the dominant social ideology of live-to-work. Reorienting economic priorities shifts the work–life balance away from the workplace.		Economically, work downshifts are defined in terms of reductions in either actual or potential income, work hours, and spending levels.[8] Following a path of earnings that is lower than the established market path is a downshift in potential earnings in favor of gaining other non-material benefits.		On an individual level, work downshifting is a voluntary reduction in annual income. Downshifters desire meaning in life outside of work and, therefore, will opt to decrease the amount of time spent at work or work hours. Reducing the number of hours of work, consequently, lowers the amount earned.[10] Simply not working overtime or taking a half-day a week for leisure time, are work downshifts.		Career downshifts are another way of downshifting economically and entail lowering previous aspirations of wealth, a promotion or higher social status.[11] Quitting a job to work locally in the community, from home or to start a business are examples of career downshifts. Although more radical, these changes do not mean stopping work altogether.		Many reasons are cited by workers for this choice and usually center on a personal cost–benefit analysis of current working situations and the quality extracurricular activities. High stress, pressure from employers to increase productivity, and long commutes are the costs of a job.[8] If the downshifter wants more non-material benefits like leisure time, a healthy family life, or personal freedom, then, switching jobs could be a desirable option.		Another aspect of downshifting is being a conscious consumer or actively practicing alternative forms of consumption. Proponents of downshifting point to consumerism as a primary source of stress and dissatisfaction because it creates a society of individualistic consumers who measure both social status and general happiness by an unattainable quantity of material possessions. Instead of buying goods for personal satisfaction, consumption downshifting, purchasing only the necessities, is a way to focus on quality of life rather than quantity.[7]		This realignment of spending priorities promotes the functional utility of goods over their ability to convey status which is evident in downshifters being, in general, less brand-conscious.[12] These consumption habits also facilitate the option of working and earning less because annual spending is proportionally lower. Reducing spending is less demanding than more extreme downshifts in other areas, like employment, as it requires only minor lifestyle changes.		Changes to public policy can make downshifting a more realistic option for a greater number of citizens. Universal healthcare and secure pension systems free people from the burden of accumulating wealth in order to build individual safety nets. Unions, business, and governments can implement more flexible working hours, part-time work, and other non-traditional work arrangements that enable people to work less, while still maintaining employment.[13]		The catch-phrase of International Downshifting Week is "Slow Down and Green Up".[3] Whether intentional or unintentional, generally, the choices and practices of downshifters nurture environmental health because they reject the fast-paced lifestyle fuelled by fossil fuels and adopt more sustainable lifestyles. The latent function of consumption downshifting is to reduce, to some degree, the carbon footprint of the individual downshifter. An example is to shift from a corporate to a small farming lifestyle.[14]		As a response the hectic pace of life and stresses in urban areas, downshifting geographically is a relocation to a smaller, rural, or more slow-paced community.[8] This is a more drastic change, but because the access to the internet is widespread and possible, downshifting geographically does not bring total removal from mainstream culture.		Although downshifting is primarily motivated by personal desire and not by a conscious political stance, it does define societal overconsumption as the source of much personal discontent. By redefining life satisfaction in non-material terms, downshifters assume an alternative lifestyle but continue to coexist in a society and political system preoccupied with the economy. In general, downshifters are politically apathetic because mainstream politicians mobilize voters through the hip-pocket nerve, proposing governmental solutions to periods of financial hardship and economic recessions. This economic rhetoric is meaningless to downshifters who have forgone worrying about money.[15]		Although consumers do hold the majority in the United States, the UK, and Australia, a significant minority, approximately 20 to 25 percent,[16] of these countries' citizens identify themselves in some respect as downshifters. Downshifting is not an isolated or unusual choice. Politics still centers around consumerism and unrestricted growth, but downshifting value, such as family priorities and workplace regulation, are appearing in political debates and campaigns.		Like downshifters, the Cultural Creatives is another social movement whose ideology and practices diverge from mainstream consumerism and according to Paul Ray, are followed by at least a quarter of U.S. citizens.[17]		In his book In Praise of Slowness, Carl Honoré relates followers of downshifting and simple living to the global Slow Movement.		The emergence of a large and diverse class of downshifters challenges the economically bias ideas for improving society.[18] Downshifting and similar, post-materialist ideologies are rising in popularity, but as a result of their grassroots nature, and relatively inconspicuous, non-confrontational subcultures, they represent unorganized social movements without political aspirations or motivating grievances.		
Graduate unemployment, or educated unemployment, is unemployment among people with an academic degree.						Research[1] undertaken proved that unemployment and underemployment of graduates are devastating phenomena in their lives. A high incidence of either are indicators of institutional ineffectiveness and inefficiency. Since the beginning of the economic recession in the US economy in 2007, an increasing number of graduates have been unable to find permanent positions in their chosen field. According to statistics, the unemployment rate for recent college graduates has been higher than all college graduates in the past decade, implying that it has been more difficult for graduates to find a job in recent years.[2][3] One year after graduation, the unemployment rate of 2007–2008 bachelor's degree recipients was 9%.[4] Underemployment among graduates is high. Educated unemployment or underemployment is due to a mismatch between the aspirations of graduates and employment opportunities available to them.		Aggravating factors for unemployment are the rapidly increasing quantity of international graduates competing for an inadequate number of suitable jobs, schools not keeping their curriculums relevant to the job market, the growing pressure on schools to increase access to education (which usually requires a reduction in educational quality), and students being constantly told that an academic degree is the only route to a secure future.[5]		College and Universities cost thousands of dollars a semester, not including study materials, books, room, and board. Tuition has gone up 1,120 percent in the last thirty years.[6] Students have been given the impression that employers are looking for people who, through tests and grades, have demonstrated that they are high achievers. In many recent surveys, that has been proved otherwise. Employers are looking for people who have learned how to learn, and have gained substantial communication skills as well as critical thinking abilities.[7] Graduates are not meeting employers needs. Students are also struggling to pay off their student loans. Without the desired, and needed jobs, graduates are accumulating debt and struggling to pay back their loans. 15 percent of the student borrowers default within the first three years of repayment.[8] Many resort to returning to live with their parents and having to work multiple part-time jobs. Loans average about twenty to thirty thousand dollars.[9] Higher education becomes an investment in which students are expecting to find a job with enough income to pay off the loans in a timely manner.		In June 2013, 11.8 million persons were unemployed, putting the unemployment rate at 7.6 percent. The state of the economy is a large contributor to these numbers. In June, 2001 the unemployment rate was 4.6% [10] After 9/11/2001, the unemployment rate skyrocketed to 5.7% in November 2001[11] and rose drastically in 2009 to 10% in October.[12] In September, 2015, unemployment is reported by the Labor Department to be at 5.1%.[12] However, some economists dispute that as accurate and claim that unemployment is much higher due to the number of people who have stopped looking for jobs.[13] The lack of jobs available, and skills desired by employers, are beginning to prove to be another major cause for graduate unemployment in the U.S. Graduates are completing school with a degree and a head full of knowledge, but still lack work experience to impress white-collar employers.[14]		Education Percentage High school graduate 86.68% Some college 55.60% Associates and/or bachelor's degree 38.54% Master's degree 7.62% Doctorate or professional degree 2.94%[15]		College majors ranked in ascending order by the percentage of college graduates with degrees in those fields who are also employed in jobs that do not require a college degree. Data is from the Federal Reserve Bank of New York, the United States Census Bureau, and the American Community Survey. Note: The unemployment and underemployment rates are for recent college graduates (between the ages 22 and 27).[16]		The markets for China's graduates share much in common with those of other countries. China's recent upsurge in graduate unemployment relates to a number of things. One important aspect is its education policy-making and economic development as well as reforms in the economy and in its higher education. Recently the annual growth in the numbers of new graduates was estimated at 7,270,000 for 2014. It has been stated that the rate of young unemployed graduates should logically bring about a withdrawal from higher education.[17] At 8% annual growth, the Chinese labor market will generate about eight million jobs, but these are mainly in manufacturing and require low-level qualifications.[18] This rising enrollment made employment an issue and a serious challenge for China. Including the graduates who are not employed last year, the number of unemployed graduates may reach 8,100,000. However, in the first half of 2014, there were 67,000 Chinese private businesses failing. These businesses employed 34.2 percent of the graduates in 2011.[19]		In 2013, it was estimated that at least 600,000 graduates from the prior year had yet to find employment. This is in addition to the approximately 7 million students who were leaving their universities with the desire to enter the workforce immediately.[20] In the study 2010 Chinese College Students Employment Report it named 15 professions that had the highest unemployment percentages in China. The survey said that, between 2007–2009, for three consecutive years, law majors had one of the highest unemployment rates for a bachelor's degree. Another field of high unemployment for the last three consecutive years was in the fields of computer science and technology. In many of China's universities, professions majoring in English have had a high level of unemployment[21] This tendency was still occurring during 2010 to 2013.		From 1900 to 1911, China abolished the civil service examination system and established a modern schooling system based on Western models.		Since 1978, the government has been reforming its economy from a Soviet-style centrally planned economy to a more market-oriented economy to increase productivity, living standards, and technological quality without exacerbating inflation, unemployment, and budget deficits.[27]		China’s economy regained momentum in the early 1990s. The Asian Financial Crisis of 1998–99 influenced the economy by slowing growth and as a consequence experts submitted proposals to state organs to stimulate economic recovery. This involved increasing student numbers and intensifying the modification of education as a way of stimulating internal consumption.		In 2008, the unemployment rate of graduates was more than 30%.[28] In this year the unemployment rate of graduates from top universities was 10%.[29]		In 2009, the employment rate of graduates who had bachelor's degree was in the 88% range.[30]		In 2010, the employment rate of college graduates rose 3.2% in 2009 reaching 91.2%.[31]		in 2012, Prime Minister Zhu warned that increased foreign competition brought by China's entry into the World Trade Organisation could lead to a doubling of the official urban unemployment rate over the next few years from 3.5% to 7%, or around 30 million people.[32]		In 2013, data released by the Chinese government indicated that the rate of graduate unemployment was 33.6%.[33]		In 2014, based on official Chinese date, roughly 15% of the new grads are unemployed six months after graduation. However, Cheng, a professor of political science states the authentic unemployment is actually 2.3 million which means the rate is around 30%.[34]		China's higher education system prior to the 1999 expansion was not prepared for large-scale growth as it was basically characterized as "education for examinations," and the reforms in the 1990s did not change this feature. The lack of diversity in curricula at different levels and in different divisions of higher education determined that graduates lacked the specialty and the flexibility to respond to market demand. Moreover, before the 1999 expansion, a national job market had not yet been established. With a focus on immediate economic growth, the policy makers appear to have made the 1999 expansion decision without a big picture of the future structure of China's market-oriented economy, and without knowing in which economic sectors manpower needs would increase.		China has had a long history of regional disparities, and disparities between urban and rural areas. Disparities in economic development are paralleled by disparities in higher education: top universities, for instance, are all located in those regions such as Shanghai, Beijing and Shenzhen. Such disparities in education are reflected in both quality and quantity.[1] In addition, Chinese Academy of Social Sciences Institute of Sociology and Social Sciences Documentation Publishing House jointly published the social blue book 2014 Chinese Social Situation Analysis and Prediction,[35] which released a set of employment survey reports based on 1678 graduates from 12 universities.[36] As is shown in the blue book, two months after graduation, the unemployment rate of undergraduates from rural families is higher than undergraduates from urban families, which is 30.5%.		The Chinese government has taken some measures to try to solve the crisis and it hopes injecting huge investments into the economy will create jobs and relieve much of the pressure. But some experts predict that building infrastructure will only provide manual jobs for ordinary workers and will not benefit college graduates.		Another measure is to boost postgraduate enrollments. The Ministry of Education of the People's Republic of China plans to expand enrollments of masters students by 5% and doctoral students by 1.7%. Given the decline in jobs, many graduates will choose to study further and this year almost 1.25 million first degree-holders will be taking the postgraduate entrance examinations.		Yet expanding postgraduate enrollments cannot solve the problem of graduate unemployment as it can only offer some relief or postpone the current employment pressure. Indeed, in recent years, employment of master's degree graduates has become problematic. Diverting graduates to rural areas is a third measure. But a vast gap exists between urban and rural areas in terms of developmental level, opportunities, and living conditions. Thus, most graduates prefer to work in cities.		To encourage them to go to the countryside, the government has come up with policies such as preferential treatment when graduates (after two years service) apply to become government officials, or extra points are added to their scores in examinations for graduate study. But these policies are not attractive given the low salaries graduates earn in country areas.		The Ministry of Education of the People's Republic of China has recently been calling for the whole society, including overseas Chinese, to contribute ideas to improve education overall. Promoting creative and vocational education has been raised as a way of providing new graduates with creative education and job skills to meet the needs of the market and to face the challenges of a changing world in the decades to come.		Perhaps this approach constitutes a more fundamental strategy that will eventually solve the graduate employment problem, although the impact is likely to take many years to become apparent.[28] China's education department has already stated clearly that it wants to turn 600 universities into polytechnics, in order to give students more technical and employment-related curriculum, instead of only providing academic and theoretical subjects.[34]		The employment situation for new college graduates is different from the working population in general. The graduate unemployment crisis in China represents a wasteful investment of scarce resources. Large sums of money have consequently been invested in educating unemployed graduates which could otherwise have been invested in job-creating productive programmes. With a flood of new graduates, individuals are having a tough time finding jobs in an increasingly competitive labor market. Furthermore， it produces permanent scars on youth. Farlie and Kletzer (1999) estimated that being unemployed while young results in lower future earnings by a magnitude of 8.4% and 13.0% for males and females, respectively. Meanwhile, graduates have some negative expectations under the pressure of seeking jobs. Nanjing Normal University has surveyed students who expected to graduate in 2006 about "College Student's Attitudes about Job Seeking and Career". 44.21% prefer to get an employment contract first, then consider pursuing a new job position which is what they really desire to be employed for an average of 2 years.[37] This phenomenon not only causes underemployment and high turnover in the job market, but also, graduates will have lower levels of job satisfaction, work commitment, job involvement and internal work motivation. Obviously, these of problems will bring more risks for employers as well.		Another widespread criticism is that, since the acceleration of enrollment starting from 1999, many schools, which were originally vocational ones, have been turned into universities. This has resulted in the number of university increasing greatly, which also means an increase in graduates with university degrees. But the truth is that the quality of these students' education is often even lower than vocational school graduates. The reason is that vocational school graduates have technical abilities which university graduates often lack. It's quite common that universities often put more emphasis on academic research rather than teaching practical skills required for jobs, which employers often value. What is more, some employers only pay attention to graduates from prestigious universities, which result in the decrease of competences of normal college graduates. In order to solve this, it is said that the Chinese government is considering restoring these so-called Sanben universities to what they originally were.		Graduate unemployment will be more likely to promote postgraduate school education. Half of graduates would like to consider attending postgraduate schools to enhance their ability to seek expert jobs. Government interventions are designed to alleviate graduate unemployment by encouraging young job seekers to "Go west, go down to where motherland and people are in greatest need."[38]		The China Youth Daily has reported that some graduates have worked for years in villages of Hainan, China’s most southerly province. In 2003, the Communist Youth League recruited over 50,000 graduates to provide volunteer service in education, health care, agriculture, and cultural development in western provinces. As well as receiving a stipend, a State Council circular issued in 2005 promised the graduate volunteers preferential policies in civil service tests and graduate school entrance exams. Moreover, graduates had an opportunity to be self-employed as the Chinese government launched policies which were formulated to encourage college graduates to carve out their own future.[39] However, many college graduates remain underemployed or unemployed even after completing their advanced degree.		According to a survey of more than 30,000 graduates from 10 European countries about 3–4 years after graduation, only a minority of 10–20% of graduates face substantial problems in the labor market or end up in positions not commensurate with their level of education. There is a clear North-South differential in Europe with respect to transition and objective employment measures, while the pattern is more differentiated with respect to the perceived utilisation of knowledge, the self-rated adequacy of position and the job satisfaction.[40]		Among OECD nations in 2013, the worst unemployment rates for graduates were in by Greece, Spain and Portugal.[41]		30. International Labour Office (2012)"Long-term consequences of the youth jobs crisis",Global Employment Trends for Youth 2012， pp. 19–20.		
Shift work is an employment practice designed to make use of, or provide service across, all 24 hours of the clock each day of the week (often abbreviated as 24/7). The practice typically sees the day divided into shifts, set periods of time during which different groups of workers perform their duties. The term "shift work" includes both long-term night shifts and work schedules in which employees change or rotate shifts.[1][2][3]		In medicine and epidemiology, shift work is considered a risk factor for some health problems in some individuals, as disruption to circadian rhythms may increase the probability of developing cardiovascular disease, cognitive impairment, diabetes, and obesity, among other conditions.[4][5]		Shift work can also contribute to strain in marital, family, and personal relationships.[6]						Shift work increases the risk for the development of many disorders. Shift work sleep disorder is a circadian rhythm sleep disorder characterized by insomnia, excessive sleepiness, or both. Shift work is considered essential for the diagnosis.[7] The risk of diabetes mellitus type 2 is increased in shift workers, especially men. People working rotating shifts are more vulnerable than others.[8]		Women whose work involves night shifts have a 48% increased risk of developing breast cancer.[6][9] This may be due to alterations in circadian rhythm: melatonin, a known tumor suppressor, is generally produced at night and late shifts may disrupt its production.[9] The WHO's International Agency for Research on Cancer listed "shiftwork that involves circadian disruption" as a probable carcinogen.[10][11] Shift work may also increase the risk of other types of cancer.[12]		Shift work also increases the risk of developing cluster headaches,[13] heart attacks,[14] fatigue, stress, sexual dysfunction,[15] depression,[16] dementia, obesity,[7] metabolic disorders, gastrointestinal disorders, musculoskeletal disorders, and reproductive disorders.[6]		Shift work also can worsen chronic diseases, including sleep disorders, digestive diseases, heart disease, hypertension, epilepsy, mental disorders, substance abuse, asthma, and any health conditions that are treated with medications affected by the circadian cycle.[6] Artificial lighting may additionally contribute to disturbed homeostasis.[17] Shift work may also increase a person's risk of smoking.[6]		The health consequences of shift work may depend on chronotype, that is, being a day person or a night person, and what shift a worker is assigned to.		Different shift schedules will have different impacts on the health of a shift worker. The way the shift pattern is designed affects how shift workers sleep, eat and take holidays. Some shift patterns can exacerbate fatigue by limiting rest, increasing stress, overworking staff or disrupting their time off.[18]		Compared with the day shift, injuries and accidents have been estimated to increase by 15% on evening shifts and 28% on night shifts. Longer shifts are also associated with more injuries and accidents: 10-hour shifts had 13% more and 12-hour shifts had 28% more than 8-hour shifts.[6] Other studies have showed a link between fatigue and workplace injuries and accidents. Workers with sleep deprivation are far more likely to be injured or involved in an accident.[7]		One study suggests that, for those working a night shift (such as 23:00 to 07:00), it may be advantageous to sleep in the evening (14:00 to 22:00) rather than the morning (08:00 to 16:00). The study's evening sleep subjects had 37% fewer episodes of attentional impairment than the morning sleepers.[19]		There are four major determinants of cognitive performance and alertness in healthy shift-workers. They are: circadian phase, sleep inertia, acute sleep deprivation and chronic sleep deficit.[20] The circadian phase is relatively fixed in humans; attempting to shift it so that an individual is alert during the circadian bathyphase is difficult. Sleep during the day is shorter and less consolidated than night-time sleep.[7] Before a night shift, workers generally sleep less than before a day shift.[16]		The effects of sleep inertia wear off after 2–4 hours of wakefulness,[20] such that most workers who wake up in the morning and go to work suffer some degree of sleep inertia at the beginning of their shift. The relative effects of sleep inertia vs. the other factors are hard to quantify; however, the benefits of napping appear to outweigh the cost associated with sleep inertia.		Acute sleep deprivation occurs during long shifts with no breaks, as well as during night shifts when the worker sleeps in the morning and is awake during the afternoon, prior to the work shift. A night shift worker with poor daytime sleep may be awake for more than 18 hours by the end of his shift. The effects of acute sleep deprivation can be compared to impairment due to alcohol intoxication,[7] with 19 hours of wakefulness corresponding to a BAC of 0.05%, and 24 hours of wakefulness corresponding to a BAC of 0.10%.[6][21] Much of the effect of acute sleep deprivation can be countered by napping, with longer naps giving more benefit than shorter naps.[22] Some industries, specifically the Fire Service, have traditionally allowed workers to sleep while on duty, between calls for service. In one study of EMS providers, 24-hour shifts were not associated with a higher frequency of negative safety outcomes when compared to shorter shifts.[23]		Chronic sleep deficit occurs when a worker sleeps for fewer hours than is necessary over multiple days or weeks. The loss of two hours of nightly sleep for a week causes an impairment similar to those seen after 24 hours of wakefulness. After two weeks of such deficit, the lapses in performance are similar to those seen after 48 hours of continual wakefulness.[24] The number of shifts worked in a month by EMS providers was positively correlated with the frequency of reported errors and adverse events.[23]		Shift work has been shown to negatively affect workers, and has been classified as a specific disorder (shift work sleep disorder). Circadian disruption by working at night causes symptoms like excessive sleepiness at work and sleep disturbances. Shift work sleep disorder also creates a greater risk for human error at work.[25] Shift work disrupts cognitive ability and flexibility and impairs attention, motivation, decision making, speech, vigilance, and overall performance.[7]		In order to mitigate the negative effects of shift work on safety and health, many countries have enacted regulations on shift work. The European Union, in its directive 2003/88/EC, has established a 48-hour limit on working time (including overtime) per week; a minimum rest period of 11 consecutive hours per 24-hour period; and a minimum uninterrupted rest period of 24 hours of mandated rest per week (which is in addition to the 11 hours of daily rest).[25][26] The EU directive also limits night work involving "special hazards or heavy physical or mental strain" to an average of eight hours in any 24-hour period.[25][26] The EU directive allows for limited derogations from the regulation, and special provisions allow longer working hours for transportation and offshore workers, fishing vessel workers, and doctors in training (see also medical resident work hours).[26]		Fatigue due to shift work has contributed to several industrial disasters, including the Three Mile Island accident, the Space Shuttle Challenger disaster and the Chernobyl disaster.[7] The Alaska Oil Spill Commission's final report on the Exxon Valdez oil spill disaster found that it was "conceivable" that excessive work hours contributed to crew fatigue, which in turn contributed to the vessel's running aground.[27]		The practices and policies put in place by managers of round-the-clock or 24/7 operations can significantly influence shift worker alertness (and hence safety) and performance.[28]		Air traffic controllers typically work an 8-hour day, 5 days per week. Research has shown that when controllers remain "in position" for more than two hours, even at low traffic levels, performance can deteriorate rapidly, so they are typically placed "in position" for 30-minute intervals (with 30 minutes between intervals).		These practices and policies can include selecting an appropriate shift schedule or rota and using an employee scheduling software to maintain it, setting the length of shifts, managing overtime, increasing lighting levels, providing shift worker lifestyle training, retirement compensation based on salary in the last few years of employment (which can encourage excessive overtime among older workers who may be less able to obtain adequate sleep), or screening and hiring of new shift workers that assesses adaptability to a shift work schedule.[29] Mandating a minimum of 10 hours between shifts is an effective strategy to encourage adequate sleep for workers. Allowing frequent breaks and scheduling 8 or 10-hour shifts instead of 12 hour shifts can also minimize fatigue and help to mitigate the negative health effects of shift work.[6]		Shift work was once characteristic primarily of the manufacturing industry, where it has a clear effect of increasing the use that can be made of capital equipment and allows for up to three times the production compared to just a day shift. It contrasts with the use of overtime to increase production at the margin. Both approaches incur higher wage costs. Although 2nd-shift worker efficiency levels are typically 3–5% below 1st shift, and 3rd shift 4–6% below 2nd shift, the productivity level, i.e. cost per employee, is often 25% to 40% lower on 2nd and 3rd shifts due to fixed costs which are "paid" by the first shift.[30]		The 42-hour work-week allows for the most even distribution of work time. A 3:1 ratio of work days to days off is most effective for eight-hour shifts, and a 2:2 ratio of work days to days off is most effective for twelve-hour shifts.[28][31][32] Eight-hour shifts and twelve-hour shifts are common in manufacturing and health care. Twelve-hour shifts are also used with a very slow rotation in the petroleum industry. Twenty-four-hour shifts are common in health care and emergency services.[16]		The shift plan or rota is the central component of a shift schedule.[28] The schedule includes considerations of shift overlap, shift change times and alignment with the clock, vacation, training, shift differentials, holidays, etc., whereas the shift plan determines the sequence of work and free days within a shift system.		Rotation of shifts can be fast, in which a worker changes shifts more than once a week, or slow, in which a worker changes shifts less than once a week. Rotation can also be forward, when a subsequent shift starts later, or backward, when a subsequent shift starts earlier.[16]		Though shift work itself remains necessary in many occupations, employers can alleviate some of the negative health consequences of shift work. The United States National Institute for Occupational Safety and Health recommends employers avoid quick shift changes and any rotating shift schedules should rotate forward. Employers should also attempt to minimize the number of consecutive night shifts, long work shifts and overtime work. A poor work environment can exacerbate the strain of shift work. Adequate lighting, clean air, proper heat and air conditioning, and reduced noise can all make shift work more bearable for workers.[33]		Good sleep hygiene is recommended.[6] This includes blocking out noise and light during sleep, maintaining a regular, predictable sleep routine, avoiding heavy foods and alcohol before sleep, and sleeping in a comfortable, cool environment. Alcohol consumption, caffeine consumption and heavy meals in the few hours before sleep can worsen shift work sleep disorders.[6][7] Exercise in the three hours before sleep can make it difficult to fall asleep.[6]		Free online training programs are available to educate workers and managers about the risks associated with shift work and strategies they can use to prevent these.[34]		Algorithmic scheduling of shift work can lead to what has been colloquially termed as "clopening"[35] where the shift-worker has to work the closing shift of one day and the opening shift of the next day back-to-back resulting in short rest periods between shifts and fatigue. Co-opting employees to fill the shift roster helps to ensure that the human costs[36] are taken into account in a way which is hard for an algorithm to do as it would involve knowing the constraints and considerations of each individual shift worker and assigning a cost metric to each of those factors.[37] Shift based hiring which is a recruitment concept that hires people for individual shifts, rather than hiring employees before scheduling them into shifts enables shift workers to indicate their preferences and availabilities for unfilled shifts through a shift-bidding mechanism. Through this process, the shift hours are evened out by human-driven market mechanism rather than an algorithmic process. This openness can lead to work hours that are tailored to an individual's lifestyle and schedule while ensuring that shifts are optimally filled, in contrast to the generally poor human outcomes of fatigue, stress, estrangement with friends and family and health problems that have been reported with algorithm-based scheduling of work-shifts.[38][39]		Melatonin may increase sleep length during both daytime and nighttime sleep in people who work night shifts. Zopiclone has also been investigated as a potential treatment, but It is unclear if it is effective in increasing daytime sleep time in shift workers. There are however no reports of side effects.[25]		Modafinil and R-modafinil are useful to improve alertness and reduce sleepiness in shift workers.[25][40] Modafinil has a low risk of abuse compared to other similar agents.[41] However, 10% more participants reported side effects, nausea and headache, while taking Modafinil. In post-marketing surveillance Modafinil was associated with the severe Steven Johnson syndrome. The European Medicines Agency withdrew the license for Modafinil for shift workers for the European market because it judged that the benefits did not outweigh the side effects.[25]		Using caffeine and naps before night shifts can decrease sleepiness. Caffeine has also been shown to reduce errors made by shift workers.[25]		According to data from the National Health Interview Survey Occupational Health Survey, 29% of all U.S. workers in 2010 worked an alternative shift (not a regular day shift).[42] 15% of workers worked the night shift regularly.[25] Prevalence rates were higher for workers aged 18–29 compared to other ages. Those with a Bachelor’s degree and higher had a lower prevalence rate of alternative shifts compared to workers with less education. Among all occupations, food preparation and serving occupations had the highest prevalence of working an alternative shift (63%).[42]		One of the ways in which working alternative shifts can impair health is through decreasing sleep opportunities. Among all workers, those who usually worked the night shift had a much higher prevalence of short sleep duration (44.0%, representing approximately 2.2 million night shift workers) than those who worked the day shift (28.8%, representing approximately 28.3 million day shift workers). An especially high prevalence of short sleep duration was reported by night shift workers in the transportation and warehousing (69.7%) and health-care and social assistance (52.3%) industries.[43]		It is estimated that 15-20% of workers in industrialized countries are employed in shift work.[7] Shift work is common in the transportation sector as well. Some of the earliest instances appeared with the railroads, where freight trains have clear tracks to run on at night. Shift work has also been traditional in law enforcement and the armed forces. Military personnel, pilots, and others that regularly change time zones while performing shift work experience jet lag and consequently suffer sleep disorders.[7]		Service industries now increasingly operate on some shift system; for example a restaurant or convenience store will normally each day be open for much longer than a working day. Shift work is also the norm in governmental and private employment in fields related to public safety and healthcare, such as Emergency Medical Services, police, firefighting, security and hospitals. Shift work is a contributing factor in many cases of medical errors.[7]		Those in the field of meteorology, such as the National Weather Service and private forecasting companies, also utilize shift work, as constant monitoring of the weather is necessary.		Much of the Internet services and telecommunication industry relies on shift work to maintain worldwide operations and uptime.		There are many industries requiring 24/7 coverage that employ workers on a shift basis, including:		
A curriculum vitae (English: /kəˈrɪkjʊləm ˈviːtaɪ, -ˈwiːtaɪ, -ˈvaɪtiː/)[1][2] (often shortened CV or vita) is a written overview of a person's experience and other qualifications for a job opportunity. In some countries, a CV is typically the first item that a potential employer encounters regarding the job seeker and is typically used to screen applicants, often followed by an interview. CVs may also be requested for applicants to postsecondary programs, scholarships, grants and bursaries. In the 2010s, some applicants provide an electronic text of their CV to employers using email, an online employment website or using a job-oriented social networking service' website, such as LinkedIn.						In the United Kingdom, most Commonwealth countries, and Ireland, a CV is short (usually a maximum of two sides of A4 paper), and therefore contains only a summary of the job seeker's employment history, qualifications, education, and some personal information. It is akin to a resume in North America. Some parts of Asia require applicants' photos, date of birth, and most recent salary information. CVs are often tailored to change the emphasis of the information according to the particular position for which the job seeker is applying. A CV can also be extended to include an extra page for the jobseeker's publications if these are important for the job.		In the United States, Canada, Australia, Germany, and India, a CV is a comprehensive document used in academic circles and medical careers that elaborate on education, publications, and other achievements. A CV contains greater detail than a résumé, a shorter summary which is more often used in applications for jobs, but it is often expected that professionals use a short CV that highlights the current focus of their academic lives and not necessarily their full history. A CV is generally used when applying for a position in academia, while a resume is generally used when applying for a position in industry, non-profit, and the public sector.[3]		Curriculum vitae is a Latin expression which can be loosely translated as [the] course of [my] life. In current usage, curriculum is less marked as a foreign loanword. Traditionally the word vitae is rendered in English using the ligature æ, hence vitæ,[4] although this convention is less common in contemporary practice.		The plural of curriculum vitae, in Latin, is formed following Latin rules of grammar as curricula vitae, and is used along with curricula vitarum,[5] both of which are debated as being more grammatically correct than the other.		In English, the plural of the full expression curriculum vitae is seldom used; the plural of curriculum on its own is usually written as "curricula",[6] rather than the traditional curriculums.[7]		
Sexual harassment is bullying or coercion of a sexual nature, or the unwelcome or inappropriate promise of rewards in exchange for sexual favors.[1] In most modern legal contexts, sexual harassment is illegal. As defined by the United States' Equal Employment Opportunity Commission (EEOC), "It is unlawful to harass a person (an applicant or employee) because of that person's sex." Harassment can include "sexual harassment" or unwelcome sexual advances, requests for sexual favors, and other verbal or physical harassment of a sexual nature. The legal definition of sexual harassment varies by jurisdiction. Sexual harassment is subject to a directive in the European Union.[2]		Although laws surrounding sexual harassment exist, they generally do not prohibit simple teasing, offhand comments, or minor isolated incidents — that is, they do not impose a "general civility code".[3] In the workplace, harassment may be considered illegal when it is so frequent or severe that it creates a hostile or offensive work environment or when it results in an adverse employment decision (such as the victim being fired or demoted, or when the victim decides to quit the job). The legal and social understanding of sexual harassment, however, varies by culture.		In the context of US employment, the harasser can be the victim's supervisor, a supervisor in another area, a co-worker, or someone who is not an employee of the employer, such as a client or customer, and harassers or victims can be of any gender.[4]		It includes a range of actions from mild transgressions to sexual abuse or sexual assault.[5] Sexual harassment is a form of illegal employment discrimination in many countries, and is a form of abuse (sexual and psychological) and bullying. For many businesses or organizations, preventing sexual harassment, and defending employees from sexual harassment charges, have become key goals of legal decision-making.		The concept of sexual harassment, in its modern understanding, is a relatively new one, dating from the 1970s onwards; although other related concepts have existed prior to this in many cultures. The term sexual harassment was used in 1973 in "Saturn's Rings", a report authored by Mary Rowe to the then-President and Chancellor of the Massachusetts Institute of Technology (MIT) about various forms of gender issues.[6] Rowe has stated that she believes she was not the first to use the term, since sexual harassment was being discussed in women's groups in Massachusetts in the early 1970s, but that MIT may have been the first or one of the first large organizations to discuss the topic (in the MIT Academic Council), and to develop relevant policies and procedures. MIT at the time also recognized the injuries caused by racial harassment and the harassment of women of color, which may be both racial and sexual. The President of MIT also stated that harassment (and favoritism) are antithetical to the mission of a university as well as intolerable for individuals.		In the book In Our Time: Memoir of a Revolution (1999), journalist Susan Brownmiller quotes the Cornell activists who in 1975 thought they had coined the term sexual harassment: "Eight of us were sitting in an office ... brainstorming about what we were going to write on posters for our speak-out. We were referring to it as 'sexual intimidation,' 'sexual coercion,' 'sexual exploitation on the job.' None of those names seemed quite right. We wanted something that embraced a whole range of subtle and un-subtle persistent behaviors. Somebody came up with 'harassment.' 'Sexual harassment!' Instantly we agreed. That's what it was."[7]		These activists, Lin Farley, Susan Meyer, and Karen Sauvigne went on to form Working Women's Institute which, along with the Alliance Against Sexual Coercion, founded in 1976 by Freada Klein, Lynn Wehrli, and Elizabeth Cohn-Stuntz, were among the pioneer organizations to bring sexual harassment to public attention in the late 1970s.		Still the term was largely unknown until the early 1990s when Anita Hill witnessed and testified against Supreme Court of the United States nominee Clarence Thomas.[8] Since Hill testified in 1991, the number of sexual harassment cases reported in US and Canada increased 58 percent and have climbed steadily.[8]		Sexual harassment may occur in a variety of circumstances—in workplaces as varied as factories, school, academia, Hollywood and the music business.[9][10][11][12][13][14][15] Often, but not always, the perpetrator is in a position of power or authority over the victim (due to differences in age, or social, political, educational or employment relationships) or expecting to receive such power or authority in form of promotion. Forms of harassment relationships include:		With the advent of the Internet, social interactions, including sexual harassment, increasingly occur online, for example in video games.		According to the 2014 PEW research statistics on online harassment, 25% of women and 13% of men between the ages of 18 and 24 have experienced sexual harassment while online.[17]		The United States' Equal Employment Opportunity Commission (EEOC) defines workplace sexual harassment as "unwelcome sexual advances, requests for sexual favors, and other verbal or physical conduct of a sexual nature constitute sexual harassment when this conduct explicitly or implicitly affects an individual's employment, unreasonably interferes with an individual's work performance, or creates an intimidating, hostile, or offensive work environment” (EEOC).[18]		One of the difficulties in understanding sexual harassment, is that it involves a range of behaviors. In most cases (although not in all cases) it is difficult for the victim to describe what they experienced. This can be related to difficulty classifying the situation or could be related to stress and humiliation experienced by the recipient. Moreover, behavior and motives vary between individual cases.[21]		Dzeich et al. has divided harassers into two broad classes:		Langelan describes four different classes of harassers.[22]		Sexual harassment and assault may be prevented by secondary school,[24] college,[25][26] and workplace education programs.[27] At least one program for fraternity men produced "sustained behavioral change".[25][28]		Many sororities and fraternities in the United States take preventative measures against hazing and hazing activities during the participants' pledging processes (which may often include sexual harassment). Many Greek organizations and universities nationwide have anti-hazing policies that explicitly recognize various acts and examples of hazing, and offer preventative measures for such situations.[29]		Effects of sexual harassment can vary depending on the individuality of the recipient and the severity and duration of the harassment. Often, sexual harassment incidents fall into the category of the "merely annoying."[citation needed] In other situations, harassment may lead to temporary or prolonged stress or depression depending on the recipient's psychological abilities to cope and the type of harassment and the social support or lack thereof for the recipient. Psychologists and social workers report that severe or chronic sexual harassment can have the same psychological effects as rape or sexual assault.[30][31] Victims who do not submit to harassment may also experience various forms of retaliation, including isolation and bullying.		As an overall social and economic effect every year, sexual harassment deprives women from active social and economic participation and costs hundreds of millions of dollars in lost educational and professional opportunities for mostly girls and women.[32] However, the quantity of men implied in these conflicts is significant.		Sexual harassment, by definition, is unwanted and not to be tolerated. However, there often are a number of effective ways for offended and injured people to overcome the resulting psychological effects, remain in or return to society, regain healthy feelings within personal relationships when they were affected by the outside relationship trauma, regain social approval, and recover the ability to concentrate and be productive in educational, work, etc. environments. This may include stress management and therapy, cognitive-behavioral therapy,[33] friends and family support, etc.		Immediate psychological and legal counseling are recommended since self-treatment may not release stress or remove trauma, and simply reporting to authorities may not have the desired effect, may be ignored, or may further injure the victim at its response.		A 1991 study done by K.R. Yount found three dominant strategies developed by a sample of women coal miners to manage sexual harassment on the job: the "lady", the "flirt", and the "tomboy". The "ladies" were typically the older women workers who tended to disengage from the men, kept their distance, avoided using profanity, avoided engaging in any behavior that might be interpreted as suggestive. They also tended to emphasize by their appearance and manners that they were ladies. The consequences for the "ladies" were that they were the targets of the least amount of come-ons, teasing and sexual harassment, but they also accepted the least prestigious and lowest-paid jobs.[34]		The "flirts" were most often the younger single women. As a defense mechanism, they pretended to be flattered when they were the targets of sexual comments. Consequently, they became perceived as the "embodiment of the female stereotype,...as particularly lacking in potential and were given the fewest opportunities to develop job skills and to establish social and self-identities as miners."[34]		The "tomboys" were generally single women, but were older than the "flirts". They attempted to separate themselves from the female stereotype and focused on their status as coal miners and tried to develop a "thick skin". They responded to harassment with humor, comebacks, sexual talk of their own, or reciprocation. As a result, they were often viewed as sluts or sexually promiscuous and as women who violated the sexual double standard. Consequently, they were subjected to intensified and increased harassment by some men. It was not clear whether the tomboy strategy resulted in better or worse job assignments.[34]		The findings of this study may be applicable to other work settings, including factories, restaurants, offices, and universities. The study concludes that individual strategies for coping with sexual harassment are not likely to be effective and may have unexpected negative consequences for the workplace and may even lead to increased sexual harassment. Women who try to deal with sexual harassment on their own, regardless of what they do, seem to be in a no-win situation.[34]		Common psychological, academic, professional, financial, and social effects of sexual harassment and retaliation:		Some of the psychological and health effects that can occur in someone who has been sexually harassed as a result of stress and humiliation: depression; anxiety; panic attacks; sleeplessness; nightmares; shame; guilt; difficulty concentrating; headaches; fatigue; loss of motivation; stomach problems; eating disorders (such as weight loss or gain); alcoholism; feeling betrayed, violated, angry, violent towards the perpetrator, powerless or out of control; increased blood pressure; loss of confidence or self-esteem; withdrawal; isolation; overall loss of trust in people; traumatic stress; post-traumatic stress disorder (PTSD); complex post-traumatic stress disorder; suicidal thoughts or attempts, and suicide.[35][36][37][38][39]		Retaliation and backlash against a victim are very common, particularly a complainant. Victims who speak out against sexual harassment are often labeled troublemakers who are on their own "power trips", or who are looking for attention. Similar to cases of rape or sexual assault, the victim often becomes the accused, with their appearance, private life, and character likely to fall under intrusive scrutiny and attack.[40] They risk hostility and isolation from colleagues, supervisors, teachers, fellow students, and even friends. They may become the targets of mobbing or relational aggression.[35]		Women are not necessarily sympathetic to female complainants who have been sexually harassed. If the harasser was male, internalized sexism (or jealousy over the sexual attention towards the victim) may encourage some women to react with as much hostility towards the complainant as some male colleagues.[41] Fear of being targeted for harassment or retaliation themselves may also cause some women to respond with hostility.[42] For example, when Lois Jenson filed her lawsuit against Eveleth Taconite Co., the women shunned her both at work and in the community—many of these women later joined her suit.[43] Women may even project hostility onto the victim in order to bond with their male coworkers and build trust.[42]		Retaliation has occurred when a sexual harassment victim suffers a negative action as a result of the harassment. For example, a complainant be given poor evaluations or low grades, have their projects sabotaged, be denied work or academic opportunities, have their work hours cut back, and other actions against them which undermine their productivity, or their ability to advance at work or school, being fired after reporting sexual harassment or leading to unemployment as they may be suspended, asked to resign, or be fired from their jobs altogether. Retaliation can even involve further sexual harassment, and also stalking and cyberstalking of the victim.[41][42] Moreover, a school professor or employer accused of sexual harassment, or who is the colleague of a perpetrator, can use their power to see that a victim is never hired again, or never accepted to another school.		Of the women who have approached her to share their own experiences of being sexually harassed by their teachers, feminist and writer Naomi Wolf wrote in 2004:		I am ashamed of what I tell them: that they should indeed worry about making an accusation because what they fear is likely to come true. Not one of the women I have heard from had an outcome that was not worse for her than silence. One, I recall, was drummed out of the school by peer pressure. Many faced bureaucratic stonewalling. Some women said they lost their academic status as golden girls overnight; grants dried up, letters of recommendation were no longer forthcoming. No one was met with a coherent process that was not weighted against them. Usually, the key decision-makers in the college or university—especially if it was a private university—joined forces to, in effect, collude with the faculty member accused; to protect not him necessarily but the reputation of the university, and to keep information from surfacing in a way that could protect other women. The goal seemed to be not to provide a balanced forum, but damage control.[44]		Another woman who was interviewed by sociologist Helen Watson said, "Facing up to the crime and having to deal with it in public is probably worse than suffering in silence. I found it to be a lot worse than the harassment itself."[45]		Backlash stress is stress resulting from an uncertainty regarding changing norms for interacting with women in the workplace.[46] Backlash stress now deters many male workers from befriending female colleagues, or providing them with any assistance, such as holding doors open. As a result, women are being handicapped by a lack of the necessary networking and mentorship.[47][48]		Most companies have policies against sexual harassment, however these policies are not designed and should not attempt to "regulate romance" which goes against human urges.[49]		Act upon a report of harassment inside the organization should be:		The investigation should be designed to obtain a prompt and thorough collection of the facts, an appropriate responsive action, and an expeditious report to the complainant that the investigation has been concluded, and, to the full extent appropriate, the action taken.		When organizations do not take the respective satisfactory measures for properly investigating, stress and psychological counseling and guidance, and just deciding of the problem this could lead to:		Studies show that organizational climate (an organization's tolerance, policy, procedure etc.) and workplace environment are essential for understanding the conditions in which sexual harassment is likely to occur, and the way its victims will be affected (yet, research on specific policy and procedure, and awareness strategies is lacking). Another element which increases the risk for sexual harassment is the job’s gender context (having few women in the close working environment or practicing in a field which is atypical for women).[54]		According to Dr. Orit Kamir, the most effective way to avoid sexual harassment in the work place, and also influence the public’s state of mind, is for the employer to adopt a clear policy prohibiting sexual harassment and to make it very clear to their employees. Many women prefer to make a complaint and to have the matter resolved within the workplace rather than to "air out the dirty laundry" with a public complaint and be seen as a traitor by colleagues, superiors and employers, adds Kamir.[55][56][57]		Most prefer a pragmatic solution that would stop the harassment and prevent future contact with the harasser rather than turning to the police. More about the difficulty in turning an offence into a legal act can be found in Felstiner & Sarat's (1981) study,[58] which describes three steps a victim (of any dispute) must go through before turning to the justice system: naming – giving the assault a definition, blaming – understanding who is responsible for the violation of rights and facing them, and finally, claiming – turning to the authorities.		The Declaration on the Elimination of Violence Against Women classifies violence against women into three categories: that occurring in the family, that occurring within the general community, and that perpetrated or condoned by the State. The term sexual harassment is used in defining violence occurring in the general community, which is defined as: "Physical, sexual and psychological violence occurring within the general community, including rape, sexual abuse, sexual harassment and intimidation at work, in educational institutions and elsewhere, trafficking in women and forced prostitution."[59]		In India, the case of Vishakha and others v State of Rajasthan in 1997 has been credited with establishing sexual harassment as illegal.[60] In Israel, the 1988 Equal Employment Opportunity Law made it a crime for an employer to retaliate against an employee who had rejected sexual advances, but it wasn't until 1998 that the Israeli Sexual Harassment Law made such behavior illegal.[61]		In May 2002, the European Union Council and Parliament amended a 1976 Council Directive on the equal treatment of men and women in employment to prohibit sexual harassment in the workplace, naming it a form of sex discrimination and violation of dignity. This Directive required all Member States of the European Union to adopt laws on sexual harassment, or amend existing laws to comply with the Directive by October 2005.[62]		In 2005, China added new provisions to the Law on Women's Right Protection to include sexual harassment.[63] In 2006, "The Shanghai Supplement" was drafted to help further define sexual harassment in China.[64]		Sexual harassment was specifically criminalized for the first time in modern Egyptian history in June 2014.[65]		Sexual harassment remains legal in Kuwait[66] and Djibouti.[67]		The United Nations General Recommendation 19 to the Convention on the Elimination of all Forms of Discrimination Against Women defines sexual harassment of women to include:		such unwelcome sexually determined behavior as physical contact and advances, sexually colored remarks, showing pornography and sexual demands, whether by words or actions. Such conduct can be humiliating and may constitute a health and safety problem; it is discriminatory when the woman has reasonable ground to believe that her objection would disadvantage her in connection with her employment, including recruitment or promotion, or when it creates a hostile working environment.		While such conduct can be harassment of women by men, many laws around the world which prohibit sexual harassment recognize that both men and women may be harassers or victims of sexual harassment. However, most claims of sexual harassment are made by women.[68]		There are many similarities, and also important differences in laws and definitions used around the world.		In 2016 a stricter law proscribing sexual harassment was proposed in Morocco specifying fines and a possible jail sentence of up to 6 months.[69] The existing law against harassment was reported to not be upheld, as harassment was not reported to police by victimes and even when reported, was not investigated by police or prosecuted by the courts.[69][70]		The Sex Discrimination Act 1984 defines sexual harassment as "... a person sexually harasses another person (the person harassed ) if: (a) the person makes an unwelcome sexual advance, or an unwelcome request for sexual favours, to the person harassed; or (b) engages in other unwelcome conduct of a sexual nature in relation to the person harassed; in circumstances in which a reasonable person, having regard to all the circumstances, would have anticipated the possibility that the person harassed would be offended, humiliated or intimidated."[71]		In the European Union, there is a directive on sexual harassment. The Directive 2002/73/EC - equal treatment of 23 September 2002 amending Council Directive 76/207/EEC on the implementation of the principle of equal treatment for men and women as regards access to employment, vocational training and promotion, and working conditions states:[2]		For the purposes of this Directive, the following definitions shall apply: (...)		The Convention on preventing and combating violence against women and domestic violence also addresses the issue of sexual harassment (Article 40), using a similar definition.[72]		Sexual harassment is defined as, when any verbal, non-verbal or physical action is used to change a victim's sexual status against the will of the victim and resulting in the victim feeling inferior or hurting the victim's dignity. Man and woman are looked upon as equal, and any action trying to change the balance in status with the differences in sex as a tool, is also sexual harassment. In the workplace, jokes, remarks, etc., are only deemed discriminatory if the employer has stated so in their written policy. Law number 1385 of December 21, 2005 regulates this area.[73]		In France, both the Criminal Code and the Labor Code are relevant to the issue of sexual harassment. Until May 4, 2012, article 222-33 of the French Criminal Code described sexual harassment as "The fact of harassing anyone in order to obtain favors of a sexual nature".[74] Since 2002, it recognized the possibility of sexual harassment between co-workers and not only by supervisors. On May 4, 2012, the Conseil constitutionnel (French Supreme Court) quashed the definition of the criminal code as being too vague.[75] The 2012 decision resulted from a law on priority preliminary rulings on the issue of constitutionality. As a consequence of this decision, all pending procedures before criminal courts were cancelled. Several feminist NGOs, such as AFVT, criticized this decision. President François Hollande, the Minister of Justice (Christine Taubira) and the Minister of Equality (Najat Belkacem) asked that a new law be voted rapidly. As a result, LOI n°2012-954 du 6 août 2012 was voted in, providing a new definition.[76] In addition to criminal provisions, the French Labor code also prohibits sexual harassment.[77] The legislator voted a law in 2008[78] that copied the 2002/73/EC Directive[79] definition without modifying the French Labour Code.		Sexual harassment is no statutory offence in Germany. In special cases it might be chargeable as "Insult" (with sexual context) as per § 185 Strafgesetzbuch but only if special circumstances show an insulting nature.		The victim has only a right to self-defend while the attack takes place. If a perpetrator kisses or gropes the victim, they may only fight back while this is happening. If the victim would, for instance, slap back after the attacker already stopped, the victim might be chargeable for assault as per § 223 Strafgesetzbuch.[80]		In June 2016 the governing coalition decided about the key points of a tightening of the law governing sexual offenses (Sexualstrafrecht, literally: law on the punishment of sexual delicts). At July 7, 2016 the Bundestag passed the resolution[81] and by autumn 2016 the draft bill will be presented to the second chamber, the Bundesrat.[82] By this change, sexual harassment shall become punishable under the Sexualstrafrecht.[83]		In response to the EU Directive 2002/73/EC, Greece enacted Law 3488/2006 (O.G.A.'.191). The law specifies that sexual harassment is a form of gender-based discrimination in the workplace. Victims also have the right to compensation.[84] Prior to this law, the policy on sexual harassment in Greece was very weak. Sexual harassment was not defined by any law, and victims could only use general laws, which were very poor in addressing the issue.[85][86]		In the Criminal Code, Russian Federation, (CC RF), there exists a law which prohibits utilization of an office position and material dependence for coercion of sexual interactions (Article 118, current CC RF). However, according to the Moscow Center for Gender Studies, in practice, the courts do not examine these issues.[87]		The Daily Telegraph quotes a survey in which "100 per cent of female professionals [in Russia] said they had been subjected to sexual harassment by their bosses, 32 per cent said they had had intercourse with them at least once and another seven per cent claimed to have been raped."[88]		A ban on discrimination was included in the Federal Constitution (Article 4, Paragraph 2 of the old Federal Constitution) in 1981 and adopted in Article 8, paragraph 2 of the revised Constitution. The ban on sexual harassment in the workplace forms part of the Federal Act on Gender Equality (GEA) of 24 March 1995, where it is one of several provisions which prohibit discrimination in employment and which are intended to promote equality. Article 4 of the GEA defines the circumstances, Article 5 legal rights and Article 10 protection against dismissal during the complaints procedure. Article 328, paragraph 1 of the Code of Obligations (OR), Article 198 (2) of the Penal Code (StGB) and Article 6, paragraph 1 of the Employment Act (ArG) contain further statutory provisions on the ban on sexual harassment. The ban on sexual harassment is intended exclusively for employers, within the scope of their responsibility for protection of legal personality, mental and physical well-being and health.[citation needed]		Article 4 of the GEA of 1995 defines sexual harassment in the workplace as follows: "Any behaviour of a sexual nature or other behaviour attributable to gender which affronts the human dignity of males and females in the workplace. This expressly includes threats, the promise of advantages, the application of coercion and the exercise of pressure to achieve an accommodation of a sexual nature."[citation needed]		The Discrimination Act of 1975, was modified to establish sexual harassment as a form of discrimination in 1986.[89] It states that harassment occurs where there is unwanted conduct on the ground of a person's sex or unwanted conduct of a sexual nature and that conduct has the purpose or effect of violating a person's dignity, or of creating an intimidating, hostile, degrading, humiliating or offensive environment for them. If an employer treats someone less favourably because they have rejected, or submitted to, either form of harassment described above, this is also harassment.[90]		Sexual harassment in India is termed "Eve teasing" and is described as: unwelcome sexual gesture or behaviour whether directly or indirectly as sexually coloured remarks; physical contact and advances; showing pornography; a demand or request for sexual favours; any other unwelcome physical, verbal or non-verbal conduct being sexual in nature or passing sexually offensive and unacceptable remarks. The critical factor is the unwelcomeness of the behaviour, thereby making the impact of such actions on the recipient more relevant rather than intent of the perpetrator.[60] According to the Indian constitution, sexual harassment infringes the fundamental right of a woman to gender equality under Article 14 and her right to life and live with dignity under Article 21.[91]		In 1997, the Supreme Court of India in a Public Interest Litigation, defined sexual harassment at workplace, preventive measures and redress mechanism. The judgement is popularly known as Vishaka Judgement.[92] In April 2013, India enacted its own law on sexual harassment in the workplace - The Sexual Harassment of Women at Workplace (Prevention, Prohibition and Redressal) Act, 2013. Almost 16 years after the Supreme Court's landmark guidelines on prevention of sexual harassment in the workplace (known as the "Vishaka Guidelines"), the Act has endorsed many of the guidelines, and is a step towards codifying gender equality. The Act is intended to include all women employees in its ambit, including those employed in the unorganized sector, as well as domestic workers.		The Act has identified sexual harassment as a violation of the fundamental rights of a woman to equality under articles 14 and 15 of the Constitution of India and her right to life and to live with dignity under article 21 of the Constitution; as well as the right to practice any profession or to carry on any occupation, trade or business which includes a right to a safe environment free from sexual harassment. The Act also states that the protection against sexual harassment and the right to work with dignity are universally recognized human rights by international conventions and instruments such as Convention on the Elimination of all Forms of Discrimination against Women, which has been ratified on the 25th June, 1993 by the Government of India.[93]		The Criminal Law (Amendment) Act, 2013 introduced changes to the Indian Penal Code, making sexual harassment an expressed offence under Section 354 A, which is punishable up to three years of imprisonment and or with fine. The Amendment also introduced new sections making acts like disrobing a woman without consent, stalking and sexual acts by person in authority an offence.		The 1998 Israeli Sexual Harassment Law interprets sexual harassment broadly, and prohibits the behavior as a discriminatory practice, a restriction of liberty, an offence to human dignity, a violation of every person's right to elementary respect, and an infringement of the right to privacy. Additionally, the law prohibits intimidation or retaliation thus related to sexual harassment are defined by the law as "prejudicial treatment".[61]		Sexual Harassment, or sekuhara in Japanese, appeared most dramatically in Japanese discourse in 1989, when a court case in Fukuoka ruled in favor of an woman who had been subjected to the spreading of sexual rumors by a co-worker. When the case was first reported, it spawned a flurry of public interest: 10 books were published, including English-language feminist guidebooks to ‘how not to harass women’ texts for men.[94] Sekuhara was named 1989’s ‘Word of the Year.’ The case was resolved in the victim’s favor in 1992, awarding her about $13,000 in damages, the first sexual harassment lawsuit in Japanese history.[95]		Laws then established two forms of sexual harassment: daisho, in which rewards or penalties are explicitly linked to sexual acts, and kankyo, in which the environment is made unpleasant through sexual talk or jokes, touching, or hanging sexually explicit posters. This applies to everyone in an office, including customers.[94]		Pakistan has promulgated harassment law in 2010 with nomenclature as "The Protection Against Harassment of Women at The Workplace Act, 2010". This law defines the act of harassment in following terms. "Harassment means any unwelcome sexual advance, request for sexual favors or other verbal or written communication or physical conduct of a sexual nature or sexually demeaning attitude, causing interference with work performance or creating an intimidating, hostile or offensive work environment, or the attempt to punish the complainant for refusal to such a request or is made a condition for employment." Pakistan has adopted a Code of Conduct for Gender Justice in the Workplace that will deal with cases of sexual harassment. The Alliance Against Sexual Harassment At workplace (AASHA) announced they would be working with the committee to establish guidelines for the proceedings. AASHA defines sexual harassment much the same as it is defined in the U.S. and other cultures.[96]		The Anti-Sexual Harassment Act of 1995 was enacted:		primarily to protect and respect the dignity of workers, employees, and applicants for employment as well as students in educational institutions or training centers. This law, consisting of ten sections, provides for a clear definition of work, education or training-related sexual harassment and specifies the acts constituting sexual harassment. It likewise provides for the duties and liabilities of the employer in cases of sexual harassment, and sets penalties for violations of its provisions. It is to be noted that a victim of sexual harassment is not barred from filing a separate and independent action for damages and other relief aside from filing the charge for sexual harassment.[97]		In the US, the Civil Rights Act of 1964 prohibits employment discrimination based on race, sex, color, national origin or religion. Initially only intended to combat sexual harassment of women, {42 U.S.C. § 2000e-2} the prohibition of sex discrimination covers both females and males. This discrimination occurs when the sex of the worker is made as a condition of employment (i.e. all female waitpersons or male carpenters) or where this is a job requirement that does not mention sex but ends up preventing many more persons of one sex than the other from the job (such as height and weight limits). This act only applies to employers with 15 or more employees.[98]		Barnes v. Train (1974) is commonly viewed as the first sexual harassment case in America, even though the term "sexual harassment" was not used.[99] The term "sexual harassment" was coined and popularized by Lin Farley in 1975, based on a pattern she recognized during a 1974 Cornell University class she taught on women and work.[100] In 1976, Williams v. Saxbe established sexual harassment as a form of sex discrimination when sexual advances by a male supervisor towards a female employee, if proven, would be deemed an artificial barrier to employment placed before one gender and not another. In 1980 the Equal Employment Opportunity Commission (EEOC) issued regulations defining sexual harassment and stating it was a form of sex discrimination prohibited by the Civil Rights Act of 1964. In the 1986 case of Meritor Savings Bank v. Vinson, the Supreme Court first recognized "sexual harassment" as a violation of Title VII, established the standards for analyzing whether the conduct was welcome and levels of employer liability, and that speech or conduct in itself can create a "hostile environment".[101] The Civil Rights Act of 1991 added provisions to Title VII protections including expanding the rights of women to sue and collect compensatory and punitive damages for sexual discrimination or harassment, and the case of Ellison v. Brady resulted in rejecting the reasonable person standard in favor of the "reasonable woman standard" which allowed for cases to be analyzed from the perspective of the complainant and not the defendant.[102] Also in 1991, Jenson v. Eveleth Taconite Co. became the first sexual harassment case to be given class action status paving the way for others. Seven years later, in 1998, through that same case, new precedents were established that increased the limits on the "discovery" process in sexual harassment cases, that then allowed psychological injuries from the litigation process to be included in assessing damages awards. In the same year, the courts concluded in Faragher v. City of Boca Raton, Florida, and Burlington v. Ellerth, that employers are liable for harassment by their employees.[103][104] Moreover, Oncale v. Sundowner Offshore Services set the precedent for same-sex harassment, and sexual harassment without motivation of "sexual desire", stating that any discrimination based on sex is actionable so long as it places the victim in an objectively disadvantageous working condition, regardless of the gender of either the victim, or the harasser.		In the 2006 case of Burlington Northern & Santa Fe Railway Co. v. White, the standard for retaliation against a sexual harassment complainant was revised to include any adverse employment decision or treatment that would be likely to dissuade a "reasonable worker" from making or supporting a charge of discrimination.		During 2007 alone, the U.S. Equal Employment Opportunity Commission and related state agencies received 12,510 new charges of sexual harassment on the job.[105]		The 2010 case, Reeves v. C.H. Robinson Worldwide, Inc. ruled that a hostile work environment can be created in a workplace where sexually explicit language and pornography are present. A hostile workplace may exist even if it is not targeted at any particular employee.[106][107]		From 2010 to 2013, the sexual harassment claims filed by male employees in the United States have tripled, even though the total number of cases filed went down by 3%. Male employees stated that 59% of their harassers were female and 41% reported their harasser as male.[108]		Title IX of the Education Amendments of 1972 (United States) states "No person in the United States shall, on the basis of sex, be excluded from participation in, be denied the benefits of, or be subjected to discrimination under any education program or activity receiving Federal financial assistance."		In Franklin v. Gwinnett County Public Schools (1992), the U.S. Supreme Court held that private citizens could collect damage awards when teachers sexually harassed their students.[109] In Bethel School District No. 403 v. Fraser (1986) the courts ruled that schools have the power to discipline students if they use "obscene, profane language or gestures" which could be viewed as substantially interfering with the educational process, and inconsistent with the "fundamental values of public school education."[110] Under regulations issued in 1997 by the U.S. Department of Education, which administers Title IX, school districts should be held responsible for harassment by educators if the harasser "was aided in carrying out the sexual harassment of students by his or her position of authority with the institution."[111] In Davis v. Monroe County Board of Education, and Murrell v. School Dist. No. 1, 1999, schools were assigned liability for peer-to-peer sexual harassment if the plaintiff sufficiently demonstrated that the administration's response shows "deliberate indifference" to "actual knowledge" of discrimination.[112][113]		There are a number of legal options for a complainant in the U.S.: mediation, filing with the EEOC or filing a claim under a state Fair Employment Practices (FEP) statute (both are for workplace sexual harassment), filing a common law tort, etc.[114] Not all sexual harassment will be considered severe enough to form the basis for a legal claim. However, most often there are several types of harassing behaviors present, and there is no minimum level for harassing conduct under the law.[32] Many more experienced sexual harassment than have a solid legal case against the accused.[citation needed] Because of this, and the common preference for settling, few cases ever make it to federal court.[114] The section below "EEOC Definition" describes the legal definitions that have been created for sexual harassment in the workplace. Definitions similar to the EEOC definition have been created for academic environments in the U.S. Department of Education Sexual Harassment Guidance.[115]		The Equal Employment Opportunity Commission claims that it is unlawful to harass an applicant or employee of any sex in the work place. The harassment could include sexual harassment. The EEOC says that the victim and harasser could be any gender and that the other does not have to be of the opposite sex. The law does not ban offhand comments, simple teasing, or incidents that aren't very serious. If the harassment gets to the point where it creates a harsh work environment, it will be taken care of.[4] In 1980, the Equal Employment Opportunity Commission produced a set of guidelines for defining and enforcing Title VII (in 1984 it was expanded to include educational institutions). The EEOC defines sexual harassment as:		Unwelcome sexual advances, requests for sexual favors, or other verbal or physical conduct of a sexual nature when:		1. and 2. are called "quid pro quo" (Latin for "this for that" or "something for something"). They are essentially "sexual bribery", or promising of benefits, and "sexual coercion".		Type 3. known as "hostile work environment", is by far the most common form. This form is less clear cut and is more subjective.[41]		Note: a workplace harassment complainant must file with the EEOC and receive a "right to sue" clearance, before they can file a lawsuit against a company in federal court.[32]		Quid pro quo means "this for that". In the workplace, this occurs when a job benefit is directly tied to an employee submitting to unwelcome sexual advances. For example, a supervisor promises an employee a raise if he or she will go out on a date with him or her, or tells an employee he or she will be fired if he or she doesn't sleep with him or her.[116] Quid pro quo harassment also occurs when an employee makes an evaluative decision, or provides or withholds professional opportunities based on another employee's submission to verbal, nonverbal or physical conduct of a sexual nature. Quid pro quo harassment is equally unlawful whether the victim resists and suffers the threatened harm or submits and thus avoids the threatened harm.[117]		This occurs when an employee is subjected to comments of a sexual nature, unwelcome physical contact, or offensive sexual materials as a regular part of the work environment. For the most part, a single isolated incident will not be enough to prove hostile environment harassment unless it involves extremely outrageous and egregious conduct. The courts will try to decide whether the conduct is both "serious" and "frequent." Supervisors, managers, co-workers and even customers can be responsible for creating a hostile environment.[118]		The line between "quid pro quo" and "hostile environment" harassment is not always clear and the two forms of harassment often occur together. For example, an employee's job conditions are affected when a sexually hostile work environment results in a constructive discharge. At the same time, a supervisor who makes sexual advances toward a subordinate employee may communicate an implicit threat to retaliate against her if she does not comply.[119]		"Hostile environment" harassment may acquire characteristics of "quid pro quo" harassment if the offending supervisor abuses his authority over employment decisions to force the victim to endure or participate in the sexual conduct. Sexual harassment may culminate in a retaliatory discharge if a victim tells the harasser or her employer she will no longer submit to the harassment, and is then fired in retaliation for this protest. Under these circumstances it would be appropriate to conclude that both harassment and retaliation in violation of section 704(a) of Title VII have occurred."		In the United States, there are no federal laws prohibiting discrimination against employees based on their sexual orientation. However, Executive Order 13087, signed by President Bill Clinton, outlaws discrimination based on sexual orientation against federal government employees. If a small business owner owns his or her business in a state where there is a law against sexual orientation discrimination, the owner must abide to the law regardless of there not being a federal law. Twenty states and the District of Columbia have laws against this form of discrimination in the workplace. These states include California, Connecticut, Colorado, Hawaii, Illinois, Iowa, Maine, Maryland, Massachusetts, Minnesota, Nevada, New Hampshire, New Jersey, New Mexico, New York, Oregon, Rhode Island, Vermont, Washington, and Wisconsin.[120] For example, California has laws in place to protect employees who may have been discriminated against based upon sexual orientation or perceived sexual orientation. California law prohibits discrimination against those "with traits not stereotypically associated with their gender", such as mannerisms, appearance, or speech. Sexual orientation discrimination comes up, for instance, when employers enforce a dress code, permit women to wear makeup but not men, or require men and women to only use restrooms designated for their particular sex regardless of whether they are transgender.		Retaliation has occurred when an employee suffers a negative action after he or she has made a report of sexual harassment, file a grievance, assist someone else with a complaint, or participate in discrimination prevention activities. Negative actions can include being fired, demotion, suspension, denial of promotion, poor evaluation, unfavorable job reassignment—any adverse employment decision or treatment that would be likely to dissuade a "reasonable worker" from making or supporting a charge of discrimination. (See Burlington Northern & Santa Fe Railway Co. v. White.)[121] Retaliation is as illegal as the sexual harassment itself, but also as difficult to prove. Also, retaliation is illegal even if the original charge of sexual harassment was not proven.		In ancient Rome, according to Bruce W. Frier and Thomas A.J. McGinn, what is now called sexual harassment[122] was then any of accosting, stalking, and abducting. Accosting was "harassment through attempted seduction"[122] or "assault[ing] another's chastity with smooth talk ... contrary to good morals",[122] which was more than the lesser offense(s) of "obscene speech, dirty jokes, and the like",[122] "foul language",[122] and "clamor",[122] with the accosting of "respectable young girls" who however were dressed in slaves' clothing being a lesser offense[122] and the accosting of "women ... dressed as prostitutes" being an even lesser offense.[122] Stalking was "silently, persistently pursuing"[122] if it was "contrary to good morals",[123] because a pursuer's "ceaseless presence virtually ensures appreciable disrepute".[122] Abducting an attendant, who was someone who follows someone else as a companion and could be a slave,[124] was "successfully forcing or persuading the attendant to leave the side of the intended target",[124] but abducting while the woman "has not been wearing respectable clothing" was a lesser offense.[122]		Though the phrase sexual harassment is generally acknowledged to include clearly damaging and morally deplorable behavior, its boundaries can be broad and controversial. Accordingly, misunderstandings can occur. In the US, sexual harassment law has been criticized by persons such as the criminal defense lawyer Alan Dershowitz and the legal writer and libertarian Eugene Volokh, for imposing limits on the right to free speech.[125]		Prof. in organizational studies Jana Raver from the Queen's School of Business criticized sexual harassment policy in the Ottawa Business Journal as helping maintain archaic stereotypes of women as "delicate, asexual creatures" who require special protection when at the same time complaints are lowering company profits.[51] Camille Paglia says that young girls can end up acting in such ways as to make sexual harassment easier, such that for example, by acting "nice" they can become a target. Paglia commented in an interview with Playboy, "Realize the degree to which your niceness may invoke people to say lewd and pornographic things to you--sometimes to violate your niceness. The more you blush, the more people want to do it."[126]		Other critics assert that sexual harassment is a very serious problem, but current views focus too heavily on sexuality rather than on the type of conduct that undermines the ability of women or men to work together effectively. Viki Shultz, a law professor at Yale University comments, "Many of the most prevalent forms of harassment are designed to maintain work-particularly the more highly rewarded lines of work-as bastions of male competence and authority."[127] Feminist Jane Gallop sees this evolution of the definition of sexual harassment as coming from a "split" between what she calls "power feminists" who are pro-sex (like herself) and what she calls "victim feminists", who are not. She argues that the split has helped lead to a perversion of the definition of sexual harassment, which used to be about sexism but has come to be about anything that's sexual.[128]		There is also concern over abuses of sexual harassment policy by individuals as well as by employers and administrators using false or frivolous accusations as a way of expelling employees they want to eliminate for other reasons. These employees often have virtually no recourse thanks to the at-will law in most US states.[129]		O'Donohue and Bowers outlined 14 possible pathways to false allegations of sexual harassment: "lying, borderline personality disorder, histrionic personality disorder, psychosis, gender prejudice, substance abuse, dementia, false memories, false interpretations, biased interviews, sociopathy, personality disorders not otherwise specified."[130]		There is also discussion of whether some recent trends towards more revealing clothing and permissive habits have created a more sexualized general environment, in which some forms of communication are unfairly labeled harassment, but are simply a reaction to greater sexualization in everyday environments.[131]		There are many debates about how organizations should deal with sexual harassment. Some observers feel strongly that organizations should be held to a zero tolerance standard of "Must report - must investigate - must punish."		Others write that those who feel harassed should in most circumstances have a choice of options.[56][57][132]		Media related to Sexual harassment at Wikimedia Commons		
Affirmative action, also known as reservation in India and Nepal, positive discrimination in the UK, and employment equity (in a narrower context) in Canada and South Africa, is the policy of favoring members of a disadvantaged group who suffer or have suffered from discrimination within a culture.[1][2][3][4] Historically and internationally, support for affirmative action has sought to achieve goals such as bridging inequalities in employment and pay, increasing access to education, promoting diversity, and redressing apparent past wrongs, harms, or hindrances.		The nature of affirmative action policies varies from region to region. Some countries, such as India, use a quota system, whereby a certain percentage of government jobs, political positions, and school vacancies must be reserved for members of a certain group. In some other regions where quotas are not used, minority group members are given preference or special consideration in selection processes. In the United States, affirmative action in employment and education has been the subject of legal and political controversy, and in 2003, a pair of US Supreme Court decisions (Grutter v. Bollinger and Gratz v. Bollinger) permitted educational institutions to consider race as a factor when admitting students while prohibiting the use of quotas.[5] In other countries, such as the UK,[6][7][8] affirmative action is rendered illegal because it does not treat all races equally. This approach to equal treatment is described as being "color blind." In such countries, the focus tends to be on ensuring equal opportunity and, for example, targeted advertising campaigns to encourage ethnic minority candidates to join the police force. This is sometimes described as "positive action."						The term "affirmative action" was first used in the United States in "Executive Order No. 10925",[9] signed by President John F. Kennedy on 6 March 1961, which included a provision that government contractors "take affirmative action to ensure that applicants are employed, and employees are treated during employment, without regard to their race, creed, color, or national origin."[10] It was used to promote actions that achieve non-discrimination. In 1965, President Lyndon B. Johnson issued Executive Order 11246 which required government employers to take "affirmative action" to "hire without regard to race, religion and national origin". This prevented employers from discriminating against members of disadvantaged groups. In 1967, gender was added to the anti-discrimination list.[11]		Affirmative action is intended to promote the opportunities of defined minority groups within a society to give them equal access to that of the majority population.[12]		It is often instituted for government and educational settings to ensure that certain designated "minority groups" within a society are able to participate in all provided opportunities including promotional, educational, and training opportunities.[13]		The stated justification for affirmative action by its proponents is that it helps to compensate for past discrimination, persecution or exploitation by the ruling class of a culture,[14] and to address existing discrimination.[15]		Several different studies investigated the effect of affirmative action on women. Kurtulus (2012) in her review of affirmative action and the occupational advancement of minorities and women during 1973-2003 showed that the effect of affirmative action on advancing black, Hispanic, and white women into management, professional, and technical occupations occurred primarily during the 1970s and early 1980s. During this period, contractors grew their shares of these groups more rapidly than noncontractors because of the implementation of affirmative action. But the positive effect of affirmative action vanished entirely in the late 1980s, which Kurtulus says may be due to the slowdown into advanced occupation for women and minorities because of the political shift of affirmative action that started by President Reagan. Becoming a federal contractor increased white women's share of professional occupations by 0.183 percentage points, or 7.3 percent, on average during these three decades, and increased black women's share by 0.052 percentage points (or by 3.9 percent). Becoming a federal contractor also increased Hispanic women's and black men's share of technical occupations on average by 0.058 percent and 0.109 percentage points respectively (or by 7.7 and 4.2 percent). These represent a substantial contribution of affirmative action to overall trends in the occupational advancement of women and minorities over the three decades under the study.[16] A further study by Kim and Kim (2014) considered the impact of four primary factors on support for affirmative action programs for women: gender; political factors; psychological factors; and social structure. They found that, "Affirmative action both corrects existing unfair treatment and gives women equal opportunity in the future."[17]		Law regarding quotas and affirmative action varies widely from nation to nation. Caste-based quotas are used in India for reservation. However, they are illegal in the United States, where no employer, university, or other entity may create a set number required for each race.[18]		In 2012, the European Union Commission approved a plan for women to constitute 40% of non-executive board directorships in large listed companies in Europe by 2020.[19] In Sweden, the Supreme Court has ruled that "affirmative action" ethnic quotas in universities are discrimination and hence unlawful. It said that the requirements for the intake should be the same for all. The justice minister said that the decision left no room for uncertainty.[20]		In some countries that have laws on racial equality, affirmative action is rendered illegal because it does not treat all races equally. This approach of equal treatment is sometimes described as being "color blind", in hopes that it is effective against discrimination without engaging in reverse discrimination.		In such countries, the focus tends to be on ensuring equal opportunity and, for example, targeted advertising campaigns to encourage ethnic minority candidates to join the police force. This is sometimes described as "positive action."		The apartheid government, as a matter of state policy, favoured white-owned, especially Afrikaner-owned companies. The aforementioned policies achieved the desired results, but in the process they marginalised and excluded black people. Skilled jobs were also reserved for white people, and blacks were largely used as unskilled labour, enforced by legislation including the Mines and Works Act, the Job Reservations Act, the Native Building Workers Act, the Apprenticeship Act and the Bantu Education Act,[21] creating and extending the "colour bar" in South African labour.[22] Then the whites successfully persuaded the government to enact laws that highly restricted the blacks' employment opportunities.		Since the 1960s the apartheid laws had been weakened. Consequently, from 1975 to 1990 the real wages of black manufacturing workers rose by 50%, while those of whites rose by 1%.[23]		The economic and politically structured society during the apartheid ultimately caused disparities in employment, occupation and income within labour markets, which provided advantages to certain groups and characteristics of people. This in due course was the motivation to introduce affirmative action in South Africa, following the end of apartheid.[24]		Following the transition to democracy in 1994, the African National Congress-led government chose to implement affirmative action legislation to correct previous imbalances (a policy known as employment equity). As such, all employers were compelled by law to employ previously disenfranchised groups (blacks, Indians, Chinese and Coloureds). A related, but distinct concept is Black Economic Empowerment.[25]		The Employment Equity Act and the Broad Based Black Economic Empowerment Act aim to promote and achieve equality in the workplace (in South Africa termed "equity"), by advancing people from designated groups. The designated groups who are to be advanced include all people of colour, women (including white women) and people with disabilities (including white peolpe). Employment Equity legislation requires companies employing more than 50 people to design and implement plans to improve the representativity of workforce demographics, and report them to the Department of Labour.[26]		Employment Equity also forms part of a company's Black Economic Empowerment scorecard: in a relatively complex scoring system, which allows for some flexibility in the manner in which each company meets its legal commitments, each company is required to meet minimum requirements in terms of representation by previously disadvantaged groups. The matters covered include equity ownership, representation at employee and management level (up to board of director level), procurement from black-owned businesses and social investment programs, amongst others.		The policies of Employment Equity and, particularly, Black Economic empowerment have been criticised both by those who view them as discriminatory against white people, and by those who view them as ineffectual.[27][28][29][30][31]		These laws cause disproportionally high costs for small companies and reduce economic growth and employment.[23] The laws may give the black middle-class some advantage but can make the worse-off blacks even poorer.[23] Moreover, the Supreme Court has ruled that in principle blacks may be favored, but in practice this should not lead to unfair discrimination against the others.[23][23]		As mentioned previously affirmative action was introduced through the Employment Equality Act, 55 in 1998, 4 years after the end of apartheid. This act was passed to promote the constitutional right of equality and exercise true democracy. This idea was to eliminate unfair discrimination in employment, to ensure the implementation of employment equity to redress the effects of discrimination, to achieve a diverse workforce broadly representative of our people, to promote economic development and efficiency in the workforce and to give effects to the obligations of the Republic as a member of the International Labour Organisation.[24][32]		Many embraced the Act; however some concluded that the act contradicted itself. The act eliminates unfair discrimination in certain sectors of the national labour market by imposing similar constraints on another.[24]		With the introduction of Affirmative Action, Black Economic Empowerment (BEE) rose additionally in South Africa. The BEE was not a moral initiative to redress the wrongs of the past but to promote growth and strategies that aim to realize a country's full potential. The idea was targeting the weakest link in economics, which was inequality and which would help develop the economy. This is evident in the statement by the Department of Trade and Industry, "As such, this strategy stresses a BEE process that is associated with growth, development and enterprise development, and not merely the redistribution of existing wealth".[33][34] Similarities between the BEE and affirmative action are apparent; however there is a difference. BEE focuses more on employment equality rather than taking wealth away from the skilled white labourers.[33]		The main goal of Affirmative Action is for a country to reach its full potential. This occurrence would result in a completely diverse workforce in economic and social sectors. Thus broadening the economic base and therefore stimulating economic growth.[35]		Once applied within the country, many different outcomes arose, some positive and some negative. This depended on the approach to and the view of The Employment Equality Act and affirmative action.		Positive: Pre-Democracy, the apartheid governments discriminated against non-white races, so with affirmative action, the country started to redress past discriminations. Affirmative action also focused on combating structural racism and racial inequality, hoping to maximize diversity in all levels of society and sectors.[36] Achieving this would elevate the status of the perpetual underclass and to restore equal access to the benefits of society.[24]		Negative: Though affirmative action had its positives, negatives arose. A quota system was implemented, which aimed to achieve targets of diversity in a work force. This target affected the hiring and level of skill in the work force, ultimately affecting the free market.[35][36] Affirmative action created marginalization for coloured and Indian races in South Africa, as well as developing and aiding the middle and elite classes, leaving the lower class behind. This created a bigger gap between the lower and middle class, which led to class struggles and a greater segregation.[32][36] Entitlement began to arise with the growth of the middle and elite classes, as well as race entitlement. Many believe that affirmative action is discrimination in reverse. With all these negatives, much talent started to leave the country.[24] Many negative consequences of affirmative action, specifically the quota system, drive skilled labour away, resulting in bad economic growth. This is due to very few international companies wanting to invest in South Africa.[36]		With these negative and positive outcomes of affirmative action, it is evident that the concept of affirmative action is continually evolving.[36]		There is affirmative action in education for minority nationalities. This may equate to lowering minimum requirements for the National University Entrance Examination, which is a mandatory exam for all students to enter university.[37][38] Some universities set quotas for minority (non-Han) student intake.[39] Further, minority students enrolled in ethnic minority-oriented specialties (e.g. language and literature programs) are provided with scholarships and/or pay no tuition, and are granted a monthly stipend.		A class-based affirmative action policy was incorporated into the admission practices of the four most selective universities in Israel during the early to mid-2000s. In evaluating the eligibility of applicants, neither their financial status nor their national or ethnic origins are considered. The emphasis, rather, is on structural disadvantages, especially neighborhood socioeconomic status and high school rigor, although several individual hardships are also weighed. This policy made the four institutions, especially the echelons at the most selective departments, more diverse than they otherwise would have been. The rise in geographic, economic and demographic diversity of a student population suggests that the plan's focus on structural determinants of disadvantage yields broad diversity dividends.[40]		Israeli citizens who are; Women, Arabs, Blacks or people with disabilities are entitled to Affirmative Action in the civil service employment.[41] Also Israeli citizens who are Arabs, Blacks or people with disabilities are entitled for Affirmative Actions are entitled for full University tuition scholarships by the state.[42]		Izraeli in her study of gender Politics in Israel showed that the paradox of affirmative action for women directors is that the legitimation for legislating their inclusion on boards also resulted in the exclusion of women's interested as a legitimate issue on the boards' agendas. "The new culture of the men's club is seductive token women are under the pressure to become "social males" and prove that their competence as directors, meaning that they are not significantly different from men. In the negotiation for status as worthy peers, emphasizing gender signals that a woman is an "imposter," someone who does not rightfully belong in the position she is claiming to fill." And once affirmative action for women is fulfilled, and then affirmative action shares the element, as Izareli put it, the "group equality discourse," making it easier for other groups to claim for a fairer distribution of resources. This suggests that affirmative action can have applications for different groups in Israel.[43]		Reservation in India is a form of affirmative action designed to improve the well-being of backward and under-represented communities defined primarily by their caste.		The Malaysian New Economic Policy or NEP serves as a form of affirmative action. Malaysia provides affirmative action to the majority because in general, the Malays have lower income than the Chinese who have traditionally been involved in businesses and industries.[44] Malaysia is a multi-ethnic country, with Malays making up the majority of close to 52% of the population. About 23% of the population are Malaysians of Chinese descent, while Malaysians of Indian descent comprise about 7% of the population. During more than 100 years of British colonization, the Malays were discriminated against employment because the British preferred to bring in influx of migrant workers from China and India.		(See also Bumiputra) The mean income for Malays, Chinese and Indians in 1957/58 were 134, 288 and 228 respectively. In 1967/68 it was 154, 329 and 245, and in 1970 it was 170, 390 and 300. Mean income disparity ratio for Chinese/Malays rose from 2.1 in 1957/58 to 2.3 in 1970, whereas for Indians/Malays the disparity ratio also rose from 1.7 to 1.8 in the same period.[45] The Malays viewed Independence as restoring their proper place in their own country's socioeconomic order while the non-Malays were opposing government efforts to advance Malay political primacy and economic welfare.		In 1981 the Standardization policy of Sri Lankan universities was introduced as an affirmative action program for students from areas which had lower rates of education than other areas due to missionary activity in the north and east, which essentially were the Tamil areas. Successive governments cultivated a historical myth after the colonial powers had left that the British had practised communal favouritism towards Christians and the minority Tamil community for the entire 200 years they had controlled Sri Lanka. However, the Sinhalese in fact benefitted from trade and plantation cultivations over the rest of the other groups and their language and culture as well as the religion of Buddhism was fostered and made into mediums for schools over the Tamil language, which did not have the same treatment and Tamils learned English instead as there was no medium for Tamil until near independence. Tamils' knowledge of English and education came from the very American missionary activity by overseas Christians that the British were concerned will anger the Sinhalese and destroy their trading relationships, so they sent them to the Tamil areas instead to teach, thinking it would have no consequences and due to their small numbers. The British sending the missionaries to the north and east was for the protection of the Sinhalese and in fact showed favouritism to the majority group instead of the minorities to maintain trading relationships and benefits from them. The Tamils, out of this random benefit from learning English and basic education excelled and flourished and were able to take many civil service jobs to the chagrin of the Sinhalese. The myth of Divide and Rule is untrue. The 'policy of standardisation' was typical of affirmative action policies, in that it required drastically lower standards for Sinhalese students than for the more academic Tamils who had to get about ten more marks to enter into universities. The policy, were it not implemented would have prevented the civil wars ahead as the policies had no basis and in fact is an example of discrimination against the Tamil ethnic group.[46]		A 2004 legislation requires that, for a firm with 100 employees or more wishing to compete for government contracts, at least 1 per cent of its employees must be Taiwanese aborigines.[47] Ministry of Education and Council of Aboriginal Affairs announced in 2002 that Taiwanese Aboriginal students would have their high-school or undergraduate entrance exams boosted by 33% for demonstrating some knowledge of their tribal language and culture.[48] The percentage of boost have been revised several times, and the latest percentage is 35% in 2013.[49]		In certain university education programs, including legal and medical education, there are quotas for persons who reach a certain standard of skills in the Swedish language; for students admitted in these quotas, the education is partially arranged in Swedish.[50][51] The purpose of the quotas is to guarantee that a sufficient number of professionals with skills in Swedish are educated for nationwide needs.[50] The quota system has met with criticism from the Finnish speaking majority, some of whom consider the system unfair. In addition to these linguistic quotas, women may get preferential treatment in recruitment for certain public sector jobs if there is a gender imbalance in the field.		No distinctions based on race, religion or sex are allowed under the 1958 French Constitution.[52] Since the 1980s, a French version of affirmative action based on neighborhood is in place for primary and secondary education. Some schools, in neighborhoods labeled "Priority Education Zones", are granted more funds than the others. Students from these schools also benefit from special policies in certain institutions (such as Sciences Po).[53]		The French Ministry of Defence tried in 1990 to make it easier for young French soldiers of North-African descent to be promoted in rank and obtain driving licenses. After a strong protest by a young French lieutenant[54] in the Ministry of Defence newspaper (Armées d'aujourd'hui), the driving license and rank plan was cancelled. After the Sarkozy election, a new attempt in favour of Arab-French students was made, but Sarkozy did not gain enough political support to change the French constitution. However, some French schools do implement affirmative action in that they are obligated to take a certain number of students from impoverished families.[55]		Additionally, following the Norwegian example, after 27 January 2014, women must represent at least 20% of board members in all stock exchange listed or state owned companies. After 27 January 2017, the proportion will increase to 40%. All appointments of males as directors will be invalid as long as the quota is not met, and monetary penalties may apply for other directors.[56]		Article 3 of the German Basic Law provides for equal rights of all people regardless of sex, race or social background. There are programs stating that if men and women have equal qualifications, women have to be preferred for a job; moreover, the disabled should be preferred to non-disabled people. This is typical for all positions in state and university service as of 2007[update], typically using the phrase "We try to increase diversity in this line of work". In recent years, there has been a long public debate about whether to issue programs that would grant women a privileged access to jobs in order to fight discrimination. Germany's Left Party brought up the discussion about affirmative action in Germany's school system. According to Stefan Zillich, quotas should be "a possibility" to help working class children who did not do well in school gain access to a Gymnasium (University-preparatory school).[57] Headmasters of Gymnasien have objected, saying that this type of policy would "be a disservice" to poor children.[58]		In all public stock companies (ASA) boards, either gender should be represented by at least 40%.[59] This affects roughly 400 companies of over 300,000 in total.[60]		Seierstad & Opsahl in their study of the effects of affirmative action on presence, prominence, and social capital of women directors in Norway found that there are few boards chaired by a woman, from the beginning of the implementation of affirmative action policy period to August 2009, the proportion of boards led by a woman has increased from 3.4% to 4.3%. This suggests that the law has had a marginal effect on the sex of the chair and the boards remain internally segregated. Although at the beginning of our observation period, only 7 of 91 prominent directors were women. The gender balance among prominent directors has changed considerable through the period, and at the end of the period, 107 women and 117 men were prominent directors. Interestingly, by applying more restrictive definitions of prominence, the proportion of directors who are women generally increases. If only considering directors with at least three directorships, 61.4% of them are women. When considering directors with seven or more directorships, all of them are women. Thus, affirmative action increase the female population in the director position.[61]		Romani people are allocated quotas for access to public schools and state universities.[62] There is evidence that some ethnic Romanians exploit the system so they can be themselves admitted to universities, which has drawn criticism from Roma representatives.[63]		Quota systems existed in the USSR for various social groups including ethnic minorities (as a compensation of their "cultural backwardness"), women and factory workers.		Quotas for access to university education, offices in the Soviet system and the Communist Party existed: for example, the position of First Secretary of a Soviet Republic's (or Autonomous Republic's) Party Committee was always filled by a representative of this republic's "titular ethnicity".		Modern Russia retains this system partially. Quotas are abolished, however, preferences for some ethnic minorities and inhabitants of certain territories[64] remain.		The Constitutional Court declared in October 2005 that affirmative action i.e. "providing advantages for people of an ethnic or racial minority group" as being against its Constitution.[65]		In the UK, any discrimination, quotas or favouritism due to sex, race and ethnicity among other "protected characteristics" is generally illegal for any reason in education, employment, during commercial transactions, in a private club or association, and while using public services.[6][7][8] The Equality Act 2010 established the principles of equality and their implementation in the UK.[66]		Specific exemptions include:		The equality section of the Canadian Charter of Rights and Freedoms explicitly permits affirmative action type legislation, although the Charter does not require legislation that gives preferential treatment. Subsection 2 of Section 15 states that the equality provisions do "not preclude any law, program or activity that has as its object the amelioration of conditions of disadvantaged individuals or groups including those that are disadvantaged because of race, national or ethnic origin, colour, religion, sex, age or mental or physical disability."		The Canadian Employment Equity Act requires employers in federally-regulated industries to give preferential treatment to four designated groups: Women, people with disabilities, aboriginal people, and visible minorities. Less than one-third of Canadian Universities offer alternative admission requirements for students of aboriginal descent. Some provinces and territories also have affirmative action-type policies. For example, in Northwest Territories in the Canadian north, aboriginal people are given preference for jobs and education and are considered to have P1 status. Non-aboriginal people who were born in the NWT or have resided half of their life there are considered a P2, as well as women and people with disabilities.[69]		The concept of affirmative action was introduced in the early 1960s in the United States, as a way to combat racial discrimination in the hiring process, with the concept later expanded to address gender discrimination.[11] Affirmative action was first created from Executive Order 10925, which was signed by President John F. Kennedy on 6 March 1961 and required that government employers "not discriminate against any employee or applicant for employment because of race, creed, color, or national origin" and "take affirmative action to ensure that applicants are employed, and that employees are treated during employment, without regard to their race, creed, color, or national origin".[70][71]		On 24 September 1965, President Lyndon B. Johnson signed Executive Order 11246, thereby replacing Executive Order 10925 and affirming Federal Government's commitment "to promote the full realization of equal employment opportunity through a positive, continuing program in each executive department and agency".[3] Affirmative action was extended to women by Executive Order 11375 which amended Executive Order 11246 on 13 October 1967, by adding "sex" to the list of protected categories. In the U.S. affirmative action's original purpose was to pressure institutions into compliance with the nondiscrimination mandate of the Civil Rights Act of 1964.[15][72] The Civil Rights Acts do not cover veterans, people with disabilities, or people over 40. These groups are protected from discrimination under different laws.[73]		Affirmative action has been the subject of numerous court cases,[74] and has been questioned upon its constitutional legitimacy. In 2003, a Supreme Court decision regarding affirmative action in higher education (Grutter v. Bollinger, 539 US 244 – Supreme Court 2003) permitted educational institutions to consider race as a factor when admitting students.[5] Alternatively, some colleges use financial criteria to attract racial groups that have typically been under-represented and typically have lower living conditions. Some states such as California (California Civil Rights Initiative), Michigan (Michigan Civil Rights Initiative), and Washington (Initiative 200) have passed constitutional amendments banning public institutions, including public schools, from practicing affirmative action within their respective states. Conservative activists have alleged that colleges quietly use illegal quotas to increase the number of minorities and have launched numerous lawsuits to stop them.[75]		Individuals of Māori or other Polynesian descent are often afforded improved access to university courses, or have scholarships earmarked specifically for them.[6] Affirmative action is provided for under section 73 of the Human Rights Act 1993[76] and section 19(2) of the New Zealand Bill of Rights Act 1990.[77]		Some Brazilian universities (state and federal) have created systems of preferred admissions (quotas) for racial minorities (blacks and Amerindians), the poor and people with disabilities. There are also quotas of up to 20% of vacancies reserved for people with disabilities in the civil public services.[78] The Democrats party, accusing the board of directors of the University of Brasília of "Nazism", appealed to the Supreme Federal Court against the constitutionality of the quotas the University reserves for minorities.[79] The Supreme Court unanimously approved their constitutionality on 26 April 2012.[80]		The International Convention on the Elimination of All Forms of Racial Discrimination stipulates (in Article 2.2) that affirmative action programs may be required of countries that ratified the convention, in order to rectify systematic discrimination. It states, however, that such programs "shall in no case entail as a consequence the maintenance of unequal or separate rights for different racial groups after the objectives for which they were taken have been achieved."[81]		The United Nations Human Rights Committee states that "the principle of equality sometimes requires States parties to take affirmative action in order to diminish or eliminate conditions which cause or help to perpetuate discrimination prohibited by the Covenant. For example, in a State where the general conditions of a certain part of the population prevent or impair their enjoyment of human rights, the State should take specific action to correct those conditions. Such action may involve granting for a time to the part of the population concerned certain preferential treatment in specific matters as compared with the rest of the population. However, as long as such action is needed to correct discrimination, in fact, it is a case of legitimate differentiation under the Covenant."[81]		The principle of affirmative action is to promote societal equality through the preferential treatment of socioeconomically disadvantaged people. Often, these people are disadvantaged for historical reasons, such as oppression or slavery.[82] Historically and internationally, support for affirmative action has sought to achieve a range of goals: bridging inequalities in employment and pay; increasing access to education; enriching state, institutional, and professional leadership with the full spectrum of society; redressing apparent past wrongs, harms, or hindrances, in particular addressing the apparent social imbalance left in the wake of slavery and slave laws.		A 2017 study found that affirmative action in the United States "increases the black share of employees over time: in 5 years after an establishment is first regulated, the black share of employees increases by an average of 0.8 percentage points. Strikingly, the black share continues to grow at a similar pace even after an establishment is deregulated. I argue that this persistence is driven in part by affirmative action inducing employers to improve their methods for screening potential hires."[83]		According to a poll taken by USA Today in 2005, majority of Americans support affirmative action for women, while views on minority groups were more split.[84] Men are only slightly more likely to support affirmative action for women; though a majority of both do.[84] However, a slight majority of Americans do believe that affirmative action goes beyond ensuring access and goes into the realm of preferential treatment.[84] More recently, a Quinnipiac poll from June 2009 finds that 55% of Americans feel that affirmative action in general should be discontinued, though 55% support it for people with disabilities.[85] A Gallup poll from 2005 showed that 72% of black Americans and 44% of white Americans supported racial affirmative action (with 21% and 49% opposing), with support and opposition among Hispanics falling between those of blacks and whites. Support among blacks, unlike among whites, had almost no correlation with political affiliation.[86]		A 2009 Quinnipiac University Polling Institute survey found 65% of American voters opposed the application of affirmative action to gay people, with 27% indicating they supported it.[87]		A Leger poll taken in 2010 found 59% of Canadians opposed considering race, gender, or ethnicity when hiring for government jobs.[88]		A 2014 Pew Research Center poll found that 63% of Americans thought affirmative action programs aimed at increasing minority representation on college campuses were "a good thing", compared to 30% who thought they were "a bad thing".[89] The following year, Gallup released a poll showing that 67% of Americans supported affirmative action programs aimed at increasing female representation, compared to 58% who supported such programs aimed at increasing the representation of racial minorities.[90]		Opponents of affirmative action such as George Sher believe that affirmative action devalues the accomplishments of people who are chosen based on the social group to which they belong rather than their qualifications, thus rendering affirmative action counterproductive.[91] Opponents,[92] who sometimes say that affirmative action is reverse discrimination, further claim that affirmative action has undesirable side-effects in addition to failing to achieve its goals. They argue that it hinders reconciliation, replaces old wrongs with new wrongs, undermines the achievements of minorities, and encourages individuals to identify themselves as disadvantaged, even if they are not. It may increase racial tension and benefit the more privileged people within minority groups at the expense of the least fortunate within majority groups (such as lower-class white people).[93]		Opponents claim that cases such as Fisher v. University of Texas are few of the many examples that show how reverse discrimination can take place. In 2008, Abigail Fisher, who is a native to Texas, sued the University of Texas at Austin, claiming that she was denied admission to the university because she was "white". The students that are of top 10% in the applicants of the University of Texas are admitted and there are students that compete to barely make it in on the threshold, such as Abigail Fisher. In such cases, race becomes an important factor in deciding who gets admitted to the university, and Fisher argued that discriminating and accepting students according to their race is a violation of the Equal Protection Clause of the Fourteenth Amendment, which ensures equal protection of the law and the citizen's privilege as a citizen of United States. The constitutionality of affirmative action in college admissions is now back before the Supreme Court in the case Fisher v. University of Texas (2016).[94]		American economist and social and political commentator, Thomas Sowell identified some negative results of race-based affirmative action in his book, Affirmative Action Around the World: An Empirical Study.[95] Sowell writes that affirmative action policies encourage non-preferred groups to designate themselves as members of preferred groups (i.e., primary beneficiaries of affirmative action) to take advantage of group preference policies; that they tend to benefit primarily the most fortunate among the preferred group (e.g., upper and middle class blacks), often to the detriment of the least fortunate among the non-preferred groups (e.g., poor whites or Asians); that they reduce the incentives of both the preferred and non-preferred to perform at their best – the former because doing so is unnecessary and the latter because it can prove futile – thereby resulting in net losses for society as a whole; and that they increase animosity toward preferred groups.		Mismatching is the term given to the negative effect that affirmative action has when it places a student into a college that is too difficult for him or her. For example, according to the theory, in the absence of affirmative action, a student will be admitted to a college that matches his or her academic ability and have a good chance of graduating. However, according to the mismatching theory, affirmative action often places a student into a college that is too difficult, and this increases the student's chance of dropping out. Thus, according to the theory, affirmative action hurts its intended beneficiaries, because it increases their dropout rate.[96][97]		Evidence in support of the mismatching theory was presented by Gail Heriot, a professor of law at the University of San Diego and a member of the U.S. Commission on Civil Rights, in a 24 August 2007 article published in the Wall Street Journal. The article reported on a 2004 study that was conducted by UCLA law professor Richard Sander and published in the Stanford Law Review. The study concluded that there were 7.9% fewer black attorneys than there would have been if there had been no affirmative action. The study was titled, "A Systemic Analysis of Affirmative Action in American Law Schools."[98] The article also states that because of mismatching, blacks are more likely to drop out of law school and fail bar exams.[99]		Sander's paper on mismatching has been criticized by several law professors, including Ian Ayres and Richard Brooks from Yale who argue that eliminating affirmative action would actually reduce the number of black lawyers by 12.7%.[100] A 2008 study by Jesse Rothstein and Albert H. Yoon confirmed Sander's mismatch findings, but also found that eliminating affirmative action would "lead to a 63 percent decline in black matriculants at all law schools and a 90 percent decline at elite law schools."[101] These high numbers predictions were doubted in a review of previous studies by Peter Arcidiacono and Michael Lovenheim. Their 2016 article found a strong indication that affirmative action results in a mismatch effect. They argued that the attendance by some African-American students to less-selective schools would significantly improve the low first attempt rate at passing the state bar, but they cautioned that such improvements could be outweighed by decreases in law school attendance.[102]		A 2011 study proposed that mismatch can only occur when a selective school possesses private information that, had this information been disclosed, would have changed the student's choice of school. The study found that this is in fact the case for Duke University, and that this information predicts the student's academic performance after beginning college.[103]		A 2016 study on affirmative action in India fails to find evidence for the mismatching hypothesis.[104]		
A schedule, often called a rota or roster, is a list of employees, and associated information e.g. location, working times, responsibilities for a given time period e.g. week, month or sports season.		A schedule is necessary for the day-to-day operation of many businesses e.g. retail store, manufacturing facility and some offices. The process of creating a schedule is called scheduling. An effective workplace schedule balances the needs of stakeholders such as management, employees and customers.		A daily schedule is usually ordered chronologically, which means the first employees working that day are listed at the top, followed by the employee who comes in next, etc. A weekly or monthly schedule is usually ordered alphabetically, employees being listed on the left hand side of a grid, with the days of the week on the top of the grid.[citation needed] In shift work, a schedule usually employs a recurring shift plan.		A schedule is most often created by a manager. In larger operations, a human resources manager or scheduling specialist may be solely dedicated to creating and maintaining the schedule. A schedule by this definition is sometimes referred to as workflow.[citation needed]		Software is often used to enable organizations to better manage staff scheduling. Organizations commonly use spreadsheet software or employee scheduling software to create and manage shifts, assignments, and employee preferences. For large organisations employee scheduling can be complex,[citation needed] and optimising this is framed as the Nurse scheduling problem in Operations Research.[1] Advanced employee scheduling software also provides ways to connect with the staff, ask for their preferences and communicate the schedule to them.[2]		An On call shift or On-call scheduling is a practice which requires employees to be available to be called onto last-minute shifts without pre-scheduling.[3] In the United States, the practice has been opposed by labor rights groups as "unfair and detrimental to employees."[3]				
Refusal of work is behavior which refuses to adapt to regular employment.[1]		As actual behavior, with or without a political or philosophical program, it has been practiced by various subcultures and individuals. Radical political positions have openly advocated refusal of work. From within Marxism it has been advocated by Paul Lafargue and the Italian workerist/autonomists (e.g. Antonio Negri, Mario Tronti),[1] the French ultra-left (e.g. Échanges et Mouvement); and within anarchism (especially Bob Black and the post-left anarchy tendency).[2]						International human rights law does not recognize the refusal of work or right not to work by itself except the right to strike. However the Abolition of Forced Labour Convention adopted by International Labour Organization in 1957 prohibits all forms of forced labour.[3]		Wage slavery refers to a situation where a person's livelihood depends on wages, especially when the dependence is total and immediate.[4][5] It is a negatively connoted term used to draw an analogy between slavery and wage labor, and to highlight similarities between owning and employing a person. The term 'wage slavery' has been used to criticize economic exploitation and social stratification, with the former seen primarily as unequal bargaining power between labor and capital (particularly when workers are paid comparatively low wages, e.g. in sweatshops),[6] and the latter as a lack of workers' self-management.[7][8][9] The criticism of social stratification covers a wider range of employment choices bound by the pressures of a hierarchical social environment (i.e. working for a wage not only under threat of starvation or poverty, but also of social stigma or status diminution).[10][11][12]		Similarities between wage labor and slavery were noted at least as early as Cicero.[13] Before the American Civil War, Southern defenders of African American slavery invoked the concept to favorably compare the condition of their slaves to workers in the North.[14][15] With the advent of the industrial revolution, thinkers such as Proudhon and Marx elaborated the comparison between wage labor and slavery in the context of a critique of property not intended for active personal use.[16][17]		The introduction of wage labor in 18th century Britain was met with resistance – giving rise to the principles of syndicalism.[18][19][20][21] Historically, some labor organizations and individual social activists, have espoused workers' self-management or worker cooperatives as possible alternatives to wage labor.[8][20]		The Right to be Lazy is an essay by Cuban-born French revolutionary Marxist Paul Lafargue, written from his London exile in 1880. The essay polemicizes heavily against then-contemporary liberal, conservative, Christian and even socialist ideas of work. Lafargue criticizes these ideas from a Marxist perspective as dogmatic and ultimately false by portraying the degeneration and enslavement of human existence when being subsumed under the primacy of the "right to work", and argues that laziness, combined with human creativity, is an important source of human progress.		He manifests that "When, in our civilized Europe, we would find a trace of the native beauty of man, we must go seek it in the nations where economic prejudices have not yet uprooted the hatred of work...The Greeks in their era of greatness had only contempt for work: their slaves alone were permitted to labor: the free man knew only exercises for the body and mind...The philosophers of antiquity taught contempt for work, that degradation of the free man, the poets sang of idleness, that gift from the Gods."[22] And so he says "Proletarians, brutalized by the dogma of work, listen to the voice of these philosophers, which has been concealed from you with jealous care: A citizen who gives his labor for money degrades himself to the rank of slaves." (The last sentence a quote from Cicero.[13])		However, Marx himself condemned these ideas (see main article on Paul Lafargue )		Raoul Vaneigem, important theorist of the post-surrealist Situationist International which was influential in the May 68 events in France, wrote The Book of Pleasures. In it he says that "You reverse the perspective of power by returning to pleasure the energies stolen by work and constraint...As sure as work kills pleasure, pleasure kills work. If you are not resigned to dying of disgust, then you will be happy enough to rid your life of the odious need to work, to give orders (and obey them), to lose and to win, to keep up appearances, and to judge and be judged."[23]		Autonomist philosopher Bifo defines refusal of work as not "so much the obvious fact that workers do not like to be exploited, but something more. It means that the capitalist restructuring, the technological change, and the general transformation of social institutions are produced by the daily action of withdrawal from exploitation, of rejection of the obligation to produce surplus value, and to increase the value of capital, reducing the value of life."[1] More simply he states "Refusal of work means...I don’t want to go to work because I prefer to sleep. But this laziness is the source of intelligence, of technology, of progress. Autonomy is the self-regulation of the social body in its independence and in its interaction with the disciplinary norm."[1]		As a social development Bifo remembers "that one of the strong ideas of the movement of autonomy proletarians during the 70s was the idea "precariousness is good". Job precariousness is a form of autonomy from steady regular work, lasting an entire life. In the 1970s many people used to work for a few months, then to go away for a journey, then back to work for a while. This was possible in times of almost full employment and in times of egalitarian culture. This situation allowed people to work in their own interest and not in the interest of capitalists, but quite obviously this could not last forever, and the neoliberal offensive of the 1980s was aimed to reverse the rapport de force."[1] As a response to these developments his view is that "the dissemination of self-organized knowledge can create a social framework containing infinite autonomous and self-reliant worlds."[1]		From this possibility of self-determination even the notion of Workers' self-management is seen as problematic since "Far from the emergence of proletarian power, ...this self-management as a moment of the self-harnessing of the workers to capitalist production in the period of real subsumption... Mistaking the individual capitalist (who, in real subsumption disappears into the collective body of share ownership on one side, and hired management on the other) rather than the enterprise as the problem, ... the workers themselves became a collective capitalist, taking on responsibility for the exploitation of their own labor. Thus, far from breaking with 'work',...the workers maintained the practice of clocking-in, continued to organize themselves and the community around the needs of the factory, paid themselves from profits arising from the sale of watches, maintained determined relations between individual work done and wage, and continued to wear their work shirts throughout the process."[24]		André Gorz was an Austrian and French social philosopher. Also a journalist, he co-founded Le Nouvel Observateur weekly in 1964. A supporter of Jean-Paul Sartre's existentialist version of Marxism after World War Two, in the aftermath of the May '68 student riots, he became more concerned with political ecology. His central theme was wage labour issues such as liberation from work, the just distribution of work, social alienation, and a guaranteed basic income.[25] Among his works critical of work and the work ethic include Critique de la division du travail (Seuil, 1973. Collective work), Farewell to the Working Class (1980 – Galilée and Le Seuil, 1983, Adieux au Prolétariat), Critique of Economic Reason (Verso, 1989 first published 1988) and Reclaiming Work: Beyond the Wage-Based Society (1999).		The Abolition of Work, Bob Black's most widely read essay, draws upon the ideas of Charles Fourier, William Morris, Herbert Marcuse, Paul Goodman, and Marshall Sahlins. In it he argues for the abolition of the producer- and consumer-based society, where, Black contends, all of life is devoted to the production and consumption of commodities. Attacking Marxist state socialism as much as market capitalism, Black argues that the only way for humans to be free is to reclaim their time from jobs and employment, instead turning necessary subsistence tasks into free play done voluntarily – an approach referred to as "ludic". The essay argues that "no-one should ever work", because work – defined as compulsory productive activity enforced by economic or political means – is the source of most of the misery in the world. Black denounces work for its compulsion, and for the forms it takes – as subordination to a boss, as a "job" which turns a potentially enjoyable task into a meaningless chore, for the degradation imposed by systems of work-discipline, and for the large number of work-related deaths and injuries – which Black typifies as "homicide". He views the subordination enacted in workplaces as "a mockery of freedom", and denounces as hypocrites the various theorists who support freedom while supporting work. Subordination in work, Black alleges, makes people stupid and creates fear of freedom. Because of work, people become accustomed to rigidity and regularity, and do not have the time for friendship or meaningful activity. Most workers, he states, are dissatisfied with work (as evidenced by petty deviance on the job), so that what he says should be uncontroversial; however, it is controversial only because people are too close to the work-system to see its flaws.		Play, in contrast, is not necessarily rule-governed, and is performed voluntarily, in complete freedom, as a gift economy. He points out that hunter-gatherer societies are typified by play, a view he backs up with the work of Marshall Sahlins; he recounts the rise of hierarchal societies, through which work is cumulatively imposed, so that the compulsive work of today would seem incomprehensibly oppressive even to ancients and medieval peasants. He responds to the view that "work," if not simply effort or energy, is necessary to get important but unpleasant tasks done, by claiming that first of all, most important tasks can be rendered ludic, or "salvaged" by being turned into game-like and craft-like activities, and secondly that the vast majority of work does not need doing at all. The latter tasks are unnecessary because they only serve functions of commerce and social control that exist only to maintain the work-system as a whole. As for what is left, he advocates Charles Fourier's approach of arranging activities so that people will want to do them. He is also skeptical but open-minded about the possibility of eliminating work through labor-saving technologies. He feels the left cannot go far enough in its critiques because of its attachment to building its power on the category of workers, which requires a valorization of work.		The anti-work ethic states that labor tends to cause unhappiness, therefore, the quantity of labor ought to be lessened. The ethic appeared in anarchist circles and have come to prominence with essays such as In Praise of Idleness and Other Essays by Bertrand Russell, The Right to Useful Unemployment by Ivan Illich, and The Abolition of Work by Bob Black,[26] published in 1985.		Friedrich Nietzsche was a notable philosopher who presented a critique of work and an anti-work ethic. In 1881, he wrote:		The eulogists of work. Behind the glorification of 'work' and the tireless talk of the 'blessings of work' I find the same thought as behind the praise of impersonal activity for the public benefit: the fear of everything individual. At bottom, one now feels when confronted with work – and what is invariably meant is relentless industry from early till late – that such work is the best police, that it keeps everybody in harness and powerfully obstructs the development of reason, of covetousness, of the desire for independence. For it uses up a tremendous amount of nervous energy and takes it away from reflection, brooding, dreaming, worry, love, and hatred; it always sets a small goal before one's eyes and permits easy and regular satisfactions. In that way a society in which the members continually work hard will have more security: and security is now adored as the supreme goddess..."		The Idler is a bi-yearly British magazine devoted to promoting its ethos of 'idle living' and all that entails. It was founded in 1993 by Tom Hodgkinson and Gavin Pretor-Pinney with the intention of exploring alternative ways of working and living.[27]		The term slacker is commonly used to refer to a person who avoids work (especially British English), or (primarily in North American English) an educated person who is viewed as an underachiever.[28][29]		While use of the term slacker dates back to about 1790 or 1898 depending on the source, it gained some recognition during the British Gezira Scheme, when Sudanese labourers protested their relative powerlessness by working lethargically, a form of protest known as 'slacking'.[30] The term achieved a boost in popularity after its use in the films Back to the Future by Robert Zemeckis and Richard Linklater's Slacker.[28][31]		NEET is an acronym for the government classification for people currently "Not in Employment, Education or Training". It was first used in the United Kingdom but its use has spread to other countries, including Japan, China, and South Korea.		In the United Kingdom, the classification comprises people aged between 16 and 24 (some 16-year-olds are still of compulsory school age). In Japan, the classification comprises people aged between 15 and 34 who are unemployed, unmarried, not enrolled in school or engaged in housework, and not seeking work or the technical training needed for work. The "NEET group" is not a uniform set of individuals but consists of those who will be NEET for a short time while essentially testing out a variety of opportunities and those who have major and often multiple issues and are at long term risk of remaining disengaged.		In Brazil, "nem-nem" (short of nem estudam nem trabalham (neither working nor studying) is a term with similar meaning.[32]		In Mexico, "Ni-Ni" (short of Ni estudia Ni trabaja) is also applied.		Freeter (フリーター, furītā) (other spellings below) is a Japanese expression for people between the age of 15 and 34 who lack full-time employment or are unemployed, excluding homemakers and students. They may also be described as underemployed or freelance workers. These people do not start a career after high school or university but instead usually live as so-called parasite singles with their parents and earn some money with low skilled and low paid jobs.		The word freeter or freeta was first used around 1987 or 1988 and is thought to be an amalgamation of the English word free (or perhaps freelance) and the German word Arbeiter ("worker").[33]		Parasite single (パラサイトシングル, parasaito shinguru) is a Japanese term for a single person who lives with their parents until their late twenties or early thirties in order to enjoy a carefree and comfortable life. In English, the expression "sponge" or "basement dweller" may sometimes be used.		The expression is mainly used in reference to Japanese society, but similar phenomena can also be found in other countries worldwide. In Italy, 30-something singles still relying on their mothers are joked about, being called Bamboccioni (literally: grown-up babies) and in Germany they are known as Nesthocker (German for an altricial bird), who are still living at Hotel Mama.		Such behaviour is considered normal in Greece, both because of the traditional strong family ties and because of the low wages.[34]		A vagrant is a person in a situation of poverty, who wanders from place to place without a home or regular employment or income. Many towns in the developed world have shelters for vagrants. Common terminology is a tramp or a 'gentleman of the road'.		Laws against vagrancy in the United States have partly been invalidated as violative of the due process clauses of the U.S. Constitution.[35] However, the FBI report on crime in the United States for 2005 lists 24,359 vagrancy violations.[36]		Cynicism (Greek: κυνισμός), in its original form, refers to the beliefs of an ancient school of Greek philosophers known as the Cynics (Greek: Κυνικοί, Latin: Cynici). Their philosophy was that the purpose of life was to live a life of Virtue in agreement with Nature. This meant rejecting all conventional desires for wealth, power, health, and fame, and by living a simple life free from all possessions. They believed that the world belonged equally to everyone, and that suffering was caused by false judgments of what was valuable and by the worthless customs and conventions which surrounded society. The first philosopher to outline these themes was Antisthenes, who had been a pupil of Socrates in the late 5th century BCE. He was followed by Diogenes of Sinope, who lived in a tub on the streets of Athens. Diogenes took Cynicism to its logical extremes, and came to be seen as the archetypal Cynic philosopher. He was followed by Crates of Thebes who gave away a large fortune so he could live a life of Cynic poverty in Athens. Cynicism spread with the rise of Imperial Rome in the 1st century, and Cynics could be found begging and preaching throughout the cities of the Empire. It finally disappeared in the late 5th century, although many of its ascetic and rhetorical ideas were adopted by early Christianity. The name Cynic derives from the Greek word κυνικός, kynikos, "dog-like" and that from κύων, kyôn, "dog" (genitive: kynos).[37] It seems certain that the word dog was also thrown at the first Cynics as an insult for their shameless rejection of conventional manners, and their decision to live on the streets. Diogenes, in particular, was referred to as the Dog.[38]		In Hinduism, sadhu is a common term for a mystic, an ascetic, practitioner of yoga (yogi) and/or wandering monks. The sadhu is solely dedicated to achieving the fourth and final Hindu goal of life, moksha (liberation), through meditation and contemplation of Brahman. Sadhus often wear ochre-colored clothing, symbolizing renunciation.		A hobo is a migratory worker or homeless vagabond, often penniless.[39] The term originated in the western—probably northwestern—United States during the last decade of the 19th century.[40] Unlike tramps, who worked only when they were forced to, and bums, who didn't work at all, hobos were workers who wandered.[40][41]		In British English and traditional American English usage, a tramp is a long term homeless person who travels from place to place as an itinerant vagrant, traditionally walking or hiking all year round.		While some tramps may do odd jobs from time to time, unlike other temporarily homeless people they do not seek out regular work and support themselves by other means such as begging or scavenging. This is in contrast to:		Both terms, "tramp" and "hobo" (and the distinction between them), were in common use between the 1880s and the 1940s. Their populations and the usage of the terms increased during the Great Depression.		Like "hobo" and "bum," the word "tramp" is considered vulgar in American English usage, having been subsumed in more polite contexts by words such as "homeless person" or "vagrant." In colloquial American English, the word "tramp" can also mean a sexually promiscuous female or even prostitute.		Tramps used to be known euphemistically in England and Wales as "gentlemen of the road."		Tramp is derived from the Middle English as a verb meaning to "walk with heavy footsteps", and to go hiking.[42] Bart Kennedy, a self-described tramp of 1900 US, once said "I listen to the tramp, tramp of my feet, and wonder where I was going, and why I was going."[43]		A gutter punk is a homeless or transient individual, often through means of freighthopping or hitchhiking. Gutter punks are often juveniles who are in some way associated with the anarcho-punk subculture.[44] In certain regions, gutter punks are notorious for panhandling and often display cardboard signs that make statements about their lifestyles.[44] Gutter punks are generally characterized as being voluntarily unemployed.[44]		
Gold-collar worker (GCW) is a neologism which has been used to describe either young, low-wage workers who invest in conspicuous luxury,[1] or highly skilled knowledge workers, traditionally classified as white collar, but who have recently become essential enough to business operations as to warrant a new classification.						These are 18- to 25-year-old persons in a position to divert a significant portion of their earnings towards material luxuries. They typically have fewer than 2 years of post-high school education. Like their counterparts attending college, they are often employed as retail workers or in the hospitality industry, particularly food service as servers. This group tends to have more disposable income than college students, who often pay high tuition costs, take on a number of financial loans, and often move away from their parents. A lack of financial obligations leaves young people in this situation with a higher level of discretionary/disposable income, which they use to finance luxury goods. Thus, the term also carries a connotation of immaturity, the extension of youth, and nurtured adolescence, as in the movie Failure to Launch, where a man still lives with his parents despite being well into his 30s and owning a business. In the UK the expression 'two-bob billionaire' is used, in that whilst one feels wealthy and hardworking, one's status is in fact illusory.		The main challenge faced by gold-collar workers is the short-lived nature of their financial security. More often than not, these people marry and have children, and take on additional financial responsibilities such as mortgages and health insurance. With partial or no higher education, however, their job prospects could be viewed as narrow and fairly restricted.		Gold Collar often refers to entry level workers (or unskilled workers from middle-class families), in their twenties who want flex time hours, self-control, independence, empowerment, to furnish their own offices, a signing bonus, full tuition reimbursement, flex benefits, to work as a team, casual Friday every day, to work from home, to have fun, and don't want to feel loyal to the company.[2]		It has been reported that the term 'Gold-Collar worker' was first used by Robert Earl Kelley in his 1985 book The Gold-Collar Worker: Harnessing the Brainpower of the New Work Force.[3] Here he discussed a new generation of workers who use American business' most important resource, brainpower. A quote from the book summary states, "They are a new breed of workers, and they demand a new kind of management. Intelligent, independent, and innovative, these employees are incredibly valuable. They are scientists and mathematicians, doctors and lawyers, engineers and computer programmers, stock analysts and even community planners. They are as distinct from their less skilled white-collar counterparts—bank tellers, bookkeepers, clerks, and other business functionaries—as they are from blue-collar laborers. And they account for over 40 percent of America's workforce."[3]		The color gold applies to these workers because they are highly skilled.[4] When Kelley's book was published in 1985, these were typically understood as being young, college-educated, and specialized.		
Employment is a relationship between two parties, usually based on a contract where work is paid for, where one party, which may be a corporation, for profit, not-for-profit organization, co-operative or other entity is the employer and the other is the employee.[1] Employees work in return for payment, which may be in the form of an hourly wage, by piecework or an annual salary, depending on the type of work an employee does or which sector she or he is working in. Employees in some fields or sectors may receive gratuities, bonus payment or stock options. In some types of employment, employees may receive benefits in addition to payment. Benefits can include health insurance, housing, disability insurance or use of a gym. Employment is typically governed by employment laws or regulations or legal contracts.						An employee contributes labor and expertise to an endeavor of an employer or of a person conducting a business or undertaking (PCBU)[2] and is usually hired to perform specific duties which are packaged into a job. In a corporate context, an employee is a person who is hired to provide services to a company on a regular basis in exchange for compensation and who does not provide these services as part of an independent business.[3]		Employer and managerial control within an organization rests at many levels and has important implications for staff and productivity alike, with control forming the fundamental link between desired outcomes and actual processes. Employers must balance interests such as decreasing wage constraints with a maximization of labor productivity in order to achieve a profitable and productive employment relationship.		The main ways for employers to find workers and for people to find employers are via jobs listings in newspapers (via classified advertising) and online, also called job boards. Employers and job seekers also often find each other via professional recruitment consultants which receive a commission from the employer to find, screen and select suitable candidates. However, a study has shown that such consultants may not be reliable when they fail to use established principles in selecting employees.[1] A more traditional approach is with a "Help Wanted" sign in the establishment (usually hung on a window or door[4] or placed on a store counter).[3] Evaluating different employees can be quite laborious but setting up different techniques to analyze their skill to measure their talents within the field can be best through assessments.[5] Employer and potential employee commonly take the additional step of getting to know each other through the process of job interview.		Training and development refers to the employer's effort to equip a newly hired employee with necessary skills to perform at the job, and to help the employee grow within the organization. An appropriate level of training and development helps to improve employee's job satisfaction.[6]		There are many ways that employees are paid, including by hourly wages, by piecework, by yearly salary, or by gratuities (with the latter often being combined with another form of payment). In sales jobs and real estate positions, the employee may be paid a commission, a percentage of the value of the goods or services that they have sold. In some fields and professions (e.g., executive jobs), employees may be eligible for a bonus if they meet certain targets. Some executives and employees may be paid in stocks or stock options, a compensation approach that has the added benefit, from the company's point of view, of helping to align the interests of the compensated individual with the performance of the company.		Employee benefits are various non-wage compensation provided to employee in addition to their wages or salaries. The benefits can include: housing (employer-provided or employer-paid), group insurance (health, dental, life etc.), disability income protection, retirement benefits, daycare, tuition reimbursement, sick leave, vacation (paid and non-paid), social security, profit sharing, funding of education, and other specialized benefits. In some cases, such as with workers employed in remote or isolated regions, the benefits may include meals. Employee benefits can improve the relationship between employee and employer and lowers staff turnover.[7]		Organizational justice is an employee's perception and judgement of employer's treatment in the context of fairness or justice. The resulting actions to influence the employee-employer relationship is also a part of organizational justice.[7]		Employees can organize into trade or labor unions, which represent the work force to collectively bargain with the management of organizations about working, and contractual conditions and services.[8]		Usually, either an employee or employer may end the relationship at any time, often subject to a certain notice period. This is referred to as at-will employment. The contract between the two parties specifies the responsibilities of each when ending the relationship and may include requirements such as notice periods, severance pay, and security measures.[8] In some professions, notably teaching, civil servants, university professors, and some orchestra jobs, some employees may have tenure, which means that they cannot be dismissed at will. Another type of termination is a layoff.		Wage labor is the socioeconomic relationship between a worker and an employer, where the worker sells their labor under a formal or informal employment contract. These transactions usually occur in a labor market where wages are market determined.[6][7] In exchange for the wages paid, the work product generally becomes the undifferentiated property of the employer, except for special cases such as the vesting of intellectual property patents in the United States where patent rights are usually vested in the original personal inventor. A wage laborer is a person whose primary means of income is from the selling of his or her labor in this way.[8]		In modern mixed economies such as that of the OECD countries, it is currently the dominant form of work arrangement. Although most work occurs following this structure, the wage work arrangements of CEOs, professional employees, and professional contract workers are sometimes conflated with class assignments, so that "wage labor" is considered to apply only to unskilled, semi-skilled or manual labor.[9]		Wage labor, as institutionalized under today's market economic systems, has been criticized,[8] especially by both mainstream socialists and anarcho-syndicalists,[9][10][11][12] using the pejorative term wage slavery.[13][14] Socialists draw parallels between the trade of labor as a commodity and slavery. Cicero is also known to have suggested such parallels.[15]		The American philosopher John Dewey posited that until "industrial feudalism" is replaced by "industrial democracy", politics will be "the shadow cast on society by big business".[16] Thomas Ferguson has postulated in his investment theory of party competition that the undemocratic nature of economic institutions under capitalism causes elections to become occasions when blocs of investors coalesce and compete to control the state.[17]		Australian employment has been governed by the Fair Work Act since 2009.[18]		Bangladesh Association of International Recruiting Agencies (BAIRA) is an association of national level with its international reputation of co-operation and welfare of the migrant workforce as well as its approximately 1200 members agencies in collaboration with and support from the Government of Bangladesh.[9]		In the Canadian province of Ontario, formal complaints can be brought to the Ministry of Labour. In the province of Quebec, grievances can be filed with the Commission des normes du travail.[12]		Pakistan has Contract Labor, Minimum Wage and Provident Funds Acts. Contract labor in Pakistan must be paid minimum wage and certain facilities are to be provided to labor. However, the Acts are not yet fully implemented.[9]		India has Contract Labor, Minimum Wage, Provident Funds Act and various other acts to comply with. Contract labor in India must be paid minimum wage and certain facilities are to be provided to labor. However, there is still a large amount of work that remains to be done to fully implement the Act.[12]		In the Philippines, private employment is regulated under the Labor Code of the Philippines by the Department of Labor and Employment.[19]		In the United Kingdom, employment contracts are categorized by the government into the following types:[20]		For purposes of U.S. federal income tax withholding, 26 U.S.C. § 3401(c) provides a definition for the term "employee" specific to chapter 24 of the Internal Revenue Code:		"For purposes of this chapter, the term “employee” includes an officer, employee, or elected official of the United States, a State, or any political subdivision thereof, or the District of Columbia, or any agency or instrumentality of any one or more of the foregoing. The term “employee” also includes an officer of a corporation."[21] This definition does not exclude all those who are commonly known as 'employees'. “Similarly, Latham’s instruction which indicated that under 26 U.S.C. § 3401(c) the category of ‘employee’ does not include privately employed wage earners is a preposterous reading of the statute. It is obvious that within the context of both statutes the word ‘includes’ is a term of enlargement not of limitation, and the reference to certain entities or categories is not intended to exclude all others.”[22]		Employees are often contrasted with independent contractors, especially when there is dispute as to the worker's entitlement to have matching taxes paid, workers compensation, and unemployment insurance benefits. However, in September 2009, the court case of Brown v. J. Kaz, Inc. ruled that independent contractors are regarded as employees for the purpose of discrimination laws if they work for the employer on a regular basis, and said employer directs the time, place, and manner of employment.[19]		In non-union work environments, in the United States, unjust termination complaints can be brought to the United States Department of Labor.[23]		Labor unions are legally recognized as representatives of workers in many industries in the United States. Their activity today centers on collective bargaining over wages, benefits, and working conditions for their membership, and on representing their members in disputes with management over violations of contract provisions. Larger unions also typically engage in lobbying activities and electioneering at the state and federal level.[19]		Most unions in America are aligned with one of two larger umbrella organizations: the AFL-CIO created in 1955, and the Change to Win Federation which split from the AFL-CIO in 2005. Both advocate policies and legislation on behalf of workers in the United States and Canada, and take an active role in politics. The AFL-CIO is especially concerned with global trade issues.[17]		According to Swedish law,[24] there are three types of employment.		There are no laws about minimum salary in Sweden. Instead there are agreements between employer organizations and trade unions about minimum salaries, and other employment conditions.		There is a type of employment contract which is common but not regulated in law, and that is Hour employment (swe: Timanställning), which can be Normal employment (unlimited), but the work time is unregulated and decided per immediate need basis. The employee is expected to be answering the phone and come to work when needed, e.g. when someone is ill and absent from work. They will receive salary only for actual work time and can in reality be fired for no reason by not being called anymore. This type of contract is common in the public sector.[25]		Young workers are at higher risk for occupational injury and face certain occupational hazards at a higher rate; this is generally due to their employment in high-risk industries. For example, in the United States, young people are injured at work at twice the rate of their older counterparts.[26] These workers are also at higher risk for motor vehicle accidents at work, due to less work experience, a lower use of seatbelts, and higher rates of distracted driving.[27][28] To mitigate this risk, those under the age of 17 are restricted from certain types of driving, including transporting people and goods under certain circumstances.[27]		High-risk industries for young workers include agriculture, restaurants, waste management, and mining.[26][27] In the United States, those under the age of 18 are restricted from certain jobs that are deemed dangerous under the Fair Labor Standards Act.[27]		Youth employment programs are most effective when they include both theoretical classroom training and hands-on training with work placements.[29]		Those older than the statutory defined retirement age may continue to work, either out of enjoyment or necessity. However, depending on the nature of the job, older workers may need to transition into less-physical forms of work to avoid injury. Working past retirement age also has positive effects, because it gives a sense of purpose and allows people to maintain social networks and activity levels.[30] Older workers are often found to be discriminated against by employers.[31]		Employment is no guarantee of escaping poverty, the International Labour Organization (ILO) estimates that as many as 40% of workers are poor, not earning enough to keep their families above the $2 a day poverty line.[25] For instance, in India most of the chronically poor are wage earners in formal employment, because their jobs are insecure and low paid and offer no chance to accumulate wealth to avoid risks.[25] According to the UNRISD, increasing labor productivity appears to have a negative impact on job creation: in the 1960s, a 1% increase in output per worker was associated with a reduction in employment growth of 0.07%, by the first decade of this century the same productivity increase implies reduced employment growth by 0.54%.[25] Both increased employment opportunities and increased labor productivity (as long as it also translates into higher wages) are needed to tackle poverty. Increases in employment without increases in productivity leads to a rise in the number of "working poor", which is why some experts are now promoting the creation of "quality" and not "quantity" in labor market policies.[25] This approach does highlight how higher productivity has helped reduce poverty in East Asia, but the negative impact is beginning to show.[25] In Vietnam, for example, employment growth has slowed while productivity growth has continued.[25] Furthermore, productivity increases do not always lead to increased wages, as can be seen in the United States, where the gap between productivity and wages has been rising since the 1980s.[25]		Researchers at the Overseas Development Institute argue that there are differences across economic sectors in creating employment that reduces poverty.[25] 24 instances of growth were examined, in which 18 reduced poverty. This study showed that other sectors were just as important in reducing unemployment, such as manufacturing.[25] The services sector is most effective at translating productivity growth into employment growth. Agriculture provides a safety net for jobs and economic buffer when other sectors are struggling.[25]		Scholars conceptualize the employment relationship in various ways.[32] A key assumption is the extent to which the employment relationship necessarily includes conflicts of interests between employers and employees, and the form of such conflicts.[33] In economic theorizing, the labor market mediates all such conflicts such that employers and employees who enter into an employment relationship are assumed to find this arrangement in their own self-interest. In human resource management theorizing, employers and employees are assumed to have shared interests (or a unity of interests, hence the label “unitarism”). Any conflicts that exist are seen as a manifestation of poor human resource management policies or interpersonal clashes such as personality conflicts, both of which can and should be managed away. From the perspective of pluralist industrial relations, the employment relationship is characterized by a plurality of stakeholders with legitimate interests (hence the label “pluralism), and some conflicts of interests are seen as inherent in the employment relationship (e.g., wages v. profits). Lastly, the critical paradigm emphasizes antagonistic conflicts of interests between various groups (e.g., the competing capitalist and working classes in a Marxist framework) that are part of a deeper social conflict of unequal power relations. As a result, there are four common models of employment:[34]		These models are important because they help reveal why individuals hold differing perspectives on human resource management policies, labor unions, and employment regulation.[35] For example, human resource management policies are seen as dictated by the market in the first view, as essential mechanisms for aligning the interests of employees and employers and thereby creating profitable companies in the second view, as insufficient for looking out for workers’ interests in the third view, and as manipulative managerial tools for shaping the ideology and structure of the workplace in the fourth view.[36]		Literature on the employment impact of economic growth and on how growth is associated with employment at a macro, sector and industry level was aggregated in 2013.[37]		Researchers found evidence to suggest growth in manufacturing and services have good impact on employment. They found GDP growth on employment in agriculture to be limited, but that value-added growth had a relatively larger impact.[25] The impact on job creation by industries/economic activities as well as the extent of the body of evidence and the key studies. For extractives, they again found extensive evidence suggesting growth in the sector has limited impact on employment. In textiles however, although evidence was low, studies suggest growth there positively contributed to job creation. In agri-business and food processing, they found impact growth to be positive.[37]		They found that most available literature focuses on OECD and middle-income countries somewhat, where economic growth impact has been shown to be positive on employment. The researchers didn't find sufficient evidence to conclude any impact of growth on employment in LDCs despite some pointing to the positive impact, others point to limitations. They recommended that complementary policies are necessary to ensure economic growth's positive impact on LDC employment. With trade, industry and investment, they only found limited evidence of positive impact on employment from industrial and investment policies and for others, while large bodies of evidence does exist, the exact impact remains contested.[37]		Researchers have also explored the relationship between employment and illicit activities. Using evidence from Africa, a research team found that a program for Liberian ex-fighters reduced work hours on illicit activities. The employment program also reduced interest in mercenary work in nearby wars. The study concludes that while the use of capital inputs or cash payments for peaceful work created a reduction in illicit activities, the impact of training alone is rather low.[38]		The balance of economic efficiency and social equity is the ultimate debate in the field of employment relations.[39] By meeting the needs of the employer; generating profits to establish and maintain economic efficiency; whilst maintaining a balance with the employee and creating social equity that benefits the worker so that he/she can fund and enjoy healthy living; proves to be a continuous revolving issue in westernized societies.[39]		Globalization has effected these issues by creating certain economic factors that disallow or allow various employment issues. Economist Edward Lee (1996) studies the effects of globalization and summarizes the four major points of concern that affect employment relations:		What also results from Lee’s (1996) findings is that in industrialized countries an average of almost 70 per cent of workers are employed in the service sector, most of which consists of non-tradable activities. As a result, workers are forced to become more skilled and develop sought after trades, or find other means of survival. Ultimately this is a result of changes and trends of employment, an evolving workforce, and globalization that is represented by a more skilled and increasing highly diverse labor force, that are growing in non standard forms of employment (Markey, R. et al. 2006).[39]		Various youth subcultures have been associated with not working, such as the hippie subculture in the 1960s and 1970s (which endorsed the idea of "dropping out" of society) and the punk subculture, in which some members live in anarchist squats (illegal housing).		One of the alternatives to work is engaging in postsecondary education at a college, university or professional school. One of the major costs of obtaining a postsecondary education is the opportunity cost of forgone wages due to not working. At times when jobs are hard to find, such as during recessions, unemployed individuals may decide to get postsecondary education, because there is less of an opportunity cost.		Workplace democracy is the application of democracy in all its forms (including voting systems, debates, democratic structuring, due process, adversarial process, systems of appeal) to the workplace.[40][41]		When an individual entirely owns the business for which they labor, this is known as self-employment. Self-employment often leads to incorporation. Incorporation offers certain protections of one's personal assets.[39] Individuals who are self-employed may own a small business. They may also be considered to be an entrepreneur.		In some countries, individuals who are not working can receive social assistance support (e.g., welfare or food stamps) to enable them to rent housing, buy food, repair or replace household goods, maintenance of children and observe social customs that require financial expenditure.		Workers who are not paid wages, such as volunteers who perform tasks for charities, hospitals or not-for-profit organizations, are generally not considered employed. One exception to this is an internship, an employment situation in which the worker receives training or experience (and possibly college credit) as the chief form of compensation.[40]		Those who work under obligation for the purpose of fulfilling a debt, such as an indentured servant, or as property of the person or entity they work for, such as a slave, do not receive pay for their services and are not considered employed. Some historians suggest that slavery is older than employment, but both arrangements have existed for all recorded history. Indentured servitude and slavery are not considered compatible with human rights and democracy.[40]		
Underemployment or disguised unemployment refers to a job that is insufficient in some important way for a worker, relative to a standard,[2] which results in the under-utilization of the worker. Examples include holding a part-time job despite desiring full-time work, and overqualification, where the employee has education, experience, or skills beyond the requirements of the job.[3][4]		Underemployment has been studied from a variety of perspectives, including economics, management, psychology, and sociology. In economics, for example, the term underemployment has three different distinct meanings and applications. All meanings involve a situation in which a person is working, unlike unemployment, where a person who is searching for work cannot find a job. All meanings involve under-utilization of labor which is missed by most official (governmental agency) definitions and measurements of unemployment.		In economics, underemployment can refer to:		Underemployment is a significant cause of poverty: although the worker may be able to find part-time work, the part-time pay may not be sufficient for basic needs. Underemployment is a problem particularly in developing countries, where the unemployment rate is often quite low, as most workers are doing subsistence work or occasional part-time jobs. The global average of full-time workers per adult population is only 26%, compared to 30-52% in developed countries and 5-20% in most of Africa.[7]						In one usage, underemployment describes the employment of workers with high skill levels and postsecondary education who are working in relatively low-skilled, low-wage jobs. For example, someone with a college degree may be tending bar, or working as a factory assembly line worker. This may result from the existence of unemployment, which makes workers with bills to pay (and responsibilities) take almost any jobs available, even if they do not use their full talents. This can also occur with individuals who are being discriminated against, lack appropriate trade certification or academic degrees (such as a high school or college diploma), have disabilities or mental illnesses, or have served time in prison.		Two common situations which can lead to underemployment are immigrants and new graduates. When highly trained immigrants arrive in a country, their foreign credentials may not be recognized or accepted in their new country, or they may have to do a lengthy or costly re-credentialing process. As a result, when doctors or engineers from other countries immigrate, they may be unable to work in their profession, and they may have to seek menial work. New graduates may also face underemployment, because even though they have completed the technical training for a given field for which there is a good job market, they lack experience. So a recent graduate with a master's degree in accounting or business administration may have to work in a low-paid job as a barista or store clerk–jobs which do not require a university degree–until they are able to find work in their professional field.		Another example of underemployment is someone who holds high skills for which there is low market-place demand. While it is costly in terms of money and time to acquire academic credentials, many types of degrees, particularly those in the liberal arts, produce significantly more graduates than can be properly employed.[8] Employers have responded to the oversupply of graduates by raising the academic requirements of many occupations higher than is really necessary to perform the work.[9] A number of surveys show that skill-based underemployment in North America and Europe can be a long-lasting phenomenon. If university graduates spend too long in situations of underemployment, the skills they gained from their degrees can atrophy from disuse or become out of date. For example, a person who graduates with a PhD in English literature has advanced research and writing skills when they graduate, but if she or he works as a store clerk for a number of years, these research and writing skills will atrophy from disuse. Similarly, technically specialized workers may find themselves unable to acquire positions commensurate with their skills for extended lengths of time following layoffs.[10] A skilled machinist who is laid off may find that she cannot find another machinist job, so she may work as a server in a restaurant, a position which does not use her professional skills.		Given that most university study in Western countries is subsidized (either because it takes place at a state university or public university, or because the student receives government loans or grants), this type of underemployment may also be an ineffective use of public resources. Several solutions have been proposed to reduce skill-based underemployment: for example, government-imposed restrictions on enrollment in public universities in fields with a very low labor market demand (e.g. fine arts), or changes in degree cost structure that reflected potential labour market demand.		A related kind of underemployment refers to "involuntary part-time" workers. These are workers who could (and would like to) be working for the standard work-week (typically full-time employment means 40 hours per week in the United States) who can only find part-time work. Underemployment is more prevalent during times of economic stagnation (during recessions or depressions). Obviously, during the Great Depression of the 1930s, many of those who were not unemployed were underemployed. These kinds of underemployment arise because labor markets typically do not "clear" using wage adjustment. Instead, there is non-wage rationing of jobs.		Underemployment can also be used in regional planning to describe localities where economic activity rates are unusually low. This can be induced by a lack of job opportunities, training opportunities, or services such as childcare and public transportation. Such difficulties may lead residents to accept economic inactivity rather than register as unemployed or actively seek jobs because their prospects for regular employment appear so bleak. (These people are often called discouraged workers and are not counted officially as being "unemployed.") The tendency to get by without work (to exit the labor force, living off relatives, friends, personal savings, or non-recorded economic activities) can be aggravated if it is made difficult to obtain unemployment benefits.		Relatedly, in macroeconomics, "underemployment" simply refers to excess unemployment, i.e., high unemployment relative to full employment or the natural rate of unemployment, also called the NAIRU. Thus, in Keynesian economics, reference is made to underemployment equilibrium. Economists calculate the cyclically-adjusted full employment unemployment rate, e.g. 4% or 6% unemployment, which in a given context is regarded as "normal" and acceptable. Sometimes, this rate is equated with the NAIRU. The difference between the observed unemployment rate and cyclically adjusted full employment unemployment rate is one measure of the societal level of underemployment. By Okun's Law, it is correlated with the gap between potential output and the actual real GDP. This "GDP gap" and the degree of underemployment of labor would be larger if they incorporated the roles of underemployed labor, involuntary part-time labor, and discouraged workers.		The third definition of "underemployment" describes a polar opposite phenomenon: to some economists, the term refers to "overstaffing" or "hidden unemployment," the practice of businesses or entire economies employing workers who are not fully occupied (in other words, employees who are not economically productive, or underproductive, or economically inefficient). This may be because of legal or social restrictions on firing and lay-offs (e.g. union rules requiring managers to make a case to fire a worker or spend time and money fighting the union) or because they are overhead workers, or because the work is highly seasonal (which is the case in accounting firms focusing on tax preparation, as well as agriculture). The presence of this issue in white collar office jobs is described in the boreout phenomenon, which posits that the major issue facing office workers is lack of work and boredom.		This kind of underemployment does not refer to the kind of non-work time done by, for instance, firefighters or lifeguards, who spend a lot of their time waiting and watching for emergency or rescue work to do; this kind of activity is necessary to ensure that if (e.g.) three fires occur at once, there are sufficient firefighters available.		This kind of underemployment may exist for structural or cyclical reasons. In many economies, some firms become insulated from fierce competitive pressures and grow inefficient, because they are awarded a government monopoly (e.g., telephone or electrical utilities) or due to a situation of abuse of market power (e.g., holding a monopoly position in a certain industry). As such, if they may employ more workers than necessary, they might not be getting the market signals that would pressure them to reduce their labour force, and they may end up carrying the resultant excess costs and depressed profits.		In some countries, labour laws or practices (e.g. powerful unions) may force employers to retain excess employees. Other countries (e.g. Japan) often have significant cultural influences (the relatively great importance attached to worker solidarity as opposed to shareholder rights) that result in a reluctance to shed labour in times of difficulty. In Japan, there is a long-held tradition that if a worker commits to serve a company with long and loyal service, the company will, in return, keep the worker on the payroll even during economic downturns. In centrally-planned economies, lay-offs were often not allowed, so that some state-run companies would have periods when they had more workers than they needed to complete the organization's tasks.		Cyclical underemployment refers to the tendency for the capacity utilization of firms (and therefore of their demand for labor) to be lower at times of recession or economic depression. At such times, underemployment of workers may be tolerated—and indeed may be wise business policy—given the financial cost and the degradation of morale from shedding and then re-hiring staff. Alternatively, paying underused overhead workers is seen as an investment in their future contributions to production. This kind of underemployment has been given as a possible reason why Airbus gained market share from Boeing. Unlike Airbus, which had more flexibility, Boeing was unable to ramp up production fast enough when prosperous times returned because the company had dismissed a great part of its personnel in lean times.		Another example is the tourism sector, which faces cyclical demand in areas where attractions are weather-related. In some tourism sectors, such as the sun and sand tours operated by Club Med, the company can shed bartenders, lifeguards, and sports instructors, and other staff in the off-season, because there is such a strong demand amongst young people to work for the company, because its glamorous beachfront properties are desirable places to work. However, not all tourism sectors find it so easy to recruit staff. Some tourism sectors require workers with unusual or hard-to-find skills. Northern Ontario hunting and fishing camps that require skilled guides may have an incentive to retain their staff in the off-season. Another example is companies which run tours for foreign tourists using staff speaking the travelers' native tongue. In Canada, guided tours are available for Japanese and German tourists in their native languages; in some locations, it may be hard for companies to find Japanese- or German-speaking staff, so the companies may retain their staff in the off-season.		
A break at work is a period of time during a shift in which an employee is allowed to take time off from his/her job. There are different types of breaks, and depending on the length and the employer's policies, the break may or may not be paid.		Meal breaks or lunch breaks usually range from twenty minutes to one hour. Their purpose is to allow the employee to have a meal that is regularly scheduled during the work day. For a typical daytime job, this is lunch, but this may vary for those with other work hours. It is not uncommon for this break to be unpaid, and for the entire work day from start to finish to be longer than the number of hours paid in order to accommodate this time.		According to a study, the amount of time people are taking for lunch breaks in the United States is shrinking, thereby making the term "lunch hour" a misnomer.[1] Some employers request the lunch to be taken at their work station or do not offer lunch breaks at all. Many employees are taking shorter lunch breaks in order to compete with other employees for a better position, and to show their productivity.[2]		In some places, such as the state of California, meal breaks are legally mandated.[1] Penalties can be severe for failing to adequately staff one's business premises so that all employees can rotate through their mandatory meal and rest breaks. For example, on April 16, 2007, the Supreme Court of California unanimously affirmed a trial court judgment requiring Kenneth Cole Productions to pay an additional hour of pay for each day that a store manager had been forced to work a nine-hour shift without a break.[3] On April 12, 2012 the Supreme Court of California issued its long-awaited opinion in Brinker Restaurant Corp., et al. v. Superior Court.,[4] which addressed a number of issues that have been the subject of much litigation in California for many years. The California Supreme court ruled that employers satisfy their California Labor Code section 512 obligation to "provide" meal periods to nonexempt employees by (1) relieving employees of all duty; (2) relinquishing control over their activities and permitting them a reasonable opportunity to take an uninterrupted 30-minute break; and (3) not impeding or discouraging them from doing so. Importantly, the court agreed that employers are not obliged to "police" meal breaks to ensure that no work is performed. Even if an employee chooses to work during a properly provided meal period, an employer will not be liable for any premium pay, and will only be liable to pay for the time worked during a meal period so long as the employer knew or reasonably should have known that the employee was working during the meal period.[5]						A short break to allow an employee to use a restroom or WC and will generally last less than 10 minutes. Many employers expect their employees to use the facilities during their regularly scheduled breaks and lunches. Denying employees rights to use the facilities as needed could adversely affect workplace sanitation and workers' health and could create legal issues for both these and other reasons.[6] Employers and co-workers often frown on employees who are seen as taking too many of these breaks, and this could be a cause for progressive discipline from a written warning up to termination. In today's setting however, restroom breaks are generally accepted and not tracked by employers. In 2017, an official in the Swedish town of Overtornea proposed an hour-long break for sexual activity.[7]		A coffee break in the United States and elsewhere is a short mid-morning rest period granted to employees in business and industry, corresponding with the Commonwealth terms "elevenses", "smoko" (in Australia), "morning tea", "tea break", or even just "tea". An afternoon coffee break, or afternoon tea, often occurs as well.		The origin of the tea break as is now incorporated into the law of most countries, stems from research undertaken in England in the early 1900s. A F Stanley Kent, an Oxford graduate and the first Professor of Physiology at University College, Bristol, undertook scientific research on Industrial Fatigue at the request of the Home Office (UK). This work followed the International Congress of Hygiene and Demography held in Brussels in 1903 where a resolution was passed that "the various governments should facilitate as far as possible investigation into the subject of Industrial Fatigue". This was due to its noted bearing on incidence of accidents and excessive sickness. The monotony of work and the effect of alcohol on muscular activity and mental fatigue were also mentioned. The Tea Break came as a direct result of this work.		When Kent was sent by the Home Secretary to stop wartime munitions production as a trial to test the effect of a tea break on productivity, the factory manager refused on the grounds that he had a production schedule within which he must comply. Meeting this challenge, Kent showed the letter from the Home Secretary and observed that if necessary he would have the police called to arrest the manager who blocked the Home Office directive. The results of Kent's study were presented to both Houses of Parliament on 17 August 1915 in an "Interim Report on Industrial Fatigue by Physiological Methods". It was the first time that the government had owned and operated factories and therefore had the right to intervene in their operational methods. Again presenting to both Houses of Parliament on 16 August 1916, Kent read from his "Blue Book" that during his research it had been "possible to obtain information upon...such [matters] as the need to provide canteens in munitions factories, the question of proper feeding of the factory worker, provision of accommodation in factories for the changing and drying of shoes and clothing, and the proper use of appliances provided for ventilating the work-rooms".[8]		The coffee break allegedly originated in the late 19th century in Stoughton, Wisconsin, with the wives of Norwegian immigrants. The city celebrates this every year with the Stoughton Coffee Break Festival.[9] In 1951, Time noted that "[s]ince the war, the coffee break has been written into union contracts".[10] The term subsequently became popular through a Pan-American Coffee Bureau ad campaign of 1952 which urged consumers, "Give yourself a Coffee-Break — and Get What Coffee Gives to You."[11] John B. Watson, a behavioral psychologist who worked with Maxwell House later in his career, helped to popularize coffee breaks within the American culture.[12]		Coffee breaks usually last from 10 to 20 minutes and frequently occur at the end of the first third of the work shift. In some companies and some civil service, the coffee break may be observed formally at a set hour. In some places, a "cart" with hot and cold beverages and cakes, breads and pastries arrives at the same time morning and afternoon, an employer may contract with an outside caterer for daily service, or coffee breaks may take place away from the actual work-area in a designated cafeteria or tea room.		More generally, the phrase "coffee break" has also come to denote any break from work.		Snack breaks are usually shorter than meal breaks, and allow an employee to have a quick snack, or to accomplish other personal needs. Similar types of breaks include restroom and smoke breaks. These breaks are also required in the state of California; one 10-15 minute break for every 3.5 hours worked. A few other states have similar laws, but most do not.[citation needed] Some employers allow employees to stop their work for short durations at any time to take care of these needs.		Many companies in the 21st century do not allow smoking on their property, although some employers allow workers to leave the premises to smoke, and some jurisdictions have laws prohibiting smoking in an enclosed place where others are employed. Smoke breaks can be of different lengths but for the most part are shorter than lunch breaks. Some employers are very strict about smoking. A criticism of smoking breaks is that non-smoking employees do not receive the small respite because they simply do not smoke. To certain working environments, however, smoking breaks are widely accepted and seen by some as a good way to network with colleagues (and the management).		
A skilled worker is any worker who has special skill, training, knowledge, and (usually acquired) ability in their work. A skilled worker may have attended a college, university or technical school. Or, a skilled worker may have learned their skills on the job. Examples of skilled labor include software development, paramedics, police officers, soldiers, physicians, crane operators, drafters, painters, plumbers, craftsmen, cooks and accountants. These workers can be either blue-collar or white-collar workers, with varied levels of training or education.						In the northern region of the United States, craft unions may have served as the catalyst to ferment a strong solidarity in favor of skilled labor in the period of the Gilded Age (1865-1900).[1]		In the early 1880s, the craft unions of skilled workers walked hand in hand with the Knights of Labor but the harmony did not last long and by 1885, the Knights' leadership became hostile to trade unions. The Knights argued that the specialization of industrialization had undermined the bargaining power of skilled labor. This was partly true in the 1880s but it had not yet made obsolete the existence of craft unionism.[2]		...The impact of scientific management upon skilled workers should not be overstressed, especially in the period before World War I.[3]		The period between 1901 and 1925 signals the rise and fall of the Socialist Party of America which depended on skilled workers. In 1906, with the publication of The Jungle, the most popular voice of socialism in the early 20th century, Upton Sinclair gave them ignorant "...Negroes and the lowest foreigners —Greeks, Roumanians, Sicilians and Slovaks" hell.[4]		There was a divergence in status within the working class between skilled and unskilled labor due to the fall in prices of some products and the skilled workers' rising standard of living after the depression of 1929. Skilled workers were the heart of the labor movement before World War I but during the 1920s, they lost much of their enthusiasm and the movement suffered thereby.[5]		In the 20th century, in Nazi Germany, the lower class was subdivided into:		After the end of World War II, West Germany surpassed France in the employment of skilled labor needed at a time when industrialization was sweeping Europe at a fast pace. West Germany's preponderance in the training of skilled workers in technical schools, was the main factor to outweigh the balance between the two countries. In the period between 1950 and 1970, the number of technicians and engineers in West Germany rose from 160,000 to approximately 570,000 by promoting skilled workers through the ranks so that those who were performing skilled labor in 1950 had already become technicians and engineers by 1970.[7]		In the first decade of the 21st century, the average wage of a highly skilled machinist in the United States of America is $3,000 to $4,000 per month. In China, the average wage for a factory worker is $150 a month.[8]		While most (if not all) jobs require some level of skill, "skilled workers" bring some degree of expertise to the performance of a given job. For example, a factory worker who inspects new televisions for whether they turn on or off can fulfil this job with little or no knowledge of the inner workings of televisions. However, someone who repairs televisions would be considered a skilled worker, since such a person would possess the knowledge to be able to identify and correct problems with a television.		In addition to the general use of the term, various agencies or governments, both federal and local, may require skilled workers to meet additional specifications. Such definitions can affect matters such as immigration, licensure and eligibility for travel or residency. For example, according to U.S. Citizenship and Immigration Services, skilled worker positions are not seasonal or temporary and require at least two years of experience or training.		Skilled work varies in type (i.e., service versus labor), education requirements (i.e., apprenticeship versus graduate college) and availability (freelance versus on-call). Such differences are often reflected in titling, opportunity, responsibility and (most significantly) salary.		Both skilled and non-skilled workers are vital and indispensable for the smooth-running of a free-market and/or capitalist society. According to Alan Greenspan, former Chairman of the Federal Reserve Bank, "...Enhancing elementary and secondary school sensitivity to market forces should help restore the balance between the demand for and the supply of skilled workers in the United States."[9]		Generally, however, individual skilled workers are more valued to a given company than individual non-skilled workers, as skilled workers tend to be more difficult to replace. As a result, skilled workers tend to demand more in the way of financial compensation because of their efforts. According to Greenspan, corporate managers are willing to bid up pay packages to acquire skilled workers as they identify the lack of skilled labor as one of today's greatest problems.[10]		Education can be received in a variety of manners, and is acknowledged through various means. Below is a sampling of educational conventions.		In American industry, there has been a change in the concentration of skilled workers from the areas of past economic might e. g. steel, automobile, textile and chemicals to the more recent (21st century) industry developments e. g. computers, telecommunications and information technology which is commonly stated to represent a plus rather than a minus for the American standard of living.[11]		Due to globalization, regional shortages of skilled workers, migration, outsourcing, and other factors, the methods of procuring skilled workers has changed in recent years.		All countries are in a process of change and transition which makes possible the migration of skilled workers from places of lower to higher opportunities in training and better working conditions. Although materialistic rewards play a role in skilled workers migration, it is the lack of security, opportunity and suitable rewards in the homeland that fundamentally makes this massive movement of people possible, going from places of lesser development to affluent societies.[12][13]		Educational poaching is a concern among the developing nations, with the richest nations benefiting from educational resources of the nations who can least afford to lose the most productive career years of their highly skilled professionals. This factor disincentives investment in education in both the developing and developed world, as foreign students and foreign workers limit opportunities for citizens in the receiving countries.[14] Some developing countries see the migration of domestically trained professionals abroad not as a drain but as a gain, a "brain bank" from which to draw at a price; for these professionals, on their return with their accumulated skills, would contribute to the growth of the homeland; cultural factors favor the return of these professionals for a short or a long while.[15] However, policy in the United States is geared toward making non-immigrant visas eligible for adjustment of status to permanent residence status.[16]		Canada Skilled Worker immigration		On January 1, 2015, the Government of Canada implemented the Express Entry Immigration system under the Economic Class including the Federal Skilled Worker Program.nder Express Entry, Federal Skilled Workers across 347 eligible occupations who meet minimum entry criteria, submit an expression of interest profile to the Express Entry Pool. The profiles of candidates in the pool are ranked under a Comprehensive Ranking System.[17]		South Africa		Under Apartheid, the development of skilled workers was concentrated on the white inhabitants but after the socio-political upheaval of the 1990s, these same skilled workers are emigrating, a highly sensitive subject in contemporary South African Society. The media in South Africa has increasingly covered the "brain drain" in the 1990s. Starting in 1994, when a democratically elected government took control of the reins of power, official South African statistics show a greater emigration of skilled workers. The validity of this data has been questioned.[18][19]		European Union		The European Union brought policy into force that paved the way for skilled workers from outside the Union to work and live in the EU under the Blue Card (European Union) Scheme. The key reasons for introducing this policy are an ageing population in general and an increasing shortage of skilled workers in many member states.		Highly skilled workers migration intensity		The demand for Information Technology (IT) skilled workers is on the rise. This has led to a lessening of the immigration restrictions prevalent in various countries. Migration of skilled workers from Asia to the United States, Canada, the United Kingdom and Australia is common, specially among students and the temporary migration of IT skilled workers. Data shows, however, that the migration of skilled workers from Canada, Germany, the United Kingdom and France to the United States is only temporary and is more like a brain exchange than a "brain drain".[20]		World Bank Policy on Fair Exchange		Brain Drain literature focuses mainly on the high cost of skilled migration for the homeland or sending country. This loss can be partly offset if the migration is only temporary. Developing countries invest heavily in education. However, temporary migration can generate a substantial remittance of capital flow to the homeland. This flow of capital plus the additional knowledge gained would do more than compensate the homeland for the investment made originally in educating the skilled worker. The key to temporary migration is a change in the trade and immigration policies of the receiving country and a stepping-up of the demands of the sending country for the return migration of skilled workers.[21][22]		
Practice-based professional learning (pbpl) is understood in contrast to 'classroom-' or 'theory-based' learning. It is kindred to terms such as 'work-based learning', 'workplace' or 'work-centred' learning. Distinctive, though, are a concern for professional learning, and the preference for 'practice' rather than 'work'. While it does not disdain propositional knowledge and what is sometimes called 'theory', its prime interest is in the formation of self-renewing and effective professional practices—a distinct theoretical position in its own right.		The range of concerns may be seen, for example, in the UK Open University's practice-based professional learning centre,[1] one of the Higher Education Funding Council for England's centres for excellence in teaching and learning.[2] Its interests cover the inter-relation of various forms of professional knowledge, ways of fostering them and their co-ordination, workplaces as sites of learning, the assessment of practice-based learning achievements, and the use of modern technologies to support distributed learning.		Other centres for excellence occupy some or all of the territory, notably the Professional Development Unit at the University of Chester,[3] SCEPTrE in the University of Surrey,[4] CEPLW in the University of Westminster[5] and NCWBLP in Middlesex University.[6]		Interest in this territory is not confined to the UK, with some of the most respected work associated with David Boud at the University of Technology, Sydney, NSW.[7]		
Work ethic is a belief that work, hard work and diligence has a moral benefit and an inherent ability, virtue or value to strengthen character.[1] It is about prioritizing work and putting it in the center of life. Social ingrainment of this value is considered to enhance character through hard work that is respective to an individuals field of work.[2]						A strong work ethic is vital for achieving goals. A work ethic is a set of moral principles a person uses in their job. People who possess a strong work ethic embody certain principles that guide their work behavior, leading them to produce high-quality work consistently and the output feeds the individual to stay on track. A good work ethic fuels an individuals needs and goals, it is related to the initiative by a person for the objectives. It is considered as a source of self respect, satisfaction, and fulfillment.		Factors are:[3]		A Negative work ethic is a behavior of a single individual or a group that has led to a systematic lack of productivity, reliability, accountability and a growing sphere of unprofessional/unhealthy relationships (E.g., power politics, lack of social skills, etc.).[4]		Steven Malanga refers to "what was once understood as the work ethic—not just hard work but also a set of accompanying virtues, whose crucial role in the development and sustaining of free markets too few now recall."[5]		Max Weber quotes the ethical writings of Benjamin Franklin:		Remember, that time is money. He that can earn ten shillings a day by his labor, and goes abroad, or sits idle, one half of that day, though he spends but sixpence during his diversion or idleness, ought not to reckon that the only expense; he has really spent, or rather thrown away, five shillings besides.		Remember, that money is the prolific, generating nature. Money can beget money, and its offspring can beget more, and so on. Five shillings turned is six, turned again is seven and threepence, and so on, till it becomes a hundred pounds. The more there is of it, the more it produces every turning, so that the profits rise quicker and quicker. He that kills a breeding sow, destroys all her offspring to the thousandth generation. He that murders a crown, destroys all that it might have produced, even scores of pounds.[6]		Weber notes that this is not a philosophy of mere greed, but a statement laden with moral language. It is in effect an ethical response to the natural desire for hedonic reward, a statement of the value of delayed gratification to achieve self-actualization. Franklin claims that Bible readings revealed to him the usefulness of virtue. Indeed, this reflects the then christian search for ethic for living and the struggle to make a living.[7]		Experimental studies have shown that people with fair work ethic are able to tolerate tedious jobs with equitable monetary rewards and benefits, they are highly critical, have a tendency for workaholism and a negative relation with leisure activity concepts. They valued meritocracy and egalitarianism.[8]		In the 1940s work ethic was considered very important, nonconformist ideals were dealt autocratically. Suppression of humor in the workplace was one of them. It is recorded that at the Ford Company a worker John Gallo was fired for being "caught in the act of smiling."[9]		Countercultural groups and communities, most notably freethinkers, have challenged these values in recent decades, characterizing them as submissive to authority and to social convention, and not valuable in and of themselves, but only if work ethic brings a positive result. An alternative perspective has arisen in recent years, suggesting that the work ethic is being subverted in a broader, more mainstream and more readily marketed-to proportion of society. This perspective has given rise to the phrase "work smart".[citation needed]		In the 19th century, the Arts and Crafts movement of William Morris in the UK and Elbert Hubbard in the US noted how "alienation" of workers from ownership of the tools of production and their work product was destructive of the work ethic because in the expanding firms of that era, then the workers saw no point in doing more than the minimum.[citation needed]		The industrial engineer Frederick Winslow Taylor (1856-1915) revised the notion of work ethic as a means of management control that delude workers about the actual reality for accumulated advantage, which is a form of avarice. Marxists, and some non-Marxist sociologists[who?], do not regard "work ethic" as a useful sociological concept. They argue that having a "work ethic" in excess of management's control doesn't appear rational in any mature industry where the employee can't rationally hope to become more than a manager whose fate still depends on the owner's decisions.		The French Leftist philosopher André Gorz (1923-2007) wrote:		"The work ethic has become obsolete. It is no longer true that producing more means working more, or that producing more will lead to a better way of life. The connection between more and better has been broken; our needs for many products and services are already more than adequately met, and many of our as-yet-unsatisfied needs will be met not by producing more, but by producing differently, producing other things, or even producing less. This is especially true as regards our needs for air, water, space, silence, beauty, time and human contact.		Neither is it true any longer that the more each individual works, the better off everyone will be. In a post-industrial society, not everyone has to work hard in order to survive, though may be forced to anyway due to the economic system. The present crisis has stimulated technological change of an unprecedented scale and speed: 'the micro-chip revolution'. The object and indeed the effect of this revolution has been to make rapidly increasing savings in labour, in the industrial, administrative and service sectors. Increasing production is secured in these sectors by decreasing amounts of labour. As a result, the social process of production no longer needs everyone to work in it on a full-time basis. The work ethic ceases to be viable in such a situation and workbased society is thrown into crisis."[10]		Socialists believe that the concept of "hard work" is meant[by whom?] to delude the working class into becoming loyal servants to the elite, and that working hard, in itself, is not automatically an honorable thing, but only a means to creating more wealth for the people at the top of the economic pyramid. In the Soviet Union, the regime portrayed work ethic as an ideal to strive for.[11]		Recession holds back work ethic because the generation that inherits it lives in an economy that isn’t ready to receive them. Without the work there to do, the ethic that’s attached to it fails to generate distinctive value. The negative work ethic and power structures that doesn't value or credit work done or unethically attribute work done as a service or with higher moral ideals have dissolved the ethic presented in the society and turned the focus onto self-centered perks and individualism. Further, urbanization and an emphasis on large-scale businesses has led to eliminating avenues for learning vital concepts about work. Millennials in a research identified what made them unique was consumerist trends like technology use, music/pop culture, liberal/­tolerant beliefs, greater intelligence, and clothes than work, they were not able to distinguish it with traditional understandings of work ethic.[12]		
