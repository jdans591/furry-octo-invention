An adventure is an undertaking into the unknown, often having a connotation of danger and excitement.		Adventure or The Adventure may also refer to:		
Arousal is the physiological and psychological state of being awoken or of sense organs stimulated to a point of perception. It involves activation of the reticular activating system in the brainstem, the autonomic nervous system, and the endocrine system, leading to increased heart rate and blood pressure and a condition of sensory alertness, mobility, and readiness to respond.		The arousal system involves many different neural systems. Five major systems originating in the brainstem, with connections extending throughout the cortex, are based on the brain's neurotransmitters: acetylcholine, norepinephrine, dopamine, histamine, and serotonin. When these systems are in action, the receiving neural areas become sensitive and responsive to incoming signals, producing alertness and cortical activity.		Arousal is important in regulating consciousness, attention, alertness, and information processing. It is crucial for motivating certain behaviours, such as mobility, the pursuit of nutrition, the fight-or-flight response and sexual activity (the arousal phase of Masters and Johnson's human sexual response cycle). It is also important in emotion and has been included in theories such as the James-Lange theory of emotion. According to Hans Eysenck, differences in baseline arousal level lead people to be extraverts or introverts.		The Yerkes-Dodson law states that an optimal level of arousal for performance exists, and too little or too much arousal can adversely affect task performance. One interpretation of the Yerkes-Dodson Law is the Easterbrook cue-utilisation hypothesis. Easterbrook states that an increase of arousal decreases the number of cues that can be used.						The arousal system involves many different neural systems. Five major systems originating in the brainstem, with connections extending throughout the cortex, are based on the brain's neurotransmitters, acetylcholine, norepinephrine, dopamine, histamine, and serotonin.[1] When stimulated, these systems produce cortical activity and alertness.		The noradrenergic system is a bundle of axons that originate in the locus coeruleus and ascends up into the neocortex, limbic system, and basal forebrain. Most of the neurons are projected to the posterior cortex which is important with sensory information, and alertness. The activation of the locus coeruleus and release of norepinephrine causes wakefulness and increases vigilance. The neurons that project into the basal forebrain impact cholinergic neurons that results in a flood of acetylcholine into the cerebral cortex.		The acetylcholinergic system has its neurons located in the pons and in the basal forebrain. Stimulation of these neurons result in cortical activity, shown from EEG records, and alertness. All of the other four neurotransmitters play a role in activating the acetylcholine neurons.		Another arousal system, the dopaminergic system, releases dopamine produced by the substantia nigra. The neurons arise in the ventral tegmental area in the midbrain, and projects to the nucleus accumbens, the striatum forebrain, limbic system, and prefrontal cortex. The limbic system is important for control of mood, and the nucleus accumbens signal excitement and arousal. The path terminating in the prefrontal cortex is important in regulating motor movements, especially reward oriented movements.		The serotonergic system has almost all of its serotonergic neurons originating in the raphe nuclei. This system projects to the limbic system and the prefrontal cortex. Stimulation of these axons and release of serotonin causes cortical arousal and impacts locomotion and mood.		The neurons of the histamergenic system are in the tuberomammillary nucleus of the hypothalamus. These neurons send pathways to the cerebral cortex, thalamus, and the basal forebrain, where they stimulate the release of acetylcholine into the cerebral cortex.		All of these systems are linked and show similar redundancy. The pathways described are ascending pathways, but there also arousal pathways that descend. One example is the ventrolateral preoptic area, which release GABA reuptake inhibitors, which interrupt wakefulness and arousal. Neurotransmitters of the arousal system, such as acetylcholine and norepinephrine, work to inhibit the ventrolateral preoptic area.		Arousal is important in regulating consciousness, attention, and information processing. It is crucial for motivating certain behaviors, such as mobility, the pursuit of nutrition, the fight-or-flight response and sexual activity (see Masters and Johnson's human sexual response cycle, where it is known as the arousal phase). Arousal is also an essential element in many influential theories of emotion, such as the James-Lange theory of emotion or the Circumplex Model. According to Hans Eysenck, differences in baseline arousal level lead people to be either extraverts or introverts. Later research suggests that extroverts and introverts likely have different arousability. Their baseline arousal level is the same, but the response to stimulation is different.[3]		The Yerkes–Dodson law states that there is a relationship between arousal and task performance, essentially arguing that there is an optimal level of arousal for performance, and too little or too much arousal can adversely affect task performance. One interpretation of the Yerkes–Dodson law is the Easterbrook cue-utilisation theory. It predicted that high levels of arousal will lead to attention narrowing, during which the range of cues from the stimulus and the environment decreases.[4] According to this hypothesis, attention will be focused primarily on the arousing details (cues) of the stimulus, so that information central to the source of the emotional arousal will be encoded while peripheral details will not.[5]		In positive psychology, arousal is described as a response to a difficult challenge for which the subject has moderate skills.[2]		Eysenck's theory of arousal describes the different natural frequency or arousal states of the brains of people who are introverted versus people who are extroverted. The theory states that the brains of extroverts are naturally less stimulated, so these types have a predisposition to seek out situations and partake in behaviors that will stimulate arousal.[6] Whereas extroverts are naturally under-stimulated and therefore actively engage in arousing situations, introverts are naturally over-stimulated and therefore avoid intense arousal. Campbell and Hawley (1982) studied the differences in introverts versus extroverts responses to particular work environments in the library.[6] The study found that introverts were more likely to choose quiet areas with minimal to no noise or people. Extroverts were more likely to choose areas with much activity with more noise and people.[6] Daoussiss and McKelvie's (1986) research showed that introverts performed worse on memory tasks when they were in the presence of music compared to silence. Extroverts were less affected by the presence of music.[6] Similarly, Belojevic, Slepcevic and Jokovljevic (2001) found that introverts had more concentration problems and fatigue in their mental processing when work was coupled with external noise or distracting factors.[6] The level of arousal surrounding the individuals greatly affected their ability to perform tasks and behaviors, with the introverts being more affected than the extroverts, because of each's naturally high and low levels of stimulation, respectively.		Neuroticism or emotional instability and extroversion are two factors of the Big Five Personality Index. These two dimensions of personality describe how a person deals with anxiety-provoking or emotional stimuli as well as how a person behaves and responds to relevant and irrelevant external stimuli in their environment. Neurotics experience tense arousal which is characterized by tension and nervousness. Extroverts experience high energetic arousal which is characterized by vigor and energy.[7] Gray (1981) claimed that extroverts have a higher sensitivity to reward signals than to punishment in comparison to introverts. Reward signals aim to raise the energy levels.[7] Therefore, extroverts typically have a higher energetic arousal because of their greater response to rewards.		Hippocrates theorized that there are four personality types: choleric, melancholic, sanguine, and phlegmatic.		Put in terms of the five factor level of personality, choleric people are high in neuroticism and high in extraversion. The choleric react immediately, and the arousal is strong, lasting, and can easily create new excitement about similar situations, ideas, or impressions.[8] Melancholic people are high in neuroticism and low in extraversion (or more introverted). The melancholic are slow to react and it takes time for an impression to be made upon them if any is made at all. However, when aroused by something, melancholics have a deeper and longer lasting reaction, especially when exposed to similar experiences.[8] Sanguine people are low in neuroticism (or more emotionally stable) and high in extraversion. The sanguine are quickly aroused and excited, like the cholerics, but unlike the cholerics, their arousal is shallow, superficial, and shortly leaves them as quickly as it developed.[8] Phlegmatic people are low in neuroticism and low in extraversion. The phlegmatic are slower to react and the arousal is fleeting.[8]		The contrasts in the different temperaments come from individuals variations in a person's brain stem, limbic system, and thalamocortical arousal system. These changes are observed by electroencephalogram (EEG) recordings which monitor brain activity.[9] Limbic system activation is typically linked to neuroticism, with high activation showing high neuroticism.[10] Cortical arousal is associated with introversion–extraversion differences, with high arousal associated with introversion.[10] Both the limbic system and the thalamocortical arousal system are influenced by the brainstem activation.[10] Robinson's study (1982) concluded that melancholic types had the greatest natural frequencies, or a "predominance of excitation", meaning that melancholics (who are characterized by introversion) have a higher internal level of arousal.[9] Sanguine people (or those with high extraversion and low neuroticism) had the lowest overall levels of internal arousal, or a "predominance of inhibition".[9] Melancholics also had the highest overall thalamocortical excitation, whereas cholerics (those with high extraversion and high neuroticism) had the lowest intrinsic thalamocortical excitation.[9]		The differences in the internal system levels is the evidence that Eysenck used to explain the differences between the introverted and the extroverted. Ivan Pavlov, the founder of classical conditioning, also partook in temperament studies with animals. Pavlov's findings with animals are consistent with Eysenck's conclusions. In his studies, melancholics produced an inhibitory response to all external stimuli, which holds true that melancholics shut out outside arousal, because they are deeply internally aroused.[9] Pavlov found that cholerics responded to stimuli with aggression and excitement whereas melancholics became depressed and unresponsive.[9] The high neuroticism which characterizes both melancholics and cholerics manifested itself differently in the two types because of the different levels of internal arousal they had.		The Cannon–Bard theory is a theory of undifferentiated arousal, where the physical and emotional states occur at the same time in response to an event. This theory states that an emotionally provoking event results in both the physiological arousal and the emotion occurring concurrently.[11] For example, if a person's dear family member dies, a potential physiological response would be tears falling down the person's face and their throat feeling dry; they are "sad". The Cannon–Bard theory states that the tears and the sadness both happen at the same time. The process goes: event (family member dies) → physiological arousal (tears) and emotion (sadness) simultaneously.[11] The fact that people can experience different emotions when they have the same pattern of physiological arousal is one argument in favor of the Cannon-Bard theory. For example, a person may have a heart racing and rapid breathing when they are angry or afraid. Even though not completely in accordance with the theory, it is taken as one piece of evidence in favor of the Cannon–Bard theory that physiological reactions sometimes happen more slowly than experiences of emotion. For example, if you are in the forest or woods, a sudden sound can create an immediate response of fear, while the physical symptoms of fear follow that feeling, and do not precede it.[12]		The James–Lange theory describes how emotion is caused by the bodily changes which come from the perception of the emotionally arousing experience or environment.[13] This theory states that events cause the autonomic nervous system to induce physiological arousal, characterized by muscular tension, heart rate increases, perspiration, dryness of mouth, tears, etc.[14] According to James and Lange, the emotion comes as a result of the physiological arousal.[15] The bodily feeling as a reaction to the situation actually is the emotion.[13] For example, if someone just deeply insulted a person and their family, the person's fists might ball up and they might begin to perspire and become tense all around. The person feels that their fists are balled and that they are tense. The person then realizes that they are angry. The process here is: event (insult) --> physiological arousal (balled fists, sweat, tension) --> interpretation ("I have balled fists, and tension") --> emotion (anger: "I am angry").[15] This type of theory emphasizes the physiological arousal as the key, in that the cognitive processes alone would not be sufficient evidence of an emotion.		The Schachter–Singer two-factor theory or the cognitive labeling theory takes into account both the physiological arousal and the cognitive processes that respond to an emotion-provoking situation. Schachter and Singer's theory states that an emotional state is the product of the physiological arousal and the cognition regarding the state of arousal. Thus, cognition determines how the physical response is labeled; for example, as "anger", "joy", or "fear".[13] In this theory, emotion is seen as a product of the interaction between the state of arousal and how one's thought processes appraise the current situation.[16] The physiological arousal does not provide the label for the emotion; cognition does. For example, if a person is being pursued by a serial killer, the person will likely be sweating and their heart will be racing, which is their physiological state. The person's cognitive label will come from assessing their quickly beating heart and sweat as "fear". Then they will feel the emotion of "fear", but only after it has been established through cognition. The process is: the event (serial killer chasing the person) --> physiological arousal (sweat, heart racing) --> cognitive label (reasoning; "this is fear") --> emotion (fear).[15]		Arousal is involved in the detection, retention, and retrieval of information in the memory process. Emotionally arousing information can lead to better memory encoding, therefore influencing better retention and retrieval of information. Arousal is related to selective attention during the encoding process by showing that people are more subject to encode arousing information than neutral information.[5] The selectivity of encoding arousing stimuli produces better long-term memory results than the encoding of neutral stimuli.[17] In other words, the retention and accumulation of information is strengthened when exposed to arousing events or information. Arousing information is also retrieved or remembered more vividly and accurately.[18]		Although arousal improves memory under most circumstances, there are some considerations. Arousal at learning is associated more with long-term recall and retrieval of information than short-term recall of information. For example, one study found that people could remember arousing words better after one week of learning them than merely two minutes after learning them.[19] Another study found that arousal affects the memory of people in different ways. Eysenck found an association between memory and the arousal of introverts versus extroverts. Higher levels of arousal increased the amount of words retrieved by extroverts and decreased the amount of words retrieved by introverts.[19]		A person's level of arousal when introduced to stimuli can be indicative of his or her preferences. One study found that familiar stimuli are often preferred to unfamiliar stimuli. The findings suggested that the exposure to unfamiliar stimuli was correlated to avoidance behaviors. The unfamiliar stimuli may lead to increased arousal and increased avoidance behaviors.[20]		On the contrary, increased arousal can increase approach behaviors as well. People are said to make decisions based on their emotional states. They choose specific options that lead to more favorable emotional states.[21] When a person is aroused, he or she may find a wider range of events appealing[22] and view decisions as more salient, specifically influencing approach-avoidance conflict.[21] The state of arousal might lead a person to view a decision more positively than he or she would have in a less aroused state.		The reversal theory accounts for the preference of either high or low arousal in different situations. Both forms of arousal can be pleasant or unpleasant, depending on a person's moods and goals at a specific time.[23] Wundt's and Berlyne's hedonic curve differ from this theory. Both theorists explain a person's arousal potential in terms of his or her hedonic tone. These individual differences in arousal demonstrate Eysenck's theory that extroverts prefer increased stimulation and arousal, whereas introverts prefer lower stimulation and arousal.[24]		Altered experiences of arousal are associated with both anxiety and depression.		Depression can influence a person's level of arousal by interfering with the right hemisphere's functioning. Arousal in women has been shown to be slowed in the left visual field due to depression, indicating the influence of the right hemisphere.[25]		Arousal and anxiety have a different relationship than arousal and depression. People who suffer from anxiety disorders tend to have abnormal and amplified perceptions of arousal. The distorted perceptions of arousal then create fear and distorted perceptions of the self. For example, a person may believe that he or she will get sick from being so nervous about taking an exam. The fear of the arousal of nervousness and how people will perceive this arousal will then contribute to levels of anxiety.[26]		This is caused by withdrawal from alcohol or barbiturates, acute encephalitis, head trauma resulting in coma, partial seizures in epilepsy, metabolic disorders of electrolyte imbalance, Intra-cranial space- occupying lesions, Alzheimer's disease, rabies, hemispheric lesions in stroke and multiple sclerosis.[27]		Anatomically this is a disorder of the limbic system, hypothalamus, temporal lobes, amygdala and frontal lobes.[27] It is not to be confused with mania.		
Travel is the movement of people between relatively distant geographical locations, and can involve travel by foot, bicycle, automobile, train, boat, bus, airplane, or other means, with or without luggage, and can be one way or round trip.[1][2] Travel can also include relatively short stays between successive movements.						The origin of the word "travel" is most likely lost to history. The term "travel" may originate from the Old French word travail, which means 'work'.[3] According to the Merriam Webster dictionary, the first known use of the word travel was in the 14th century. It also states that the word comes from Middle English travailen, travelen (which means to torment, labor, strive, journey) and earlier from Old French travailler (which means to work strenuously, toil). In English we still occasionally use the words "travail", which means struggle. According to Simon Winchester in his book The Best Travelers' Tales (2004), the words "travel" and "travail" both share an even more ancient root: a Roman instrument of torture called the tripalium (in Latin it means "three stakes", as in to impale). This link may reflect the extreme difficulty of travel in ancient times. Today, travel may or may not be much easier depending upon the destination you choose (e.g. Mt. Everest, the Amazon rainforest), how you plan to get there (tour bus, cruise ship, or oxcart), and whether you decide to "rough it" (see extreme tourism and adventure travel). "There's a big difference between simply being a tourist and being a true world traveler", notes travel writer Michael Kasum. This is, however, a contested distinction as academic work on the cultures and sociology of travel has noted.[4]		Reasons for traveling include recreation,[5] tourism[5] or vacationing,[5] research travel[5] the gathering of information, visiting people, volunteer travel for charity, migration to begin life somewhere else, religious pilgrimages[5] and mission trips, business travel,[5] trade,[5] commuting, and other reasons, such as to obtain health care[5] or waging or fleeing war or for the enjoyment of traveling. Travellers may use human-powered transport such as walking or bicycling; or vehicles, such as public transport, automobiles, trains and airplanes.		Motives for travel include:		Travel may be local, regional, national (domestic) or international. In some countries, non-local internal travel may require an internal passport, while international travel typically requires a passport and visa. A trip may also be part of a round-trip, which is a particular type of travel whereby a person moves from one location to another and returns.[7]		While early travel tended to be slower, more dangerous, and more dominated by trade and migration, cultural and technological advances over many years have tended to mean that travel has become easier and more accessible.[8] The evolution of technology in such diverse fields as horse tack and bullet trains has contributed to this trend.		While travel in the Middle Ages offered hardships and challenges, it was important to the economy and to society. The wholesale sector depended (for example) on merchants dealing with/through caravan or sea-voyagers, end-user retailing often demanded the services of many itinerant peddlers wandering from village to hamlet, gyrovagues and wandering friars brought theology and pastoral support to neglected areas, travelling minstrels practised the never-ending tour, and armies ranged far and wide[9] in various crusades[10] and in sundry other wars.[11]		Pilgrimages involved streams of travellers both locally (Canterbury Tales-style) and internationally.[12]		Travel by water often provided more comfort and speed than land-travel, at least until the advent of a network of railways in the 19th century. Airships and airplanes took over much of the role of long-distance surface travel in the 20th century.		Authorities emphasize the importance of taking precautions to ensure travel safety.[13] When traveling abroad, the odds favor a safe and incident-free trip, however, travelers can be subject to difficulties, crime and violence.[14] Some safety considerations include being aware of one's surroundings,[13] avoiding being the target of a crime,[13] leaving copies of one's passport and itinerary information with trusted people,[13] obtaining medical insurance valid in the country being visited[13] and registering with one's national embassy when arriving in a foreign country.[13] Many countries do not recognize drivers' licenses from other countries; however most countries accept international driving permits.[15] Automobile insurance policies issued in one's own country are often invalid in foreign countries, and it is often a requirement to obtain temporary auto insurance valid in the country being visited.[15] It is also advisable to become oriented with the driving-rules and -regulations of destination countries.[15] Wearing a seat belt is highly advisable for safety reasons; many countries have penalties for violating seatbelt laws.[15]		There are three main statistics which may be used to compare the safety of various forms of travel (based on a DETR survey in October 2000):[16]		
Homer (Ancient Greek: Ὅμηρος [hómɛːros], Hómēros) is the name ascribed by the ancient Greeks to the author of the Iliad and the Odyssey, two epic poems which are the central works of ancient Greek literature. The Iliad is set during the Trojan War, the ten-year siege of the city of Troy by a coalition of Greek states. It focuses on a quarrel between King Agamemnon and the warrior Achilles lasting a few weeks during the last year of the war. The Odyssey focuses on the journey home of Odysseus, king of Ithaca, after the fall of Troy.		Many accounts of Homer's life circulated in classical antiquity, the most widespread being that he was a blind bard from Ionia, a region of central coastal Anatolia in present-day Turkey. Current scholarship suggests that these traditions are merely legends.[1][2][3]		The Homeric Question—by whom, when, where and under what circumstances were the Iliad and Odyssey composed—continues to be debated. Broadly speaking, modern scholarly opinion falls into two groups. One holds that most of the Iliad and (according to some) the Odyssey are the works of a single poet of genius. The other considers the Homeric poems to be the result of a process of working and re-working by many contributors, and that "Homer" is best seen as a label for an entire tradition.[3] It is generally accepted that the poems were composed at some point around the late 8th or early 7th century BCE.[4] The poems are in Homeric Greek, also known as Epic Greek, a literary language which shows a mixture of features of the Ionic and Aeolic dialects from different centuries; the predominant influence is Eastern Ionic.[5][6] Most researchers believe that the poems were originally transmitted orally.[7]		From antiquity until the present day, the influence of the Homeric epics on Western civilization has been great, inspiring many of its most famous works of literature, music, art and film.[8] The Homeric epics were the greatest influence on ancient Greek culture and education; to Plato, Homer was simply the one who "has taught Greece" – ten Hellada pepaideuken.[9][10]						The chronological period of Homer depends on the meaning to be assigned to the word "Homer". Was Homer a single person, an imaginary person representing a group of poets, or the imaginary author of a traditional body of oral myths? If the works attributed either wholly or partially to a blind poet named Homer were really authored by such a person, then he must have lived in a specific era, which can be described as "the life and times of Homer". If on the other hand Homer is to be considered a mythical character, the legendary founder of a guild of rhapsodes (professional performers of epic poetry) called the Homeridae, then "Homer" means the works attributed to the rhapsodes of the guild, who might have composed primarily in a single century or over a period of centuries.		Much of the geographic and material content of the Iliad and Odyssey appears to be consistent with the Aegean Late Bronze Age, the time when Troy flourished: before the time of the Greek alphabet. In a third and last interpretation, the term "Homer" can be used to refer to traditional elements of oral myth known to, but not originated by the rhapsodes; from these they composed oral poetry, which reflected the culture of Mycenaean Greece. This information is often called "the world of Homer" (or of Odysseus, or the Iliad). The Homeric period would in that case cover a number of historical periods, especially the Mycenaean Age, prior to the first delivery of a work called the Iliad.		Aside from the authorship of the works, another question is whether there ever was a uniform text of the Iliad or Odyssey. Considered word-for-word, the printed texts as we know them are the product of the scholars of the last three centuries. Each edition of the Iliad or Odyssey is a little different, as the editors rely on different manuscripts and fragments, and make different choices as to the most accurate text to use. The term "accuracy" implies an original uniform text. The extant manuscripts of the whole work date to no earlier than the 10th century CE. These are at the end of a thousand-year chain of lost manuscripts, copied as each generation of manuscripts disintegrated or were lost or destroyed. The numerous extant manuscripts are so similar that a single original can be postulated.[11]		The time gap in the chain is bridged by the scholia, or notes, on the existing manuscripts, which indicate that the original had been published by Aristarchus of Samothrace in the 2nd century BCE. Librarian of the Library of Alexandria, he had noticed a wide divergence in the works attributed to Homer, and was trying to restore a more authentic copy. He had collected several manuscripts, which he named: the Sinopic, the Massiliotic, etc. The one he selected for correction was the koine, which Murray translates as "the Vulgate". Aristarchus was known for his conservative selection of material. He marked lines that he thought were spurious, not of Homer. The entire last book of the Odyssey was so marked.		The koine had in turn come from the first librarian at Alexandria, Zenodotus, who flourished at the beginning of the 3rd century BCE. He also was attempting to restore authenticity to manuscripts he found in a state of chaos. He set a precedent by marking passages he considered spurious, and by himself filling in material that seemed to be missing. Neither Zenodotus nor Aristarchus mentioned any authentic master copy from which to make corrections. Their method was intuitive. The current division into 24 books each for the Iliad and Odyssey came from Zenodotus.		Murray rejects the concept that an authoritative text for the Vulgate existed at the time of Zenodotus. He resorts to the fragments, the quotations of Homer in other works. About 200 existed at the time Murray wrote. Some of these match the current texts, some seem to paraphrase them, and some are not represented at all. Murray cites the Shield of Achilles, which also appears as the Shield of Heracles in Hesiod. Murray concludes that the epic poems were still in "a fluid state". He presents 150 BCE as the date after which the text solidifies around the Vulgate. Of the 5th century BCE, Murray said "'Homer' meant to them ... 'the author of the Iliad and the Odyssey', but we cannot be sure that either ... was exactly what we mean by those words."[12]		The earliest mention of a work of Homer was by Callinus, a poet who flourished about 650 BCE. He attributed the Thebais, an epic about the attack on Boeotian Thebes by the epigonoi, to Homer. The Thebais was written about the time of the appearance of the Greek alphabet, but it could originally have been transmitted orally. The Iliad is mentioned by name in Herodotus with regard to the early 6th century, but there is no telling what Iliad that is. Almost all the ancient sources, from the very earliest, appear determined that a Homer, author of the Iliad and Odyssey, existed. The author of the Hymn to Apollo identifies himself in the last verse of the poem as a blind man from Chios.		Nevertheless it is possible to make a case that Homer was only a mythological character, the supposed founder of the Homeridae. Martin West has asserted that "Homer" is "not the name of a historical poet, but a fictitious or constructed name."[13] Oliver Taplin, in the Oxford History of the Classical World's article on Homer, states that the elements of his life "are largely ... demonstrable fictions."[14] Another attack on the biographical details comes from G.S. Kirk, who said: "Antiquity knew nothing definite about the life and personality of Homer."[15] Taplin prefers instead to speak of Homer as "a historical context for the poems." His dates for this context are 750–650 BCE, without considering Murray's "fluid state".		With or without Homer, according to Murray, there is little likelihood that the Iliad and Odyssey of the early sources are the ones we know. Based on the assumption that the Iliad was recited at the Panathenaic Games, which started in 566 BCE, Gregory Nagy selects a date of the 6th century for the fixation of the epics, as opposed to Murray’s 150 BCE.[16] All of these views are only philologic. Regardless of whether there was or was not a Homer, or whether the texts of the Homerica were or were not close to those that exist today, philology alone does not shed any light on the similarities between Mycenaean culture and the geographical and material props of the world of Homer.		Archaeology, however, continues to support the theory that much detailed information survived in the form of formulae and stock pieces to be combined creatively by the rhapsodes of later centuries. A number of combined archaeological and philological works have been written on the topic, such as Denys Page's "History and the Homeric Iliad" and Martin P. Nilsson's "The Mycenaean Origin of Greek Mythology." The linguist Calvert Watkins went so far as to seek an inherited Proto-Indo-European language origin for some epithets and the epic verse form.[17] If he is correct, the stock themes and verses of rhapsodes may be far older than the Trojan War, which would, in that case, have been only the latest opportunity for an epic.		Various traditions have survived purporting to give details of Homer's birthplace and background. The satirist Lucian, in his True History, describes him as a Babylonian called Tigranes, who assumed the name Homer when taken "hostage" (homeros) by the Greeks.[18] When the Emperor Hadrian asked the Oracle at Delphi about Homer, the Pythia proclaimed that he was Ithacan, the son of Epikaste and Telemachus, from the Odyssey.[19] These stories were incorporated into the various "lives of Homer",[20] "compiled from the Alexandrian period onwards".[21]		The "lives of Homer" refer to a set of longer fragments on the topic of the life and works of Homer written by authors who for the most part remain anonymous. Some were attributed to more famous authors. In the 20th century, all the vitae were gathered into a standard reference work by Thomas W. Allen and included in Homeri Opera, (the works of Homer), first published in 1912 by Oxford University Press. This edition has been informally known as "the Oxford Homer" and the Vitae Homeri section as "the lives of Homer" or just "the lives". Volume V of Homeri Opera numbers each of the vitae.[22]		Homeri Opera records vitae collected from various sources: the Vita Herodotea, pp. 192–218, now known as Pseudo-Herodotus, because it is probably not of Herodotus; the Certamen Homeri et Hesiodi, pp. 225–38, with fragments on 218–21; and the two Plutarchi vitae (now Pseudo-Plutarch), pp. 238–45. Allen also records some vitae that are identified as IV (elsewhere as Vita Scorialenses I[23]), pp. 245–46; V (Vita Scorialensis II), pp. 247–50; VI (Vita Romana[24]), pp. 250–53; and finally VII, which contains three extracts: Eustathius, pp. 253–55, John Tzetzes, pp. 254–55, and Suidas, pp. 256–68, now identified as Hesychius Milesius. Nagy reorganizes the list into eleven, Vita 1 through Vita 10, with Plutarch being divided into 3a and 3b. In addition Nagy adds Vita 11 from the Chrestomathia of Proclus, pp. 99–102.[25] The varying and contradictory biographical information in these sources is termed by Nagy "Variations on a Theme of Homer", after the model of the names of certain musical compositions.[26]		Herodotus estimates that Homer lived no more than 400 years before his own time,[27] which would place him at around 850 BCE or later. Pseudo-Herodotus estimates that he was born 622 years before Xerxes I placed a pontoon bridge over the Hellespont in 480 BCE, which would place him at 1102 BCE, 168 years after the fall of Troy in 1270 BCE.[28]		Homer is a name of unknown origin, ostensibly Greek. However, many Greek words, and especially names in the east, where the Greeks were in contact with eastern language speakers, were loans, approximations, or paraphrases of foreign words. For example, Darius to the Greeks was Dārayava(h)uš,, "holding firm the good", to himself and the other Old Persian speakers. Cadmus, overthrown king of Thebes, reported to have been Phoenician, was probably seen as an "easterner", from the Semitic triliteral root q-d-m, "the east".[29] Priam was perhaps from Luwian Priya-muwa-, which means "exceptionally courageous". Many names have a derivation from a foreign language but also fit or partially fit derivations from Proto-Indo-European through Greek. There are but few rules to assist the linguist in identifying which is the most likely.		Etymologies for the name Homeros reach beyond the Greek. On the one hand, he may have a Hellenized Phoenician name. West conjectures a Phoenician prototype for Homer's name as a patronymic, Homeridae (male progeny from the line of Homer), *benê ômerîm ("sons of speakers")—i.e. professional tale-tellers.[30] Here the patronymic would designate the guild. In Greek, the Homer in Homeridae would have to be in the singular, the implied single ancestor of a clan practicing a hereditary trade. The hypothetical semitic ancestors are in the plural; where ben can be used for one father, the -id- construction can never designate a plural father.		On the other hand, Proto-Indo-European etymologies are also available. The poet's name is homophonous with Greek ὅμηρος (hómēros), "hostage" (or "surety").[31] This word is in the Attic dialect, and was a word in general use. In the vitae of Pseudo-Herodotus and Plutarch, it had a relatively obscure meaning "blind", which is interpreted as meaning "he who accompanies; he who is forced to follow (a guide)".[32] The geographic specificity of the word typically is explained by a presumption that it was known mainly in Aeolis on the coast of Asia Minor, the locale where Homer performed, and therefore is a word of the Aeolic dialect.[33] There is no linguistic reason other than usage for thinking so. The letter eta brands the word as being East Greek, as opposed to the West Greek Cretan form, which has an alpha instead. Ionic and Attic also were East Greek. Proclus' Chrestomathia, however, explicitly says, "the tuphloi were called homeroi by the Aeolians"[34] Throughout Pseudo-Herodotus, ὅμηρος (hómēros) is synonymous with the standard Greek τυφλός (tuphlós), meaning "blind".		The characterization of Homer as a blind bard begins in extant literature with the last verse in the Delian Hymn to Apollo, the third of the Homeric Hymns,[35] later cited to support this notion by Thucydides.[36] The author of the hymn claims to be a blind bard from Chios. This claim is quite different from the mere attribution of the hymn to Homer by a third party from a different time. The claim cannot be false without the supposition of a deliberate fraud, rather than a mere mistake. Also, critics have long taken as self-referential[37] a passage in the Odyssey describing a blind bard, Demodocus, in the court of the Phaeacian king, who recounts stories of Troy to the shipwrecked Odysseus.[38]		Despite the insistence of the surviving sources that Homer was blind, there are many serious objections to the "blind" theory. A few of the vitae imply that he was not blind. If he could not write, then he was illiterate and incapable of composition. A large poem would have been beyond the capacity of human memory without the assistance of written cues. Moreover, the images in the poem are very graphic, but a blind man would never have experienced the scenes of the images. Answers exist to all the objections.[39] The example of John Milton, who composed and dictated Paradise Lost while totally blind, demonstrates that a blind man can compose an epic. Albert B. Lord's The Singer of Tales, on the topic of epics sung by modern rhapsodes, shows that epics of that size have been in fact being composed spontaneously from memorized elements in modern times. The problem of visual cues can be solved if Homer can be presumed not to have been blind from birth, but to have become blind, which is the point of view of Pseudo-Herodotus.		In the latter source, Homer, after losing his sight to disease, embarks on a career as a wandering rhapsode, or impromptu composer of poems at public gatherings. Either at the beginning of his career or early in it, he assumes a stage name, reputedly "the blind man", which declares himself to be in the category of blind prophets, who see with inspired inner vision, but not with outer, bringing a sort of divine glamor to the performance. Not all the vitae agree on the meaning of the name. There is nothing biological about the Greek roots. The word is segmented Hom-eros, where Hom is from Greek homou, "together",[40] and the second -ar- in arariskein, "join together",[41] the eta in -eros being East Greek. The "blind" meaning joins together the blind man and his guide. Other unions are certainly possible, provided they are attested. Gregory Nagy uses a phrase, phone homereusai, "fitting [the song] together with the voice" found in Hesiod, a contemporary of Homer, to interpret Homeros as "he who fits (the song) together".[42]		Consideration of the name as a type leaves open the possibility that any rhapsode could conform to it—that is, there was no biographic original named Homer. West says, "The probability is that 'Homer' was not the name of a historical Greek poet but is the imaginary ancestor of the Homeridai; such guild-names in -idai and -adai are not normally based on the name of an historical person."[30] They were upholding their function as rhapsodes or "lay-stitchers" specialising in the recitation of Homeric poetry.		William Ihne examining the sources counted as many 19 locations in classical times that claimed Homer as a citizen, including Athens, which accepted Smyrna as Homer’s native city, but insisted the city was its colony. The cause of these multiple claims was civic competition for the honor.[43] Ihne chose Smyrna because some of the Vitae identify the word Homer as Aeolic, and Smyrna had an Aeolic background. These circumstances give precedence to the longest, most detailed vita, that of Pseudo-Herodotus, which is one of the sources that identify Smyrna as originally Aeolian.		The Aeolians were one of the three major ethnic groups of ancient Greece, the other two being Ionians and Dorians. Aeolians came mainly from Thessaly, occupying also Boeotia at an early date, after the Trojan War, in parallel to the occupation of Peloponnesus by the Dorians. They had their own dialect of East Greek. Hesiod as a Boeotian was a member of the group, which is substantiated by the Aeolic phrases related to the name of Homer found in his works. The Aeolians colonized the northwest coast of Asia Minor, calling their region Aeolis, and Lesbos.[44] The villages to which they immigrated were already populated by the descendants of the Trojan War population. They were keeping the lore alive, according to Pseudo-Herodotus. Aeolis extended from the coast opposite Lesbos to Smyrna on the edge of Ionia. The Aeolian League contained 12 cities, including Smyrna. To the south were the 12 cities, or dodecapolis, of the Ionian League. At about 688 BCE Smyrna was taken by Colophonians who had ostensibly come to a festival there and it passed into Ionian hands.[45]		The political relevance of the two leagues came to a practical end in the latter half of the 5th century BCE when most of the cities around the Aegean joined, or were forced to join, the Delian League, a koine implementing the hegemony of Athens. Each city must contribute men and ships or money to a common defense force. The treasury was kept at Athens. The details and conjoined events are the topic of Thucydides’ History of the Peloponnesian War. Inscriptions from those times offer a basis for the study of Aeolic. Buck distinguished three dialects, Thessalian, Boeotian, and Lesbian.[46]		The Ionian cities in Asia Minor spoke a dialect of Ionic. In the border region between Ionia and Aeolis it was modified to include features taken from Aeolic, creating an Ionic-Aeolic mixture similar to that of the Homeric poems.[47] For example, Chios had always been a member of the Ionian League,[48] and yet Chian “contains a few special characteristics, which are of Aeolic origin.”[49] The same sort of admixture did not occur at the Ionic-Dorian border in southwestern Anatolia.		From the fact that Lesbian acquired more Ionic features in poetry over the course of time Janko argues for “a northward expansion of Ionian population and speech at the expense of the Aeolians.”[50] Aeolic was gradually assimilating to Ionic, but after the 5th century BCE both began to assimilate to the now widespread sister dialect of Ionic, Attic, and the koine that developed from it in the Hellenistic period. Attic began to appear in the inscriptions of Ionia in the 4th century BCE and had displaced Ionian by about 100 BCE. In 281 BCE the new kingdom of Pergamon acquired the Aeolic coast of Anatolia, separating Lesbian, which was gone from the kingdom by the 3rd century BCE. Lesbian went on until the 1st century CE and was the last Aeolic dialect to disappear.[51]		G.S. Kirk, who tends to be somewhat skeptical concerning the biographic details given in the vitae, at least extends a limited credibility to some basic circumstances as “at all plausible.” Homer is most frequently said to have been born in the Ionian region of Asia Minor, at Smyrna, or on the island of Chios, dying on the Cycladic island of Ios.[21] These areas were either Aeolian or partially so. Smyrna had not yet been taken by the Ionians. Chios had been settled by pre-Hellenic tribesmen from Thessaly, but the language remains unknown. They may have been Aeolic-speaking. The association with Chios dates back to at least Semonides of Amorgos, who cited Iliad 6.146 as by "the man of Chios".[52] An eponymous bardic guild, known as the Homeridae (sons of Homer), or Homeristae ('Homerizers')[53] existed there, tracing descent from an ancestor of that name. On Ios were used some words known to be Aeolic; for example, Homêreôn was one of the names for a month in the calendar of Ios.[54] The Smyrna connection is alluded to in the original name posited for him by several vitae: Melesigenes, “born of Meles", a river which flowed by that city.		The poems give evidence of familiarity with the natural details and place-names of this area of Asia Minor;[55] for example, Homer refers to meadow birds at the mouth of the Caystros,[56] a storm in the Icarian sea,[57] and mentions that women in Maeonia and Caria stain ivory with scarlet.[58] However, Homer also had a geographical knowledge of all Mycenaean Greece that has been verified by discovery of most of the sites. Wilhelm Dörpfeld, the classical archaeologist,[59] suggests that Homer had visited many of the places and regions which he describes in his epics, such as Mycenae, Troy and more. According to Diodorus Siculus, Homer had even visited Egypt.[60]		The Roman satirist Lucian depicts Homer as a Babylonian named Tigran, who accepted the name Homer after he had been taken captive by the Greeks claiming that his name Όμηρος means hostage.[61] Some vitae depict Homer as a wandering minstrel, like Thamyris[62] or Hesiod, who walked as far as Chalkis to sing at the funeral games of Amphidamas.[63] We are given the image of a "blind, begging singer who hangs around with little people: shoemakers, fisherman, potters, sailors, elderly men in the gathering places of harbour towns".[64] The poems, on the other hand, give us evidence of singers at the courts of the nobility. There is a strong aristocratic bias in the poems demonstrated by the lack of any major protagonists of non-aristocratic stock, and by episodes such as the beating down of the commoner Thersites by the king Odysseus for daring to criticize his superiors. Scholars are divided as to which category, if any, the court singer or the wandering minstrel, the historic "Homer" belonged.[65]		Most of the twelve vitae have little concern for historicity. Scorialenses I says “we only hear the report, and do not know anything.” Most therefore report several origin stories. They are typically at least in part mythical. Whether the latter are given unfeigned credibility is not clear. For instance, Homer was the son of the river Meles and a nymph. Pseudo-Plutarch I, relying less on mythology, presents an alternative genealogy that makes Homer and Hesiod cousins. The only account that presumes a historical character and a real-life setting without resorting to mythology is the more lengthy Pseudo-Herodotus' Life of Homer.		The Greeks of the sixth and early fifth centuries BCE understood by the works of "Homer", generally, "the whole body of heroic tradition as embodied in hexameter verse".[66] The entire Epic Cycle was included. The genre included further poems on the Trojan War, such as the Little Iliad, the Nostoi, the Cypria, and the Epigoni, as well as the Theban poems about Oedipus and his sons. Other works, such as the corpus of Homeric Hymns, the comic mini-epic Batrachomyomachia ("The Frog-Mouse War"), and the Margites, were also attributed to him. Two other poems, the Capture of Oechalia and the Phocais, were also assigned Homeric authorship.		Herodotus mentions both the Iliad and the Odyssey as works of Homer.[67] He quotes a few lines from them both, which are the same in today's editions. The passage quoted from the Iliad mentions that Paris stopped at Sidon before bringing Helen to Troy. From the fact that the Cypria has Paris going directly to Troy from Sparta, Herodotus concludes that it was not written by Homer.		In Works and Days, Hesiod says that he crossed to Euboea to contend in the games held by the sons of Amphidamas at Chalcis.[68] There he won with a hymnos and took away the prize of a tripod, which he dedicated to the Muses of Mount Helicon, where he first began with aoide, "song". One of the vitae, the Certamen, picks up this theme. Homer and Hesiod were contemporaries, it says. They both attended the funeral games of Amphidamas, conducted by his son, Ganyctor, and both contended in the contest of sophia, "wit". In it, one was required to ask a question of the other, who must reply in verse.		Unable to decide, the judge had them each recite from their poems. Hesiod quoted Works and Days; Homer, the Iliad, both citing texts as they are now. But neither poem can then have been the modern version. Hesiod cannot have described beforehand the very event in which he was participating. The Iliad is supposed to have been written already, but it is not called that, however. The victory was given to Hesiod because his poem was about peace, while Homer's poem was about war.		After the contest, Homer continued his wandering, composing and reciting epic poetry. The Certamen mentions the Thebaid, quoting the first line, which differs but little from the first line of the Iliad as it is now. It had 7,000 lines, as did the subsequent Epigoni, with a similar first line. The Certamen qualifies the attribution to Homer with "some say …" Subsequently, he wrote the epitaph for the tomb of Midas, for which he got a silver bowl, and then the Odyssey in 12,000 lines (today's is 12,110). He had already written the Iliad in 15,500 lines (today's is 15,693). Just these three epics alone are 34,500 lines, word-for-word, we are asked to believe, without reference to the rest of the prodigious Epic Cycle. Then he went to Athens, and to Argos, where he delivered lines 559–568 of Book 2 of the Iliad, with the addition of two more not in the current version. Subsequently, he went to Delos, where he delivered the "Hymn to Apollo", and was made a citizen of all the Ionian states. Finally he went to Ios, where he slipped on some clay and suffered a fatal fall.		The term Epikos Kuklos ("Epic Cycle") refers to a series of ten epic poems written by different authors purporting to tell an interconnected sequence of stories covering all Greek mythology. Themes were selected from them for Greek drama as well. The name appears in the Chrestomathia of Eutychius Proclus, a synopsis of Greek literature, known only through further abridged fragments written by Patriarch Photios I of Constantinople. No etymology was given. Evelyn-White hypothesizes that they were "written round" the Iliad and Odyssey and had a "clearly imitative" structure.[69] In this view Homer need have written no more than the Iliad, or the Iliad and Odyssey, with the Homeridae responsible for all the rest. The unity of theme and structure came from the close association of the authors in the guild or school.		Proclus does not subscribe to the authorships of the Certamen. He provides the names of other authors where they were available in his sources. These 10 epics, of which only Photius' abridgements of Proclus' synopses survive, and scattered fragments of other authors in other times, are as follows. First and oldest, the Titanomachia ("War of the Titans"), eight fragments, is said to have been written by either Eumelus of Corinth, floruit 760–740 BCE, or Arctinus of Miletus, floruit in the First Olympiad, starting 776 BCE.[70]		The Theban Cycle consists of three epics:[71] the Oidipodeia ("Story of Oedipus"), 6600 lines by Cinaethon of Sparta, floruit 764 BCE;[72] the Thebais ("Thebaid"), attributed to Homer;[73] and Epigonoi ("Epigoni"), attributed to Homer.[74] The Trojan Cycle consists of six epics and the Iliad and Odyssey, eight in all:[69] Kupria ("Cypria") in 11 books, attributed to either Homer, Stasinus, a younger contemporary of Homer, or one Hegesias;[75] Aethiopis in five books, sequent of the Iliad, which is sequent of Kupria, by Arctinus;[76] Ilias Mikra ("Little Iliad") in four books by Lesches of Mitylene, floruit 660 BCE;[77] Iliou Persis ("Sack of Ilium") by Arctinus;[78] Nostoi ("Returns") by Agias of Troezen,[79] floruit 740 BCE; and Telegonia ("Telegony"), by Eugammn of Cyrene, floruit 567 BCE.[80]		The idea that Homer was responsible for just the two outstanding epics, the Iliad and the Odyssey, did not win consensus until 350 BCE.[81] Although some scholars, such as W. B. Stanford,[82] argue that the stylistic similarities are too consistent to support the theory of multiple authorship, more recent scholars, such as Gregory Nagy[83] and Martin West,[84] find it unlikely that both epics were composed by the same person. Martin West writes: "Most scholars nowadays consider that the Iliad and the Odyssey are the work of different authors. This is what is indicated by the many differences of narrative manner, theology, ethics, vocabulary, and geographical perspective, and by the apparently imitative character of certain passages of the Odyssey in relation to the Iliad."[85]		One view which attempts to bridge the differences holds that the Iliad was composed by "Homer" in his maturity, while the Odyssey was a work of his old age. The Batrachomyomachia, Homeric Hymns and cyclic epics are generally agreed to be later than the Iliad and the Odyssey.[citation needed]		Most scholars agree that the Iliad and Odyssey underwent a process of standardisation and refinement out of older material beginning in the 8th century BCE. An important role in this standardisation appears to have been played by the Athenian tyrant Hipparchus, who reformed the recitation of Homeric poetry at the Panathenaic festival. Many classicists hold that this reform must have involved the production of a canonical written text.		Other scholars[who?] still support the idea that Homer was a real person. Since nothing is known about the life of this Homer, the common joke—also recycled with regard to Shakespeare—has it that the poems "were not written by Homer, but by another man of the same name."[86][87] Samuel Butler argues, based on literary observations, that a young Sicilian woman wrote the Odyssey (but not the Iliad),[88] an idea further pursued by Robert Graves in his novel Homer's Daughter and Andrew Dalby in Rediscovering Homer.[89]		Independent of the question of single authorship is the near-universal agreement, after the work of Milman Parry,[90] that the Homeric poems are dependent on an oral tradition, a generations-old technique that was the collective inheritance of many singer-poets (aoidoi). An analysis of the structure and vocabulary of the Iliad and Odyssey shows that the poems contain many formulaic phrases typical of extempore epic traditions; even entire verses are at times repeated. Parry and his student Albert Lord pointed out that such elaborate oral tradition, foreign to today's literate cultures, is typical of epic poetry in a predominantly oral cultural milieu, the key words being "oral" and "traditional". Parry started with "traditional": the repetitive chunks of language, he said, were inherited by the singer-poet from his predecessors, and were useful to him in composition. Parry called these repetitive chunks "formulas".		Exactly when these poems would have taken on a fixed written form is subject to debate. The traditional solution is the "transcription hypothesis", wherein a non-literate "Homer" dictates his poem to a literate scribe between the 8th and 6th centuries BCE. The Greek alphabet was introduced in the early 8th century BCE, so it is possible that Homer himself was of the first generation of authors who were also literate. The classicist Barry B. Powell suggests that the Greek alphabet was invented c. 800 BCE by one man, whom he calls the "adapter," in order to write down oral epic poetry.[91] More radical Homerists like Gregory Nagy contend that a canonical text of the Homeric poems as "scripture" did not exist until the Hellenistic period (3rd to 1st century BCE).		New methods also try to elucidate the question. Combining information technologies and statistics stylometry analyzes various linguistic units: words, parts of speech, and sounds. Based on the frequencies of Greek letters, a first study of Dietmar Najock[92] particularly shows the internal cohesion of the Iliad and the Odyssey. Taking into account the repartition of the letters, a recent study of Stephan Vonfelt[93] highlights the unity of the works of Homer compared to Hesiod. The thesis of modern analysts being questioned, the debate remains open.		The study of Homer is one of the oldest topics in scholarship, dating back to antiquity. The aims and achievements of Homeric studies have changed over the course of the millennia. In the last few centuries, they have revolved around the process by which the Homeric poems came into existence and were transmitted over time to us, first orally and later in writing.		Some of the main trends in modern Homeric scholarship have been, in the 19th and early 20th centuries, Analysis and Unitarianism (see Homeric Question), schools of thought which emphasized on the one hand the inconsistencies in, and on the other the artistic unity of, Homer; and in the 20th century and later Oral Theory, the study of the mechanisms and effects of oral transmission, and Neoanalysis, the study of the relationship between Homer and other early epic material.		The language used by Homer is an archaic version of Ionic Greek, with admixtures from certain other dialects, such as Aeolic Greek. It later served as the basis of Epic Greek, the language of epic poetry, typically in dactylic hexameter.		Aristotle remarks in his Poetics that Homer was unique among the poets of his time, focusing on a single unified theme or action in the epic cycle.[94]		The cardinal qualities of the style of Homer are well articulated by Matthew Arnold:		[T]he translator of Homer should above all be penetrated by a sense of four qualities of his author:—that he is eminently rapid; that he is eminently plain and direct, both in the evolution of his thought and in the expression of it, that is, both in his syntax and in his words; that he is eminently plain and direct in the substance of his thought, that is, in his matter and ideas; and finally, that he is eminently noble.[95]		The peculiar rapidity of Homer is due in great measure to his use of hexameter verse. It is characteristic of early literature that the evolution of the thought, or the grammatical form of the sentence, is guided by the structure of the verse; and the correspondence which consequently obtains between the rhythm and the syntax—the thought being given out in lengths, as it were, and these again divided by tolerably uniform pauses—produces a swift flowing movement such as is rarely found when periods are constructed without direct reference to the metre. That Homer possesses this rapidity without falling into the corresponding faults, that is, without becoming either fluctuant or monotonous, is perhaps the best proof of his unequalled poetic skill. The plainness and directness of both thought and expression which characterise him were doubtless qualities of his age, but the author of the Iliad (similar to Voltaire, to whom Arnold happily compares him) must have possessed this gift in a surpassing degree. The Odyssey is in this respect perceptibly below the level of the Iliad.		Rapidity or ease of movement, plainness of expression, and plainness of thought are not distinguishing qualities of the great epic poets Virgil, Dante,[96] and Milton. On the contrary, they belong rather to the humbler epico-lyrical school for which Homer has been so often claimed. The proof that Homer does not belong to that school—and that his poetry is not in any true sense ballad poetry—is furnished by the higher artistic structure of his poems and, as regards style, by the fourth of the qualities distinguished by Arnold: the quality of nobleness. It is his noble and powerful style, sustained through every change of idea and subject, that finally separates Homer from all forms of ballad poetry and popular epic.		Like the French epics, such as the Chanson de Roland, Homeric poetry is indigenous and, by the ease of movement and its resultant simplicity, distinguishable from the works of Dante, Milton and Virgil. It is also distinguished from the works of these artists by the comparative absence of underlying motives or sentiment. In Virgil's poetry, a sense of the greatness of Rome and Italy is the leading motive of a passionate rhetoric, partly veiled by the considered delicacy of his language. Dante and Milton are still more faithful exponents of the religion and politics of their time. Even the French epics display sentiments of fear and hatred of the Saracens; but, in Homer's works, the interest is purely dramatic. There is no strong antipathy of race or religion; the war turns on no political events; the capture of Troy lies outside the range of the Iliad; and even the protagonists are not comparable to the chief national heroes of Greece. So far as can be seen, the chief interest in Homer's works is that of human feeling and emotion, and of drama; indeed, his works are often referred to as "dramas".		The excavations of Heinrich Schliemann at Hisarlik in the late 19th century provided initial evidence to scholars that there was an historical basis for the Trojan War. Research into oral epics in Serbo-Croatian and Turkic languages, pioneered by the aforementioned Parry and Lord, began convincing scholars that long poems could be preserved with consistency by oral cultures until they are written down.[90] The decipherment of Linear B in the 1950s by Michael Ventris (and others) convinced many of a linguistic continuity between 13th century BCE Mycenaean writings and the poems attributed to Homer.		It is probable, therefore, that the story of the Trojan War as reflected in the Homeric poems derives from a tradition of epic poetry founded on a war which actually took place. It is crucial, however, not to underestimate the creative and transforming power of subsequent tradition: for instance, Achilles, the most important character of the Iliad, is strongly associated with southern Thessaly, but his legendary figure is interwoven into a tale of war whose kings were from the Peloponnese.[citation needed] Tribal wanderings were frequent, and far-flung, ranging over much of Greece and the Eastern Mediterranean.[97] The epic weaves brilliantly the disiecta membra (scattered remains) of these distinct tribal narratives, exchanged among clan bards, into a monumental tale in which Greeks join collectively to do battle on the distant plains of Troy.		An account of the transmission of the Iliad from oral tradition through wax pad, papyrus, parchment, to paper (editio princeps) is given by Nioletseas M.M[98] Though evincing many features characteristic of oral poetry, the Iliad and Odyssey were at some point committed to writing. The Greek script, adapted from a Phoenician syllabary around 800 BCE, made possible the notation of the complex rhythms and vowel clusters that make up hexameter verse. Homer's poems appear to have been recorded shortly after the alphabet's invention: an inscription from Ischia in the Bay of Naples, c. 740 BCE, appears to refer to a text of the Iliad; likewise, illustrations seemingly inspired by the Polyphemus episode in the Odyssey are found on Samos, Mykonos and in Italy, dating from the first quarter of the seventh century BCE. We have little information about the early condition of the Homeric poems, but in the second century BCE, Alexandrian editors stabilized this text from which all modern texts descend. Homer's works, which are about fifty percent speeches,[99] provided models in persuasive speaking and writing that were emulated throughout the ancient and medieval Greek worlds.[100] Fragments of Homer account for nearly half of all identifiable Greek literary papyrus finds in Egypt.[101]		In late antiquity, knowledge of Greek declined in Latin-speaking western Europe and, along with it, knowledge of Homer's poems. It was not until the fifteenth century CE that Homer's work began to be read once more in Italy. By contrast it was continually read and taught in the Greek-speaking Eastern Roman Empire where the majority of the classics also survived. The first printed edition appeared in 1488 (edited by Demetrios Chalkokondyles and published by Bernardus Nerlius (it), Nerius Nerlius, and Demetrius Damilas (el) in Florence, Italy).		One often finds books of the Iliad and Odyssey cited by the corresponding letter of the Greek alphabet, with upper-case letters referring to a book number of the Iliad and lower-case letters referring to the Odyssey. Thus Ξ 200 would be shorthand for Iliad book 14, line 200, while ξ 200 would be Odyssey 14.200. The following table presents this system of numeration:		This is a partial list of translations into English of Homer's Iliad and Odyssey.		
William Clark (August 1, 1770 – September 1, 1838) was an American explorer, soldier, Indian agent, and territorial governor.[1] A native of Virginia, he grew up in prestatehood Kentucky before later settling in what became the state of Missouri. Clark was a planter and slaveholder.[2]		Along with Meriwether Lewis, Clark helped lead the Lewis and Clark Expedition of 1804 to 1806 across the Louisiana Purchase to the Pacific Ocean, and claimed the Pacific Northwest for the United States.[3] Before the expedition, he served in a militia and the United States Army. Afterward, he served in a militia and as governor of the Missouri Territory. From 1822 until his death in 1838, he served as Superintendent of Indian Affairs.						William Clark was born in Caroline County, Virginia, on August 1, 1770, the ninth of ten children of John and Ann Rogers Clark.[4][5] His parents were natives of King and Queen County, and were of English and possibly Scots ancestry.[6] The Clarks were common planters in Virginia, owners of modest estates and a few slaves,[7] and members of the Anglican Church.		Clark did not have any formal education; like many of his contemporaries, he was tutored at home. In later years, he was self-conscious about his convoluted grammar and inconsistent spelling—he spelled "Sioux" 27 different ways in his journals of the Lewis and Clark Expedition—and sought to have his journals corrected before publication.[8] The spelling of American English was not standardized in Clark's youth, but his vocabulary suggests he was well read.[9]		Clark's five older brothers fought in Virginia units during the American Revolutionary War (1775–1783), but William was too young.[6] His oldest brother, Jonathan Clark, served as a colonel during the war, rising to the rank of brigadier general in the Virginia militia years afterward. His second-oldest brother, George Rogers Clark, rose to the rank of general, spending most of the war in Kentucky fighting against British-allied American Indians. After the war, the two oldest Clark brothers made arrangements for their parents and family to relocate to Kentucky.		William, his parents, his three sisters, and the Clark family's slaves arrived in Kentucky in March 1785, having first traveled overland to Redstone Landing in present-day Brownsville, Pennsylvania. They completed the journey down the Ohio River by flatboat. The Clark family settled at "Mulberry Hill", a plantation along Beargrass Creek near Louisville. This was William Clark's primary home until 1803. In Kentucky, his older brother George Rogers Clark taught William wilderness survival skills.[10]		Kentuckians fought the Northwest Indian War against American Indians, who were trying to preserve their territory north of the Ohio River. In 1789, 19-year-old William Clark joined a volunteer militia force under Major John Hardin.[11] Clark kept a detailed journal of the expedition, beginning a lifelong practice. Hardin was advancing against the Wea Indians, who had been raiding settlements in Kentucky, on the Wabash River. In error, the undisciplined Kentucky militia attacked a peaceful Shawnee hunting camp, where they killed a total of eight men, women, and children.[12]		In 1790, Clark was commissioned by General Arthur St. Clair, governor of the Northwest Territory, as a captain in the Clarksville, Indiana militia. One older source says he was sent on a mission to the Creek and Cherokee, whom the US hoped to keep out of the war, in the Southeast. His responsibilities are unclear.[13] He may have visited New Orleans at that time. His travels prevented him from participating in General Josiah Harmar's disastrous campaign into the Northwest Territory that year.[14]		In 1791, Clark served as an ensign and acting lieutenant with expeditions under generals Charles Scott and James Wilkinson.[15] He enlisted in the Legion of the United States and was commissioned as a lieutenant on March 6, 1792 under Anthony Wayne. On September 4, 1792 he was assigned to the 4th Sub-Legion. He was involved in several skirmishes with Indians during the continuing Northwest Indian War.[13] At the Battle of Fallen Timbers in 1794, Clark commanded a company of riflemen who drove back the enemy on the left flank, killing a number of Native Americans and Canadians. This decisive US victory brought the Northwest Indian War to an end. In 1795, Clark was dispatched on a mission to New Madrid, Missouri. Clark also served as an adjutant and quartermaster while in the militia.[15]		William Clark resigned his commission on July 4, 1796 and retired due to poor health,[15] although he was only 26 years old. He returned to Mulberry Hill, his family's plantation near Louisville.[15]		In 1803, Meriwether Lewis recruited Clark, then age 33, to share command of the newly formed Corps of Discovery, whose mission was to explore the territory of the Louisiana Purchase, establish trade with Native Americans and the sovereignty of the US. They were to find a waterway from the US to the Pacific Ocean and claim the Oregon territory for the United States before European nations did.[3] Clark spent three years on the expedition to the Pacific Coast. A slave owner known to deal harshly with his slaves, he brought York, one of his slaves, with him. The indigenous nations treated York with respect, and many of the Native Americans were interested in his appearance, which "played a key role in diplomatic relations".[16][17]		Although Clark was refused a promotion to the rank of captain when Jefferson asked the Senate to appoint him, at Lewis' insistence, he exercised equal authority, and continued the mission. Clark concentrated chiefly on the drawing of maps, the management of the expedition's supplies, and leading hunting expeditions for game.[18]		In 1807, President Jefferson appointed Clark as the brigadier general of the militia in the Louisiana Territory, and the US agent for Indian affairs. At the time, trade was a major goal and the US established the factory system. The government and its appointees licensed traders to set up trading posts in Native American territory. Native American relations were handled in what became the War Department.[15] Clark set up his headquarters in St. Louis, Missouri.		There he became a member of the Freemasons, a secret fraternal group. The records of his initiation do not exist, but on September 18, 1809, Saint Louis Lodge No. 111 issued a traveling certificate for Clark.[19]		As a reward for their contributions during their expedition to the Pacific Lewis and Clark were given government positions. Jefferson appointed Meriwether Lewis territorial governor of Upper Louisiana, commander-in-chief of the militia, and superintendent of Indian Affairs.[20] Although he was in charge of Indian affairs, Clark was under the supervision of the Governor of the Louisiana Territory. The governor had final say of all decisions made in the territory. Although Clark had primary duties in dealing with the Native Americans, "the territorial governor held the title of ex officio superintendent of Indian affairs.[21]		Clark's experiences during his cross-continent expedition gave him the tools to be the ideal candidate for a diplomat to the Native Americans. That was Jefferson's motives behind giving Clark these duties, although it would not be until Madison's presidency that Clark's title became official. President James Madison appointed Clark as Missouri territorial governor and thus ex officio superintendent of Indian affairs in that region, during the summers of 1808 and 1813. In the earlier period, Clark performed the same duties that he would have if he held the title.[22] During the years while Clark held position under Governor Lewis, he was continuously involved in decisionmaking with him. Clark was consulted on affairs on a regular basis. In Louisiana and Missouri, Clark served the United States government for the longest term in history as diplomat to the Native American peoples.		Indian diplomacy occupied much of Clark's time; the dutiful soldier and bureaucrat never wavered in his commitment to an expansionist national agenda that expected Indians to surrender their lands, abandon their traditional ways, and acquiesce to the dictates of the U.S. government. But he was aware of the consequences and he demonstrated genuine concern for the plight of destitute native people increasingly threatened with extinction.[23] Clark's expeditions and frontier settlement gave him unique views and feelings toward Native Americans. He felt as though he held a firm hand when he had to, but at the same time he had passion towards them as people still deserving of rights. At times he was said to be too compassionate. Clark took his position as one of extreme importance to not only the government of the United States, but to the Native American people as well.		Clark recognized Indians' nationalism, their history, language, culture and territory and negotiated treaties between the various nations and his. He tried to protect Indians and preserve their culture by removing them from the evil influences of white society, providing life-saving inoculations, having their portraits painted, and assembling a museum of Indian artifacts. At the same time, he removed Indians from their ancestral lands; encouraged federal "civilization" and "education" programs to change native lifestyles, religious beliefs, and cultural practices; and usually promoted the interests of American citizens over Indian needs and desires.[24]		During the War of 1812, Clark led several campaigns, among them in 1814, one along the Mississippi River, up to the Prairie du Chien-area. He established the short-lived Fort Shelby, the first post in what is now Wisconsin. Soon, the post was captured by the British. When the Missouri Territory was formed in 1813, Clark was appointed as the governor by President Madison.[15] He was reappointed to the position by Madison in 1816, and in 1820 by President Monroe.[15]		William Clark appeared before Supreme Court Judge John B.C. Lucas in St. Louis on July 6, 1813, to take the oath of office as governor of the Missouri Territory.[25] Clark's road to a gubernatorial appointment was long and complex. Upon Lewis' appointment by Jefferson, Clark backed him and at times filled the role of governor without holding official position, due to Lewis' complications in life, whether it was debt, loneliness, or drinking. Upon the death of Lewis in 1809, Clark declined to take office for varying reasons.		By the time he was appointed governor, Clark appreciated his own capabilities and embraced them rather than turning them away. when he took office, America was involved in the War of 1812 with the British. Clark feared the influence the British would have on the Native Americans. British tactics would include the use of Indians as allies in the fighting against the United States. In return for British victory, Indians would either be able to continue to occupy their current land or receive lands back that were taken from them previously by the United States Government. Clark held office for the next seven years until he was voted out of office in 1820, in the first election after Missouri became a state. He was defeated by Alexander McNair.		In 1822, Clark was appointed Superintendent of Indian Affairs by President James Monroe, a new position created by Congress after the factory system was abolished.[15] Clark served in that position until his death; his title changed with the creation of the Office of Indian Affairs in 1824 and finally the Bureau of Indian Affairs in 1829, both within the War Department. From 1824 to 1825, he was additionally appointed surveyor general of Illinois, Missouri and the Territory of Arkansaw. It was around this time that Clark received as a gift from a Potowatomi chief in Missouri a rare smoking pipe or calumet that is now in the British Museum's collection.[26]		As the Superintendent of Indian Affairs, Clark was the most important man on Native American matters west of the Mississippi. As superintendent at St. Louis, Clark took on some additional duties: he issued licenses and granted passports to traders and travelers; provided payments for injuries and injustices to both whites and Indians; invoked military force to arrest lawbreakers; prevented or terminated hostilities between tribes; removed unauthorized persons from Indian country or confiscated their property; established, marked, and surveyed boundaries; distributed annuities and made sure that treaty provisions were delivered; and conducted treaty councils.[27] Of the four superintendents of Indian affairs, the others were the governors of Michigan, Florida and Arkansas territories; Clark had by far the largest superintendency.[27]		Though Clark tried to maintain peaceful relations with indigenous nations and negotiated peace treaties, he was involved in President Andrew Jackson's Indian removal policy. This included "his duty to oversee removal". He managed retaliation against Black Hawk and those allied with him in the Black Hawk War, when hostilities arose between them and the Americans. Clark issued "an extermination order", which he gave to Lewis Cass, a man who played a central role in Jackson's removal policy.[28]		Clark believed in the Jeffersonian ideology in which assimilation would be the best course of action for Native Americans. However, in the end, relocation of the Indians from their native lands became the government's primary goal. Clark's government position on Native American affairs kept him at the forefront of countless relocations. He expressed sympathy for those uprooted tribes and promoted their interests as he understood them, nevertheless, he agreed with and implemented the policy of Indian removal, negotiating 37, or one-tenth, of all ratified treaties between American Indians and the United States. Over the course of his career, millions of acres passed from Indian to U.S. ownership through Clark's hand.[29]		After returning from his cross-country expedition, Clark married Julia Hancock on January 5, 1808, at Fincastle, Virginia. They had five children:[15] Meriwether Lewis Clark, Sr. (1809–1881), named after his friend and expedition partner; William Preston Clark (1811–1840); Mary Margaret Clark (1814–1821); George Rogers Hancock Clark (1816–1858), named after Clark's older brother; and John Julius Clark (1818–1831), named after his oldest brother Jonathan and Clark's wife.		After Julia's death in 1820, William Clark married Julia's first cousin, Harriet Kennerly Radford.[30] They had three children together: Jefferson Kearny Clark (1824–1900), named after the president; Edmund Clark (1826–1827), named after another of his older brothers; and Harriet Clark, named after her mother (dates unknown; died as child). His second wife Harriet died in 1831.		His son by a sister of Chief Red Grizzly Bear, Tzi-Kal-Tza/Halahtookit Clark, was alive in 1866 and features in a photo taken that year. "Oral history says that this man called himself 'Clark', that he was captured at the end of the Nez Perce Indian war of 1877, and that he died of malaria fifteen hundred miles away from home. There are many gaps in the story of Clark's Nez Perce son, many questions for which historians will never find answers. What is undisputed, however, is that sixty years after befriending the men of the Lewis and Clark expedition, the Nez Perce had ceded nine-tenths of their tribal lands to the United States government, and by October 1877, the last of the free Nez Perce were confined to the reservation at Lapwai or held in a prison camp in Indian Territory."[31] Clark also served as a guardian to Jean Baptiste Charbonneau, the son of Sacagawea and Toussaint Charbonneau.		William Clark died in St. Louis on September 1, 1838 at age 68. He was buried in the Bellefontaine Cemetery, where a 35-foot (11 m) gray granite obelisk was erected to mark his grave. Clark was originally buried at his nephew John O'Fallon's property, in 1838. That area is now known as O'Fallon Park. The funeral procession stretched for more than a mile and cannons fired a military salute. The entire city of St. Louis mourned his passing.[32]		Clark and six of his family members were later buried at Bellefontaine Cemetery on October 23, 1860. The monument that marks their graves was dedicated in 1904 on the centennial anniversary of the Louisiana Purchase. Clark's son, Jefferson Kearney Clark, designed the monument and paid $25,000 for it ($425,000 in 2005 figures). Jefferson Clark's wife had to complete the building of the monument after Jefferson died in 1900. Many years later, the monument was restored and rededicated on May 21, 2004, to mark the bicentennial of the Corps of Discovery's departure from St. Charles, Missouri. Members of the Shoshone, Osage, and Mandan tribes spoke at the ceremony, marking Clark's service to these Indian nations during the final years of his life.[32]		Captain Meriwether Lewis and Lieutenant William Clark commanded the Corps of Discovery to map the Pacific Northwest. They were honored with a 3-cent stamp July 24, 1954 on the 150th anniversary. The 1803 Louisiana Purchase doubled the size of the United States. Lewis and Clark described and sketched flora and fauna and described the native inhabitants they encountered before returning to St. Louis in 1806.[35]		Both Lewis and Clark appear on the gold Lewis and Clark Exposition dollars minted for the Lewis and Clark Centennial Exposition. Among the Early United States commemorative coins, they were produced in both 1904 and 1905 and survive in relatively small numbers.		The Lewis and Clark expedition was celebrated on May 14, 2004, the 200th anniversary of its outset depicting the two on a hilltop outlook. Two companion 37-cent stamps showed portraits of Meriwether Lewis and William Clark. A special 32-page booklet accompanied the issue in eleven cities along the route taken by the Corps of Discovery. An image of the stamp can be found on Arago online at the link in the footnote.[36]		
In positive psychology, flow, also known as the zone, is the mental state of operation in which a person performing an activity is fully immersed in a feeling of energized focus, full involvement, and enjoyment in the process of the activity. In essence, flow is characterized by complete absorption in what one does and loses sense of space and time.		Named by Mihály Csíkszentmihályi, the concept has been widely referenced across a variety of fields (and has an especially big recognition in occupational therapy), though the concept has existed for thousands of years under other guises, notably in some Eastern religions.[1] Achieving flow is often colloquially referred to as being in the zone.		Flow shares many characteristics with hyperfocus. However, hyperfocus is not always described in a positive light. Some examples include spending "too much" time playing video games or getting side-tracked and pleasurably absorbed by one aspect of an assignment or task to the detriment of the overall assignment. In some cases, hyperfocus can "capture" a person, perhaps causing them to appear unfocused or to start several projects, but complete few.						Jeanne Nakamura and Csíkszentmihályi identify the following six factors as encompassing an experience of flow:[2]		Those aspects can appear independently of each other, but only in combination do they constitute a so-called flow experience. Additionally, psychology writer Kendra Cherry has mentioned three other components that Csíkszentmihályi lists as being a part of the flow experience:[3]		Just as with the conditions listed above, these conditions can be independent of one another.		Flow is so named because during Csíkszentmihályi's 1975 interviews several people described their "flow" experiences using the metaphor of a water current carrying them along.[4]		Mihaly Csikszentmihályi and his fellow researchers began researching flow after Csikszentmihályi became fascinated by artists who would essentially get lost in their work. Artists, especially painters, got so immersed in their work that they would disregard their need for food, water and even sleep. Thus, the origin of research on the theory of flow came about when Csikszentmihályi tried to understand this phenomenon experienced by these artists. Flow research became prevalent in the 1980s and 1990s, with Csikszentmihályi and his colleagues in Italy still at the forefront. Researchers interested in optimal experiences and emphasizing positive experiences, especially in places such as schools and the business world, also began studying the theory of flow at this time. The theory of flow was greatly used in the theories of Abraham Maslow and Carl Rogers in their development of the humanistic tradition of psychology.[2]		Flow has been recognized throughout history and across cultures. The teachings of Buddhism and of Taoism speak of a state of mind known as the "action of inaction" or "doing without doing" (wu wei in Taoism) that greatly resembles the idea of flow. Also, Hindu texts on Advaita philosophy such as Ashtavakra Gita and the Yoga of Knowledge such as Bhagavad-Gita refer to a similar state.		In every given moment, there is a great deal of information made available to each individual. Psychologists have found that one's mind can attend to only a certain amount of information at a time. According to Csikszentmihályi's 2004 TED talk, that number is about "110 bits of information per second".[5] That may seem like a lot of information, but simple daily tasks take quite a lot of information. Just decoding speech takes about 60 bits of information per second.[5] That is why when having a conversation one cannot focus as much attention on other things.		For the most part (except for basic bodily feelings like hunger and pain, which are innate), people are able to decide what they want to focus their attention on. However, when one is in the flow state, they are completely engrossed with the one task at hand and, without making the conscious decision to do so, lose awareness of all other things: time, people, distractions, and even basic bodily needs. This occurs because all of the attention of the person in the flow state is on the task at hand; there is no more attention to be allocated.[6]		The flow state has been described by Csikszentmihályi as the "optimal experience" in that one gets to a level of high gratification from the experience.[7] Achieving this experience is considered to be personal and "depends on the ability" of the individual.[7] One's capacity and desire to overcome challenges in order to achieve their ultimate goals not only leads to the optimal experience, but also to a sense of life satisfaction overall.[7]		There are three common ways to measure flow experiences: the flow questionnaire (FQ), the experience sampling method (ESM), and the "standardized scales of the componential approach".[8]		The FQ requires individuals to identify definitions of flow and situations in which they believe that they have experienced flow, followed by a section that asks them to evaluate their personal experiences in these flow-inducing situations. The FQ identifies flow as a single construct, therefore allowing the results to be used to estimate differences in the likelihood of experiencing flow across a variety of factors. Another strength of the FQ is that it does not assume that everyone's flow experiences are the same. Because of this, the FQ is the ideal measure for estimating the prevalence of flow. However, the FQ has some weaknesses that more recent methods have set out to address. The FQ does not allow for measurement of the intensity of flow during specific activities. This method also does not measure the influence of the ratio of challenge to skill on the flow state.[8]		The ESM requires individuals to fill out the experience sampling form (ESF) at eight randomly chosen time intervals throughout the day. The purpose of this is to understand subjective experiences by estimating the time intervals that individuals spend in specific states during everyday life. The ESF is made up of 13 categorical items and 29 scaled items. The purpose of the categorical items is to determine the context and motivational aspects of the current actions (these items include: time, location, companionship/desire for companionship, activity being performed, reason for performing activity). Because these questions are open-ended, the answers need to be coded by researchers. This needs to be done carefully so as to avoid any biases in the statistical analysis. The scaled items are intended to measure the levels of a variety of subjective feelings that the individual may be experiencing. The ESM is more complex than the FQ and contributes to the understanding of how flow plays out in a variety of situations, however the possible biases make it a risky choice.[8]		Some researchers are not satisfied with the methods mentioned above and have set out to create their own scales. The scales developed by Jackson and Eklund are the most commonly used in research, mainly because they are still consistent with Csíkszentmihályi's definition of flow and consider flow as being both a state and a trait. Jackson and Eklund created two scales that have been proven to be psychometrically valid and reliable: the Flow State Scale-2 (which measures flow as a state) and the Dispositional Flow Scale-2 (designed to measure flow as either a general trait or domain-specific trait). The statistical analysis of the individual results from these scales gives a much more complete understanding of flow than the ESM and the FQ.[8]		A flow state can be entered while performing any activity, although it is most likely to occur when one is wholeheartedly performing a task or activity for intrinsic purposes.[6][10] Passive activities like taking a bath or even watching TV usually do not elicit flow experiences as individuals have to actively do something to enter a flow state.[11][12] While the activities that induce flow may vary and be multifaceted, Csikszentmihályi asserts that the experience of flow is similar despite the activity.[13]		Flow theory postulates three conditions that have to be met to achieve a flow state:		However, it was argued that the antecedent factors of flow are interrelated, as a perceived balance between challenges and skills requires that one knows what he or she has to do (clear goals) and how successful he or she is in doing it (immediate feedback). Thus, a perceived fit of skills and task demands can be identified as the central precondition of flow experiences.[15]		In 1987, Massimini, Csíkszentmihályi and Carli published the 8-channel model of flow shown here.[16] Antonella Delle Fave, who worked with Fausto Massimini at the University of Milan, now calls this graph the Experience Fluctuation Model.[17] The Experience Fluctuation Model depicts the channels of experience that result from different levels of perceived challenges and perceived skills. This graph illustrates one further aspect of flow: it is more likely to occur when the activity at hand is a higher-than-average challenge (above the center point) and the individual has above-average skills (to the right of the center point).[6] The center of this graph (where the sectors meet) represents one's average levels of challenge and skill across all activities an individual performs during their daily life. The further from the center an experience is, the greater the intensity of that state of being (whether it is flow or anxiety or boredom or relaxation).[10]		Several problems of this model have been discussed in literature.[15][18] One is, that it does not ensure a perceived balance between challenges and skills which is supposed to be the central precondition of flow experiences. Individuals with a low average level of skills and a high average level of challenges (or the other way round) do not necessarily experience a fit between skills and challenges when both are above their individual average.[19] In addition, one study found that low challenge situations which were surpassed by skill were associated with enjoyment, relaxation, and happiness, which, they claim, is contrary to flow theory.[20]		Schaffer (2013) proposed 7 flow conditions:		Schaffer also published a measure, the Flow Condition Questionnaire (FCQ), to measure each of these 7 flow conditions for any given task or activity.[21]		Some of the challenges to staying in flow include states of apathy, boredom, and anxiety. Being in a state of apathy is characterized when challenges are low and one's skill level is low producing a general lack of interest in the task at hand. Boredom is a slightly different state in that it occurs when challenges are low, but one's skill level exceeds those challenges causing one to seek higher challenges. Lastly, a state of anxiety occurs when challenges are so high that they exceed one's perceived skill level causing one great distress and uneasiness. These states in general differ from being in a state of flow, in that flow occurs when challenges match one's skill level.[22] Consequently, Csíkszentmihályi has said, "If challenges are too low, one gets back to flow by increasing them. If challenges are too great, one can return to the flow state by learning new skills."[3]		Csíkszentmihályi hypothesized that people with several very specific personality traits may be better able to achieve flow more often than the average person. These personality traits include curiosity, persistence, low self-centeredness, and a high rate of performing activities for intrinsic reasons only. People with most of these personality traits are said to have an autotelic personality.[10] The term “autotelic” is acquired from two Greek words, auto, meaning self, and telos meaning goal. Being Autotelic means having a self-contained activity, one that is done not with the expectation of some future benefit, but simply to experience it as the main goal.[23]		At this point, there is not much research on the autotelic personality, but results of the few studies that have been conducted suggest that indeed some people are more prone to experience flow than others. One researcher (Abuhamdeh, 2000) found that people with an autotelic personality have a greater preference for "high-action-opportunity, high-skills situations that stimulate them and encourage growth" compared to those without an autotelic personality.[10] It is in such high-challenge, high-skills situations that people are most likely to enter the flow state.		Experimental evidence shows that a balance between skills of the individual and demands of the task (compared to boredom and overload) only elicits flow experiences in individuals characterized by an internal locus of control[24] or a habitual action orientation.[25] Several correlational studies found need for achievement to be a personal characteristic that fosters flow experiences.[26][27][28]		Group flow is notably different from independent flow as it is inherently mutual. Group flow is attainable when the performance unit is a group, such as a team or musical group. When groups cooperate to agree on goals and patterns, social flow, commonly known as group cohesion, is much more likely to occur. If a group still has not entered flow, a team-level challenge may stimulate the group to harmonize.[29]		Only Csíkszentmihályi seems to have published suggestions for extrinsic applications of the flow concept, such as design methods for playgrounds to elicit the flow experience. Other practitioners of Csíkszentmihályi's flow concept focus on intrinsic[disambiguation needed] applications, such as spirituality, performance improvement, or self-help. His work has also informed the measurement of donor momentum by The New Science of Philanthropy.		In education, the concept of overlearning plays a role in a student's ability to achieve flow. Csíkszentmihályi[30] states that overlearning enables the mind to concentrate on visualizing the desired performance as a singular, integrated action instead of a set of actions. Challenging assignments that (slightly) stretch one's skills lead to flow.[31]		In the 1950s British cybernetician Gordon Pask designed an adaptive teaching machine called SAKI, an early example of "e-learning". The machine is discussed in some detail in Stafford Beer's book "Cybernetics and Management".[32] In the patent application for SAKI (1956),[33] Pask's comments (some of which are included below) indicate an awareness of the pedagogical importance of balancing student competence with didactic challenge, which is quite consistent with flow theory:		If the operator is receiving data at too slow a rate, he is likely to become bored and attend to other irrelevant data.		If the data given indicates too precisely what responses the operator is required to make, the skill becomes too easy to perform and the operator again tends to become bored.		If the data given is too complicated or is given at too great a rate, the operator is unable to deal with it. He is then liable to become discouraged and lose interest in performing or learning the skill.		Ideally, for an operator to perform a skill efficiently, the data presented to him should always be of sufficient complexity to maintain his interest and maintain a competitive situation, but not so complex as to discourage the operator. Similarly these conditions should obtain at each stage of a learning process if it is to be efficient. A tutor teaching one pupil seeks to maintain just these conditions.		Around 2000, it came to the attention of Csíkszentmihályi that the principles and practices of the Montessori Method of education seemed to purposefully set up continuous flow opportunities and experiences for students. Csíkszentmihályi and psychologist Kevin Rathunde embarked on a multi-year study of student experiences in Montessori settings and traditional educational settings. The research supported observations that students achieved flow experiences more frequently in Montessori settings.[34][35][36]		Musicians, especially improvisational soloists, may experience a state of flow while playing their instrument.[37] Research has shown that performers in a flow state have a heightened quality of performance as opposed to when they are not in a flow state. In a study performed with professional classical pianists who played piano pieces several times to induce a flow state, a significant relationship was found between the flow state of the pianist and the pianist's heart rate, blood pressure, and major facial muscles. As the pianist entered the flow state, heart rate and blood pressure decreased and the major facial muscles relaxed. This study further emphasized that flow is a state of effortless attention. In spite of the effortless attention and overall relaxation of the body, the performance of the pianist during the flow state improved.[38]		Groups of drummers experience a state of flow when they sense a collective energy that drives the beat, something they refer to as getting into the groove or entrainment. Likewise drummers and bass guitarists often describe a state of flow when they are feeling the downbeat together as being in the pocket.[39]		The concept of being in the zone during an athletic performance fits within Csíkszentmihályi's description of the flow experience, and theories and applications of being in the zone and its relationship with athletic competitive advantage are topics studied in the field of sport psychology.[40]		Timothy Gallwey's influential works on the "inner game" of sports such as golf and tennis described the mental coaching and attitudes required to "get in the zone" and fully internalize mastery of the sport.[41]		Roy Palmer suggests that "being in the zone" may also influence movement patterns as better integration of the conscious and subconscious reflex functions improves coordination. Many athletes describe the effortless nature of their performance while achieving personal bests.[42][43][44]		Mixed martial arts champion and Karate master Lyoto Machida uses meditation techniques before fights to attain mushin, a concept that, by his description, is in all respects equal to flow.		The Formula One driver Ayrton Senna, during qualifying for the 1988 Monaco Grand Prix, explained: "I was already on pole, [...] and I just kept going. Suddenly I was nearly two seconds faster than anybody else, including my team mate with the same car. And suddenly I realised that I was no longer driving the car consciously. I was driving it by a kind of instinct, only I was in a different dimension. It was like I was in a tunnel."[45]		Former 500 GP rider Wayne Gardner talking about his victory at the 1990 Australian Grand Prix on The Unrideables 2 documentary said: ''During these last five laps I had this sort of above body experience where actually raised up above and I could see myself racing. It was kind of a remote control and it's the weirdest thing I've ever had in my life. [...] After the race Mick [Doohan] and in fact Wayne Rainey said: ''How the hell did you do that?'' and I said: ''I have no idea.''''[46]		Csíkszentmihályi may have been the first to describe this concept in Western psychology, he was most certainly not the first to quantify the concept of flow or develop applications based on the concept.		For millennia, practitioners of Eastern religions such as Hinduism, Buddhism, Taoism and later in Sufism have honed the discipline of overcoming the duality of self and object as a central feature of spiritual development. Eastern spiritual practitioners have developed a very thorough and holistic set of theories around overcoming duality of self and object, tested and refined through spiritual practice instead of the systematic rigor and controls of modern science.		Csíkszentmihályi's flow concept relates to the idea of being at one with things or as psychology expert, Kendra Cherry, describes it: "complete immersion in an activity".[3] Practitioners of the varied schools of Zen Buddhism apply concepts similar to flow to aid their mastery of art forms, including, in the case of Japanese Zen Buddhism, Aikido, Cheng Hsin, Judo, Honkyoku, Kendo and Ikebana. In yogic traditions such as Raja Yoga, reference is made to a state of flow[47] in the practice of Samyama, a psychological absorption in the object of meditation.[48] Theravada Buddhism refers to "access concentration", which is a state of flow achieved through meditation and used to further strengthen concentration into jhana, and/or to develop insight.		In Islam the first mental state that precedes human action is known as al-khatir. In this state an image or thought is born in the mind.[citation needed]		Flow is one of the main reasons that people play video games.[49] It improves performance; calling the phenomenon "TV trance", a 1981 BYTE article discussed how "the best seem to enter a trance where they play but don't pay attention to the details of the game".[50] The primary goal of games is to create entertainment through intrinsic motivation, which is related to flow; that is, without intrinsic motivation it is virtually impossible to establish flow.[51] Through the balance of skill and challenge the player's brain is aroused, with attention engaged and motivation high.[52] Thus, the use of flow in games helps foster an enjoyable experience which in turn increases motivation and draws players to continue playing. As such, game designers strive to integrate flow principles into their projects.[53] Overall, the experience of play is fluid and is intrinsically psychologically rewarding independent of scores or in-game successes in the flow state.[52]		In games often much can be achieved thematically through an imbalance between challenge level and skill level. Horror games often keep challenges significantly above the player's level of competency in order to foster a continuous feeling of anxiety. Conversely, so called "relaxation games" keep the level of challenges significantly below the player's competency level, in order to achieve a similar but opposite effect.[citation needed] The video game Flow was designed as part of Jenova Chen's master's thesis for exploring the design decisions that allow players to achieve the flow state, by adjusting the difficulty dynamically during play.[54]		Flow in games has been linked to the Laws of Learning as part of the explanation for why learning-games (the use of games to introduce material, improve understanding, or increase retention) have the potential to be effective.[49] In particular, flow is intrinsically motivating, which is part of the Law of Readiness. The condition of feedback, required for flow, is associated with the feedback aspects of the Law of Exercise. This is exhibited in well designed games, in particular, where players perform at the edge of their competency as they are guided by clear goals and feedback.[52] The positive emotions associated with flow are associated with the Law of Effect. The intense experiences of being in a state of flow are directly associated with the Law of Intensity. Thus, the experience of gaming can be so engaging and motivating as it meets many of the Laws of Learning, which are inextricably connected to creating flow.		A simplified modification to flow has been combined with the technology acceptance model (TAM) to help guide the design of and explain the adoption of intrinsically motivated computer systems. This model, the hedonic-motivation system adoption model (HMSAM) is model to improve the understanding of hedonic-motivation systems (HMS) adoption.[51] HMS are systems used primarily to fulfill users' intrinsic motivations, such for online gaming, virtual worlds, online shopping, learning/education, online dating, digital music repositories, social networking, online pornography, gamified systems, and for general gamification. Instead of a minor, TAM extension, HMSAM is an HMS-specific system acceptance model based on an alternative theoretical perspective, which is in turn grounded in flow-based concept of cognitive absorption (CA). The HMSAM further builds on van der Heijden's (2004) model of hedonic system adoption[55] by including CA as a key mediator of perceived ease of use (PEOU) and of behavioral intentions to use (BIU) hedonic-motivation systems. Typically, models simplistically represent "intrinsic motivations" by mere perceived enjoyed. Instead, HMSAM uses the more complex, rich construct of CA, which includes joy, control, curiosity, focused immersion, and temporal dissociation. CA is construct that is grounded in the seminal flow literature, yet ironically CA has traditionally been used as a static construct, as if all five of its subconstructs occur at the same time—in direct contradiction to the flow literature. Thus, part of HMSAM's contribution is to return CA closer to its flow roots by re-ordering these CA subconstructs into more natural process-variance order as predicted by flow. Empirical data collection along with mediation tests further support this modeling approach.		Developers of computer software reference getting into a flow state as "wired in", or sometimes as The Zone,[56][57] hack mode,[58] or operating on software time[59] when developing in an undistracted state. Stock market operators often use the term "in the pipe" to describe the psychological state of flow when trading during high volume days and market corrections. Professional poker players use the term "playing the A-game" when referring to the state of highest concentration and strategical awareness, while pool players often call the state being in "dead stroke".		Conditions of flow, defined as a state in which challenges and skills are equally matched, play an extremely important role in the workplace. Because flow is associated with achievement, its development could have concrete implications in increasing workplace satisfaction and accomplishment. Flow researchers, such as Csikszentmihályi, believe that certain interventions may be performed to enhance and increase flow in the workplace, through which people would gain 'intrinsic rewards that encourage persistence" and provide benefits. In his consultation work, Csikszentmihályi emphasizes finding activities and environments that are conducive to flow, and then identifying and developing personal characteristics to increase experiences of flow. Applying these methods in the workplace, can improve morale by fostering a sense of greater happiness and accomplishment, which may be correlated with increased performance. In his review of Mihály Csikszentmihályi's book "Good Business: Leadership, Flow, and the Making of Meaning," Coert Visser introduces the ideas presented by Csikszentmihályi, including "good work" in which one "enjoys doing your best while at the same time contributing to something beyond yourself."[60] He then provides tools by which managers and employees can create an atmosphere that encourages good work. Some consultants suggest that the experience sampling form (EMS) method be used for individuals and teams in the workplace in order to identify how time is currently being spent, and where focus should be redirected to in order to increase flow experiences.[61]		In order to achieve flow, Csikszentmihályi lays out the following three conditions:		Csikszentmihályi argues that with increased experiences of flow, people experience "growth towards complexity". People flourish as their achievements grow and with that comes development of increasing "emotional, cognitive, and social complexity."[60] Creating a workplace atmosphere that allows for flow and growth, Csikszentmihályi argues, can increase the happiness and achievement of employees. An increasingly popular way of promoting greater flow in the workplace is using Serious Play facilitation methods. Some commercial organisations have used the concept of flow in building corporate branding and identity for example The Floow Limited which created its company brand from the concept.		There are, however, barriers to achieving flow in the workplace. In his chapter "Why Flow Doesn't Happen on the Job," Csikszentmihályi argues the first reason that flow does not occur is that the goals of one's job are not clear. He explains that while some tasks at work may fit into a larger, organization plan, the individual worker may not see where their individual task fits it. Second, limited feedback about one's work can reduce motivation and leaves the employee unaware of whether or not they did a good job. When there is little communication of feedback, an employee may not be assigned tasks that challenge them or seem important, which could potentially prevent an opportunity for flow.		In the study "Predicting flow at work: Investigating the activities and job characteristics that predict flow states at work", Karina Nielsen and Bryan Cleal used a 9- item flow scale to examine predictors of flow at two levels: activity level (such as brainstorming, problem solving, and evaluation) and at a more stable level (such as role clarity, influence, and cognitive demands). They found that activities such as planning, problem solving, and evaluation predicted transient flow states, but that more stable job characteristics were not found to predict flow at work. This study can help us identify which task at work can be cultivated and emphasized in order to help employees experience flow on the job.[62] In her article in Positive Psychology News Daily, Kathryn Britton examines the importance of experiencing flow in the workplace beyond the individual benefits it creates. She writes, "Flow isn't just valuable to individuals; it also contributes to organizational goals. For example, frequent experiences of flow at work lead to higher productivity, innovation, and employee development (Csikszentmihályi, 1991, 2004). So finding ways to increase the frequency of flow experiences can be one way for people to work together to increase the effectiveness of their workplaces."[63]		Books by Csikszentmihályi suggest that enhancing the time spent in flow makes our lives more happy and successful. Flow experiences are predicated to lead to positive affect as well as to better performance.[30][64] For example, delinquent behavior was reduced in adolescents after two years of enhancing flow through activities.[65]		However, further empirical evidence is required to substantiate these preliminary indications, as flow researchers continue to explore the problem of how to directly investigate causal consequences of flow experiences using modern scientific instrumentation to observe the neuro-physiological correlates of the flow state.[66]		Flow is an innately positive experience; it is known to "produce intense feelings of enjoyment".[6] An experience that is so enjoyable should lead to positive affect and happiness in the long run. Also, Csikszentmihályi stated that happiness is derived from personal development and growth – and flow situations permit the experience of personal development.[64]		Several studies found that flow experiences and positive affect go hand in hand,[27][67] and that challenges and skills above the individual's average foster positive affect.[68][69][70] However, the causal processes underlying those relationships remains unclear at present.		Flow experiences imply a growth principle. When one is in a flow state, he or she is working to master the activity at hand. To maintain that flow state, one must seek increasingly greater challenges. Attempting these new, difficult challenges stretches one's skills. One emerges from such a flow experience with a bit of personal growth and great "feelings of competence and efficacy".[14] By increasing time spent in flow, intrinsic motivation and self-directed learning also increases.[71]		Flow has a documented correlation with high performance in the fields of artistic and scientific creativity,[72][73] teaching,[74] learning,[75] and sports;[76][77]		Flow has been linked to persistence and achievement in activities while also helping to lower anxiety during various activities and raise self-esteem.[78]		However, evidence regarding better performance in flow situations is mixed.[66] For sure, the association between the two is a reciprocal one. That is, flow experiences may foster better performance but, on the other hand, good performance makes flow experiences more likely. Results of a longitudinal study in the academic context indicate that the causal effect of flow on performance is only of small magnitude and the strong relationship between the two is driven by an effect of performance on flow.[26] In the long run, flow experiences in a specific activity may lead to higher performance in that activity as flow is positively correlated with a higher subsequent motivation to perform and to perform well.[14]		Csikszentmihályi writes about the dangers of flow himself:		...enjoyable activities that produce flow have a potentially negative effect: while they are capable of improving the quality of existence by creating order in the mind, they can become addictive, at which point the self becomes captive of a certain kind of order, and is then unwilling to cope with the ambiguities of life.		Further, he writes:		The flow experience, like everything else, is not "good" in an absolute sense. It is good only in that it has the potential to make life more rich, intense, and meaningful; it is good because it increases the strengths and complexity of the self. But whether the consequence of any particular instance of flow is good in a larger sense needs to be discussed and evaluated in terms of more inclusive social criteria.[79]		Notes		Bibliography		
Adventure racing (also called expedition racing) is typically a multi-disciplinary team sport involving navigation over an unmarked wilderness course with races extending anywhere from two hours up to two weeks in length. (What Is Adventure Racing Video) Some races offer solo competition as well. The principle disciplines in adventure racing include trekking, mountain biking, and paddling although races can incorporate a multitude of other disciplines including climbing, abseiling, horse riding, skiing and white water rafting.[1] Teams generally vary in gender mix and in size from two to five competitors, however the premier format is considered to be mixed gender teams of four racers. There is typically no suspension of the clock during races, irrespective of length; elapsed competition time runs concurrently with real time, and competitors must choose if or when to rest.						The roots of adventure racing are deep and people debate the origin of the modern adventure race. Some point to the two-day Karrimor International Mountain Marathon, first held in 1968 as the birth of modern adventure racing. The Karrimor Marathon required two-person teams to traverse mountainous terrain while carrying all the supplies required to support themselves through the double-length marathon run.		In 1980, the Alpine Ironman was held in New Zealand. Individual competitors ran, paddled and skied to a distant finish line. Later that year, the Alpine Ironman's creator, Robin Judkins launched the better-known Coast to Coast race, which involved most of the elements of modern adventure racing: trail running, cycling and paddling. Independently, a North American race, the Alaska Mountain Wilderness Classic debuted in 1982 and involved six days of unsupported wilderness racing (carry all food and equipment, no roads, no support) over a 150-mile course. It continues today, changing courses every 3 years.		In 1989, the modern era of adventure racing had clearly arrived with Gerald Fusil's launch of the Raid Gauloises in New Zealand. Inspired by the Paris-Dakar Rally, Fusil envisioned an expanded expedition-style race in which competitors would rely on their own strength and abilities to traverse great and challenging terrain. The race included all the modern elements of adventure racing, including mixed-gender teams competing in a multi-day 400+ mile race. Building on Fusil's concept, the inaugural Southern Traverse was held in 1991.		In the early-90's, Mark Burnett read an L.A. Times article about Raid Gauloises and was inspired to not only take the race to the USA, but to promote the race as a major televised sporting event. After purchasing the rights from Gerald Fusil, Burnett launched the first "Eco-Challenge" race in 1995. Burnett promoted his event with Emmy-award winning films (tapping the talent of Mike Sears to produce the films for the first two events). The Eco-Challenge was last held in 2002. With the Eco-Challenge also came the name "adventure race", a phrase coined by journalist and author Martin Dugard, to describe the class of races embodied by the Raid and Eco-Challenge.		The United States Adventure Racing Association "USARA" was formed in 1998. The USARA was the first national governing body for the sport of adventure racing and arose from the need for safety standards, insurance and to promote the growth of adventure racing in the United States. The USARA has added national rankings, a national championship, ecological standards to the list of benefits provided for the sport of adventure racing.		In 2000, the inaugural United States Adventure Racing Association Adventure Race National Championship was held in Kernville, California. The USARA National Championship is typically held the first weekend in October and is considered the premier adventure race in the U.S. The USARA Adventure Racing National Championship has continued each year drawing the best US teams for a chance at earning the title of national champion.		In 2001, the inaugural World Championships were held in Switzerland with Team Nokia Adventure crossing the finishing line first. The concept of a world championship lay dormant until it was revived in 2004, with Canada's Raid the North Extreme serving as the AR World Championship event in Newfoundland & Labrador. The Adventure Racing World Series and its penultimate event, the AR World Championships have been held every year since. The 2013 World Championships will be the Costa Rica Adventure Racing in Costa Rica.		In 2002, the first major expedition length race to be held exclusively in the United States was launched. Primal Quest has become the premier U.S. expedition race, being held each year since its launch. In 2004, the death of veteran racer Nigel Aylott over-shadowed the race, and raised debates about the nature of Primal Quest and adventure racing.		In 2004, professional geologist Stjepan Pavicic organized the first Patagonian Expedition Race at the bottom tip of the American continent, in the Chilean Tierra del Fuego. Truly demanding routes through rough terrain of often more than 600 km soon made it be known as “the last wild race”.		In 2010, the German Adventure Race Series where held for the first time in three different locations all over Germany. Since then the popularity of the sport in Germany has grown every year. More races and venues have joined the series and the number of competitors are still growing from year to year. Competitors can start in teams of two (male, female or mixed) within the categories Master (15-20 h), Challenger (8-10 h) or Beginner (4-6 h).		In 2012, Commander Forer of the Royal Navy organized the first Sea-land navigation discipline race The Solent Amphibious Challenge. The race demanded the competitors to split up between sailing, running, and cycling in parts of the race and rendezvous at the end and sail the yacht to the finish line.		The majority of adventure races include trail running, mountain biking and a paddling event. Navigation and rope work are also featured in all but the shortest races, but this is only the beginning. Races often feature:		The rules of adventure racing vary by race. However, virtually all races include the rules of racing:		Typically races will feature an organizational meeting either the night before or the morning of the race. At this meeting the course will be revealed for the first time. For sprints, racers may follow a marked course. For longer races, racers may be given maps marked to show checkpoints ("CPs") or racers may be simply given a topographical map and coordinates (usually UTM coordinates) that indicate where the CPs will be found. Special rules, last minute changes and other information may also be provided at the meeting.		Racers are required to visit a series of checkpoints or passport controls (CPs), usually in a specific order.		Most races include one or more transition areas that teams can visit to replenish supplies. Typically, teams change to another mode of travel in a transition area. For instance, teams will end a trekking leg and transition to mountain biking in a transition area. Shorter races often feature a single transition area that teams may visit numerous times during the event. Teams will leave food, water, paddling and biking gear, fresh clothing and any other items they may need during the course of the race.		Longer races feature multiple transition areas. Team gear is transported either by a support crew (provided by the team) or by the racing staff.		Virtually all adventure races feature mandatory gear that must be carried during part or all of the race. Races will often include mandatory pre-race gear checks by race personnel and harsh penalties or disqualification may result if a team lacks the requisite equipment.		In addition to pre-race gear checks, many race organizers also include on-course gear checks. This helps to ensure that teams that start with approved gear, compete with, and finish a race with that same gear.		Adventure races attract individuals of greatly divergent abilities. To make the sport more inclusive, many race directors will "short course" racers; allow racers who miss mandatory time cut-offs to continue racing on a reduced-length course. These racers will often earn an official finish time but be "unranked" and not eligible for prizes. Some races provide the option for teams to skip certain CPs but incur a time penalty (which often must be "served" during the race).		Most adventure races are team events, with expedition length races typically requiring a set number of teammates (usually four or five) and requiring the teams to be co-ed. Many racers find the team aspect of adventure racing to be among the most enticing and demanding aspects.		Teams typically elect a team captain and designate a team navigator. Teams have different views as to the functions of each of these positions, with some teams having very little structure, while others assigned specifics rights and responsibilities to each of these persons. For example, a team that stresses a democratic philosophy may limit the captain’s role to be the keeper of the racing passport and rules, and limit the navigator’s role to carrying the map and being primarily responsible for determining the team’s position at any given time. A more regimented team may give the captain ultimate responsibility for making all decisions regarding rest schedules, rule interpretations and the like, while the navigator has full responsibility for not only tracking the team’s location, but determining route choice as well.		In France, several Grandes écoles have races organized by students.		
Learning is the act of acquiring new or modifying and reinforcing existing knowledge, behaviors, skills, values, or preferences which may lead to a potential change in synthesizing information, depth of the knowledge, attitude or behavior relative to the type and range of experience.[1] The ability to learn is possessed by humans, animals, plants[2] and some machines. Progress over time tends to follow a learning curve. Learning does not happen all at once, but it builds upon and is shaped by previous knowledge. To that end, learning may be viewed as a process, rather than a collection of factual and procedural knowledge. Learning produces changes in the organism and the changes produced are relatively permanent.[3]		Human learning may occur as part of education, personal development, schooling, or training. It may be goal-oriented and may be aided by motivation. The study of how learning occurs is part of educational psychology, neuropsychology, learning theory, and pedagogy. Learning may occur as a result of habituation or classical conditioning, seen in many animal species, or as a result of more complex activities such as play, seen only in relatively intelligent animals.[4][5] Learning may occur consciously or without conscious awareness. Learning that an aversive event can't be avoided nor escaped is called learned helplessness.[6] There is evidence for human behavioral learning prenatally, in which habituation has been observed as early as 32 weeks into gestation, indicating that the central nervous system is sufficiently developed and primed for learning and memory to occur very early on in development.[7]		Play has been approached by several theorists as the first form of learning. Children experiment with the world, learn the rules, and learn to interact through play. Lev Vygotsky agrees that play is pivotal for children's development, since they make meaning of their environment through playing educational games.						Non-associative learning refers to "a relatively permanent change in the strength of response to a single stimulus due to repeated exposure to that stimulus. Changes due to such factors as sensory adaptation, fatigue, or injury do not qualify as non-associative learning."[8]		Non-associative learning can be divided into habituation and sensitization.		Habituation is an example of non-associative learning in which the strength or probability of a response diminishes when the response is repeated. The response is typically a reflex or unconditioned response. Thus, habituation must be distinguished from extinction, which is an associative process. In operant extinction, for example, a response declines because it is no longer followed by reward. An example of habituation can be seen in small song birds—if a stuffed owl (or similar predator) is put into the cage, the birds initially react to it as though it were a real predator. Soon the birds react less, showing habituation. If another stuffed owl is introduced (or the same one removed and re-introduced), the birds react to it again as though it were a predator, demonstrating that it is only a very specific stimulus that is habituated to (namely, one particular unmoving owl in one place). Habituation has been shown in essentially every species of animal, as well as the sensitive plant Mimosa pudica[9] and the large protozoan Stentor coeruleus.[10]		Sensitization is an example of non-associative learning in which the progressive amplification of a response follows repeated administrations of a stimulus (Bell et al., 1995).[citation needed] An everyday example of this mechanism is the repeated tonic stimulation of peripheral nerves that occurs if a person rubs their arm continuously. After a while, this stimulation creates a warm sensation that eventually turns painful. The pain results from the progressively amplified synaptic response of the peripheral nerves warning that the stimulation is harmful.[clarification needed] Sensitisation is thought to underlie both adaptive as well as maladaptive learning processes in the organism.[citation needed]		Active learning occurs when a person takes control of his/her learning experience. Since understanding information is the key aspect of learning, it is important for learners to recognize what they understand and what they do not. By doing so, they can monitor their own mastery of subjects. Active learning encourages learners to have an internal dialogue in which they verbalize understandings. This and other meta-cognitive strategies can be taught to a child over time. Studies within metacognition have proven the value in active learning, claiming that the learning is usually at a stronger level as a result.[12] In addition, learners have more incentive to learn when they have control over not only how they learn but also what they learn.[13] Active learning is a key characteristic of student-centered learning. Conversely, passive learning and direct instruction are characteristics of teacher-centered learning (or traditional education).		Associative learning is the process by which a person or animal learns an association between two stimuli. In classical conditioning a previously neutral stimulus is repeatedly paired with a reflex eliciting stimulus until eventually the neutral stimulus elicits a response on its own. In operant conditioning, a behavior that is reinforced or punished in the presence of a stimulus becomes more on less likely to occur in the presence of that stimulus.		In operant conditioning, the consequences (reinforcement or punishment) of a behavior change the frequency and/or form of that behavior. Stimulus present when the behavior/consequence occurs come to control these behavior modifications.		The typical paradigm for classical conditioning involves repeatedly pairing an unconditioned stimulus (which unfailingly evokes a reflexive response) with another previously neutral stimulus (which does not normally evoke the response). Following conditioning, the response occurs both to the unconditioned stimulus and to the other, unrelated stimulus (now referred to as the "conditioned stimulus"). The response to the conditioned stimulus is termed a conditioned response. The classic example is Ivan Pavlov and his dogs. Pavlov fed his dogs meat powder, which naturally made the dogs salivate—salivating is a reflexive response to the meat powder. Meat powder is the unconditioned stimulus (US) and the salivation is the unconditioned response (UR). Pavlov rang a bell before presenting the meat powder. The first time Pavlov rang the bell, the neutral stimulus, the dogs did not salivate, but once he put the meat powder in their mouths they began to salivate. After numerous pairings of bell and food, the dogs learned that the bell signaled that food was about to come, and began to salivate when they heard the bell. Once this occurred, the bell became the conditioned stimulus (CS) and the salivation to the bell became the conditioned response (CR). Classical conditioning has been demonstrated in many species. For example, it is seen in honeybees, in the proboscis extension reflex paradigm.[14] and recently, it was demonstrated in garden pea plants.[15]		Another influential person in the world of classical conditioning is John B. Watson. Watson's work was very influential and paved the way for B.F. Skinner's radical behaviorism. Watson's behaviorism (and philosophy of science) stood in direct contrast to Freud and other accounts based largely on introspection. Watson's view was that the introspective method was too subjective, and that we should limit the study of human development to directly observable behaviors. In 1913, Watson published the article "Psychology as the Behaviorist Views," in which he argued that laboratory studies should serve psychology best as a science. Watson's most famous, and controversial, experiment, "Little Albert", where he demonstrated how psychologists can account for the learning of emotion through classical conditioning principles.		Imprinting is a kind of learning occurring at a particular life stage that is rapid and apparently independent of the consequences of behavior. In filial imprinting, young animals, particularly birds, form an association with another individual or in some cases, an object, that they respond to as they would to a parent. In 1935, the Austrian Zoologist Konrad Lorenz discovered that certain birds follow and form a bond if the object makes sounds.		Play generally describes behavior with no particular end in itself, but that improves performance in similar future situations. This is seen in a wide variety of vertebrates besides humans, but is mostly limited to mammals and birds. Cats are known to play with a ball of string when young, which gives them experience with catching prey. Besides inanimate objects, animals may play with other members of their own species or other animals, such as orcas playing with seals they have caught. Play involves a significant cost to animals, such as increased vulnerability to predators and the risk of injury and possibly infection. It also consumes energy, so there must be significant benefits associated with play for it to have evolved. Play is generally seen in younger animals, suggesting a link with learning. However, it may also have other benefits not associated directly with learning, for example improving physical fitness.		Play, as it pertains to humans as a form of learning is central to a child's learning and development. Through play, children learn social skills such as sharing and collaboration. Children develop emotional skills such as learning to deal with the emotion of anger, through play activities. As a form of learning, play also facilitates the development of thinking and language skills in children.[16]		There are five types of play:		These five types of play are often intersecting. All types of play generate thinking and problem-solving skills in children. Children learn to think creatively when they learn through play.[17] Specific activities involved in each type of play change over time as humans progress through the lifespan. Play as a form of learning, can occur solitarily, or involve interacting with others.		Enculturation is the process by which people learn values and behaviors that are appropriate or necessary in their surrounding culture.[18] Parents, other adults, and peers shape the individual's understanding of these values.[18] If successful, enculturation results in competence in the language, values and rituals of the culture.[18] This is different from acculturation, where a person adopts the values and societal rules of a culture different from their native one.		Multiple examples of enculturation can be found cross-culturally. Collaborative practices in the Mazahua people have shown that participation in everyday interaction and later learning activities contributed to enculturation rooted in nonverbal social experience.[19] As the children participated in everyday activities, they learned the cultural significance of these interactions. The collaborative and helpful behaviors exhibited by Mexican and Mexican-heritage children is a cultural practice known as being "acomedido".[20] Chillihuani girls in Peru described themselves as weaving constantly, following behavior shown by the other adults.[21]		Episodic learning is a change in behavior that occurs as a result of an event.[22] For example, a fear of dogs that follows being bitten by a dog is episodic learning. Episodic learning is so named because events are recorded into episodic memory, which is one of the three forms of explicit learning and retrieval, along with perceptual memory and semantic memory.[23]		Multimedia learning is where a person uses both auditory and visual stimuli to learn information (Mayer 2001). This type of learning relies on dual-coding theory (Paivio 1971).		Electronic learning or e-learning is computer-enhanced learning. A specific and always more diffused e-learning is mobile learning (m-learning), which uses different mobile telecommunication equipment, such as cellular phones.		When a learner interacts with the e-learning environment, it's called augmented learning. By adapting to the needs of individuals, the context-driven instruction can be dynamically tailored to the learner's natural environment. Augmented digital content may include text, images, video, audio (music and voice). By personalizing instruction, augmented learning has been shown to improve learning performance for a lifetime.[24] See also minimally invasive education.		Moore (1989)[25] purported that three core types of interaction are necessary for quality, effective online learning:		In his theory of transactional distance, Moore (1993)[26] contented that structure and interaction or dialogue bridge the gap in understanding and communication that is created by geographical distances (known as transactional distance).		Rote learning is memorizing information so that it can be recalled by the learner exactly the way it was read or heard. The major technique used for rote learning is learning by repetition, based on the idea that a learner can recall the material exactly (but not its meaning) if the information is repeatedly processed. Rote learning is used in diverse areas, from mathematics to music to religion. Although it has been criticized by some educators, rote learning is a necessary precursor to meaningful learning.		Meaningful learning is the concept that learned knowledge (e.g., a fact) is fully understood to the extent that it relates to other knowledge. To this end, meaningful learning contrasts with rote learning in which information is acquired without regard to understanding. Meaningful learning, on the other hand, implies there is a comprehensive knowledge of the context of the facts learned.[27]		Informal learning occurs through the experience of day-to-day situations (for example, one would learn to look ahead while walking because of the danger inherent in not paying attention to where one is going). It is learning from life, during a meal at table with parents, play, exploring, etc.		Formal learning is learning that takes place within a teacher-student relationship, such as in a school system. The term formal learning has nothing to do with the formality of the learning, but rather the way it is directed and organized. In formal learning, the learning or training departments set out the goals and objectives of the learning.[28]		Nonformal learning is organized learning outside the formal learning system. For example, learning by coming together with people with similar interests and exchanging viewpoints, in clubs or in (international) youth organizations, workshops.		The educational system may use a combination of formal, informal, and nonformal learning methods. The UN and EU recognize these different forms of learning (cf. links below). In some schools, students can get points that count in the formal-learning systems if they get work done in informal-learning circuits. They may be given time to assist international youth workshops and training courses, on the condition they prepare, contribute, share and can prove this offered valuable new insight, helped to acquire new skills, a place to get experience in organizing, teaching, etc.		To learn a skill, such as solving a Rubik's Cube quickly, several factors come into play at once:		Tangential learning is the process by which people self-educate if a topic is exposed to them in a context that they already enjoy. For example, after playing a music-based video game, some people may be motivated to learn how to play a real instrument, or after watching a TV show that references Faust and Lovecraft, some people may be inspired to read the original work.[29] Self-education can be improved with systematization. According to experts in natural learning, self-oriented learning training has proven an effective tool for assisting independent learners with the natural phases of learning.[30]		Dialogic learning is a type of learning based on dialogue.		This learning is not planned by the instructor or the student, but occurs as a byproduct of another activity—an experience, observation, self-reflection, interaction, unique event, or common routine task. This learning happens in addition to or apart from the instructor's plans and the student's expectations.		Incidental learning is an occurrence that is not generally accounted for using the traditional methods of instructional objectives and outcomes assessment. This type of learning occurs in part as a product of social interaction and active involvement in both online and onsite courses. Research implies that some un-assessed aspects of onsite and online learning challenge the equivalency of education between the two modalities. Both onsite and online learning have distinct advantages with traditional on-campus students experiencing higher degrees of incidental learning in three times as many areas as online students. Additional research is called for to investigate the implications of these findings both conceptually and pedagogically.[31]		Benjamin Bloom has suggested three domains of learning:		These domains are not mutually exclusive. For example, in learning to play chess, the person must learn the rules (cognitive domain)—but must also learn how to set up the chess pieces and how to properly hold and move a chess piece (psychomotor). Furthermore, later in the game the person may even learn to love the game itself, value its applications in life, and appreciate its history (affective domain).[32]		Transfer of learning is the application of skill, knowledge or understanding to resolve a novel problem or situation that happens when certain conditions are fulfilled. Research indicates that learning transfer is infrequent; most common when "... cued, primed, and guided..."[33] and has sought to clarify what it is, and how it might be promoted through instruction.		Over the history of its discourse, various hypotheses and definitions have been advanced. First, it is speculated that different types of transfer exist, including: near transfer, the application of skill to solve a novel problem in a similar context; and far transfer, the application of skill to solve novel problem presented in a different context.[34] Furthermore, Perkins and Salomon (1992) suggest that positive transfer in cases when learning supports novel problem solving, and negative transfer occurs when prior learning inhibits performance on highly correlated tasks, such as second or third-language learning.[35] Concepts of positive and negative transfer have a long history; researchers in the early 20th century described the possibility that "...habits or mental acts developed by a particular kind of training may inhibit rather than facilitate other mental activities".[36] Finally, Schwarz, Bransford and Sears (2005) have proposed that transferring knowledge into a situation may differ from transferring knowledge out to a situation as a means to reconcile findings that transfer may both be frequent and challenging to promote.[37]		A significant and long research history has also attempted to explicate the conditions under which transfer of learning might occur. Early research by Ruger, for example, found that the "level of attention", "attitudes", "method of attack" (or method for tackling a problem), a "search for new points of view", "a careful testing of hypothesis" and "generalization" were all valuable approaches for promoting transfer.[38] To encourage transfer through teaching, Perkins and Salomon recommend aligning ("hugging") instruction with practice and assessment, and "bridging", or encouraging learners to reflect on past experiences or make connections between prior knowledge and current content.[35]		There are several internal factors that affect learning.[43][44] They are		Animals gain knowledge in two ways. First is learning—in which an animal gathers information about its environment and uses this information. For example, if an animal eats something that hurts its stomach, it learns not to eat that again. The second is innate knowledge that is genetically inherited. An example of this is when a horse is born and can immediately walk. The horse has not learned this behavior; it simply knows how to do it.[45] In some scenarios, innate knowledge is more beneficial than learned knowledge. However, in other scenarios the opposite is true—animals must learn certain behaviors when it is disadvantageous to have a specific innate behavior. In these situations, learning evolves in the species.		In a changing environment, an animal must constantly gain new information to survive. However, in a stable environment, this same individual needs to gather the information it needs once, and then rely on it for the rest of its life. Therefore, different scenarios better suit either learning or innate knowledge. Essentially, the cost of obtaining certain knowledge versus the benefit of already having it determines whether an animal evolved to learn in a given situation, or whether it innately knew the information. If the cost of gaining the knowledge outweighes the benefit of having it, then the animal does not evolve to learn in this scenario—but instead, non-learning evolves. However, if the benefit of having certain information outweighs the cost of obtaining it, then the animal is far more likely to evolve to have to learn this information.[45]		Non-learning is more likely to evolve in two scenarios. If an environment is static and change does not or rarely occurs, then learning is simply unnecessary. Because there is no need for learning in this scenario—and because learning could prove disadvantageous due to the time it took to learn the information—non-learning evolves. However, if an environment is in a constant state of change, then learning is disadvantageous. Anything learned is immediately irrelevant because of the changing environment.[45] The learned information no longer applies. Essentially, the animal would be just as successful if it took a guess as if it learned. In this situation, non-learning evolves. In fact, a study of Drosophila melanogaster showed that learning can actually lead to a decrease in productivity, possibly because egg-laying behaviors and decisions were impaired by interference from the memories gained from the new learned materials or because of the cost of energy in learning.[46]		However, in environments where change occurs within an animal's lifetime but is not constant, learning is more likely to evolve. Learning is beneficial in these scenarios because an animal can adapt to the new situation, but can still apply the knowledge that it learns for a somewhat extended period of time. Therefore, learning increases the chances of success as opposed to guessing.[45] An example of this is seen in aquatic environments with landscapes subject to change. In these environments, learning is favored because the fish are predisposed to learn the specific spatial cues where they live.[47]		Machine learning, a branch of artificial intelligence, concerns the construction and study of systems that can learn from data. For example, a machine learning system could be trained on email messages to learn to distinguish between spam and non-spam messages.		
Roughing It is a book of semi-autobiographical travel literature by Mark Twain. It was written in 1870–71 and published in 1872,[2][3] as a prequel to his first book The Innocents Abroad (1869).		The book follows the travels of young Mark Twain through the Wild West during the years 1861–1867. After a brief stint as a Confederate cavalry militiaman (not included in the account), he joined his brother Orion Clemens, who had been appointed Secretary of the Nevada Territory, on a stagecoach journey west. Twain consulted his brother's diary to refresh his memory and borrowed heavily from his active imagination for many stories in the book.		Roughing It illustrates many of Twain's early adventures, including a visit to Salt Lake City, gold and silver prospecting, real-estate speculation, a journey to the Kingdom of Hawaii, and his beginnings as a writer. This memoir provides examples of Twain's rough-hewn humor, which would become a staple of his writing in such later books as Adventures of Huckleberry Finn (1884), The Adventures of Tom Sawyer (1876), and A Connecticut Yankee in King Arthur's Court (1889).						U.S. astronauts Frank Borman and Jim Lovell read Roughing It aloud to pass the time aboard NASA's Gemini VII, a 14-day-long Earth orbital mission in December 1965.[4]		Various sections of Roughing It were borrowed by television series such as Bonanza.[5] A 1960 hour-long adaptation was broadcast on NBC starring Andrew Prine and James Daly.[5]		A four-hour 2002 mini-series adaptation was broadcast on Hallmark Channel. Directed by Charles Martin Smith, it starred James Garner as an elderly Samuel Clemens and Robin Dunne as a young Clemens.[5]		
Jason Lewis (born 13 September 1967) is an English award-winning author,[1] explorer and sustainability campaigner credited with being the first person to circumnavigate the globe by human power.[2][3] He is also the first person to cross North America on inline skates (1996), and the first to cross the Pacific Ocean by pedal power (2000). Together with Stevie Smith, Lewis completed the first crossing of the Atlantic Ocean from mainland Europe to North America by human power (1995).						Lewis set off with friend and fellow adventurer Stevie Smith from Greenwich, London on 12 July 1994, to complete the world's first human-powered circumnavigation, and the two dubbed the journey Expedition 360. By July 2007, Lewis had travelled over 60,000 km (37,000 mi). He successfully ended his 4,833-day expedition on 6 October 2007, having travelled 74,842 km (46,505 mi).[4][5]		In mid-1994, Lewis and Smith mountain-biked 1,700 miles through France, Spain and Portugal to the port of Lagos, Portugal. Departing on 13 October 1994, Lewis and Smith then pedaled 111 consecutive days and 4,500 miles across the Atlantic Ocean from Portugal to Miami, Florida in a wooden pedal-powered boat named Moksha.		Lewis then roller bladed thousands of miles across North America. He was struck by a drunk driver in Pueblo, Colorado, and spent nine months recovering from two broken legs. He finished the North American expedition leg in 1996.		In 1998 and 1999, Lewis and Smith spent 53 days pedaling Moksha across the Pacific Ocean from San Francisco, California to Hilo, Hawaii, where Smith ended his journey. In four days, Lewis and a small group of supporters hiked the 80 miles across Hawaii.		After 73 days of solo pedaling Moksha across the doldrums, Lewis completed the Pacific Ocean crossing from Hawaii to the island atoll of Tarawa. In May 2000, he was accompanied by Moksha's builder, Chris Tipper, to pedal the 1,300-mile stretch from Tarawa to the Solomon Islands. With the help of friend and expedition supporter April Abril, Lewis then pedaled Moksha 1,450 miles for 32 days across the Coral Sea to Australia.		In 2001, Lewis and a group of supporters spent 88 days cycling 3,500 miles across the Australian outback, starting near Cooktown, Queensland, and finishing in the port city of Darwin, Northern Territory.		After spending many years raising funds to continue Expedition 360, Lewis was reunited with Moksha in 2005. He and expedition supporter Lourdes Arango pedaled 450 nautical miles from Darwin, Australia to Dili, East Timor.		Throughout 2005, Lewis kayaked thousands of miles through the Indonesian archipelago from East Timor to Singapore. In 2006, he biked from Singapore to the Himalayas, and biked and hiked through the Himalayas to the port of Mumbai.		Covering 2,000 nautical miles in 46 days during early 2007, Lewis and friend Sher Dhillon pedaled Moksha from Mumbai, India, crossing the Arabian Sea to Djibouti.		Lewis then planned to travel through Ethiopia, Sudan, Egypt, and the Middle East before reaching Europe[6] – encountering a problem in Sudan. The Egyptian authorities would not let him pass through their waters, and when his visa for Sudan ran out he was left with an "impossible decision".[citation needed] He attempted to kayak across Lake Nasser to Abu Simbel but was arrested on suspicion of spying. He was released, but the Egyptian authorities forbade him from mountain-biking the 178-mile journey to Aswan. He completed this section illegally by riding partly at night.[7] During his journey through Sudan he encountered actors Ewan McGregor and Charley Boorman who were travelling south as part of their Long Way Down motorbike trip.		In July 2007, Lewis reached Syria, and then cycled across Turkey, Bulgaria, Romania, Austria, Germany, and Belgium before returning to London on 6 October. Pulling Moksha in tow, Lewis crossed the Greenwich Meridian Line where he had begun his expedition 13 years earlier.		During his expedition, Lewis twice survived malaria, septicaemia, a bout of mild schizophrenia, and a crocodile attack near Australia in 2005.[8]		As part of a wider interest in sustainability and education, Lewis has visited more than 900 schools in 37 countries, giving talks to students and involving them in a variety of programs to promote world citizenship, zero carbon emission travel, and awareness of consumption habits on the health of the planet.		Throughout his 13-year expedition, Lewis' friend, cinematographer Kenny Brown, collected many hundreds of hours of footage, and has compiled the work into a feature-length documentary titled, The Expedition.		In 2012, artist Kris Stacks and writer Anthony DiMatteo created a 27-page black and white webcomic based on the writings of Lewis. The free webcomic was titled Expedition360.		Lewis currently divides his time between England and Pueblo, Colorado. He regularly delivers inspirational speeches about global sustainability, and appears for book signings and readings to promote The Expedition trilogy. He also frequently writes for magazines and travel books. Lewis is vegan and a strong animal rights supporter, known for saying he "won't eat anything that has a face."		In his earlier years before Expedition 360, Lewis worked as a window cleaner, and as a member of a rock n' roll cover band. Before carrying out his 13-year human-powered circumnavigation, Lewis had never crossed an ocean before. Nor had he roller bladed, kayaked, or rode a bike for more than a few miles.		
Adventure fiction is fiction that usually presents danger, or gives the reader a sense of excitement.						In the Introduction to the Encyclopedia of Adventure Fiction, Critic Don D'Ammassa defines the genre as follows:		.. An adventure is an event or series of events that happens outside the course of the protagonist's ordinary life, usually accompanied by danger, often by physical action. Adventure stories almost always move quickly, and the pace of the plot is at least as important as characterization, setting and other elements of a creative work.[1]		D'Ammassa argues that adventure stories make the element of danger the focus; hence he argues that Charles Dickens' novel A Tale of Two Cities is an adventure novel because the protagonists are in constant danger of being imprisoned or killed, whereas Dickens' Great Expectations is not because "Pip's encounter with the convict is an adventure, but that scene is only a device to advance the main plot, which is not truly an adventure."[1]		Adventure has been a common theme since the earliest days of written fiction. Indeed, the standard plot of Medieval romances was a series of adventures. Following a plot framework as old as Heliodorus, and so durable as to be still alive in Hollywood movies, a hero would undergo a first set of adventures before he met his lady. A separation would follow, with a second set of adventures leading to a final reunion.		Variations kept the genre alive. From the mid-19th century onwards, when mass literacy grew, adventure became a popular subgenre of fiction. Although not exploited to its fullest, adventure has seen many changes over the years - from being constrained to stories of knights in armor to stories of high-tech espionages.		Examples of that period include Sir Walter Scott, Alexandre Dumas, père,[2] Jules Verne, Brontë Sisters, H. Rider Haggard, Victor Hugo,[3] Emilio Salgari, Louis Henri Boussenard, Thomas Mayne Reid, Sax Rohmer, Edgar Wallace, and Robert Louis Stevenson.		Adventure novels and short stories were popular subjects for American pulp magazines, which dominated American popular fiction between the Progressive Era and the 1950s.[4] Several pulp magazines such as Adventure, Argosy, Blue Book, Top-Notch, and Short Stories specialized in this genre. Notable pulp adventure writers included Edgar Rice Burroughs, Talbot Mundy, Theodore Roscoe, Johnston McCulley, Arthur O. Friel, Harold Lamb, Carl Jacobi, George F. Worts,[4] Georges Surdez, H. Bedford-Jones, and J. Allan Dunn.[5]		Adventure fiction often overlaps with other genres, notably war novels, crime novels, sea stories, Robinsonades, spy stories (as in the works of John Buchan, Eric Ambler and Ian Fleming), science fiction, fantasy, (Robert E. Howard and J.R.R. Tolkien both combined the secondary world story with the adventure novel)[6] and Westerns. Not all books within these genres are adventures. Adventure fiction takes the setting and premise of these other genres, but the fast-paced plot of an adventure focuses on the actions of the hero within the setting.[according to whom?] With a few notable exceptions (such as Baroness Orczy, Leigh Brackett and Marion Zimmer Bradley)[7] adventure fiction as a genre has been largely dominated by male writers, though female writers are now becoming common.		Adventure stories written specifically for children began in the 19th century. Early examples include Johann David Wyss' The Swiss Family Robinson (1812), Frederick Marryat's The Children of the New Forest (1847), and Harriet Martineau's The Peasant and the Prince (1856).[8] The Victorian era saw the development of the genre, with W.H.G. Kingston, R. M. Ballantyne, and G. A. Henty specializing in the production of adventure fiction for boys.[9] This inspired writers who normally catered to adult audiences to essay such works, such as Robert Louis Stevenson writing Treasure Island for a child readership.[9] In the years after the First World War, writers such as Arthur Ransome developed the adventure genre by setting the adventure in Britain rather than distant countries, while Geoffrey Trease, Rosemary Sutcliff[10] and Esther Forbes brought a new sophistication to the historical adventure novel.[9] Modern writers such as Mildred D. Taylor (Roll of Thunder, Hear My Cry) and Philip Pullman (the Sally Lockhart novels) have continued the tradition of the historical adventure.[9] The modern children's adventure novel sometimes deals with controversial issues like terrorism (Robert Cormier, After the First Death, (1979)) [9] and warfare in the Third World (Peter Dickinson, AK, (1990)).[9]		
An adventure is an exciting or unusual experience. It may also be a bold, usually risky undertaking, with an uncertain outcome.[1] Adventures may be activities with some potential for physical danger such as traveling, exploring, skydiving, mountain climbing, scuba diving, river rafting or participating in extreme sports. The term also broadly refers to any enterprise that is potentially fraught with physical, financial or psychological risk, such as a business venture, or other major life undertakings.		The word adventuress can mean a female who enjoys or partakes in adventures, but (particularly in older literature) it can also have the negative connotation of one who schemes for material advancement by the use her sexuality; a gold digger[2] As an instance of the latter the Oxford English Dictionary cites "Our Adventuress had the pickings of a few Feathers from an old Gentleman who fell in Love with her".						Adventurous experiences create psychological arousal,[3] which can be interpreted as negative (e.g. fear) or positive (e.g. flow). For some people, adventure becomes a major pursuit in and of itself. According to adventurer André Malraux, in his La Condition Humaine (1933), "If a man is not ready to risk his life, where is his dignity?".[full citation needed] Similarly, Helen Keller stated that "Life is either a daring adventure or nothing."[4]		Outdoor adventurous activities are typically undertaken for the purposes of recreation or excitement: examples are adventure racing and adventure tourism. Adventurous activities can also lead to gains in knowledge, such as those undertaken by explorers and pioneers – the British adventurer Jason Lewis, for example, uses adventures to draw global sustainability lessons from living within finite environmental constraints on expeditions to share with schoolchildren. Adventure education intentionally uses challenging experiences for learning.		Some of the oldest and most widespread stories in the world are stories of adventure such as Homer's The Odyssey.[5][6][7]		The knight errant was the form the "adventure seeker" character took in the late Middle Ages.		The adventure novel exhibits these "protagonist on adventurous journey" characteristics as do many popular feature films, such as Star Wars[8] and Raiders of the Lost Ark.[9]		Adventure books may have the theme of the hero or main character going to face the wilderness or Mother Nature. Examples include books such as Hatchet or My Side of the Mountain. These books are less about "questing", such as in mythology or other adventure novels, but more about surviving on their own, living off the land, gaining new experiences, and becoming closer to the natural world.		Many adventures are based on the idea of a quest: the hero goes off in pursuit of a reward, whether it be a skill, prize, or perhaps the safety of a person. On the way, the hero must overcome various obstacles. Mythologist Joseph Campbell discussed his notion of the monomyth in his book, The Hero with a Thousand Faces. Campbell proposed that the heroic mythological stories from culture to culture followed a similar underlying pattern, starting with the "call to adventure", followed by a hazardous journey, and eventual triumph.		Many video games are adventure games.		From ancient times, travelers and explorers have written about their adventures. Journals which became best-sellers in their day were written, such as Marco Polo's journal The Travels of Marco Polo or Mark Twain's Roughing It. Others were personal journals, only later published, such as the journals of Lewis and Clark or Captain James Cook's journals. There are also books written by those not directly a part of the adventure in question, such as The Right Stuff by Tom Wolfe, or books written by those participating in the adventure but in a format other than that of a journal, such as Conquistadors of the Useless by Lionel Terray. Documentaries often use the theme of adventure as well.		There are many sports classified as adventure sports, due to their inherent danger and excitement. Some of these include mountain climbing, skydiving, or other extreme sports.		
In narratology and comparative mythology, the monomyth, or the hero's journey, is the common template of a broad category of tales that involve a hero who goes on an adventure, and in a decisive crisis wins a victory, and then comes home changed or transformed.[1]		The study of hero myth narratives started in 1871 with anthropologist Edward Taylor's observations of common patterns in plots of hero's journeys.[2] Later on, others introduced various theories on hero myth narratives such as Otto Rank and his Freudian psychoanalytic approach to myth,[3] Lord Raglan's unification of myth and rituals,[2] and eventually hero myth pattern studies were popularized by Joseph Campbell, who was influenced by Carl Jung's view of myth. In his 1949 work The Hero with a Thousand Faces, Campbell described the basic narrative pattern as follows:		A hero ventures forth from the world of common day into a region of supernatural wonder: fabulous forces are there encountered and a decisive victory is won: the hero comes back from this mysterious adventure with the power to bestow boons on his fellow man.[4]		Campbell and other scholars, such as Erich Neumann, describe narratives of Gautama Buddha, Moses, and Christ in terms of the monomyth. While others, such as Otto Rank and Lord Raglan, describe hero narrative patterns in terms of Freudian psychoanalysis and ritualistic senses. Critics argue that the concept is too broad or general to be of much usefulness in comparative mythology. Others say that the hero's journey is only a part of the monomyth; the other part is a sort of different form, or color, of the hero's journey.						Campbell borrowed the word monomyth from Joyce's Finnegans Wake (1939). Campbell was a notable scholar of James Joyce's work and in A Skeleton Key to Finnegans Wake (1944) co-authored the seminal analysis of Joyce's final novel.[5][6] Campbell's singular the monomyth implies that the "hero's journey" is the ultimate narrative archetype, but the term monomyth has occasionally been used more generally, as a term for a mythological archetype or a supposed mytheme that re-occurs throughout the world's cultures.[7][8] Omry Ronen referred to Vyacheslav Ivanov's treatment of Dionysus as an "avatar of Christ" (1904) as "Ivanov's monomyth".[9]		The phrase "the hero's journey", used in reference to Campbell's monomyth, first entered into popular discourse through two documentaries. The first, released in 1987, The Hero's Journey: The World of Joseph Campbell, was accompanied by a 1990 companion book, The Hero's Journey: Joseph Campbell on His Life and Work (with Phil Cousineau and Stuart Brown, eds.). The second was Bill Moyers's series of seminal interviews with Campbell, released in 1988 as the documentary (and companion book) The Power of Myth. Cousineau in the introduction to the revised edition of The Hero's Journey wrote "the monomyth is in effect a metamyth, a philosophical reading of the unity of mankind's spiritual history, the Story behind the story".[10]		Campbell describes 17 stages of the monomyth. Not all monomyths necessarily contain all 17 stages explicitly; some myths may focus on only one of the stages, while others may deal with the stages in a somewhat different order.[page needed] In the terminology of Claude Lévi-Strauss, the stages are the individual mythemes which are "bundled" or assembled into the structure of the monomyth.[11]		The 17 stages may be organized in a number of ways, including division into three "acts" or sections:		In the departure part of the narrative, the hero or protagonist lives in the ordinary world and receives a call to go on an adventure. The hero is reluctant to follow the call, but is helped by a mentor figure.		The initiation section begins with the hero then traversing the threshold to the unknown or "special world", where he faces tasks or trials, either alone or with the assistance of helpers.		The hero eventually reaches "the innermost cave" or the central crisis of his adventure, where he must undergo "the ordeal" where he overcomes the main obstacle or enemy, undergoing "apotheosis" and gaining his reward (a treasure or "elixir").		The hero must then return to the ordinary world with his reward. He may be pursued by the guardians of the special world, or he may be reluctant to return, and may be rescued or forced to return by intervention from the outside.		In the return section, the hero again traverses the threshold between the worlds, returning to the ordinary world with the treasure or elixir he gained, which he may now use for the benefit of his fellow man. The hero himself is transformed by the adventure and gains wisdom or spiritual power over both worlds.		Campbell's approach has been very widely received in narratology, mythography and psychotherapy, especially since the 1980s, and a number of variant summaries of the basic structure have been published. The general structure of Campbell's exposition has been noted before and described in similar terms in comparative mythology of the 19th and early 20th century, notably by Russian folklorist Vladimir Propp who divided the structure of Russian folk tales into 31 "functions".[12]		The following is a more detailed account of Campbell's original 1949 exposition of the monomyth in 17 stages.		The hero begins in a situation of normality from which some information is received that acts as a call to head off into the unknown.		Campbell: "...(the call of adventure is to) a forest, a kingdom underground, beneath the waves, or above the sky, a secret island, lofty mountaintop, or profound dream state; but it is always a place of strangely fluid and polymorphous beings, unimaginable torments, super human deeds, and impossible delight. The hero can go forth of his own volition to accomplish the adventure, as did Theseus when he arrived in his father's city, Athens, and heard the horrible history of the Minotaur; or he may be carried or sent abroad by some benign or malignant agent as was Odysseus, driven about the Mediterranean by the winds of the angered god, Poseidon. The adventure may begin as a mere blunder... or still again, one may be only casually strolling when some passing phenomenon catches the wandering eye and lures one away from the frequented paths of man. Examples might be multiplied, ad infinitum, from every corner of the world."		Often when the call is given, the future hero first refuses to heed it. This may be from a sense of duty or obligation, fear, insecurity, a sense of inadequacy, or any of a range of reasons that work to hold the person in his or her current circumstances.		Campbell: "Refusal of the summons converts the adventure into its negative. Walled in boredom, hard work, or 'culture,' the subject loses the power of significant affirmative action and becomes a victim to be saved. His flowering world becomes a wasteland of dry stones and his life feels meaningless—even though, like King Minos, he may through titanic effort succeed in building an empire or renown. Whatever house he builds, it will be a house of death: a labyrinth of cyclopean walls to hide from him his minotaur. All he can do is create new problems for himself and await the gradual approach of his disintegration." [2]		Once the hero has committed to the quest, consciously or unconsciously, his guide and magical helper appears or becomes known. More often than not, this supernatural mentor will present the hero with one or more talismans or artifacts that will aid him later in his quest.		Campbell: "For those who have not refused the call, the first encounter of the hero journey is with a protective figure (often a little old crone or old man) who provides the adventurer with amulets against the dragon forces he is about to pass. What such a figure represents is the benign, protecting power of destiny. The fantasy is a reassurance—promise that the peace of Paradise, which was known first within the mother womb, is not to be lost; that it supports the present and stands in the future as well as in the past (is omega as well as alpha); that though omnipotence may seem to be endangered by the threshold passages and life awakenings, protective power is always and ever present within or just behind the unfamiliar features of the world. One has only to know and trust, and the ageless guardians will appear. Having responded to his own call, and continuing to follow courageously as the consequences unfold, the hero finds all the forces of the unconscious at his side. Mother Nature herself supports the mighty task. And in so far as the hero's act coincides with that for which his society is ready, he seems to ride on the great rhythm of the historical process." [3]		This is the point where the person actually crosses into the field of adventure, leaving the known limits of his or her world and venturing into an unknown and dangerous realm where the rules and limits are unknown.		Campbell: "With the personifications of his destiny to guide and aid him, the hero goes forward in his adventure until he comes to the 'threshold guardian' at the entrance to the zone of magnified power. Such custodians bound the world in four directions — also up and down — standing for the limits of the hero's present sphere, or life horizon. Beyond them is darkness, the unknown and danger; just as beyond the parental watch is danger to the infant and beyond the protection of his society danger to the members of the tribe. The usual person is more than content, he is even proud, to remain within the indicated bounds, and popular belief gives him every reason to fear so much as the first step into the unexplored. The adventure is always and everywhere a passage beyond the veil of the known into the unknown; the powers that watch at the boundary are dangerous; to deal with them is risky; yet for anyone with competence and courage the danger fades." [4]		The belly of the whale represents the final separation from the hero's known world and self. By entering this stage, the person shows willingness to undergo a metamorphosis. When First entering the stage the hero may encounter a minor danger or set back.		Campbell: "The idea that the passage of the magical threshold is a transit into a sphere of rebirth is symbolized in the worldwide womb image of the belly of the whale. The hero, instead of conquering or conciliating the power of the threshold, is swallowed into the unknown and would appear to have died. This popular motif gives emphasis to the lesson that the passage of the threshold is a form of self-annihilation. Instead of passing outward, beyond the confines of the visible world, the hero goes inward, to be born again. The disappearance corresponds to the passing of a worshipper into a temple—where he is to be quickened by the recollection of who and what he is, namely dust and ashes unless immortal. The temple interior, the belly of the whale, and the heavenly land beyond, above, and below the confines of the world, are one and the same. That is why the approaches and entrances to temples are flanked and defended by colossal gargoyles: dragons, lions, devil-slayers with drawn swords, resentful dwarfs, winged bulls. The devotee at the moment of entry into a temple undergoes a metamorphosis. Once inside he may be said to have died to time and returned to the World Womb, the World Navel, the Earthly Paradise. Allegorically, then, the passage into a temple and the hero-dive through the jaws of the whale are identical adventures, both denoting in picture language, the life-centering, life-renewing act." [5]		The road of trials is a series of tests that the person must undergo to begin the transformation. Often the person fails one or more of these tests, which often occur in threes.		Campbell: "Once having traversed the threshold, the hero moves in a dream landscape of curiously fluid, ambiguous forms, where he must survive a succession of trials. This is a favorite phase of the myth-adventure. It has produced a world literature of miraculous tests and ordeals. The hero is covertly aided by the advice, amulets, and secret agents of the supernatural helper whom he met before his entrance into this region. Or it may be that he here discovers for the first time that there is a benign power everywhere supporting him in his superhuman passage. The original departure into the land of trials represented only the beginning of the long and really perilous path of initiatory conquests and moments of illumination. Dragons have now to be slain and surprising barriers passed — again, again, and again. Meanwhile there will be a multitude of preliminary victories, unretainable ecstasies and momentary glimpses of the wonderful land." [6]		Campbell: "The ultimate adventure, when all the barriers and ogres have been overcome, is commonly represented as a mystical marriage of the triumphant hero-soul with the Queen Goddess of the World. This is the crisis at the nadir, the zenith, or at the uttermost edge of the earth, at the central point of the cosmos, in the tabernacle of the temple, or within the darkness of the deepest chamber of the heart. The meeting with the goddess (who is incarnate in every woman) is the final test of the talent of the hero to win the boon of love (charity: amor fati), which is life itself enjoyed as the encasement of eternity. And when the adventurer, in this context, is not a youth but a maid, she is the one who, by her qualities, her beauty, or her yearning, is fit to become the consort of an immortal. Then the heavenly husband descends to her and conducts her to his bed—whether she will or not. And if she has shunned him, the scales fall from her eyes; if she has sought him, her desire finds its peace." [7]		In this step, the hero faces those temptations, often of a physical or pleasurable nature, that may lead him or her to abandon or stray from his or her quest, which does not necessarily have to be represented by a woman. Woman is a metaphor for the physical or material temptations of life, since the hero-knight was often tempted by lust from his spiritual journey.		Campbell: "The crux of the curious difficulty lies in the fact that our conscious views of what life ought to be seldom correspond to what life really is. Generally we refuse to admit within ourselves, or within our friends, the fullness of that pushing, self-protective, malodorous, carnivorous, lecherous fever which is the very nature of the organic cell. Rather, we tend to perfume, whitewash, and reinterpret; meanwhile imagining that all the flies in the ointment, all the hairs in the soup, are the faults of some unpleasant someone else. But when it suddenly dawns on us, or is forced to our attention that everything we think or do is necessarily tainted with the odor of the flesh, then, not uncommonly, there is experienced a moment of revulsion: life, the acts of life, the organs of life, woman in particular as the great symbol of life, become intolerable to the pure, the pure, pure soul. The seeker of the life beyond life must press beyond (the woman), surpass the temptations of her call, and soar to the immaculate ether beyond." [8]		In this step the person must confront and be initiated by whatever holds the ultimate power in his or her life. In many myths and stories this is the father, or a father figure who has life and death power. This is the center point of the journey. All the previous steps have been moving into this place, all that follow will move out from it. Although this step is most frequently symbolized by an encounter with a male entity, it does not have to be a male; just someone or thing with incredible power.		Campbell: "Atonement consists in no more than the abandonment of that self-generated double monster—the dragon thought to be God (superego) and the dragon thought to be Sin (repressed id). But this requires an abandonment of the attachment to ego itself, and that is what is difficult. One must have a faith that the father is merciful, and then a reliance on that mercy. Therewith, the center of belief is transferred outside of the bedeviling god's tight scaly ring, and the dreadful ogres dissolve. It is in this ordeal that the hero may derive hope and assurance from the helpful female figure, by whose magic (pollen charms or power of intercession) he is protected through all the frightening experiences of the father's ego-shattering initiation. For if it is impossible to trust the terrifying father-face, then one's faith must be centered elsewhere (Spider Woman, Blessed Mother); and with that reliance for support, one endures the crisis—only to find, in the end, that the father and mother reflect each other, and are in essence the same. The problem of the hero going to meet the father is to open his soul beyond terror to such a degree that he will be ripe to understand how the sickening and insane tragedies of this vast and ruthless cosmos are completely validated in the majesty of Being. The hero transcends life with its peculiar blind spot and for a moment rises to a glimpse of the source. He beholds the face of the father, understands—and the two are atoned." [9]		This is the point of realization in which a greater understanding is achieved. Armed with this new knowledge and perception, the hero is resolved and ready for the more difficult part of the adventure		Campbell: "Those who know, not only that the Everlasting lies in them, but that what they, and all things, really are is the Everlasting, dwell in the groves of the wish fulfilling trees, drink the brew of immortality, and listen everywhere to the unheard music of eternal concord." [10]		The ultimate boon is the achievement of the goal of the quest. It is what the person went on the journey to get. All the previous steps serve to prepare and purify the person for this step, since in many myths the boon is something transcendent like the elixir of life itself, or a plant that supplies immortality, or the holy grail.		Campbell: "The gods and goddesses then are to be understood as embodiments and custodians of the elixir of Imperishable Being but not themselves the Ultimate in its primary state. What the hero seeks through his intercourse with them is therefore not finally themselves, but their grace, i.e., the power of their sustaining substance. This miraculous energy-substance and this alone is the Imperishable; the names and forms of the deities who everywhere embody, dispense, and represent it come and go. This is the miraculous energy of the thunderbolts of Zeus, Yahweh, and the Supreme Buddha, the fertility of the rain of Viracocha, the virtue announced by the bell rung in the Mass at the consecration, and the light of the ultimate illumination of the saint and sage. Its guardians dare release it only to the duly proven." [11]		Having found bliss and enlightenment in the other world, the hero may not want to return to the ordinary world to bestow the boon onto his fellow man.		Campbell: "When the hero-quest has been accomplished, through penetration to the source, or through the grace of some male or female, human or animal, personification, the adventurer still must return with his life-transmuting trophy. The full round, the norm of the monomyth, requires that the hero shall now begin the labor of bringing the runes of wisdom, the Golden Fleece, or his sleeping princess, back into the kingdom of humanity, where the boon may redound to the renewing of the community, the nation, the planet or the ten thousand worlds. But the responsibility has been frequently refused. Even Gautama Buddha, after his triumph, doubted whether the message of realization could be communicated, and saints are reported to have died while in the supernal ecstasy. Numerous indeed are the heroes fabled to have taken up residence forever in the blessed isle of the unaging Goddess of Immortal Being." [12]		Sometimes the hero must escape with the boon, if it is something that the gods have been jealously guarding. It can be just as adventurous and dangerous returning from the journey as it was to go on it.		Campbell: "If the hero in his triumph wins the blessing of the goddess or the god and is then explicitly commissioned to return to the world with some elixir for the restoration of society, the final stage of his adventure is supported by all the powers of his supernatural patron. On the other hand, if the trophy has been attained against the opposition of its guardian, or if the hero's wish to return to the world has been resented by the gods or demons, then the last stage of the mythological round becomes a lively, often comical, pursuit. This flight may be complicated by marvels of magical obstruction and evasion." [13]		Just as the hero may need guides and assistants to set out on the quest, often he or she must have powerful guides and rescuers to bring them back to everyday life, especially if the person has been wounded or weakened by the experience.		Campbell: "The hero may have to be brought back from his supernatural adventure by assistance from without. That is to say, the world may have to come and get him. For the bliss of the deep abode is not lightly abandoned in favor of the self-scattering of the wakened state. 'Who having cast off the world,' we read, 'would desire to return again? He would be only there.' And yet, in so far as one is alive, life will call. Society is jealous of those who remain away from it, and will come knocking at the door. If the hero. . . is unwilling, the disturber suffers an ugly shock; but on the other hand, if the summoned one is only delayed—sealed in by the beatitude of the state of perfect being (which resembles death)—an apparent rescue is effected, and the adventurer returns."[14]		The trick in returning is to retain the wisdom gained on the quest, to integrate that wisdom into a human life, and then maybe figure out how to share the wisdom with the rest of the world.		Campbell: "The returning hero, to complete his adventure, must survive the impact of the world. Many failures attest to the difficulties of this life-affirmative threshold. The first problem of the returning hero is to accept as real, after an experience of the soul-satisfying vision of fulfillment, the passing joys and sorrows, banalities and noisy obscenities of life. Why re-enter such a world? Why attempt to make plausible, or even interesting, to men and women consumed with passion, the experience of transcendental bliss? As dreams that were momentous by night may seem simply silly in the light of day, so the poet and the prophet can discover themselves playing the idiot before a jury of sober eyes. The easy thing is to commit the whole community to the devil and retire again into the heavenly rock dwelling, close the door, and make it fast. But if some spiritual obstetrician has drawn the shimenawa across the retreat, then the work of representing eternity in time, and perceiving in time eternity, cannot be avoided" The hero returns to the world of common day and must accept it as real.[15]		This step is usually represented by a transcendental hero like Jesus or Gautama Buddha. For a human hero, it may mean achieving a balance between the material and spiritual. The person has become comfortable and competent in both the inner and outer worlds.		Campbell: "Freedom to pass back and forth across the world division, from the perspective of the apparitions of time to that of the causal deep and back—not contaminating the principles of the one with those of the other, yet permitting the mind to know the one by virtue of the other—is the talent of the master. The Cosmic Dancer, declares Nietzsche, does not rest heavily in a single spot, but gaily, lightly, turns and leaps from one position to another. It is possible to speak from only one point at a time, but that does not invalidate the insights of the rest. The individual, through prolonged psychological disciplines, gives up completely all attachment to his personal limitations, idiosyncrasies, hopes and fears, no longer resists the self-annihilation that is prerequisite to rebirth in the realization of truth, and so becomes ripe, at last, for the great at-one-ment. His personal ambitions being totally dissolved, he no longer tries to live but willingly relaxes to whatever may come to pass in him; he becomes, that is to say, an anonymity."[16]		Mastery leads to freedom from the fear of death, which in turn is the freedom to live.[citation needed] This is sometimes referred to as living in the moment, neither anticipating the future nor regretting the past.		Campbell: "The hero is the champion of things becoming, not of things become, because he is. "Before Abraham was, I AM." He does not mistake apparent changelessness in time for the permanence of Being, nor is he fearful of the next moment (or of the 'other thing'), as destroying the permanent with its change. 'Nothing retains its own form; but Nature, the greater renewer, ever makes up forms from forms. Be sure there's nothing perishes in the whole universe; it does but vary and renew its form.' Thus the next moment is permitted to come to pass." [17]		The monomyth concept has been very popular in American literary studies and writing guides since at least the 1970s. Christopher Vogler, a Hollywood film producer and writer, created a 7-page company memo, A Practical Guide to The Hero With a Thousand Faces,[17] based on Campbell's work. Vogler's memo was later developed into the late 1990s book, The Writer's Journey: Mythic Structure For Writers. George Lucas' Star Wars (1977) was notably classified as monomyth almost as soon as it came out.[18] Numerous other works of popular fiction have been forwarded as examples of the monomyth template, including Spenser's The Fairie Queene,[19] Melville's Moby Dick,[20] Charlotte Brontë's Jane Eyre,[21] works by Charles Dickens, Faulkner, Maugham, J. D. Salinger,[22] Hemingway,[23] Mark Twain,[24] W. B. Yeats,[25] C. S. Lewis,[26] and J. R. R. Tolkien,[27] Seamus Heaney[28] and Stephen King,[29] among numerous others.		In addition to the extensive discussion between Campbell and Bill Moyers broadcast in 1988 on PBS as The Power of Myth (Filmed at "Skywalker Ranch"), on Campbell's influence on the Star Wars films, Lucas himself gave an extensive interview for the biography Joseph Campbell: A Fire in the Mind (Larsen and Larsen, 2002, pages 541-543) on this topic. In this interview, Lucas states that in the early 1970s after completing his early film, American Graffiti, "it came to me that there really was no modern use of mythology...so that's when I started doing more strenuous research on fairy tales, folklore and mythology, and I started reading Joe's books. Before that I hadn't read any of Joe's books... It was very eerie because in reading The Hero with A Thousand Faces I began to realize that my first draft of Star Wars was following classical motifs" (p. 541). Twelve years after the making of The Power of Myth, Moyers and Lucas met again for the 1999 interview, the Mythology of Star Wars with George Lucas & Bill Moyers, to further discuss the impact of Campbell's work on Lucas's films.[30] In addition, the National Air and Space Museum of the Smithsonian Institution sponsored an exhibit during the late 1990s called Star Wars: The Magic of Myth which discussed the ways in which Campbell's work shaped the Star Wars films.[31] A companion guide of the same name was published in 1997.		The American comedy-drama Northern Exposure featured as its protagonist a physician (Joel Fleischman, played by Rob Morrow) who wished to escape his contractual obligation to serve as town doctor for the town of Cicely, Alaska. In episode 15 of season 6 "The Quest", Dr. Fleishman finally achieves this goal by undertaking a hero's journey based on Campbell's archetype.[32]		Scholars have questioned the validity or usefulness of the monomyth category.		According to Northup (2006), mainstream scholarship of comparative mythology since Campbell has moved away from "highly general and universal" categories in general.[33] This attitude is illustrated by e.g. Consentino (1998), who remarks "It is just as important to stress differences as similarities, to avoid creating a (Joseph) Campbell soup of myths that loses all local flavor."[34] Similarly, Ellwood (1999) stated "A tendency to think in generic terms of people, races ... is undoubtedly the profoundest flaw in mythological thinking."[35]		Others have found the categories Campbell works with so vague as to be meaningless, and lacking the support required of scholarly argument: Crespi (1990), writing in response to Campbell's filmed presentation of his model[36] characterized it as "... unsatisfying from a social science perspective. Campbell's ethnocentrism will raise objections, and his analytic level is so abstract and devoid of ethnographic context that myth loses the very meanings supposed to be embedded in the 'hero.'" In Sacred Narrative: Readings in the Theory of Myth (1984), editor Alan Dundes dismisses Campbell's work, characterizing him as a popularizer: "like most universalists, he is content to merely assert universality rather than bother to document it. […] If Campbell's generalizations about myth are not substantiated, why should students consider his work?"[37]		In a similar vein, American philosopher John Shelton Lawrence and American religious scholar Robert Jewett have discussed an "American Monomyth" in many of their books, The American Monomyth, The Myth of the American Superhero (2002), and Captain America and the Crusade Against Evil: The Dilemma of Zealous Nationalism (2003). They present this as an American reaction to the Campbellian monomyth. The "American Monomyth" storyline is: A community in a harmonious paradise is threatened by evil; normal institutions fail to contend with this threat; a selfless superhero emerges to renounce temptations and carry out the redemptive task; aided by fate, his decisive victory restores the community to its paradisiacal condition; the superhero then recedes into obscurity.[16]		The monomyth has also been criticized for focusing on the masculine journey. From Girl to Goddess: The Heroine's Journey through Myth and Legend (2010), by Valerie Estelle Frankel, sets out what Frankel considers the steps of the female hero's journey, which is different from Campbell's monomyth.[38]		According to changingminds.org, "[Campbell's] much admired and much-copied pattern has also been criticized as leading to 'safe' moviemaking, in which writers use his structure as a template, thus leading to 'boring' repeats, albeit in different clothes."[39]		While Frank Herbert's Dune (1965) on the surface appears to follow the monomyth, this was in fact to subvert it and take a critical view, as the author said in 1979, "The bottom line of the Dune trilogy is: beware of heroes. Much better [to] rely on your own judgment, and your own mistakes."[40] He wrote in 1985, "Dune was aimed at this whole idea of the infallible leader because my view of history says that mistakes made by a leader (or made in a leader's name) are amplified by the numbers who follow without question."[41]		Science fiction author David Brin in a 1999 Salon article criticized the monomyth template as supportive of "despotism and tyranny", indicating that he thinks modern popular fiction should strive to depart from it in order to support more progressivist values.[42]		In narratology and comparative mythology, others have proposed narrative patterns such as psychoanalyst Otto Rank in 1909 and antropologist Lord Raglan in 1936. Both have lists of different cross-cultural traits often found in the accounts of heroes, including mythical heroes.[43][44] According to Robert Segal, "The theories of Rank, Campbell, and Raglan typify the array of analyses of hero myths."[2]		Poet Robert Bly, Michael J. Meade, and others involved in the men's movement have applied and expanded the concepts of the hero's journey and the monomyth as a metaphor for personal spiritual and psychological growth, particularly in the mythopoetic men's movement.[45][46]		Characteristic of the mythopoetic men's movement is a tendency to retell fairy tales and engage in their exegesis as a tool for personal insight. Using frequent references to archetypes as drawn from Jungian analytical psychology, the movement focuses on issues of gender role, gender identity and wellness for modern men.[46] Advocates would often engage in storytelling with music, these acts being seen as a modern extension to a form of "new age shamanism" popularized by Michael Harner at approximately the same time.		Among its most famous advocates were the poet Robert Bly, whose book Iron John: A Book About Men was a best-seller, being an exegesis of the fairy tale "Iron John" by the Brothers Grimm.[45]		The mythopoetic men's movement spawned a variety of groups and workshops, led by authors such as Bly and Robert L. Moore.[46] Some serious academic work came out of this movement, including the creation of various magazines and non-profit organizations.[45]		
The Disneyland Railroad is a 3-foot (914 mm) narrow-gauge heritage railroad and attraction in the Disneyland theme park of the Disneyland Resort in Anaheim, California, in the United States. Its route is 1.2 miles (1.9 km) long with four train stations, encircling almost everything in the park. The rail line, which was built by WED Enterprises, is operated with two steam locomotives built by WED and three historic steam locomotives originally built by Baldwin Locomotive Works. The attraction originated as a concept created by Walt Disney, who drew inspiration from the ridable miniature Carolwood Pacific Railroad built in his backyard. Since 1955 when the Disneyland Railroad first opened to the public at the park's grand opening, it has been consistently billed as one of the top attractions, and for many years visitors had to buy a top-tier ticket to ride the train. It is one of the world's most popular steam-powered railroads, with an estimated 6.6 million passengers served each year. (Full article...)		August 7: Raksha Bandhan (Hinduism, 2017); Assyrian Martyrs Day (1933)		Joseph Marie Jacquard (d. 1834) · Richard Sykes (b. 1942) · Frances Oldham Kelsey (d. 2015)		The filmography of Laurel and Hardy, a motion picture comedy team, consists of 106 films released between 1921 and 1951. Together they appeared in 34 silent shorts, 45 sound shorts, and 27 full-length sound feature films. Stan Laurel and Oliver Hardy were established as film comedians prior to their teaming, with Laurel appearing in over 50 silent films and Hardy in over 250. Although they first worked together in the film The Lucky Dog (1921), this was a chance pairing and it was not until 1926 when both separately signed contracts with the Hal Roach film studio that they appeared in movie shorts together. Laurel and Hardy officially became a team the following year, in their eleventh silent short film The Second Hundred Years (1927). The pair remained with the Roach studio until 1940. Between 1941 and 1945 they appeared in eight features and one short for 20th Century Fox and Metro-Goldwyn-Mayer. After finishing their movie commitments Laurel and Hardy concentrated on stage shows, embarking on a music hall tour of Great Britain. (Full list...)		In a Roman Osteria is an oil painting on canvas completed by the Danish painter Carl Bloch in 1866. Commissioned by the merchant Moritz G. Melchior, it depicts the interior of an osteria, with Melchior and some friends in the background. It has been in the collection of the National Gallery of Denmark since 1935.		Painting: Carl Bloch		Wikipedia is hosted by the Wikimedia Foundation, a non-profit organization that also hosts a range of other projects:		This Wikipedia is written in English. Started in 2001 (2001), it currently contains 5,455,325 articles. Many other Wikipedias are available; some of the largest are listed below.		
Nansen's Fram expedition was an 1893–1896 attempt by the Norwegian explorer Fridtjof Nansen to reach the geographical North Pole by harnessing the natural east–west current of the Arctic Ocean. In the face of much discouragement from other polar explorers, Nansen took his ship Fram to the New Siberian Islands in the eastern Arctic Ocean, froze her into the pack ice, and waited for the drift to carry her towards the pole. Impatient with the slow speed and erratic character of the drift, after 18 months Nansen and a chosen companion, Hjalmar Johansen, left the ship with a team of dogs and sledges and made for the pole. They did not reach it, but they achieved a record Farthest North latitude of 86°13.6′N before a long retreat over ice and water to reach safety in Franz Josef Land. Meanwhile, Fram continued to drift westward, finally emerging in the North Atlantic Ocean.		The idea for the expedition had arisen after items from the American vessel Jeannette, which had sunk off the north coast of Siberia in 1881, were discovered three years later off the south-west coast of Greenland. The wreckage had obviously been carried across the polar ocean, perhaps across the pole itself. Based on this and other debris recovered from the Greenland coast, the meteorologist Henrik Mohn developed a theory of transpolar drift, which led Nansen to believe that a specially designed ship could be frozen in the pack ice and follow the same track as the Jeannette wreckage, thus reaching the vicinity of the pole.		Nansen supervised the construction of a vessel with a rounded hull and other features designed to withstand prolonged pressure from ice. The ship was rarely threatened during her long imprisonment, and emerged unscathed after three years. The scientific observations carried out during this period contributed significantly to the new discipline of oceanography, which subsequently became the main focus of Nansen's scientific work. Fram's drift and Nansen's sledge journey proved conclusively that there were no significant land masses between the Eurasian continents and the North Pole, and confirmed the general character of the north polar region as a deep, ice-covered sea. Although Nansen retired from exploration after this expedition, the methods of travel and survival he developed with Johansen influenced all the polar expeditions, north and south, which followed in the subsequent three decades.						In September 1879 the Jeannette, an ex-Royal Navy gunboat converted by the US Navy for Arctic exploration, and commanded by George W. De Long, entered the pack ice north of the Bering Strait. She remained ice-bound for nearly two years, drifting to the area of the New Siberian Islands, before being crushed and sunk on 13 June 1881.[2] Her crew escaped in boats and made for the Siberian coast; most, including De Long, subsequently perished either during the boat journey or in the wastelands of the Lena River delta.[3] Three years later, relics from the Jeannette appeared on the opposite side of the world, in the vicinity of Julianehaab on the southwest coast of Greenland. These items, frozen into the drifting ice, included clothing bearing crew members' names and documents signed by De Long; they were indisputably genuine.[4]		In a lecture given in 1884 to the Norwegian Academy of Science and Letters Dr. Henrik Mohn, one of the founders of modern meteorology, argued that the finding of the Jeannette relics indicated the existence of an ocean current flowing from east to west across the entire Arctic Ocean. The Danish governor of Julianehaab, writing of the find, surmised that an expedition frozen into the Siberian sea might, if its ship were to prove strong enough, cross the polar ocean and land in South Greenland.[4] These theories were read with interest by the 23-year-old Fridtjof Nansen, then working as a curator at the Bergen Museum while completing his doctoral studies.[5] Nansen was already captivated by the frozen north; two years earlier he had experienced a four-month voyage on the sealer Viking, which had included three weeks trapped in drifting ice.[6] An expert skier, Nansen was making plans to lead the first crossing of the Greenland icecap,[7] an objective delayed by the demands of his academic studies, but triumphantly achieved in 1888–89. Through these years Nansen remembered the east–west Arctic drift theory and its inherent possibilities for further polar exploration, and shortly after his return from Greenland he was ready to announce his plans.[8]		In February 1890 Nansen addressed a meeting of the Norwegian Geographical Society in Oslo (then called Christiania). After drawing attention to the failures of the many expeditions which had approached the North Pole from the west, he considered the implications of the discovery of the Jeannette items, along with further finds of driftwood and other debris from Siberia or Alaska that had been identified along the Greenland coast. "Putting all this together," Nansen said, "we are driven to the conclusion that a current flows ... from the Siberian Arctic Sea to the east coast of Greenland," probably passing across the Pole. It seemed that the obvious thing to do was "to make our way into the current on that side of the Pole where it flows northward, and by its help to penetrate into those regions which all who have hitherto worked against [the current] have sought in vain to reach."[9]		Nansen's plan required a small, strong and manoeuvrable ship, powered by sail and an engine, capable of carrying fuel and provisions for twelve men for five years.[10] The vessel would follow Jeannette's route to the New Siberian Islands, and in the approximate position of Jeannette's sinking, when ice conditions were right "we shall plough our way in amongst the ice as far as we can."[10] The ship would then drift with the ice towards the pole and eventually reach the sea between Greenland and Spitsbergen. Should the ship founder, a possibility which Nansen thought very unlikely, the party would camp on a floe and allow itself to be carried towards safety. Nansen observed: "If the Jeannette Expedition had had sufficient provisions, and had remained on the ice-floe on which the relics were found, the result would doubtless have been very different from what it was."[11]		When Nansen's plans became public knowledge The New York Times was enthusiastic, deeming it "highly probable that there is a comparatively short and direct route across the Arctic Ocean by way of the North Pole, and that nature herself has supplied a means of communication across it."[12] However, most experienced polar hands were dismissive. The American explorer Adolphus Greely called it "an illogical scheme of self-destruction";[13] his assistant, Lieutenant David Brainerd, called it "one of the most ill-advised schemes ever embarked on", and predicted that it would end in disaster.[14] Sir Allen Young, a veteran of the searches for Sir John Franklin's lost expedition, did not believe that a ship could be built to withstand the crushing pressure of the ice: "If there is no swell the ice must go through her, whatever material she is made of."[15] Sir Joseph Hooker, who had sailed south with James Clark Ross in 1839–43, was of the same opinion, and thought the risks were not worth taking.[16][17] However, the equally experienced Sir Leopold McClintock called Nansen's project "the most adventurous programme ever brought under the notice of the Royal Geographical Society". The Swedish philanthropist Oscar Dickson, who had financed Baron Nordenskiöld's conquest of the North-East Passage in 1878–79, was sufficiently impressed to offer to meet Nansen's costs. With Norwegian nationalism on the rise, however, this gesture from their union partner Sweden provoked hostility in the Norwegian press; Nansen decided to rely solely on Norwegian support, and declined Dickson's proposal.[18]		Nansen's original estimate for the total cost of the expedition was 300,000 kr. After giving a passionate speech before the Parliament of Norway (the Storting),[a] Nansen was awarded a grant of NOK 200,000; the balance was raised from private contributions which included 20,000 kroner from King Oscar II of Norway and Sweden. The Royal Geographical Society in London gave £300 (about NOK 6,000).[20] Unfortunately, Nansen had underestimated the financing required—the ship alone would cost more than the total at his disposal. A renewed plea to the Storting produced a further NOK 80,000, and a national appeal raised the grand total to NOK 445,000. According to Nansen's own account, he made up the remaining deficiency from his own resources.[21] His biographer Roland Huntford records that the final deficit of NOK 12,000 was cleared by two wealthy supporters, Axel Heiberg and an English expatriate, Charles Dick.[22]		To design and build his ship Nansen chose Colin Archer, Norway's leading shipbuilder and naval architect. Archer was well known for a particular hull design that combined seaworthiness with a shallow draught, and had pioneered the design of "double-ended" craft in which the conventional stern was replaced by a point, increasing manoeuvrability.[23] Nansen records that Archer made "plan after plan of the projected ship; one model after another was prepared and abandoned".[24] Finally, agreement was reached on a design, and on 9 June 1891 the two men signed the contract.[23]		Nansen wanted the ship in one year; he was eager to get away before anyone else could adopt his ideas and forestall him.[25] The ship's most significant external feature was the roundness of the hull, designed so that there was nothing upon which the ice could get a grip. Bow, stern and keel were rounded off, and the sides smoothed so that, in Nansen's words, the vessel would "slip like an eel out of the embraces of the ice".[26] To give exceptional strength the hull was sheathed in South American greenheart, the hardest timber available. The three layers of wood forming the hull provided a combined thickness of between 24 and 28 inches (60–70 cm), increasing to around 48 inches (1.25 metres) at the bow, which was further protected by a protruding iron stem. Added strength was provided by crossbeams and braces throughout the length of the hull.[26]		The ship was rigged as a three-masted schooner, with a total sail area of 6,000 square feet (560 m2). Its auxiliary engine of 220 horse-power was capable of speeds up to 7 knots (13 km/h; 8.1 mph).[27] However, speed and sailing qualities were secondary to the requirement of providing a safe and warm stronghold for Nansen and his crew during a drift that might extend for several years, so particular attention was paid to the insulation of the living quarters.[20] At around 400 gross register tonnage, the ship was considerably larger than Nansen had first anticipated,[b] with an overall length of 128 feet (39 m) and a breadth of 36 feet (11 m), a ratio of just over three to one, giving her an unusually stubby appearance.[c][29] This odd shape was explained by Archer: "A ship that is built with exclusive regard to its suitability for [Nansen's] object must differ essentially from any known vessel."[30] On 6 October 1892, at Archer's yard at Larvik, the ship was launched by Nansen's wife Eva after a brief ceremony. The ship was named Fram, meaning "Forward".[29]		For his Greenland expedition of 1888–89 Nansen had departed from the traditional dependence on large-scale personnel, ships and backup, relying instead on a small well-trained group.[31] Using the same principle for the Fram voyage, Nansen chose a party of just twelve from the thousands of applications that poured in from all over the world. One applicant was the 20-year-old Roald Amundsen, future conqueror of the South Pole, whose mother stopped him from going. The English explorer Frederick Jackson applied, but Nansen wanted only Norwegians, so Jackson organised his own expedition to Franz Josef Land.[32]		To captain the ship and act as the expedition's second-in-command Nansen chose Otto Sverdrup, an experienced sailor who had taken part in the Greenland crossing. Theodore Jacobsen, who had experience in the Arctic as skipper of a sloop, signed on as Fram's mate, and a young naval lieutenant, Sigurd Scott Hansen, took charge of meteorological and magnetic observations. The ship's doctor, and the expedition's botanist, was Henrik Blessing, who graduated in medicine just before Fram's sailing date. Hjalmar Johansen, an army reserve lieutenant and dog-driving expert, was so determined to join the expedition that he agreed to sign on as stoker, the only position by then available. Likewise Adolf Juell, with 20 years' experience at sea as mate and captain, took the post of cook on the Fram voyage.[33] Ivar Mogstad was an official at Gaustad psychiatric hospital, but his technical abilities as a handyman and mechanic impressed Nansen.[34] The oldest man in the party, at 40, was the chief engineer, Anton Amundsen (no relation of Roald). The second engineer, Lars Pettersen, kept his Swedish nationality from Nansen, and although it was soon discovered by his shipmates, he was allowed to remain with the expedition, the only non-Norwegian in the party.[35] The remaining crew members were Peter Henriksen, Bernhard Nordahl and Bernt Bentzen, the last–named joining the expedition in Tromsø at very short notice.[33]		Before the start of the voyage Nansen decided to deviate from his original plan: instead of following Jeannette's route to the New Siberian Islands by way of the Bering Strait, he would make a shorter journey, taking Nordenskiöld's North-East Passage along the northern coast of Siberia.[d][37] Fram left Christiania on 24 June 1893, seen on her way by a cannon salute from the fort and the cheers of thousands of well-wishers.[38] This was the first of a series of farewells as Fram sailed round the coast and moved northward, reaching Bergen on 1 July (where there was a great banquet in Nansen's honour), Trondheim on 5 July and Tromsø, north of the Arctic Circle, a week later. The last Norwegian port of call was Vardø, where Fram arrived on 18 July. After the final provisions were taken on board, Nansen, Sverdrup, Hansen and Blessing spent their last hours ashore in a sauna, being beaten with birch twigs by two young girls.[39][40]		The first leg of the journey eastward took Fram across the Barents Sea towards Novaya Zemlya and then to the North Russian settlement of Khabarova where the first batch of dogs was brought on board. On 3 August Fram weighed anchor and moved cautiously eastward, entering the Kara Sea the next day.[41] Few ships had sailed the Kara Sea before, and charts were incomplete. On 18 August, in the area of the Yenisei River delta, an uncharted island was discovered and named Sverdrup Island after Fram's commander.[42][43] Fram was now moving towards the Taimyr Peninsula and Cape Chelyuskin, the most northerly point of the Eurasian continental mass. Heavy ice slowed the expedition's progress, and at the end of August it was held up for four days while the ship's boiler was repaired and cleaned. The crew also experienced the dead water phenomenon, where a ship's forward progress is impeded by energy dissipation caused by a layer of fresh water lying on top of heavier salt water.[43] On 9 September a wide stretch of ice-free water opened up, and next day Fram rounded Cape Chelyuskin—the second ship to do so, after Nordenskiöld's Vega in 1878—and entered the Laptev Sea.[43]		After being prevented by ice from reaching the mouth of the Olenyok River, where a second batch of dogs was waiting to be picked up, Fram moved north and east towards the New Siberian Islands. Nansen's hope was to find open water to 80° north latitude and then enter the pack; however, on 20 September ice was sighted just south of 78°. Fram followed the line of the ice before stopping in a small bay beyond the 78° mark. On 28 September it became evident that the ice would not break up, and the dogs were moved from the ship to kennels on the ice. On 5 October the rudder was raised to a position of safety and the ship, in Scott Hansen's words, was "well and truly moored for the winter".[44] The position was 78°49′N, 132°53′E.[45]		On 9 October Fram had her first experience of ice pressure. Archer's design was quickly vindicated as the ship rose and fell, the ice being unable to grip the hull.[45] Otherwise the first weeks in the ice were disappointing, as the unpredictable drift moved Fram in gyratory fashion, sometimes north, sometimes south;[37] by 19 November, after six weeks, Fram was south of the latitude at which she had entered the ice.[46]		After the sun disappeared on 25 October the ship was lit by electric lamps from a wind-powered generator.[47] The crew settled down to a comfortable routine in which boredom and inactivity were the main enemies. Men began to irritate each other, and fights sometimes broke out.[48] Nansen attempted to start a newspaper, but the project soon fizzled out through lack of interest. Small tasks were undertaken and scientific observations maintained, but there was no urgency. Nansen expressed his frustration in his journal: "I feel I must break through this deadness, this inertia, and find some outlet for my energies." And later: "Can't something happen? Could not a hurricane come and tear up this ice?"[49] Only after the turn of the year, in January 1894, did the northerly direction become generally settled. The 80° mark was finally passed on 22 March.[50]		Based on the uncertain direction and slow speed of the drift, Nansen calculated that it might take the ship five years to reach the pole.[51] In January 1894 he had first discussed with both Henriksen and Johansen the possibility of making a sledge journey with the dogs, from Fram to the pole, though they made no immediate plans.[51] Nansen's first attempts to master dog-driving were an embarrassing failure,[52] but he persevered and gradually achieved better results.[53] He also discovered that the normal cross-country skiing speed was the same as that of dogs pulling loaded sledges. Men could travel under their own power, skiing, rather than riding on the sledge, and loads could be correspondingly increased. This, according to biographer and historian Roland Huntford, amounted to a revolution in polar travel methods.[54]		On 19 May, two days after the celebrations for Norway's National Day, Fram passed 81°, indicating that the ship's northerly speed was slowly increasing, though it was still barely a mile (1.6 km) a day. With a growing conviction that a sledge journey might be necessary to reach the pole, in September Nansen decreed that everyone would practice skiing for two hours a day. On 16 November he revealed his intention to the crew: he and one companion would leave the ship and start for the pole when the 83° mark was passed. After reaching the pole the pair would make for Franz Josef Land, and then cross to Spitsbergen where they hoped to find a ship to take them home. Three days later Nansen asked Hjalmar Johansen, the most experienced dog-driver among the crew, to join him on the polar journey.[55]		The crew spent the following months preparing for the forthcoming dash for the pole. They built sledges that would facilitate fast travel over rough terrain and constructed kayaks on the Inuit model, for use during the expected water crossings.[56] There were endless trials of special clothing and other gear. Violent and prolonged tremors began to shake the ship on 3 January 1895, and two days later the crew disembarked, expecting the ship to be crushed. Instead the pressure lessened, and the crew went back on board and resumed preparations for Nansen's journey. After the excitement it was noted that Fram had drifted beyond Greely's Farthest North record of 83°24, and on 8 January was at 83°34′N.[57]		On 17 February 1895 Nansen began a farewell letter to his wife, Eva, writing that should he come to grief "you will know that your image will be the last I see."[58] He was also reading everything he could about Franz Josef Land, his intended destination after the pole. The archipelago had been discovered in 1873 by Julius Payer, and was incompletely mapped.[e] It was, however, apparently the home of countless bears and seals, and Nansen saw it as an excellent food source on his return journey to civilization.[60]		On 14 March, with the ship at 84°4′N, the pair finally began their polar march. This was their third attempt to leave the ship; on 26 February and again on the 28th, damage to sledges had forced them to return after travelling short distances.[61][62] After these mishaps Nansen thoroughly overhauled his equipment, minimised the travelling stores, recalculated weights and reduced the convoy to three sledges, before giving the order to start again. A supporting party accompanied the pair and shared the first night's camp. The next day, Nansen and Johansen skied on alone.[63][64]		The pair initially traveled mainly over flat snowfields. Nansen had allowed 50 days to cover the 356 nautical miles (660 km; 410 mi) to the pole, requiring an average daily journey of seven nautical miles (13 km; 8.1 mi). On 22 March a sextant observation showed that the pair had travelled 65 nautical miles (120 km; 75 mi) towards the pole at a daily average of over nine nautical miles (17 km; 10 mi). This had been achieved despite very low temperatures, typically around −40 °F (−40 °C), and small scale mishaps including the loss of the sledgemeter that recorded mileage.[65] However, as the surfaces became uneven and made skiing more difficult, their speeds slowed. A sextant reading on 29 March of 85°56′N indicated that a week's travel had brought them 47 nautical miles (87 km; 54 mi) nearer to the pole, but also showed that their average daily distances were falling. More worryingly, a theodolite reading that day suggested that they were at only 85°15′N, and they had no means of knowing which of the readings was correct.[66] They realised that they were fighting a southerly drift, and that distances travelled did not necessarily equate to northerly progression.[67] Johansen's diary indicated his failing spirits: "My fingers are all destroyed. All mittens are frozen stiff ... It is becoming worse and worse ... God knows what will happen to us".[68]		On 3 April, after days of difficult travel, Nansen privately began to wonder if the pole might, after all, be out of reach. Unless the surface improved, their food would not last them to the pole and then on to Franz Josef Land.[67] The next day they calculated their position at a disappointing 86°3'; Nansen confided in his diary that: "I have become more and more convinced we ought to turn before time."[69] After making camp on 7 April Nansen scouted ahead on snowshoes looking for a path forward, but saw only "a veritable chaos of iceblocks stretching as far as the horizon". He decided that they would go no further north, and would head for Cape Fligely in Franz Josef Land. Nansen recorded the latitude of their final northerly camp as 86°13.6′N, almost three degrees (169.6 nautical miles, or 314 km) beyond Greely's previous Farthest North mark.[70]		The change of direction to south-west provided much better travelling conditions, probably because the course to Franz Josef Land was broadly parallel to the lines of disturbance in the ice instead of perpendicular to them.[71] Progress was swift: "If this goes on," Nansen recorded on 13 April, "the return journey will be quicker than I thought."[72] However, the same diary entry records a mishap that day: both men's watches had stopped. Although Nansen's journal comment is mild, the incident was potentially disastrous.[73][74] Without the correct time they could not calculate their longitude and thus maintain the correct course to Franz Josef Land. They restarted the watches based on Nansen's guesswork that their longitude was 86°E, but the pair were no longer certain where they were. If they were farther west than Nansen's assumption, they might miss Franz Josef Land altogether, and head for the open Atlantic.[73]		The direction of the drift became northerly, hampering the pair's progress. By 18 April, after 11 days' travel from Farthest North, they had only made 40 nautical miles (74 km; 46 mi) to the south.[75] They now travelled over much more broken terrain with wide open leads of water. On about 20 April they were cheered by the sight of a large piece of driftwood stuck in a floe, the first object from the outside world they had seen since Fram had entered the ice. Johansen carved his and Nansen's initials on it, with the latitude and date. A day or two later they spotted the tracks of an Arctic fox, the first trace of a living creature other than their dogs since leaving Fram. Other tracks soon appeared, and Nansen began to believe that land might be near.[76]		The latitude calculated on 9 May, 84°3′N, was disappointing—Nansen had hoped they were farther south.[77] However, as May progressed they began to see bear tracks, and by the end of the month seals, gulls and whales were plentiful. By Nansen's calculations, they had reached 82°21′N on 31 May, placing them only 50 nautical miles (93 km; 58 mi) from Cape Fligely at the northern extremity of Franz Josef Land, if his longitude estimate was accurate.[78] In the warmer weather the ice began to break up, making travel more difficult. Since 24 April dogs had been killed at regular intervals to feed the others, and by the beginning of June only seven of the original 28 remained. On 21 June the pair jettisoned all surplus equipment and supplies, planning to travel light and live off the now plentiful supplies of seal and birds. After a day's travel in this manner they decided to rest on a floe, waterproof the kayaks and build up their own strength for the next stage of their journey. They remained camped on the floe for a whole month.[79]		On 23 July, the day after leaving the camp, Nansen had the first indisputable glimpse of land. He wrote: "At last the marvel has come to pass—land, land, and after we had almost given up our belief in it!"[80] In the succeeding days the pair struggled towards this land, which seemingly grew no nearer, although by the end of July they could hear the distant sound of breaking surf.[81] On 4 August they survived a polar bear attack; two days later they reached the edge of the ice, and only water lay between them and the land. On 6 August they shot the last two Samoyed dogs, converted the kayaks into a catamaran by lashing sledges and skis across them, and raised a sail.[82]		Nansen called this first land "Hvidtenland" ("White Island").[83] After making camp on an ice foot they ascended a slope and looked about them. It was apparent that they were in an archipelago, but what they could see bore no relation to their incomplete map of Franz Josef Land.[84] They could only continue south in the hopes of finding a geographical feature they could pinpoint with certainty. On 16 August Nansen tentatively identified a headland as Cape Felder, marked on Payer's maps as on the western coast of Franz Josef Land.[85] Nansen's objective was now to reach a hut and supplies that had been left by an earlier expedition at a location known as Eira Harbour, at the southern end of the islands. However, contrary winds and loose ice made further progress in the kayak hazardous, and on 28 August Nansen decided that, with another polar winter drawing near, they should stay where they were and await the following spring.[86]		As the base for their winter quarters, Nansen and Johansen found a beach in a sheltered cove on what is now called Jackson Island, with a plentiful supply of stones and moss for building materials. They excavated a hole three feet deep, raised walls around it using loose rocks and stones, and stretched walrus skins over the top to form a roof. A chimney was improvised using snow and walrus bones. This shelter, which they called "The Hole", was finally ready on 28 September, and was to be their home for the next eight months.[87] Their situation was uncomfortable, but not life-threatening; there was a plentiful supply of bear, walrus and seal to stock up their larder. The chief enemy was boredom; to pass time they were reduced to reading Nansen's sailing almanac and navigation tables by the light of their blubber lamp, and then reading them again.[88]		At Christmas the pair celebrated with chocolate and bread from their sledging rations. On New Year's Eve Johansen recorded that Nansen finally adopted the familiar form of address, having until then maintained formalities ("Mr Johansen", "Professor Nansen") throughout the journey.[88][89] In the New Year they fashioned themselves simple outer clothing—smocks and trousers—from a discarded sleeping bag, in readiness for the resumption of their journey when the weather grew warmer. On 19 May 1896, after weeks of preparation, they were ready. Nansen left a note in the hut to inform a possible finder: "We are going south west, along the land, to cross over to Spitsbergen".[90]		For more than two weeks they followed the shoreline southwards. Nothing they saw seemed to fit with their rudimentary map of Franz Josef Land, and Nansen speculated whether they were in uncharted lands between Franz Josef Land and Spitsbergen. On 4 June a change in conditions allowed them to launch their kayaks for the first time since leaving their winter quarters. A week later, Nansen was forced to dive into the icy waters to rescue the kayaks which, still tied together, had drifted away after being carelessly moored. He managed to reach the craft and, with a last effort, to haul himself aboard. Despite his frozen condition he shot and retrieved two guillemots as he paddled the catamaran back.[91]		On 13 June walruses attacked and damaged the kayaks, causing another stop for repairs. On 17 June, as they prepared to leave again, Nansen thought he heard a dog bark and went to investigate. He then heard voices, and a few minutes later encountered a human being.[92] It was Frederick Jackson, who had organised his own expedition to Franz Josef Land after being rejected by Nansen, and had based his headquarters at Cape Flora on Northbrook Island, the southernmost island of the archipelago.[92] Jackson's own account records that his first reaction to this sudden meeting was to assume the figure to be a shipwrecked sailor, perhaps from the expedition's supply ship Windward which was due to call that summer. As he approached, Jackson saw "a tall man, wearing a soft felt hat, loosely made, voluminous clothes and long shaggy hair and beard, all reeking with black grease". After a moment's awkward hesitation, Jackson recognised his visitor: "You are Nansen, aren't you?", and received the reply "Yes, I am Nansen."[93]		Johansen was rescued, and the pair taken to the base at Cape Flora, where they posed for photographs (in one instance re-enacting the Jackson–Nansen meeting) before taking baths and haircuts. Both men seemed in good health, despite their ordeal; Nansen had put on 21 pounds (9.5 kg) in weight since the start of the expedition, and Johansen 13 pounds (5.9 kg).[94] In honour of his rescuer, Nansen named the island where he had wintered "Frederick Jackson Island".[95] For the next six weeks Nansen had little to do but await the arrival of Windward, worrying that he might have to spend the winter at Cape Flora, and sometimes regretting that he and Johansen had not pressed on to Spitsbergen.[96] Johansen noted in his journal that Nansen had changed from the overbearing personality of the Fram days, and was now subdued and polite, adamant that he would never undertake such a journey again.[97] On 26 July Windward finally arrived; on 7 August, with Nansen and Johansen aboard, she sailed south and on 13 August reached Vardø. A batch of telegrams was sent, informing the world of Nansen's safe return.[98]		Before his departure from Fram, Nansen appointed Sverdrup as leader of the rest of the expedition, with orders to continue with the drift towards the Atlantic Ocean unless circumstances warranted abandoning the ship and marching for land. Nansen left precise instructions about keeping up the scientific work, especially the ocean depth soundings and the tests for the thickness of the ice. He concluded: "May we meet in Norway, whether it be on board of this vessel or without her."[99]		Sverdrup's main task now was to keep his crew busy. He ordered a thorough spring cleaning, and set a party to chip away some of the surrounding ice which was threatening to destabilise the ship. Although there was no immediate danger to Fram, Sverdrup oversaw the repair and overhaul of sledges, and the organisation of provisions should it after all be necessary to abandon ship and march to land. With the arrival of warmer weather as the 1895 summer approached, Sverdrup resumed daily ski practice.[100] Amid these activities a full programme of meteorological, magnetic and oceanographic activities continued under Scott Hansen; Fram had become a moving oceanographic, meteorological and biological laboratory.[5]		As the drift proceeded the ocean became deeper; soundings gave successive depths of 6,000 feet (1,800 m), 9,000 feet (2,700 m) and 2,000 feet (610 m), a progression which indicated that no undiscovered land mass was nearby.[101] On 15 November 1895 Fram reached 85°55′N, only 19 nautical miles (35 km; 22 mi) below Nansen's Farthest North mark.[102] From this point on, the drift was generally to the south and west, although progress was for long periods almost imperceptible. Inactivity and boredom led to increased drinking; Scott Hansen recorded that Christmas and New Year passed "with the usual hot punch and consequent hangover", and wrote that he was "getting more and more disgusted with drunkenness".[103] By mid-March 1896, the position was 84°25′N, 12°50′E, placing the ship north of Spitsbergen. On 13 June a lead opened and, for the first time in nearly three years, Fram became a living ship. It was a further two months, on 13 August 1896, before she found open water and, with a blast from her cannon, left the ice behind.[103] She had emerged from the ice just north and west of Spitsbergen, close to Nansen's original prediction, proving him right and his detractors wrong.[104] Later that same day a ship was sighted—Søstrone, a seal hunter from Tromsø. Sverdrup rowed across for news, and learned that nothing had been heard from Nansen. Fram called briefly at Spitsbergen, where the Swedish explorer-engineer Salomon Andrée was preparing for the balloon flight that he hoped would take him to the pole. After a short time ashore, Sverdrup and his crew began the trip south to Norway.[103]		In the course of the expedition, rumours circulated that Nansen had reached the North Pole, the first as early as April 1894, in the French Newspaper Le Figaro.[105] In September 1895 Eva Nansen was informed that messages signed by Nansen had been discovered, "sent from the North Pole".[105] In February 1896 The New York Times ran a dispatch from Irkutsk, in Siberia, from a supposed Nansen agent, claiming that Nansen had reached the pole and found land there. Charles P. Daly of the American Geographical Society called this "startling news" and, "if true, the most important discovery that has been made in ages."[106]		Experts were sceptical of all such reports, and Nansen's arrival in Vardø quickly put paid to them. In Vardø, he and Johansen were greeted by Professor Mohn, the originator of the polar drift theory, who was in the town by chance.[107] The pair waited for the weekly mail steamer to take them south, and on 18 August arrived in Hammerfest to an enthusiastic reception. The lack of news about Fram was preying on Nansen's mind; however, on 20 August he received news that Sverdrup had brought the ship to the tiny port of Skjervøy, south of Hammerfest, and was now continuing with her to Tromsø.[108] The next day, Nansen and Johansen sailed into Tromsø and joined their comrades in an emotional reunion.[109]		After days of celebration and recuperation the ship left Tromsø on 26 August. The voyage south was a triumphal procession, with receptions at every port. Fram finally arrived in Christiania on 9 September, escorted into the harbour by a squadron of warships and welcomed by thousands—the largest crowds the city had ever seen, according to Huntford.[110] Nansen and his crew were received by King Oscar; on the way to the reception they passed through a triumphal arch formed by 200 gymnasts. Nansen and his family stayed at the palace as special guests of the king; by contrast, Johansen remained in the background, largely overlooked, and writing that "reality, after all, is not so wonderful as it appeared to me in the midst of our hard life."[109]		The traditional approach to Arctic exploration had relied on large-scale forces, with a presumption that European techniques could be successfully transplanted into the hostile polar climate. Over the years this strategy had brought little success, and had led to heavy losses of men and ships.[111] By contrast, Nansen's method of using small, trained crews, and harnessing Inuit and Sami expertise in his methods of travel, had ensured that his expedition was completed without a single casualty or major mishap.[111]		Although it did not achieve the objective of reaching the North Pole, the expedition made major geographical and scientific discoveries. Sir Clements Markham, president of Britain's Royal Geographical Society, declared that the expedition had resolved "the whole problem of Arctic geography".[112] It was now established that the North Pole was located not on land, nor on a permanent ice sheet, but on shifting, unpredictable pack ice.[113] The Arctic Ocean was a deep basin, with no significant land masses north of the Eurasian continent—any hidden expanse of land would have blocked the free movement of ice.[f][115] Nansen had proved the polar drift theory; furthermore, he had noted the presence of a Coriolis force driving the ice to the right of the wind direction, due to the effect of the Earth's rotation. This discovery would be developed by Nansen's pupil, Vagn Walfrid Ekman, who later became the leading oceanographer of his time.[115][116] From its programme of scientific observation the expedition provided the first detailed oceanographic information from the area; in due course the scientific data gathered during the Fram voyage would run to six published volumes.[5]		Throughout the expedition Nansen continued to experiment with equipment and techniques, altering the designs of skis and sledges and investigating types of clothing, tents and cooking apparatus, thereby revolutionising methods of Arctic travel.[117][118] In the era of polar exploration which followed his return, explorers routinely sought Nansen's advice as to methods and equipment—although sometimes they chose not to follow it, usually to their cost.[119][120] According to Huntford, the South Pole heroes Amundsen, Scott, and Ernest Shackleton were all Nansen's acolytes.[118]		Nansen's status was never seriously challenged, although he did not escape criticism. American explorer Robert Peary wondered why Nansen had not returned to the ship when his polar dash was thwarted after a mere three weeks away. "Was he ashamed to go back after so short an absence, or had there been a row ... or did he go off for Franz Josef Land from sensational motives or business reasons?"[121] Adolphus Greely, who had initially dismissed the entire expedition as infeasible, admitted that he had been proved wrong but nevertheless drew attention to "the single blemish"—Nansen's decision to leave his comrades hundreds of miles from land. "It passes comprehension", Greely wrote, "how Nansen could have thus deviated from the most sacred duty devolving on the commander of a naval expedition."[122] Nansen's reputation nevertheless survived; a hundred years after the expedition the British explorer Wally Herbert called the Fram voyage "one of the most inspiring examples of courageous intelligence in the history of exploration".[121]		The Fram voyage was Nansen's final expedition. He was appointed to a research professorship at the University of Christiania in 1897, and to a full professorship in oceanography in 1908.[5] He became independently wealthy as a result of the publication of his expedition account;[123] in his later career he served the newly independent kingdom of Norway in different capacities, and was awarded the Nobel Peace Prize for 1922, in recognition of his work on behalf of refugees.[5] Hjalmar Johansen never settled back into normal life. After years of drifting, debt and drunkenness he was given the opportunity, through Nansen's influence, to join Roald Amundsen's South Pole expedition in 1910. Johansen quarreled violently with Amundsen at the expedition's base camp, and was omitted from the South Pole party. He committed suicide within a year of his return from Antarctica.[124] Otto Sverdrup remained as captain of Fram, and in 1898 took the ship, with a new crew, to the Canadian Arctic for four years' exploration.[125] In later years Sverdrup helped to raise funds that enabled the ship to be restored and housed in a permanent museum.[126] He died in November 1930, seven months after Nansen's death.[127][128]		Nansen's farthest north record lasted for just over five years. On 24 April 1900 a party of three from an Italian expedition led by the Duke of the Abruzzi reached 86°34′N, having left Franz Josef Land with dogs and sledges on 11 March. The party barely made it back; one of their support groups of three men vanished entirely.[129]		Notes		References		
Mother Nature (sometimes known as Mother Earth or the Earth-Mother) is a common personification of nature that focuses on the life-giving and nurturing aspects of nature by embodying it, in the form of the mother.						The word "nature" comes from the Latin word, "natura", meaning birth or character (see nature (innate)). In English its first recorded use (in the sense of the entirety of the phenomena of the world) was in 1266 A.D.. "Natura", and the personification of Mother Nature, was widely popular in the Middle Ages. As a concept, seated between the properly divine and the human, it can be traced to Ancient Greece, though Earth (or "Eorthe" in the Old English period) may have been personified as a goddess. The Norse also had a goddess called Jord (or Earth).		The earliest written dated literal references to the term "Mother Earth" occur in Mycenaean Greek. Ma-ka (transliterated as ma-ga), "Mother Gaia", written in Linear B syllabic script (13th or 12th century BC).[1] The various myths of nature goddesses such as Inanna/Ishtar (myths and hymns attested on Mesopotamian tablets as early as the 3rd millennium BC) show that the personification of the creative and nurturing sides of nature as female deities has deep roots. In Greece, the pre-Socratic philosophers had "invented" nature when they abstracted the entirety of phenomena of the world as singular: physis, and this was inherited by Aristotle. Later medieval Christian thinkers did not see nature as inclusive of everything, but thought that she had been created by God; her place lay on earth, below the unchanging heavens and moon. Nature lay somewhere in the center, with agents above her (angels), and below her (demons and hell). For the medieval mind she was only a personification, not a goddess.		In Greek mythology, Persephone, daughter of Demeter (goddess of the harvest), was abducted by Hades (god of the dead), and taken to the underworld as his queen. Demeter was so distraught that no crops would grow and the "entire human race [would] have perished of cruel, biting hunger if Zeus had not been concerned" (Larousse 152). Zeus forced Hades to return Persephone to her mother, but while in the underworld, Persephone had eaten pomegranate seeds, the food of the dead and thus, she must spend part of each year with Hades in the underworld. Demeter's grief for her daughter in the realm of the dead, is reflected in the barren winter months and her joy when Persephone returns is reflected in the bountiful summer months.		Demeter would take the place of her grandmother, Gaia, and her mother, Rhea, as goddess of the earth in a time when humans and gods thought the activities of the heavens more sacred than those of earth.[2]		Roman Epicurean poet Lucretius opens his didactic poem De rerum natura by addressing Venus as a veritable mother of nature.[3] Lucretius uses Venus as "a personified symbol for the generative aspect of nature".[4] This largely has to do with the nature of Lucretius' work, which presents a nontheistic understanding of the world that eschews superstition.[4]		Algonquian legend says that "beneath the clouds lives the Earth-Mother from whom is derived the Water of Life, who at her bosom feeds plants, animals and human" (Larousse 428). She is also known as Nokomis, the Grandmother.		In Inca mythology, Mama Pacha or Pachamama is a fertility goddess who presides over planting and harvesting. Pachamama is usually translated as "Mother Earth" but a more literal translation would be "Mother Universe" (in Aymara and Quechua mama = mother / pacha = world, space-time or the universe).[5] Pachamama and her husband, Inti, are the most benevolent deities and are worshiped in parts of the Andean mountain ranges (stretching from present day Ecuador to Chile and Argentina).		In the Southeast Asian Indochina countries of Cambodia, Laos and Thailand, earth (terra firma) is personified as Phra Mae Thorani, but her role in Buddhist mythology differs considerably from that of Mother Nature. In the Malay Archipelago, that role is filled by Dewi Sri, The Rice-mother in the East Indies.		
Lionel Terray (25 July 1921 – 19 September 1965) was a French climber who made many first ascents, including Makalu in the Himalaya (with Jean Couzy on 15 May 1955) and Cerro Fitzroy in the Patagonian Andes (with Guido Magnone in 1952).		A climbing guide and ski instructor, Terray was active in mountain combat against Germany during World War II. After the war, he became well known as one of the best Chamonix climbers and guides, noted for his speedy ascents of some of the most notorious climbs in the French, Italian, and Swiss Alps: the Walker Spur of the Grandes Jorasses, the south face of the Aiguille Noire de Peuterey, the north-east face of Piz Badile, and the north face of the Eiger. Terray, frequently with climbing partner Louis Lachenal, broke previous climbing speed records.		Terray was a member of Maurice Herzog's 1950 expedition to the Nepalese Himalayan peak, Annapurna, the highest peak climbed at the time, and the first 8000-meter peak climbed (although British climbers George Mallory, Andrew Irvine, George Finch, Geoffrey Bruce, Henry Morshead, Teddy Norton and Howard Somervell had reached higher altitudes on Mount Everest during the 1920s). Terray did not reach the summit of Annapurna, but together with the Sherpa Adjiba he aided summitteers Maurice Herzog and Louis Lachenal down from the mountain. Both Herzog and Lachenal experienced extreme frostbite and subsequently underwent amputations.[1] Despite these events, the French team returned to Paris to huge public acclaim, and Herzog's expedition book Annapurna became an international bestseller.		Terray was also one of the main participants in the great attempt to rescue four climbers trapped on the north face of the Eiger in 1957. This mission forms the subject of Jack Olsen's famous book The Climb Up To Hell, in which Terray's skill and bravery receive special mention.		In the late 1950s and early 1960s, Terray made a number of first ascents in Peru, including the highest unclimbed peak in the central Andes at the time, 20,981-foot Huantsan. He also made first ascents of lower but more difficult peaks, including Willka Wiqi, Soray, Tawllirahu, and Chakrarahu, possibly the hardest peak in the Peruvian Andes and considered unclimbable at the time. One of Terray's finest achievements was the first ascent of 25,295-foot Jannu in Nepal in 1962. He also climbed the Nilgiris near Annapurna, and led the successful 1964 first ascent of 12,240 foot Mount Huntington, in the Alaska Range, by the northwest ridge.[2]		Terray died on a rock climb in the Vercors, south of Grenoble, on 19 September 1965, several years after the publication of his climbing memoir, Conquistadors of the Useless.		His grave is situated in Chamonix, France. A traffic circle is named for him in Chamonix, WSW of town.		
Saga:		Anthology films:		Animated films:		Legends:		TV Specials:		Star Wars is an American epic space opera franchise, centered on a film series created by George Lucas. It depicts the adventures of various characters "a long time ago in a galaxy far, far away".		The franchise began in 1977 with the release of the film Star Wars (later subtitled Episode IV: A New Hope in 1981),[2][3] which became a worldwide pop culture phenomenon. It was followed by the successful sequels The Empire Strikes Back (1980) and Return of the Jedi (1983); these three films constitute the original Star Wars trilogy. A prequel trilogy was released between 1999 and 2005, which received mixed reactions from both critics and fans. A sequel trilogy began in 2015 with the release of Star Wars: The Force Awakens. All seven films were nominated for Academy Awards (with wins going to the first two films) and have been commercial successes, with a combined box office revenue of over US$7.5 billion,[4] making Star Wars the third highest-grossing film series.[5] Spin-off films include the animated Star Wars: The Clone Wars (2008) and Rogue One (2016), the latter of which is the first in a planned series of anthology films.		The series has spawned an extensive media franchise including books, television series, computer and video games, and comic books, all of which take place within the same continuity as the films, resulting in significant development of the series's fictional universe, with the non-canonical works falling under the defunct Star Wars Legends label. Star Wars also holds a Guinness World Records title for the "Most successful film merchandising franchise. In 2015, the total value of the Star Wars franchise was estimated at US$42 billion,[6][7] making Star Wars the second highest-grossing media franchise of all time.		In 2012, The Walt Disney Company bought Lucasfilm for US$4.06 billion and earned the distribution rights to all subsequent Star Wars films, beginning with the release of The Force Awakens in 2015.[8] The former distributor, 20th Century Fox, retains the physical distribution rights for the first two Star Wars trilogies, owns permanent rights for the original 1977 film and continues to hold the rights for the prequel trilogy and the first two sequels to A New Hope until May 2020.[9][10] Walt Disney Studios owns digital distribution rights to all the Star Wars films, excluding A New Hope.[10][11]						The Star Wars franchise takes place in a distant unnamed fictional galaxy at an undetermined point in the ancient past, where many species of aliens (often humanoid) co-exist. People own robotic droids, who assist them in their daily routines, and Space travel is common.		The spiritual and mystical element of the Star Wars galaxy is known as "The Force". It is described in the original film as "an energy field created by all living things [that] surrounds us, penetrates us, [and] binds the galaxy together".[12] The people who are born deeply connected to the Force have better reflexes; through training and meditation, they are able to achieve various supernatural feats (such as telekinesis, clairvoyance, precognition, and mind control). The Force is wielded by two major factions at conflict: the Jedi whom harness the light side of the Force, and the Sith whom use the dark side of the force through hate and aggression.		In 1971, Universal Studios made a contract for George Lucas to direct two films. In 1973, American Graffiti was completed, and released to critical acclaim including Academy Award nominations for Best Director and Original Screenplay for George Lucas. Months later, Lucas started work on his second film, by starting the script draft, The Journal of the Whills told the tale of the training of apprentice CJ Thorpe as a "Jedi-Bendu" space commando by the legendary Mace Windy. After Universal rejected the film, 20th Century Fox decided to invest on it.[24] On April 17, 1973, Lucas felt frustrated about his story being too difficult to understand, so he began writing a 13-page script with thematic parallels to Akira Kurosawa's The Hidden Fortress, this draft was renamed The Star Wars.[25] By 1974, he had expanded the script into a rough draft screenplay, adding elements such as the Sith, the Death Star, and a protagonist named Annikin Starkiller. Numerous subsequent drafts would go, through numerous drastic changes, before evolving into the script of the original film.		Star Wars was released on May 25, 1977. It was followed by The Empire Strikes Back, released on May 21, 1980, and Return of the Jedi, released on May 25, 1983. The opening crawl of the sequels disclosed that they were numbered as "Episode V" and "Episode VI" respectively, though the films were generally advertised solely under their subtitles. Though the first film in the series was simply titled Star Wars, with its 1981 re-release it had the subtitle Episode IV: A New Hope added to remain consistent with its sequel, and to establish it as the middle chapter of a continuing saga.[26] The plot of the original trilogy centers on the Galactic Civil War of the Rebel Alliance trying to free the galaxy from the crutches of the Galactic Empire, as well as on Luke Skywalker's quest to become a Jedi.		Near the orbit of the desert planet Tatooine, a Rebel spaceship is intercepted by the Empire. Aboard, the deadliest Imperial agent Darth Vader and his stormtroopers capture Princess Leia Organa, a secret member of the rebellion. Before her capture, Leia makes sure the astromech R2-D2, along with the protocol droid C-3PO, escapes with stolen Imperial blueprints stored inside and a holographic message for the retired Jedi Knight Obi-Wan Kenobi, who has been living in exile on Tatooine. The droids fall under the ownership of Luke Skywalker, an orphan farm boy raised by his step-uncle and aunt. Luke helps the droids locate Obi-Wan, now a solitary old hermit known as Ben Kenobi, who reveals himself as a friend of Luke's absent father, the Jedi Knight Anakin Skywalker. Obi-Wan confides to Luke that Anakin was "betrayed and murdered" by Vader (who was Obi-Wan's former Jedi apprentice) years ago, and he gives Luke his father's former lightsaber to keep.[27] After viewing Leia's message, they both hire the smuggler Han Solo and his Wookiee co-pilot Chewbacca to, aboard their space freighter the Millennium Falcon, help them deliver the stolen blueprints inside R2-D2 to the Rebel Alliance with the hope of finding a weakness to the Empire's planet-destroying space station: the Death Star.[12]		For The Star Wars second draft, Lucas made heavy simplifications. It added a mystical energy field known as "The Force" and introduced the young hero on a farm as Luke Starkiller. Annikin became Luke's father, a wise Jedi knight. The third draft killed the father Annikin, replacing him with mentor figure Ben Kenobi. Later, Lucas felt the film would not in fact be the first in the sequence, but a film in the second trilogy in the saga. The draft contained a sub-plot leading to a sequel about "The Princess of Ondos", and by that time some months later Lucas had negotiated a contract that gave him rights to make two sequels. Not long after, Lucas hired author Alan Dean Foster, to write two sequels as novels.[28] In 1976, a fourth draft had been prepared for principal photography. The film was titled Adventures of Luke Starkiller, as taken from the Journal of the Whills, Saga I: The Star Wars. During production, Lucas changed Luke's name to Skywalker and altered the title to simply The Star Wars and finally Star Wars.[29] At that point, Lucas was not expecting the film to have sequels. The fourth draft of the script underwent subtle changes it discarded "the Princess of Ondos" sub-plot, to become a self-contained film, that ended with the destruction of the Galactic Empire itself by way of destroying the Death Star. However, Lucas previously conceived of the film as the first of a series. The intention was that if Star Wars was successful, Lucas could adapt Dean Foster's novels into low-budget sequels.[30] By that point, Lucas had developed an elaborate backstory to aid his writing process.[31]		Before its release, Lucas considered walking away from Star Wars sequels, thinking the film would be a flop. However the film exceeded all expectations. The success of the film, as well as its merchandise sales, and Lucas desire to create an independent film-making center. Both led Lucas to make Star Wars the basis of an elaborate film serial,[32] and use the profits to finance his film-making center, Skywalker Ranch.[33] Alan Dean Foster was already writing the first sequel-novel Splinter of the Mind's Eye, released in 1978. But Lucas decided not to adapt Foster's work. Knowing a sequel, would be allowed more budget. At first, Lucas envisioned a series of films with no set number of entries, like the James Bond series. In an interview with Rolling Stone in August 1977, he said that he wanted his friends to each take a turn at directing the films and giving unique interpretations on the series.[citation needed] Also adding that the backstory in which Darth Vader turns to the dark side, kills Luke's father and fights Obi-Wan Kenobi on a volcano as the Galactic Republic falls would make an excellent sequel.[citation needed]		Three years after the destruction of the Death Star, the Rebels are forced to evacuate their secret base on Hoth as they are hunted by the Empire. At the request of the late Obi-Wan's spirit, Luke travels to the swamp-infested world of Dagobah to find the exiled Jedi Master Yoda and begin his Jedi training. However, Luke's training is interrupted by Vader, who lures him into a trap by capturing Han and Leia at Cloud City, governed by Han's old friend Lando Calrissian. During a fierce lightsaber duel with the Sith Lord, Luke learns that Vader is his father.[35]		After the success of the original film, Lucas hired science fiction author Leigh Brackett to write Star Wars II with him. They held story conferences and, by late November 1977, Lucas had produced a handwritten treatment called The Empire Strikes Back. It was similar to the final film, except that Darth Vader does not reveal he is Luke's father.[citation needed]		Brackett finished her first draft in early 1978; in it, Luke's father appeared as a ghost to instruct Luke.[36] Lucas has said he was disappointed with it, but before he could discuss it with her, she died of cancer.[37] With no writer available, Lucas had to write his next draft himself. It was this draft in which Lucas first made use of the "Episode" numbering for the films; Empire Strikes Back was listed as Episode II.[38] As Michael Kaminski argues in The Secret History of Star Wars, the disappointment with the first draft probably made Lucas consider different directions in which to take the story.[39] He made use of a new plot twist: Darth Vader claims to be Luke's father. According to Lucas, he found this draft enjoyable to write, as opposed to the yearlong struggles writing the first film, and quickly wrote two more drafts,[40] both in April 1978. This new story point of Darth Vader being Luke's father had drastic effects on the series.[41] After writing these two drafts, Lucas revised the backstory between Anakin Skywalker, Kenobi, and the Emperor.[42]		With this new backstory in place, Lucas decided that the series would be a trilogy, changing Empire Strikes Back from Episode II to Episode V in the next draft.[40] Lawrence Kasdan, who had just completed writing Raiders of the Lost Ark, was then hired to write the next drafts, and was given additional input from director Irvin Kershner. Kasdan, Kershner, and producer Gary Kurtz saw the film as a more serious and adult film, which was helped by the new, darker storyline, and developed the series from the light adventure roots of the first film.[43]		A year after Vader's shocking revelation, Luke leads a rescue attempt to save Han from the gangster Jabba the Hutt. Afterward, Luke returns to Dagobah to complete his Jedi training, only to find the 900-year-old Yoda on his deathbed. In his last words Yoda confirms that Vader is Luke's father, Anakin Skywalker, and that Luke must confront his father again in order to complete his training. Moments later, the spirit of Obi-Wan reveals to Luke that Leia is his twin sister, but Obi-Wan also insists that Luke must face Vader again. As the Rebels lead an attack on the Death Star II, Luke engages Vader in another lightsaber duel as Emperor Palpatine watches; both Sith Lords intend to turn Luke to the dark side of the Force and take him as their apprentice.[44]		By the time Lucas began writing Episode VI in 1981 (then titled Revenge of the Jedi), much had changed. Making Empire Strikes Back was stressful and costly, and Lucas' personal life was disintegrating. Burned out and not wanting to make any more Star Wars films, he vowed that he was done with the series in a May 1983 interview with Time magazine. Lucas' 1981 rough drafts had Darth Vader competing with the Emperor for possession of Luke—and in the second script, the "revised rough draft", Vader became a sympathetic character. Lawrence Kasdan was hired to take over once again and, in these final drafts, Vader was explicitly redeemed and finally unmasked. This change in character would provide a springboard to the "Tragedy of Darth Vader" storyline that underlies the prequels.[45]		After losing much of his fortune in a divorce settlement in 1987, George Lucas had no desire to return to Star Wars, and had unofficially canceled the sequel trilogy by the time of Return of the Jedi.[46] At that point, the prequels were only still a series of basic ideas partially pulled from his original drafts of "The Star Wars". Nevertheless, technical advances in the late 1980s and 1990s continued to fascinate Lucas, and he considered that they might make it possible to revisit his 20-year-old material. After Star Wars became popular once again, in the wake of Dark Empire and other comics in Dark Horse's comic book line and Timothy Zahn's trilogy of novels, Lucas saw that there was still a large audience. His children were older, and with the explosion of CGI technology he was now considering returning to directing.[47]		The prequel trilogy consists of Episode I: The Phantom Menace, released on May 19, 1999; Episode II: Attack of the Clones, released on May 16, 2002; and Episode III: Revenge of the Sith, released on May 19, 2005.[48] The plot focuses on the fall of the Galactic Republic, as well as the tragedy of Anakin Skywalker's turn to the dark side.		About 32 years before the start of the Galactic Civil War, the corrupt Trade Federation sets a blockade around the planet Naboo. The Sith Lord Darth Sidious had secretly planned the blockade to give his alter ego, Senator Palpatine, a pretense to overthrow and replace the Supreme Chancellor of the Republic. At the Chancellor's request, the Jedi Knight Qui-Gon Jinn and his apprentice, a younger Obi-Wan Kenobi, are sent to Naboo to negotiate with the Federation. However, the two Jedi are forced to instead help the Queen of Naboo, Padmé Amidala, escape from the blockade and plead her planet's crisis before the Republic Senate on Coruscant. When their starship is damaged during the escape, they land on Tatooine for repairs. Palpatine dispatches his first Sith apprentice, Darth Maul, to hunt down the Queen and her Jedi protectors. While on Tatooine, Qui-Gon discovers a nine-year-old slave named Anakin Skywalker. Qui-Gon helps liberate the boy from slavery, believing Anakin to be the "Chosen One" foretold by a Jedi prophecy to bring balance to the Force. However, the Jedi Council (led by Yoda) suspects the boy possesses too much fear and anger within him.[49]		By 1993, it was announced, in Variety among other sources, that Lucas would be making the prequels. He began penning more to the story, now indicating the series would be a tragic one examining Anakin Skywalker's fall to the dark side. Lucas also began to change how the prequels would exist relative to the originals; at first they were supposed to be a "filling-in" of history tangential to the originals, but now he saw that they could form the beginning of one long story that started with Anakin's childhood and ended with his death. This was the final step towards turning the film series into a "Saga".[50] In 1994, Lucas began writing the screenplay to the first prequel, initially titled Episode I: The Beginning. Following the release of that film, Lucas announced that he would also be directing the next two, and began work on Episode II.[51]		Ten years after the Battle of Naboo, Anakin is reunited with Padmé, now serving as the Senator of Naboo, and they fall in love despite Anakin's obligations to the Jedi Order. At the same time, the entire galaxy gets swept up in the Clone Wars between the armies of the Republic, led by the Jedi Order, and the Confederacy of Independent Systems, led by the fallen Jedi Count Dooku.[52]		The first draft of Episode II was completed just weeks before principal photography, and Lucas hired Jonathan Hales, a writer from The Young Indiana Jones Chronicles, to polish it.[53] Unsure of a title, Lucas had jokingly called the film "Jar Jar's Great Adventure".[54] In writing The Empire Strikes Back, Lucas initially decided that Lando Calrissian was a clone and came from a planet of clones which caused the "Clone Wars" mentioned by both Luke Skywalker and Princess Leia in A New Hope;[55][56] he later came up with an alternate concept of an army of clone shocktroopers from a remote planet which attacked the Republic and were repelled by the Jedi.[57] The basic elements of that backstory became the plot basis for Episode II, with the new wrinkle added that Palpatine secretly orchestrated the crisis.[52]		Three years after the start of the Clone Wars, Anakin and Obi-Wan lead a rescue mission to save the kidnapped Chancellor Palpatine from Count Dooku and the droid commander General Grievous. Later, Anakin begins to have prophetic visions of his secret wife Padmé dying in childbirth. Palpatine, who had been secretly engineering the Clone Wars to destroy the Jedi Order, convinces Anakin that the dark side of the Force holds the power to save Padmé's life. Desperate, Anakin submits to Palpatine's Sith teachings and is renamed Darth Vader. While Palpatine re-organizes the Republic into the tyrannical Empire, Vader participates in the extermination of the Jedi Order; culminating in a lightsaber duel between himself and his former master Obi-Wan on the volcanic planet Mustafar.[58]		Lucas began working on Episode III before Attack of the Clones was released, offering concept artists that the film would open with a montage of seven Clone War battles.[59] As he reviewed the storyline that summer, however, he says he radically re-organized the plot.[60] Michael Kaminski, in The Secret History of Star Wars, offers evidence that issues in Anakin's fall to the dark side prompted Lucas to make massive story changes, first revising the opening sequence to have Palpatine kidnapped and his apprentice, Count Dooku, murdered by Anakin as the first act in the latter's turn towards the dark side.[61] After principal photography was complete in 2003, Lucas made even more massive changes in Anakin's character, re-writing his entire turn to the dark side; he would now turn primarily in a quest to save Padmé's life, rather than the previous version in which that reason was one of several, including that he genuinely believed that the Jedi were evil and plotting to take over the Republic. This fundamental re-write was accomplished both through editing the principal footage, and new and revised scenes filmed during pick-ups in 2004.[62]		On August 15, 2008, the animated film Star Wars: The Clone Wars was released theatrically as a lead-in to the animated TV series with the same name.		Lucas often exaggerated the amount of material he wrote for the series; much of it stemmed from the post‐1978 period when the series grew into a phenomenon. Michael Kaminski explained that these exaggerations were both a publicity and security measure. Kaminski rationalized that since the series' story radically changed throughout the years, it was always Lucas' intention to change the original story retroactively because audiences would only view the material from his perspective.[58][63]		A sequel trilogy was reportedly planned (Episodes VII, VIII and IX) by Lucasfilm as a sequel to the original Star Wars trilogy (Episodes IV, V and VI), released between 1977 and 1983.[64] While the similarly discussed Star Wars prequel trilogy (Episodes I, II and III) was ultimately released between 1999 and 2005, Lucasfilm and George Lucas had for many years denied plans for a sequel trilogy, insisting that Star Wars is meant to be a six-part series.[65][66] In May 2008 (2008-05), speaking about the upcoming Star Wars: The Clone Wars film, Lucas maintained his status on the sequel trilogy: "I get asked all the time, 'What happens after Return of the Jedi?,' and there really is no answer for that. The movies were the story of Anakin Skywalker and Luke Skywalker, and when Luke saves the galaxy and redeems his father, that's where that story ends."[67]		In January 2012, Lucas announced that he would step away from blockbuster films and instead produce smaller arthouse films. Asked whether the criticism he received following the prequel trilogy and the alterations to the re-releases of the original trilogy had influenced his decision to retire, Lucas said: "Why would I make any more when everybody yells at you all the time and says what a terrible person you are?"[68]		Despite insisting that a sequel trilogy would never happen, Lucas began working on story treatments for three new Star Wars films in 2011. In October 2012, The Walt Disney Company agreed to buy Lucasfilm and announced that Star Wars Episode VII would be released in 2015. Later, it was revealed that the three new upcoming films (Episodes VII–IX) would be based on story treatments that had been written by George Lucas prior to the sale of Lucasfilm.[69] The co-chairman of Lucasfilm, Kathleen Kennedy became president of the company, reporting to Walt Disney Studios chairman Alan Horn. In addition, Kennedy will serve as executive producer on new Star Wars feature films, with franchise creator and Lucasfilm founder Lucas serving as creative consultant.[70]		The sequel trilogy began with Episode VII: The Force Awakens, released on December 18, 2015.		About 30 years after the destruction of the Death Star II, Luke Skywalker has vanished following the demise of the new Jedi Order he was attempting to build. The remnants of the Empire have become the First Order, and seek to destroy Luke and the New Republic, while the Resistance opposes, led by princess-turned-general Leia Organa and backed by the Republic. On Jakku, Resistance pilot Poe Dameron obtains a map to Luke's location. Stormtroopers under the command of Kylo Ren, the son of Leia and Han Solo, capture Poe. Poe's droid BB-8 escapes with the map, and encounters a scavenger Rey. Kylo tortures Poe and learns of BB-8. Stormtrooper FN-2187 defects from the First Order, and frees Poe who dubs him "Finn", while both escape in a TIE fighter that crashes on Jakku, seemingly killing Poe. Finn finds Rey and BB-8, but the First Order does too; both escape Jakku in a stolen Millennium Falcon. The Falcon is recaptured by Han and Chewbacca, smugglers again since abandoning the Resistance. They agree to help deliver the map inside BB-8 to the Resistance.		The screenplay for Episode VII was originally set to be written by Michael Arndt, but in October 2013 it was announced that writing duties would be taken over by Lawrence Kasdan and J. J. Abrams.[71][72] On January 25, 2013, The Walt Disney Studios and Lucasfilm officially announced J. J. Abrams as Star Wars Episode VII's director and producer, along with Bryan Burk and Bad Robot Productions.[73]		On November 20, 2012, The Hollywood Reporter reported that Lawrence Kasdan and Simon Kinberg will write and produce Episodes VIII and IX.[74] Kasdan and Kinberg were later confirmed as creative consultants on those films, in addition to writing standalone films. In addition, John Williams, who wrote the music for the previous six episodes, has been hired to compose the music for Episodes VII, VIII and IX.[75]		On March 12, 2015, Lucasfilm announced that Looper director Rian Johnson would direct Episode VIII with Ram Bergman as producer for Ram Bergman Productions.[76] Reports initially claimed Johnson would also direct Episode IX, but it was later confirmed he would write only a story treatment.[77][78] Johnson later wrote on his Twitter that the information about him writing a treatment for Episode IX is old, and he's not involved with the writing of that film.[79] When asked about Episode VIII in an August 2014 interview, Johnson said "it's boring to talk about, because the only thing I can really say is, I'm just happy. I don't have the terror I kind of expected I would, at least not yet. I'm sure I will at some point."[80]		Principal photography on The Last Jedi began in February 2016.[81] Additional filming took place in Dubrovnik from March 9 to March 16, 2016,[82][83] as well as in Ireland in May 2016.[84] Principal photography wrapped in July 2016.[85][86][87] On December 27, 2016, Carrie Fisher died after going into cardiac arrest a few days earlier. Before her death, Fisher had completed filming her role as General Leia Organa in The Last Jedi.[88] The film is to be released on December 15, 2017.[89]		Production on Episode IX is scheduled to begin sometime in 2017.[90] Variety and Reuters reported that Carrie Fisher was slated for a key role in Episode IX.[91] Now, Lucasfilm, Disney and others involved with the film will need to find a way to address her death and what will become of her character.[92][93][94] In January 2017, Lucasfilm stated they would not digitally generate Fisher's performance for the film.[95] In April 2017, Todd Fisher and Billie Lourd gave Disney permission to use recent footage of Fisher for the film,[96] but later that month, Kennedy stated that Fisher will not appear in the film.[97][98] Principal photography of Star Wars: Episode IX is set to begin in January 2018.[99][100]		On February 5, 2013, Disney CEO Bob Iger confirmed the development of two standalone films, each individually written by Lawrence Kasdan and Simon Kinberg.[101] On February 6, Entertainment Weekly reported that Disney is working on two films featuring Han Solo and Boba Fett.[102] Disney CFO Jay Rasulo has described the standalone films as origin stories.[103] Kathleen Kennedy explained that the standalone films will not crossover with the films of the sequel trilogy, stating, "George was so clear as to how that works. The canon that he created was the Star Wars saga. Right now, Episode VII falls within that canon. The spin-off movies, or we may come up with some other way to call those films, they exist within that vast universe that he created. There is no attempt being made to carry characters (from the standalone films) in and out of the saga episodes. Consequently, from the creative standpoint, it's a roadmap that George made pretty clear."[104]		In April 2015, Lucasfilm and Kennedy announced that the standalone films would be referred to as the Star Wars Anthology films.[105][106][107] Rogue One: A Star Wars Story was released on December 16, 2016 as the first in an anthology series of films separate from the main episodic saga.		The story about the group of rebels who stole the Death Star plans, ending directly before Episode IV: A New Hope.		The idea for the film was conceived by John Knoll who worked as a visual effects supervisor of the prequel trilogy films.[108] In May 2014, Lucasfilm announced Gareth Edwards as the director of the first anthology film, with Gary Whitta writing the first draft, for a release on December 16, 2016.[109] On March 12, 2015, the film's title was revealed to be Rogue One, with Chris Weitz rewriting the script, and starring Felicity Jones, Ben Mendelsohn, and Diego Luna.[110][111] In April 2015, a teaser trailer was shown during the closing of the Star Wars Celebration. Lucasfilm also announced filming would begin in the summer of 2015, and the plot synopsis. Director Edwards stated, "It comes down to a group of individuals who don't have magical powers that have to somehow bring hope to the galaxy."; and describing the style of the film as similar to that of a war film: "It's the reality of war. Good guys are bad. Bad guys are good. It's complicated, layered; a very rich scenario in which to set a movie."[112][113] After its debut, Rogue One received generally positive reviews, with its performances, action sequences, soundtrack, visual effects and darker tone being praised. The film grossed over US$500 million worldwide within a week of its release.[114] Characters from the animated series appear, Saw Gerrera (from The Clone Wars) in a pivotal role in the plot and Chopper (from Star Wars: Rebels) in a cameo.[115]		A film focusing on Han Solo before the events of Episode IV: A New Hope.		The script was written by Star Wars veteran Lawrence Kasdan, and his son Jon Kasdan. The film also stars Alden Ehrenreich as a young Han Solo, Joonas Suotamo as Chewbacca (after serving as a double for the character in The Force Awakens and The Last Jedi), Donald Glover as Lando Calrissian, and also Emilia Clarke and Woody Harrelson. Directors Phil Lord and Christopher Miller began principal photography on the film, but due to creative differences, the pair left the project in June 2017 with three and a half weeks remaining in principal photography. Academy Award-winning director Ron Howard was announced as their replacement. While his first Star Wars film, Howard had previously collaborated with producing company Lucasfilm as an actor in the George Lucas-directed film American Graffiti (1973) and as a director (Willow (1988).[116] Howard was one of the three directors George Lucas asked to direct Episode I: The Phantom Menace, though Howard declined, saying, "George, you should do it!").[117] The film is distributed by Walt Disney Studios Motion Pictures and will be released on May 25, 2018.		A third Anthology film will be released in 2020.[118] A writer for the film has been hired as of September 2016.[119]		On February 6, 2013, Entertainment Weekly reported that Lucasfilm hired Josh Trank to direct a Star Wars stand-alone film, with the news being confirmed soon after.[120] However in November 2016, Disney announced that their contract with Trank was terminated due to the overwhelmingly negative reviews of the Fantastic Four.[121] It was reported that the film was still in early development at Lucasfilm, and it was rumored that the film would focus on bounty hunter Boba Fett. However Lucasfilm never confirmed what the plot was about, however it confirmed the film Josh Trank left, was a different film from the 'Han Solo spin-off.[122]		On August 18, 2016, Ewan McGregor told a fan on Twitter, that he would be open to return to the role of Obi-Wan, albeit for an Obi-Wan spin-off film, should he be approached; wanting to tell a story between Episode III and IV.[123] Fans have shown interest in the idea; a fan-trailer for an Obi-Wan film, with footage from the film Last Days in the Desert became viral and widely praised by fans.[124] Lucasfilm and McGregor have denied the development of such film, despite fans' continued interest, and rumors. The film was voted as the most wanted anthology film in a pool by The Hollywood Reporter despite being only rumors.[125] Days before the Star Wars Rebels episode "Twin Suns" (where Obi-wan appeared) aired, McGregor said again, that he would like to do it, if Lucasfilm wanted him to.[126] Fellow cast member Joel Edgerton who played Luke Skywalker's step uncle, Owen, in the prequel trilogy, said he would like to reprise his role in an Obi-Wan standalone film, if it were to be made, and also expressed interest in playing Boba Fett without taking off the mask, in the rumored Boba Fett film.[127] Samuel L. Jackson has expressed interest in returning as Mace Windu, insisting that his character survived his death.[128] Rosario Dawson expressed interest playing Ahsoka Tano in a live-action film.[129]		The defunct term Star Wars Expanded Universe (1977–2014, abbreviated as EU), was an umbrella term for all officially licensed Star Wars storytelling materials set outside the events depicted within the theatrical films, including television specials, animated series, video games, novels and comics. Lucasfilm maintained internal continuity between the films and the EU material until Lucasfilm declared in April 25, 2014, that the EU would be rebranded as Star Wars Legends, no longer be considered canon to the franchise, and cease production (reprints under the Legends label are non-canon,[130] Downloadable content for the massively multiplayer online game Star Wars: The Old Republic is the only Legends material still in production. The change was made "to give maximum creative freedom to the filmmakers and also preserve an element of surprise and discovery for the audience" on the development of the sequel trilogy and anthology films.[130] The restructured Star Wars canon only included the original six feature films, the animated film Star Wars: The Clone Wars, and the ensuing Star Wars: The Clone Wars animated series. All creative developments across all types of media would be overseen and coordinated by the Lucasfilm Story Group. The animated series Star Wars Rebels was the first project released in the revised canon, followed by multiple comics series from Marvel, novels published by Del Rey, and the sequel film The Force Awakens.		The popularity of the 1977 film spawned a two-hour Star Wars Holiday Special which aired on CBS in 1978, chronicling Chewbacca's return to his home planet of Kashyyyk to celebrate "Life Day" with his family. Lucas loathed the special and forbade it to ever be aired again after its original broadcast, or reproduced on home video.[132][133] A pair of television films centered on the Ewok Wicket from Return of the Jedi—Caravan of Courage: An Ewok Adventure (1984)[134] and Ewoks: The Battle for Endor (1985)[135]—were followed by the animated TV series Star Wars: Droids (1985–1986)[136][137] and Star Wars: Ewoks (1985–1986).[138] The animated micro-series Star Wars: Clone Wars (2003–2005) debuted after the release of Attack of the Clones and ended before the release of Revenge of the Sith, and depicts events occurring between both films.[139] All of those programs were declared non-canon, in April 2014.		The next animated series all are considered canonical, to the franchise, and were CGI-animated. Star Wars: The Clone Wars (2008–2014) premiered with an animated film of the same name and was set within the same time period as the previous series.[140][141] Star Wars Rebels (2014–present) is set between Revenge of the Sith and A New Hope, and follows a band of rebels as they fight the Galactic Empire.[142][143] The 2D-animated micro-series Star Wars Forces of Destiny (2017–present) focuses on female characters including Princess Leia, Rey, Jyn Erso, Ahsoka Tano, and Padmé Amidala. The first eight episodes will premiere on YouTube in July 2017, with eight more episodes airing in Fall 2017.[144]		A live-action television project has been in varying stages of development at Lucasfilm since 2005, when Lucas announced plans for a television series set between the prequel and original trilogies.[145] The proposed series explores criminal and political power struggles in the decades prior to A New Hope,[145] and as of December 2015 was still in development at Lucasfilm.[146]		The Star Wars website produces web shows featuring behind the scenes interviews with directors, producers, cast-members, and writers involved with the franchise.[147][148]		Star Wars-based fiction predates the release of the first film, with the 1976 novelization of Star Wars (ghost-written by Alan Dean Foster and credited to Lucas). The first Expanded Universe story appeared in Marvel Comics' Star Wars #7 in January 1978 (the first six issues of the series having been an adaptation of the film), followed quickly by Alan Dean Foster's novel Splinter of the Mind's Eye the following month.[149]		Foster's 1978 novel, Splinter of the Mind's Eye, was the first EU novel to be released, set between the original 1977 film and The Empire Strikes Back. The EU novels additional content greatly expanded the Star Wars timeline before and after the film series. Star Wars EU fiction flourished during the time of the original trilogy (1977–1983) but slowed to a trickle until the 1990s, when Timothy Zahn's Thrawn trilogy (1992-1994) sparked new interest in the franchise. Followed by Shadows of the Empire (1996) a multimedia project consisting of a novel by Steve Perry, a video game, and a comic book; set between The Empire Strikes Back and Return of the Jedi.[150] Since then, several hundred tie-in novels have been published by Bantam and Del Rey. The events 20 years after Return of the Jedi were explored in New Jedi Order series, focusing on EU characters alongside series originals. For younger audiences, three other series, the Jedi Apprentice told adventures of Obi-Wan Kenobi and his master Qui-Gon Jinn in the years before The Phantom Menace. While The Jedi Quest followed Obi-Wan as the master of Anakin Skywalker in between The Phantom Menace and Attack of the Clones. The Last of the Jedi was about Obi-Wan and the last few surviving Jedi set almost immediately after Revenge of the Sith. Following Disney's purchase of the Star Wars franchise, the EU was rebranded as Star Wars Legends and declared non-canonical. The Legends banner is used for non-canon materials in re-print.[151]		Disney Publishing Worldwide also announced that from September 2014, onwards Del Rey would publish a canonical line of Star Wars books, guided by the Lucasfilm Story Group, and releasing on a bi-monthly schedule.[152]		George Lucas canonized elements from the novels in the films, such as the name of capital planet Coruscant, in The Phantom Menace, while Dave Filoni canonized the villain character of Grand Admiral Thrawn in Star Wars Rebels, both first appeared in the EU, in Thrawn Trilogy of novels by Timothy Zahn.		Marvel Comics published Star Wars comic book series and adaptations from 1977 to 1986. A wide variety of creators worked on this series, including Roy Thomas, Archie Goodwin, Howard Chaykin, Al Williamson, Carmine Infantino, Gene Day, Walt Simonson, Michael Golden, Chris Claremont, Whilce Portacio, Jo Duffy, and Ron Frenz. The Los Angeles Times Syndicate published a Star Wars newspaper strip by Russ Manning, Goodwin and Williamson[153][154] with Goodwin writing under a pseudonym. In the late 1980s, Marvel announced it would publish a new Star Wars comic by Tom Veitch and Cam Kennedy. However, in December 1991, Dark Horse Comics acquired the Star Wars license, and used it to publish a large number of original adventures set in the Star Wars universe, varying from ambitious sequels to the original trilogy like the popular Dark Empire,[155] to parody comics, like Tag and Bink.[156]		On January 3, 2014, all previously published works were re-branded as Legends,[157], Dark Horse publishing rights to Star Wars were transferred to Marvel —since 2009, also a Disney subsidiary—, to publish a series that would be considered canonical to the franchise, the first release arrived on January 14, 2015. In 2017, IDW acquired rights to publish Star Wars Adventures a single comic series for all ages, published in parallel to Marvel publications.[158]		George Lucas adopted elements from the comics in the films, such as the character Aayla Secura, who was introduced in Dark Horse Comics' Star Wars series, Lucas liked the character so much, that he included her in Attack of the Clones.[159]		Star Wars videogames commercialization started in 1982 with Star Wars: The Empire Strikes Back published for the Atari 2600 by Parker Brothers. Since then, Star Wars has opened the way to a myriad of space-flight simulation games, first-person shooter games, role-playing video games, RTS games, and others released on all consoles including PS3, PSP, PS2, Xbox 360, Nintendo DS, Wii, and the current Xbox One and PlayStation 4. The mobile game Star Wars: Uprising, as well as the upcoming Star Wars: Battlefront II to be released for Xbox One and PlayStation 4, are the only canonical videogames to the franchise.		The most critically acclaimed is the first game in the Knights of the Old Republic series,[160] which focuses on the Jedi Order 4000 years before the films. Star Wars: The Force Unleashed and its sequel focused on Darth Vader's "secret apprentice" hunting down the remaining Jedi, 3 years before A New Hope, also proved popular among fans, they released[161][162] for the PS3, Xbox 360, with ports for the other consoles.		The best-selling games so far are the Lego Star Wars and the Battlefront series, with 12 million and 10 million units respectively[163][164] both have recent installments. The Battlefront series focuses on recreating the wars depicted on the films, while The Lego Star Wars series recreates: The Original 6 films, The Clone Wars, and The Force Awakens on LEGO.		A 13-episode radio adaptation of the original 1977 film written by science fiction author Brian Daley and directed by John Madden was first broadcast on National Public Radio in 1981.[165][166][167] The broadcast was an overwhelming success, and a 10-episode adaptation of The Empire Strikes Back followed in 1982. Return of the Jedi was adapted into six episodes in 1996.[166][168] In 1983, Buena Vista Records released an original, 30-minute Star Wars audio drama titled Rebel Mission to Ord Mantell, written by Daley.[167][168] In the 1990s, Time Warner Audio Publishing adapted several Star Wars series from Dark Horse Comics into audio dramas.[168]		Before Disney's acquisition of the franchise, George Lucas had established a partnership in 1986 with the company's Imagineering division to create an attraction at Disney parks. The first such attraction, Star Tours, opened at Disneyland in 1987, with several versions opening at other Disney theme parks over the following years.[169][170] The Star Tours rides at Disneyland and Disney's Hollywood Studios closed in 2010, while Tokyo Disneyland's version closed in 2012 and Disneyland Paris' in 2016. All of the original Star Tours rides were then refurbished into Star Tours–The Adventures Continue. The new attraction randomly shuffles several scenes, allowing up to 54 combinations of different adventures. The successor attraction opened at Disney's Hollywood Studios and Disneyland in 2011, at Tokyo Disneyland in 2013, and at Disneyland Paris in 2017.[171]		Disney, which now owns the Star Wars franchise, expressed plans to expand the franchise's presence in all of their theme parks since August 2014. At that time, there were rumors to include a major Star Wars-themed expansion to Disney's Hollywood Studios.[172] When asked whether or not Disney has an intellectual property franchise that's comparable to Harry Potter at Universal theme parks, Disney chairman and CEO Bob Iger mentioned Cars, Disney Princesses, and promised that Star Wars, "is going to be just that."[173] Iger formally announced a 14-acre Star Wars-themed land expansion at the 2015 D23 Expo. In the 2017 D23, it was revealed that the area would be named Star Wars: Galaxy's Edge.[174] The area will debut at Disneyland and Disney's Hollywood Studios at an unspecified date—will include two new attractions inspired by the Millennium Falcon and "a climactic battle between the First Order and the resistance".[175] A Star Wars-themed hotel, a deluxe resort to be built near Disney's Hollywood Studios, was also announced at the 2017 D23 Expo.[176][177][178][174]		From 1997 to 2015, Disney's Hollywood Studios park hosted Star Wars Weekends, an annual festival, during specific dates from May to June.[179][180] Since 2007, the parks include the live show Jedi Training: Trials of the Temple, where children are selected to learn the teachings of the Jedi Knights and the Force to become Padawan learners; the show is present at Disney's Hollywood Studios and at the Tomorrowland Terrace at Disneyland.[181][182] Since November 2015, Disneyland hosts a seasonal Star Wars-themed event entitled Season of the Force, which also runs in Disney's Hollywood Studios in Walt Disney World. An exhibition called Star Wars Launch Bay, featuring exhibits and meet-and-greets was also added.[183][184][185]		The success of the Star Wars films led the franchise to become one of the most merchandised franchises in the world. In 1977, while filming the original film, George Lucas decided to take a 500,000-dollar pay-cut to his own salary as director, in exchange for fully owning the merchandising rights of the franchise to himself. Over the franchise's lifetime, such exchange cost 20th Century Fox, more than US$20 billion in merchandising revenue profits.[186] Disney acquired the merchandising rights when part of purchasing Lucasfilm.		Kenner made the first Star Wars action figures to coincide with the release of the film, and today the remaining 80's figures sell at extremely high prices in auctions. Since the 90's Hasbro holds the rights to create action figures based on the saga. Pez dispensers have been produced.[citation needed] Star Wars was the first intellectual property to be licensed in Lego Group history, which has produced a Star Wars Lego theme.[187] Lego has produced animated parody short films to promote their sets, among them Revenge of the Brick (2005) and The Quest for R2-D2 (2009), the former parodies Revenge of the Sith, while the later The Clone Wars film. Due to their success, LEGO created animated comedy mini-series among them The Yoda Chronicles (2013-2014) and Droid Tales (2015) originally airing on Cartoon Network, but since 2014 moved into Disney XD.[188] The Lego Star Wars video games are critically acclaimed best sellers.[citation needed]		In 1977 with the board game Star Wars: Escape from the Death Star[189] (not to be confused with another board game with the same title, published in 1990).[190] The board game Risk has been adapted to the series in two editions by Hasbro: and Star Wars Risk: The Clone Wars Edition[191] (2005) and Risk: Star Wars Original Trilogy Edition[192] (2006).		Three different official tabletop role-playing games have been developed for the Star Wars universe: a version by West End Games in the 1980s and 1990s, one by Wizards of the Coast in the 2000s, and one by Fantasy Flight Games in the 2010s.		Star Wars trading cards have been published since the first "blue" series, by Topps, in 1977.[193] Dozens of series have been produced, with Topps being the licensed creator in the United States. Some of the card series are of film stills, while others are original art. Many of the cards have become highly collectible with some very rare "promos", such as the 1993 Galaxy Series II "floating Yoda" P3 card often commanding US$1 000 or more. While most "base" or "common card" sets are plentiful, many "insert" or "chase cards" are very rare.[194] From 1995 until 2001, Decipher, Inc. had the license for, created and produced a collectible card game based on Star Wars; the Star Wars Collectible Card Game (also known as SWCCG).		Aside from its well-known science fictional technology, Star Wars features elements such as knighthood, chivalry, and princesses that are related to archetypes of the fantasy genre.[195] The Star Wars world, unlike fantasy and science-fiction films that featured sleek and futuristic settings, was portrayed as dirty and grimy. Lucas' vision of a "used future" was further popularized in the science fiction-horror films Alien,[196] which was set on a dirty space freighter; Mad Max 2, which is set in a post-apocalyptic desert; and Blade Runner, which is set in a crumbling, dirty city of the future. Lucas made a conscious effort to parallel scenes and dialogue between films, and especially to parallel the journeys of Luke Skywalker with that of his father Anakin when making the prequels.[49]		Star Wars contains many themes of political science that mainly favor democracy over dictatorship. Political science has been an important element of Star Wars since the franchise first launched in 1977. The plot climax of Revenge of the Sith is modeled after the fall of the democratic Roman Republic and the formation of an empire.[197][198][199]		The stormtroopers from the movies share a name with the Nazi stormtroopers (see also Sturmabteilung). Imperial officers' uniforms also resemble some historical German Army uniforms (see Wehrmacht) and the political and security officers of the Empire resemble the black clad SS down to the imitation silver death's head insignia on their officer's caps. World War II terms were used for names in Star Wars; examples include the planets Kessel (a term that refers to a group of encircled forces), Hoth (Hermann Hoth was a German general who served on the snow laden Eastern Front), and Tatooine (Tataouine - a province south of Tunis in Tunisia, roughly where Lucas filmed for the planet; Libya was a WWII arena of war).[200] Palpatine being Chancellor before becoming Emperor mirrors Adolf Hitler's role as Chancellor before appointing himself Dictator. The Great Jedi Purge alludes to the events of The Holocaust, the Great Purge, the Cultural Revolution, and the Night of the Long Knives. In addition, Lucas himself has drawn parallels between Palpatine and his rise to power to historical dictators such as Julius Caesar, Napoleon Bonaparte, and Adolf Hitler. The final medal awarding scene in A New Hope, however, references Leni Riefenstahl's Triumph of the Will.[201] The space battles in A New Hope were based on filmed World War I and World War II dogfights.[202]		Continuing the use of Nazi inspiration for the Empire, J. J. Abrams, the director of Star Wars: The Force Awakens, has said that the First Order, an Imperial offshoot which will possibly serve as the main antagonist of the sequel trilogy, is also inspired by another aspect of the Nazi regime. Abrams spoke of how several Nazis fled to Argentina after the war and he claims that the concept for the First Order came from conversations between the scriptwriters about what would have happened if they had started working together again.[203]		The Star Wars saga has had a significant impact on modern popular culture.[204] Star Wars references are deeply embedded in popular culture;[205] Phrases like "evil empire" and "May the Force be with you" have become part of the popular lexicon.[206] The first Star Wars film in 1977 was a cultural unifier,[207] enjoyed by a wide spectrum of people.[208] The film can be said to have helped launch the science fiction boom of the late 1970s and early 1980s, making science fiction films a blockbuster genre or mainstream.[209] This very impact also made it a prime target for parody works and homages, with popular examples including Spaceballs, Family Guy's Laugh It Up, Fuzzball, Robot Chicken's "Star Wars Episode I", "Star Wars Episode II" and "Star Wars Episode III", and Hardware Wars by Ernie Fosselius.		In 1989, the Library of Congress selected the original Star Wars film for preservation in the U.S. National Film Registry, as being "culturally, historically, or aesthetically significant."[210] Its sequel, The Empire Strikes Back, was selected in 2010.[211][212] Despite these callings for archival, it is unclear whether copies of the 1977 and 1980 theatrical sequences of Star Wars and Empire—or copies of the 1997 Special Edition versions—have been archived by the NFR, or indeed if any copy has been provided by Lucasfilm and accepted by the Registry.[213][214]		The Star Wars saga has inspired many fans to create their own non-canon material set in the Star Wars galaxy. In recent years, this has ranged from writing fan-fiction to creating fan films. In 2002, Lucasfilm sponsored the first annual Official Star Wars Fan Film Awards, officially recognizing filmmakers and the genre. Because of concerns over potential copyright and trademark issues, however, the contest was initially open only to parodies, mockumentaries, and documentaries. Fan-fiction films set in the Star Wars universe were originally ineligible, but in 2007 Lucasfilm changed the submission standards to allow in-universe fiction entries.[215] Lucasfilm, for the most part, has allowed but not endorsed the creation of these derivative fan-fiction works, so long as no such work attempts to make a profit from or tarnish the Star Wars franchise in any way.[216] While many fan films have used elements from the licensed Expanded Universe to tell their story, they are not considered an official part of the Star Wars canon.		
Risk is the potential of gaining or losing something of value.[1] Values (such as physical health, social status, emotional well-being, or financial wealth) can be gained or lost when taking risk resulting from a given action or inaction, foreseen or unforeseen. Risk can also be defined as the intentional interaction with uncertainty.[2] Uncertainty is a potential, unpredictable, and uncontrollable outcome; risk is a consequence of action taken in spite of uncertainty.[3]		Risk perception is the subjective judgment people make about the severity and probability of a risk, and may vary person to person. Any human endeavor carries some risk, but some are much riskier than others.[4]						The Oxford English Dictionary cites the earliest use of the word in English (in the spelling of risque from its from French original, 'risque' ) as of 1621, and the spelling as risk from 1655. It defines risk as:		(Exposure to) the possibility of loss, injury, or other adverse or unwelcome circumstance; a chance or situation involving such a possibility.[5]		The International Organization for Standardization publication ISO 31000 (2009) / ISO Guide 73:2002 definition of risk is the 'effect of uncertainty on objectives'. In this definition, uncertainties include events (which may or may not happen) and uncertainties caused by ambiguity or a lack of information. It also includes both negative and positive impacts on objectives. Many definitions of risk exist in common usage, however this definition was developed by an international committee representing over 30 countries and is based on the input of several thousand subject matter experts.		Very different approaches to risk management are taken in different fields, e.g. "Risk is the unwanted subset of a set of uncertain outcomes" (Cornelius Keating).		Risk is ubiquitous in all areas of life and risk management is something that we all must do, whether we are managing a major organization or simply crossing the road. When describing risk however, it is convenient to consider that risk practitioners operate in some specific practice areas.		Economic risks can be manifested in lower incomes or higher expenditures than expected. The causes can be many, for instance, the hike in the price for raw materials, the lapsing of deadlines for construction of a new operating facility, disruptions in a production process, emergence of a serious competitor on the market, the loss of key personnel, the change of a political regime, or natural disasters.		Risks in personal health may be reduced by primary prevention actions that decrease early causes of illness or by secondary prevention actions after a person has clearly measured clinical signs or symptoms recognized as risk factors. Tertiary prevention reduces the negative impact of an already established disease by restoring function and reducing disease-related complications. Ethical medical practice requires careful discussion of risk factors with individual patients to obtain informed consent for secondary and tertiary prevention efforts, whereas public health efforts in primary prevention require education of the entire population at risk. In each case, careful communication about risk factors, likely outcomes and certainty must distinguish between causal events that must be decreased and associated events that may be merely consequences rather than causes.		In epidemiology, the lifetime risk of an effect is the cumulative incidence, also called incidence proportion over an entire lifetime.[11]		In terms of occupational health & safety management, the term 'risk' may be defined as the most likely consequence of a hazard, combined with the likelihood or probability of it occurring.		Health, safety, and environment (HSE) are separate practice areas; however, they are often linked. The reason for this is typically to do with organizational management structures; however, there are strong links among these disciplines. One of the strongest links between these is that a single risk event may have impacts in all three areas, albeit over differing timescales. For example, the uncontrolled release of radiation or a toxic chemical may have immediate short-term safety consequences, more protracted health impacts, and much longer-term environmental impacts. Events such as Chernobyl, for example, caused immediate deaths, and in the longer term, deaths from cancers, and left a lasting environmental impact leading to birth defects, impacts on wildlife, etc.		Over time, a form of risk analysis called environmental risk analysis has developed. Environmental risk analysis is a field of study that attempts to understand events and activities that bring risk to human health or the environment.[12]		Human health and environmental risk is the likelihood of an adverse outcome (See adverse outcome pathway). As such, risk is a function of hazard and exposure. Hazard is the intrinsic danger or harm that is posed, e.g. the toxicity of a chemical compound. Exposure is the likely contact with that hazard. Therefore, the risk of even a very hazardous substance approaches zero as the exposure nears zero, given a person's (or other organism's) biological makeup, activities and location (See exposome).[13] Another example of health risks are when certain behaviors, such as risky sexual behaviors, increase the likelihood of contracting HIV.[14]		Individual risk perception and risk taking can also be influenced by social factors. A study using representative household data in the US, Italy and Austria finds evidence that risk taking levels can be influenced by the immediate social environment and by the welfare regime of a state (i.e. different support networks). The study also finds that these factors can interact.[15]		Information technology risk, or IT risk, IT-related risk, is a risk related to information technology. This relatively new term was developed as a result of an increasing awareness that information security is simply one facet of a multitude of risks that are relevant to IT and the real world processes it supports.		The increasing dependencies of modern society on information and computers networks (both in private and public sectors, including military)[16][17][18] has led to new terms like IT risk and Cyberwarfare.		Information security means protecting information and information systems from unauthorized access, use, disclosure, disruption, modification, perusal, inspection, recording or destruction.[19] Information security grew out of practices and procedures of computer security. Information security has grown to information assurance (IA) i.e. is the practice of managing risks related to the use, processing, storage, and transmission of information or data and the systems and processes used for those purposes. While focused dominantly on information in digital form, the full range of IA encompasses not only digital but also analog or physical form. Information assurance is interdisciplinary and draws from multiple fields, including accounting, fraud examination, forensic science, management science, systems engineering, security engineering, and criminology, in addition to computer science.		So, IT risk is narrowly focused on computer security, while information security extends to risks related to other forms of information (paper, microfilm). Information assurance risks include the ones related to the consistency of the business information stored in IT systems and the information stored by other means and the relevant business consequences.		Insurance is a risk treatment option which involves risk sharing. It can be considered as a form of contingent capital and is akin to purchasing an option in which the buyer pays a small premium to be protected from a potential large loss.		Insurance risk is often taken by insurance companies, who then bear a pool of risks including market risk, credit risk, operational risk, interest rate risk, mortality risk, longevity risks, etc.[20]		Means of assessing risk vary widely between professions. Indeed, they may define these professions; for example, a doctor manages medical risk, while a civil engineer manages risk of structural failure. A professional code of ethics is usually focused on risk assessment and mitigation (by the professional on behalf of client, public, society or life in general).		In the workplace, incidental and inherent risks exist. Incidental risks are those that occur naturally in the business but are not part of the core of the business. Inherent risks have a negative effect on the operating profit of the business.		The experience of many people who rely on human services for support is that 'risk' is often used as a reason to prevent them from gaining further independence or fully accessing the community, and that these services are often unnecessarily risk averse.[21] "People's autonomy used to be compromised by institution walls, now it's too often our risk management practices", according to John O'Brien.[22] Michael Fischer and Ewan Ferlie (2013) find that contradictions between formal risk controls and the role of subjective factors in human services (such as the role of emotions and ideology) can undermine service values, so producing tensions and even intractable and 'heated' conflict.[23]		A high reliability organization (HRO) is an organization that has succeeded in avoiding catastrophes in an environment where normal accidents can be expected due to risk factors and complexity. Most studies of HROs involve areas such as nuclear aircraft carriers, air traffic control, aerospace and nuclear power stations. Organizations such as these share in common the ability to consistently operate safely in complex, interconnected environments where a single failure in one component could lead to catastrophe. Essentially, they are organizations which appear to operate 'in spite' of an enormous range of risks.		Some of these industries manage risk in a highly quantified and enumerated way. These include the nuclear power and aircraft industries, where the possible failure of a complex series of engineered systems could result in highly undesirable outcomes. The usual measure of risk for a class of events is then: R = probability of the event × the severity of the consequence.		The total risk is then the sum of the individual class-risks; see below.[citation needed]		In the nuclear industry, consequence is often measured in terms of off-site radiological release, and this is often banded into five or six decade-wide bands.[clarification needed]		The risks are evaluated using fault tree/event tree techniques (see safety engineering). Where these risks are low, they are normally considered to be "broadly acceptable". A higher level of risk (typically up to 10 to 100 times what is considered broadly acceptable) has to be justified against the costs of reducing it further and the possible benefits that make it tolerable—these risks are described as "Tolerable if ALARP". Risks beyond this level are classified as "intolerable".		The level of risk deemed broadly acceptable has been considered by regulatory bodies in various countries—an early attempt by UK government regulator and academic F. R. Farmer used the example of hill-walking and similar activities, which have definable risks that people appear to find acceptable. This resulted in the so-called Farmer Curve of acceptable probability of an event versus its consequence.		The technique as a whole is usually referred to as probabilistic risk assessment (PRA) (or probabilistic safety assessment, PSA). See WASH-1400 for an example of this approach.		In finance, risk is the chance that the return achieved on an investment will be different from that expected, and also takes into account the size of the difference. This includes the possibility of losing some or all of the original investment. In a view advocated by Damodaran, risk includes not only "downside risk" but also "upside risk" (returns that exceed expectations).[24] Some regard the standard deviation of the historical returns or average returns of a specific investment as providing some historical measure of risk; see modern portfolio theory. Financial risk may be market-dependent, determined by numerous market factors, or operational, resulting from fraudulent behavior (e.g. Bernard Madoff). Recent studies suggest that endocrine levels may play a role in risk-taking in financial decision-making.[25][26]		A fundamental idea in finance is the relationship between risk and return (see modern portfolio theory). The greater the potential return one might seek, the greater the risk that one generally assumes. A free market reflects this principle in the pricing of an instrument: strong demand for a safer instrument drives its price higher (and its return correspondingly lower) while weak demand for a riskier instrument drives its price lower (and its potential return thereby higher). For example, a US Treasury bond is considered to be one of the safest investments. In comparison to an investment or speculative grade corporate bond, US Treasury notes and bonds yield lower rates of return. The reason for this is that a corporation is more likely to default on debt than the U.S. government. Because the risk of investing in a corporate bond is higher, investors are offered a correspondingly higher rate of return.		A popular risk measure is value-at-risk (VaR).		There are different types of VaR: long term VaR, marginal VaR, factor VaR and shock VaR. The latter is used in measuring risk during the extreme market stress conditions.		In finance, risk has no single definition.		Artzner et al.[27] write "we call risk the investor's future net worth". In Novak [28] "risk is a possibility of an undesirable event".		In financial markets, one may need to measure credit risk, information timing and source risk, probability model risk, operational risk and legal risk if there are regulatory or civil actions taken as a result of "investor's regret".		With the advent of automation in financial markets, the concept of "real-time risk" has gained a lot of attention. Aldridge and Krawciw[29] define real-time risk as the probability of instantaneous or near-instantaneous loss, and can be due to flash crashes, other market crises, malicious activity by selected market participants and other events. A well-cited example[30] of real-time risk was a US $440 million loss incurred within 30 minutes by Knight Capital Group (KCG) on August 1, 2012; the culprit was a poorly-tested runaway algorithm deployed by the firm. Regulators have taken notice of real-time risk as well. Basel III[31] requires real-time risk management framework for bank stability.		It is not always obvious if financial instruments are "hedging" (purchasing/selling a financial instrument specifically to reduce or cancel out the risk in another investment) or "speculation" (increasing measurable risk and exposing the investor to catastrophic loss in pursuit of very high windfalls that increase expected value).		Some people may be "risk seeking", i.e. their utility function's second derivative is positive. Such an individual willingly pays a premium to assume risk (e.g. buys a lottery ticket).		The financial audit risk model expresses the risk of an auditor providing an inappropriate opinion (or material misstatement) of a commercial entity's financial statements. It can be analytically expressed as		where AR is audit risk, IR is inherent risk, CR is control risk and DR is detection risk.		Note: As defined, audit risk does not consider the impact of an auditor misstatement and so is stated as a simple probability. The impact of misstatement must be considered when determining an acceptable audit risk.[32]		Security risk management involves protection of assets from harm caused by deliberate acts. A more detailed definition is: "A security risk is any event that could result in the compromise of organizational assets i.e. the unauthorized use, loss, damage, disclosure or modification of organizational assets for the profit, personal interest or political interests of individuals, groups or other entities constitutes a compromise of the asset, and includes the risk of harm to people. Compromise of organizational assets may adversely affect the enterprise, its business units and their clients. As such, consideration of security risk is a vital component of risk management."[33]		One of the growing areas of focus in risk management is the field of human factors where behavioral and organizational psychology underpin our understanding of risk based decision making. This field considers questions such as "how do we make risk based decisions?", "why are we irrationally more scared of sharks and terrorists than we are of motor vehicles and medications?"		In decision theory, regret (and anticipation of regret) can play a significant part in decision-making, distinct from risk aversion[34] (preferring the status quo in case one becomes worse off).		Framing[35] is a fundamental problem with all forms of risk assessment. In particular, because of bounded rationality (our brains get overloaded, so we take mental shortcuts), the risk of extreme events is discounted because the probability is too low to evaluate intuitively. As an example, one of the leading causes of death is road accidents caused by drunk driving – partly because any given driver frames the problem by largely or totally ignoring the risk of a serious or fatal accident.		For instance, an extremely disturbing event (an attack by hijacking, or moral hazards) may be ignored in analysis despite the fact it has occurred and has a nonzero probability. Or, an event that everyone agrees is inevitable may be ruled out of analysis due to greed or an unwillingness to admit that it is believed to be inevitable. These human tendencies for error and wishful thinking often affect even the most rigorous applications of the scientific method and are a major concern of the philosophy of science.		All decision-making under uncertainty must consider cognitive bias, cultural bias, and notational bias: No group of people assessing risk is immune to "groupthink": acceptance of obviously wrong answers simply because it is socially painful to disagree, where there are conflicts of interest.		Framing involves other information that affects the outcome of a risky decision. The right prefrontal cortex has been shown to take a more global perspective[36] while greater left prefrontal activity relates to local or focal processing.[37]		From the Theory of Leaky Modules[38] McElroy and Seta proposed that they could predictably alter the framing effect by the selective manipulation of regional prefrontal activity with finger tapping or monaural listening.[39] The result was as expected. Rightward tapping or listening had the effect of narrowing attention such that the frame was ignored. This is a practical way of manipulating regional cortical activation to affect risky decisions, especially because directed tapping or listening is easily done.		A growing area of research has been to examine various psychological aspects of risk taking. Researchers typically run randomized experiments with a treatment and control group to ascertain the effect of different psychological factors that may be associated with risk taking. Thus, positive and negative feedback about past risk taking can affect future risk taking. In an experiment, people who were led to believe they are very competent at decision making saw more opportunities in a risky choice and took more risks, while those led to believe they were not very competent saw more threats and took fewer risks.[40]		The concept of risk-based maintenance is an advanced form of Reliability centered maintenance. In case of chemical industries, apart from probability of failure, consequences of failure is also very important. Therefore, the selection of maintenance policies should be based on risk, instead of reliability. Risk-based maintenance methodology acts as a tool for maintenance planning and decision making to reduce the probability of failure and its consequences. In risk-based maintenance decision making, the maintenance resources can be utilized optimally based on the risk class (high, medium, or low) of equipment or machines, to achieve tolerable risk criteria.[41]		Since risk assessment and management is essential in security management, both are tightly related. Security assessment methodologies like CRAMM contain risk assessment modules as an important part of the first steps of the methodology. On the other hand, risk assessment methodologies like Mehari evolved to become security assessment methodologies. An ISO standard on risk management (Principles and guidelines on implementation) was published under code ISO 31000 on 13 November 2009.		There are many formal methods used to "measure" risk.		Often the probability of a negative event is estimated by using the frequency of past similar events. Probabilities for rare failures may be difficult to estimate. This makes risk assessment difficult in hazardous industries, for example nuclear energy, where the frequency of failures is rare, while harmful consequences of failure are severe.		Statistical methods may also require the use of a cost function, which in turn may require the calculation of the cost of loss of a human life. This is a difficult problem. One approach is to ask what people are willing to pay to insure against death[42] or radiological release (e.g. GBq of radio-iodine),[citation needed] but as the answers depend very strongly on the circumstances it is not clear that this approach is effective.		Risk is often measured as the expected value of an undesirable outcome. This combines the probabilities of various possible events and some assessment of the corresponding harm into a single value. See also Expected utility. The simplest case is a binary possibility of Accident or No accident. The associated formula for calculating risk is then:		For example, if performing activity X has a probability of 0.01 of suffering an accident of A, with a loss of 1000, then total risk is a loss of 10, the product of 0.01 and 1000.		Situations are sometimes more complex than the simple binary possibility case. In a situation with several possible accidents, total risk is the sum of the risks for each different accident, provided that the outcomes are comparable:		For example, if performing activity X has a probability of 0.01 of suffering an accident of A, with a loss of 1000, and a probability of 0.000001 of suffering an accident of type B, with a loss of 2,000,000, then total loss expectancy is 12, which is equal to a loss of 10 from an accident of type A and 2 from an accident of type B.		One of the first major uses of this concept was for the planning of the Delta Works in 1953, a flood protection program in the Netherlands, with the aid of the mathematician David van Dantzig.[43] The kind of risk analysis pioneered there has become common today in fields like nuclear power, aerospace and the chemical industry.		In statistical decision theory, the risk function is defined as the expected value of a given loss function as a function of the decision rule used to make decisions in the face of uncertainty.		People may rely on their fear and hesitation to keep them out of the most profoundly unknown circumstances. Fear is a response to perceived danger. Risk could be said to be the way we collectively measure and share this "true fear"—a fusion of rational doubt, irrational fear, and a set of unquantified biases from our own experience.		The field of behavioral finance focuses on human risk-aversion, asymmetric regret, and other ways that human financial behavior varies from what analysts call "rational". Risk in that case is the degree of uncertainty associated with a return on an asset. Recognizing and respecting the irrational influences on human decision making may do much to reduce disasters caused by naive risk assessments that presume rationality but in fact merely fuse many shared biases.		According to one set of definitions, fear is a fleeting emotion ascribed to a particular object, while anxiety is a trait of fear (this is referring to "trait anxiety", as distinct from how the term "anxiety" is generally used) that lasts longer and is not attributed to a specific stimulus (these particular definitions are not used by all authors cited on this page).[44] Some studies show a link between anxious behavior and risk (the chance that an outcome will have an unfavorable result).[45] Joseph Forgas introduced valence based research where emotions are grouped as either positive or negative (Lerner and Keltner, 2000). Positive emotions, such as happiness, are believed to have more optimistic risk assessments and negative emotions, such as anger, have pessimistic risk assessments. As an emotion with a negative valence, fear, and therefore anxiety, has long been associated with negative risk perceptions. Under the more recent appraisal tendency framework of Jennifer Lerner et al., which refutes Forgas’ notion of valence and promotes the idea that specific emotions have distinctive influences on judgments, fear is still related to pessimistic expectations.[46]		Psychologists have demonstrated that increases in anxiety and increases in risk perception are related and people who are habituated to anxiety experience this awareness of risk more intensely than normal individuals.[47] In decision-making, anxiety promotes the use of biases and quick thinking to evaluate risk. This is referred to as affect-as-information according to Clore, 1983. However, the accuracy of these risk perceptions when making choices is not known.[48]		Experimental studies show that brief surges in anxiety are correlated with surges in general risk perception.[48] Anxiety exists when the presence of threat is perceived (Maner and Schmidt, 2006).[47] As risk perception increases, it stays related to the particular source impacting the mood change as opposed to spreading to unrelated risk factors.[48] This increased awareness of a threat is significantly more emphasized in people who are conditioned to anxiety.[49] For example, anxious individuals who are predisposed to generating reasons for negative results tend to exhibit pessimism.[49] Also, findings suggest that the perception of a lack of control and a lower inclination to participate in risky decision-making (across various behavioral circumstances) is associated with individuals experiencing relatively high levels of trait anxiety.[47] In the previous instance, there is supporting clinical research that links emotional evaluation (of control), the anxiety that is felt and the option of risk avoidance.[47]		There are various views presented that anxious/fearful emotions cause people to access involuntary responses and judgments when making decisions that involve risk. Joshua A. Hemmerich et al. probes deeper into anxiety and its impact on choices by exploring "risk-as-feelings" which are quick, automatic, and natural reactions to danger that are based on emotions. This notion is supported by an experiment that engages physicians in a simulated perilous surgical procedure. It was demonstrated that a measurable amount of the participants' anxiety about patient outcomes was related to previous (experimentally created) regret and worry and ultimately caused the physicians to be led by their feelings over any information or guidelines provided during the mock surgery. Additionally, their emotional levels, adjusted along with the simulated patient status, suggest that anxiety level and the respective decision made are correlated with the type of bad outcome that was experienced in the earlier part of the experiment.[50] Similarly, another view of anxiety and decision-making is dispositional anxiety where emotional states, or moods, are cognitive and provide information about future pitfalls and rewards (Maner and Schmidt, 2006). When experiencing anxiety, individuals draw from personal judgments referred to as pessimistic outcome appraisals. These emotions promote biases for risk avoidance and promote risk tolerance in decision-making.[49]		It is common for people to dread some risks but not others: They tend to be very afraid of epidemic diseases, nuclear power plant failures, and plane accidents but are relatively unconcerned about some highly frequent and deadly events, such as traffic crashes, household accidents, and medical errors. One key distinction of dreadful risks seems to be their potential for catastrophic consequences,[51] threatening to kill a large number of people within a short period of time.[52] For example, immediately after the September 11 attacks, many Americans were afraid to fly and took their car instead, a decision that led to a significant increase in the number of fatal crashes in the time period following the 9/11 event compared with the same time period before the attacks.[53][54]		Different hypotheses have been proposed to explain why people fear dread risks. First, the psychometric paradigm[51] suggests that high lack of control, high catastrophic potential, and severe consequences account for the increased risk perception and anxiety associated with dread risks. Second, because people estimate the frequency of a risk by recalling instances of its occurrence from their social circle or the media, they may overvalue relatively rare but dramatic risks because of their overpresence and undervalue frequent, less dramatic risks.[54] Third, according to the preparedness hypothesis, people are prone to fear events that have been particularly threatening to survival in human evolutionary history.[55] Given that in most of human evolutionary history people lived in relatively small groups, rarely exceeding 100 people,[56] a dread risk, which kills many people at once, could potentially wipe out one’s whole group. Indeed, research found[57] that people’s fear peaks for risks killing around 100 people but does not increase if larger groups are killed. Fourth, fearing dread risks can be an ecologically rational strategy.[58] Besides killing a large number of people at a single point in time, dread risks reduce the number of children and young adults who would have potentially produced offspring. Accordingly, people are more concerned about risks killing younger, and hence more fertile, groups.[59]		The relationship between higher levels of risk perception and "judgmental accuracy" in anxious individuals remains unclear (Joseph I. Constans, 2001). There is a chance that "judgmental accuracy" is correlated with heightened anxiety. Constans conducted a study to examine how worry propensity (and current mood and trait anxiety) might influence college student’s estimation of their performance on an upcoming exam, and the study found that worry propensity predicted subjective risk bias (errors in their risk assessments), even after variance attributable to current mood and trait anxiety had been removed.[48] Another experiment suggests that trait anxiety is associated with pessimistic risk appraisals (heightened perceptions of the probability and degree of suffering associated with a negative experience), while controlling for depression.[47]		In his seminal work Risk, Uncertainty, and Profit, Frank Knight (1921) established the distinction between risk and uncertainty.		... Uncertainty must be taken in a sense radically distinct from the familiar notion of Risk, from which it has never been properly separated. The term "risk," as loosely used in everyday speech and in economic discussion, really covers two things which, functionally at least, in their causal relations to the phenomena of economic organization, are categorically different. ... The essential fact is that "risk" means in some cases a quantity susceptible of measurement, while at other times it is something distinctly not of this character; and there are far-reaching and crucial differences in the bearings of the phenomenon depending on which of the two is really present and operating. ... It will appear that a measurable uncertainty, or "risk" proper, as we shall use the term, is so far different from an unmeasurable one that it is not in effect an uncertainty at all. We ... accordingly restrict the term "uncertainty" to cases of the non-quantitive type.:[60]		Thus, Knightian uncertainty is immeasurable, not possible to calculate, while in the Knightian sense risk is measurable.		Another distinction between risk and uncertainty is proposed by Douglas Hubbard:[61][62]		In this sense, one may have uncertainty without risk but not risk without uncertainty. We can be uncertain about the winner of a contest, but unless we have some personal stake in it, we have no risk. If we bet money on the outcome of the contest, then we have a risk. In both cases there are more than one outcome. The measure of uncertainty refers only to the probabilities assigned to outcomes, while the measure of risk requires both probabilities for outcomes and losses quantified for outcomes.		The terms risk attitude, appetite, and tolerance are often used similarly to describe an organization's or individual's attitude towards risk-taking. One's attitude may be described as risk-averse, risk-neutral, or risk-seeking. Risk tolerance looks at acceptable/unacceptable deviations from what is expected.[clarification needed] Risk appetite looks at how much risk one is willing to accept. There can still be deviations that are within a risk appetite. For example, recent research finds that insured individuals are significantly likely to divest from risky asset holdings in response to a decline in health, controlling for variables such as income, age, and out-of-pocket medical expenses.[63]		Gambling is a risk-increasing investment, wherein money on hand is risked for a possible large return, but with the possibility of losing it all. Purchasing a lottery ticket is a very risky investment with a high chance of no return and a small chance of a very high return. In contrast, putting money in a bank at a defined rate of interest is a risk-averse action that gives a guaranteed return of a small gain and precludes other investments with possibly higher gain. The possibility of getting no return on an investment is also known as the rate of ruin.		Hubbard also argues that defining risk as the product of impact and probability presumes, unrealistically, that decision-makers are risk-neutral.[62][page needed] A risk-neutral person's utility is proportional to the expected value of the payoff. For example, a risk-neutral person would consider 20% chance of winning $1 million exactly as desirable as getting a certain $200,000. However, most decision-makers are not actually risk-neutral and would not consider these equivalent choices. This gave rise to prospect theory and cumulative prospect theory. Hubbard proposes to instead describe risk as a vector quantity that distinguishes the probability and magnitude of a risk. Risks are simply described as a set or function[vague] of possible payoffs (gains or losses) with their associated probabilities. This array is collapsed into a scalar value according to a decision-maker's risk tolerance.		This is a list of books about risk issues.		
Travel is the movement of people between relatively distant geographical locations, and can involve travel by foot, bicycle, automobile, train, boat, bus, airplane, or other means, with or without luggage, and can be one way or round trip.[1][2] Travel can also include relatively short stays between successive movements.						The origin of the word "travel" is most likely lost to history. The term "travel" may originate from the Old French word travail, which means 'work'.[3] According to the Merriam Webster dictionary, the first known use of the word travel was in the 14th century. It also states that the word comes from Middle English travailen, travelen (which means to torment, labor, strive, journey) and earlier from Old French travailler (which means to work strenuously, toil). In English we still occasionally use the words "travail", which means struggle. According to Simon Winchester in his book The Best Travelers' Tales (2004), the words "travel" and "travail" both share an even more ancient root: a Roman instrument of torture called the tripalium (in Latin it means "three stakes", as in to impale). This link may reflect the extreme difficulty of travel in ancient times. Today, travel may or may not be much easier depending upon the destination you choose (e.g. Mt. Everest, the Amazon rainforest), how you plan to get there (tour bus, cruise ship, or oxcart), and whether you decide to "rough it" (see extreme tourism and adventure travel). "There's a big difference between simply being a tourist and being a true world traveler", notes travel writer Michael Kasum. This is, however, a contested distinction as academic work on the cultures and sociology of travel has noted.[4]		Reasons for traveling include recreation,[5] tourism[5] or vacationing,[5] research travel[5] the gathering of information, visiting people, volunteer travel for charity, migration to begin life somewhere else, religious pilgrimages[5] and mission trips, business travel,[5] trade,[5] commuting, and other reasons, such as to obtain health care[5] or waging or fleeing war or for the enjoyment of traveling. Travellers may use human-powered transport such as walking or bicycling; or vehicles, such as public transport, automobiles, trains and airplanes.		Motives for travel include:		Travel may be local, regional, national (domestic) or international. In some countries, non-local internal travel may require an internal passport, while international travel typically requires a passport and visa. A trip may also be part of a round-trip, which is a particular type of travel whereby a person moves from one location to another and returns.[7]		While early travel tended to be slower, more dangerous, and more dominated by trade and migration, cultural and technological advances over many years have tended to mean that travel has become easier and more accessible.[8] The evolution of technology in such diverse fields as horse tack and bullet trains has contributed to this trend.		While travel in the Middle Ages offered hardships and challenges, it was important to the economy and to society. The wholesale sector depended (for example) on merchants dealing with/through caravan or sea-voyagers, end-user retailing often demanded the services of many itinerant peddlers wandering from village to hamlet, gyrovagues and wandering friars brought theology and pastoral support to neglected areas, travelling minstrels practised the never-ending tour, and armies ranged far and wide[9] in various crusades[10] and in sundry other wars.[11]		Pilgrimages involved streams of travellers both locally (Canterbury Tales-style) and internationally.[12]		Travel by water often provided more comfort and speed than land-travel, at least until the advent of a network of railways in the 19th century. Airships and airplanes took over much of the role of long-distance surface travel in the 20th century.		Authorities emphasize the importance of taking precautions to ensure travel safety.[13] When traveling abroad, the odds favor a safe and incident-free trip, however, travelers can be subject to difficulties, crime and violence.[14] Some safety considerations include being aware of one's surroundings,[13] avoiding being the target of a crime,[13] leaving copies of one's passport and itinerary information with trusted people,[13] obtaining medical insurance valid in the country being visited[13] and registering with one's national embassy when arriving in a foreign country.[13] Many countries do not recognize drivers' licenses from other countries; however most countries accept international driving permits.[15] Automobile insurance policies issued in one's own country are often invalid in foreign countries, and it is often a requirement to obtain temporary auto insurance valid in the country being visited.[15] It is also advisable to become oriented with the driving-rules and -regulations of destination countries.[15] Wearing a seat belt is highly advisable for safety reasons; many countries have penalties for violating seatbelt laws.[15]		There are three main statistics which may be used to compare the safety of various forms of travel (based on a DETR survey in October 2000):[16]		
A video game is an electronic game that involves interaction with a user interface to generate visual feedback on a video device such as a TV screen or computer monitor. The word video in video game traditionally referred to a raster display device, but as of the 2000s, it implies any type of display device that can produce two- or three-dimensional images. Some theorists categorize video games as an art form, but this designation is controversial.		The electronic systems used to play video games are known as platforms; examples of these are personal computers and video game consoles. These platforms range from large mainframe computers to small handheld computing devices. Specialized video games such as arcade games, in which the video game components are housed in a large, typically coin-operated chassis, while common in the 1980s in video arcades, have gradually declined due to the widespread availability of affordable home video game consoles (e.g., PlayStation 4, Xbox One and Nintendo Wii U) and video games on desktop and laptop computers and smartphones.		The input device used for games, the game controller, varies across platforms. Common controllers include gamepads, joysticks, mouse devices, keyboards, the touchscreens of mobile devices, and buttons, or even, with the Kinect sensor, a person's hands and body. Players typically view the game on a video screen or television or computer monitor, or sometimes on virtual reality head-mounted display goggles. There are often game sound effects, music and, in the 2010s, voice actor lines which come from loudspeakers or headphones. Some games in the 2000s include haptic, vibration-creating effects, force feedback peripherals and virtual reality headsets. In the 2010s, the video game industry is of increasing commercial importance, with growth driven particularly by the emerging Asian markets and mobile games, which are played on smartphones. As of 2015, video games generated sales of USD 74 billion annually worldwide, and were the third-largest segment in the U.S. entertainment market, behind broadcast and cable TV.						Early games used interactive electronic devices with various display formats. The earliest example is from 1947—a "Cathode ray tube Amusement Device" was filed for a patent on 25 January 1947, by Thomas T. Goldsmith Jr. and Estle Ray Mann, and issued on 14 December 1948, as U.S. Patent 2455992.[1] Inspired by radar display technology, it consisted of an analog device that allowed a user to control a vector-drawn dot on the screen to simulate a missile being fired at targets, which were drawings fixed to the screen.[2] Other early examples include: The Nimrod computer at the 1951 Festival of Britain; OXO a tic-tac-toe Computer game by Alexander S. Douglas for the EDSAC in 1952; Tennis for Two, an electronic interactive game engineered by William Higinbotham in 1958; Spacewar!, written by MIT students Martin Graetz, Steve Russell, and Wayne Wiitanen's on a DEC PDP-1 computer in 1961; and the hit ping pong-style Pong, a 1972 game by Atari. Each game used different means of display: NIMROD used a panel of lights to play the game of Nim,[3] OXO used a graphical display to play tic-tac-toe[4] Tennis for Two used an oscilloscope to display a side view of a tennis court,[2] and Spacewar! used the DEC PDP-1's vector display to have two spaceships battle each other.[5]		In 1971, Computer Space, created by Nolan Bushnell and Ted Dabney, was the first commercially sold, coin-operated video game. It used a black-and-white television for its display, and the computer system was made of 74 series TTL chips.[6] The game was featured in the 1973 science fiction film Soylent Green. Computer Space was followed in 1972 by the Magnavox Odyssey, the first home console. Modeled after a late 1960s prototype console developed by Ralph H. Baer called the "Brown Box", it also used a standard television.[2][7] These were followed by two versions of Atari's Pong; an arcade version in 1972 and a home version in 1975 that dramatically increased video game popularity.[8] The commercial success of Pong led numerous other companies to develop Pong clones and their own systems, spawning the video game industry.[9]		A flood of Pong clones eventually led to the video game crash of 1977, which came to an end with the mainstream success of Taito's 1978 shooter game Space Invaders,[10] marking the beginning of the golden age of arcade video games and inspiring dozens of manufacturers to enter the market.[10][11] The game inspired arcade machines to become prevalent in mainstream locations such as shopping malls, traditional storefronts, restaurants, and convenience stores.[12] The game also became the subject of numerous articles and stories on television and in newspapers and magazines, establishing video gaming as a rapidly growing mainstream hobby.[13][14] Space Invaders was soon licensed for the Atari VCS (later known as Atari 2600), becoming the first "killer app" and quadrupling the console's sales.[15] This helped Atari recover from their earlier losses,[16] and in turn the Atari VCS revived the home video game market during the second generation of consoles, up until the North American video game crash of 1983.[17] The home video game industry was revitalized shortly afterwards by the widespread success of the Nintendo Entertainment System,[18] which marked a shift in the dominance of the video game industry from the United States to Japan during the third generation of consoles.[19]		The term "platform" refers to the specific combination of electronic components or computer hardware which, in conjunction with software, allows a video game to operate.[20] The term "system" is also commonly used. The distinctions below are not always clear and there may be games that bridge one or more platforms. In addition to personal computers, there are other devices which have the ability to play games but are not dedicated video game machines, such as smartphones, PDAs and graphing calculators.		In common use a "PC game" refers to a form of media that involves a player interacting with a personal computer connected to a video monitor. Personal computers are not dedicated game platforms, so there may be differences running the same game in different hardwares, also the openness allows some features to developers like reduced software cost,[21] increased flexibility, increased innovation, emulation, creation of modifications ("mods"), open hosting for online gaming (in which a person plays a video game with people who are in a different household) and others.		A "console game" is played on a specialized electronic device that connects to a common television set or composite video monitor, unlike PCs, which can run all sorts of computer programs, a console is a dedicated video game platform manufactured by a specific company. Usually consoles only run games developed for it, or games from other platform made by the same company, but never games developed by its direct competitor, even if the same game is available on different platforms. It often comes with a specific game controller. Major console platforms include Xbox and PlayStation.		A "handheld" gaming device is a small, self-contained electronic device that is portable and can be held in a user's hands. It features the console, a small screen, speakers and buttons, joystick or other game controllers in a single unit. Like consoles, handhelds are dedicated platforms, and share almost the same characteristics. Handheld hardware usually is less powerful than PC or console hardwares. Some handheld games from the late 1970s and early 1980s could only play one game. In the 1990s and 2000s, a number of handheld games used cartridges, which enabled them to be used to play many different games.		"Arcade game" generally refers to a game played on an even more specialized type of electronic device that is typically designed to play only one game and is encased in a special, large coin-operated cabinet which has one built-in console, controllers (joystick, buttons, etc.), a CRT screen, and audio amplifier and speakers. Arcade games often have brightly painted logos and images relating to the theme of the game. While most arcade games are housed in a vertical cabinet, which the user typically stands in front of to play, some arcade games use a tabletop approach, in which the display screen is housed in a table-style cabinet with a see-through table top. With table-top games, the users typically sit to play. In the 1990s and 2000s, some arcade games offered players a choice of multiple games. In the 1980s, video arcades were businesses in which game players could use a number of arcade video games. In the 2010s, there are far fewer video arcades, but some movie theaters and family entertainment centers still have them.		The web browser has also established itself as platform in its own right in the 2000s, while providing a cross-platform environment for video games designed to be played on a wide spectrum of hardware from personal computers and tablet computers to smartphones. This in turn has generated new terms to qualify classes of web browser-based games. These games may be identified based on the website that they appear, such as with "Facebook" games. Others are named based on the programming platform used to develop them, such as Java and Flash games.		With the advent of standard operating systems for mobile devices such as iOS and Android and devices with greater hardware performance, mobile gaming has become a significant platform. While many mobile games share similar concepts with browser games, these games may utilize features of smart devices that are not necessary present on other platforms such as global positing information and camera devices to support augmented reality gameplay. Mobile games also led into the development of microtransactions as a valid revenue model for casual games.		Virtual reality (VR) games generally require players to use a special head-mounted unit that provides stereoscopic screens and motion tracking to immerse a player within virtual environment that responds to their head movements. Some VR systems include control units for the player's hands as to provide a direct way to interact with the virtual world. VR systems generally require a separate computer, console, or other processing device that couples with the head-mounted unit.		A video game, like most other forms of media, may be categorized into genres. Video game genres are used to categorize video games based on their gameplay interaction rather than visual or narrative differences.[22][23] A video game genre is defined by a set of gameplay challenges and are classified independent of their setting or game-world content, unlike other works of fiction such as films or books. For example, a shooter game is still a shooter game, regardless of whether it takes place in a fantasy world or in outer space.[24][25]		Because genres are dependent on content for definition, genres have changed and evolved as newer styles of video games have come into existence. Ever advancing technology and production values related to video game development have fostered more lifelike and complex games which have in turn introduced or enhanced genre possibilities (e.g., virtual pets), pushed the boundaries of existing video gaming or in some cases add new possibilities in play (such as that seen with titles specifically designed for devices like Sony's EyeToy). Some genres represent combinations of others, such as massively multiplayer online role-playing games, or, more commonly, MMORPGs. It is also common to see higher level genre terms that are collective in nature across all other genres such as with action, music/rhythm or horror-themed video games.[citation needed]		Casual games derive their name from their ease of accessibility, simple to understand gameplay and quick to grasp rule sets. Additionally, casual games frequently support the ability to jump in and out of play on demand. Casual games as a format existed long before the term was coined and include video games such as Solitaire or Minesweeper which can commonly be found pre-installed with many versions of the Microsoft Windows operating system. Examples of genres within this category are match three, hidden object, time management, puzzle or many of the tower defense style games. Casual games are generally available through app stores and online retailers such as PopCap, Zylom and GameHouse or provided for free play through web portals such as Newgrounds. While casual games are most commonly played on personal computers, phones or tablets, they can also be found on many of the on-line console system download services (e.g., the PlayStation Network, WiiWare or Xbox Live).		Serious games are games that are designed primarily to convey information or a learning experience to the player. Some serious games may even fail to qualify as a video game in the traditional sense of the term. Educational software does not typically fall under this category (e.g., touch typing tutors, language learning programs, etc.) and the primary distinction would appear to be based on the title's primary goal as well as target age demographics. As with the other categories, this description is more of a guideline than a rule. Serious games are games generally made for reasons beyond simple entertainment and as with the core and casual games may include works from any given genre, although some such as exercise games, educational games, or propaganda games may have a higher representation in this group due to their subject matter. These games are typically designed to be played by professionals as part of a specific job or for skill set improvement. They can also be created to convey social-political awareness on a specific subject.		One of the longest-running serious games franchises would be Microsoft Flight Simulator first published in 1982 under that name. The United States military uses virtual reality based simulations, such as VBS1 for training exercises,[26] as do a growing number of first responder roles (e.g., police, firefighters, EMTs).[27] One example of a non-game environment utilized as a platform for serious game development would be the virtual world of Second Life, which is currently used by several United States governmental departments (e.g., NOAA, NASA, JPL), Universities (e.g., Ohio University, MIT) for educational and remote learning programs[28] and businesses (e.g., IBM, Cisco Systems) for meetings and training.[29]		Tactical media in video games plays a crucial role in making a statement or conveying a message on important relevant issues. This form of media allows for a broader audience to be able to receive and gain access to certain information that otherwise may not have reached such people. An example of tactical media in video games would be newsgames. These are short games related to contemporary events designed to illustrate a point.[30] For example, Take Action Games is a game studio collective that was co-founded by Susana Ruiz and has made successful serious games. Some of these games include Darfur is Dying, Finding Zoe, and In The Balance. All of these games bring awareness to important issues and events in an intelligent and well thought out manner.[31]		On 23 September 2009, U.S. President Barack Obama launched a campaign called "Educate to Innovate" aimed at improving the technological, mathematical, scientific and engineering abilities of American students. This campaign states that it plans to harness the power of interactive games to help achieve the goal of students excelling in these departments.[32][33] This campaign has stemmed into many new opportunities for the video game realm and has contributed to many new competitions. Some of these competitions include the Stem National Video Game Competition and the Imagine Cup.[34][35] Both of these examples are events that bring a focus to relevant and important current issues that are able to be addressed in the sense of video games to educate and spread knowledge in a new form of media. www.NobelPrize.org uses games to entice the user to learn about information pertaining to the Nobel prize achievements while engaging in a fun to play video game.[36] There are many different types and styles of educational games all the way from counting to spelling to games for kids and games for adults. Some other games do not have any particular targeted audience in mind and intended to simply educate or inform whoever views or plays the game.		Video game can use several types of input devices to translate human actions to a game, the most common game controllers are keyboard and mouse for "PC games, consoles usually come with specific gamepads, handheld consoles have built in buttons. Other game controllers are commonly used for specific games like racing wheels, light guns or dance pads. Digital cameras can also be used as game controllers capturing movements of the body of the player.		As technology continues to advance, more can be added onto the controller to give the player a more immersive experience when playing different games. There are some controllers that have presets so that the buttons are mapped a certain way to make playing certain games easier. Along with the presets, a player can sometimes custom map the buttons to better accommodate their play style. On keyboard and mouse, different actions in the game are already preset to keys on the keyboard. Most games allow the player to change that so that the actions are mapped to different keys that are more to their liking. The companies that design the controllers are trying to make the controller visually appealing and also feel comfortable in the hands of the consumer.		An example of a technology that was incorporated into the controller was the touchscreen. It allows the player to be able to interact with the game differently than before. The person could move around in menus easier and they are also able to interact with different objects in the game. They can pick up some objects, equip others, or even just move the objects out of the players path. Another example is motion sensor where a persons movement is able to be captured and put into a game. Some motion sensor games are based on where the controller is. The reason for that is because there is a signal that is sent from the controller to the console or computer so that the actions being done can create certain movements in the game. Other type of motion sensor games are webcam style where the person can move around in front of it and the actions done are repeated in a character of the game you are playing as.		Video game development and authorship, much like any other form of entertainment, is frequently a cross-disciplinary field. Video game developers, as employees within this industry are commonly referred, primarily include programmers and graphic designers. Over the years this has expanded to include almost every type of skill that one might see prevalent in the creation of any movie or television program, including sound designers, musicians, and other technicians; as well as skills that are specific to video games, such as the game designer. All of these are managed by producers.		In the early days of the industry, it was more common for a single person to manage all of the roles needed to create a video game. As platforms have become more complex and powerful in the type of material they can present, larger teams have been needed to generate all of the art, programming, cinematography, and more. This is not to say that the age of the "one-man shop" is gone, as this is still sometimes found in the casual gaming and handheld markets,[37] where smaller games are prevalent due to technical limitations such as limited RAM or lack of dedicated 3D graphics rendering capabilities on the target platform (e.g., some cellphones and PDAs).[citation needed]		With the growth of the size of development teams in the industry, the problem of cost has increased. Development studios need to be able to pay their staff a competitive wage in order to attract and retain the best talent, while publishers are constantly looking to keep costs down in order to maintain profitability on their investment. Typically, a video game console development team can range in sizes of anywhere from 5 to 50 people, with some teams exceeding 100. In May 2009, one game project was reported to have a development staff of 450.[38] The growth of team size combined with greater pressure to get completed projects into the market to begin recouping production costs has led to a greater occurrence of missed deadlines, rushed games and the release of unfinished products.[39]		A phenomenon of additional game content at a later date, often for additional funds, began with digital video game distribution known as downloadable content (DLC). Developers can use digital distribution to issue new storylines after the main game is released, such as Rockstar Games with Grand Theft Auto IV (The Lost and Damned and The Ballad of Gay Tony), or Bethesda with Fallout 3 and its expansions. New gameplay modes can also become available, for instance, Call of Duty and its zombie modes,[40][41][42] a multiplayer mode for Mushroom Wars or a higher difficulty level for Metro: Last Light. Smaller packages of DLC are also common, ranging from better in-game weapons (Dead Space, Just Cause 2), character outfits (LittleBigPlanet, Minecraft), or new songs to perform (SingStar, Rock Band, Guitar Hero).		A variation of downloadable content is expansion packs. Unlike DLC, expansion packs add a whole section to the game that either already existed in the game's code or was recently developed after the game had already been released. Expansions add new maps, missions, weapons, and other things that weren't previously accessible in the original game. An example of an expansion is Bungie's most recent game, Destiny, when they released the Rise of Iron expansion. The expansion added new weapons, new maps, higher levels, and also remade old missions so that the difficulty would be meet the new levels that were added to the characters. Expansions are added to the base game to help prolong the life of the game itself until the company is able to produce a sequel or a new game all together. Developers at times plan out their games life and already have the code for the expansion in the game but inaccessible by players and they would unlock the expansions as time went on to the players, sometimes at no extra cost and other times it costs extra to get the expansion. There are also some developers who make the game and then make the expansions as time goes on so that they could see what the players would like to have and what they can do to make the game better. There are also expansions that are set apart from the original game and are considered a stand-alone game, an example of that is Ubisoft's expansion Assassin's Creed IV: Black Flag Freedom's Cry which takes place control of a different character than that of the original game.		Many games produced for the PC are designed such that technically oriented consumers can modify the game. These mods can add an extra dimension of replayability and interest. Developers such as id Software, Valve Corporation, Crytek, Bethesda, Epic Games and Blizzard Entertainment ship their games with some of the development tools used to make the game, along with documentation to assist mod developers. The Internet provides an inexpensive medium to promote and distribute mods, and they may be a factor in the commercial success of some games.[43] This allows for the kind of success seen by popular mods such as the Half-Life mod Counter-Strike.		Cheating in computer games may involve cheat codes and hidden spots implemented by the game developers,[44][45] modification of game code by third parties,[46][47] or players exploiting a software glitch. Modifications are facilitated by either cheat cartridge hardware or a software trainer.[46] Cheats usually make the game easier by providing an unlimited amount of some resource; for example weapons, health, or ammunition; or perhaps the ability to walk through walls.[45][46] Other cheats might give access to otherwise unplayable levels or provide unusual or amusing features, like altered game colors or other graphical appearances.		Software errors not detected by software testers during development can find their way into released versions of computer and video games. This may happen because the glitch only occurs under unusual circumstances in the game, was deemed too minor to correct, or because the game development was hurried to meet a publication deadline. Glitches can range from minor graphical errors to serious bugs that can delete saved data or cause the game to malfunction. In some cases publishers will release updates (referred to as patches) to repair glitches. Sometimes a glitch may be beneficial to the player; these are often referred to as exploits.		Easter eggs are hidden messages or jokes left in games by developers that are not part of the main game.[48] Easter eggs are secret responses that occur as a result of an undocumented set of commands. The results can vary from a simple printed message or image, to a page of programmer credits or a small videogame hidden inside an otherwise serious piece of software. Videogame cheat codes are a specific type of Easter egg, in which entering a secret command will unlock special powers or new levels for the player.[49][50]		Although departments of computer science have been studying the technical aspects of video games for years, theories that examine games as an artistic medium are a relatively recent development in the humanities. The two most visible schools in this emerging field are ludology and narratology. Narrativists approach video games in the context of what Janet Murray calls "Cyberdrama". That is to say, their major concern is with video games as a storytelling medium, one that arises out of interactive fiction. Murray puts video games in the context of the Holodeck, a fictional piece of technology from Star Trek, arguing for the video game as a medium in which the player is allowed to become another person, and to act out in another world.[51] This image of video games received early widespread popular support, and forms the basis of films such as Tron, eXistenZ and The Last Starfighter.		Ludologists break sharply and radically from this idea. They argue that a video game is first and foremost a game, which must be understood in terms of its rules, interface, and the concept of play that it deploys. Espen J. Aarseth argues that, although games certainly have plots, characters, and aspects of traditional narratives, these aspects are incidental to gameplay. For example, Aarseth is critical of the widespread attention that narrativists have given to the heroine of the game Tomb Raider, saying that "the dimensions of Lara Croft's body, already analyzed to death by film theorists, are irrelevant to me as a player, because a different-looking body would not make me play differently... When I play, I don't even see her body, but see through it and past it."[52] Simply put, ludologists reject traditional theories of art because they claim that the artistic and socially relevant qualities of a video game are primarily determined by the underlying set of rules, demands, and expectations imposed on the player.		While many games rely on emergent principles, video games commonly present simulated story worlds where emergent behavior occurs within the context of the game. The term "emergent narrative" has been used to describe how, in a simulated environment, storyline can be created simply by "what happens to the player."[53] However, emergent behavior is not limited to sophisticated games. In general, any place where event-driven instructions occur for AI in a game, emergent behavior will exist. For instance, take a racing game in which cars are programmed to avoid crashing, and they encounter an obstacle in the track: the cars might then maneuver to avoid the obstacle causing the cars behind them to slow and/or maneuver to accommodate the cars in front of them and the obstacle. The programmer never wrote code to specifically create a traffic jam, yet one now exists in the game.		An emulator is a program that replicates the behavior of a video game console, allowing games to run on a different platform from the original hardware. Emulators exist for PCs, smartphones and consoles other than the original. Emulators are generally used to play old games, hack existing games, translate unreleased games in a specific region, or add enhanced features to games like improved graphics, speed up or down, bypass regional lockouts, or online multiplayer support.		Some manufacturers have released official emulators for their own consoles. For example, the Virtual Console allows users to play games for old Nintendo consoles on the Wii, Wii U, and 3DS. Virtual Console is part of Nintendo's strategy for deterring video game piracy.[54] In November 2015, Microsoft launched backwards compatibility of Xbox 360 games on Xbox One console via emulation.[55] Also, Sony announced relaunching PS2 games on PS4 via emulation.[56] According to Sony Computer Entertainment America v. Bleem, creating an emulator for a proprietary video game console is legal.[57] However, Nintendo claims that emulators promote the distribution of illegally copied games.[58]		The November 2005 Nielsen Active Gamer Study, taking a survey of 2,000 regular gamers, found that the U.S. games market is diversifying. The age group among male players has expanded significantly in the 25–40 age group. For casual online puzzle-style and simple mobile cell phone games, the gender divide is more or less equal between men and women. More recently there has been a growing segment of female players engaged with the aggressive style of games historically considered to fall within traditionally male genres (e.g., first-person shooters). According to the ESRB, almost 41% of PC gamers are women.[59] Participation among African-Americans is even lower. One survey of over 2000 game developers returned responses from only 2.5% who identified as black.[60]		When comparing today's industry climate with that of 20 years ago, women and many adults are more inclined to be using products in the industry. While the market for teen and young adult men is still a strong market, it is the other demographics which are posting significant growth. The Entertainment Software Association (ESA) provides the following summary for 2011 based on a study of almost 1,200 American households carried out by Ipsos MediaCT:[61]		A 2006 academic study, based on a survey answered by 10,000 gamers, identified the gaymers (gamers that identify as gay) as a demographic group.[62][63][64] A follow-up survey in 2009 studied the purchase habits and content preferences of people in the group.[65][66][67] Based on the study by NPD group in 2011, approximately 91 percent of children aged 2–17 play games.[68]		Video game culture is a worldwide new media subculture formed around video games and game playing. As computer and video games have increased in popularity over time, they have had a significant influence on popular culture. Video game culture has also evolved over time hand in hand with internet culture as well as the increasing popularity of mobile games. Many people who play video games identify as gamers, which can mean anything from someone who enjoys games to someone who is passionate about it. As video games become more social with multiplayer and online capability, gamers find themselves in growing social networks. Gaming can both be entertainment as well as competition, as a new trend known as electronic sports is becoming more widely accepted. In the 2010s, video games and discussions of video game trends and topics can be seen in social media, politics, television, film and music.		Video gaming has traditionally been a social experience. Multiplayer video games are those that can be played either competitively, sometimes in Electronic Sports, or cooperatively by using either multiple input devices, or by hotseating. Tennis for Two, arguably the first video game, was a two player game, as was its successor Pong. The first commercially available game console, the Magnavox Odyssey, had two controller inputs. Since then, most consoles have been shipped with two or four controller inputs. Some have had the ability to expand to four, eight or as many as 12 inputs with additional adapters, such as the Multitap. Multiplayer arcade games typically feature play for two to four players, sometimes tilting the monitor on its back for a top-down viewing experience allowing players to sit opposite one another.		Many early computer games for non-PC descendant based platforms featured multiplayer support. Personal computer systems from Atari and Commodore both regularly featured at least two game ports. PC-based computer games started with a lower availability of multiplayer options because of technical limitations. PCs typically had either one or no game ports at all. Network games for these early personal computers were generally limited to only text based adventures or MUDs that were played remotely on a dedicated server. This was due both to the slow speed of modems (300-1200-bit/s), and the prohibitive cost involved with putting a computer online in such a way where multiple visitors could make use of it. However, with the advent of widespread local area networking technologies and Internet based online capabilities, the number of players in modern games can be 32 or higher, sometimes featuring integrated text and/or voice chat. Massively multiplayer online game (MMOs) can offer extremely high numbers of simultaneous players; Eve Online set a record with 65,303 players on a single server in 2013.[69]		It has been shown that action video game players have better hand–eye coordination and visuo-motor skills, such as their resistance to distraction, their sensitivity to information in the peripheral vision and their ability to count briefly presented objects, than nonplayers.[70] Researchers found that such enhanced abilities could be acquired by training with action games, involving challenges that switch attention between different locations, but not with games requiring concentration on single objects. It has been suggested by a few studies that online/offline video gaming can be used as a therapeutic tool in the treatment of different mental health concerns.[which?]		In Steven Johnson's book, Everything Bad Is Good for You, he argues that video games in fact demand far more from a player than traditional games like Monopoly. To experience the game, the player must first determine the objectives, as well as how to complete them. They must then learn the game controls and how the human-machine interface works, including menus and HUDs. Beyond such skills, which after some time become quite fundamental and are taken for granted by many gamers, video games are based upon the player navigating (and eventually mastering) a highly complex system with many variables. This requires a strong analytical ability, as well as flexibility and adaptability. He argues that the process of learning the boundaries, goals, and controls of a given game is often a highly demanding one that calls on many different areas of cognitive function. Indeed, most games require a great deal of patience and focus from the player, and, contrary to the popular perception that games provide instant gratification, games actually delay gratification far longer than other forms of entertainment such as film or even many books.[71] Some research suggests video games may even increase players' attention capacities.[72]		Learning principles found in video games have been identified as possible techniques with which to reform the U.S. education system.[73] It has been noticed that gamers adopt an attitude while playing that is of such high concentration, they do not realize they are learning, and that if the same attitude could be adopted at school, education would enjoy significant benefits.[74] Students are found to be "learning by doing" while playing video games while fostering creative thinking.[75]		The U.S. Army has deployed machines such as the PackBot and UAV vehicles, which make use of a game-style hand controller to make it more familiar for young people.[76] According to research discussed at the 2008 Convention of the American Psychological Association, certain types of video games can improve the gamers' dexterity as well as their ability to do problem solving. A study of 33 laparoscopic surgeons found that those who played video games were 27 percent faster at advanced surgical procedures and made 37 percent fewer errors compared to those who did not play video games. A second study of 303 laparoscopic surgeons (82 percent men; 18 percent women) also showed that surgeons who played video games requiring spatial skills and hand dexterity and then performed a drill testing these skills were significantly faster at their first attempt and across all 10 trials than the surgeons who did not play the video games first.[77]		The research showing benefits from action games has been questioned due to methodological shortcomings, such as recruitment strategies and selection bias, potential placebo effects, and lack of baseline improvements in control groups.[78] In addition, many of the studies are cross-sectional, and of the longitudinal interventional trials, not all have found effects.[78] A response to this pointed out that the skill improvements from action games are more broad than predicted, such as mental rotation, which is not a common task in action games.[79] Action gamers are not only better at ignoring distractions, but also at focusing on the main task.[80]		Like other media, such as rock music (notably heavy metal music and gangsta rap), video games have been the subject of objections, controversies and censorship, for instance because of depictions of violence, criminal activities, sexual themes, alcohol, tobacco and other drugs, propaganda, profanity or advertisements. Critics of video games include parents' groups, politicians, religious groups, scientists and other advocacy groups. Claims that some video games cause addiction or violent behavior continue to be made and to be disputed.[81]		There have been a number of societal and scientific arguments about whether the content of video games change the behavior and attitudes of a player, and whether this is reflected in video game culture overall. Since the early 1980s, advocates of video games have emphasized their use as an expressive medium, arguing for their protection under the laws governing freedom of speech and also as an educational tool. Detractors argue that video games are harmful and therefore should be subject to legislative oversight and restrictions. The positive and negative characteristics and effects of video games are the subject of scientific study. Results of investigations into links between video games and addiction, aggression, violence, social development, and a variety of stereotyping and sexual morality issues are debated.[82] A study was done that showed that young people who have had a greater exposure to violence in video games ended up behaving more aggressively towards people in a social environment.[83]		In spite of the negative effects of video games, certain studies indicate that they may have value in terms of academic performance, perhaps because of the skills that are developed in the process. “When you play ... games you’re solving puzzles to move to the next level and that involves using some of the general knowledge and skills in maths, reading and science that you’ve been taught during the day,” said Alberto Posso an Associate Professor at the Royal Melbourne Institute of Technology, after analysing data from the results of standardized testing completed by over 12,000 high school students across Australia. As summarized by The Guardian,[84] the study [published in the International Journal of Communication], "found that students who played online games almost every day scored 15 points above average in maths and reading tests and 17 points above average in science." However, the reporter added an important comment that was not provided by some of the numerous Web sites that published a brief summary of the Australian study study: "[the] methodology cannot prove that playing video games were the cause of the improvement." The Guardian also reported that a Columbia University study indicated that extensive video gaming by students in the 6 to 11 age group provided a greatly increased chance of high intellectual functioning and overall school competence.		In an interview with CNN, Edward Castronova, a professor of Telecommunications at Indiana University Bloomington said he was not surprised by the outcome of the Australian study but also discussed the issue of causal connection. "Though there is a link between gaming and higher math and science scores, it doesn't mean playing games caused the higher scores. It could just be that kids who are sharp are looking for a challenge, and they don't find it on social media, and maybe they do find it on board games and video games," he explained.[85]		Video games have also been proven to raise self-esteem and build confidence. It gives people an opportunity to do things that they cannot do offline, and to discover new things about themselves. There is a social aspect to gaming as well – research has shown that a third of video game players make good friends online. As well as that, video games are also considered to be therapeutic as it helps to relieve stress.[86] Although short term, studies have shown that children with developmental delays gain a temporary physical improvement in health when they interact and play video games on a regular, and consistent basis due to the cognitive benefits and the use of hand eye coordination[87]		The Entertainment Software Rating Board (ESRB) gives video games maturity ratings based on their content. For example, a game might be rated T for Teen if the game contained obscene words or violence. If a game contains explicit violence or sexual themes, it is likely to receive an M for Mature rating, which means that no one under 17 should play it. There is a rated "A/O" games for "Adults Only" these games have massive violence or nudity. There are no laws that prohibit children from purchasing "M" rated games in the United States. Laws attempting to prohibit minors from purchasing "M" rated games were established in California, Illinois, Michigan, Minnesota, and Louisiana, but all were overturned on the grounds that these laws violated the First Amendment.[88] However, many stores have opted to not sell such games to children anyway. Of course, video game laws vary from country to country. One of the most controversial games of all time, Manhunt 2 by Rockstar Studios, was given an AO rating by the ESRB until Rockstar could make the content more suitable for a mature audience. Video game manufacturers usually exercise tight control over the games that are made available on their systems, so unusual or special-interest games are more likely to appear as PC games. Free, casual, and browser-based games are usually played on available computers, mobile phones, tablet computers or PDAs.		Pan European Game Information (PEGI) is a system that was developed to standardize the game ratings in all of Europe (not just European Union, although the majority are EU members), the current members are: all EU members, except Germany and the 10 accession states; Norway; Switzerland. Iceland is expected to join soon, as are the 10 EU accession states. For all PEGI members, they use it as their sole system, with the exception of the UK, where if a game contains certain material,[89] it must be rated by BBFC. The PEGI ratings are legally binding in Vienna and it is a criminal offence to sell a game to someone if it is rated above their age.[90]		Stricter game rating laws mean that Germany does not operate within the PEGI. Instead, they adopt their own system of certification which is required by law. The Unterhaltungssoftware Selbstkontrolle (USK or Voluntary Certification of Entertainment Software) checks every game before release and assigns an age rating to it – either none (white), 6 years of age (yellow), 12 years of age (green), 16 years of age (blue) or 18 years of age (red). It is forbidden for anyone, retailers, friends or parents alike, to allow a child access to a game for which he or she is underage. If a game is considered to be harmful to young people (for example because of extremely violent, pornographic or racist content), it may be referred to the BPjM (Bundesprüfstelle für jugendgefährdende Medien – Federal Verification Office for Child-Endangering Media) who may opt to place it on the Index upon which the game may not be sold openly or advertised in the open media. Such indexed titles are not "banned" and can still be legally obtained by adults, but it is considered a felony to supply these titles to a child.		According to the market research firm SuperData, as of May 2015, the global games market was worth USD 74.2 billion. By region, North America accounted for $23.6 billion, Asia for $23.1 billion, Europe for $22.1 billion and South America for $4.5 billion. By market segment, mobile games were worth $22.3 billion, retail games 19.7 billion, free-to-play MMOs 8.7 billion, social games $7.9 billion, PC DLC 7.5 billion, and other categories $3 billion or less each.[91][92]		In the United States, also according to SuperData, the share of video games in the entertainment market grew from 5% in 1985 to 13% in 2015, becoming the third-largest market segment behind broadcast and cable television. The research firm anticipated that Asia would soon overtake North America as the largest video game market due to the strong growth of free-to-play and mobile games.[92]		Sales of different types of games vary widely between countries due to local preferences. Japanese consumers tend to purchase much more console games than computer games, with a strong preference for games catering to local tastes.[citation needed] Another key difference is that, despite the decline of arcades in the West, arcade games remain the largest sector of the Japanese gaming industry.[citation needed] In South Korea, computer games are generally preferred over console games, especially MMORPG games and real-time strategy games. Computer games are also popular in China.[93]		Gaming conventions are an important showcase of the industry. The annual gamescom in Cologne in August is the world's leading expo for video games in attendance.[94] The E3 in June in Los Angeles is also of global importance, but is an event for industry insiders only.[95] The Tokyo Game Show in September is the main fair in Asia. Other notable conventions and trade fairs include Brasil Game Show in October, Paris Games Week in October–November, EB Games Expo (Australia) in October, KRI, ChinaJoy in July and the annual Game Developers Conference. Some publishers, developers and technology producers also host their own regular conventions, with BlizzCon, QuakeCon, Nvision and the X shows being prominent examples.		Short for electronic sports, are video game competitions played most by professional players individually or in teams that gained pupularity from the late 2000s, the most common genres are fighting, first-person shooter (FPS), multiplayer online battle arena (MOBA) and real-time strategy. There are certain games that are made for just competitive multiplayer purposes. With those type of games, players focus entirely one choosing the right character or obtaining the right equipment in the game to help them when facing other players. Tournaments are held so that people in the area or from different regions can play against other players of the same game and see who is the best. Major League Gaming (MLG) is a company that reports tournaments that are held across the country. The players that compete in these tournaments are given a rank depending on their skill level in the game that they choose to play in and face other players that play that game. The players that also compete are mostly called professional players for the fact that they have played the game they are competing in for many, long hours. Those players have been able to come up with different strategies for facing different characters. The professional players are able to pick a character to their liking and be able to master how to use that character very effectively. With strategy games, players tend to know how to get resources quick and are able to make quick decisions about where their troops are to be deployed and what kind of troops to create.		There are many video game museums around the world, including the Computer Games Museum in Berlin[96] and the Museum of Soviet Arcade Machines in Moscow and Saint-Petersburg.[97][98] The Museum of Art and Digital Entertainment in Oakland, California is a dedicated video game museum focusing on playable exhibits of console and computer games.[99] The Video Game Museum of Rome is also dedicated to preserving video games and their history.[100] The International Center for the History of Electronic Games at The Strong in Rochester, New York contains one of the largest collections of electronic games and game-related historical materials in the world, including a 5,000-square-foot (460 m2) exhibit which allows guests to play their way through the history of video games.[101][102][103] The Smithsonian Institution in Washington, D.C. has three video games on permanent display: Pac-Man, Dragon's Lair, and Pong.[104]		The Museum of Modern Art has added a total of 20 video games and one video game console to its permanent Architecture and Design Collection since 2012.[105][106] In 2012, the Smithsonian American Art Museum ran an exhibition on "The Art of Video Games".[107] However, the reviews of the exhibit were mixed, including questioning whether video games belong in an art museum.[108][109]				
Wilderness or wildland is a natural environment on Earth that has not been significantly modified by human activity. It may also be defined as: "The most intact, undisturbed wild natural areas left on our planet—those last truly wild places that humans do not control and have not developed with roads, pipelines or other industrial infrastructure."[1] The term is usually limited to terrestrial environments, though the seas and outer space also are little developed by people.		Some governments establish them by law or administrative acts, usually in land tracts that have not been modified by human action in great measure. The main feature of them is that human motorized activity is significantly restricted. These actions seek not only to preserve what already exists, but also to promote and advance a natural expression and development. Wilderness areas can be found in preserves, conservation preserves, National Forests, National Parks and even in urban areas along rivers, gulches or otherwise undeveloped areas. These areas are considered important for the survival of certain species, biodiversity, ecological studies, conservation, solitude, and recreation. Wilderness is deeply valued for cultural, spiritual, moral, and aesthetic reasons. Some nature writers believe wilderness areas are vital for the human spirit and creativity.[2] They may also preserve historic genetic traits and provide habitat for wild flora and fauna that may be difficult to recreate in zoos, arboretums or laboratories.		The word wilderness derives from the notion of "wildness"—in other words, that which is not controlled by humans. The mere presence or activity of people does not disqualify an area from being "wilderness." Many ecosystems that are, or have been, inhabited or influenced by activities of people may still be considered "wild." This way of looking at wilderness includes areas within which natural processes operate without human interference.[citation needed]		The WILD Foundation states that wilderness areas have two dimensions: they must be biologically intact and legally protected.[3][4] The World Conservation Union (IUCN) classifies wilderness at two levels, Ia (Strict Nature Reserves) and Ib (Wilderness Areas). Most scientists and conservationists[by whom?] agree that no place on earth is completely untouched by humanity, either due to past occupation by indigenous people, or through global processes such as climate change. Activities on the margins of specific wilderness areas, such as fire suppression and the interruption of animal migration also affect the interior of wildernesses.[5]		Especially in wealthier, industrialized nations, it has a specific legal meaning as well: as land where development is prohibited by law. Many nations have designated wilderness, including the United States, Australia, Canada, New Zealand, and South Africa. Many new parks are currently being planned and legally passed by various Parliaments and Legislatures at the urging of dedicated individuals around the globe who believe that "in the end, dedicated, inspired people empowered by effective legislation will ensure that the spirit and services of wilderness will thrive and permeate our society, preserving a world that we are proud to hand over to those who come after us."[6]						Looked at through the lens of the visual arts, nature and wildness have been important subjects in various epochs of world history. An early tradition of landscape art occurred in the Tang Dynasty (618-907). The tradition of representing nature as it is became one of the aims of Chinese painting and was a significant influence in Asian art. Artists in the tradition of Shan shui (lit. mountain-water-picture), learned to depict mountains and rivers "from the perspective of nature as a whole and on the basis of their understanding of the laws of nature… as if seen through the eyes of a bird." In the 13th century, Shih Erh Chi recommended avoiding painting "scenes lacking any places made inaccessible by nature."[7]		For most of human history, the greater part of the Earth's terrain was wilderness, and human attention was concentrated in settled areas. The first known laws to protect parts of nature date back to the Babylonian Empire and Chinese Empire. Ashoka, the Great Mauryan King, defined the first laws in the world to protect flora and fauna in Edicts of Ashoka around 3rd Century B.C. In the Middle Ages, the Kings of England initiated one of the world’s first conscious efforts to protect natural areas. They were motivated by a desire to be able to hunt wild animals in private hunting preserves rather than a desire to protect wilderness. Nevertheless, in order to have animals to hunt they would have to protect wildlife from subsistence hunting and the land from villagers gathering firewood.[8] Similar measures were introduced in other European countries.		The idea of wilderness having intrinsic value emerged in the Western world in the 19th century. British artists John Constable and J. M. W. Turner turned their attention to capturing the beauty of the natural world in their paintings. Prior to that, paintings had been primarily of religious scenes or of human beings. William Wordsworth’s poetry described the wonder of the natural world, which had formerly been viewed as a threatening place. Increasingly the valuing of nature became an aspect of Western culture.[8]		By the mid-19th century, in Germany, "Scientific Conservation," as it was called, advocated "the efficient utilization of natural resources through the application of science and technology." Concepts of forest management based on the German approach were applied in other parts of the world, but with varying degrees of success.[9] Over the course of the 19th century wilderness became viewed not as a place to fear but a place to enjoy and protect, hence came the conservation movement in the latter half of the 19th century. Rivers were rafted and mountains were climbed solely for the sake of recreation, not to determine their geographical context.		In 1861, following an intense lobbying by artists (painters), the French Waters and Forests Military Agency set an « artistic reserve » in Fontainebleau State Forest. With a total of 1 097 hectares, it is known to be the first World nature reserve.		Global conservation became an issue at the time of the dissolution of the British Empire in Africa in the late 1940s. The British established great wildlife preserves there. As before, this interest in conservation had an economic motive: in this case, big game hunting. Nevertheless, this led to growing recognition in the 1950s and the early 1960s of the need to protect large spaces for wildlife conservation worldwide. The World Wildlife Fund (WWF), founded in 1961, grew to be one of the largest conservation organizations in the world.[8]		Early conservationists advocated the creation of a legal mechanism by which boundaries could be set on human activities in order to preserve natural and unique lands for the enjoyment and use of future generations. This profound shift in wilderness thought reached a pinnacle in the US with the passage of the Wilderness Act of 1964, which allowed for parts of U.S. National Forests to be designated as "wilderness preserves". Similar acts, such as the 1975 Eastern Wilderness Act, followed.		Nevertheless, initiatives for wilderness conservation continue to increase. There are a growing number of projects to protect tropical rainforests through conservation initiatives. There are also large-scale projects to conserve wilderness regions, such as Canada's Boreal Forest Conservation Framework. The Framework calls for conservation of 50 percent of the 6,000,000 square kilometres of boreal forest in Canada's north.[10] In addition to the World Wildlife Fund, organizations such as the Wildlife Conservation Society, the WILD Foundation, The Nature Conservancy, Conservation International, The Wilderness Society (United States) and many others are active in such conservation efforts.		The 21st century has seen another slight shift in wilderness thought and theory. It is now understood that simply drawing lines around a piece of land and declaring it a wilderness does not necessarily make it a wilderness. All landscapes are intricately connected and what happens outside a wilderness certainly affects what happens inside it. For example, air pollution from Los Angeles and the California Central Valley affects Kern Canyon and Sequoia National Park. The national park has miles of "wilderness" but the air is filled with pollution from the valley. This gives rise to the paradox of what a wilderness really is; a key issue in 21st century wilderness thought.		The creation of National Parks, beginning in the 19th century, preserved some especially attractive and notable areas, but the pursuits of commerce, lifestyle, and recreation combined with increases in human population have continued to result in human modification of relatively untouched areas. Such human activity often negatively impacts native flora and fauna. As such, to better protect critical habitats and preserve low-impact recreational opportunities, legal concepts of "wilderness" were established in many countries, beginning with the United States (see below).		The first National Park was Yellowstone, which was signed into law by U.S. President Ulysses S. Grant on 1 March 1872.[11] The Act of Dedication declared Yellowstone a land "hereby reserved and withdrawn from settlement, occupancy, or sale under the laws of the United States, and dedicated and set apart as a public park or pleasuring ground for the benefit and enjoyment of the people."[12]		The world's second national park, the Royal National Park, located just 32 km to the south of Sydney, Australia, was established in 1879.[13]		The U.S. concept of national parks soon caught on in Canada, which created Banff National Park in 1885, at the same time as the transcontinental Canadian Pacific Railway was being built. The creation of this and other parks showed a growing appreciation of wild nature, but also an economic reality. The railways wanted to entice people to travel west. Parks such as Banff and Yellowstone gained favor as the railroads advertised travel to "the great wild spaces" of North America. When outdoorsman Teddy Roosevelt became president of the United States, he began to enlarge the U.S. National Parks system, and established the National Forest system.[8]		By the 1920s, travel across North America by train to experience the "wilderness" (often viewing it only through windows) had become very popular. This led to the commercialization of some of Canada's National Parks with the building of great hotels such as the Banff Springs Hotel and Chateau Lake Louise.		Despite their similar name, national parks in England and Wales are quite different from national parks in many other countries. Unlike most other countries, in England and Wales, designation as a national park may include substantial settlements and human land uses which are often integral parts of the landscape, and land within a national park remains largely in private ownership. Each park is operated by its own national park authority.		By the later 19th century it had become clear that in many countries wild areas had either disappeared or were in danger of disappearing. This realization gave rise to the conservation movement in the United States, partly through the efforts of writers and activists such as John Burroughs, Aldo Leopold, and John Muir, and politicians such as U.S. President Teddy Roosevelt.		The idea of protecting nature for nature's sake began to gain more recognition in the 1930s with American writers like Aldo Leopold, calling for a "land ethic" and urging wilderness protection. It had become increasingly clear that wild spaces were disappearing rapidly and that decisive action was needed to save them. Wilderness preservation is central to deep ecology; a philosophy that believes in an inherent worth of all living beings, regardless of their instrumental utility to human needs.[14]		Two different groups had emerged within the US environmental movement by the early 20th century: the conservationists and the preservationists. The initial consensus among conservationists was split into "utilitarian conservationists" later to be referred to as conservationists, and "aesthetic conservationists" or preservationists. The main representative for the former was Gifford Pinchot, first Chief of the United States Forest Service, and they focused on the proper use of nature, whereas the preservationists sought the protection of nature from use.[9] Put another way, conservation sought to regulate human use while preservation sought to eliminate human impact altogether. The management of US public lands during the years 1960s and 70s reflected these dual visions, with conservationists dominating the Forest Service, and preservationists the Park Service[15]		The World Conservation Union (IUCN) classifies wilderness at two levels, Ia (Strict Nature Preserves) and Ib (Wilderness areas). For the global standard of wilderness (1b) protection, governance and management, read Wilderness Protected Areas: Management Guidelines for IUCN Category 1b Protected Areas.[16]		Forty-eight countries have wilderness areas established via legislative designation as IUCN protected area management Category 1b sites that do not overlap with any other IUCN designation. They are: Australia, Austria, Bahamas, Bangladesh, Bermuda, Bosnia and Herzegovina, Botswana, Canada, Cayman Islands, Costa Rica, Croatia, Cuba, Czech Republic, Democratic Republic of Congo, Denmark, Dominican Republic, Equatorial Guinea, Estonia, Finland, French Guyana, Greenland, Iceland, India, Indonesia, Japan, Latvia, Liechtenstein, Luxembourg, Malta, Marshall Islands, Mexico, Mongolia, Nepal, New Zealand, Norway, Northern Mariana Islands, Portugal, Seychelles, Serbia, Singapore, Slovakia, Slovenia, Spain, Sri Lanka, Sweden, Tanzania, United States of America, and Zimbabwe. At publication, there are 2,992 marine and terrestrial wilderness areas registered with the IUCN as solely Category 1b sites.[17]		Twenty-two other countries have wilderness areas. These wilderness areas are established via administrative designation or wilderness zones within protected areas. Whereas the above listing contains countries with wilderness exclusively designated as Category 1b sites, some of the below-listed countries contain protected areas with multiple management categories including Category 1b. They are: Argentina, Bhutan, Brazil, Chile, Honduras, Germany, Italy, Kenya, Malaysia, Namibia, Nepal, Pakistan, Panama, Peru, Philippines, the Russian Federation, South Africa, Switzerland, Uganda, Ukraine, the United Kingdom of Great Britain and Northern Ireland, Venezuela, and Zambia.[17]		Since 1861, the French Waters and Forests Military Agency (Administration des Eaux et Forêts) put a strong protection on what was called the « artistic reserve » in Fontainebleau State Forest. With a total of 1 097 hectares, it is known to be the first World nature reserve.		Then in the 1950s,[18] Integral Biological Reserves (Réserves Biologiques Intégrales, RBI) are dedicated to man free ecosystem evolution, on the contrary of Managed Biological reserves (Réserves Biologiques Dirigées, RBD) where a specific management is applied to conserve vulnerable species or threatened habitats.		Integral Biological Reserves occurs in French State Forests or City Forests and are therefore managed by the National Forests Office. In such reserves, all harvests coupe are forbidden excepted exotic species elimination or track safety works to avoid fallen tree risk to visitors (already existing tracks in or on the edge of the reserve).		At the end of 2014,[19] there were 60 Integral Biological Reserves in French State Forests for a total area of 111 082 hectares and 10 in City Forests for a total of 2 835 hectares.		In Greece there are some parks called "ethniki drimoi" (εθνικοί δρυμοί, national forests) that are under protection of the Greek government. Such parks include: Olympus, Parnassos and Parnitha National Parks.		There are seven wilderness areas in New Zealand as defined by the National Parks Act 1980 and the Conservation Act 1987 that fall well within the IUCN definition. Wilderness areas cannot have any human intervention and can only have indigenous species re-introduced into the area if it is compatible with conservation management strategies.		In New Zealand wilderness areas are remote blocks of land that have high natural character. The Conservation Act 1987 prevents any access by vehicles and livestock, the construction of tracks and buildings, and all indigenous natural resources are protected.[20] They are generally over 40,000 ha in size.[21]		In the United States, a Wilderness Area is an area of federal land set aside by an act of Congress. Human activities in wilderness areas are restricted to scientific study and non-mechanized recreation; horses are permitted but mechanized vehicles and equipment, such as cars and bicycles, are not.		The United States was the first country to officially designate land as "wilderness" through the Wilderness Act of 1964. The Wilderness Act was—and is still—an important part of wilderness designation because it created the legal definition of wilderness and founded the National Wilderness Preservation System. The Wilderness Act defines wilderness as "an area where the earth and its community of life are untrammelled by man, where man himself is a visitor who does not remain."[22]		Wilderness designation helps preserve the natural state of the land and protects flora and fauna by prohibiting development and providing for non-mechanized recreation only.		The first administratively protected wilderness area in the United States was the Gila National Forest. In 1922, Aldo Leopold, then a ranking member of the U.S. Forest Service, proposed a new management strategy for the Gila National Forest. His proposal was adopted in 1924, and 750 thousand acres of the Gila National Forest became the Gila Wilderness.[23]		'The Great Swamp in New Jersey was the first formally designated wilderness refuge in the United States. It was declared a wildlife refuge on 3 November 1960. In 1966 it was declared a National Natural Landmark and, in 1968, it was given wilderness status. Properties in the swamp had been acquired by a small group of residents of the area, who donated the assembled properties to the federal government as a park for perpetual protection. Today the refuge amounts to 7,600 acres (31 km2) that are within thirty miles of Manhattan.[24]		While wilderness designations were originally granted by an Act of Congress for Federal land that retained a "primeval character", meaning that it had not suffered from human habitation or development, the Eastern Wilderness Act of 1975 extended the protection of the NWPS to areas in the eastern States that were not initially considered for inclusion in the Wilderness Act. This act allowed lands that did not meet the constraints of size, roadlessness, or human impact to be designated as wilderness areas under the belief that they could be returned to a "primeval" state through preservation.[25]		Approximately 107,500,000 acres (435,000 km2) are designated as wilderness in the United States. This accounts for 4.82% of the country's total land area; however, 54% of that amount is found in Alaska (recreation and development in Alaskan wilderness is often less restrictive), while only 2.58% of the lower continental United States is designated as wilderness. Following the Omnibus Public Land Management Act of 2009 there are 756 separate wilderness designations in the United States ranging in size from Florida's Pelican Island at 5 acres (20,000 m2) to Alaska's Wrangell-Saint Elias at 9,078,675 acres (36,740.09 km2).		In Western Australia,[26] a Wilderness Area is an area that has a wilderness quality rating of 12 or greater and meets a minimum size threshold of 8,000 hectares in temperate areas or 20,000 hectares in arid and tropical areas. A wilderness area is gazetted under section 62(1)(a) of the Conservation and Land Management Act 1984 by the Minister on any land that is vested in the Conservation Commission of Western Australia.		At the forefront of the international wilderness movement has been The WILD Foundation, its founder Ian Player and its network of sister and partner organizations around the globe. The pioneer World Wilderness Congress in 1977 introduced the wilderness concept as an issue of international importance, and began the process of defining the term in biological and social contexts. Today, this work is continued by many international groups who still look to the World Wilderness Congress as the international venue for wilderness and to The WILD Foundation network for wilderness tools and action. The WILD Foundation also publishes the standard references for wilderness professionals and others involved in the issues: Wilderness Management: Stewardship and Protection of Resources and Values, the International Journal of Wilderness, A Handbook on International Wilderness Law and Policy and Protecting Wild Nature on Native Lands are the backbone of information and management tools for international wilderness issues.		The Wilderness Specialist Group within the World Commission on Protected Areas (WTF/WCPA) of the International Union for the Conservation of Nature (IUCN) plays a critical role in defining legal and management guidelines for wilderness at the international level and is also a clearing-house for information on wilderness issues.[27] The IUCN Protected Areas Classification System defines wilderness as "A large area of unmodified or slightly modified land, and/or sea retaining its natural character and influence, without permanent or significant habitation, which is protected and managed so as to preserve its natural condition (Category 1b)." The WILD Foundation founded the WTF/WCPA in 2002 and remains co-chair.		According to a major study, Wilderness: Earth's Last Wild Places, carried out by Conservation International, 46% of the world's land mass is wilderness. For purposes of this report, "wilderness" was defined as an area that "has 70% or more of its original vegetation intact, covers at least 10,000 square kilometers (3,900 sq mi) and must have fewer than five people per square kilometer."[28] However, an IUCN/UNEP report published in 2003, found that only 10.9% of the world's land mass is currently a Category 1 Protected Area, that is, either a strict nature reserve (5.5%) or protected wilderness (5.4%).[29] Such areas remain relatively untouched by humans. Of course, there are large tracts of lands in National Parks and other protected areas that would also qualify as wilderness. However, many protected areas have some degree of human modification or activity, so a definitive estimate of true wilderness is difficult.		The Wildlife Conservation Society generated a human footprint using a number of indicators, the absence of which indicate wildness: human population density, human access via roads and rivers, human infrastructure for agriculture and settlements and the presence of industrial power (lights visible from space). The society estimates that 26% of the Earth's land mass falls into the category of "Last of the wild." The wildest regions of the world include the tundra, the taiga, the Amazonian rain forest, the Tibetan Plateau, the Australian outback and deserts such as the Sahara, and the Gobi.[30] However, from the 1970s, numerous geoglyphs have been discovered on deforested land in the Amazon rainforest, leading to claims about Pre-Columbian civilizations.[31][32] The BBC's Unnatural Histories claimed that the Amazon rainforest, rather than being a pristine wilderness, has been shaped by man for at least 11,000 years through practices such as forest gardening and terra preta.[33]		It should be noted that the percentage of land area designated "wilderness" does not necessarily reflect a measure of its biodiversity. Of the last natural wilderness areas, the taiga—which is mostly wilderness—represents 11% of the total land mass in the Northern Hemisphere.[34] Tropical rainforest represent a further 7% of the world's land base.[35] Estimates of the Earth's remaining wilderness underscore the rate at which these lands are being developed, with dramatic declines in biodiversity as a consequence.		The American concept of wilderness has been criticized by some nature writers. For example, William Cronon writes that what he calls a wilderness ethic or cult may "teach us to be dismissive or even contemptuous of such humble places and experiences", and that "wilderness tends to privilege some parts of nature at the expense of others", using as an example "the mighty canyon more inspiring than the humble marsh."[36] This is most clearly visible with the fact that nearly all U.S. National Parks preserve spectacular canyons and mountains, and it was not until the 1940s that a swamp became a national park—the Everglades. In the mid-20th century national parks started to protect biodiversity, not simply attractive scenery.		Cronon also believes the passion to save wilderness "poses a serious threat to responsible environmentalism" and writes that it allows people to "give ourselves permission to evade responsibility for the lives we actually lead....to the extent that we live in an urban-industrial civilization but at the same time pretend to ourselves that our real home is in the wilderness".[36]		Michael Pollan has argued that the wilderness ethic leads people to dismiss areas whose wildness is less than absolute. In his book Second Nature, Pollan writes that "once a landscape is no longer 'virgin' it is typically written off as fallen, lost to nature, irredeemable."[37] Another challenge to the conventional notion of wilderness comes from Robert Winkler in his book, Going Wild: Adventures with Birds in the Suburban Wilderness. "On walks in the unpeopled parts of the suburbs," Winkler writes, "I’ve witnessed the same wild creatures, struggles for survival, and natural beauty that we associate with true wilderness."[38] Attempts have been made, as in the Pennsylvania Scenic Rivers Act, to distinguish "wild" from various levels of human influence: in the Act, "wild rivers" are "not impounded", "usually not accessible except by trail", and their watersheds and shorelines are "essentially primitive".[39]		Another source of criticism is that the criteria for wilderness designation is vague and open to interpretation. For example, the Wilderness Act states that wilderness must be roadless. The definition given for roadless is "the absences of roads which have been improved and maintained by mechanical means to insure relatively regular and continuous use."[40] However, there have been added sub-definitions that have, in essence, made this standard unclear and open to interpretation.		Coming from a different direction, some criticism from the Deep Ecology movement argues against conflating "wilderness" with "wilderness reservations", viewing the latter term as an oxymoron that, by allowing the law as a human construct to define nature, unavoidably voids the very freedom and independence of human control that defines wilderness.[41] True wilderness requires the ability of life to undergo speciation with as little interference from humanity as possible.[42] Anthropologist and scholar on wilderness Layla AbdelRahim argues that it is necessary to understand the principles that govern the economies of mutual aid and diversification in wilderness from a non-anthropocentric perspective.[43]		
Exploration is the act of searching for the purpose of discovery of information or resources. Exploration occurs in all non-sessile animal species, including humans. In human history, its most dramatic rise was during the Age of Discovery when European explorers sailed and charted much of the rest of the world for a variety of reasons. Since then, major explorations after the Age of Discovery have occurred for reasons mostly aimed at information discovery.		In scientific research, exploration is one of three purposes of empirical research (the other two being description and explanation). The term is commonly used metaphorically. For example, an individual may speak of exploring the Internet, sexuality, etc.						The Phoenicians (1550 BCE–300 BCE) traded throughout the Mediterranean Sea and Asia Minor though many of their routes are still unknown today. The presence of tin in some Phoenician artifacts suggests that they may have traveled to Britain. According to Virgil's Aeneid and other ancient sources, the legendary Queen Dido was a Phoenician from Tyre who sailed to North Africa and founded the city of Carthage.		Hanno the Navigator (500 BC), a Carthaginean navigator explored the Western Coast of Africa.		The Greek explorer from Marseille, Pytheas (380 – c. 310 BC) was the first to circumnavigate Great Britain, explore Germany, and reach Thule (most commonly thought to be the Shetland Islands or Iceland).		The Romans organized expeditions to cross the Sahara desert with five different routes:		All these expeditions were supported by legionaries and had mainly a commercial purpose. Only the one done by emperor Nero seemed to be a preparative for the conquest of Ethiopia or Nubia: in 62 AD two legionaries explored the sources of the Nile river.		One of the main reasons of the explorations was to get gold using the camel to transport it.[1]		The explorations near the African western and eastern coasts were supported by Roman ships and deeply related to the naval commerce (mainly toward the Indian Ocean). Romans organized several explorations also in Northern Europe, and as far as Asia up to China .		During the 2nd century BC, the Han dynasty explored much of the Eastern Northern Hemisphere. Starting in 139 BC, the Han diplomat Zhang Qian traveled west in an unsuccessful attempt to secure an alliance with the Da Yuezhi against the Xiongnu (the Yuezhi had been evicted from Gansu by the Xiongnu in 177 BC); however, Zhang's travels discovered entire countries which the Chinese were unaware of, including the remnants of the conquests of Alexander the Great (r. 336–323 BC).[2] When Zhang returned to China in 125 BC, he reported on his visits to Dayuan (Fergana), Kangju (Sogdiana), and Daxia (Bactria, formerly the Greco-Bactrian Kingdom which had just been subjugated by the Da Yuezhi).[3] Zhang described Dayuan and Daxia as agricultural and urban countries like China, and although he did not venture there, described Shendu (the Indus River valley of Northwestern India) and Anxi (Arsacid territories) further west.[4]		From about 800 Ad to 1040 AD, the Vikings explored Europe and much of the Western Northern Hemisphere via rivers and oceans. For example, it is known that the Norwegian Viking explorer, Erik the Red (950–1003), sailed to and settled in Greenland after being expelled from Iceland, while his son, the Icelandic explorer Leif Ericson (980–1020), reached Newfoundland and the nearby North American coast, and is believed to be the first European to land in North America.		Polynesians were a maritime people, who populated and explored the central and south Pacific for around 5,000 years, up to about 1280 when they discovered New Zealand. The key invention to their exploration was the outrigger canoe, which provided a swift and stable platform for carrying goods and people. Based on limited evidence, it is thought that the voyage to New Zealand was deliberate. It is unknown if one or more boats went to New Zealand, or the type of boat, or the names of those who migrated. 2011 studies at Wairau Bar in New Zealand show a high probability that one origin was Ruahine Island in the Society Islands. Polynesians may have used the prevailing north easterly trade winds to reach New Zealand in about three weeks. The Cook Islands are in direct line along the migration path and may have been an intermediate stopping point. There are cultural and language similarities between Cook Islanders and New Zealand Maori. Early Maori had different legends of their origins, but the stories were misunderstood and reinterpreted in confused written accounts by early European historians in New Zealand trying to present a coherent pattern of Maori settlement in New Zealand.		Mathematical modelling based on DNA genome studies, using state-of-the-art techniques, have shown that a large number of Polynesian migrants (100–200), including women, arrived in New Zealand around the same time, in about 1280. Otago University studies have tried to link distinctive DNA teeth patterns, which show special dietary influence, with places in or nearby the Society Islands.[5]		The Chinese explorer, Wang Dayuan (fl. 1311–1350) made two major trips by ship to the Indian Ocean. During 1328–1333, he sailed along the South China Sea and visited many places in Southeast Asia and reached as far as South Asia, landing in Sri Lanka and India. Then in 1334–1339, he visited North Africa and East Africa. Later, the Chinese admiral Zheng He (1371–1433) made seven voyages to Arabia, East Africa, India, Indonesia and Thailand.		The Age of Discovery, also known as the Age of Exploration, is one of the most important periods of geographical exploration in human history. It started in the early 15th century and lasted until the 17th century. In that period, Europeans discovered and/or explored vast areas of the Americas, Africa, Asia and Oceania. Portugal and Spain dominated the first stages of exploration, while other European nations followed, such as England, Netherlands, and France.		The most important explorers of this period include: Diogo Cão (c.1452 –c.1486) who discovered and ascended the Congo River and reached the coasts of the present-day Angola and Namibia; Bartolomeu Dias (c. 1450–1500), who was the first European to reach the Cape of Good Hope and other parts of the South African coast; Christopher Columbus (1451–1506), who led a Castilian (Spanish) expedition across the Atlantic, discovering America; Vasco da Gama (1460–1524), a navigator who made the first trip from Europe to India and back by the Cape of Good Hope, discovering the ocean route to the East; Pedro Alvares Cabral (c. 1467/68–c.1520) who, following the path of Gama, claimed Brazil and led the first expedition that linked Europe, Africa, America, and Asia; Diogo Dias, who discovered the eastern coast of Madagascar and rounded the corner of Africa; explorers such as Diogo Fernandes Pereira and Pedro Mascarenhas (1470–1555), among others, who discovered and mapped the Mascarene Islands and other archipelagos; António de Abreu (c.1480–c.1514) and Francisco Serrão (14?–1521), who led the first direct European fleet into the Pacific Ocean (on its western edges), through the Sunda Islands, reaching the Moluccas; Juan Ponce de León (1474–1521), who discovered and mapped the coast of Florida; Vasco Núñez de Balboa (c. 1475–1519), who was the first European to view the Pacific Ocean from American shores (after crossing the Isthmus of Panama) confirming that America was a separate continent from Asia; Ferdinand Magellan (1480–1521), who was the first navigator to cross the Pacific Ocean, discovering the Strait of Magellan, the Tuamotus and Mariana Islands, achieving a nearly complete circumnavigation of the Earth, in multiple voyages, for the first time; Juan Sebastian Elcano (1476–1526), who completed the first global circumnavigation; Aleixo Garcia (14?–1527), who explored the territories of present-day southern Brazil, Paraguay and Bolivia, crossing the Chaco and reaching the Andes (near Sucre); Jorge de Menezes (c. 1498–?), who discovered Papua New Guinea; García Jofre de Loaísa (1490–1526), who discovered the Marshall Islands; Álvar Núñez Cabeza de Vaca (1490–1558), who discovered the Mississippi River and was the first European to sail the Gulf of Mexico and cross Texas; Jacques Cartier (1491–1557), who drew the first maps of part of central and maritime Canada; Andres de Urdaneta (1498–1568), who discovered the maritime route from Asia to the Americas; Francisco Vázquez de Coronado (1510–1554), who discovered the Grand Canyon and the Colorado River; Francisco de Orellana (1511–1546), who was the first European to navigate the length of the Amazon River.		Continuing in the second half of the 16th century and the 17th century with explorers such as Andrés de Urdaneta (1498–1568), who discovered the maritime route from Asia to the Americas; Álvaro de Mendaña (1542–1595), who discovered the Tuvalu archipelago, the Marquesas, the Solomon Islands and Wake Island; Willem Janszoon (1570–1630), who made the first recorded European landing in Australia; Pedro Fernandes de Queirós (1565–1614), who discovered the Pitcairn Islands and the Vanuatu archipelago; Yñigo Ortiz de Retez, who discovered and reached eastern and northern New Guinea; Luis Váez de Torres (1565–1613), who discovered the Torres Strait between Australia and New Guinea; Henry Hudson (156?–1611), who explored the Hudson Bay in Canada; Samuel de Champlain (1574–1635), who explored St. Lawrence River and the Great Lakes (in Canada and northern United States); Abel Tasman (1603–1659), who explored North Australia, discovered Tasmania and New Zealand; and René-Robert Cavelier, Sieur de La Salle (1643–1687), who explored the Great Lakes region of the United States and Canada, and the entire length of the Mississippi River.		Long after the golden age of discovery, other explorers completed the world map, such as various Russians explorers, reaching the Siberian Pacific coast and the Bering Strait, at the extreme edge of Asia and Alaska (North America); Vitus Bering (1681–1741) who in the service of the Russian Navy, explored the Bering Strait, the Bering Sea, the North American coast of Alaska, and some other northern areas of the Pacific Ocean; and James Cook, who explored the east coast of Australia, the Hawaiian Islands, and circumnavigated the Antarctic continent.		Humanity is continuing to follow the impulse to explore, moving beyond Earth. Space exploration started in the 20th century with the invention of exo-atmospheric rockets. This has given humans the opportunity to travel to the Moon, and to send robotic explorers to other planets and far beyond.		Both of the Voyager probes have left the Solar System, bearing imprinted gold discs with multiple data types.		A recent scientific study, performed on mobile phone data of an entire European country and on GPS tracks of private vehicles in Italy, demonstrated that even today individuals naturally split into two well-defined categories according to their mobility habits: returners and explorers.[6] According to this research published on Nature Communications,[6] today's explorers have a tendency to wander between a large number of different locations showing a star-like mobility pattern: they have a central core of locations (composed by home and work places) around which distant core of locations gravitates.[6][7] Interestingly, two features characterize today's explorers: 1) they are more likely to spread infectious diseases when traveling, due to their mobility patterns; 2) they tend to communicate preferably with other explorers, i.e. they show a communication homophily with people in the same mobility category.[6]		Humans have developed specialized tools and strategies to explore specific areas of the Earth, including the arctic, caves, deserts, oceans, urban environments, as well as the Moon. With robotic machines, humans have also explored many parts of the heliosphere, and through measurements, beyond the Solar System and the Milky Way as part of an ongoing global space exploration initiative.		
Charles Lutwidge Dodgson (/ˈtʃɑːrlz ˈlʌtwɪdʒ ˈdɒdsən/;[1][2][3] 27 January 1832 – 14 January 1898), better known by his pen name Lewis Carroll (/ˈkærəl/), was an English writer, mathematician, logician, Anglican deacon, and photographer. His most famous writings are Alice's Adventures in Wonderland, its sequel Through the Looking-Glass, which includes the poem "Jabberwocky", and the poem The Hunting of the Snark, all examples of the genre of literary nonsense. He is noted for his facility at word play, logic and fantasy. There are societies in many parts of the world[4] dedicated to the enjoyment and promotion of his works and the investigation of his life.						Dodgson's family was predominantly northern English, with Irish connections, conservative and High Church Anglican. Most of Dodgson's male ancestors were army officers or Church of England clergy. His great-grandfather, also named Charles Dodgson, had risen through the ranks of the church to become the Bishop of Elphin.[5] His paternal grandfather, another Charles, had been an army captain, killed in action in Ireland in 1803 when his two sons were hardly more than babies.[6] The older of these sons – yet another Charles Dodgson – was Carroll's father. He went to Westminster School and then to Christ Church, Oxford. He reverted to the other family tradition and took holy orders. He was mathematically gifted and won a double first degree, which could have been the prelude to a brilliant academic career. Instead, he married his first cousin Frances Jane Lutwidge[7] in 1830 and became a country parson.[8]		Dodgson was born in the small parsonage at Daresbury in Cheshire near the towns of Warrington and Runcorn,[9] the eldest boy but already the third child of the four-and-a-half-year-old marriage. Eight more children followed. When Charles was 11, his father was given the living of Croft-on-Tees in North Yorkshire, and the whole family moved to the spacious rectory. This remained their home for the next 25 years.		Charles's father was an active and highly conservative cleric of the Church of England who later became the Archdeacon of Richmond[10] and involved himself, sometimes influentially, in the intense religious disputes that were dividing the church. He was High Church, inclining to Anglo-Catholicism, an admirer of John Henry Newman and the Tractarian movement, and did his best to instil such views in his children. Young Charles was to develop an ambiguous relationship with his father's values and with the Church of England as a whole.[11]		During his early youth, Dodgson was educated at home. His "reading lists" preserved in the family archives testify to a precocious intellect: at the age of seven, he was reading books such as The Pilgrim's Progress. He also suffered from a stammer – a condition shared by most of his siblings[12] – that often influenced his social life throughout his years. At the age of twelve, he was sent to Richmond Grammar School (now part of Richmond School) at nearby Richmond.		In 1846, Dodgson entered Rugby School where he was evidently unhappy, as he wrote some years after leaving:		I cannot say ... that any earthly considerations would induce me to go through my three years again ... I can honestly say that if I could have been ... secure from annoyance at night, the hardships of the daily life would have been comparative trifles to bear.[13]		Scholastically, though, he excelled with apparent ease. "I have not had a more promising boy at his age since I came to Rugby", observed mathematics master R. B. Mayor.[14]		He left Rugby at the end of 1849 and matriculated at Oxford in May 1850 as a member of his father's old college, Christ Church.[15] After waiting for rooms in college to become available, he went into residence in January 1851.[16] He had been at Oxford only two days when he received a summons home. His mother had died of "inflammation of the brain" – perhaps meningitis or a stroke – at the age of 47.[16]		His early academic career veered between high promise and irresistible distraction. He did not always work hard but was exceptionally gifted and achievement came easily to him. In 1852, he obtained first-class honours in Mathematics Moderations and was shortly thereafter nominated to a Studentship by his father's old friend Canon Edward Pusey.[17][18] In 1854, he obtained first-class honours in the Final Honours School of Mathematics, standing first on the list, graduating Bachelor of Arts.[19][20] He remained at Christ Church studying and teaching, but the next year he failed an important scholarship through his self-confessed inability to apply himself to study.[21][22] Even so, his talent as a mathematician won him the Christ Church Mathematical Lectureship in 1855,[23] which he continued to hold for the next 26 years.[24] Despite early unhappiness, Dodgson was to remain at Christ Church, in various capacities, until his death.[25]		The young adult Charles Dodgson was about 6 feet (1.83 m) tall and slender, and he had curly brown hair and blue or grey eyes (depending on the account). He was described in later life as somewhat asymmetrical, and as carrying himself rather stiffly and awkwardly, although this might be on account of a knee injury sustained in middle age. As a very young child, he suffered a fever that left him deaf in one ear. At the age of 17, he suffered a severe attack of whooping cough, which was probably responsible for his chronically weak chest in later life. Another defect which he carried into adulthood was what he referred to as his "hesitation", a stammer that he acquired in early childhood and which plagued him throughout his life.[25]		The stammer has always been a significant part of the image of Dodgson. It is said that he stammered only in adult company and was free and fluent with children, but there is no evidence to support this idea.[26] Many children of his acquaintance remembered the stammer, while many adults failed to notice it. Dodgson himself seems to have been far more acutely aware of it than most people whom he met; it is said that he caricatured himself as the Dodo in Alice's Adventures in Wonderland, referring to his difficulty in pronouncing his last name, but this is one of the many "facts" often repeated for which no first-hand evidence remains. He did indeed refer to himself as the dodo, but whether or not this reference was to his stammer is simply speculation.[25]		Dodgson's stammer did trouble him, but it was never so debilitating that it prevented him from applying his other personal qualities to do well in society. He lived in a time when people commonly devised their own amusements and when singing and recitation were required social skills, and the young Dodgson was well equipped to be an engaging entertainer. He reportedly could sing tolerably well and was not afraid to do so before an audience. He was adept at mimicry and storytelling, and was reputedly quite good at charades.[25]		In the interim between his early published writings and the success of the Alice books, Dodgson began to move in the pre-Raphaelite social circle. He first met John Ruskin in 1857 and became friendly with him. He developed a close relationship with Dante Gabriel Rossetti and his family, and also knew William Holman Hunt, John Everett Millais, and Arthur Hughes, among other artists. He knew fairy-tale author George MacDonald well – it was the enthusiastic reception of Alice by the young MacDonald children that persuaded him to submit the work for publication.[25][27]		In broad terms, Dodgson has traditionally been regarded as politically, religiously, and personally conservative. Martin Gardner labels Dodgson as a Tory who was "awed by lords and inclined to be snobbish towards inferiors."[28] The Reverend W. Tuckwell, in his Reminiscences of Oxford (1900), regarded him as "austere, shy, precise, absorbed in mathematical reverie, watchfully tenacious of his dignity, stiffly conservative in political, theological, social theory, his life mapped out in squares like Alice's landscape."[29] In The Life and Letters of Lewis Carroll, the editor states that "his Diary is full of such modest depreciations of himself and his work, interspersed with earnest prayers (too sacred and private to be reproduced here) that God would forgive him the past, and help him to perform His holy will in the future."[30] When a friend asked him about his religious views, Dodgson wrote in response that he was a member of the Church of England, but "doubt[ed] if he was fully a 'High Churchman'". He added:		I believe that when you and I come to lie down for the last time, if only we can keep firm hold of the great truths Christ taught us—our own utter worthlessness and His infinite worth; and that He has brought us back to our one Father, and made us His brethren, and so brethren to one another—we shall have all we need to guide us through the shadows. Most assuredly I accept to the full the doctrines you refer to—that Christ died to save us, that we have no other way of salvation open to us but through His death, and that it is by faith in Him, and through no merit of ours, that we are reconciled to God; and most assuredly I can cordially say, "I owe all to Him who loved me, and died on the Cross of Calvary."		Dodgson also expressed interest in other fields. He was an early member of the Society for Psychical Research, and one of his letters suggests that he accepted as real what was then called "thought reading".[32] Dodgson wrote some studies of various philosophical arguments. In 1895, he developed a philosophical regressus-argument on deductive reasoning in his article "What the Tortoise Said to Achilles", which appeared in one of the early volumes of Mind.[33] The article was reprinted in the same journal a hundred years later in 1995, with a subsequent article by Simon Blackburn titled "Practical Tortoise Raising".[34]		From a young age, Dodgson wrote poetry and short stories, contributing heavily to the family magazine Mischmasch and later sending them to various magazines, enjoying moderate success. Between 1854 and 1856, his work appeared in the national publications The Comic Times and The Train, as well as smaller magazines such as the Whitby Gazette and the Oxford Critic. Most of this output was humorous, sometimes satirical, but his standards and ambitions were exacting. "I do not think I have yet written anything worthy of real publication (in which I do not include the Whitby Gazette or the Oxonian Advertiser), but I do not despair of doing so some day," he wrote in July 1855.[25] Sometime after 1850, he did write puppet plays for his siblings' entertainment, of which one has survived: La Guida di Bragia.[35]		In 1856, he published his first piece of work under the name that would make him famous. A romantic poem called "Solitude" appeared in The Train under the authorship of "Lewis Carroll". This pseudonym was a play on his real name: Lewis was the anglicised form of Ludovicus, which was the Latin for Lutwidge, and Carroll an Irish surname similar to the Latin name Carolus, from which comes the name Charles.[8] The transition went as follows: "Charles Lutwidge" translated into Latin as "Carolus Ludovicus". This was then translated back into English as "Carroll Lewis" and then reversed to make "Lewis Carroll".[36] This pseudonym was chosen by editor Edmund Yates from a list of four submitted by Dodgson, the others being Edgar Cuthwellis, Edgar U. C. Westhill, and Louis Carroll.[37]		In 1856, Dean (i.e., head of the college) Henry Liddell arrived at Christ Church, bringing with him his young family, all of whom would figure largely in Dodgson's life over the following years, and would greatly influence his writing career. Dodgson became close friends with Liddell's wife Lorina and their children, particularly the three sisters Lorina, Edith, and Alice Liddell. He was widely assumed for many years to have derived his own "Alice" from Alice Liddell; the acrostic poem at the end of Through the Looking Glass spells out her name in full, and there are also many superficial references to her hidden in the text of both books. It has been noted that Dodgson himself repeatedly denied in later life that his "little heroine" was based on any real child,[38][39] and he frequently dedicated his works to girls of his acquaintance, adding their names in acrostic poems at the beginning of the text. Gertrude Chataway's name appears in this form at the beginning of The Hunting of the Snark, and it is not suggested that this means that any of the characters in the narrative are based on her.[39]		Information is scarce (Dodgson's diaries for the years 1858–1862 are missing), but it seems clear that his friendship with the Liddell family was an important part of his life in the late 1850s, and he grew into the habit of taking the children on rowing trips (first the boy Harry, and later the three girls) accompanied by an adult friend[40] to nearby Nuneham Courtenay or Godstow.[41]		It was on one such expedition on 4 July 1862 that Dodgson invented the outline of the story that eventually became his first and greatest commercial success. He told the story to Alice Liddell and she begged him to write it down, and Dodgson eventually (after much delay) presented her with a handwritten, illustrated manuscript entitled Alice's Adventures Under Ground in November 1864.[41]		Before this, the family of friend and mentor George MacDonald read Dodgson's incomplete manuscript, and the enthusiasm of the MacDonald children encouraged Dodgson to seek publication. In 1863, he had taken the unfinished manuscript to Macmillan the publisher, who liked it immediately. After the possible alternative titles were rejected – Alice Among the Fairies and Alice's Golden Hour – the work was finally published as Alice's Adventures in Wonderland in 1865 under the Lewis Carroll pen-name, which Dodgson had first used some nine years earlier.[27] The illustrations this time were by Sir John Tenniel; Dodgson evidently thought that a published book would need the skills of a professional artist. Annotated versions provide insights into many of the ideas and hidden meanings that are prevalent in these books.[42][43] Critical literature has often proposed Freudian interpretations of the book as "a descent into the dark world of the subconscious", as well as seeing it as a satire upon contemporary mathematical advances.[44][45]		The overwhelming commercial success of the first Alice book changed Dodgson's life in many ways. The fame of his alter ego "Lewis Carroll" soon spread around the world. He was inundated with fan mail and with sometimes unwanted attention. Indeed, according to one popular story, Queen Victoria herself enjoyed Alice In Wonderland so much that she commanded that he dedicate his next book to her, and was accordingly presented with his next work, a scholarly mathematical volume entitled An Elementary Treatise on Determinants.[46][47] Dodgson himself vehemently denied this story, commenting "... It is utterly false in every particular: nothing even resembling it has occurred";[47][48] and it is unlikely for other reasons. As T.B. Strong comments in a Times article, "It would have been clean contrary to all his practice to identify [the] author of Alice with the author of his mathematical works".[49][50] He also began earning quite substantial sums of money but continued with his seemingly disliked post at Christ Church.[27]		Late in 1871, he published the sequel Through the Looking-Glass and What Alice Found There. (The title page of the first edition erroneously gives "1872" as the date of publication.[51]) Its somewhat darker mood possibly reflects changes in Dodgson's life. His father's death in 1868 plunged him into a depression that lasted some years.[27]		In 1876, Dodgson produced his next great work The Hunting of the Snark, a fantastical "nonsense" poem exploring the adventures of a bizarre crew of nine tradesmen and one beaver, who set off to find the snark. Painter Dante Gabriel Rossetti reputedly became convinced that the poem was about him.[27]		In 1895, 30 years after publication of his masterpieces, Carroll attempted a comeback, producing a two-volume tale of the fairy siblings Sylvie and Bruno. Carroll entwines two plots set in two alternative worlds, one set in rural England and the other in the fairytale kingdoms of Elfland, Outland, and others. The fairytale world satirizes English society, and more specifically the world of academia. Sylvie and Bruno came out in two volumes and is considered a lesser work, although it has remained in print for over a century.		In 1856, Dodgson took up the new art form of photography under the influence first of his uncle Skeffington Lutwidge, and later of his Oxford friend Reginald Southey.[52] He soon excelled at the art and became a well-known gentleman-photographer, and he seems even to have toyed with the idea of making a living out of it in his very early years.[27]		A study by Roger Taylor and Edward Wakeling exhaustively lists every surviving print, and Taylor calculates that just over half of his surviving work depicts young girls, though about 60% of his original photographic portfolio is now missing.[53] Dodgson also made many studies of men, women, boys, and landscapes; his subjects also include skeletons, dolls, dogs, statues, paintings, and trees.[54] His pictures of children were taken with a parent in attendance and many of the pictures were taken in the Liddell garden because natural sunlight was required for good exposures.[40]		He also found photography to be a useful entrée into higher social circles.[55] During the most productive part of his career, he made portraits of notable sitters such as John Everett Millais, Ellen Terry, Dante Gabriel Rossetti, Julia Margaret Cameron, Michael Faraday, Lord Salisbury, and Alfred, Lord Tennyson.[27]		By the time that Dodgson abruptly ceased photography (1880, over 24 years), he had established his own studio on the roof of Tom Quad, created around 3,000 images, and was an amateur master of the medium, though fewer than 1,000 images have survived time and deliberate destruction. He stopped taking photographs because keeping his studio working was too time-consuming.[56] He used the wet collodion process; commercial photographers who started using the dry-plate process in the 1870s took pictures more quickly.[57] Popular taste changed with the advent of Modernism, affecting the types of photographs that he produced.		To promote letter writing, Dodgson invented "The Wonderland Postage-Stamp Case" in 1889. This was a cloth-backed folder with twelve slots, two marked for inserting the most commonly used penny stamp, and one each for the other current denominations up to one shilling. The folder was then put into a slipcase decorated with a picture of Alice on the front and the Cheshire Cat on the back. It intended to organize stamps wherever one stored their writing utensils; Carroll expressly notes in Eight or Nine Wise Words About Letter-Writing it is not intended to be carried in a pocket or purse, as the most common individual stamps could easily be carried on their own. The pack included a copy of a pamphletted version of this lecture.[58][59]		Another invention was a writing tablet called the nyctograph that allowed note-taking in the dark, thus eliminating the need to get out of bed and strike a light when one woke with an idea. The device consisted of a gridded card with sixteen squares and system of symbols representing an alphabet of Dodgson's design, using letter shapes similar to the Graffiti writing system on a Palm device.[60]		He also devised a number of games, including an early version of what today is known as Scrabble. He appears to have invented – or at least certainly popularized – the "doublet" (see word ladder), a form of brain-teaser that is still popular today, changing one word into another by altering one letter at a time, each successive change always resulting in a genuine word. For instance, CAT is transformed into DOG by the following steps: CAT, COT, DOT, DOG.[27] The games and puzzles of Lewis Carroll were the subject of Martin Gardner's March 1960 Mathematical Games column in Scientific American.		Other items include a rule for finding the day of the week for any date; a means for justifying right margins on a typewriter; a steering device for a velociam (a type of tricycle); new systems of parliamentary representation;[61] more fair elimination rules for tennis tournaments; a new sort of postal money order; rules for reckoning postage; rules for a win in betting; rules for dividing a number by various divisors; a cardboard scale for the Senior Common Room at Christ Church which, held next to a glass, ensured the right amount of liqueur for the price paid; a double-sided adhesive strip to fasten envelopes or mount things in books; a device for helping a bedridden invalid to read from a book placed sideways; and at least two ciphers for cryptography.[27]		Within the academic discipline of mathematics, Dodgson worked primarily in the fields of geometry, linear and matrix algebra, mathematical logic, and recreational mathematics, producing nearly a dozen books under his real name. Dodgson also developed new ideas in linear algebra (e.g., the first printed proof of the Kronecker-Capelli theorem),[62][63] probability, and the study of elections (e.g., Dodgson's method) and committees; some of this work was not published until well after his death. His occupation as Mathematical Lecturer at Christ Church gave him some financial security.[64]		His mathematical work attracted renewed interest in the late 20th century. Martin Gardner's book on logic machines and diagrams and William Warren Bartley's posthumous publication of the second part of Carroll's symbolic logic book have sparked a reevaluation of Carroll's contributions to symbolic logic.[65][66][67] Robbins' and Rumsey's investigation[68] of Dodgson condensation, a method of evaluating determinants, led them to the Alternating Sign Matrix conjecture, now a theorem. The discovery in the 1990s of additional ciphers that Carroll had constructed, in addition to his "Memoria Technica", showed that he had employed sophisticated mathematical ideas in their creation.[69]		Dodgson wrote and received as many as 98,721 letters, according to a special letter register which he devised. He documented his advice about how to write more satisfying letters in a missive entitled "Eight or Nine Wise Words About Letter-Writing".[70]		Dodgson's existence remained little changed over the remaining twenty years of his life, throughout his growing wealth and fame. He continued to teach at Christ Church until 1881 and remained in residence there until his death. The two volumes of his last novel Sylvie and Bruno were published in 1889 and 1893, but the intricacy of this work was apparently not appreciated by contemporary readers; it achieved nothing like the success of the Alice books, with disappointing reviews and sales of only 13,000 copies.[71][72]		The only known occasion on which he travelled abroad was a trip to Russia in 1867 as an ecclesiastic, together with the Reverend Henry Liddon. He recounts the travel in his "Russian Journal", which was first commercially published in 1935.[73] On his way to Russia and back, he also saw different cities in Belgium, Germany, partitioned Poland, and France.		He died of pneumonia following influenza on 14 January 1898 at his sisters' home, "The Chestnuts", in Guildford. He was two weeks away from turning 66 years old. His funeral was held at the nearby St Mary's Church.[74] He is buried in Guildford at the Mount Cemetery.[27]		Some late twentieth-century biographers have suggested that Dodgson's interest in children had an erotic element, including Morton N. Cohen in his Lewis Carroll: A Biography (1995),[75] Donald Thomas in his Lewis Carroll: A Portrait with Background (1995), and Michael Bakewell in his Lewis Carroll: A Biography (1996). Cohen, in particular, claims that Dodgson's "sexual energies sought unconventional outlets", and further writes:		We cannot know to what extent sexual urges lay behind Charles's preference for drawing and photographing children in the nude. He contended the preference was entirely aesthetic. But given his emotional attachment to children as well as his aesthetic appreciation of their forms, his assertion that his interest was strictly artistic is naïve. He probably felt more than he dared acknowledge, even to himself.[76]		Cohen goes on to note that Dodgson "apparently convinced many of his friends that his attachment to the nude female child form was free of any eroticism", but adds that "later generations look beneath the surface" (p. 229). He argues that Dodgson may have wanted to marry the 11-year-old Alice Liddell, and that this was the cause of the unexplained "break" with the family in June 1863,[27] an event for which other explanations are offered. Biographers Derek Hudson and Roger Lancelyn Green stop short of identifying Dodgson as a paedophile (Green also edited Dodgson's diaries and papers), but they concur that he had a passion for small female children and next to no interest in the adult world. Catherine Robson refers to Carroll as "the Victorian era's most famous (or infamous) girl lover".[77]		Several other writers and scholars have challenged the evidential basis for Cohen's and others' views about this interest of Dodgson. Lebailly has endeavoured to set Dodgson's child-photography within the "Victorian Child Cult", which perceived child-nudity as essentially an expression of innocence. Lebailly claims that studies of child nudes were mainstream and fashionable in Dodgson's time, and that most photographers made them as a matter of course, including Oscar Gustave Rejlander and Julia Margaret Cameron. Lebailly continues that child nudes even appeared on Victorian Christmas cards, implying a very different social and aesthetic assessment of such material. Lebailly concludes that it has been an error of Dodgson's biographers to view his child-photography with 20th- or 21st-century eyes, and to have presented it as some form of personal idiosyncrasy, when it was in fact a response to a prevalent aesthetic and philosophical movement of the time.		Karoline Leach's reappraisal of Dodgson focused in particular on his controversial sexuality. She argues that the allegations of paedophilia rose initially from a misunderstanding of Victorian morals, as well as the mistaken idea – fostered by Dodgson's various biographers – that he had no interest in adult women. She termed the traditional image of Dodgson "the Carroll Myth". She drew attention to the large amounts of evidence in his diaries and letters that he was also keenly interested in adult women, married and single, and enjoyed several relationships with them that would have been considered scandalous by the social standards of his time. She also pointed to the fact that many of those whom he described as "child-friends" were girls in their late teens and even twenties.[78] She argues that suggestions of paedophilia emerged only many years after his death, when his well-meaning family had suppressed all evidence of his relationships with women in an effort to preserve his reputation, thus giving a false impression of a man interested only in little girls. Similarly, Leach points to a 1932 biography by Langford Reed as the source of the dubious claim that many of Carroll's female friendships ended when the girls reached the age of fourteen.[79]		In addition to the biographical works that have discussed Dodgson's sexuality, there are modern artistic interpretations of his life and work that do so as well — in particular, Dennis Potter in his play Alice and his screenplay for the motion picture Dreamchild, and Robert Wilson in his film Alice.		Dodgson had been groomed for the ordained ministry in the Church of England from a very early age and was expected to be ordained within four years of obtaining his master's degree, as a condition of his residency at Christ Church. He delayed the process for some time but was eventually ordained as a deacon on 22 December 1861. But when the time came a year later to be ordained as a priest, Dodgson appealed to the dean for permission not to proceed. This was against college rules and, initially, Dean Liddell told him that he would have to consult the college ruling body, which would almost certainly have resulted in his being expelled. For unknown reasons, Liddell changed his mind overnight and permitted Dodgson to remain at the college in defiance of the rules.[80] Dodgson never became a priest, unique amongst senior students of his time.		There is currently no conclusive evidence about why Dodgson rejected the priesthood. Some have suggested that his stammer made him reluctant to take the step, because he was afraid of having to preach.[81] Wilson[82] quotes letters by Dodgson describing difficulty in reading lessons and prayers rather than preaching in his own words. But Dodgson did indeed preach in later life, even though not in priest's orders, so it seems unlikely that his impediment was a major factor affecting his choice.[citation needed] Wilson also points out that the Bishop of Oxford, Samuel Wilberforce, who ordained Dodgson, had strong views against clergy going to the theatre, one of Dodgson's great interests. Others have suggested that he was having serious doubts about Anglicanism.[citation needed] He was interested in minority forms of Christianity (he was an admirer of F. D. Maurice) and "alternative" religions (theosophy).[83] Dodgson became deeply troubled by an unexplained sense of sin and guilt at this time (the early 1860s) and frequently expressed the view in his diaries that he was a "vile and worthless" sinner, unworthy of the priesthood and this sense of sin and unworthiness may well have affected his decision to abandon being ordained to the priesthood.[84]		At least four complete volumes and around seven pages of text are missing from Dodgson's 13 diaries.[85] The loss of the volumes remains unexplained; the pages have been removed by an unknown hand. Most scholars assume that the diary material was removed by family members in the interests of preserving the family name, but this has not been proven.[86] Except for one page, material is missing from his diaries for the period between 1853 and 1863 (when Dodgson was 21–31 years old).[87][88] This was a period when Dodgson began suffering great mental and spiritual anguish and confessing to an overwhelming sense of his own sin. This was also the period of time when he composed his extensive love poetry, leading to speculation that the poems may have been autobiographical.[89][90]		Many theories have been put forward to explain the missing material. A popular explanation for one missing page (27 June 1863) is that it might have been torn out to conceal a proposal of marriage on that day by Dodgson to the 11-year-old Alice Liddell. However, there has never been any evidence to suggest that this was so, and a paper offers some evidence to the contrary which was discovered by Karoline Leach in the Dodgson family archive in 1996.[91]		This paper is known as the "cut pages in diary document", and was compiled by various members of Carroll's family after his death. Part of it may have been written at the time when the pages were destroyed, though this is unclear. The document offers a brief summary of two diary pages that are missing, including the one for 27 June 1863. The summary for this page states that Mrs. Liddell told Dodgson that there was gossip circulating about him and the Liddell family's governess, as well as about his relationship with "Ina", presumably Alice's older sister Lorina Liddell. The "break" with the Liddell family that occurred soon after was presumably in response to this gossip.[92][93] An alternative interpretation has been made regarding Carroll's rumoured involvement with "Ina": Lorina was also the name of Alice Liddell's mother. What is deemed most crucial and surprising is that the document seems to imply that Dodgson's break with the family was not connected with Alice at all; until a primary source is discovered, the events of 27 June 1863 will remain in doubt.		In his diary for 1880, Dodgson recorded experiencing his first episode of migraine with aura, describing very accurately the process of "moving fortifications" that are a manifestation of the aura stage of the syndrome.[94] Unfortunately, there is no clear evidence to show whether this was his first experience of migraine per se, or if he may have previously suffered the far more common form of migraine without aura, although the latter seems most likely, given the fact that migraine most commonly develops in the teens or early adulthood. Another form of migraine aura called Alice in Wonderland syndrome has been named after Dodgson's little heroine because its manifestation can resemble the sudden size-changes in the book. It is also known as micropsia and macropsia, a brain condition affecting the way that objects are perceived by the mind. For example, an afflicted person may look at a larger object such as a basketball and perceive it as if it were the size of a golf ball. Some authors have suggested that Dodgson may have suffered from this type of aura and used it as an inspiration in his work, but there is no evidence that he did.[95][96]		Dodgson also suffered two attacks in which he lost consciousness. He was diagnosed by a Dr. Morshead, Dr. Brooks, and Dr. Stedman, and they believed the attack and a consequent attack to be an "epileptiform" seizure (initially thought to be fainting, but Brooks changed his mind). Some have concluded from this that he was a lifetime sufferer of this condition, but there is no evidence of this in his diaries beyond the diagnosis of the two attacks already mentioned.[94] Some authors, Sadi Ranson in particular, have suggested that Carroll may have suffered from temporal lobe epilepsy in which consciousness is not always completely lost but altered, and in which the symptoms mimic many of the same experiences as Alice in Wonderland. Carroll had at least one incident in which he suffered full loss of consciousness and awoke with a bloody nose, which he recorded in his diary and noted that the episode left him not feeling himself for "quite sometime afterward". This attack was diagnosed as possibly "epileptiform" and Carroll himself later wrote of his "seizures" in the same diary.		Most of the standard diagnostic tests of today were not available in the nineteenth century. Dr. Yvonne Hart, consultant neurologist at the John Radcliffe Hospital, Oxford, considered Dodgson's symptoms. Her conclusion, quoted in Jenny Woolf's 2010 The Mystery of Lewis Carroll, is that Dodgson very likely had migraine, and may have had epilepsy, but she emphasises that she would have considerable doubt about making a diagnosis of epilepsy without further information.[97]		On Copenhagen Street in Islington is the Lewis Carroll Children's Library.[98]		In 1982, his great-nephew unveiled a memorial stone to him in Poets' Corner, Westminster Abbey.[99]		
Recreation is an activity of leisure, leisure being discretionary time.[1] The "need to do something for recreation" is an essential element of human biology and psychology.[2] Recreational activities are often done for enjoyment, amusement, or pleasure and are considered to be "fun".						The term recreation appears to have been used in English first in the late 14th century, first in the sense of "refreshment or curing of a sick person",[3] and derived turn from Latin (re: "again", creare: "to create, bring forth, beget.).		Humans spend their time in activities of daily living, work, sleep, social duties, and leisure, the latter time being free from prior commitments to physiologic or social needs,[4] a prerequisite of recreation. Leisure has increased with increased longevity and, for many, with decreased hours spent for physical and economic survival, yet others argue that time pressure has increased for modern people, as they are committed to too many tasks.[5] Other factors that account for an increased role of recreation are affluence, population trends, and increased commercialization of recreational offerings.[6] While one perception is that leisure is just "spare time", time not consumed by the necessities of living, another holds that leisure is a force that allows individuals to consider and reflect on the values and realities that are missed in the activities of daily life, thus being an essential element of personal development and civilization.[1] This direction of thought has even been extended to the view that leisure is the purpose of work, and a reward in itself,[1] and "leisure life" reflects the values and character of a nation.[6] Leisure is considered a human right under the Universal Declaration of Human Rights.[7]		Recreation is difficult to separate from the general concept of play, which is usually the term for children's recreational activity. Children may playfully imitate activities that reflect the realities of adult life. It has been proposed that play or recreational activities are outlets of or expression of excess energy, channeling it into socially acceptable activities that fulfill individual as well as societal needs, without need for compulsion, and providing satisfaction and pleasure for the participant.[8] A traditional view holds that work is supported by recreation, recreation being useful to "recharge the battery" so that work performance is improved. Work, an activity generally performed out of economic necessity and useful for society and organized within the economic framework, however can also be pleasurable and may be self-imposed thus blurring the distinction to recreation. Many activities may be work for one person and recreation for another, or, at an individual level, over time recreational activity may become work, and vice versa. Thus, for a musician, playing an instrument may be at one time a profession, and at another a recreation. Similarly, it may be difficult to separate education from recreation as in the case of recreational mathematics.[9]		Recreation is an essential part of human life and finds many different forms which are shaped naturally by individual interests but also by the surrounding social construction.[2] Recreational activities can be communal or solitary, active or passive, outdoors or indoors, healthy or harmful, and useful for society or detrimental. A significant section of recreational activities are designated as hobbies which are activities done for pleasure on a regular basis. A list of typical activities could be almost endless including most human activities, a few examples being reading, playing or listening to music, watching movies or TV, gardening, hunting, sports, studies, and travel. Some recreational activities - such as gambling, recreational drug use, or delinquent activities - may violate societal norms and laws.		Public space such as parks and beaches are essential venues for many recreational activities. Tourism has recognized that many visitors are specifically attracted by recreational offerings.[10] In support of recreational activities government has taken an important role in their creation, maintenance, and organization, and whole industries have developed merchandise or services. Recreation-related business is an important factor in the economy; it has been estimated that the outdoor recreation sector alone contributes $730 billion annually to the U.S. economy and generates 6.5 million jobs.[11]		Many recreational activities are organized, typically by public institutions, voluntary group-work agencies, private groups supported by membership fees, and commercial enterprises.[12] Examples of each of these are the National Park Service, the YMCA, the Kiwanis, and Disney World.		Recreation has many health benefits, and, accordingly, Therapeutic Recreation has been developed to take advantage of this effect. The National Council for Therapeutic Recreation Certification (NCTRC) is the nationally recognized credentialing organization for the profession of Therapeutic Recreation. Professionals in the field of Therapeutic Recreation who are certified by the NCTRC are called "Certified Therapeutic Recreation Specialists". The job title "Recreation Therapist" is identified in the U.S. Dept of Labor's Occupation Outlook. Such therapy is applied in rehabilitation, psychiatric facilities for youth and adults, and in the care of the elderly, the disabled, or people with chronic diseases. Recreational physical activity is important to reduce obesity, and the risk of osteoporosis[13] and of cancer, most significantly in men that of colon and prostate,[14] and in women that of the breast;[15] however, not all malignancies are reduced as outdoor recreation has been linked to a higher risk of melanoma.[14] Extreme adventure recreation naturally carries its own hazards.		A recreation specialist would be expected to meet the recreational needs of a community or assigned interest group. Educational institutions offer courses that lead to a degree as a Bachelor of Arts in recreation management. People with such degrees often work in parks and recreation centers in towns, on community projects and activities. Networking with instructors, budgeting, and evaluation of continuing programs are common job duties.		In the United States, most states have a professional organization for continuing education and certification in recreation management. The National Recreation and Park Association administers a certification program called the CPRP (Certified Park and Recreation Professional)[16] that is considered a national standard for professional recreation specialist practices.		
Extreme sports are recreational activities perceived as involving a high degree of risk.[1][2][3] These activities often involve speed, height, a high level of physical exertion, and highly specialized gear.[1]						The definition of an extreme sport is not exact and the origin of the term is unclear, but it gained popularity in the 1990s when it was picked up by marketing companies to promote the X Games and when the Extreme Sports Channel and Extreme.com launched. More recently, the commonly used definition from research is "a competitive (comparison or self-evaluative) activity within which the participant is subjected to natural or unusual physical and mental challenges such as speed, height, depth or natural forces and where fast and accurate cognitive perceptual processing may be required for a successful outcome" by Dr. Rhonda Cohen (2012).[4]		While use of the term "extreme sport" has spread far and wide to describe a multitude of different activities, exactly which sports are considered 'extreme' is debatable. There are, however, several characteristics common to most extreme sports.[5] While not the exclusive domain of youth, extreme sports tend to have a younger-than-average target demographic. Extreme sports are rarely sanctioned by schools.[citation needed] Extreme sports tend to be more solitary than traditional sports[6] (rafting and paintballing are notable exceptions, as they are done in teams). In addition, beginning extreme athletes tend to work on their craft without the guidance of a coach (though some may hire a coach later).		Activities categorized by media as extreme sports differ from traditional sports due to the higher number of inherently uncontrollable variables. These environmental variables are frequently weather and terrain related, including wind, snow, water and mountains. Because these natural phenomena cannot be controlled, they inevitably affect the outcome of the given activity or event.		In a traditional sporting event, athletes compete against each other under controlled circumstances. While it is possible to create a controlled sporting event such as X Games, there are environmental variables that cannot be held constant for all athletes. Examples include changing snow conditions for snowboarders, rock and ice quality for climbers, and wave height and shape for surfers.		Whilst traditional sporting judgment criteria may be adopted when assessing performance (distance, time, score, etc.), extreme sports performers are often evaluated on more subjective and aesthetic criteria.[7] This results in a tendency to reject unified judging methods, with different sports employing their own ideals[8] and indeed having the ability to evolve their assessment standards with new trends or developments in the sports.		While the exact definition and what is included as extreme sport is debatable, some attempted to make classification for extreme sports.[9]		One argument is that to qualify as an "extreme sport" both expression terms need to be fulfilled;		Along this definition, an activity such as bungee jumping may not qualify as no skill or physical ability is required to execute a good jump (i.e., avoid poor execution). A passenger in a canyon jet boat ride will not fulfill the requirements, as the skill required pertains to the pilot, not the passengers. "Thrill seeking" might in these cases be a more suitable qualification than "extreme sport".		Extreme sports may be subdivided into:		These sports require the use of snow, ice or water) sports ("sports de glisse" in French) and rolling sports. Another subdivision can be made along motorized and non motorized vehicle sports, resulting in the following matrix;		No vehicle is required (rock climbing, canyoning, ice climbing, parkour, psicobloc etc.)		The origin of the divergence of the term "extreme sports" from "sports" may date to the 1950s in the appearance of a phrase usually, but wrongly, attributed to Ernest Hemingway.[10] The phrase is;		There are only three sports: bullfighting, motor racing, and mountaineering; all the rest are merely games.		The implication of the phrase was that the word "sport" defined an activity in which one might be killed. The other activities being termed "games". The phrase may have been invented by either writer Barnaby Conrad or automotive author Ken Purdy.[10]		The Dangerous Sports Club of Oxford University, England was founded by David Kirke, Chris Baker, Ed Hulton and Alan Weston. They first came to wide public attention by inventing modern day bungee jumping, by making the first modern jumps on 1 April 1979, from the Clifton Suspension Bridge, Bristol, England. They followed the Clifton Bridge effort with a jump from the Golden Gate Bridge in San Francisco, California (including the first female bungee jump by Jane Wilmot), and with a televised leap from the Royal Gorge Suspension Bridge in Colorado, sponsored by and televised on the popular American television program That's Incredible! Bungee jumping was treated as a novelty for a few years, then became a craze for young people, and is now an established industry for thrill seekers. The Club also pioneered a surrealist form of skiing, holding three events at St. Moritz, Switzerland, in which competitors were required to devise a sculpture mounted on skis and ride it down a mountain. The event reached its limits when the Club arrived in St. Moritz with a London double-decker bus, wanting to send it down the ski slopes, and the Swiss resort managers refused.		Other Club activities included expedition hang gliding from active volcanoes; the launching of giant (60 ft) plastic spheres with pilots suspended in the centre (zorbing); microlight flying; and BASE jumping (in the early days of this sport).		In recent decades the term extreme sport was further promoted after the Extreme Sports Channel, Extreme.com launched and then the X Games, a multi-sport event was created and developed by ESPN.[11][12] The first X Games (known as 1995 Extreme Games) were held in Newport, Providence, Mount Snow, and Vermont in the United States.[13][14]		Certain extreme sports clearly trace back to other extreme sports, or combinations thereof. For example, windsurfing was conceived as a result of efforts to equip a surfboard with a sailing boat's propulsion system (mast and sail). Kitesurfing on the other hand was conceived by combining the propulsion system of kite buggying (a parafoil) with the bi-directional boards used for wakeboarding. Wakeboarding is in turn derived from snowboarding and waterskiing.		Some contend[15] that the distinction between an extreme sport and a conventional one has as much to do with marketing as with the level of danger involved or the adrenaline generated. For example, rugby union is both dangerous and adrenaline-inducing but is not considered an extreme sport due to its traditional image, and because it does not involve high speed or an intention to perform stunts (the aesthetic criteria mentioned above) and also it does not have changing environmental variables for the athletes. Demolition derby racing, predominantly an adult sport, is not thought of as 'extreme' while BMX racing, a youth sport, is.[citation needed]		One common aspect of an extreme sport is a counter-cultural aura — a rejection of authority and of the status quo by disaffected youth. Some youth of Generation Y have seized upon activities which they can claim as their own, and have begun rejecting more traditional sports in increasing numbers.[6]		A feature of such activities in the view of some is their alleged capacity to induce an adrenaline rush in participants.[16] However, the medical view is that the rush or high associated with the activity is not due to adrenaline being released as a response to fear, but due to increased levels of dopamine, endorphins and serotonin because of the high level of physical exertion.[17] Furthermore, a recent study suggests that the link to adrenaline and 'true' extreme sports is tentative.[18] The study defined 'true' extreme sports as a leisure or recreation activity where the most likely outcome of a mismanaged accident or mistake was death. This definition was designed to separate the marketing hype from the activity.		Eric Brymer[19] also found that the potential of various extraordinary human experiences, many of which parallel those found in activities such as meditation, was an important part of the extreme sport experience. Those experiences put the participants outside their comfort zone and are often done in conjunction with adventure travel.		Some of the sports have existed for decades and their proponents span generations, some going on to become well known personalities. Rock climbing and ice climbing have spawned publicly recognizable names such as Edmund Hillary, Chris Bonington, Wolfgang Güllich and more recently Joe Simpson. Another example is surfing, invented centuries ago by the inhabitants of Hawaii.		Extreme sports by their nature can be extremely dangerous, conducive to fatalities, near-fatalities and other serious injuries, and sometimes consist in treading along the brink of death. This imminent and inherent danger in these sports has been considered a somewhat necessary part of its appeal,[20] which is partially a result of pressure for athletes to make more money and provide maximum entertainment.[21]		Occasionally persons with various physical disabilities participate in extreme sports. Nonprofit organizations such as Adaptive Action Sports seek to increase awareness of the participation in action sports by members of the disabled community, as well as increase access to the adaptive technologies that make participation possible and to competitions such as The X Games.[22][23]		Extreme sports is a sub-category of sports than are described as any kind of sport "of a character or kind farthest removed from the ordinary or average".[24] These kinds of sports often carry out the potential risk of serious and permanent physical injury and even death.[25] However, these sports also have the potential to produce drastic benefits on mental and physical health.		Health is an individual's state of mental and physical well-being in which they are maximising their daily potential. It is divided into two main categories: mental and physical.		Mental health is a cognitive state of well-being, in this state the individual is aware of his or how own potential and is able to; cope with stresses of normal life and work productively, as well as willing to give their contribution to his or her community in a beneficial way.[26]		Physical health can be defined as a state of complete physical well-being in which an individual is mechanically fit to perform their daily activities and duties without any problem.[27]		Extreme sports trigger the release of the hormone adrenaline.[28] This is a very powerful hormone as it can cause human beings to perform tremendous stunts of many sorts. However, with a bad mix of other hormones it can lead people to execute terrible actions. It is believed that the implementation of extreme sports on mental health patients improves their perspective and recognition of aspects of life.[25]		
Meriwether Lewis (August 18, 1774 – October 11, 1809) was an American explorer, soldier, politician, and public administrator, best known for his role as the leader of the Lewis and Clark Expedition, also known as the Corps of Discovery, with William Clark.		Their mission was to explore the territory of the Louisiana Purchase, establish trade with, and sovereignty over the natives near the Missouri River, and claim the Pacific Northwest and Oregon Country for the United States before European nations. They also collected scientific data, and information on indigenous nations.[1] President Thomas Jefferson appointed him Governor of Upper Louisiana in 1806.[2][3] He died of gunshot wounds in what was either a murder or suicide, in 1809.						Meriwether Lewis was born in Albemarle County, Colony of Virginia, in the present-day community of Ivy.[4] He was the son of Lt. William Lewis of Locust Hill (1733 – November 17, 1779),[5] who was of Welsh ancestry, and Lucy Meriwether (February 4, 1752 – September 8, 1837), daughter of Thomas Meriwether and Elizabeth Thornton, who were both of English ancestry. (Thornton was the daughter of Francis Thornton and Mary Taliaferro). After his father died of pneumonia, he moved with his mother and stepfather Captain John Marks to Georgia in May 1780.[6] They settled along the Broad River in the Goosepond Community within the Broad River Valley in Wilkes County (now Oglethorpe County).		Lewis had no formal education until he was 13 years of age, but during his time in Georgia he enhanced his skills as a hunter and outdoorsman. He would often venture out in the middle of the night in the dead of winter with only his dog to go hunting. Even at an early age, he was interested in natural history, which would develop into a lifelong passion. His mother taught him how to gather wild herbs for medicinal purposes. In the Broad River Valley, Lewis first dealt with American Indians. This was the traditional territory of the Cherokee, who resented encroachment by the colonists. Lewis seems to have been a champion for them among his own people. While in Georgia, he met Eric Parker, who encouraged him to travel. At thirteen, Lewis was sent back to Virginia for education by private tutors. His father's older brother Nicholas Lewis became his guardian.[6] One of his tutors was Parson Matthew Maury, an uncle of Matthew Fontaine Maury. In 1793, Lewis graduated from Liberty Hall (now Washington and Lee University).		That year he joined the Virginia militia, and in 1794 he was sent as part of a detachment involved in putting down the Whiskey Rebellion. In 1795 Lewis joined the U.S. Army, commissioned as an Ensign (an Army rank that was later abolished and was equivalent to a modern Second Lieutenant). By 1800 he rose to Captain, and ended his service there in 1801. Among his commanding officers was William Clark, who would later become his companion in the Corps of Discovery.		On April 1, 1801, Lewis was appointed as an aide by President Thomas Jefferson, whom he knew through Virginia society in Albemarle County. Lewis resided in the presidential mansion, and frequently conversed with various prominent figures in politics, the arts and other circles.[7] He compiled information on the personnel and politics of the United States Army, which had seen an influx of Federalist officers as a result of "midnight appointments" made by outgoing president John Adams in 1801.[8]		When Jefferson began to plan for an expedition across the continent, he chose Lewis to lead the expedition. Meriwether Lewis recruited Clark, then age 33, to share command of the expedition.		After the Louisiana Purchase in 1803, Thomas Jefferson wanted to get an accurate sense of the new land and its resources. The President also hoped to find a "direct and practicable water communication across this continent, for the purposes of commerce with Asia."[9] In addition, Jefferson placed special importance on declaring U.S. sovereignty over the Native Americans along the Missouri River.[10][11][12][13]		The two-year exploration by Lewis and Clark was the first transcontinental expedition to the Pacific Coast by the United States; however, Lewis and Clark reached the Pacific 12 years after Sir Alexander Mackenzie had done so overland in Canada.[9] When they left Fort Mandan in April 1805 they were accompanied by the sixteen-year-old Shoshone Indian woman, Sacagawea, the wife of the French-Canadian fur trader, Toussaint Charbonneau. The Corps of Discovery made contact with many Native Americans in the trans-Mississippi West and found them accustomed to dealing with European traders and already connected to global markets. After crossing the Rocky Mountains, the expedition reached the Oregon Country (which was disputed land beyond the Louisiana Purchase) and the Pacific Ocean in November 1805. They returned in 1806, bringing with them an immense amount of information about the region as well as numerous plant and animal specimens.[14] They demonstrated the possibility of overland travel to the Pacific coast. The success of their journey helped to strengthen the American concept of "Manifest destiny" - the idea that the United States was destined to reach all the way across North America from Atlantic to Pacific.[15][16]		After returning from the expedition, Lewis received a reward of 1,600 acres (6.5 km2) of land. He also initially made arrangements to publish the Corps of Discovery journals, but had difficulty completing his writing. In 1807, Jefferson appointed him governor of the Louisiana Territory; he settled in St. Louis.		Lewis' record as an administrator is mixed. He published the first laws in the Upper-Louisiana Territory, established roads and furthered Jefferson's mission as a strong proponent of the fur trade. He negotiated peace among several quarrelling Indian tribes. His duty to enforce Indian treaties was to protect the western Indian lands from encroachment,[8] which was opposed by the rush of settlers looking to open new lands for settlements. But due to his quarreling with local political leaders, controversy over his approvals of trading licenses, land grant politics, and Indian depredations, some historians have argued that Lewis was a poor administrator. That view has been reconsidered in recent biographies. Lewis's primary quarrels were with his territorial secretary Frederick Bates. Bates was accused of undermining Lewis to seek Lewis's dismissal and his own appointment as governor. Because of the slow-moving mail system, former president Jefferson and Lewis's superiors in Washington got the impression that Lewis did not adequately keep in touch with them.[17] Bates wrote letters to Lewis's superiors accusing Lewis of profiting from a mission to return a Mandan chief to his tribe. Because of Bates' accusation, the War Department refused to reimburse Lewis for a large sum he personally advanced for the mission. When Lewis's creditors heard that Lewis would not be reimbursed for the expenses, they called Lewis's notes, forcing him to liquidate his assets, including land he was granted for the Lewis and Clark Expedition. One of the primary reasons Lewis set out for Washington on this final trip was to clear up questions raised by Bates and to seek a reimbursement of the money he had advanced for the territorial government. The U.S. government finally reimbursed the expenses to Lewis's estate two years after his death. Bates eventually became governor of Missouri. Though some historians have speculated that Lewis abused alcohol or opiates based upon an account attributed to Gilbert Russell at Fort Pickering on Lewis's final journey,[18] others have argued that Bates never alleged that Lewis suffered from such addictions and that Bates certainly would have used them against Lewis if Lewis suffered from those conditions.		Lewis was a Freemason, initiated, passed and raised in the "Door To Virtue Lodge No. 44" in Albemarle, Virginia, between 1796 and 1797.[19] On August 2, 1808, Lewis and several of his acquaintances submitted a petition to the Grand Lodge of Pennsylvania requesting dispensation to establish a lodge in St. Louis. Lewis was nominated and recommended to serve as the first Master of the proposed Lodge, which was warranted as Lodge No. 111 on September 16, 1808.[20] (See List of Notable Freemasons)		Unlike William Clark, who brought his slave York on the westward expedition, Lewis did not have a personal valet slave. Although Lewis attempted to supervise enslaved people while running his mother's plantation before the expedition, he left that post and had no valet during the expedition. Lewis made assignments to York but allowed Clark to supervise him; Lewis also granted York and Sacagewea votes during expedition meetings. Later, Lewis hired a free African-American man as his valet, John Pernia. Pernia accompanied Lewis during his final journey, although his wages were considerably in arrears. After Lewis' death, Pernia continued to Monticello and asked Jefferson to pay the $240 owed him, but was refused. Pernia later committed suicide.[21]		On September 3, 1809, Lewis set out for Washington, D.C., where he hoped to resolve issues regarding the denied payment of drafts he had drawn against the War Department while serving as governor of the Upper Louisiana Territory, leaving him in potentially ruinous debt. Lewis carried his journals with him for delivery to his publisher. He intended to travel to Washington by ship from New Orleans, but changed his plans while floating down the Mississippi River from St. Louis. He disembarked and decided instead to make an overland journey via the Natchez Trace and then east to Washington. (The Natchez Trace was the old pioneer road between Natchez, Mississippi, and Nashville, Tennessee). Robbers preyed on travelers on that road and sometimes killed their victims.[22] Lewis had written his will before his journey and also attempted suicide on this journey, but was restrained.[23][page needed]		According to a lost October 18, 1809 letter to Thomas Jefferson, Lewis stopped at an inn on the Natchez Trace called Grinder's Stand, about 70 miles (110 km) southwest of Nashville on October 10, 1809. After dinner, he retired to his one-room cabin. In the predawn hours of October 11, the innkeeper's wife (Priscilla Griner) heard gunshots. Servants found Lewis badly injured from multiple gunshot wounds, one each to the head and gut. He bled out on his buffalo hide robe and died shortly after sunrise. The Nashville Democratic Clarion published the account, which newspapers across the country repeated and embellished. The Nashville newspaper also reported that Lewis's throat was cut.[24] Money that Lewis had borrowed from Major Gilbert Russell at Fort Pickering to complete the journey was missing.		While Lewis's friend Thomas Jefferson and some modern historians have generally accepted Lewis's death as a suicide, debate continues, as discussed below. No one reported seeing Lewis shoot himself. Three inconsistent somewhat contemporary accounts are attributed to Mrs. Grinder, who left no written account or testimony—some thus believe her testimony was fabricated, while others point to it as proof of suicide.[25] Mrs. Grinder claimed Lewis acted strangely the night before his death: standing and pacing during dinner and talking to himself in the way one would speak to a lawyer, with face flushed as if it had come on him in a fit. She continued to hear him talking to himself after he retired, and then at some point in the night, she heard multiple gunshots, a scuffle, and someone calling for help. She claimed to be able to see Lewis through the slit in the door crawling back to his room. However, she never explained why she never investigated further at the time, but only the next morning sent her children to look for Lewis's servants. Another account claimed the servants found Lewis in the cabin, wounded and bloody, with part of his skull gone, but he lived for several hours. In the last account attributed to Mrs. Grinder, three men followed Lewis up the Natchez Trace, and he pulled his pistols and challenged them to a duel. In that account, Mrs. Grinder said that she heard voices and gunfire in Lewis's cabin about 1 a.m. She found the cabin empty and a large amount of gunpowder on the floor. Thus, in this account, Lewis's body was found outside the cabin.		Lewis's mother and relatives always contended it was murder. A coroner's jury held an inquest immediately after Lewis's death as provided by local law; however, they did not charge anyone with murdering Lewis.[26] The jury foreman kept a pocket diary of the proceedings, which disappeared in the early 1900s.[citation needed]		When William Clark and Thomas Jefferson were informed of Lewis' death, both accepted the conclusion of suicide. Based on their positions and the never-found Lewis letter of mid-September 1809, historian Stephen Ambrose dismisses the murder theory as "not convincing".[8]		The only doctor to examine Lewis's body did not do so until 40 years later, in 1848.[25] The Tennessee State Commission, including Dr. Samuel B. Moore, charged with locating Lewis's grave and erecting a monument over it, opened Lewis's grave. The commission wrote in its official report that though the impression had long prevailed that Lewis died by his own hand, "it seems to be more probable that he died by the hands of an assassin."[27]		In the book The History of the Lewis and Clark Expedition, first printed in 1893, the editor Elliott Coues expressed doubt about Thomas Jefferson's conclusion that Lewis committed suicide, despite including the former President's Memoir of Meriwether Lewis in his book.[28]		From 1993–2010, about 200 of Lewis's kin (through his sister Jane, as he had no children) sought to have the body exhumed for forensic analysis, to try to determine whether the death was a suicide. A Tennessee coroner's jury in 1996 recommended exhumation. However, since Lewis's gravesite is in a national monument, the National Park Service must approve. The agency refused the request in 1998, citing possible disturbance to the bodies of more than 100 pioneers buried nearby. In 2008, the Department of the Interior approved the exhumation, but rescinded that decision in 2010 after the change in administrations, stating that decision is final.[citation needed] It is nonetheless improving the grave site and visitor facility.[29]		Historian Paul Russell Cutright wrote a detailed refutation of the murder/robbery theory, concluding that it "lacks legs to stand on".[30] He stressed Lewis's debts, heavy drinking, and possible morphine/opium use, failure to prepare the expedition's journals for publication, repeated failure to find a wife, and the deterioration of his friendship with Thomas Jefferson.[8][30] This refutation was countered by Eldon G. Chuinard, who argued for the murder hypothesis. Leading Lewis scholars Donald Jackson, Jay H. Buckley, Clay S. Jenkinson and others, have stated that, regardless of their leanings or beliefs, that the facts of his death are not known, there are no eyewitnesses, and that the reliability of reports of those in the place or vicinity cannot be considered certain. Author Peter Stark believes that post-traumatic stress disorder (PTSD) may have been a contributor to Meriwether Lewis's condition after spending months traversing hostile Indian territory, particularly because travelers coming afterward exhibited the same symptoms. [31]		The explorer was buried near present-day Hohenwald, Tennessee, near his place of death. His grave was located about 200 yards from Grinder's Stand, alongside the Natchez Trace. (That section of the 1801 Natchez Trace was built by the U.S. Army under the direction of Lewis' mentor Thomas Jefferson, during Lewis' lifetime).		At first, the grave was unmarked. Alexander Wilson, an ornithologist and friend of Lewis' who visited the grave in May 1810, during a trip to New Orleans to sell his drawings, wrote that he gave the innkeeper Robert Griner money to erect a fence around the grave to protect it from animals.[32]		The State of Tennessee erected a monument over Lewis' grave in 1848. Lemuel Kirby, a stonemason from Columbia, Tennessee, chose the design of a broken column, commonly used at the time to symbolize a life cut short.[33]		An iron fence erected around the base of the monument was partially dismantled during the Civil War by General Hood's detachments marching from Shiloh toward Franklin; they forged the iron into horseshoes[34]		A September 1905 article in Everybody's Magazine called attention to Lewis' abandoned and overgrown grave.[35] A county road worker, Teen Cothran, took the initiative to open a road to the cemetery. Thereafter, a local Tennessee Meriwether Lewis Monument Committee was soon formed to push for restoring Lewis' gravesite. In 1925, in response to the committee's work, President Calvin Coolidge designated Lewis' grave as the fifth National Monument in the South.		In 2009, the Lewis and Clark Trail Heritage Foundation organized a commemoration for Lewis in conjunction with their 41st annual meeting October 3–7, 2009.[36] It included the first national memorial service at his gravesite. On October 7, 2009, near the 200th anniversary of Lewis' death, about 2,500 people (National Park Service estimate) from more than 25 states gathered at his grave to acknowledge Lewis' life and achievements. Speakers included William Clark's descendant Peyton "Bud" Clark, Lewis' collateral descendants Howell Bowen and Tom McSwain, and Stephanie Ambrose Tubbs (Stephen Ambrose's daughter; her father wrote Undaunted Courage, an award-winning book about the Lewis and Clark Expedition). A bronze bust of Lewis was dedicated at the Natchez Trace Parkway for a planned visitor center at the gravesite area. The District of Columbia and governors of 20 states associated with the Lewis and Clark Trail sent flags flown over state capital buildings to be carried to Lewis' grave by residents of the states, acknowledging the significance of Lewis' contribution in the creation of their states.[37]		The 2009 ceremony at Lewis' grave was the final bicentennial event honoring the Lewis and Clark Expedition. Re-enactors from the Lewis and Clark Bicentennial participated, and official attendees included representatives from Jefferson's Monticello. Lewis and Clark descendants and family members, along with representatives of St. Louis Lodge #1, past presidents of the Lewis and Clark Trail Heritage Foundation, and the Daughters of the American Revolution, carried wreaths and led a formal procession to Lewis' grave. Samples of plants which Lewis discovered on the expedition were brought from the Trail states and laid on his grave. The U.S. Army was represented by the 101st Airborne Infantry Band and its Army chaplain. The National Park Service announced that it would rehabilitate the site.[38]		For many years, Lewis' legacy was overlooked, inaccurately assessed, and somewhat tarnished by his alleged suicide.[8] Yet his contributions to science, the exploration of the Western U.S., and the lore of great world explorers, are considered incalculable.[8]		Four years after Lewis' death, Thomas Jefferson wrote:		Of courage undaunted, possessing a firmness and perseverance of purpose which nothing but impossibilities could divert from its direction, ... honest, disinterested, liberal, of sound understanding and a fidelity to truth so scrupulous that whatever he should report would be as certain as if seen by ourselves, with all these qualifications as if selected and implanted by nature in one body for this express purpose, I could have no hesitation in confiding the enterprise to him.[39]		Jefferson wrote that Lewis had a "luminous and discriminating intellect." William Clark's first son Meriwether Lewis Clark was named after Lewis; the senior Meriwether Clark passed the name on to his son, Meriwether Lewis Clark, Jr.		Captain Meriwether Lewis and Lieutenant (de facto Co-Captain and posthumously, officially promoted to Captain in advance of the bicentennial) William Clark commanded the Corps of Discovery to map the course of the Missouri River to its source and the Pacific Northwest overland and water routes to and from the mouth of the Columbia River. They were honored with a 3-cent stamp July 24, 1954 on the 150th anniversary. The 1803 Louisiana Purchase doubled the size of the United States. Lewis and Clark described and sketched its flora and fauna and described the native inhabitants they encountered before returning to St. Louis in 1806.[40]		Both Lewis and Clark appear on the gold Lewis and Clark Exposition dollars minted for the Lewis and Clark Centennial Exposition. Among the Early United States commemorative coins, they were produced in both 1904 and 1905 and survive in relatively small numbers.		The Lewis and Clark Expedition was celebrated on May 14, 2004, the 200th anniversary of its outset, by depicting the two on a hilltop outlook: two companion 37-cent USPS stamps showed portraits of Meriwether Lewis and William Clark. A special 32-page booklet accompanied the issue in eleven cities along the route taken by the Corps of Discovery. An image of the stamp can be found on Arago online at the link in the footnote.[41]		The plant genus Lewisia (family Portulacaceae), popular in rock gardens and which includes the bitterroot (Lewisia rediviva), the state flower of Montana, is named after Lewis, as is Lewis' woodpecker (Melanerpes lewis) and a subspecies of cutthroat trout, the westslope cutthroat (Oncorhynchus clarki lewisi).		Geographic names that honor him include:		Three US Navy vessels have been named in honor of Lewis: the Liberty ship SS Meriwether Lewis, the Polaris armed nuclear submarine USS Lewis and Clark and the supply ship USNS Lewis and Clark.		Lewis & Clark College, Portland, Oregon, was named for Meriwether Lewis and William Clark.		Lewis-Clark State College, Lewiston, Idaho, was named for Meriwether Lewis and William Clark.		Meriwether Lewis' relationship with Thomas Jefferson; Lewis' multiple expeditions, journals, and discoveries; and details surrounding Lewis' death play major roles in James Rollins' seventh Sigma Force novel, The Devil Colony.		The mystery surrounding Meriwether Lewis' death played a role in the 2016 book, The Secret History of Twin Peaks, by author Mark Frost.		
The term mountaineering describes the sport of mountain climbing. While some scholars identify mountaineering-related activities as climbing (rock and ice) and trekking up mountains,[2] others are also adding backpacking, hiking, skiing, via ferrata and wilderness activities,[3] and still others state that mountaineering activities also include indoor climbing, sport climbing and bouldering.[4] However most of the scholars, the term mountaineering understand as a climbing (which now refers to adventure climbing or sports climbing) and trekking (hill walking in 'exotic' places).[5][6] Hiking in the mountains can also be a simple form of mountaineering when it involves scrambling, or short stretches of the more basic grades of rock climbing, as well as crossing glaciers.		While mountaineering began as attempts to reach the highest point of unclimbed big mountains it has branched into specializations that address different aspects of the mountain and consists of three (3) areas: rock-craft, snow-craft, and skiing, depending on whether the route chosen is over rock, snow or ice. All require experience, athletic ability, and technical knowledge to maintain safety.[7]		Mountaineering is often called Alpinism, especially in European languages, which implies climbing routes with minimal equipment in high and often snow and ice-covered mountains such as the Alps, where technical difficulties frequently exceed environmental and physical challenges. A mountaineer who pursues this more technical and minimalist style of mountain climbing is sometimes called an Alpinist, although use of the term may vary between countries and eras. The word alpinism was born in the 19th century to refer to climbing for the purpose of enjoying climbing itself as a sport or recreation, distinct from merely climbing while hunting or as a religious pilgrimage that had been done generally at that time.[8][9]		The UIAA or Union Internationale des Associations d'Alpinisme is the world governing body in mountaineering and climbing, addressing issues like access, medical, mountain protection, safety, youth and ice climbing.[10]						Historically, many cultures have harbored superstitions about mountains, which they often regarded as sacred due to their perceived proximity with heaven, such as Mount Olympus for the Ancient Greeks.		On April 26, 1336 famous Italian poet Petrarch climbed to the summit of 1,912m Mount Ventoux overlooking the Bay of Marseilles, claiming to be inspired by Philip V of Macedon's ascent of Mount Haemo, making him the first known alpinist.		One of the first European mountains visited by many tourists was Sněžka. This was mainly due to the relatively minor technical difficulties ascent and the fact that since the sixteenth century, many resort visitors flocked to the nearby Cieplice Śląskie-Zdrój and highly visible Sněžka, visually dominant over all Krkonoše was for them an important attraction. The first confirmed ascent took place in the year 1456.		In 1492 Antoine de Ville, lord of Domjulien and Beaupré, was the first to ascend the Mont Aiguille, in France, with a little team, using ladders and ropes. It appears to be the first recorded climb of any technical difficulty, and has been said to mark the beginning of mountaineering.		In 1573 Francesco De Marchi and Francesco Di Domenico ascended Corno Grande, the highest peak in the Apennine Mountains. During the Enlightenment, as a product of the new spirit of curiosity for the natural world, many mountain summits were surmounted for the first time.		In 1741 Richard Pococke and William Windham made a historic visit to Chamonix. In 1757 Swiss scientist Horace-Bénédict de Saussure made the first of several unsuccessful attempts on Mont Blanc in France, finally offering a reward, which was claimed in 1786 by Jacques Balmat and Michel-Gabriel Paccard.		By the early 19th century many of the alpine peaks were reached, including the Grossglockner in 1800, the Ortler in 1804, the Jungfrau in 1811, the Finsteraarhorn in 1812, and the Breithorn in 1813. In 1808 Marie Paradis became the first female to climb Mont Blanc, followed in 1838 by Henriette d'Angeville.		Picture of a mountaineer by Josef Feid Anastasius Grün		Sněžka - one of the first European mountains visited by tourists		Jacques Balmat (1762-1834) (left) at the side of Horace-Benedict de Saussure (1740-1799), "The Father of Alpinism", in a monument erected in Chamonix		Henriette d'Angeville (1794-1871)		The beginning of mountaineering as a sport in the UK is generally dated to the ascent of the Wetterhorn in 1854 by English mountaineer Sir Alfred Wills, who made mountaineering fashionable in Britain. This inaugurated what became known as the Golden age of alpinism, with the first mountaineering club - the Alpine Club - being founded in 1857.[11][12]		Prominent figures of the period include Lord Francis Douglas, Florence Crauford Grove, Charles Hudson, E. S. Kennedy, William Mathews, A. W. Moore, Leslie Stephen, Francis Fox Tuckett, John Tyndall, Horace Walker and Edward Whymper. Well-known guides of the era include Christian Almer, Jakob Anderegg, Melchior Anderegg, J. J. Bennen, Michel Croz, Johannes Zumtaugwald.		In the early years of the "golden age", scientific pursuits were intermixed with the sport, such as by the physicist John Tyndall. In the later years, it shifted to a more competitive orientation as pure sportsmen came to dominate the London-based Alpine Club and alpine mountaineering overall.[13]		One of the most dramatic events was the spectacular first ascent of the Matterhorn in 1865 by a party led by English illustrator Edward Whymper, in which four of the party members fell to their deaths. This ascent is generally regarded as marking the end of the mountaineering golden age. By this point the sport of mountaineering had largely reached its modern form, with a body of professional guides, equipment and fixed guidelines.		Mountaineering in the Americas became popular in the 1800s.		In North America, Pikes Peak (14,415 ft (4,394 m)) in the Colorado Rockies (discovered in 1806) was first climbed by Edwin James and two others in 1820. Though lower than Pikes Peak, the heavily glaciated Fremont Peak (13,745 ft (4,189 m)) in Wyoming was thought to be the tallest mountain in the Rockies when it was first climbed by John C. Frémont and two others in 1842. Pico de Orizaba (18,491 ft (5,636 m)), the tallest peak in Mexico and third tallest in North America, was first climbed by U.S. military personnel which included William F. Raynolds and a half dozen other climbers in 1848. Heavily glaciated and more technical climbs in North American were not achieved until the late 19th and early 20th centuries.		In 1897 Mount Saint Elias (18,008 ft (5,489 m)) on the Alaska-Yukon border was summitted by the Duke of the Abruzzi and party. But it was not until 1913 that Mount Mckinley (20,237 ft (6,168 m)), the tallest peak in North America was successfully climbed by Hudson Stuck. Mount Logan (19,551 ft (5,959 m)), the tallest peak in Canada was first summitted by a half dozen climbers in 1925 in an expedition that took more than two months.		In 1879-1880 the exploration of the highest Andes in South America began when English mountaineer Edward Whymper climbed Chimborazo (20,564 ft (6,268 m)) and explored the mountains of Ecuador. The Cordillera between Chile and Argentina was visited by Paul Güssfeldt in 1883, who ascended the volcano Maipo (17,270 ft (5,260 m)) and attempted to climb the tallest mountain in the Americas, Aconcagua (22,837 ft (6,961 m)) that same year but was unsuccessful. The summit of Aconcagua was finally reached on January 14, 1897 by Swiss mountaineer Matthias Zurbriggen during an expedition led by Edward FitzGerald that began in December 1896. The Andes of Bolivia were first explored by Sir William Martin Conway in 1898, who later visited the mountains of Tierra del Fuego on the southern tip of South America.		It took until the late 19th century for European explorers to penetrate Africa. Mount Kilimanjaro in Africa was climbed in 1889 by Austrian mountaineer Ludwig Purtscheller and German geologist Hans Meyer, Mt. Kenya in 1899 by Halford Mackinder,[14] and a peak of Ruwenzori by H. J. Moore in 1900.		Focus shifted toward the exploration of other ranges such as the Pyrenees and the Caucasus Mountains; the latter owed much to the initiative of D. W. Freshfield who was the first man to conquer the summit of Mount Kazbek. Most of its great peaks were successfully conquered by the late 1880s.		New Zealand's Southern Alps were first visited in 1882 by the Irish mountaineer Rev. William Spotswood Green, and on December 25, 1894 Kiwi mountaineer Tom Fyfe and his party summitted Aoraki / Mount Cook.		By the turn of the 20th century, mountaineering had acquired a more international flavour;[15]		The last and greatest mountain range was the Himalayas in Central Asia. They had initially been surveyed by the British Empire for military and strategic reasons. In 1892 Sir William Martin Conway explored the Karakoram Himalayas, and climbed a peak of 23,000 ft (7,000 m) In 1895 Albert F. Mummery died while attempting Nanga Parbat, while in 1899 D. W. Freshfield took an expedition to the snowy regions of Sikkim.		In 1899, 1903, 1906, and 1908 American mountaineer Mrs. Fanny Bullock Workman (one of the first professional female mountaineers) made ascents in the Himalayas, including one of the Nun Kun peaks (23,300 ft). A number of Gurkha sepoys were trained as expert mountaineers by Charles Granville Bruce, and a good deal of exploration was accomplished by them.		In 1902 the Eckenstein-Crowley Expedition, led by English mountaineer Oscar Eckenstein and English occultist Aleister Crowley was the first to attempt to scale Chogo Ri (now known as K2 in the west). They reached 22,000 feet (6,700 m) before turning back due to weather and other mishaps. Undaunted, in 1905 Crowley led the first expedition to Kangchenjunga, the third highest mountain in the world. Four members of the party were killed in an avalanche and they failed to reach the summit.		Eckenstein was also a pioneer in developing new equipment and climbing methods. He started using shorter ice axes which could be used single-handed,[16] designed the modern crampons and improved on the nail patterns used for the climbing boots.		By the 1950s, all the eight-thousanders but two had been climbed starting with Annapurna in 1950 by Maurice Herzog and Louis Lachenal. The last great peak was the highest of them all, Mount Everest. The British had made several attempts in the 1920s; the 1922 expedition reached 8,320 metres (27,300 ft) before being aborted on the third summit attempt after an avalanche killed seven porters. The 1924 expedition saw another height record achieved but still failed to reach the summit with confirmation when George Mallory and Andrew Irvine disappeared on the final attempt. The summit was finally reached on May 29, 1953 by Sir Edmund Hillary and Tenzing Norgay from the south side in Nepal.		Just a few months later, Hermann Buhl made the first ascent of Nanga Parbat (8,125 m), a siege-style expedition culminating in a last 1,300 meters walking alone, being under the influence of drugs: pervitin (based on the stimulant methamphetamine used by soldiers during World War II), padutin and tea from coca leaves. K2 (8,611 m), the second-highest peak in the world, was first scaled in 1954 by Lino Lacedelli and Achille Compagnoni. In 1964, the final eight-thousander to be climbed was Shishapangma (8,013 m), the lowest of all the 8,000 metre peaks.		Mountaineering techniques vary greatly depending on location, season, and the particular route a mountaineer chooses to climb. Mountaineers train to climb on all types of terrain whether it be snow, glacier, glacial Ice, water ice, or rock. Each type of terrain presents its own hazards. Climbers must be skilled in dealing with the different challenges that could arise from different terrain.		Compacted snow conditions allow mountaineers to progress on foot. Frequently crampons are required to travel efficiently over snow and ice. Crampons attach to a mountaineer's boots to provide additional traction on hard snow (névé) and ice. Using various techniques from alpine skiing and mountaineering to ascend/descend a mountain is a form of the sport by itself, called ski mountaineering. Ascending and descending a snow slope safely requires the use of an ice axe and many different footwork techniques that have been developed over the past century, mainly in Europe (e.g. French technique and German technique). The progression of footwork from the lowest angle slopes to the steepest terrain is first to splay the feet to a rising traverse, to kicking steps, to front pointing the crampons. The progression of ice axe technique from the lowest angle slopes to the steepest terrain is to use the ice axe first as a walking stick, then a stake, then to use the front pick as a dagger below the shoulders or above, and finally to swinging the pick into the slope over the head. These various techniques may involve questions of differing ice-axe design depending on terrain, and even whether a mountaineer uses one or two ice axes. Anchors for the rope in snow are sometimes unreliable, and include the snow stakes, called pickets, deadman devices called flukes which are fashioned from aluminium, or devised from buried objects that might include an ice axe, skis, rocks or other objects. Bollards, which are simply carved out of consolidated snow or ice, also sometimes serve as anchors.		When travelling over glaciers, crevasses pose a grave danger. These giant cracks in the ice are not always visible as snow can be blown and freeze over the top to make a snowbridge. At times snowbridges can be as thin as a few inches. Climbers use a system of ropes to protect themselves from such hazards. Basic gear for glacier travel includes crampons and ice axes. Teams of two to five climbers tie into a rope equally spaced. If a climber begins to fall the other members of the team perform a self-arrest to stop the fall. The other members of the team enact a crevasse rescue to pull the fallen climber from the crevasse.		Multiple methods are used to travel safely over ice. The lead climber can place ice screws in the ice and attach the rope for protection. Each climber on the team must clip past the anchor, and the last climber picks up the anchor itself. Occasionally, slinged icicles or bollards are also used. This allows for safety should the entire team be taken off their feet. This technique is known as Simul-climbing and is sometimes also used on steep snow and easy rock.		If the terrain becomes too steep, standard ice climbing techniques are used in which each climber is belayed, moving one at a time.		Alpine rock climbing involves technical skills including the ability to place traditional protection (cams, nuts, hexes) into the rock to safely ascend a mountain. Climbers will climb multiple pitches of rock in order to reach the top. Typically, teams of 2 will climb with one leader placing the protection, and a follower belaying. The leader will reach a point on the rock to build an anchor. This anchor could be created by using slings around a tree, a large rock horn or boulder, or by using protection devices like cams and nuts to build an anchor in cracks. Once anchored, the leader will then belay the follower up to their position. The leader will then transfer all the necessary protection devices (known as a rack) to the follower. The follower then becomes the leader and will ascend the next pitch. This process will continue until the climbers either reach the top, or run into different terrain. In alpine climbing, it is common for climbers to see routes of mixed terrain. This means climbers may need to move efficiently from climbing glacier, to rock, to ice, back and forth in a number of variations.		Climbers use a few different forms of shelter depending on the situation and conditions. Shelter is a very important aspect of safety for the climber as the weather in the mountains may be very unpredictable. Tall mountains may require many days of camping on the mountain.		The "Base Camp" of a mountain is an area used for staging an attempt at the summit. Base camps are positioned to be safe from the harsher conditions above. There are base camps on many popular or dangerous mountains. Where the summit cannot be reached from base camp in a single day, a mountain will have additional camps above base camp. For example, the southeast ridge route on Mount Everest has Base Camp plus (normally) camps I through IV.		The European alpine regions, in particular, have a network of mountain huts (called "refuges" in France, "rifugi" in Italy, "cabanes" in Switzerland, "Hütten" in Germany and Austria, "Bothies" in Scotland, "koča" in Slovenia, "chaty" in Slovakia, "schroniska" in Poland, "refugios" in Spain, "hytte" or "koie" in Norway, and "cabane" in Romanian). Such huts exist at many different heights, including in the high mountains themselves – in extremely remote areas, more rudimentary shelters may exist. The mountain huts are of varying size and quality, but each is typically centred on a communal dining room and have dormitories equipped with mattresses, blankets or duvets, and pillows; guests are expected to bring and to use their own sleeping bag liner. The facilities are usually rudimentary but, given their locations, huts offer vital shelter, make routes more widely accessible (by allowing journeys to be broken and reducing the weight of equipment needing to be carried), and offer good value. In Europe, all huts are staffed during the summer (mid-June to mid-September) and some are staffed in the spring (mid-March to mid-May). Elsewhere, huts may also be open in the fall. Huts also may have a part that is always open, but unmanned, a so-called winter hut. When open and manned, the huts are generally run by full-time employees, but some are staffed on a voluntary basis by members of Alpine clubs (such as Swiss Alpine Club and Club alpin français) or in North America by Alpine Club of Canada. The manager of the hut, termed a guardian or warden in Europe, will usually also sell refreshments and meals; both to those visiting only for the day and to those staying overnight. The offering is surprisingly wide; given that most supplies, often including fresh water, must be flown in by helicopter, and may include glucose-based snacks (such as Mars and Snickers bars) on which climbers and walkers wish to stock up, cakes and pastries made at the hut, a variety of hot and cold drinks (including beer and wine), and high carbohydrate dinners in the evenings. Not all huts offer a catered service, though, and visitors may need to provide for themselves. Some huts offer facilities for both, enabling visitors wishing to keep costs down to bring their own food and cooking equipment and to cater using the facilities provided. Booking for overnight stays at huts is deemed obligatory, and in many cases is essential as some popular huts; even with more than 100 bed spaces may be full during good weather and at weekends. Once made, the cancellation of a reservation is advised as a matter of courtesy – and, indeed, potentially of safety, as many huts keep a record of where climbers and walkers state they planned to walk to next. Most huts may be contacted by telephone and most take credit cards as a means of payment. In the UK the term Hut is used for any cottage or cabin used as a base for walkers or climbers and these are mostly owned by mountaineering clubs for use by members or visiting clubs and generally do not have wardens or permanent staff, but have cooking and washing facilities and heating. In the Scottish Highlands small simple unmanned shelters without cooking facilities known as Bothies are maintained to break up cross country long routes and act as base camps to certain mountains.		In the mountaineering context, a bivouac or "biv(v)y" is a makeshift resting or sleeping arrangement in which the climber has less than the full complement of shelter, food and equipment that would normally be present at a conventional campsite. This may involve simply getting a sleeping bag and Bivouac sack/bivvy bag and lying down to sleep. Typically bivvy bags are made from breathable waterproof membranes, which move moisture away from the climber into the outside environment while preventing outside moisture from entering the bag. Many times small partially sheltered areas such as a bergschrund, cracks in rocks or a trench dug in the snow are used to provide additional shelter from wind. These techniques were originally used only in emergency; however some climbers steadfastly committed to alpine style climbing specifically plan for bivouacs in order to save the weight of a tent when suitable snow conditions or time is unavailable for construction of a snow cave. The principal hazard associated with bivouacs is the greater level of exposure to cold and other elements present in harsh conditions high on the mountain.		Tents are the most common form of shelter used on the mountain. These may vary from simple tarps to much heavier designs intended to withstand harsh mountain conditions. In exposed positions, windbreaks of snow or rock may be required to shelter the tent. One of the downsides to tenting is that high winds and snow loads can be dangerous and may ultimately lead to the tent's failure and collapse. In addition, the constant flapping of the tent fabric can hinder sleep and raise doubts about the security of the shelter. When choosing a tent, alpinists tend to rely on specialised mountaineering tents that are specifically designed for high winds and moderate to heavy snow loads. Tent stakes can be buried in the snow ("deadman") for extra security.		Where conditions permit, snow caves are another way to shelter high on the mountain. Some climbers do not use tents at high altitudes unless the snow conditions do not allow for snow caving, since snow caves are silent and much warmer than tents. They can be built relatively easily, given sufficient time, using a snow shovel. A correctly made snow cave will hover around freezing, which relative to outside temperatures can be very warm. They can be dug anywhere where there is at least four feet of snow. The addition of a good quality bivvy bag and closed cell foam sleeping mat will also increase the warmth of the snow cave. Another shelter that works well is a quinzee, which is excavated from a pile of snow that has been work hardened or sintered (typically by stomping). Igloos are used by some climbers, but are deceptively difficult to build and require specific snow conditions.		Mountaineering is considered to be one of the most dangerous activities in the world. Loss of life is not uncommon on most major extreme altitude mountaineering destinations every year. Dangers in mountaineering are sometimes divided into two categories: objective hazards that exist without regard to the climber's presence, like rockfall, avalanches and inclement weather, and subjective hazards that relate only to factors introduced by the climber. Equipment failure and falls due to inattention, fatigue or inadequate technique are examples of subjective hazards. A route continually swept by avalanches and storms is said to have a high level of objective danger, whereas a technically far more difficult route that is relatively safe from these dangers may be regarded as objectively safer.		In all, mountaineers must concern themselves with dangers: falling rocks, falling ice, snow-avalanches, the climber falling, falls from ice slopes, falls down snow slopes, falls into crevasses and the dangers from altitude and weather.[17] To select and follow a route using one's skills and experience to mitigate these dangers is to exercise the climber's craft.		Every rock mountain is slowly disintegrating due to erosion, the process being especially rapid above the snow-line. Rock faces are constantly swept by falling stones, which may be possible to dodge. Falling rocks tend to form furrows in a mountain face, and these furrows (couloirs) have to be ascended with caution, their sides often being safe when the middle is stoneswept. Rocks fall more frequently on some days than on others, according to the recent weather. Ice formed during the night may temporarily bind rocks to the face but warmth of the day or lubricating water from melting snow or rain may easily dislodge these rocks. Local experience is a valuable help on determining typical rock fall on such routes.		The direction of the dip of rock strata sometimes determines the degree of danger on a particular face; the character of the rock must also be considered. Where stones fall frequently debris will be found below, whilst on snow slopes falling stones cut furrows visible from a great distance. In planning an ascent of a new peak or an unfamiliar route, mountaineers must look for such traces. When falling stones get mixed in considerable quantity with slushy snow or water a mud avalanche is formed (common in the Himalayas). It is vital to avoid camping in their possible line of fall.		The places where ice may fall can always be determined beforehand. It falls in the broken parts of glaciers (seracs) and from overhanging cornices formed on the crests of narrow ridges. Large icicles are often formed on steep rock faces, and these fall frequently in fine weather following cold and stormy days. They have to be avoided like falling stones. Seracs are slow in formation, and slow in arriving (by glacier motion) at a condition of unstable equilibrium. They generally fall in or just after the hottest part of the day. A skillful and experienced ice-man will usually devise a safe route through a most intricate ice-fall, but such places should be avoided in the afternoon of a hot day. Hanging glaciers (i.e. glaciers perched on steep slopes) often discharge themselves over steep rock-faces, the snout breaking off at intervals. They can always be detected by their debris below. Their track should be avoided.		A rock climber's skill is shown by their choice of handhold and foothold, and their adhesion to the holds once chosen. Much depends on the ability to estimate the capability of the rock to support the weight placed on it. Many loose rocks are quite firm enough to bear a person's weight, but experience is needed to know which can be trusted, and skill is required in transferring the weight to them without jerking. On rotten rocks the rope must be handled with special care, lest it should dislodge loose stones on to those below. Similar care must be given to handholds and footholds, for the same reason. When a horizontal traverse has to be made across very difficult rocks, a dangerous situation may arise unless at both ends of the traverse there are firm positions. Mutual assistance on hard rocks takes all manner of forms: two, or even three, people climbing on one another's shoulders, or using an ice axe propped up by others for a foothold. The great principle is that of co-operation, all the members of the party climbing with reference to the others, and not as independent units; each when moving must know what the climber in front and the one behind are doing. After bad weather steep rocks are often found covered with a veneer of ice (verglas), which may even render them inaccessible. Crampons are useful on such occasions.		Every year, 120 to 150 people die in small avalanches in the Alps alone. The vast majority of Alpine victims are reasonably experienced male skiers aged 20–35 but also include ski instructors and guides.[citation needed] However a significant number of climbers are killed in Scottish avalanches often on descent and often triggered by the victims. There is always a lot of pressure to risk a snow crossing. Turning back takes a lot of extra time and effort, supreme leadership, and most importantly there is seldom an avalanche that proves the right decision was made. Making the decision to turn around is especially hard if others are crossing the slope, but any next person could become the trigger.		There are many types of avalanche, but two types are of the most concern. These are Snow Avalanches and Ice Avalanches:		Snow Avalanches		Dangerous slides are most likely to occur on the same slopes preferred by many skiers: long and wide open, few trees or large rocks, 30 to 45 degrees of angle, large load of fresh snow, soon after a big storm, on a slope "lee to the storm". Solar radiation can trigger slides as well. These will typically be a point release or wet slough type of avalanche. The added weight of the wet slide can trigger a slab avalanche. Ninety percent of reported victims are caught in avalanches triggered by themselves or others in their group.[citation needed]		Ice Avalanches are a hazard that exists in glaciated mountain ranges. They are caused by the collapse of unstable ice blocks from a steep or overhanging part of a glacier, referred to as a hanging glacier. Due to the fact that they are part of a glacier, ice avalanches can have large amounts of rock in them. Ice avalanches are quite dangerous because they can travel long distances, sometimes as far as 8 km out onto the glacier valley floor. Ice avalanches are a common everyday occurrence in ranges such as the Alaska Range, Saint Elias Mountains, or Columbia Icefield.		When going off-piste or travelling in alpine terrain, parties are advised to always carry:		They are also advised to have had avalanche training. Ironically, expert skiers who have avalanche training make up a large percentage of avalanche fatalities; perhaps because they are the ones more likely to ski in areas prone to avalanches, and certainly because most people do not practice enough with their equipment to be truly fast and efficient rescuers.[citation needed]		Even with proper rescue equipment and training, there is a one-in-five chance of dying if caught in a significant avalanche, and only a 50/50 chance of being found alive if buried more than a few minutes. The best solution is to learn how to avoid risky conditions.[citation needed]		For travel on slopes consisting of ice or hard snow, crampons are a standard part of a mountaineer's equipment. While step-cutting can sometimes be used on snow slopes of moderate angle, this can be a slow and tiring process, which does not provide the higher security of crampons. However, in soft snow or powder, crampons are easily hampered by balling of snow, which reduces their effectiveness. In either case, an ice axe not only assists with balance but provides the climber with the possibility of self-arrest in case of a slip or fall. On a true ice slope however, an ice axe is rarely able to effect a self-arrest. As an additional safety precaution on steep ice slopes, the climbing rope is attached to ice screws buried into the ice.		Snow slopes are very common, and usually easy to ascend. At the foot of a snow or ice slope is generally a big crevasse, called a bergschrund, where the final slope of the mountain rises from a snow-field or glacier. Such bergschrunds are generally too wide to be stepped across, and must be crossed by a snow bridge, which needs careful testing and a painstaking use of the rope. A steep snow slope in bad condition may be dangerous, as the whole body of snow may start as an avalanche. Such slopes are less dangerous if ascended directly, rather than obliquely, for an oblique or horizontal track cuts them across and facilitates movement of the mass. New snow lying on ice is especially dangerous. Experience is needed for determining the feasibility of advancement over snow in doubtful condition. Snow on rocks is usually rotten unless it is thick; snow on snow is likely to be sound. A day or two of fine weather will usually bring new snow into sound condition. Snow cannot lie at a very steep angle, though it often deceives the eye as to its slope. Snow slopes seldom exceed 40°. Ice slopes may be much steeper. Snow slopes in early morning are usually hard and safe, but the same in the afternoon are quite soft and possibly dangerous; hence the advantage of an early start.		Crevasses are the slits or deep chasms formed in the substance of a glacier as it passes over an uneven bed. They may be open or hidden. In the lower part of a glacier the crevasses are open. Above the snow-line they are frequently hidden by arched-over accumulations of winter snow. The detection of hidden crevasses requires care and experience. After a fresh fall of snow they can only be detected by sounding with the pole of the ice axe, or by looking to right and left where the open extension of a partially hidden crevasse may be obvious. The safeguard against accident is the rope, and no one should ever cross a snow-covered glacier unless roped to one, or even better to two companions. Anyone venturing onto crevasses should be trained in crevasse rescue.		The primary dangers caused by bad weather center on the changes it causes in snow and rock conditions, making movement suddenly much more arduous and hazardous than under normal circumstances.		Whiteouts make it difficult to retrace a route while rain may prevent taking the easiest line only determined as such under dry conditions. In a storm the mountaineer who uses a compass for guidance has a great advantage over a merely empirical observer. In large snow-fields it is, of course, easier to go wrong than on rocks, but intelligence and experience are the best guides in safely navigating objective hazards.		Summer thunderstorms may produce intense lightning.[17] If a climber happens to be standing on or near the summit, they risk being struck. There are many cases where people have been struck by lightning while climbing mountains. In most mountainous regions, local storms develop by late morning and early afternoon. Many climbers will get an "alpine start"; that is before or by first light so as to be on the way down when storms are intensifying in activity and lightning and other weather hazards are a distinct threat to safety. High winds can speed the onset of hypothermia, as well as damage equipment such as tents used for shelter.[17][18] Under certain conditions, storms can also create waterfalls which can slow or stop climbing progress. A notable example is the Föhn wind acting upon the Eiger.		Rapid ascent can lead to altitude sickness.[17][19] The best treatment is to descend immediately. The climber's motto at high altitude is "climb high, sleep low", referring to the regimen of climbing higher to acclimatise but returning to lower elevation to sleep. In the South American Andes, the chewing of coca leaves has been traditionally used to treat altitude sickness symptoms.		Common symptoms of altitude sickness include severe headache, sleep problems, nausea, lack of appetite, lethargy and body ache. Mountain sickness may progress to HACE (High Altitude Cerebral Edema) and HAPE (High Altitude Pulmonary Edema), both of which can be fatal within 24 hours.[17][19][20]		In high mountains, atmospheric pressure is lower and this means that less oxygen is available to breathe.[17] This is the underlying cause of altitude sickness. Everyone needs to acclimatise, even exceptional mountaineers that have been to high altitude before.[21] Generally speaking, mountaineers start using bottled oxygen when they climb above 7,000 m. Exceptional mountaineers have climbed 8000-metre peaks (including Everest) without oxygen, almost always with a carefully planned program of acclimatisation.		Solar radiation increases significantly as the atmosphere gets thinner with increasing altitude thereby absorbing less ultraviolet radiation.[17][18] Snow cover reflecting the radiation can amplify the effects by up to 75% increasing the risks and damage from sunburn and snow blindness.[18]		In 2005, researcher and mountaineer John Semple established that above-average ozone concentrations on the Tibetan Plateau may pose an additional risk to climbers.[22]		Some mountains are active volcanoes as in the case of the many stratovolcanoes that form the highest peaks in island arcs and in parts of the Andes. Some of these volcanic mountains may cause several hazards if they erupt, such as lahars, pyroclastic flows, rockfalls, lava flows, heavy tephra fall, volcanic bomb ejections and toxic gases.		There are two main styles of mountaineering: Expedition style and Alpine style.		A mountaineer who adopts Alpine style is referred to as an Alpine Mountaineer. Alpine Mountaineers are typically found climbing in medium-sized glaciated mountain areas such as the Alps or Rocky Mountains. Medium-sized generally refers to altitudes in the intermediate altitude (7,000' to 12,000') and first half of high altitude (12,000' to 18,000') ranges. However, alpine style ascents have been done throughout history on extreme altitude (18,000' to 29,000') peaks also, albeit in lower volume to expedition style ascents. Alpine style refers to a particular style of mountain climbing that involves a mixture of snow climbing, ice climbing, rock climbing, and glacier travel, where climbers generally single carry their loads between camps, in a single push for the summit. Light and fast is the mantra of the Alpine Mountaineer.		The term alpine style contrasts with expedition style (as commonly undertaken in the Himalayan region or other large ranges of the world), which could be viewed as slow and heavy, where climbers may use porters, pack animals, glacier airplanes, cooks, multiple carries between camps, usage of fixed lines etc. A mountaineer who adopts this style of climbing is referred to as an Expedition Mountaineer. Expedition mountaineers still employ the skill sets of the alpine mountaineer, except they have to deal with expanded time scale, more severe weather exposure, and additional skills unique to expeditionary climbing. The prevalence of expedition style climbing in the Himalaya is largely a function of the nature of the mountains in the region. Because Himalayan base camps can take days or weeks to trek to, and Himalayan mountains can take weeks or perhaps even months to climb, a large number of personnel and amount of supplies may be helpful. This is why expedition style climbing is frequently used on large and isolated peaks in the Himalaya. In Europe and North America there is less of a need for expedition style climbing on most medium-sized mountains. These mountains can often be easily accessed by car or air, are at a lower altitude and can be climbed in a shorter time scale. Expedition style mountaineering can be found in the larger high altitude and extreme altitude North American ranges such as the Alaska Range and Saint Elias Mountains. These remote mountaineering destinations can require up to a 2-week trek by foot, just to make it to base camp. Most expeditions in these regions choose a glacier flight to basecamp. Route length in days from basecamp can vary in these regions, typically from 10 days to 1 month during the climbing season. Winter mountaineering on major peaks in these ranges can generally consume between 30 and 60 days depending on the route, and can generally only be tackled via expedition style mountaineering during this season.		The differences between, and advantages and disadvantages of, the two kinds of climbing are as follows:[7]		Mountaineering has become a popular sport throughout the world. In Europe the sport largely originated in the Alps, and is still immensely popular there. Other notable mountain ranges frequented by climbers include the Caucasus, the Pyrenees, Rila mountains, the Tatra Mountains and the rest of the Carpathian Mountains, as well the Sudetes. In North America climbers frequent the Rocky Mountains, the Sierra Nevada of California, the Cascades of the Pacific Northwest, the high peaks of The Alaska Range and Saint Elias Mountains.		There has been a long tradition of climbers going on expeditions to the Greater Ranges, a term generally used for the Andes (e.g. the Cordillera Blanca in Peru) and the high peaks of Asia including the Himalayas (e.g. the Mount Everest of Nepal/Tibet), Karakoram, Hindu Kush, Pamir Mountains, Tien Shan and Kunlun Mountains. In the past this was often on exploratory trips or to make first ascents. With the advent of cheaper, long-haul air travel, mountaineering holidays in the Greater Ranges are now undertaken much more frequently and ascents of even Everest and Vinson Massif (the highest mountain in Antarctica) are offered as a "package holiday".		Other mountaineering areas of interest include the Southern Alps of New Zealand, the Coast Mountains of British Columbia, the Scottish Highlands and the mountains of Scandinavia, especially Norway.		The true accessibility of mountaineering is found in a combination of factors that can be divided into typical destination accessibility and real (physical) accessibility. These two groups have then been subdivided into elements related to transport links and in situ services (in the case of destination accessibility) and to factors covering the social, economic, weather and psychophysical environments, as well as the presence of mountaineering activities (in the case of real accessibility). Destination accessibility is defined as the ability to provide appropriate access for visitors into a destination and also ensure all necessary services facilitating a convenient stay. Destination accessibility must be understood as the combination of: transport links factors, in situ services factors. While the factors that influence mountaineering destination accessibility (transport links and in situ services) are not much different from any other excursion, real accessibility factors are. Real accessibility may be dependent (in a positive or negative way) upon social factors, economic factors, weather factors, psychophysical factors, carrying capacity factors.[6]		
Raiders of the Lost Ark (also known as Indiana Jones and the Raiders of the Lost Ark) is a 1981 American action adventure film directed by Steven Spielberg, with a screenplay written by Lawrence Kasdan, from a story by George Lucas and Philip Kaufman. It was produced by Frank Marshall for Lucasfilm Ltd., with Lucas and Howard Kazanjian as executive producers. Starring Harrison Ford, it was the first installment in the Indiana Jones film franchise to be released, though it is the second in internal chronological order. It pits Indiana Jones (Ford) against a group of Nazis who are searching for the Ark of the Covenant, which Adolf Hitler believes will make his army invincible. The film co-stars Karen Allen as Indiana's former lover, Marion Ravenwood; Paul Freeman as Indiana's rival, French archaeologist René Belloq; John Rhys-Davies as Indiana's sidekick, Sallah; Ronald Lacey as Gestapo agent Arnold Toht; and Denholm Elliott as Indiana's colleague, Marcus Brody.		The film originated from Lucas' desire to create a modern version of the serials of the 1930s and 1940s. Production was based at Elstree Studios, England; but filming also took place in La Rochelle, France, Tunisia, Hawaii, and California from June to September 1980.		Released on June 12, 1981, Raiders of the Lost Ark became the year's top-grossing film and remains one of the highest-grossing films ever made. It was nominated for eight Academy Awards in 1982, including Best Picture, and won four for Best Art Direction, Film Editing, Sound, and Visual Effects with a fifth Academy Award: a Special Achievement Award for Sound Effects Editing. The film's critical and popular success led to three additional films, Indiana Jones and the Temple of Doom (1984), Indiana Jones and the Last Crusade (1989), and Indiana Jones and the Kingdom of the Crystal Skull (2008), with a fifth slated for 2020; the television series The Young Indiana Jones Chronicles (1992–1996), and 15 video games as of 2009. In 1999, the film was included in the U.S. Library of Congress' National Film Registry as having been deemed "culturally, historically, or aesthetically significant".		Raiders of the Lost Ark is often ranked as one of the greatest films of all time, both in the action-adventure genre, and in general.[3] The film also ranks #2 on Empire's 2008 list of the 500 greatest movies of all time.[4]						In 1936, archaeologist Indiana Jones braves an ancient booby-trapped temple in Peru and retrieves a golden idol. He is confronted by rival archaeologist René Belloq and the indigenous Hovito people. Surrounded and outnumbered, Indy surrenders the idol to Belloq and escapes aboard a waiting floatplane.		Jones returns to his teaching position at Marshall College, where he is interviewed by two Army Intelligence agents. They inform him that the Nazis are searching for his old mentor, Abner Ravenwood, under whom Jones studied at the University of Chicago. The Nazis know that Ravenwood is the leading expert on the ancient city of Tanis in Egypt, and that he possesses the headpiece of the Staff of Ra. Jones deduces that the Nazis are searching for the Ark of the Covenant – the Nazis believe that if they acquire the Ark, their armies will become invincible. The Staff of Ra is the key to finding the Well of Souls, a secret chamber in which the Ark is buried.		The agents authorize Jones to recover the Ark to prevent the Nazis from obtaining it. He travels to Nepal and discovers that Abner has died, and the headpiece is in the possession of Ravenwood's daughter Marion. Jones visits Marion at her tavern, where she reveals her bitter feelings toward him from a previous romantic affair. She physically rebuffs his offer to buy the headpiece, and Jones leaves. Shortly after, a group of thugs arrive with their Nazi commander, Arnold Toht. Toht threatens Marion to get the headpiece, but when Jones returns to the bar to fight the Nazis and save Marion, her bar is accidentally set on fire; during the fight, the headpiece ends up in the fire and Toht severely burns his hand trying to take the hot headpiece, and flees the tavern screaming. Indy and Marion escape with the headpiece, and Marion decides to accompany Indy in his search for the Ark so he can repay his debt to her.		The pair travels to Cairo, where they meet up with Indy's friend Sallah, a skilled excavator. Sallah informs them that Belloq and the Nazis are digging for the Well of Souls with a replica of the headpiece (created from the scar on Toht's hand). They quickly realize the Nazi headpiece is incomplete and that the Nazis are digging in the wrong place. The Nazis kidnap Marion and it appears to Jones that she is killed in an exploding truck. After a confrontation with Belloq in a local bar, Indy and Sallah infiltrate the Nazi dig site and use their staff to correctly locate the Ark. Indy discovers Marion is alive, bound and gagged in a tent, but does not release her for fear of alerting the Nazis. Indy, Sallah, and a small group of diggers unearth the Well of Souls and acquire the Ark. Belloq and Nazi officer Colonel Dietrich arrive, seize the Ark from Jones, throwing Marion into the Well of Souls with him before sealing it back up. Jones and Marion escape to a local airstrip, where Jones has a fistfight with a Nazi mechanic and destroys the flying wing that was to transport the Ark to Berlin. The panicked Nazis remove the Ark in a truck and set off for Cairo, but Jones catches them and retakes it. He makes arrangements to take the Ark to London aboard a tramp steamer.		The next day, a Nazi U-boat appears and intercepts the ship. Belloq and Dietrich seize the Ark and Marion but cannot locate Jones, who stows away aboard the U-boat and travels with them to an island in the Aegean Sea. Once there, Belloq plans to test the power of the Ark before presenting it to Hitler. Jones reveals himself and threatens to destroy the Ark with a panzerfaust, but Belloq calls his bluff and Jones surrenders rather than destroy such an important historical artifact. The Nazis take Indy and Marion to an area where the Ark will be opened and tie them to a post to observe. Belloq performs a ceremonial opening of the Ark, which appears to contain nothing but sand, all that remains of the Ten Commandments. Suddenly, angelic ghost-like beings emerge from the Ark. Indy cautions Marion to keep her eyes closed and not to observe what happens next. Belloq and the others look on in astonishment as the apparitions are suddenly revealed to be angels of death. A vortex of flame forms above the Ark and shoots bolts of fiery energy into the gathered Nazi soldiers, killing them all. As Belloq, Toht and Dietrich all scream in terror, the Ark turns its fury on them: Dietrich's head shrivels up, Toht's face is melted off his skull and Belloq's head explodes. Flames then engulf the remains of the doomed assembly, save for Indy and Marion, and the pillar of fire rises into the sky. The Ark's lid is blasted high into the air before dropping back down onto the Ark and sealing it. Jones and Marion find their ropes burned off and embrace.		In Washington, D.C., the Army Intelligence agents inform Jones and Marcus Brody that the Ark is someplace safe and will be studied by "top men". The Ark is shown being stored in a giant government warehouse among countless similar crates.		Producer Frank Marshall played a pilot in the airplane fight sequence. The stunt team was ill, so he took the role instead. The result was three days in a hot cockpit, which he joked was over "140 degrees".[5] Pat Roach plays the Nazi mechanic with whom Jones brawls in this sequence, as well as a massive sherpa who battles Jones in Marion's bar. He had the rare opportunity to be killed twice in one film.[16] Special-effects supervisor Dennis Muren made a cameo as a Nazi spy on the seaplane Jones takes from San Francisco to Manila.[17]		In 1973, George Lucas wrote The Adventures of Indiana Smith.[18] Like Star Wars, which he also wrote, it was an opportunity to create a modern version of the film serials of the 1930s and 1940s.[5] Lucas discussed the concept with Philip Kaufman, who worked with him for several weeks and came up with the Ark of the Covenant as the plot device.[19] Kaufman was told about the Ark by his dentist when he was a child.[20] The project stalled when Clint Eastwood hired Kaufman to direct The Outlaw Josey Wales.[19] Lucas shelved the idea, deciding to concentrate on his outer space adventure that would become Star Wars. In late May 1977, Lucas was in Hawaii, trying to escape the enormous success of Star Wars. Friend and colleague Steven Spielberg was also there, on vacation from work on Close Encounters of the Third Kind. While building a sand castle at the Mauna Kea Beach Hotel,[21] Spielberg expressed an interest in directing a James Bond film. Lucas convinced his friend Spielberg that he had conceived a character "better than James Bond" and explained the concept of Raiders of the Lost Ark. Spielberg loved it, calling it "a James Bond film without the hardware",[22] although he told Lucas that the surname 'Smith' was not right for the character. Lucas replied, "OK. What about 'Jones'?" Indiana was the name of Lucas' Alaskan Malamute, whose habit of riding in the passenger seat as Lucas drove was also the inspiration for Star Wars' Chewbacca.[5] Spielberg was at first reluctant to sign on, as Lucas had told him that he would want Spielberg for an entire trilogy, and Spielberg did not want to work on two more scripts. Lucas told him, however, that he already had the next two movies written, so Spielberg agreed. But when the time came for the first sequel, it was revealed that Lucas had nothing written for either sequel.[5]		The following year, Lucas focused on developing Raiders and the Star Wars sequel The Empire Strikes Back, during which Lawrence Kasdan and Frank Marshall joined the project as screenwriter and producer respectively. Between January 23 – 27, 1978, for nine hours a day, Lucas, Kasdan, and Spielberg discussed the story and visual ideas. Spielberg came up with Jones being chased by a boulder,[5] which was inspired by Carl Barks' Uncle Scrooge comic "The Seven Cities of Cibola". Lucas later acknowledged that the idea for the idol mechanism in the opening scene and deadly traps later in the film were inspired by several Uncle Scrooge comics.[23] Lucas came up with a submarine, a monkey giving the Hitler salute, and Marion punching Jones in Nepal.[22] Kasdan used a 100-page transcript of their conversations for his first script draft,[24] which he worked on for six months.[5] Ultimately, some of their ideas were too grand and had to be cut: a mine chase,[25] an escape in Shanghai using a rolling gong as a shield,[26] and a jump from an airplane in a raft, all of which made it into the prequel, Indiana Jones and the Temple of Doom.[5]		Spielberg and Lucas disagreed on the character: although Spielberg saw him as a Bondian playboy, Lucas felt the character's academic and adventurer elements made him complex enough.[27] Spielberg had a darker vision of Jones, interpreting him as an alcoholic similar to Humphrey Bogart's character Fred C. Dobbs in The Treasure of the Sierra Madre (1948). This characterization fell away during the later drafts, though elements survive in Jones's reaction when he believes Marion to be dead.[22] Costume designer Deborah Nadoolman credits Secret of the Incas (1954), starring Charlton Heston, as an influence on the development of the character, noting that the crew watched the film together several times. Nadoolman based the look of Ford's costume on that of Heston's, and observed that Indiana is a "kinder and gentler" Harry Steele.[28]		Initially, the film was rejected by every major studio in Hollywood, mostly due to the $20 million budget and the deal Lucas was offering.[29] Eventually Paramount agreed to finance the film, with Lucas negotiating a five-picture deal. By April 1980, Kasdan's fifth draft was produced, and production was getting ready to shoot at Elstree Studios, with Lucas trying to keep costs down.[8] With four illustrators, Raiders of the Lost Ark was Spielberg's most storyboarded film of his career to date, further helping the film economically. He and Lucas agreed on a tight schedule to keep costs down and to follow the "quick and dirty" feel of the old Saturday matinée serials. Special effects were done using puppets, miniature models, animation, and camera trickery.[5] "We didn't do 30 or 40 takes; usually only four. It was like silent film--shoot only what you need, no waste", Spielberg said. "Had I had more time and money, it would have turned out a pretentious movie."[30]		Principal photography began on June 23, 1980, at La Rochelle, France, with scenes involving the Nazi submarine,[8] which had been rented from the production of Das Boot. The U-boat pen was a real one from World War II.[5] The crew moved to Elstree Studios[8] for the Well of Souls scenes, the opening sequence temple interiors and Marion Ravenwood's bar.[31] The Well of Souls scene required 7,000 snakes. The only venomous snakes were the cobras, but one crew member was bitten on set by a python.[5] The bulk of the snakes numbers were made up with giant but harmless legless lizards known as Scheltopusiks (Pseudopus apodus) which occur from the Balkan Peninsula of southeastern Europe to Central Asia. Growing to 1.3 m they are the largest legless lizards in the world and are often mistaken for snakes despite some very obvious differences such as the presence of eyelids and external ear openings, which are both absent from all snakes, and a notched rather than forked tongue. In the finished film, during the scene in which Indiana comes face-to-face with the cobra, a reflection in glass screen that protected Ford from the snake was seen,[5] an issue that was corrected in the 2003 digitally-enhanced re-release. Unlike Indiana, neither Ford nor Spielberg has a fear of snakes, but Spielberg said that seeing all the snakes on the set writhing around made him "want to puke".[5]		The opening scene in the Peruvian jungle was filmed on the island of Kauai, one of the islands of Hawaii, to where Spielberg would return for Jurassic Park. The "temple" location is on the Huleia River, on the Kipu Ranch, south from Kaumualii Highway on the east coast, just south of Lihue, the island's main town. Kipu is a working cattle ranch, not generally open to the public.[32] The Peruvian section (but actually filmed in Hawaii) featured live tarantulas of a Mexican species (Brachypelma) on Harrison Ford and Alfred Molina, and are harmless to humans, and in fact of a species which are commonly kept as exotic pets. A fiberglass boulder 22 feet (7 m) in diameter was made for the scene where Indiana escapes from the temple; Spielberg was so impressed by production designer Norman Reynolds' realization of his idea that he gave the boulder a more prominent role in the film and told Reynolds to let the boulder roll another 50 feet (15 m).[33]		The scenes set in Egypt were filmed in Tunisia, and the canyon where Indiana threatens to blow up the Ark was shot in Sidi Bouhlel, just outside Tozeur.[34] The canyon location had been used for the Tatooine scenes from 1977's Star Wars (many of the location crew members were the same for both films[5]) where R2-D2 was attacked by Jawas.[5] The Tanis scenes were filmed in nearby Sedala, a harsh place due to heat and disease. Several cast and crew members fell ill and Rhys-Davies defecated in his costume during one shot.[5] Spielberg averted disease by eating only canned foods from England, but did not like the area and quickly condensed the scheduled six-week shoot to four-and-a-half weeks. Much was improvised: the scene where Marion puts on her dress and attempts to leave Belloq's tent was improvised as was the entire plane fight. During that scene's shooting, a wheel went over Ford's knee and tore his left leg's cruciate ligament, but he refused local medical help and simply put ice on it.[5]		The fight scenes in the town were filmed in Kairouan, while Ford was suffering from dysentery. Stuntman Terry Richards had practiced for weeks with his sword to create the scripted fight scene, choreographing a fight between the swordsman and Jones' whip.[35] However, after filming the initial shots of the scene, after lunch due to Ford's dysentery, Ford and Spielberg agreed to cut the scene down to a gunshot, with Ford saying to Spielberg "Let's just shoot the sucker".[36] It was later voted in at No.5 on Playboy magazine's list of best all time scenes.[35][37] Most of the truck chase was shot by second unit director Michael D. Moore following Spielberg's storyboards, including Indiana being dragged by the truck (performed by stuntman Terry Leonard), in tribute to a famous Yakima Canutt stunt. Spielberg then filmed all the shots with Ford himself in and around the truck cab.[5] Lucas directed a few other second unit shots, in particular the monkey giving the Nazi salute.[30]		The interior staircase set in Washington, D.C. was filmed in San Francisco's City Hall. The University of the Pacific's campus in Stockton, California, stood in for the exterior of the college where Jones works, while his classroom and the hall where he meets the American intelligence agents was filmed at the Royal Masonic School for Girls in Rickmansworth, Hertfordshire, England, which was again used in The Last Crusade. His home exteriors were filmed in San Rafael, California.[31] Opening sequence exteriors were filmed in Kauai, Hawaii, with Spielberg wrapping in September in 73 days, finishing under schedule in contrast to his previous film, 1941.[8][22] The Washington, D.C. coda, although it appeared in the script's early drafts, was not included in early edits but was added later when it was realized that there was no resolution to Jones' relationship with Marion.[38] Shots of the Douglas DC-3 Jones flies on to Nepal were taken from Lost Horizon, and a street scene was from a shot in The Hindenburg.[30] Filming of Jones boarding a Boeing Clipper flying-boat was complicated by the lack of a surviving aircraft. Eventually, a post-war British Short Solent flying-boat formerly owned by Howard Hughes was located in California and substituted.[39]		The special visual effects for Raiders were provided by Industrial Light & Magic and include: a matte shot to establish the Pan Am flying boat in the water[40] and miniature work to show the plane taking off and flying, superimposed over a map; animation effects for the beam in the Tanis map room; and a miniature car and passengers[41] superimposed over a matte painting for a shot of a Nazi car being forced off a cliff. The bulk of effects shots were featured in the climactic sequence wherein the Ark of the Covenant (which was designed by Brian Muir and Keith Short) is opened and God's wrath is unleashed. This sequence featured animation, a woman to portray a beautiful spirit's face, rod puppet spirits moved through water to convey a sense of floating,[42] a matte painting of the island, and cloud tank effects to portray clouds. The melting of Toht's head was done by exposing a gelatine and plaster model of Ronald Lacey's head to a heat lamp with an under-cranked camera, while Dietrich's crushed head was a hollow model from which air was withdrawn. When the film was originally submitted to the Motion Picture Association of America, it received an R rating because of the scene in which Belloq's head explodes. The filmmakers were able to receive a PG rating when they added a veil of fire over the exploding head scene. (PG-13 rating was not created until 1984.[17]) The firestorm that cleanses the canyon at the finish was a miniature canyon filmed upside down.[42]		Ben Burtt, the sound effects supervisor, made extensive use of traditional foley work in yet another of the production's throwbacks to days of the Republic serials. He selected a .30-30 Winchester rifle for the sound of Jones' pistol. Sound effects artists struck leather jackets and baseball gloves with a baseball bat to create a variety of punching noises and body blows. For the snakes in the Well of Souls sequence, fingers running through cheese casserole and sponges sliding over concrete were used for the slithering noises. The sliding lid on a toilet cistern provided the sound for the opening of the Ark, and the sound of the boulder in the opening is a car rolling down a gravel driveway in neutral. Burtt also used, as he did in many of his films, the ubiquitous Wilhelm scream when a Nazi falls from a truck. In addition to his use of such time-honored foley work, Burtt also demonstrated the modern expertise honed during his award-winning work on Star Wars. He employed a synthesizer for the sounds of the Ark, and mixed dolphins' and sea lions' screams for those of the spirits within.[43]		John Williams composed the score for Raiders of the Lost Ark, which was the only score in the series performed by the London Symphony Orchestra, the same orchestra that performed the scores for the Star Wars saga. The score most notably features the well-known "Raiders March". This piece came to symbolize Indiana Jones and was later used in the scores for the other three films. Williams originally wrote two different candidates for Jones's theme, but Spielberg enjoyed them so much that he insisted that both be used together in what became the "Raiders March".[44] The alternately eerie and apocalyptic theme for the Ark of the Covenant is also heard frequently in the score, with a more romantic melody representing Marion and, more broadly, her relationship with Jones. The score as a whole received an Oscar nomination for Best Original Score, but lost to the score to Chariots of Fire composed by Vangelis.		The only video game based exclusively on the film is Raiders of the Lost Ark, released in 1982 by Atari for their Atari 2600 console.[45] The first third of the video game Indiana Jones' Greatest Adventures, released in 1994 by JVC for Nintendo's Super Nintendo Entertainment System, is based entirely on the film. Several of the film's sequences are reproduced (the boulder run and the showdown with the Cairo Swordsman among them); however, several inconsistencies with the film are present in the game, such as Nazi soldiers and bats being present in the Well of Souls sequence, for example.[46] The game was developed by LucasArts and Factor 5. In the 1999 game Indiana Jones and the Infernal Machine, a bonus level brings Jones back to the Peruvian temple of the film's opening scene.[47] In 2008, to coincide with the release of Kingdom of the Crystal Skull, Lego released the Lego Indiana Jones line—which included building sets based on Raiders of the Lost Ark[48]—and LucasArts published a video game based on the toyline, Lego Indiana Jones: The Original Adventures, which was developed by Traveller's Tales.[49]		Marvel Comics published a comic book adaptation of the film by writer Walt Simonson and artists John Buscema and Klaus Janson. It was published as Marvel Super Special #18[50] and as a three-issue limited series.[51] This was followed with the comic book series The Further Adventures of Indiana Jones which was published monthly from January 1983 through March 1986.		In 1981, Kenner released a 12-inch (30 cm) doll of Indiana Jones, and the following year they released nine action figures of the film's characters, three playsets, as well as toys of the Nazi truck and Jones' horse. They also released a board game. In 1984, miniature metal versions of the characters were released for a role playing game, The Adventures of Indiana Jones, and in 1995 Micro Machines released die-cast toys of the film's vehicles.[52] Hasbro released action figures based on the film, ranging from 3 to 12 inches (7.6 to 30.5 cm), to coincide with Kingdom of the Crystal Skull on May 1, 2008.[53] Later in 2008, and in 2011, two high-end sixth scale (1:6) collectible action figures were released by Sideshow Collectibles, and Hot Toys, Ltd. respectively. A novelization by Ryder Windham was released in April 2008 by Scholastic to tie in with the release of Kingdom of the Crystal Skull. A previous novelization by Scottish author Campbell Armstrong (under the pseudonym Campbell Black) was concurrently released with the film in 1981. A book about the making of the film was also released, written by Derek Taylor.		The film was released on VHS, Betamax and VideoDisc in pan and scan only, and on laserdisc in both pan and scan and widescreen. For its 1999 VHS re-issue, the film was remastered in THX and made available in widescreen. The outer package was retitled Indiana Jones and the Raiders of the Lost Ark for consistency with the film's prequel and sequel. The subsequent DVD release in 2003 features this title as well. The title in the film itself remains unchanged, even in the restored DVD print. In the DVD, two subtle digital revisions were added. First, a connecting rod from the giant boulder to an offscreen guidance track in the opening scene was removed from behind the running Harrison Ford; second, a reflection in the glass partition separating Ford from the cobra in the Well of Souls was removed.[54] Shortly before the theatrical release of Kingdom of the Crystal Skull, Raiders (along with The Temple of Doom and The Last Crusade) was re-released on DVD with additional extra features not included on the previous set on May 13, 2008. The film was released on Blu-ray Disc in September 2012.[55] Previously, only Kingdom of the Crystal Skull had been available on Blu-ray.[citation needed]		Raiders of the Lost Ark opened at #14 and grossed $1,673,731 from 267 theaters ($6,269 theater average) during its opening weekend. In total, the IMAX release grossed $3,125,613 domestically.[56] The film, made on an $18 million budget, grossed $384 million worldwide throughout its theatrical releases. In North America, it was by some distance the highest-grossing film of 1981,[57] and remains one of the top twenty-five highest-grossing films ever made when adjusted for inflation.[58] Box Office Mojo estimates that the film sold more than 70 million tickets in the US in its initial theatrical run.[59]		The film was subsequently nominated for nine Academy Awards, including Best Picture, in 1982 and won four (Best Sound, Best Film Editing, Best Visual Effects, and Best Art Direction-Set Decoration (Norman Reynolds, Leslie Dilley, and Michael D. Ford). It also received a Special Achievement Award for Sound Effects Editing. It won numerous other awards, including a Grammy Award and Best Picture at the People's Choice Awards. Spielberg was also nominated for a Golden Globe Award.[60]		The film was highly acclaimed by critics and audiences alike. In his review for The New York Times, Vincent Canby praised the film, calling it, "one of the most deliriously funny, ingenious and stylish American adventure movies ever made."[61] Roger Ebert in his review for the Chicago Sun-Times wrote, "Two things, however, make Raiders of the Lost Ark more than just a technological triumph: its sense of humor and the droll style of its characters [...] We find ourselves laughing in surprise, in relief, in incredulity at the movie's ability to pile one incident upon another in an inexhaustible series of inventions."[62] He later added it to his list of "Great Movies".[63] Rolling Stone said the film was "the ultimate Saturday action matinee–a film so funny and exciting it can be enjoyed any day of the week."[64] Bruce Williamson of Playboy claimed: "There's more excitement in the first ten minutes of Raiders than any movie I have seen all year. By the time the explosive misadventures end, any movie-goer worth his salt ought to be exhausted."[65] Stephen Klain of Variety also praised the film. Yet, making an observation that would revisit the franchise with its next film, he felt that the film was surprisingly violent and bloody for a PG-rated film.[66]		There were some dissenting voices: Sight & Sound described it as an "expensively gift-wrapped Saturday afternoon pot-boiler",[67] and New Hollywood champion Pauline Kael, who once contended that she only got "really rough" on large films that were destined to be hits but were nonetheless "atrocious",[68] found the film to be a "machine-tooled adventure" from a pair of creators who "think just like the marketing division".[69] (Lucas later named a villain, played by Raiders Nazi strongman Pat Roach, in his 1988 fantasy film Willow after Kael.)[68] On Rotten Tomatoes, the film has a 94% "Certified Fresh" rating on Rotten Tomatoes,[70] as well as a 85% rating on Metacritic, indicating "universal acclaim".[71]		Following the success of Raiders, a prequel, The Temple of Doom, and two sequels, The Last Crusade and Kingdom of the Crystal Skull, were produced, with a third sequel set for release in 2020.[75] A television series, entitled The Young Indiana Jones Chronicles, was also spun off from this film, and details the character's early years. Numerous other books, comics, and video games have also been produced.		In 1998, the American Film Institute placed the film at number 60 on its top 100 films of the first century of cinema. In 2007, AFI updated the list and placed it at number 66. They also named it as the 10th most thrilling film, and named Indiana Jones as the second greatest hero. In 1999, the film was deemed "culturally, historically, or aesthetically significant" by the U.S. Library of Congress and selected for preservation in the National Film Registry. Indiana Jones has become an icon, being listed as Entertainment Weekly's third favorite action hero, while noting "some of the greatest action scenes ever filmed are strung together like pearls" in this film.[76]		An amateur, near shot-for-shot remake was made by Chris Strompolos, Eric Zala, and Jayson Lamb, then children in Ocean Springs, Mississippi. It took the boys seven years to finish, from 1982 to 1989. After production of the film, called Raiders of the Lost Ark: The Adaptation, it was shelved and forgotten until 2003, where it was discovered by Eli Roth[77][78] and acclaimed by Spielberg himself, who congratulated the boys on their hard work and said he looked forward to seeing their names on the big screen.[79] Scott Rudin and Paramount Pictures purchased the trio's life rights with the goal of producing a film based on their adventures making their remake.[80][81]		In 2014, film director Steven Soderbergh published an experimental black-and-white version of the film, with the original soundtrack and dialogue replaced by an electronic soundtrack. Soderbergh said his intention was to encourage viewers to focus on Spielberg's extraordinary staging and editing: "This filmmaker forgot more about staging by the time he made his first feature than I know to this day."[82]		Assessing the film's legacy in 1997, Bernard Weinraub, film critic for The New York Times, which had initially reviewed the film as "deliriously funny, ingenious, and stylish",[83] maintained that "the decline in the traditional family G-rated film, for 'general' audiences, probably began" with the appearance of Raiders of the Lost Ark. "Whether by accident or design," found Weinraub, "the filmmakers made a comic nonstop action film intended mostly for adults but also for children."[84] Eight years later, in 2005, viewers of Channel 4 rated the film as the 20th-best family film of all time, with Spielberg taking best over-all director honors.[85]		On Empire magazine's list of the 500 Greatest Movies of All Time, Raiders ranked second, beaten only by The Godfather.[86]		In conjunction with the Blu-ray release, a limited one-week release in IMAX theaters was announced for September 7, 2012. Steven Spielberg and sound designer Ben Burtt supervised the format conversion. No special effects or other visual elements were altered, but the audio was enhanced for surround sound.[87]		In December 2012, the University of Chicago's admissions department received a package in the mail addressed to Henry Walton Jones, Jr., Indiana Jones' full name. The address on the stamped package was listed for a hall that was the former home of the university's geology and geography department. Inside the manila envelope was a detailed replica journal similar to the one Jones used in the movie, as well as postcards and pictures of Marion Ravenwood. The admissions department posted pictures of the contents on its Internet blog, looking for any information about the package. It was discovered that the package was part of a set to be shipped from Guam to Italy that had been sold on eBay. The package with the journal had fallen out in transit and a postal worker had sent it to the university, as it had a complete address and postage, which turned out to be fake. All contents were from a Guam "prop replicator" who sells them all over the world. The university will display its replica in the main lobby of the Oriental Institute.[88]		
Fridtjof Nansen (/ˈfrɪd.tʃɒf ˈnænsən/ FRID-choff NAN-sən; 10 October 1861 – 13 May 1930) was a Norwegian explorer, scientist, diplomat, humanitarian and Nobel Peace Prize laureate. In his youth he was a champion skier and ice skater. He led the team that made the first crossing of the Greenland interior in 1888, traversing the island on cross-country skis. He won international fame after reaching a record northern latitude of 86°14′ during his North Pole expedition of 1893–96. Although he retired from exploration after his return to Norway, his techniques of polar travel and his innovations in equipment and clothing influenced a generation of subsequent Arctic and Antarctic expeditions.		Nansen studied zoology at the Royal Frederick University in Christiania (renamed Oslo in 1925), and later worked as a curator at the Bergen Museum where his research on the central nervous system of lower marine creatures earned him a doctorate and helped establish modern theories of neurology. After 1896 his main scientific interest switched to oceanography; in the course of his research he made many scientific cruises, mainly in the North Atlantic, and contributed to the development of modern oceanographic equipment. As one of his country's leading citizens, in 1905 Nansen spoke out for the ending of Norway's union with Sweden, and was instrumental in persuading Prince Carl of Denmark to accept the throne of the newly independent Norway. Between 1906 and 1908 he served as the Norwegian representative in London, where he helped negotiate the Integrity Treaty that guaranteed Norway's independent status.		In the final decade of his life, Nansen devoted himself primarily to the League of Nations, following his appointment in 1921 as the League's High Commissioner for Refugees. In 1922 he was awarded the Nobel Peace Prize for his work on behalf of the displaced victims of the First World War and related conflicts. Among the initiatives he introduced was the "Nansen passport" for stateless persons, a certificate recognised by more than 50 countries. He worked on behalf of refugees until his sudden death in 1930, after which the League established the Nansen International Office for Refugees to ensure that his work continued. This office received the Nobel Peace Prize for 1938. Nansen was honoured by many nations, and his name is commemorated in numerous geographical features, particularly in the polar regions.						The Nansen family originated in Denmark. Hans Nansen (1598–1667), a trader, was an early explorer of the White Sea region of the Arctic Ocean. In later life he settled in Copenhagen, becoming the city's borgmester in 1654. Later generations of the family lived in Copenhagen until the mid-18th century, when Ancher Antoni Nansen moved to Norway (then ruled by Denmark). His son, Hans Leierdahl Nansen (1764–1821), was a magistrate first in the Trondheim district, later in Jæren. After Norway's separation from Denmark in 1814, he entered national political life as the representative for Stavanger in the first Storting, and became a strong advocate of union with Sweden. After suffering a paralytic stroke in 1821 Hans Leierdahl Nansen died, leaving a four-year-old son, Baldur Fridtjof Nansen, the explorer's father.[1]		Baldur was a lawyer without ambitions for public life, who became Reporter to the Supreme Court of Norway. He married twice, the second time to Adelaide Johanne Thekla Isidore Bølling Wedel-Jarlsberg from Bærum, a niece of Herman Wedel-Jarlsberg who had helped frame the Norwegian constitution of 1814 and was later the Swedish king's Norwegian Viceroy.[2] Baldur and Adelaide settled at Store Frøen, an estate at Aker, a few kilometres north of Norway's capital city, Christiania (since renamed Oslo). The couple had three children; the first died in infancy, the second, born 10 October 1861, was Fridtjof Nansen.[3][4]		Store Frøen's rural surroundings shaped the nature of Nansen's childhood. In the short summers the main activities were swimming and fishing, while in the autumn the chief pastime was hunting for game in the forests. The long winter months were devoted mainly to skiing, which Nansen began to practice at the age of two, on improvised skis.[4] At the age of 10 he defied his parents and attempted the ski jump at the nearby Huseby installation. This exploit had near-disastrous consequences, as on landing the skis dug deep into the snow, pitching the boy forward: "I, head first, described a fine arc in the air ... [W]hen I came down again I bored into the snow up to my waist. The boys thought I had broken my neck, but as soon as they saw there was life in me ... a shout of mocking laughter went up."[3] Nansen's enthusiasm for skiing was undiminished, though as he records, his efforts were overshadowed by those of the skiers from the mountainous region of Telemark, where a new style of skiing was being developed. "I saw this was the only way", wrote Nansen later.[5]		At school, Nansen worked adequately without showing any particular aptitude.[4] Studies took second place to sports, or to expeditions into the forests where he would live "like Robinson Crusoe" for weeks at a time.[6] Through such experiences Nansen developed a marked degree of self-reliance. He became an accomplished skier and a highly proficient skater. Life was disrupted when, in the summer of 1877, Adelaide Nansen died suddenly. Distressed, Baldur Nansen sold the Store Frøen property and moved with his two sons to Christiania.[7] Nansen's sporting prowess continued to develop; at 18 he broke the world one-mile (1.6 km) skating record, and in the following year won the national cross-country skiing championship, a feat he would repeat on 11 subsequent occasions.[8]		In 1880 Nansen passed his university entrance examination, the examen artium. He decided to study zoology, claiming later that he chose the subject because he thought it offered the chance of a life in the open air. He began his studies at the Royal Frederick University in Christiania early in 1881.[9]		Early in 1882 Nansen took "...the first fatal step that led me astray from the quiet life of science."[10] Professor Robert Collett of the university's zoology department proposed that Nansen take a sea voyage, to study Arctic zoology at first hand. Nansen was enthusiastic, and made arrangements through a recent acquaintance, Captain Axel Krefting, commander of the sealer Viking.[10] The voyage began on 11 March 1882 and extended over the following five months. In the weeks before sealing started, Nansen was able to concentrate on scientific studies.[11] From water samples he showed that, contrary to previous assumption, sea ice forms on the surface of the water rather than below. His readings also demonstrated that the Gulf Stream flows beneath a cold layer of surface water.[12] Through the spring and early summer Viking roamed between Greenland and Spitsbergen in search of seal herds. Nansen became an expert marksman, and on one day proudly recorded that his team had shot 200 seal. In July, Viking became trapped in the ice close to an unexplored section of the Greenland coast; Nansen longed to go ashore, but this was impossible.[11] However, he began to develop the idea that the Greenland icecap might be explored, or even crossed.[8] On 17 July the ship broke free from the ice, and early in August was back in Norwegian waters.[11]		Nansen did not resume formal studies at the university. Instead, on Collett's recommendation, he accepted a post as curator in the zoological department of the Bergen Museum. He was to spend the next six years of his life there—apart from a six-month sabbatical tour of Europe—working and studying with leading figures such as Gerhard Armauer Hansen, the discoverer of the leprosy bacillus,[13] and Daniel Cornelius Danielssen, the museum's director who had turned it from a backwater collection into a centre of scientific research and education.[14] Nansen's chosen area of study was the then relatively unexplored field of neuroanatomy, specifically the central nervous system of lower marine creatures. Before leaving for his sabbatical in February 1886 he published a paper summarising his research to date, in which he stated that "anastomoses or unions between the different ganglion cells" could not be demonstrated with certainty. This unorthodox view, confirmed by the simultaneous researches of the embryologist Wilhelm His and the psychiatrist August Forel. Nansen is considered the first Norwegian defender of the neuron theory, originally proposed by Santiago Ramon y Cajal. His subsequent paper, The Structure and Combination of Histological Elements of the Central Nervous System, published in 1887, became his doctoral thesis.[15]		The idea of an expedition across the Greenland icecap grew in Nansen's mind throughout his Bergen years. In 1887, after the submission of his doctoral thesis, he finally began organising this project. Before then, the two most significant penetrations of the Greenland interior had been those of Adolf Erik Nordenskiöld in 1883, and Robert Peary in 1886. Both had set out from Disko Bay on the western coast, and had travelled about 160 kilometres (100 mi) eastward before turning back.[16] By contrast, Nansen proposed to travel from east to west, ending rather than beginning his trek at Disko Bay. A party setting out from the inhabited west coast would, he reasoned, have to make a return trip, as no ship could be certain of reaching the dangerous east coast and picking them up.[17] By starting from the east—assuming that a landing could be made there—Nansen's would be a one-way journey towards a populated area. The party would have no line of retreat to a safe base; the only way to go would be forward, a situation that fitted Nansen's philosophy completely.[18]		Nansen rejected the complex organisation and heavy manpower of other Arctic ventures, and instead planned his expedition for a small party of six. Supplies would be manhauled on specially designed lightweight sledges. Much of the equipment, including sleeping bags, clothing and cooking stoves, also needed to be designed from scratch.[19] These plans received a generally poor reception in the press;[20] one critic had no doubt that "if [the] scheme be attempted in its present form ... the chances are ten to one that he will ... uselessly throw his own and perhaps others' lives away".[21] The Norwegian parliament refused to provide financial support, believing that such a potentially risky undertaking should not be encouraged. The project was eventually launched with a donation from a Danish businessman, Augustin Gamél; the rest came mainly from small contributions from Nansen's countrymen, through a fundraising effort organised by students at the university.[22]		Despite the adverse publicity, Nansen received numerous applications from would-be adventurers. He wanted expert skiers, and attempted to recruit from the skiers of Telemark, but his approaches were rebuffed.[23] Nordenskiöld had advised Nansen that Sami people, from Finland in the far north of Norway, were expert snow travellers, so Nansen recruited a pair, Samuel Balto and Ole Nielsen Ravna. The remaining places went to Otto Sverdrup, a former sea-captain who had more recently worked as a forester; Oluf Christian Dietrichson, an army officer, and Kristian Kristiansen, an acquaintance of Sverdrup's. All had experience of outdoor life in extreme conditions, and were experienced skiers.[24] Just before the party's departure, Nansen attended a formal examination at the university, which had agreed to receive his doctoral thesis. In accordance with custom he was required to defend his work before appointed examiners acting as "devil's advocates". He left before knowing the outcome of this process.[24]		On 3 June 1888 Nansen's party was picked up from the north-western Icelandic port of Ísafjörður by the sealer Jason. A week later the Greenland coast was sighted, but progress was hindered by thick pack ice. On 17 July, with the coast still 20 kilometres (12 mi) away, Nansen decided to launch the small boats; they were within sight of the Sermilik Fjord, which Nansen believed would offer a route up on to the icecap.[25]		The expedition left Jason "in good spirits and with the highest hopes of a fortunate result", according to Jason's captain.[25] There followed days of extreme frustration for the party as, prevented by weather and sea conditions from reaching the shore, they drifted southwards with the ice. Most of this time was spent camping on the ice itself—it was too dangerous to launch the boats. By 29 July they were 380 kilometres (240 mi) south of the point where they had left the ship. On that day they finally reached land, but were too far south to begin the crossing. After a brief rest, Nansen ordered the team back into the boats and to begin rowing north.[26]		During the next 12 days the party battled northward along the coast through the ice floes. On the first day they encountered a large Eskimo encampment near Cape Steen Bille,[27] and there were further occasional contacts with the nomadic native population as the journey continued. On 11 August, when they had covered about 200 kilometres (120 mi) and had reached Umivik Bay, Nansen decided that although they were still far south of his intended starting place, they needed to begin the crossing before the season became too advanced for travel.[28] After landing at Umivik, they spent the next four days preparing for their journey, and on the evening of 15 August they set out. They were heading north-west, towards Christianhaab (now Qasigiannguit) on the west Greenland shores of Disko Bay, 600 kilometres (370 mi) away.[29]		Over the next few days the party struggled to ascend the inland ice over a treacherous surface with many hidden crevasses. The weather was generally bad; on one occasion all progress was halted for three days by violent storms and continuous rain.[30] On 26 August Nansen concluded that there was now no chance of reaching Christianhaab by mid-September, when the last ship was due to leave. He therefore ordered a change of course, almost due west towards Godthaab (now Nuuk), a shorter journey by at least 150 kilometres (93 mi). The rest of the party, according to Nansen, "hailed the change of plan with acclamation".[31] They continued climbing, until on 11 September they had reached a height of 8,922 feet (2,719 m) above sea level, the summit of the icecap with temperatures dropping to −50 °F (−46 °C) at night. From then on the downward slope made travelling easier, although the terrain was difficult and the weather remained hostile.[32] Progress was slow because of fresh snowfalls which made dragging the sledges as hard as pulling them through sand. By 26 September they had battled their way down to the edge of a fjord that ran westward towards Godthaab. From their tent, some local willows and parts of the sledges Sverdrup constructed a makeshift boat, and on 29 September Nansen and Sverdrup began the last stage of the journey, rowing down the fjord.[33] Four days later, on 3 October 1888, they reached Godthaab, where they were greeted by the town's Danish representative. His first words were to inform Nansen that he had been awarded his doctorate, a matter that "could not have been more remote from my thoughts at that moment".[34] The crossing had been accomplished in 49 days, making 78 days in total since they had left the Jason; throughout the journey the team had maintained careful meteorological, geographical and other records relating to the previously unexplored interior.[8] The rest of the team arrived in Godthaab on 12 October.		Nansen soon learned that no ship was likely to call at Godthaab until the following spring, though they were able to send letters back to Norway via a boat leaving Ivigtut at the end of October. He and his party therefore spent the next seven months in Greenland, hunting, fishing and studying the life of the local inhabitants.[35] On 15 April 1889 the Danish ship Hvidbjørnen finally entered the harbour, and Nansen and his comrades prepared to depart. "It was not without sorrow that we left this place and these people, among whom we had enjoyed ourselves so well", Nansen recorded.[36]		Hvidbjørnen reached Copenhagen on 21 May 1889. News of the crossing had preceded its arrival, and Nansen and his companions were feted as heroes. This welcome, however, was dwarfed by the reception in Christiania a week later, when crowds of between thirty and forty thousand—a third of the city's population—thronged the streets as the party made its way to the first of a series of receptions. The interest and enthusiasm generated by the expedition's achievement led directly to the formation that year of the Norwegian Geographical Society.[37]		Nansen accepted the position of curator of the Royal Frederick University's zoology collection, a post which carried a salary but involved no duties; the university was satisfied by the association with the explorer's name.[37] Nansen's main task in the following weeks was writing his account of the expedition, but he found time late in June to visit London, where he met the Prince of Wales (the future King Edward VII), and addressed a meeting of the Royal Geographical Society (RGS).[37]		The RGS president, Sir Mountstuart Elphinstone Grant Duff, said that Nansen has claimed "the foremost place amongst northern travellers", and later awarded him the Society's prestigious Founder's Medal. This was one of many honours Nansen received from institutions all over Europe.[38] He was invited by a group of Australians to lead an expedition to Antarctica, but declined, believing that Norway's interests would be better served by a North Pole conquest.[39]		On 11 August 1889 Nansen announced his engagement to Eva Sars, the daughter of Michael Sars, a zoology professor who had died when Eva was 11 years old.[40] The couple had met some years previously, at the skiing resort of Frognerseteren, where Nansen recalled seeing "two feet sticking out of the snow".[38] Eva was three years older than Nansen, and despite the evidence of this first meeting, was an accomplished skier. She was also a celebrated classical singer who had been coached in Berlin by Désirée Artôt, one-time paramour of Tchaikovsky. The engagement surprised many; since Nansen had previously expressed himself forcefully against the institution of marriage, Otto Sverdrup assumed he had read the message wrongly. The wedding took place on 6 September 1889, less than a month after the engagement.[40]		Nansen first began to consider the possibility of reaching the North Pole by using the natural drift of the polar ice when, in 1884, he read the theories of Henrik Mohn, the distinguished Norwegian meteorologist. Artifacts found on the Greenland coast had been identified as coming from the lost US Arctic exploration vessel Jeannette, which had been crushed and sunk in June 1881 on the opposite side of the Arctic Ocean, off the Siberian coast. Mohn surmised that the location of the artefacts indicated the existence of an ocean current, flowing from east to west all the way across the polar sea, possibly over the pole itself. A strong enough ship might therefore enter the frozen Siberian sea, and drift to the Greenland coast via the pole.[41][42]		This idea remained with Nansen during following years. After his triumphant return from Greenland he began to develop a detailed plan for a polar venture, which he made public in February 1890 at a meeting of the recently formed Norwegian Geographical Society. Previous expeditions, he argued, had approached the North Pole from the west, and had failed because they were working against the prevailing east-west current. The secret of success was to work with this current. A workable plan, Nansen said, would require a small, strong and manoeuvrable ship capable of carrying fuel and provisions for twelve men for five years. The ship would sail to the approximate location of Jeannette's sinking, and would enter the ice. It would then drift west with the current towards the pole and beyond it, eventually reaching the sea between Greenland and Spitsbergen.[41]		Many experienced polar hands were dismissive of Nansen's plans. The retired American explorer Adolphus Greely called the idea "an illogical scheme of self-destruction".[43] Sir Allen Young, a veteran of the searches for Sir John Franklin's lost expedition,[44] and Sir Joseph Hooker, who had sailed south with James Clark Ross in 1839–43, were equally dismissive.[45][46] However, after an impassioned speech Nansen secured the support of the Norwegian parliament, which voted him a grant. The balance of funding was met by private donations and from a national appeal.[42]		Nansen chose Colin Archer, Norway's leading shipbuilder and naval architect, to design and build a suitable ship for the planned expedition. Using the toughest oak timbers available, and an intricate system of crossbeams and braces throughout its length, Archer built a vessel of extraordinary strength. Its rounded hull was designed so that it would slip upwards out of the grip of packing ice. Speed and sailing performance were secondary to the requirement of making the ship a safe and warm shelter during a predicted lengthy confinement.[42] With an overall length of 128 feet (39 m) and a beam of 36 feet (11 m), the length-to-beam ratio of just over three gave the ship its stubby appearance,[47] justified by Archer thus: "A ship that is built with exclusive regard to its suitability for [Nansen's] object must differ essentially from any known vessel."[48] The ship was launched by Eva Nansen at Archer's yard at Larvik, on 6 October 1892, and was named Fram, in English "Forward".[47]		From thousands of applicants, Nansen selected a party of twelve. Otto Sverdrup from the Greenland expedition was appointed captain of Fram and second-in-command of the expedition.[49] Competition for places on the voyage was such that reserve Army lieutenant and dog-driving expert Hjalmar Johansen signed on as ship's stoker, the only position available.[49][50]		Fram left Christiania on 24 June 1893, cheered on by thousands of well-wishers.[51] After a slow journey around the coast, the final port of call was Vardø, in the far north-east of Norway.[50] Fram left Vardø on 21 July, following the North-East Passage route pioneered by Nordenskiöld in 1878–79, along the northern coast of Siberia. Progress was impeded by fog and ice conditions in the mainly uncharted seas.[52] The crew also experienced the dead water phenomenon, where a ship's forward progress is impeded by friction caused by a layer of fresh water lying on top of heavier salt water.[53] Nevertheless, Cape Chelyuskin, the most northerly point of the Eurasian continental mass, was passed on 10 September. Ten days later, as Fram approached the area in which Jeannette had been crushed, heavy pack ice was sighted at around latitude 78°N. Nansen followed the line of the pack northwards to a position recorded as 78°49′N, 132°53′E, before ordering engines stopped and the rudder raised. From this point Fram's drift began.[54]		The first weeks in the ice were frustrating, as the drift moved unpredictably, sometimes north, sometimes south; by 19 November Fram's latitude was south of that at which she had entered the ice.[55] Only after the turn of the year, in January 1894, did the northerly direction become generally settled; the 80° mark was finally passed on 22 March.[56] Nansen calculated that, at this rate, it might take the ship five years to reach the pole.[57] As the ship's northerly progress continued at a rate rarely above a mile (1.6 km) a day, Nansen began privately to consider a new plan—a dog sledge journey towards the pole.[57] With this in mind he began to practice dog-driving, making many experimental journeys over the ice. In November Nansen announced his plan: when the ship passed latitude 83° he and Hjalmar Johansen would leave the ship with the dogs and make for the pole while Fram, under Sverdrup, continued its drift until it emerged from the ice in the North Atlantic. After reaching the pole, Nansen and Johansen would make for the nearest known land, the recently discovered and sketchily mapped Franz Josef Land. They would then cross to Spitzbergen where they would find a ship to take them home.[58]		The crew spent the rest of the 1894–95 winter preparing clothing and equipment for the forthcoming sledge journey. Kayaks were built, to be carried on the sledges until needed for the crossing of open water.[59] Preparations were interrupted early in January when violent tremors shook the ship. The crew disembarked, fearing that the vessel would be crushed, but Fram proved herself equal to the danger. On 8 January 1895 the ship's position was 83°34′N, above Greely's previous Farthest North record of 83°24.[60][n 1]		On 14 March 1895, after two false starts and with the ship's position at 84°4′N,[62] Nansen and Johansen began their journey.[63] Nansen had allowed 50 days to cover the 356 nautical miles (660 km; 410 mi) to the pole, an average daily journey of seven nautical miles (13 km; 8.1 mi). After a week of travel a sextant observation indicated that they were averaging nine nautical miles a day, (17 km; 10 mi), putting them ahead of schedule.[64] However, uneven surfaces made skiing more difficult, and their speeds slowed. They also realised that they were marching against a southerly drift, and that distances travelled did not necessarily equate to northerly progression.[65] On 3 April Nansen began to wonder whether the pole was, indeed, attainable. Unless their speed improved, their food would not last them to the pole and then on to Franz Josef Land.[65] He confided in his diary: "I have become more and more convinced we ought to turn before time."[66] On 7 April, after making camp and observing that the way ahead was "a veritable chaos of iceblocks stretching as far as the horizon", Nansen decided to turn south. He recorded the latitude of the final northerly camp as 86°13.6′N, almost three degrees beyond the previous Farthest North mark.[67]		At first Nansen and Johansen made good progress south, but on 13 April suffered a serious setback when both of their chronometers stopped. Without knowing the correct time, it was impossible for them to calculate their longitude and thus navigate their way accurately to Franz Josef Land. They restarted the watches on the basis of Nansen's guess that they were at longitude 86°E, but from then on were uncertain of their true position.[68]		Towards the end of April they observed the tracks of an Arctic fox, the first trace they had seen of a living creature other than their dogs since leaving Fram.[69] Soon they began to see bear tracks, and by the end of May seals, gulls and whales were in evidence. On 31 May, by Nansen's calculations, they were only 50 nautical miles (93 km; 58 mi) from Cape Fligely, the northernmost known point of Franz Josef Land.[70] However, travel conditions worsened as the warmer weather caused the ice to break up. On 22 June the pair decided to rest on a stable ice floe while they repaired their equipment and gathered their strength for the next stage of their journey. They remained on the floe for a month.[71] The day after leaving this camp Nansen recorded: "At last the marvel has come to pass—land, land, and after we had almost given up our belief in it!"[72] Whether this still-distant land was Franz Josef Land or a new discovery they did not know—they had only a rough sketch map to guide them.[n 2] On 6 August they reached the edge of the ice, where they shot the last of their dogs—they had been killing the weakest regularly since 24 April, to feed the others. They then lashed their two kayaks together, raised a sail and made for the land.[74]		It was soon clear that this land was part of a group of islands. As they moved slowly southwards, Nansen tentatively identified a headland as Cape Felder, on the western edge of Franz Josef Land. Towards the end of August, as the weather grew colder and travel became increasingly difficult, Nansen decided to camp for the winter.[75] In a sheltered cove, with stones and moss for building materials, the pair erected a hut which was to be their home for the next eight months.[76] With ready supplies of bear, walrus and seal to keep their larder stocked, their principal enemy was not hunger but inactivity.[77] After muted Christmas and New Year celebrations, in slowly improving weather they began to prepare to leave their refuge, but it was 19 May 1896 before they were able to resume their journey.[78]		On 17 June, during a stop for repairs after the kayaks had been attacked by a walrus, Nansen thought he heard sounds of a dog barking, and of voices. He went to investigate, and a few minutes later saw the figure of a man approaching.[79] It was the British explorer Frederick Jackson, who was leading an expedition to Franz Josef Land and was camped at Cape Flora on the nearby Northbrook Island. The two were equally astonished by their encounter; after some awkward hesitation Jackson asked: "You are Nansen, aren't you?", and received the reply "Yes, I am Nansen."[80] Johansen was soon picked up, and the pair were taken to Cape Flora where, during the following weeks, they recuperated from their ordeal. Nansen later wrote that he could "still scarcely grasp" the sudden change of fortune;[81] had it not been for the walrus attack that caused the delay, the two parties might have been unaware of each other's existence.[79]		On 7 August Nansen and Johansen boarded Jackson's supply ship Windward, and sailed for Vardø where they arrived on the 13th. They were greeted by Hans Mohn, the originator of the polar drift theory, who was in the town by chance.[82] The world was quickly informed by telegram of Nansen's safe return,[83] but as yet there was no news of Fram. Taking the weekly mail steamer south, Nansen and Johansen reached Hammerfest on 18 August, where they learned that Fram had been sighted. She had emerged from the ice north and west of Spitsbergen, as Nansen had predicted, and was now on her way to Tromsø. She had not passed over the pole, nor exceeded Nansen's northern mark.[84] Without delay Nansen and Johansen sailed for Tromsø, where they were reunited with their comrades.[85]		The homeward voyage to Christiania was a series of triumphant receptions at every port. On 9 September Fram was escorted into Christiania's harbour and welcomed by the largest crowds the city had ever seen.[86] The crew were received by King Oscar, and Nansen, reunited with family, remained at the palace for several days as a special guest. Tributes arrived from all over the world; typical was that from the British mountaineer Edward Whymper, who wrote that Nansen had made "almost as great an advance as has been accomplished by all other voyages in the nineteenth century put together".[85]		Nansen's first task on his return was to write his account of the voyage. This he did remarkably quickly, producing 300,000 words of Norwegian text by November 1896; the English translation, titled Farthest North, was ready in January 1897. The book was an instant success, and secured Nansen's long-term financial future.[87] Nansen included without comment the one significant adverse criticism of his conduct, that of Greely, who had written in Harper's Weekly on Nansen's decision to leave Fram and strike for the pole: "It passes comprehension how Nansen could have thus deviated from the most sacred duty devolving on the commander of a naval expedition."[88]		During the 20 years following his return from the Arctic, Nansen devoted most of his energies to scientific work. In 1897 he accepted a professorship in zoology at the Royal Frederick University,[89] which gave him a base from which he could tackle the major task of editing the reports of the scientific results of the Fram expedition. This was a much more arduous task than writing the expedition narrative. The results were eventually published in six volumes, and according to a later polar scientist, Robert Rudmose-Brown, "were to Arctic oceanography what the Challenger expedition results had been to the oceanography of other oceans."[90]		In 1900 Nansen became director of the Christiania-based International Laboratory for North Sea Research, and helped found the International Council for the Exploration of the Sea.[91] Through his connection with the latter body, in the summer of 1900 Nansen embarked on his first visit to Arctic waters since the Fram expedition, a cruise to Iceland and Jan Mayen Land on the oceanographic research vessel Michael Sars, named after Eva's father.[92] Shortly after his return he learned that his Farthest North record had been passed, by members of the Duke of the Abruzzi's Italian expedition. They had reached 86°34′N on 24 April 1900, in an attempt to reach the North Pole from Franz Josef Land.[93] Nansen received the news philosophically: "What is the value of having goals for their own sake? They all vanish ... it is merely a question of time."[94]		Nansen was now considered an oracle by all would-be explorers of the north and south polar regions. Abruzzi had consulted him, as had the Belgian Adrien de Gerlache, each of whom took expeditions to the Antarctic.[95] Although Nansen refused to meet his own countryman and fellow-explorer Carsten Borchgrevink (whom he considered a fraud),[96] he gave advice to Robert Falcon Scott on polar equipment and transport, prior to the 1901–04 Discovery Expedition. At one point Nansen seriously considered leading a South Pole expedition himself, and asked Colin Archer to design two ships. However, these plans remained on the drawing board.[97]		By 1901 Nansen's family had expanded considerably. A daughter, Liv, had been born just before Fram set out; a son, Kåre was born in 1897 followed by a daughter, Irmelin, in 1900 and a second son Odd in 1901.[98] The family home, which Nansen had built in 1891 from the profits of his Greenland expedition book,[99] was now too small. Nansen acquired a plot of land in the Lysaker district and built, substantially to his own design, a large and imposing house which combined some of the characteristics of an English manor house with features from the Italian renaissance. The house was ready for occupation by April 1902; Nansen called it Polhøgda (in English "polar heights"), and it remained his home for the rest of his life. A fifth and final child, son Asmund, was born at Polhøgda in 1903.[100]		The union between Norway and Sweden, imposed by the Great Powers in 1814, had been under considerable strain through the 1890s, the chief issue in question being Norway's rights to its own consular service.[101] Nansen, although not by inclination a politician, had spoken out on the issue on several occasions in defence of Norway's interests.[102] It seemed, early in the 20th century that agreement between the two countries might be possible, but hopes were dashed when negotiations broke down in February 1905. The Norwegian government fell, and was replaced by one led by Christian Michelsen, whose programme was one of separation from Sweden.[101]		In February and March Nansen published a series of newspaper articles which placed him firmly in the separatist camp. The new prime minister wanted Nansen in the cabinet, but Nansen had no political ambitions.[103] However, at Michelsen's request he went to Berlin and then to London where, in a letter to The Times, he presented Norway's legal case for a separate consular service to the English-speaking world. On 17 May 1905, Norway's Constitution Day, Nansen addressed a large crowd in Christiania, saying: "Now have all ways of retreat been closed. Now remains only one path, the way forward, perhaps through difficulties and hardships, but forward for our country, to a free Norway".[104] He also wrote a book, Norway and the Union with Sweden, specifically to promote Norway's case abroad.[105]		On 23 May the Storting passed the Consulate Act establishing a separate consular service. King Oscar refused his assent; on 27 May the Norwegian cabinet resigned, but the king would not recognise this step. On 7 June the Storting unilaterally announced that the union with Sweden was dissolved. In a tense situation the Swedish government agreed to Norway's request that the dissolution should be put to a referendum of the Norwegian people.[101] This was held on 13 August 1905 and resulted in an overwhelming vote for separation, at which point King Oscar relinquished the crown of Norway while retaining the Swedish throne. A second referendum, held in November, determined that the new independent state should be a monarchy rather than a republic. In anticipation of this, Michelsen's government had been considering the suitability of various princes as candidates for the Norwegian throne. Faced with King Oscar's refusal to allow anyone from his own House of Bernadotte to accept the crown, the favoured choice was Prince Charles of Denmark. In July 1905 Michelsen sent Nansen to Copenhagen on a secret mission to persuade Charles to accept the Norwegian throne.[106] Nansen was successful; shortly after the second referendum Charles was proclaimed king, taking the name Haakon VII. He and his wife, the British princess Maud, were crowned in the Nidaros Cathedral in Trondheim on 22 June 1906.[101]		In April 1906 Nansen was appointed Norway's first Minister in London.[107] His main task was to work with representatives of the major European powers on an Integrity Treaty which would guarantee Norway's position.[108] Nansen was popular in England, and got on well with King Edward, though he found court functions and diplomatic duties disagreeable; "frivolous and boring" was his description.[107] However, he was able to pursue his geographical and scientific interests through contacts with the Royal Geographical Society and other learned bodies. The Treaty was signed on 2 November 1907, and Nansen considered his task complete. Resisting the pleas of, among others, King Edward that he should remain in London, on 15 November Nansen resigned his post.[109] A few weeks later, still in England as the king's guest at Sandringham, Nansen received word that Eva was seriously ill with pneumonia. On 8 December he set out for home, but before he reached Polhøgda he learned, from a telegram, that Eva had died.[110]		After a period of mourning, Nansen returned to London. He had been persuaded by his government to rescind his resignation until after King Edward's state visit to Norway in April 1908. His formal retirement from the diplomatic service was dated 1 May 1908, the same day on which his university professorship was changed from zoology to oceanography. This new designation reflected the general character of Nansen's more recent scientific interests.[111] In 1905 he had supplied the Swedish physicist Walfrid Ekman with the data which established the principle in oceanography known as the Ekman spiral. Based on Nansen's observations of ocean currents recorded during the Fram expedition, Ekman concluded that the effect of wind on the sea's surface produced currents which "formed something like a spiral staircase, down towards the depths".[112] In 1909 Nansen combined with Bjørn Helland-Hansen to publish an academic paper, The Norwegian Sea: its Physical Oceanography, based on the Michael Sars voyage of 1900.[113]		Nansen had by now retired from polar exploration, the decisive step being his release of Fram to his fellow-Norwegian Roald Amundsen, who was planning a North Pole expedition.[114] When Amundsen made his controversial change of plan and set out for the South Pole, Nansen stood by him.[115][n 3] Between 1910 and 1914, Nansen participated in a several oceanographic voyages. In 1910, aboard the Norwegian naval vessel Fridtjof, he carried out researches in the northern Atlantic,[117] and in 1912 he took his own yacht, Veslemøy, to Bear Island and Spitsbergen. The main objective of the Veslemøy cruise was the investigation of salinity in the North Polar Basin.[118] One of Nansen's lasting contributions to oceanography was his work designing instruments and equipment; the "Nansen bottle" for taking deep water samples remained in use into the 21st century, in a version updated by Shale Niskin.[119]		At the request of the Royal Geographical Society, Nansen began work on a study of Arctic discoveries, which developed into a two-volume history of the exploration of the northern regions up to the beginning of the 16th century. This was published in 1911 as Nord i Tåkeheimen ("In Northern Mists").[117] That year he renewed an acquaintance with Kathleen Scott, wife of Robert Falcon Scott whose Terra Nova Expedition had sailed for Antarctica in 1910. Biographer Roland Huntford has asserted, without any compelling evidence, that Nansen and Kathleen Scott, the sculptor and wife of Captain Robert Falcon Scott, had a brief love affair.[120] Louisa Young in her biography of Lady Scott refutes the claim.[121] Many women were attracted to Nansen, and he had a reputation as a womaniser.[122] His personal life was troubled around this time; in January 1913 he received news of the suicide of Hjalmar Johansen, who had returned in disgrace from Amundsen's successful South Pole expedition.[123] In March 1913, Nansen's youngest son Asmund died after a long illness.[118]		In the summer of 1913 Nansen travelled to the Kara Sea, by the invitation of Jonas Lied, as part of a delegation investigating a possible trade route between Western Europe and the Siberian interior. The party then took a steamer up the Yenisei River to Krasnoyarsk, and travelled on the Trans-Siberian Railway to Vladivostok before turning for home. Nansen published a report from the trip in Through Siberia.[124] The life and culture of the Russian peoples aroused in Nansen an interest and sympathy he would carry through to his later life.[125] Immediately before the First World War, Nansen joined Helland-Hansen in an oceanographical cruise in eastern Atlantic waters.[126]		On the outbreak of war in 1914 Norway declared its neutrality, alongside Sweden and Denmark. Nansen was appointed president of the Norwegian Union of Defence, but had few official duties, and continued with his professional work as far as circumstances permitted.[126] As the war progressed, the loss of Norway's overseas trade led to acute shortages of food in the country, which became critical in April 1917 when the United States entered the war and placed extra restrictions on international trade. Nansen was dispatched to Washington by the Norwegian government; after months of discussion he secured food and other supplies in return for the introduction of a rationing system. When his government hesitated over the deal, he signed the agreement on his own initiative.[127]		Within a few months of the war's end in November 1918 a draft agreement had been accepted by the Paris Peace Conference to create a League of Nations, as a means of resolving disputes between nations by peaceful means.[128] The foundation of the League at this time was providential as far as Nansen was concerned, giving him a new outlet for his restless energy.[129]		He became president of the Norwegian League of Nations Society, and although the Scandinavian nations with their traditions of neutrality initially held themselves aloof, his advocacy helped to ensure that Norway became a full member of the League in 1920, and he became one of its three delegates to the League's General Assembly.[130]		In April 1920, at the League's request, Nansen began organising the repatriation of around half a million prisoners of war, stranded in various parts of the world. Of these, 300,000 were in Russia which, gripped by revolution and civil war, had little interest in their fate.[8] Nansen was able to report to the Assembly in November 1920 that around 200,000 men had been returned to their homes. "Never in my life", he said, "have I been brought into touch with so formidable an amount of suffering."[131] Nansen continued this work for a further two years until, in his final report to the Assembly in 1922, he was able to state that 427,886 prisoners had been repatriated to around 30 different countries. In paying tribute to his work, the responsible committee recorded that the story of his efforts "would contain tales of heroic endeavour worthy of those in the accounts of the crossing of Greenland and the great Arctic voyage."[132]		Even before this work was complete, Nansen was involved in a further humanitarian effort. On 1 September 1921, prompted by the British delegate Philip Noel-Baker, he accepted the post of the League's High Commissioner for Refugees.[133][134] His main brief was the resettlement of around two million Russian refugees displaced by the upheavals of the Russian Revolution. At the same time he tried to tackle the urgent problem of famine in Russia; following a widespread failure of crops around 30 million people were threatened with starvation and death. Despite Nansen's pleas on behalf of the starving, Russia's revolutionary government was feared and distrusted internationally, and the League was reluctant to come to its peoples' aid.[135] Nansen had to rely largely on fundraising from private organisations, and his efforts met with limited success.[8] Later he was to express himself bitterly on the matter:		There was in various transatlantic countries such an abundance of maize, that the farmers had to burn it as fuel in their railway engines. At the same time the ships in Europe were idle, for there were no cargoes. Simultaneously there were thousands, nay millions of unemployed. All this, while thirty million people in the Volga region—not far away and easily reached by our ships—were allowed to starve and die.[136]		A major problem impeding Nansen's work on behalf of refugees was that most of them lacked documentary proof of identity or nationality. Without legal status in their country of refuge, their lack of papers meant they were unable to go anywhere else. To overcome this, Nansen devised a document that became known as the "Nansen passport", a form of identity for stateless persons that was in time recognised by more than 50 governments, and which allowed refugees to cross borders legally. Among the more distinguished holders of Nansen passports were the artist Marc Chagall, the composer Igor Stravinsky, and the dancer Anna Pavlova.[137] Although the passport was created initially for refugees from Russia, it was extended to cover other groups.[138] After the Greco-Turkish wars of 1919–1922 Nansen travelled to Constantinople to negotiate the resettlement of hundreds of thousands of refugees, mainly ethnic Greeks who had fled from Turkey after the defeat of the Greek Army. The impoverished Greek state was unable to take them in,[8] and so Nansen devised a scheme for a population exchange whereby half a million Turks in Greece were returned to Turkey, with full financial compensation, while further loans facilitated the absorption of the refugee Greeks into their homeland.[139] Despite some controversy over the principle of a population exchange,[138] the plan was implemented successfully over a period of several years. In November 1922, while attending the Conference of Lausanne, Nansen learned that he had been awarded the Nobel Peace Prize for 1922. The citation referred to "his work for the repatriation of the prisoners of war, his work for the Russian refugees, his work to bring succour to the millions of Russians afflicted by famine, and finally his present work for the refugees in Asia Minor and Thrace".[140] Nansen donated the prize money to international relief efforts.[8]		From 1925 onwards he spent much time trying to help Armenian refugees, victims of Armenian Genocide at the hands of the Ottoman Empire during the First World War and further ill-treatment thereafter.[141] His goal was the establishment of a national home for these refugees, within the borders of Soviet Armenia. His main assistant in this endeavour was Vidkun Quisling, the future Nazi collaborator and head of a Norwegian puppet government during the Second World War.[142] After visiting the region, Nansen presented the Assembly with a modest plan for the irrigation of 36,000 hectares (360 km2 or 139 square miles) on which 15,000 refugees could be settled.[143] The plan ultimately failed, because even with Nansen's unremitting advocacy the money to finance the scheme was not forthcoming. Despite this failure, his reputation among the Armenian people remains high.[8] Nansen wrote the book, Armenia and the Near East in 1923 which describes his sympathies to the plight of the Armenians in the wake of losing its independence to the Soviet Union.[144] The book was translated in many languages including Norwegian, English, French, German, Russian and Armenian. After his visit to Armenia, Nansen wrote two additional books called "Gjennem Armenia" ("Across Armenia"), published in 1927 and "Gjennem Kaukasus til Volga" ("Through Caucasus to Volga").[145]		Within the League's Assembly, Nansen spoke out on many issues besides those related to refugees. He believed that the Assembly gave the smaller countries such as Norway a "unique opportunity for speaking in the councils of the world."[146] He believed that the extent of the League's success in reducing armaments would be the greatest test of its credibility.[147] He was a signatory to the Slavery Convention of 25 September 1926, which sought to outlaw the use of forced labour.[148] He supported a settlement of the post-war reparations issue, and championed Germany's membership of the League, which was granted in September 1926 after intensive preparatory work by Nansen.[142]		On 17 January 1919 Nansen married Sigrun Munthe, a long-time friend with whom he had had a love affair in 1905, while Eva was still alive. The marriage was resented by the Nansen children, and proved unhappy; an acquaintance writing of them in the 1920s said Nansen appeared unbearably miserable and Sigrun steeped in hate.[149]		Nansen's League of Nations commitments through the 1920s meant that he was mostly absent from Norway, and was able to devote little time to scientific work. Nevertheless, he continued to publish occasional papers.[150] He entertained the hope that he might travel to the North Pole by airship, but could not raise sufficient funding.[151] In any event he was forestalled in this ambition by Amundsen, who flew over the pole in Umberto Nobile's airship Norge in May 1926.[152] Two years later Nansen broadcast a memorial oration to Amundsen, who had disappeared in the Arctic while organising a rescue party for Nobile whose airship had crashed during a second polar voyage. Nansen said of Amundsen: "He found an unknown grave under the clear sky of the icy world, with the whirring of the wings of eternity through space."[153]		In 1926 Nansen was elected Rector of the University of St Andrews in Scotland, the first foreigner to hold this largely honorary position. He used the occasion of his inaugural address to review his life and philosophy, and to deliver a call to the youth of the next generation. He ended:		We all have a Land of Beyond to seek in our life—what more can we ask? Our part is to find the trail that leads to it. A long trail, a hard trail, maybe; but the call comes to us, and we have to go. Rooted deep in the nature of every one of us is the spirit of adventure, the call of the wild—vibrating under all our actions, making life deeper and higher and nobler.[154]		Nansen largely avoided involvement in domestic Norwegian politics, but in 1924 he was persuaded by the long-retired former Prime Minister Christian Michelsen to take part in a new anti-communist political grouping, the Fatherland League. There were fears in Norway that should the Marxist-oriented Labour Party gain power it would introduce a revolutionary programme. At the inaugural rally of the League in Oslo (as Christiania had now been renamed), Nansen declared: "To talk of the right of revolution in a society with full civil liberty, universal suffrage, equal treatment for everyone ... [is] idiotic nonsense." [155]		Following continued turmoil between the centre-right parties, there was even an independent petition in 1926 gaining some momentum that proposed for Nansen to head a centre-right national unity government on a balanced budget program, an idea he did not reject.[156] He was the headline speaker at the single largest Fatherland League rally with 15,000 attendees in Tønsberg in 1928.[157] In 1929 he went on his final tour for the League on the ship Stella Polaris, holding speeches from Bergen to Hammerfest.[158]		In between his various duties and responsibilities, Nansen had continued to take skiing holidays when he could. In February 1930, aged 68, he took a short break in the mountains with two old friends, who noted that Nansen was slower than usual and appeared to tire easily. On his return to Oslo he was laid up for several months, with influenza and later phlebitis, and was visited on his sickbed by King Haakon VII.[159][160]		Nansen was a close friend of a clergyman named Wilhelm. Nansen himself was an atheist.[161][162]		Nansen died of a heart attack, at home, on 13 May 1930. He was given a non-religious state funeral before cremation, after which his ashes were laid under a tree at Polhøgda. Nansen's daughter Liv recorded that there were no speeches, just music: Schubert's Death and the Maiden, which Eva used to sing.[163] Among the many tributes paid to him subsequently was that of Lord Robert Cecil, a fellow League of Nations delegate, who spoke of the range of Nansen's work, done with no regard for his own interests or health: "Every good cause had his support. He was a fearless peacemaker, a friend of justice, an advocate always for the weak and suffering."[164]		Nansen had been a pioneer and innovator in many fields. As a young man he embraced the revolution in skiing methods that transformed it from a means of winter travel to a universal sport, and quickly became one of Norway's leading skiers. He was later able to apply this expertise to the problems of polar travel, in both his Greenland and his Fram expeditions. He invented the "Nansen sledge" with broad, ski-like runners, the "Nansen cooker" to improve the heat efficiency of the standard spirit stoves then in use, and the layer principle in polar clothing, whereby the traditionally heavy, awkward garments were replaced by layers of lightweight material. In science, Nansen is recognised both as one of the founders of modern neurology,[165][166] and as a significant contributor to early oceanographical science, in particular for his work in establishing the Central Oceanographic Laboratory in Christiania.[167]		Through his work on behalf of the League of Nations, Nansen helped to establish the principle of international responsibility for refugees.[168] Immediately after his death the League set up the Nansen International Office for Refugees, a semi-autonomous body under the League's authority, to continue his work. The Nansen Office faced great difficulties, in part arising from the large numbers of refugees from the European dictatorships during the 1930s.[169] Nevertheless, it secured the agreement of 14 countries (including a reluctant Great Britain)[170] to the Refugee Convention of 1933. It also helped to repatriate 10,000 Armenians to Yerevan in Soviet Armenia, and to find homes for a further 40,000 in Syria and Lebanon. In 1938, the year in which it was superseded by a wider-ranging body, the Nansen Office was awarded the Nobel Peace Prize.[169]		In 1954 the League's successor body, the United Nations, established the Nansen Medal, later named the Nansen Refugee Award, given annually by the United Nations High Commissioner for Refugees to an individual, group or organisation "for outstanding work on behalf of the forcibly displaced".[171]		In his lifetime and thereafter, Nansen received honours and recognition from many countries.[172] Nansen Ski Club, the oldest continually operated ski club in the United States, located in Berlin, New Hampshire, is named in his honour. Numerous geographical features are named after him: the Nansen Basin and the Nansen-Gakkel Ridge in the Arctic Ocean;[173] Mount Nansen in the Yukon region of Canada;[174] Mount Nansen,[175] Mount Fridtjof Nansen[176] and Nansen Island,[177] all in Antarctica. Polhøgda is now home to the Fridtjof Nansen Institute, an independent foundation which engages in research on environmental, energy and resource management politics. [178] In 1968 a film of Nansen's life, Bare et liv – Historien om Fridtjof Nansen was released, directed by Sergei Mikaelyan, with Knut Wigert as Nansen.[179] In 2004 the Royal Norwegian Navy launched the first of a series of five Fridtjof Nansen-class frigates. The lead ship of the group is HNoMS Fridtjof Nansen; two others are named after Roald Amundsen and Otto Sverdrup.[180] In the ocean, Nansen is commemorated by Nansenia, small mesopelagic fishes of family Microstomatidae.[181] In space, he is commemorated by asteroid 853 Nansenia.[182] In 1964, the IAU adopted the name Nansen for an impact crater at the Lunar north pole, after the Norwegian explorer.[183]		
In ecology, sustainability (from sustain and ability) is the property of biological systems to remain diverse and productive indefinitely. Long-lived and healthy wetlands and forests are examples of sustainable biological systems. In more general terms, sustainability is the endurance of systems and processes. The organizing principle for sustainability is sustainable development, which includes the four interconnected domains: ecology, economics, politics and culture.[1] Sustainability science is the study of sustainable development and environmental science.[2]		Sustainability can also be defined as a socio-ecological process characterized by the pursuit of a common ideal.[3] An ideal is by definition unattainable in a given time and space. However, by persistently and dynamically approaching it, the process results in a sustainable system.[3]		Healthy ecosystems and environments are necessary to the survival of humans and other organisms. Ways of reducing negative human impact are environmentally-friendly chemical engineering, environmental resources management and environmental protection. Information is gained from green chemistry, earth science, environmental science and conservation biology. Ecological economics studies the fields of academic research that aim to address human economies and natural ecosystems.		Moving towards sustainability is also a social challenge that entails international and national law, urban planning and transport, local and individual lifestyles and ethical consumerism. Ways of living more sustainably can take many forms from reorganizing living conditions (e.g., ecovillages, eco-municipalities and sustainable cities), reappraising economic sectors (permaculture, green building, sustainable agriculture), or work practices (sustainable architecture), using science to develop new technologies (green technologies, renewable energy and sustainable fission and fusion power), or designing systems in a flexible and reversible manner,[4][5] and adjusting individual lifestyles that conserve natural resources.[6]		"The term 'sustainability' should be viewed as humanity's target goal of human-ecosystem equilibrium (homeostasis), while 'sustainable development' refers to the holistic approach and temporal processes that lead us to the end point of sustainability." (305)[7] Despite the increased popularity of the use of the term "sustainability", the possibility that human societies will achieve environmental sustainability has been, and continues to be, questioned—in light of environmental degradation, climate change, overconsumption, population growth and societies' pursuit of unlimited economic growth in a closed system.[8][9]		The name sustainability is derived from the Latin sustinere (tenere, to hold; sub, up). Sustain can mean “maintain", "support", or "endure”.[10][11] Since the 1980s sustainability has been used more in the sense of human sustainability on planet Earth and this has resulted in the most widely quoted definition of sustainability as a part of the concept sustainable development, that of the Brundtland Commission of the United Nations on March 20, 1987: “sustainable development is development that meets the needs of the present without compromising the ability of future generations to meet their own needs.”[12][13]		The 2005 World Summit on Social Development identified sustainable development goals, such as economic development, social development and environmental protection.[16] This view has been expressed as an illustration using three overlapping ellipses indicating that the three pillars of sustainability are not mutually exclusive and can be mutually reinforcing.[17] In fact, the three pillars are interdependent, and in the long run none can exist without the others.[18] The three pillars have served as a common ground for numerous sustainability standards and certification systems in recent years, in particular in the food industry.[19][20] Standards which today explicitly refer to the triple bottom line include Rainforest Alliance, Fairtrade and UTZ Certified.[21][22] Some sustainability experts and practitioners have illustrated four pillars of sustainability, or a quadruple bottom line. One such pillar is future generations, which emphasizes the long-term thinking associated with sustainability.[23] There is also an opinion that considers resource use and financial sustainability as two additional pillars of sustainability.[24]		Sustainable development consists of balancing local and global efforts to meet basic human needs without destroying or degrading the natural environment.[25][26] The question then becomes how to represent the relationship between those needs and the environment.		A study from 2005 pointed out that environmental justice is as important as sustainable development.[27] Ecological economist Herman Daly asked, "what use is a sawmill without a forest?"[28] From this perspective, the economy is a subsystem of human society, which is itself a subsystem of the biosphere, and a gain in one sector is a loss from another.[29] This perspective led to the nested circles figure of 'economics' inside 'society' inside the 'environment'.		The simple definition that sustainability is something that improves "the quality of human life while living within the carrying capacity of supporting eco-systems",[30] though vague, conveys the idea of sustainability having quantifiable limits. But sustainability is also a call to action, a task in progress or “journey” and therefore a political process, so some definitions set out common goals and values.[31] The Earth Charter[32] speaks of “a sustainable global society founded on respect for nature, universal human rights, economic justice, and a culture of peace.” This suggested a more complex figure of sustainability, which included the importance of the domain of 'politics'.		More than that, sustainability implies responsible and proactive decision-making and innovation that minimizes negative impact and maintains balance between ecological resilience, economic prosperity, political justice and cultural vibrancy to ensure a desirable planet for all species now and in the future.[33] Specific types of sustainability include, sustainable agriculture, sustainable architecture or ecological economics.[34] Understanding sustainable development is important but without clear targets an unfocused term like "liberty" or "justice".[35] It has also been described as a "dialogue of values that challenge the sociology of development".[36]		While the United Nations Millennium Declaration identified principles and treaties on sustainable development, including economic development, social development and environmental protection it continued using three domains: economics, environment and social sustainability. More recently, using a systematic domain model that responds to the debates over the last decade, the Circles of Sustainability approach distinguished four domains of economic, ecological, political and cultural sustainability. This in accord with the United Nations Agenda 21, which specifies culture as the fourth domain of sustainable development.[38] The model is now being used by organizations such as the United Nations Cities Programme.[39] and Metropolis[40]		Another model suggests humans attempt to achieve all of their needs and aspirations via seven modalities: economy, community, occupational groups, government, environment, culture, and physiology.[41] From the global to the individual human scale, each of the seven modalities can be viewed across seven hierarchical levels. Human sustainability can be achieved by attaining sustainability in all levels of the seven modalities.		Integral elements of sustainability are research and innovation activities. A telling example is the European environmental research and innovation policy. It aims at defining and implementing a transformative agenda to greening the economy and the society as a whole so to make them sustainable. Research and innovation in Europe are financially supported by the programme Horizon 2020, which is also open to participation worldwide.[42] Encouraging good farming practices ensures farmers fully benefit from the environment and at the same time conserving it for future generations.		Resiliency in ecology is the capacity of an ecosystem to absorb disturbance and still retain its basic structure and viability. Resilience-thinking evolved from the need to manage interactions between human-constructed systems and natural ecosystems in a sustainable way despite the fact that to policymakers a definition remains elusive. Resilience-thinking addresses how much planetary ecological systems can withstand assault from human disturbances and still deliver the services current and future generations need from them. It is also concerned with commitment from geopolitical policymakers to promote and manage essential planetary ecological resources in order to promote resilience and achieve sustainability of these essential resources for benefit of future generations of life?[43] The resiliency of an ecosystem, and thereby, its sustainability, can be reasonably measured at junctures or events where the combination of naturally occurring regenerative forces (solar energy, water, soil, atmosphere, vegetation, and biomass) interact with the energy released into the ecosystem from disturbances.[44]		A practical view of sustainability is closed systems that maintain processes of productivity indefinitely by replacing resources used by actions of people with resources of equal or greater value by those same people without degrading or endangering natural biotic systems.[45] In this way, sustainability can be concretely measured in human projects if there is a transparent accounting of the resources put back into the ecosystem to replace those displaced. In nature, the accounting occurs naturally through a process of adaptation as an ecosystem returns to viability from an external disturbance. The adaptation is a multi-stage process that begins with the disturbance event (earthquake, volcanic eruption, hurricane, tornado, flood, or thunderstorm), followed by absorption, utilization, or deflection of the energy or energies that the external forces created.[46]		In analysing systems such as urban and national parks, dams, farms and gardens, theme parks, open-pit mines, water catchments, one way to look at the relationship between sustainability and resiliency is to view the former with a long-term vision and resiliency as the capacity of human engineers to respond to immediate environmental events.[47]		The history of sustainability traces human-dominated ecological systems from the earliest civilizations to the present time.[48] This history is characterized by the increased regional success of a particular society, followed by crises that were either resolved, producing sustainability, or not, leading to decline.[49][50]		In early human history, the use of fire and desire for specific foods may have altered the natural composition of plant and animal communities.[51] Between 8,000 and 10,000 years ago, agrarian communities emerged which depended largely on their environment and the creation of a "structure of permanence."[52]		The Western industrial revolution of the 18th to 19th centuries tapped into the vast growth potential of the energy in fossil fuels. Coal was used to power ever more efficient engines and later to generate electricity. Modern sanitation systems and advances in medicine protected large populations from disease.[53] In the mid-20th century, a gathering environmental movement pointed out that there were environmental costs associated with the many material benefits that were now being enjoyed. In the late 20th century, environmental problems became global in scale.[54][55][56][57] The 1973 and 1979 energy crises demonstrated the extent to which the global community had become dependent on non-renewable energy resources.		In the 21st century, there is increasing global awareness of the threat posed by the human greenhouse effect, produced largely by forest clearing and the burning of fossil fuels.[58][59]		The philosophical and analytic framework of sustainability draws on and connects with many different disciplines and fields; in recent years an area that has come to be called sustainability science has emerged.[60]		Sustainability is studied and managed over many scales (levels or frames of reference) of time and space and in many contexts of environmental, social and economic organization. The focus ranges from the total carrying capacity (sustainability) of planet Earth to the sustainability of economic sectors, ecosystems, countries, municipalities, neighbourhoods, home gardens, individual lives, individual goods and services[clarification needed], occupations, lifestyles, behaviour patterns and so on. In short, it can entail the full compass of biological and human activity or any part of it.[61] As Daniel Botkin, author and environmentalist, has stated: "We see a landscape that is always in flux, changing over many scales of time and space."[62]		The sheer size and complexity of the planetary ecosystem has proved problematic for the design of practical measures to reach global sustainability. To shed light on the big picture, explorer and sustainability campaigner Jason Lewis has drawn parallels to other, more tangible closed systems. For example, he likens human existence on Earth — isolated as the planet is in space, whereby people cannot be evacuated to relieve population pressure and resources cannot be imported to prevent accelerated depletion of resources — to life at sea on a small boat isolated by water.[63] In both cases, he argues, exercising the precautionary principle is a key factor in survival.[64]		A major driver of human impact on Earth systems is the destruction of biophysical resources, and especially, the Earth's ecosystems. The environmental impact of a community or of humankind as a whole depends both on population and impact per person, which in turn depends in complex ways on what resources are being used, whether or not those resources are renewable, and the scale of the human activity relative to the carrying capacity of the ecosystems involved. Careful resource management can be applied at many scales, from economic sectors like agriculture, manufacturing and industry, to work organizations, the consumption patterns of households and individuals and to the resource demands of individual goods and services.[65][66]		One of the initial attempts to express human impact mathematically was developed in the 1970s and is called the I PAT formula. This formulation attempts to explain human consumption in terms of three components: population numbers, levels of consumption (which it terms "affluence", although the usage is different), and impact per unit of resource use (which is termed "technology", because this impact depends on the technology used). The equation is expressed:		In recent years, concepts based on (re-)cycling resources are increasingly gaining importance. The most prominent among these concepts might be the Circular Economy, with its comprehensive support by the Chinese and the European Union. There is also a broad range of similar concepts or schools of thought, including cradle-to-cradle laws of ecology, looped and performance economy, regenerative design, industrial ecology, biomimicry, and the blue economy. These concepts seem intuitively to be more sustainable than the current linear economic system. The reduction of resource inputs into and waste and emission leakage out of the system reduces resource depletion and environmental pollution. However, these simple assumptions are not sufficient to deal with the involved systemic complexity and disregards potential trade-offs. For example, the social dimension of sustainability seems to be only marginally addressed in many publications on the Circular Economy, and there are cases that require different or additional strategies, like purchasing new, more energy efficient equipment. A review of a team of researchers from Cambridge and TU Delft identified eight different relationship types between sustainability and the circular economy, namely a (1) conditional relation, a (2) strong conditional relation, a (3) necessary but not sufficient conditional relation, a (4) beneficial relationship a (structured and unstructured) (5) subset relation, a (6) degree relation, a cost-benefit/trade-off relation, and a (8) selective relation.[68]		Sustainability measurement is a term that denotes the measurements used as the quantitative basis for the informed management of sustainability.[69] The metrics used for the measurement of sustainability (involving the sustainability of environmental, social and economic domains, both individually and in various combinations) are evolving: they include indicators, benchmarks, audits, sustainability standards and certification systems like Fairtrade and Organic, indexes and accounting, as well as assessment, appraisal[70] and other reporting systems. They are applied over a wide range of spatial and temporal scales.[71][72]		Some of the best known and most widely used sustainability measures include corporate sustainability reporting, Triple Bottom Line accounting, World Sustainability Society, Circles of Sustainability, and estimates of the quality of sustainability governance for individual countries using the Environmental Sustainability Index and Environmental Performance Index.		According to the most recent (July 2015) revision of the official United Nations World Population Prospects, the world population is projected to reach 8.5 billion by 2030, up from the current 7.3 billion (July 2015), to exceed 9 billion people by 2050, and to reach 11.2 billion by the year 2100.[73] Most of the increase will be in developing countries whose population is projected to rise from 5.6 billion in 2009 to 7.9 billion in 2050. This increase will be distributed among the population aged 15–59 (1.2 billion) and 60 or over (1.1 billion) because the number of children under age 15 in developing countries is predicted to decrease. In contrast, the population of the more developed regions is expected to undergo only slight increase from 1.23 billion to 1.28 billion, and this would have declined to 1.15 billion but for a projected net migration from developing to developed countries, which is expected to average 2.4 million persons annually from 2009 to 2050.[74] Long-term estimates in 2004 of global population suggest a peak at around 2070 of nine to ten billion people, and then a slow decrease to 8.4 billion by 2100.[75]		Emerging economies like those of China and India aspire to the living standards of the Western world as does the non-industrialized world in general.[76] It is the combination of population increase in the developing world and unsustainable consumption levels in the developed world that poses a stark challenge to sustainability.[77]		At the global scale, scientific data now indicates that humans are living beyond the carrying capacity of planet Earth and that this cannot continue indefinitely. This scientific evidence comes from many sources but is presented in detail in the Millennium Ecosystem Assessment and the planetary boundaries framework.[78] An early detailed examination of global limits was published in the 1972 book Limits to Growth, which has prompted follow-up commentary and analysis.[79] A 2012 review in Nature by 22 international researchers expressed concerns that the Earth may be "approaching a state shift" in its biosphere.[80]		The Ecological footprint measures human consumption in terms of the biologically productive land needed to provide the resources, and absorb the wastes of the average global citizen. In 2008 it required 2.7 global hectares per person, 30% more than the natural biological capacity of 2.1 global hectares (assuming no provision for other organisms).[55] The resulting ecological deficit must be met from unsustainable extra sources and these are obtained in three ways: embedded in the goods and services of world trade; taken from the past (e.g. fossil fuels); or borrowed from the future as unsustainable resource usage (e.g. by over exploiting forests and fisheries).		The figure (right) examines sustainability at the scale of individual countries by contrasting their Ecological Footprint with their UN Human Development Index (a measure of standard of living). The graph shows what is necessary for countries to maintain an acceptable standard of living for their citizens while, at the same time, maintaining sustainable resource use. The general trend is for higher standards of living to become less sustainable. As always, population growth has a marked influence on levels of consumption and the efficiency of resource use.[67][81] The sustainability goal is to raise the global standard of living without increasing the use of resources beyond globally sustainable levels; that is, to not exceed "one planet" consumption. Information generated by reports at the national, regional and city scales confirm the global trend towards societies that are becoming less sustainable over time.[82][83]		Romanian American economist Nicholas Georgescu-Roegen, a progenitor in economics and a paradigm founder of ecological economics, has argued that the carrying capacity of Earth — that is, Earth's capacity to sustain human populations and consumption levels — is bound to decrease sometime in the future as Earth's finite stock of mineral resources is presently being extracted and put to use.[84]:303 Leading ecological economist and steady-state theorist Herman Daly, a student of Georgescu-Roegen, has propounded the same argument.[85]:369–371		At a fundamental level, energy flow and biogeochemical cycling set an upper limit on the number and mass of organisms in any ecosystem.[86] Human impacts on the Earth are demonstrated in a general way through detrimental changes in the global biogeochemical cycles of chemicals that are critical to life, most notably those of water, oxygen, carbon, nitrogen and phosphorus.[87]		The Millennium Ecosystem Assessment is an international synthesis by over 1000 of the world's leading biological scientists that analyzes the state of the Earth’s ecosystems and provides summaries and guidelines for decision-makers. It concludes that human activity is having a significant and escalating impact on the biodiversity of world ecosystems, reducing both their resilience and biocapacity. The report refers to natural systems as humanity's "life-support system", providing essential "ecosystem services". The assessment measures 24 ecosystem services concluding that only four have shown improvement over the last 50 years, 15 are in serious decline, and five are in a precarious condition.[88]		The Sustainable Development Goals (SDGs) are the current harmonized set of seventeen future international development targets.		The Official Agenda for Sustainable Development adopted on 25 September 2015 has 92 paragraphs, with the main paragraph (51) outlining the 17 Sustainable Development Goals and its associated 169 targets. This included the following seventeen goals:[89]		As of August 2015, there were 169 proposed targets for these goals and 304 proposed indicators to show compliance.[107]		The Sustainable Development Goals (SDGs) replace the eight Millennium Development Goals (MDGs), which expired at the end of 2015. The MDGs were established in 2000 following the Millennium Summit of the United Nations. Adopted by the 189 United Nations member states at the time and more than twenty international organizations, these goals were advanced to help achieve the following sustainable development standards by 2015.		According to the data that member countries represented to the United Nations, Cuba was the only nation in the world in 2006 that met the World Wide Fund for Nature's definition of sustainable development, with an ecological footprint of less than 1.8 hectares per capita, 1.5, and a Human Development Index of over 0.8, 0.855.[108][109]		Healthy ecosystems provide vital goods and services to humans and other organisms. There are two major ways of reducing negative human impact and enhancing ecosystem services and the first of these is environmental management. This direct approach is based largely on information gained from earth science, environmental science and conservation biology. However, this is management at the end of a long series of indirect causal factors that are initiated by human consumption, so a second approach is through demand management of human resource use.		Management of human consumption of resources is an indirect approach based largely on information gained from economics. Herman Daly has suggested three broad criteria for ecological sustainability: renewable resources should provide a sustainable yield (the rate of harvest should not exceed the rate of regeneration); for non-renewable resources there should be equivalent development of renewable substitutes; waste generation should not exceed the assimilative capacity of the environment.[110]		At the global scale and in the broadest sense environmental management involves the oceans, freshwater systems, land and atmosphere, but following the sustainability principle of scale it can be equally applied to any ecosystem from a tropical rainforest to a home garden.[111][112]		At a March 2009 meeting of the Copenhagen Climate Council, 2,500 climate experts from 80 countries issued a keynote statement that there is now "no excuse" for failing to act on global warming and that without strong carbon reduction "abrupt or irreversible" shifts in climate may occur that "will be very difficult for contemporary societies to cope with".[113][114] Management of the global atmosphere now involves assessment of all aspects of the carbon cycle to identify opportunities to address human-induced climate change and this has become a major focus of scientific research because of the potential catastrophic effects on biodiversity and human communities (see Energy below).		Other human impacts on the atmosphere include the air pollution in cities, the pollutants including toxic chemicals like nitrogen oxides, sulfur oxides, volatile organic compounds and airborne particulate matter that produce photochemical smog and acid rain, and the chlorofluorocarbons that degrade the ozone layer. Anthropogenic particulates such as sulfate aerosols in the atmosphere reduce the direct irradiance and reflectance (albedo) of the Earth's surface. Known as global dimming, the decrease is estimated to have been about 4% between 1960 and 1990 although the trend has subsequently reversed. Global dimming may have disturbed the global water cycle by reducing evaporation and rainfall in some areas. It also creates a cooling effect and this may have partially masked the effect of greenhouse gases on global warming.[115]		Water covers 71% of the Earth's surface. Of this, 97.5% is the salty water of the oceans and only 2.5% freshwater, most of which is locked up in the Antarctic ice sheet. The remaining freshwater is found in glaciers, lakes, rivers, wetlands, the soil, aquifers and atmosphere. Due to the water cycle, fresh water supply is continually replenished by precipitation, however there is still a limited amount necessitating management of this resource. Awareness of the global importance of preserving water for ecosystem services has only recently emerged as, during the 20th century, more than half the world’s wetlands have been lost along with their valuable environmental services. Increasing urbanization pollutes clean water supplies and much of the world still does not have access to clean, safe water.[116] Greater emphasis is now being placed on the improved management of blue (harvestable) and green (soil water available for plant use) water, and this applies at all scales of water management.[117]		Ocean circulation patterns have a strong influence on climate and weather and, in turn, the food supply of both humans and other organisms. Scientists have warned of the possibility, under the influence of climate change, of a sudden alteration in circulation patterns of ocean currents that could drastically alter the climate in some regions of the globe.[118] Ten per cent of the world's population – about 600 million people – live in low-lying areas vulnerable to sea level rise.		Loss of biodiversity stems largely from the habitat loss and fragmentation produced by the human appropriation of land for development, forestry and agriculture as natural capital is progressively converted to man-made capital. Land use change is fundamental to the operations of the biosphere because alterations in the relative proportions of land dedicated to urbanisation, agriculture, forest, woodland, grassland and pasture have a marked effect on the global water, carbon and nitrogen biogeochemical cycles and this can impact negatively on both natural and human systems.[119] At the local human scale, major sustainability benefits accrue from sustainable parks and gardens and green cities.[120][121]		Since the Neolithic Revolution about 47% of the world’s forests have been lost to human use. Present-day forests occupy about a quarter of the world’s ice-free land with about half of these occurring in the tropics.[122] In temperate and boreal regions forest area is gradually increasing (with the exception of Siberia), but deforestation in the tropics is of major concern.[123]		Food is essential to life. Feeding more than seven billion human bodies takes a heavy toll on the Earth’s resources. This begins with the appropriation of about 38% of the Earth’s land surface[124] and about 20% of its net primary productivity.[125] Added to this are the resource-hungry activities of industrial agribusiness – everything from the crop need for irrigation water, synthetic fertilizers and pesticides to the resource costs of food packaging, transport (now a major part of global trade) and retail. Environmental problems associated with industrial agriculture and agribusiness are now being addressed through such movements as sustainable agriculture, organic farming and more sustainable business practices.[126]		The underlying driver of direct human impacts on the environment is human consumption.[127] This impact is reduced by not only consuming less but by also making the full cycle of production, use and disposal more sustainable. Consumption of goods and services can be analysed and managed at all scales through the chain of consumption, starting with the effects of individual lifestyle choices and spending patterns, through to the resource demands of specific goods and services, the impacts of economic sectors, through national economies to the global economy.[128] Analysis of consumption patterns relates resource use to the environmental, social and economic impacts at the scale or context under investigation. The ideas of embodied resource use (the total resources needed to produce a product or service), resource intensity, and resource productivity are important tools for understanding the impacts of consumption. Key resource categories relating to human needs are food, energy, materials and water.		In 2010, the International Resource Panel, hosted by the United Nations Environment Programme (UNEP), published the first global scientific assessment on the impacts of consumption and production[129] and identified priority actions for developed and developing countries. The study found that the most critical impacts are related to ecosystem health, human health and resource depletion. From a production perspective, it found that fossil-fuel combustion processes, agriculture and fisheries have the most important impacts. Meanwhile, from a final consumption perspective, it found that household consumption related to mobility, shelter, food and energy-using products cause the majority of life-cycle impacts of consumption.		The Sun's energy, stored by plants (primary producers) during photosynthesis, passes through the food chain to other organisms to ultimately power all living processes. Since the industrial revolution the concentrated energy of the Sun stored in fossilized plants as fossil fuels has been a major driver of technology which, in turn, has been the source of both economic and political power. In 2007 climate scientists of the IPCC concluded that there was at least a 90% probability that atmospheric increase in CO2 was human-induced, mostly as a result of fossil fuel emissions but, to a lesser extent from changes in land use. Stabilizing the world’s climate will require high-income countries to reduce their emissions by 60–90% over 2006 levels by 2050 which should hold CO2 levels at 450–650 ppm from current levels of about 380 ppm. Above this level, temperatures could rise by more than 2 °C to produce “catastrophic” climate change.[130][131] Reduction of current CO2 levels must be achieved against a background of global population increase and developing countries aspiring to energy-intensive high consumption Western lifestyles.[132]		Reducing greenhouse emissions, is being tackled at all scales, ranging from tracking the passage of carbon through the carbon cycle[133] to the commercialization of renewable energy, developing less carbon-hungry technology and transport systems and attempts by individuals to lead carbon neutral lifestyles by monitoring the fossil fuel use embodied in all the goods and services they use.[134] Engineering of emerging technologies such as carbon-neutral fuel[135][136][137] and energy storage systems such as power to gas, compressed air energy storage,[138][139] and pumped-storage hydroelectricity[140][141][142] are necessary to store power from transient renewable energy sources including emerging renewables such as airborne wind turbines.[143]		Water security and food security are inextricably linked. In the decade 1951–60 human water withdrawals were four times greater than the previous decade. This rapid increase resulted from scientific and technological developments impacting through the economy – especially the increase in irrigated land, growth in industrial and power sectors, and intensive dam construction on all continents. This altered the water cycle of rivers and lakes, affected their water quality and had a significant impact on the global water cycle.[144] Currently towards 35% of human water use is unsustainable, drawing on diminishing aquifers and reducing the flows of major rivers: this percentage is likely to increase if climate change impacts become more severe, populations increase, aquifers become progressively depleted and supplies become polluted and unsanitary.[145] From 1961 to 2001 water demand doubled — agricultural use increased by 75%, industrial use by more than 200%, and domestic use more than 400%.[146] In the 1990s it was estimated that humans were using 40–50% of the globally available freshwater in the approximate proportion of 70% for agriculture, 22% for industry, and 8% for domestic purposes with total use progressively increasing.[144]		Water efficiency is being improved on a global scale by increased demand management, improved infrastructure, improved water productivity of agriculture, minimising the water intensity (embodied water) of goods and services, addressing shortages in the non-industrialized world, concentrating food production in areas of high productivity, and planning for climate change, such as through flexible system design. A promising direction towards sustainable development is to design systems that are flexible and reversible.[4][5] At the local level, people are becoming more self-sufficient by harvesting rainwater and reducing use of mains water.[117][147]		The American Public Health Association (APHA) defines a "sustainable food system"[148][149] as "one that provides healthy food to meet current food needs while maintaining healthy ecosystems that can also provide food for generations to come with minimal negative impact to the environment. A sustainable food system also encourages local production and distribution infrastructures and makes nutritious food available, accessible, and affordable to all. Further, it is humane and just, protecting farmers and other workers, consumers, and communities."[150] Concerns about the environmental impacts of agribusiness and the stark contrast between the obesity problems of the Western world and the poverty and food insecurity of the developing world have generated a strong movement towards healthy, sustainable eating as a major component of overall ethical consumerism.[151] The environmental effects of different dietary patterns depend on many factors, including the proportion of animal and plant foods consumed and the method of food production.[152][153][154][155] The World Health Organization has published a Global Strategy on Diet, Physical Activity and Health report which was endorsed by the May 2004 World Health Assembly. It recommends the Mediterranean diet which is associated with health and longevity and is low in meat, rich in fruits and vegetables, low in added sugar and limited salt, and low in saturated fatty acids; the traditional source of fat in the Mediterranean is olive oil, rich in monounsaturated fat. The healthy rice-based Japanese diet is also high in carbohydrates and low in fat. Both diets are low in meat and saturated fats and high in legumes and other vegetables; they are associated with a low incidence of ailments and low environmental impact.[156]		At the global level the environmental impact of agribusiness is being addressed through sustainable agriculture and organic farming. At the local level there are various movements working towards local food production, more productive use of urban wastelands and domestic gardens including permaculture, urban horticulture, local food, slow food, sustainable gardening, and organic gardening.[157][158]		Sustainable seafood is seafood from either fished or farmed sources that can maintain or increase production in the future without jeopardizing the ecosystems from which it was acquired. The sustainable seafood movement has gained momentum as more people become aware about both overfishing and environmentally destructive fishing methods.		As global population and affluence has increased, so has the use of various materials increased in volume, diversity and distance transported. Included here are raw materials, minerals, synthetic chemicals (including hazardous substances), manufactured products, food, living organisms and waste.[159] By 2050, humanity could consume an estimated 140 billion tons of minerals, ores, fossil fuels and biomass per year (three times its current amount) unless the economic growth rate is decoupled from the rate of natural resource consumption. Developed countries' citizens consume an average of 16 tons of those four key resources per capita, ranging up to 40 or more tons per person in some developed countries with resource consumption levels far beyond what is likely sustainable.[160]		Sustainable use of materials has targeted the idea of dematerialization, converting the linear path of materials (extraction, use, disposal in landfill) to a circular material flow that reuses materials as much as possible, much like the cycling and reuse of waste in nature.[161] This approach is supported by product stewardship and the increasing use of material flow analysis at all levels, especially individual countries and the global economy.[162] The use of sustainable biomaterials that come from renewable sources and that can be recycled is preferred to the use on non-renewables from a life cycle standpoint.		Synthetic chemical production has escalated following the stimulus it received during the second World War. Chemical production includes everything from herbicides, pesticides and fertilizers to domestic chemicals and hazardous substances.[163] Apart from the build-up of greenhouse gas emissions in the atmosphere, chemicals of particular concern include: heavy metals, nuclear waste, chlorofluorocarbons, persistent organic pollutants and all harmful chemicals capable of bioaccumulation. Although most synthetic chemicals are harmless there needs to be rigorous testing of new chemicals, in all countries, for adverse environmental and health effects. International legislation has been established to deal with the global distribution and management of dangerous goods.[164][165] The effects of some chemical agents needed long-term measurements and a lot of legal battles to realize their danger to human health. The classification of the toxic carcinogenic agents is handle by the International Agency for Research on Cancer.		Every economic activity produces material that can be classified as waste. To reduce waste, industry, business and government are now mimicking nature by turning the waste produced by industrial metabolism into resource. Dematerialization is being encouraged through the ideas of industrial ecology, ecodesign[166] and ecolabelling. In addition to the well-established “reduce, reuse and recycle,” shoppers are using their purchasing power for ethical consumerism.[66]		The European Union is expected to table by the end of 2015 an ambitious Circular Economy package which is expected to include concrete legislative proposals on waste management, ecodesign and limits on land fills.		On one account, sustainability "concerns the specification of a set of actions to be taken by present persons that will not diminish the prospects of future persons to enjoy levels of consumption, wealth, utility, or welfare comparable to those enjoyed by present persons."[167] Sustainability interfaces with economics through the social and ecological consequences of economic activity.[28] Sustainability economics represents: "... a broad interpretation of ecological economics where environmental and ecological variables and issues are basic but part of a multidimensional perspective. Social, cultural, health-related and monetary/financial aspects have to be integrated into the analysis."[168] However, the concept of sustainability is much broader than the concepts of sustained yield of welfare, resources, or profit margins.[169] At present, the average per capita consumption of people in the developing world is sustainable but population numbers are increasing and individuals are aspiring to high-consumption Western lifestyles. The developed world population is only increasing slightly but consumption levels are unsustainable. The challenge for sustainability is to curb and manage Western consumption while raising the standard of living of the developing world without increasing its resource use and environmental impact. This must be done by using strategies and technology that break the link between, on the one hand, economic growth and on the other, environmental damage and resource depletion.[170]		A recent UNEP report proposes a green economy defined as one that “improves human well-being and social equity, while significantly reducing environmental risks and ecological scarcities”: it "does not favor one political perspective over another but works to minimize excessive depletion of natural capital". The report makes three key findings: “that greening not only generates increases in wealth, in particular a gain in ecological commons or natural capital, but also (over a period of six years) produces a higher rate of GDP growth”; that there is “an inextricable link between poverty eradication and better maintenance and conservation of the ecological commons, arising from the benefit flows from natural capital that are received directly by the poor”; "in the transition to a green economy, new jobs are created, which in time exceed the losses in “brown economy” jobs. However, there is a period of job losses in transition, which requires investment in re-skilling and re-educating the workforce”.[171]		Several key areas have been targeted for economic analysis and reform: the environmental effects of unconstrained economic growth; the consequences of nature being treated as an economic externality; and the possibility of an economics that takes greater account of the social and environmental consequences of market behavior.[172]		Historically there has been a close correlation between economic growth and environmental degradation: as communities grow, so the environment declines. This trend is clearly demonstrated on graphs of human population numbers, economic growth, and environmental indicators.[173] Unsustainable economic growth has been starkly compared to the malignant growth of a cancer[174] because it eats away at the Earth's ecosystem services which are its life-support system. There is concern that, unless resource use is checked, modern global civilization will follow the path of ancient civilizations that collapsed through overexploitation of their resource base.[175][176] While conventional economics is concerned largely with economic growth and the efficient allocation of resources, ecological economics has the explicit goal of sustainable scale (rather than continual growth), fair distribution and efficient allocation, in that order.[177][178] The World Business Council for Sustainable Development states that "business cannot succeed in societies that fail".[179]		In economic and environmental fields, the term decoupling is becoming increasingly used in the context of economic production and environmental quality. When used in this way, it refers to the ability of an economy to grow without incurring corresponding increases in environmental pressure. Ecological economics includes the study of societal metabolism, the throughput of resources that enter and exit the economic system in relation to environmental quality.[178][180] An economy that is able to sustain GDP growth without having a negative impact on the environment is said to be decoupled. Exactly how, if, or to what extent this can be achieved is a subject of much debate. In 2011 the International Resource Panel, hosted by the United Nations Environment Programme (UNEP), warned that by 2050 the human race could be devouring 140 billion tons of minerals, ores, fossil fuels and biomass per year – three times its current rate of consumption – unless nations can make serious attempts at decoupling.[181] The report noted that citizens of developed countries consume an average of 16 tons of those four key resources per capita per annum (ranging up to 40 or more tons per person in some developed countries). By comparison, the average person in India today consumes four tons per year. Sustainability studies analyse ways to reduce resource intensity (the amount of resource (e.g. water, energy, or materials) needed for the production, consumption and disposal of a unit of good or service) whether this be achieved from improved economic management, product design, or new technology.[182]		There are conflicting views whether improvements in technological efficiency and innovation will enable a complete decoupling of economic growth from environmental degradation. On the one hand, it has been claimed repeatedly by efficiency experts that resource use intensity (i.e., energy and materials use per unit GDP) could in principle be reduced by at least four or five-fold, thereby allowing for continued economic growth without increasing resource depletion and associated pollution.[183][184] On the other hand, an extensive historical analysis of technological efficiency improvements has conclusively shown that improvements in the efficiency of the use of energy and materials were almost always outpaced by economic growth, in large part because of the rebound effect (conservation) or Jevons Paradox resulting in a net increase in resource use and associated pollution.[185][186] Furthermore, there are inherent thermodynamic (i.e., second law of thermodynamics) and practical limits to all efficiency improvements. For example, there are certain minimum unavoidable material requirements for growing food, and there are limits to making automobiles, houses, furniture, and other products lighter and thinner without the risk of losing their necessary functions.[187] Since it is both theoretically and practically impossible to increase resource use efficiencies indefinitely, it is equally impossible to have continued and infinite economic growth without a concomitant increase in resource depletion and environmental pollution, i.e., economic growth and resource depletion can be decoupled to some degree over the short run but not the long run. Consequently, long-term sustainability requires the transition to a steady state economy in which total GDP remains more or less constant, as has been advocated for decades by Herman Daly and others in the ecological economics community.		A different proposed solution to partially decouple economic growth from environmental degradation is the restore approach.[188] This approach views "restore" as a fourth component to the common reduce, reuse, recycle motto. Participants in such efforts are encouraged to voluntarily donate towards nature conservation a small fraction of the financial savings they experience through a more frugal use of resources. These financial savings would normally lead to rebound effects, but a theoretical analysis suggests that donating even a small fraction of the experienced savings can potentially more than eliminate rebound effects.[188]		The economic importance of nature is indicated by the use of the expression ecosystem services to highlight the market relevance of an increasingly scarce natural world that can no longer be regarded as both unlimited and free.[189] In general, as a commodity or service becomes more scarce the price increases and this acts as a restraint that encourages frugality, technical innovation and alternative products. However, this only applies when the product or service falls within the market system.[190] As ecosystem services are generally treated as economic externalities they are unpriced and therefore overused and degraded, a situation sometimes referred to as the Tragedy of the Commons.[189]		One approach to this dilemma has been the attempt to "internalize" these "externalities" by using market strategies like ecotaxes and incentives, tradeable permits for carbon, and the encouragement of payment for ecosystem services. Community currencies associated with Local Exchange Trading Systems (LETS), a gift economy and Time Banking have also been promoted as a way of supporting local economies and the environment.[191][192] Green economics is another market-based attempt to address issues of equity and the environment.[193] The global recession and a range of associated government policies are likely to bring the biggest annual fall in the world's carbon dioxide emissions in 40 years.[194]		Treating the environment as an externality may generate short-term profit at the expense of sustainability.[195] Sustainable business practices, on the other hand, integrate ecological concerns with social and economic ones (i.e., the triple bottom line).[196][197] Growth that depletes ecosystem services is sometimes termed "uneconomic growth" as it leads to a decline in quality of life.[198][199] Minimizing such growth can provide opportunities for local businesses. For example, industrial waste can be treated as an "economic resource in the wrong place". The benefits of waste reduction include savings from disposal costs, fewer environmental penalties, and reduced liability insurance. This may lead to increased market share due to an improved public image.[200][201] Energy efficiency can also increase profits by reducing costs.		The idea of sustainability as a business opportunity has led to the formation of organizations such as the Sustainability Consortium of the Society for Organizational Learning, the Sustainable Business Institute, and the World Council for Sustainable Development.[202] The expansion of sustainable business opportunities can contribute to job creation through the introduction of green-collar workers.[203] Research focusing on progressive corporate leaders who have integrated sustainability into commercial strategy has yielded a leadership competency model for sustainability,[204][205] and led to emergence of the concept of "embedded sustainability" – defined by its authors Chris Laszlo and Nadya Zhexembayeva as "incorporation of environmental, health, and social value into the core business with no trade-off in price or quality – in other words, with no social or green premium."[206] Laszlo and Zhexembayeva's research showed that embedded sustainability offers at least seven distinct opportunities for business value creation: a) better risk-management, b) increased efficiency through reduced waste and resource use, c) better product differentiation, d) new market entrances, e) enhanced brand and reputation, f) greater opportunity to influence industry standards, and g) greater opportunity for radical innovation.[207] Nadya Zhexembayeva's 2014 research further suggested that innovation driven by resource depletion can result in fundamental advantages for company products and services, as well as the company strategy as a whole, when right principles of innovation are applied.[208]		One school of thought, often labeled ecosocialism or ecological Marxism, asserts that the capitalist economic system is fundamentally incompatible with the ecological and social requirements of sustainability.[209] This theory rests on the premises that:		Thus, according to this analysis:		By this logic, market-based solutions to ecological crises (ecological economics, environmental economics, green economy) are rejected as technical tweaks that do not confront capitalism’s structural failures.[218][219] “Low-risk” technology/science-based solutions such as solar power, sustainable agriculture, and increases in energy efficiency are seen as necessary but insufficient.[220] “High-risk” technological solutions such as nuclear power and climate engineering are entirely rejected.[221] Attempts made by businesses to “greenwash” their practices are regarded as false advertising, and it is pointed out that implementation of renewable technology (such as Walmart’s proposition to supply their electricity with solar power) has the effect opposite of reductions in resource consumption, viz. further economic growth.[222] Sustainable business models and the triple bottom line are viewed as morally praiseworthy but ignorant to the tendency in capitalism for the distribution of wealth to become increasingly unequal and socially unstable/unsustainable.[213][223] Ecosocialists claim that the general unwillingness of capitalists to tolerate—and capitalist governments to implement—constraints on maximum profit (such as ecotaxes or preservation and conservation measures) renders environmental reforms incapable of facilitating large-scale change: “History teaches us that although capitalism has at times responded to environmental movements . . . at a certain point, at which the system’s underlying accumulation drive is affected, its resistance to environmental demands stiffens.”[224] They also note that, up until the event of total ecological collapse, destruction caused by natural disasters generally causes an increase in economic growth and accumulation; thus, capitalists have no foreseeable motivation to reduce the probability of disasters (i.e. convert to sustainable/ecological production).[225]		Ecosocialists advocate for the revolutionary succession of capitalism by ecosocialism—an egalitarian economic/political/social structure designed to harmonize human society with non-human ecology and to fulfill human needs—as the only sufficient solution to the present-day ecological crisis, and hence the only path towards sustainability.[226] Sustainability is viewed not as a domain exclusive to scientists, environmental activists, and business leaders but as a holistic project that must involve the whole of humanity redefining its place in Nature: “What every environmentalist needs to know . . . is that capitalism is not the solution but the problem, and that if humanity is going to survive this crisis, it will do so because it has exercised its capacity for human freedom, through social struggle, in order to create a whole new world—in coevolution with the planet.”[227]		Sustainability issues are generally expressed in scientific and environmental terms, as well as in ethical terms of stewardship, but implementing change is a social challenge that entails, among other things, international and national law, urban planning and transport, local and individual lifestyles and ethical consumerism.[228] "The relationship between human rights and human development, corporate power and environmental justice, global poverty and citizen action, suggest that responsible global citizenship is an inescapable element of what may at first glance seem to be simply matters of personal consumer and moral choice."[229]		Social disruptions like war, crime and corruption divert resources from areas of greatest human need, damage the capacity of societies to plan for the future, and generally threaten human well-being and the environment.[229] Broad-based strategies for more sustainable social systems include: improved education and the political empowerment of women, especially in developing countries; greater regard for social justice, notably equity between rich and poor both within and between countries; and intergenerational equity.[77] Depletion of natural resources including fresh water[230] increases the likelihood of “resource wars”.[231] This aspect of sustainability has been referred to as environmental security and creates a clear need for global environmental agreements to manage resources such as aquifers and rivers which span political boundaries, and to protect shared global systems including oceans and the atmosphere.[232]		A major hurdle to achieve sustainability is the alleviation of poverty. It has been widely acknowledged that poverty is one source of environmental degradation. Such acknowledgment has been made by the Brundtland Commission report Our Common Future[233] and the Millennium Development Goals.[234] There is a growing realization in national governments and multilateral institutions that it is impossible to separate economic development issues from environment issues: according to the Brundtland report, “poverty is a major cause and effect of global environmental problems. It is therefore futile to attempt to deal with environmental problems without a broader perspective that encompasses the factors underlying world poverty and international inequality.”[235] Individuals living in poverty tend to rely heavily on their local ecosystem as a source for basic needs (such as nutrition and medicine) and general well-being.[236] As population growth continues to increase, increasing pressure is being placed on the local ecosystem to provide these basic essentials. According to the UN Population Fund, high fertility and poverty have been strongly correlated, and the world’s poorest countries also have the highest fertility and population growth rates.[237] The word sustainability is also used widely by western country development agencies and international charities to focus their poverty alleviation efforts in ways that can be sustained by the local populace and its environment. For example, teaching water treatment to the poor by boiling their water with charcoal, would not generally be considered a sustainable strategy, whereas using PET solar water disinfection would be. Also, sustainable best practices can involve the recycling of materials, such as the use of recycled plastics for lumber where deforestation has devastated a country's timber base. Another example of sustainable practices in poverty alleviation is the use of exported recycled materials from developed to developing countries, such as Bridges to Prosperity's use of wire rope from shipping container gantry cranes to act as the structural wire rope for footbridges that cross rivers in poor rural areas in Asia and Africa.		According to Murray Bookchin, the idea that humans must dominate nature is common in hierarchical societies. Bookchin contends that capitalism and market relationships, if unchecked, have the capacity to reduce the planet to a mere resource to be exploited. Nature is thus treated as a commodity: “The plundering of the human spirit by the market place is paralleled by the plundering of the earth by capital.”[238] Social ecology, founded by Bookchin, is based on the conviction that nearly all of humanity's present ecological problems originate in, indeed are mere symptoms of, dysfunctional social arrangements. Whereas most authors proceed as if our ecological problems can be fixed by implementing recommendations which stem from physical, biological, economic etc., studies, Bookchin's claim is that these problems can only be resolved by understanding the underlying social processes and intervening in those processes by applying the concepts and methods of the social sciences.[239]		A pure capitalist approach has also been criticized in Stern Review on the Economics of Climate Change to mitigation the effects of global warming in this excerpt ...		“the greatest example of market failure we have ever seen.”[240][241]		Deep ecology is a movement founded by Arne Naess that establishes principles for the well-being of all life on Earth and the richness and diversity of life forms. The movement advocates, among other things, a substantial decrease in human population and consumption along with the reduction of human interference with the nonhuman world. To achieve this, deep ecologists advocate policies for basic economic, technological, and ideological structures that will improve the quality of life rather than the standard of living. Those who subscribe to these principles are obliged to make the necessary change happen.[242] The concept of a billion-year Sustainocene has been developed to initiate policy consideration of an earth where human structures power and fuel the needs of that species (for example through artificial photosynthesis) allowing Rights of Nature.[243]		1. Reduce dependence upon fossil fuels, underground metals, and minerals 2. Reduce dependence upon synthetic chemicals and other unnatural substances 3. Reduce encroachment upon nature		One approach to sustainable living, exemplified by small-scale urban transition towns and rural ecovillages, seeks to create self-reliant communities based on principles of simple living, which maximize self-sufficiency particularly in food production. These principles, on a broader scale, underpin the concept of a bioregional economy.[245] These approaches often utilize commons based knowledge sharing of open source appropriate technology.[246]		Other approaches, loosely based around New Urbanism, are successfully reducing environmental impacts by altering the built environment to create and preserve sustainable cities which support sustainable transport. Residents in compact urban neighborhoods drive fewer miles, and have significantly lower environmental impacts across a range of measures, compared with those living in sprawling suburbs.[247] In sustainable architecture the recent movement of New Classical Architecture promotes a sustainable approach towards construction, that appreciates and develops smart growth, architectural tradition and classical design.[248][249] This in contrast to modernist and globally uniform architecture, as well as opposing solitary housing estates and suburban sprawl.[250] Both trends started in the 1980s. The concept of Circular flow land use management has also been introduced in Europe to promote sustainable land use patterns that strive for compact cities and a reduction of greenfield land take by urban sprawl.		Large scale social movements can influence both community choices and the built environment. Eco-municipalities may be one such movement.[251] Eco-municipalities take a systems approach, based on sustainability principles. The eco-municipality movement is participatory, involving community members in a bottom-up approach. In Sweden, more than 70 cities and towns—25 per cent of all municipalities in the country—have adopted a common set of "Sustainability Principles" and implemented these systematically throughout their municipal operations. There are now twelve eco-municipalities in the United States and the American Planning Association has adopted sustainability objectives based on the same principles.[244]		There is a wealth of advice available to individuals wishing to reduce their personal and social impact on the environment through small, inexpensive and easily achievable steps.[252][253] But the transition required to reduce global human consumption to within sustainable limits involves much larger changes, at all levels and contexts of society.[254] The United Nations has recognised the central role of education, and have declared a decade of education for sustainable development, 2005–2014, which aims to "challenge us all to adopt new behaviours and practices to secure our future".[255] The Worldwide Fund for Nature proposes a strategy for sustainability that goes beyond education to tackle underlying individualistic and materialistic societal values head-on and strengthen people's connections with the natural world.[256]		Application of social sustainability requires stakeholders to look at human and labor rights, prevention of human trafficking, and other human rights risks.[257] These issues should be considered in production and procurement of various worldwide commodities. The international community has identified many industries whose practices have been known to violate social sustainability, and many of these industries have organizations in place that aid in verifying the social sustainability of products and services.[258] The Equator Principles (financial industry), Fair Wear Foundation (garments), and Electronics Industry Citizenship Coalition are examples of such organizations and initiatives. Resources are also available for verifying the life-cycle of products and the producer or vendor level, such as Green Seal for cleaning products, NSF-140 for carpet production, and even labeling of Organic food in the United States.[259]		The cultural dimension of sustainability is known as culture 21, the Agenda 21 for culture and vice versa. At least since 2005, with e.g. the start of UCLG and the first Culture Summit of UCLG in Bilbao - 18 to 20 March 2015, culture is seen as a pillar for Sustainable Development: "The organizing principle for sustainability is sustainable development, which includes the four interconnected domains: ecology, economics, politics and culture.[1]" The model is now being used by organizations such as the United Nations Cities Programme.[40] and Metropolis[41]. Culture 21 is anchored in the Sustainable Development Goals, more particularly in SDG Goal 11. Make cities and human settlements inclusive, safe, resilient and sustainable, Chapter 11.4 strengthen efforts to protect and safeguard the world’s cultural and natural heritage". More: SDG Goal 11. Make cities and human settlements inclusive, safe, resilient and sustainable		More: http://www.agenda21culture.net/		Sustainability is central to underpinning feelings of authenticity in tourism.[260] Experiences can be enhanced when substituting the contrived for the genuine, and at the same time inspire a potentially deleterious appetite for follow-up visits to the real thing: objectively authentic sites untouched by repair or rejuvenation. Feelings of authenticity at a tourist site are thus implicitly linked to sustainable tourism; as the maximisation of existential “felt” authenticity at sites of limited historical provenance increases the likelihood of return visits.[261]				
This is a list of genres of literature and entertainment, excluding genres in the visual arts. Genre is the term for any category of literature or other forms of art or entertainment, e.g. music, whether written or spoken, audio or visual, based on some set of stylistic criteria. Genres are formed by conventions that change over time as new genres are invented and the use of old ones are discontinued. Often, works fit into multiple genres by way of borrowing and recombining these conventions.		Absurdist and surreal fiction challenges causal reasoning and the purposefulness of life. There is often, though not always, a connection to comedy.		The whimsical and related styles exaggerate real life in a whimsical, eccentric, quirky or fanciful way, sometimes including 'magical' extensions of reality.		The absurdist genre focuses on the experiences of characters in situations where they cannot find any inherent purpose in life, most often represented by ultimately meaningless actions and events that call into question the certainty of existential concepts such as truth or value.		The closely related/overlapping surreal genre is predicated on deliberate violations of causal reasoning, producing events and behaviours that are obviously illogical. Constructions of surreal humour tend to involve bizarre juxtapositions, non-sequiturs, irrational or absurd situations and expressions of nonsense.		Whimsical and related styles are exemplified by films such as Underground, Amélie, Micmacs and Dieta Mediterranea (Mediterranean Food).[1]		An action story is similar to adventure, and the protagonist usually takes a risky turn, which leads to desperate situations (including explosions, fight scenes, daring escapes, etc.). Action and Adventure are usually categorized together (sometimes even as "action-adventure") because they have much in common, and many stories fall under both genres simultaneously (for instance, the James Bond series can be classified as both).		An adventure story is about a protagonist who journeys to epic or distant places to accomplish something. It can have many other genre elements included within it, because it is a very open genre. The protagonist has a mission and faces obstacles to get to their destination. Also, adventure stories usually include unknown settings and characters with prized properties or features.		Comedy is a story that tells about a series of funny, or comical events, intended to make the audience laugh. It is a very open genre, and thus crosses over with many other genres on a frequent basis.		(See also: Mystery below)		A crime story is about a crime that is being committed or was committed. It can also be an account of a criminal's life. It often falls into the action or adventure genres.		Within film, television and radio (but not theatre), drama is a genre of narrative fiction (or semi-fiction) intended to be more serious than humorous in tone,[2] focusing on in-depth development of realistic characters who must deal with realistic emotional struggles. A drama is commonly considered the opposite of a comedy, but may also be considered separate from other works of some broad genre, such as a fantasy.		A fantasy story is about magic or supernatural forces, rather than technology, though it often is made to include elements of other genres, such as science fiction elements, for instance computers or DNA, if it happens to take place in a modern or future era. Depending on the extent of these other elements, the story may or may not be considered to be a "hybrid genre" series; for instance, even though the Harry Potter series canon includes the requirement of a particular gene to be a wizard, it is referred to only as a fantasy series.		A story about a real person or event. Often, they are written in a text book format, which may or may not focus on solely that.		The genre historical fiction includes stories that are about the past. To distinguish historical fiction from any fiction that is written about an era in the past, the criteria that the book must have been written about a time that occurred in a historical context in relation to the author of the book.[3][4] The criteria that the story be set before the middle of the previous century is sometimes added.[4] Historical fiction stories include historical details and includes characters that fit into the time period of the setting, whether or not they are real historical people.[3]		A horror story is told to deliberately scare or frighten the audience, through suspense, violence or shock. H. P. Lovecraft distinguishes two primary varieties in the "Introduction" to Supernatural Horror in Literature: 1) Physical Fear or the "mundanely gruesome" and 2) the true Supernatural Horror story or the "Weird Tale". The supernatural variety is occasionally called "dark fantasy", since the laws of nature must be violated in some way, thus qualifying the story as "fantastic".		Magical realism, also called Magic realism, is literary works where magical events form part of ordinary life. The reader is forced to accept that abnormal events such as levitation, telekinesis and talking with the dead take place in the real world. The writer does not invent a new world or describes in great detail new creatures, as is usual in Fantasy; on the contrary, the author abstains from explaining the fantastic events in order to avoid making them feel extraordinary. It is often regarded as a genre exclusive to Latin American literature, but some of its chief exponents include English authors. One Hundred Years of Solitude, by Gabriel García Márquez, who received the 1982 Nobel Prize in Literature, is considered the genre's seminal work of style.		A mystery story follows an investigator as he/she attempts to solve a puzzle (often a crime). The details and clues are presented as the story continues and the protagonist discovers them and by the end of the story the mystery/puzzle is solved. For example, in the case of a crime mystery the perpetrator and motive behind the crime are revealed and the perpetrator is brought to justice. Mystery novels are often written in series which allows a more in-depth development of the primary investigator.[5][6] Specific types of mystery story include locked room mysteries.		Paranoid fiction is works of literature that explore the subjective nature of reality and how it can be manipulated by forces in power. These forces can be external, such as a totalitarian government, or they can be internal, such as a character's mental illness or refusal to accept the harshness of the world he or she is in.		Philosophical fiction is fiction in which a significant proportion of the work is devoted to a discussion of the sort of questions normally addressed in discursive philosophy. These might include the function and role of society, the purpose of life, ethics or morals, the role of art in human lives, and the role of experience or reason in the development of knowledge. Philosophical fiction works would include the so-called novel of ideas, including a significant proportion of science fiction, utopian and dystopian fiction, and Bildungsroman. The modus operandi seems to be to use a normal story to simply explain difficult and dark parts of human life.		Political fiction is a subgenre of fiction that deals with political affairs. Political fiction has often used narrative to provide commentary on political events, systems and theories. Works of political fiction often "directly criticize an existing society or... present an alternative, sometimes fantastic, reality." Prominent pieces of political fiction have included the totalitarian dystopias of the early 20th century such as Jack London's The Iron Heel and Sinclair Lewis's It Can't Happen Here. Equally influential, if not more so, have been earlier pieces of political fiction such as Gulliver's Travels (1726), Candide (1759) and Uncle Tom's Cabin (1852). Political fiction frequently employs the literary modes of satire, often in the genres of Utopian and dystopian fiction or social science fiction.		The term "romance" has multiple meanings; historical romances like those of Walter Scott would use the term to mean "a fictitious narrative in prose or verse; the interest of which turns upon marvellous and uncommon incidents".[7] But most often a romance is understood to be "love stories", emotion-driven stories that are primarily focused on the relationship between the main characters of the story. Beyond the focus on the relationship, the biggest defining characteristic of the romance genre is that a happy ending is always guaranteed...[8][9] perhaps marriage and living "happily ever after", or simply that the reader sees hope for the future of the romantic relationship.[9] Due to the wide definition of romance, romance stories cover a wide variety of subjects and often fall into other genre categories as well as romance,[8][9] such as Comedy-Romance (also known as romcom films), romantic suspense and (less common now): subcategories such as hospital romances, as found in the novels by Lucilla Andrews. See Mills & Boon imprint categories and Harlequin romances categories for a partial list of other sub-genres.		The sagas (from Icelandic saga, plural sögur) are stories about ancient Scandinavian and Germanic history, about early Viking voyages, about migration to Iceland, and of feuds between Icelandic families. They were written in the Old Norse language, mainly in Iceland. The texts are epic tales in prose, often with stanzas or whole poems in alliterative verse embedded in the text, of heroic deeds of days long gone, tales of worthy men, who were often Vikings, sometimes Pagan, sometimes Christian. The tales are usually realistic, except legendary sagas, sagas of saints, sagas of bishops and translated or recomposed romances. They are sometimes romanticised and fantastic, but always dealing with human beings one can understand.		Often strictly defined as a literary genre or form, although in practice it is also found in the graphic and performing arts. In satire, human or individual vices, follies, abuses, or shortcomings are held up to censure by means of ridicule, derision, burlesque, irony, or other methods, ideally with the intent to bring about improvement. Although satire is usually meant to be funny, the purpose of satire is not primarily humour in itself so much as an attack on something of which the author strongly disapproves, using the weapon of wit. A very common, almost defining feature of satire is its strong vein of irony or sarcasm, but parody, burlesque, exaggeration, juxtaposition, comparison, analogy, and double entendre are all frequently used in satirical speech and writing. The essential point, is that "in satire, irony is militant." This "militant irony" (or sarcasm) often professes to approve (or at least accept as natural) the very things the satirist actually wishes to attack.		Science fiction is similar to fantasy, except stories in this genre use scientific understanding to explain the universe that it takes place in. It generally includes or is centered on the presumed effects or ramifications of computers or machines; travel through space, time or alternate universes; alien life-forms; genetic engineering; or other such things. The science or technology used may or may not be very thoroughly elaborated on; stories whose scientific elements are reasonably detailed, well-researched and considered to be relatively plausible given current knowledge and technology are often referred to as hard science fiction.		A slice of life is a story that might have no plot, but represents a portion of (everyday) life. It uses naturalistic representation of real life, sometimes used as an adjective, as in "a play with 'slice of life' dialogue".		Speculative fiction speculates about worlds that are unlike the real world in various important ways. In these contexts, it generally overlaps one or more of the following: science fiction, fantasy fiction, horror fiction, supernatural fiction, superhero fiction, utopian and dystopian fiction, apocalyptic and post-apocalyptic fiction, and alternate history.		Suppositional fiction is a subcategory in which stories and characters are constrained within an internally consistent world, but this category is not necessarily associated with any particular genre.[10][11][12] A work of suppositional fiction might be science fiction, alternate history, mystery, horror, or even suppositional fantasy, depending on the intent and focus of the author.		A Thriller is a story that is usually a mix of fear and excitement. It has traits from the suspense genre and often from the action, adventure or mystery genres, but the level of terror makes it borderline horror fiction at times as well. It generally has a dark or serious theme, which also makes it similar to drama.		Urban fiction, also known as street lit, is a literary genre set, as the name implies, in a city landscape; however, the genre is as much defined by the race and culture of its characters as the urban setting. The tone for urban fiction is usually dark, focusing on the underside. Profanity (all of George Carlin's seven dirty words and urban variations thereof), sex and violence are usually explicit, with the writer not shying away from or watering-down the material. In this respect, urban fiction shares some common threads with dystopian or survivalist fiction. In the second wave of urban fiction, some variations of this model have been seen.		Stories in the Western genre are set in the American West, between the time of the Civil war and the early twentieth century.[13] The setting of a wilderness or uncivilized area is especially important to the genre, and the setting is often described richly and in-depth. They focus on the adventure of the main character(s) and the contrast between civilization or society and the untamed wilderness, often featuring the characters working to bring civilization to the wilderness.[13][14] This genre periodically overlaps with historical fiction, and while a more traditional definition of westerns is that of stories about lone men facing the frontier, more modern definitions and writings are often expanded to include any person or persons in this time period that feature a strong tone of the contrast between civilization and wilderness and emphasize the independence of the main character(s).[13]		Genres are listed under the sub-sectioned formats:		Genres in video games are somewhat different from other forms of art as most video game genres are based on the way in which the player interacts with the game. All genres from all other types of media can be applied to video games but are secondary to the genre types described below.		Genres unique to video games:		The term country music gained popularity in the 1940s in preference to the earlier term hillbilly music; it came to encompass Western music, which evolved parallel to hillbilly music from similar roots, in the mid-20th century. The term country music is used today to describe many styles and subgenres. In 2009 country music was the most listened to rush hour radio genre during the evening commute, and second most popular in the morning commute in the United States.		
The New York Times (sometimes abbreviated NYT and The Times) is an American daily newspaper, founded and continuously published in New York City since September 18, 1851, by The New York Times Company. The New York Times has won 122 Pulitzer Prizes, more than any other newspaper.[6][7][8][9] The paper's print version in 2013 had the second-largest circulation, behind The Wall Street Journal, and the largest circulation among the metropolitan newspapers in the United States. The New York Times is ranked 18th in the world by circulation. Following industry trends, its weekday circulation had fallen in 2009 to fewer than one million.[10]		Nicknamed "The Gray Lady",[11] The New York Times has long been regarded within the industry as a national "newspaper of record".[12] It has been owned by the Ochs-Sulzberger family since 1896; Arthur Ochs Sulzberger Jr., the publisher of the Times and the chairman of the New York Times Company, is the fourth generation of the family to helm the paper.[13] The New York Times international version, formerly the International Herald Tribune, is now called the New York Times International Edition.[14] The paper's motto, "All the News That's Fit to Print", appears in the upper left-hand corner of the front page.		Since the mid-1970s, The New York Times has greatly expanded its layout and organization, adding special weekly sections on various topics supplementing the regular news, editorials, sports, and features. Since 2008,[15] The New York Times has been organized into the following sections: News, Editorials/Opinions-Columns/Op-Ed, New York (metropolitan), Business, Sports of The Times, Arts, Science, Styles, Home, Travel, and other features.[16] On Sunday, The New York Times is supplemented by the Sunday Review (formerly the Week in Review),[17] The New York Times Book Review,[18] The New York Times Magazine[19] and T: The New York Times Style Magazine (T is published 13 times a year).[20] The New York Times stayed with the broadsheet full page set-up (as some others have changed into a tabloid lay-out) and an eight-column format for several years, after most papers switched to six,[21] and was one of the last newspapers to adopt color photography, especially on the front page.[22]		The New York Times was founded as the New-York Daily Times on September 18, 1851,[a] published by Raymond, Jones & Company (raising about $70,000);[24] by journalist and politician Henry Jarvis Raymond (1820–69), then a Whig Party member and later second chairman of the newly organized Republican Party National Committee, and former banker George Jones. Other early investors of the company were Edwin B. Morgan,[25] Christopher Morgan,[26] and Edward B. Wesley.[27] Sold for a penny (equivalent to 29 cents today), the inaugural edition attempted to address various speculations on its purpose and positions that preceded its release:[28]		We shall be Conservative, in all cases where we think Conservatism essential to the public good;—and we shall be Radical in everything which may seem to us to require radical treatment and radical reform. We do not believe that everything in Society is either exactly right or exactly wrong;—what is good we desire to preserve and improve;—what is evil, to exterminate, or reform.		In 1852, the newspaper started a western division, The Times of California that arrived whenever a mail boat got to California. However, when local California newspapers came into prominence, the effort failed.[29]		The newspaper shortened its name to The New-York Times on September 14, 1857. It dropped the hyphen in the city name on December 1, 1896.[30] On April 21, 1861, The New York Times departed from its original Monday–Saturday publishing schedule and joined other major dailies in adding a Sunday edition to offer daily coverage of the Civil War. One of the earliest public controversies it was involved with was the Mortara Affair, the subject of twenty editorials it published alone.[31]		The main office of The New York Times was attacked during the New York Draft Riots sparked by the beginning of military conscription for the Northern Union Army now instituted in the midst of the Civil War on July 13, 1863. At "Newspaper Row", across from City Hall, Henry Raymond, owner and editor of The New York Times, averted the rioters with "Gatling" (early machine, rapid-firing) guns, one of which he manned himself. The mob now diverted, instead attacked the headquarters of abolitionist publisher Horace Greeley's New York Tribune until forced to flee by the Brooklyn City Police, who had crossed the East River to help the Manhattan authorities.[32]		In 1869, Raymond died, and George Jones took over as publisher.[33]		The newspaper's influence grew during 1870–1 when it published a series of exposés on William Magear ("Boss") Tweed, leader of the city's Democratic Party—popularly known as "Tammany Hall" (from its early 19th Century meeting headquarters)—that led to the end of the "Tweed Ring's" domination of New York's City Hall.[34] Tweed offered The New York Times five million dollars (equivalent to more than 100 million dollars today) to not publish the story.[25] In the 1880s, The New York Times transitioned gradually from editorially supporting Republican Party candidates to becoming more politically independent and analytical.[35] In 1884, the paper supported Democrat Grover Cleveland (former Mayor of Buffalo and Governor of New York State) in his first presidential campaign.[36] While this move cost The New York Times' readership among its more progressive and Republican readers (the revenue went down from $188,000 to $56,000 from 1883-4- however some part of this was due to the price going down to two cents, in order to compete with the World and Sun), the paper eventually regained most of its lost ground within a few years.[37] After George Jones died in 1891, Charles Ransom Miller raised $1 million dollars to buy the Times, along with other fellow editors at the newspaper, printing it under the New York Times Publishing Company.[38][39] However, the newspaper was financially crippled by the Panic of 1893.[37] By 1896, The New York Times had a circulation of less than 9,000, and was losing $1,000 a day when controlling interest in it was gained by Adolph Ochs, publisher of the Chattanooga Times for $75,000.[40]		Shortly after assuming control of the paper, Ochs coined the paper's slogan, "All The News That's Fit To Print". The slogan has appeared in the paper since September 1896,[41] and has been printed in a box in the upper left hand corner of the front page since early 1897.[36] This was a jab at competing papers such as Joseph Pulitzer's New York World and William Randolph Hearst's New York Journal which were now being known for a lurid, sensationalist and often inaccurate reporting of facts and opinions known by the end of the century as "yellow journalism".[42] Under Ochs' guidance, continuing and expanding upon the Henry Raymond tradition, (which were from the era of James Gordon Bennett of the New York Herald which predated Pulitzer and Hearst's arrival in New York), The New York Times achieved international scope, circulation, and reputation (the Sunday circulation went from 9,000 in 1896 to 780,000 in 1934).[40] In 1904, The New York Times, along with The Times received the first on-the-spot wireless telegraph transmission from a naval battle, a report of the destruction of the Imperial Russian Navy's Baltic Fleet at the Battle of Port Arthur in the Straits of Tsushima off the eastern coast of Korea in the Yellow Sea in the western Pacific Ocean after just sailing across the globe from Europe from the press-boat Haimun during the Russo-Japanese War .[43] In 1910, the first air delivery of The New York Times to Philadelphia began.[36] The New York Times' first trans-Atlantic delivery by air to London occurred in 1919 by dirigible. In 1920, a "4 A.M. Airplane Edition" was sent by plane to Chicago so it could be in the hands of Republican convention delegates by evening.[44]		In the 1940s, the paper extended its breadth and reach. The crossword began appearing regularly in 1942, and the fashion section in 1946. The New York Times began an international edition in 1946. The international edition stopped publishing in 1967, when The New York Times joined the owners of the New York Herald Tribune and The Washington Post to publish the International Herald Tribune in Paris. The paper bought AM radio station WQXR (1560 kHz) in 1944.[45] Its "sister" FM station, WQXQ, would become WQXR-FM (96.3 MHz). Branded as "The Radio Stations of The New York Times", its classical music radio format was simulcast on both the AM & FM frequencies until December 1992, when the big-band and pop standards music format of station WNEW (1130 kHz – now WBBR/"Bloomberg Radio") was transferred to and adopted by WQXR; in recognition of the format change, WQXR changed its call letters to WQEW (a "hybrid" combination of "WQXR" and "WNEW").[46] By 1999, The New York Times was leasing WQEW to ABC Radio for its "Radio Disney" format.[47] In 2007, WQEW was finally purchased by Disney; in late 2014, it was sold to Family Radio (a religious radio network) and became WFME.[48] On July 14, 2009, it was announced that WQXR-FM would be sold to the WNYC radio group who, on October 8, 2009, moved the station from 96.3 to 105.9 MHz (swapping frequencies with Spanish-language station WXNY-FM, which wanted the more powerful transmitter to increase its coverage) and began operating it as a non-commercial, public radio station.[49] After the purchase, WQXR-FM retained the classical music format, whereas WNYC-FM (93.9 MHz) abandoned it, switching to a talk radio format.		On September 14, 1987, the Times printed the heaviest ever newspaper, at over 12 pounds (5.4 kg) and 1,612 pages.[50]		In 2009, the newspaper began production of local inserts in regions outside of the New York area. Beginning October 16, 2009, a two-page "Bay Area" insert was added to copies of the Northern California edition on Fridays and Sundays. The newspaper commenced production of a similar Friday and Sunday insert to the Chicago edition on November 20, 2009. The inserts consist of local news, policy, sports, and culture pieces, usually supported by local advertisements.		In addition to its New York City headquarters, the newspaper has ten news bureaus in the New York region, eleven national news bureaus and 26 foreign news bureaus.[51] The New York Times reduced its page width to 12 inches (300 mm) from 13.5 inches (340 mm) on August 6, 2007, adopting the width that has become the U.S. newspaper industry standard.[52]		In February 2013, the paper stopped offering lifelong positions for its journalists and editors.[53][relevant? – discuss]		Because of its steadily declining sales attributed to the rise of online alternative media and social media, the newspaper has been going through a downsizing for several years, offering buyouts to workers and cutting expenses,[54] in common with a general trend among print news media.[55]		In 2016, reporters for the newspaper were reportedly the target of cyber security breaches. The Federal Bureau of Investigation was reportedly investigating the attacks. The cyber security breaches have been described as possibly being related to cyberattacks that targeted other institutions, such as the Democratic National Committee.[56]		The newspaper's first building was located at 113 Nassau Street in New York City. In 1854, it moved to 138 Nassau Street, and in 1858 to 41 Park Row, making it the first newspaper in New York City housed in a building built specifically for its use.[57]		The newspaper moved its headquarters to the Times Tower, located at 1475 Broadway in 1904,[58] in an area called Longacre Square, that was later renamed Times Square in honor of the newspaper.[59] The top of the building – now known as One Times Square – is the site of the New Year's Eve tradition of lowering a lighted ball, which was started by the paper.[60] The building is also notable for its electronic news ticker – popularly known as "The Zipper" – where headlines crawl around the outside of the building.[61] It is still in use, but has been operated by Dow Jones & Company since 1995.[62] After nine years in its Times Square tower the newspaper had an annex built at 229 West 43rd Street.[63] After several expansions, the 43rd Street building became the newspaper's main headquarters in 1960 and the Times Tower on Broadway was sold the following year.[64] It served as the newspaper's main printing plant until 1997, when the newspaper opened a state-of-the-art printing plant in the College Point section of the borough of Queens.[65]		A decade later, The New York Times moved its newsroom and businesses headquarters from West 43rd Street to a new tower at 620 Eighth Avenue between West 40th and 41st Streets, in Manhattan – directly across Eighth Avenue from the Port Authority Bus Terminal. The new headquarters for the newspaper, known officially as The New York Times Building but unofficially called the new "Times Tower" by many New Yorkers, is a skyscraper designed by Renzo Piano.[66][67]		The paper's involvement in a 1964 libel case helped bring one of the key United States Supreme Court decisions supporting freedom of the press, New York Times Co. v. Sullivan. In it, the United States Supreme Court established the "actual malice" standard for press reports about public officials or public figures to be considered defamatory or libelous. The malice standard requires the plaintiff in a defamation or libel case prove the publisher of the statement knew the statement was false or acted in reckless disregard of its truth or falsity. Because of the high burden of proof on the plaintiff, and difficulty in proving malicious intent, such cases by public figures rarely succeed.[68]		In 1971, the Pentagon Papers, a secret United States Department of Defense history of the United States' political and military involvement in the Vietnam War from 1945 to 1967, were given ("leaked") to Neil Sheehan of The New York Times by former State Department official Daniel Ellsberg, with his friend Anthony Russo assisting in copying them. The New York Times began publishing excerpts as a series of articles on June 13. Controversy and lawsuits followed. The papers revealed, among other things, that the government had deliberately expanded its role in the war by conducting air strikes over Laos, raids along the coast of North Vietnam, and offensive actions taken by U.S. Marines well before the public was told about the actions, all while President Lyndon B. Johnson had been promising not to expand the war. The document increased the credibility gap for the U.S. government, and hurt efforts by the Nixon administration to fight the ongoing war.[69]		When The New York Times began publishing its series, President Richard Nixon became incensed. His words to National Security Advisor Henry Kissinger included "People have gotta be put to the torch for this sort of thing..." and "Let's get the son-of-a-bitch in jail."[70] After failing to get The New York Times to stop publishing, Attorney General John Mitchell and President Nixon obtained a federal court injunction that The New York Times cease publication of excerpts. The newspaper appealed and the case began working through the court system. On June 18, 1971, The Washington Post began publishing its own series. Ben Bagdikian, a Post editor, had obtained portions of the papers from Ellsberg. That day the Post received a call from the Assistant Attorney General, William Rehnquist, asking them to stop publishing. When the Post refused, the U.S. Justice Department sought another injunction. The U.S. District court judge refused, and the government appealed. On June 26, 1971, the U.S. Supreme Court agreed to take both cases, merging them into New York Times Co. v. United States 403 US 713. On June 30, 1971, the Supreme Court held in a 6–3 decision that the injunctions were unconstitutional prior restraints and that the government had not met the burden of proof required. The justices wrote nine separate opinions, disagreeing on significant substantive issues. While it was generally seen as a victory for those who claim the First Amendment enshrines an absolute right to free speech, many felt it a lukewarm victory, offering little protection for future publishers when claims of national security were at stake.[69]		Discriminatory practices restricting women in editorial positions were previously used by the paper. The newspaper's first general woman reporter was Jane Grant, who described her experience afterwards. She wrote, "In the beginning I was charged not to reveal the fact that a female had been hired". Other reporters nicknamed her Fluff and she was subjected to considerable hazing. Because of her gender, promotions were out of the question, according to the then-managing editor. She was there for fifteen years, interrupted by World War I.[71]		In 1935, Anne McCormick wrote to Arthur Hays Sulzberger, "I hope you won't expect me to revert to 'woman's-point-of-view' stuff."[72] Later, she interviewed major political leaders and appears to have had easier access than her colleagues did. Even those who witnessed her in action were unable to explain how she got the interviews she did.[73] Clifton Daniel said, "[After World War II,] I'm sure Adenauer called her up and invited her to lunch. She never had to grovel for an appointment."[74] Covering world leaders' speeches after World War II at the National Press Club was limited to men by a Club rule. When women were eventually allowed in to hear the speeches, they still were not allowed to ask the speakers questions, although men were allowed and did ask, even though some of the women had won Pulitzer Prizes for prior work.[75] Times reporter Maggie Hunter refused to return to the Club after covering one speech on assignment.[76] Nan Robertson's article on the Union Stock Yards, Chicago, was read aloud as anonymous by a professor, who then said, "'It will come as a surprise to you, perhaps, that the reporter is a girl,' he began... [G]asps; amazement in the ranks. 'She had used all her senses, not just her eyes, to convey the smell and feel of the stockyards. She chose a difficult subject, an offensive subject. Her imagery was strong enough to revolt you.'"[77] The New York Times hired Kathleen McLaughlin after ten years at the Chicago Tribune, where "[s]he did a series on maids, going out herself to apply for housekeeping jobs."[78]		The New York Times published on December 20, 2012, an interactive storytelling in longform multimedia, Snow Fall: The Avalanche at Tunnel Creek by reporter John Branch about the 2012 Tunnel Creek avalanche. The six-part story, which integrated video, photos, and graphics, was hailed as a watershed moment for the journalism industry.[79][80] The feature was awarded a Peabody Award, which called the piece a "spectacular example of the potential of digital-age storytelling" which "combines thorough traditional reporting of a deadly avalanche with stunning topographic video."[81] Snow Fall inspired the Times to appoint Sam Sifton "Snowfaller in Chief," expanding multimedia narratives in the newsroom in the tradition of Snow Fall.[82]		In 1896, Adolph Ochs bought The New York Times, a money-losing newspaper, and formed the New York Times Company. The Ochs-Sulzberger family, one of the United States' newspaper dynasties, has owned The New York Times ever since.[36] The publisher went public on January 14, 1969, trading at $42 a share on the American Stock Exchange.[83] After this, the family continued to exert control through its ownership of the vast majority of Class B voting shares. Class A shareholders are permitted restrictive voting rights while Class B shareholders are allowed open voting rights.		The Ochs-Sulzberger family trust controls roughly 88 percent of the company's class B shares. Any alteration to the dual-class structure must be ratified by six of eight directors who sit on the board of the Ochs-Sulzberger family trust. The Trust board members are Daniel H. Cohen, James M. Cohen, Lynn G. Dolnick, Susan W. Dryfoos, Michael Golden, Eric M. A. Lax, Arthur O. Sulzberger, Jr. and Cathy J. Sulzberger.[84]		Turner Catledge, the top editor at The New York Times from 1952 to 1968, wanted to hide the ownership influence. Arthur Sulzberger routinely wrote memos to his editor, each containing suggestions, instructions, complaints, and orders. When Catledge would receive these memos he would erase the publisher's identity before passing them to his subordinates. Catledge thought that if he removed the publisher's name from the memos it would protect reporters from feeling pressured by the owner.[85]		On January 20, 2009, The New York Times reported that its parent company, The New York Times Company, had reached an agreement to borrow $250 million from Carlos Slim, a Mexican businessman and the world's second richest person,[86] "to help the newspaper company finance its businesses".[87] The New York Times Company later repaid that loan ahead of schedule.[88] Since then, Slim has bought large quantities of the company's Class A shares, which are available for purchase by the public and offer less control over the company than Class B shares, which are privately held.[88] Slim's investments in the company included large purchases of Class A shares in 2011, when he increased his stake in the company to 8.1% of Class A shares,[89] and again in 2015, when he exercised stock options—acquired as part of a repayment plan on the 2009 loan—to purchase 15.9 million Class A shares.[88] As of March 7, 2016, Slim owned 17.4% of the company's Class A shares, according to annual filings submitted by the company.[90][91]		Although Slim is the largest shareholder in the company, his investment does not give him the ability to control the newspaper, as his stake allows him to vote only for Class A directors, who compose just a third of the company's board.[88] According to the company's 2016 annual filings, Slim did not own any of the company's Class B shares.[90]		Dual-class structures caught on in the mid-20th century as families such as the Grahams of The Washington Post Company sought to gain access to public capital without losing control. Dow Jones & Co., publisher of The Wall Street Journal, had a similar structure and was controlled by the Bancroft family but was later bought by News Corporation in 2007, which itself is controlled by Rupert Murdoch and his family through a similar dual-class structure.[92]		The newspaper is organized in three sections, including the magazine.		Some sections, such as Metro, are only found in the editions of the paper distributed in the New York–New Jersey–Connecticut Tri-State Area and not in the national or Washington, D.C. editions.[93] Aside from a weekly roundup of reprints of editorial cartoons from other newspapers, The New York Times does not have its own staff editorial cartoonist, nor does it feature a comics page or Sunday comics section.[94] In September 2008, The New York Times announced that it would be combining certain sections effective October 6, 2008, in editions printed in the New York metropolitan area. The changes folded the Metro Section into the main International / National news section and combined Sports and Business (except Saturday through Monday, when Sports is still printed as a standalone section). This change also included having the name of the Metro section be called New York outside of the Tri-State Area. The presses used by The New York Times allow four sections to be printed simultaneously; as the paper had included more than four sections all days except Saturday, the sections had to be printed separately in an early press run and collated together. The changes will allow The New York Times to print in four sections Monday through Wednesday, in addition to Saturday. The New York Times' announcement stated that the number of news pages and employee positions will remain unchanged, with the paper realizing cost savings by cutting overtime expenses.[15] According to Russ Stanton, editor of the Los Angeles Times, a competitor, the newsroom of The New York Times is twice the size of the Los Angeles Times, which has a newsroom of 600.[95] In March 2014, Vanessa Friedman was named the "fashion director and chief fashion critic" of The New York Times.[96]		When referring to people, The New York Times generally uses honorifics, rather than unadorned last names (except in the sports pages, Book Review and Magazine).[97] It stayed with an eight-column format until September 7, 1976, years after other papers had switched to six,[21] and it was one of the last newspapers to adopt color photography, with the first color photograph on the front page appearing on October 16, 1997.[22] In the absence of a major headline, the day's most important story generally appears in the top-right column, on the main page. The typefaces used for the headlines are custom variations of Cheltenham. The running text is set at 8.7 point Imperial.[98][99]		Joining a roster of other major American newspapers in the last ten years, including USA Today, The Wall Street Journal and The Washington Post, The New York Times announced on July 18, 2006, that it would be narrowing the width of its paper by six inches. In an era of dwindling circulation and significant advertising revenue losses for most print versions of American newspapers, the move, which would result in a five percent reduction in news coverage, would have a target savings of $12 million a year for the paper.[100] The change from the traditional 54 inches (1.4 m) broadsheet style to a more compact 48-inch web width (12-inch page width) was addressed by both Executive Editor Bill Keller and The New York Times President Scott Heekin-Canedy in memos to the staff. Keller defended the "more reader-friendly" move indicating that in cutting out the "flabby or redundant prose in longer pieces" the reduction would make for a better paper. Similarly, Keller confronted the challenges of covering news with "less room" by proposing more "rigorous editing" and promised an ongoing commitment to "hard-hitting, ground-breaking journalism".[101] The official change went into effect on August 6, 2007.[102]		The New York Times printed a display advertisement on its first page on January 6, 2009, breaking tradition at the paper.[103] The advertisement, for CBS, was in color and ran the entire width of the page.[104] The newspaper promised it would place first-page advertisements on only the lower half of the page.[103]		In August 2014, The Times decided to use the word "torture" to describe incidents in which interrogators "inflicted pain on a prisoner in an effort to get information." This was a shift from the paper's previous practice of describe such practices as "harsh" or "brutal" interrogations.[105]		The paper maintains a strict profanity policy. A 2007 review of a concert by punk band Fucked Up, for example, completely avoided mention of the group's name.[106] However, the Times has on occasion published unfiltered video content that includes profanity and slurs where it has determined that such video has news value.[107] During the 2016 U.S. presidential election campaign, the Times did print the words "fuck" and "pussy," among others, when reporting on the vulgar statements made by Donald Trump in a 2005 recording. Times politics editor Carolyn Ryan said: "It's a rare thing for us to use this language in our stories, even in quotes, and we discussed it at length," ultimately deciding to publish it because of its news value and because "[t]o leave it out or simply describe it seemed awkward and less than forthright to us, especially given that we would be running a video that showed our readers exactly what was said."[108]		The New York Times has won 122 Pulitzer Prizes, more than any other newspaper. The prize is awarded for excellence in journalism in a range of categories.[6]		It has also won three Peabody Awards (and jointly received two). A Peabody award was given in 2003 for the documentary Frontline: A Dangerous Business, a joint investigation by the New York Times, the Canadian Broadcasting Corporation, and WGBH's Frontline about the conditions faced by workers at McWayne Inc.[109] The newspaper won another Peabody award (in 1951) for the New York Times Youth Forum (which featured "unrehearsed discussion by students selected from private, public and parochial schools, on topics ranging from the political, educational and scientific to the international and the United Nations.")[110] Again in 2008, the newspaper won another reward for "Aggressively and creatively adding sound and moving images to its traditional package of news and features, The New York Times has stepped forward as an innovator in online journalism. Its website exemplifies a new age for the press, expanding its role in ways unimaginable only a few years ago."[111] In 2013, the Times, along with The National Film Board of Canada won a Peabody Award for the documentary "A Short History of the Highrise".[112] A personal award was also given to then chief media critic Jack Gould in 1956.[113]		The New York Times began publishing daily on the World Wide Web on January 22, 1996, "offering readers around the world immediate access to most of the daily newspaper's contents."[114] Since its online launch, the newspaper has consistently been ranked one of the top websites. Accessing some articles requires registration, though this could be bypassed in some cases through Times RSS feeds.[115] The website had 555 million pageviews in March 2005.[116] The domain nytimes.com attracted at least 146 million visitors annually by 2008 according to a Compete.com study. The New York Times Web site ranks 59th by number of unique visitors, with over 20 million unique visitors in March 2009 making it the most visited newspaper site with more than twice the number of unique visitors as the next most popular site.[117] as of May 2009[update], nytimes.com produced 22 of the 50 most popular newspaper blogs.[118] NYTimes.com is ranked 118 in the world, and 32 in the U.S. by Alexa (as of June 4, 2017).[119]		In September 2005, the paper decided to begin subscription-based service for daily columns in a program known as TimesSelect, which encompassed many previously free columns. Until being discontinued two years later, TimesSelect cost $7.95 per month or $49.95 per year,[120] though it was free for print copy subscribers and university students and faculty.[121][122] To avoid this charge, bloggers often reposted TimesSelect material,[123] and at least one site once compiled links of reprinted material.[124] On September 17, 2007, The New York Times announced that it would stop charging for access to parts of its Web site, effective at midnight the following day, reflecting a growing view in the industry that subscription fees cannot outweigh the potential ad revenue from increased traffic on a free site.[125] In addition to opening almost the entire site to all readers, The New York Times news archives from 1987 to the present are available at no charge, as well as those from 1851 to 1922, which are in the public domain.[126][127] Access to the Premium Crosswords section continues to require either home delivery or a subscription for $6.95 per month or $39.95 per year. Times columnists including Nicholas Kristof and Thomas Friedman had criticized TimesSelect,[128][129] with Friedman going so far as to say "I hate it. It pains me enormously because it's cut me off from a lot, a lot of people, especially because I have a lot of people reading me overseas, like in India ... I feel totally cut off from my audience."[130]		The New York Times was made available on the iPhone and iPod Touch in 2008,[131] and on the iPad mobile devices in 2010.[132] It was also the first newspaper to offer a video game as part of its editorial content, Food Import Folly by Persuasive Games.[133] In 2010, The New York Times editors collaborated with students and faculty from New York University's Studio 20 Journalism Masters program to launch and produce "The Local East Village", a hyperlocal blog designed to offer news "by, for and about the residents of the East Village".[134] That same year, reCAPTCHA helped to digitize old editions of The New York Times.[135]		In 2012, The New York Times introduced a Chinese-language news site, cn.nytimes.com, with content created by staff based in Shanghai, Beijing and Hong Kong, though the server was placed outside of China to avoid censorship issues.[136] In March 2013, The New York Times and National Film Board of Canada announced a partnership titled A Short History of the Highrise, which will create four short documentaries for the Internet about life in highrise buildings as part of the NFB's Highrise project, utilizing images from the newspaper's photo archives for the first three films, and user-submitted images for the final film.[137] The third project in the series, "A Short History of the Highrise", won a Peabody Award in 2013.[138]		Falling print advertising revenue and projections of continued decline resulted in a paywall being instituted in 2011, regarded as modestly successful after garnering several hundred thousand subscriptions and about $100 million in revenue as of March 2012[update].[139] The paywall was announced on March 17, 2011, that starting on March 28, 2011 (March 17, 2011, for Canada), it would charge frequent readers for access to its online content.[140] Readers would be able to access up to 20 articles each month without charge. (Although beginning in April 2012, the number of free-access articles was halved to just ten articles per month.) Any reader who wanted to access more would have to pay for a digital subscription. This plan would allow free access for occasional readers, but produce revenue from "heavy" readers. Digital subscriptions rates for four weeks range from $15 to $35 depending on the package selected, with periodic new subscriber promotions offering four-week all-digital access for as low as 99¢. Subscribers to the paper's print edition get full access without any additional fee. Some content, such as the front page and section fronts will remain free, as well as the Top News page on mobile apps.[141] In January 2013, The New York Times' Public Editor Margaret M. Sullivan announced that for the first time in many decades, the paper generated more revenue through subscriptions than through advertising.[142]		The newspaper's website was hacked on August 29, 2013, by the Syrian Electronic Army, a hacking group that supports the government of Syrian President Bashar al-Assad. The SEA managed to penetrate the paper's domain name registrar, Melbourne IT, and alter DNS records for The New York Times, putting some of its websites out of service for hours.[143]		The food section is supplemented on the web by properties for home cooks and for out-of-home dining. New York Times Cooking (cooking.nytimes.com; also available via iOS app) provides access to more than 17,000 recipes on file as of November 2016,[144] and availability of saving recipes from other sites around the web. The newspaper's restaurant search (nytimes.com/reviews/dining) allows online readers to search NYC area restaurants by cuisine, neighborhood, price, and reviewer rating. The New York Times has also published several cookbooks, including The Essential New York Times Cookbook: Classic Recipes for a New Century, published in late 2010.		The Times Reader is a digital version of The New York Times. It was created via a collaboration between the newspaper and Microsoft. Times Reader takes the principles of print journalism and applies them to the technique of online reporting. Times Reader uses a series of technologies developed by Microsoft and their Windows Presentation Foundation team. It was announced in Seattle in April 2006, by Arthur Ochs Sulzberger Jr., Bill Gates, and Tom Bodkin.[145] In 2009, the Times Reader 2.0 was rewritten in Adobe AIR.[146] In December 2013, the newspaper announced that the Times Reader app would be discontinued on January 6, 2014, urging readers of the app to instead begin using the subscription-only "Today's Paper" app.[147]		In 2008, The New York Times created an app for the iPhone and iPod Touch which allowed users to download articles to their mobile device enabling them to read the paper even when they were unable to receive a signal.[148] In April 2010, The New York Times announced it would begin publishing daily content through an iPad app.[149] As of October 2010[update], The New York Times iPad app is ad-supported and available for free without a paid subscription, but translated into a subscription-based model in 2011.[132]		In 2010, the newspaper also launched an app for Android smartphones, followed later by an app for Windows Phones.[150]		The New York Times began producing podcasts in 2006. Among the early podcasts were Inside The Times and Inside The New York Times Book Review. Several of the Times podcasts were cancelled in 2012.[151][152] The Times returned to launching new podcasts in 2016, including Modern Love with WBUR.[153] On January 30, 2017, The New York Times launched a new podcast The Daily.[154][155]		In June 2012, The New York Times launched its first official foreign-language variant, cn.nytimes.com, in Chinese,[156] viewable in both traditional and simplified Chinese characters. The project was led by Craig S. Smith on the business side and Philip P. Pan on the editorial side.		The site's initial success was interrupted in October that year following the publication of an investigative article[b] by David Barboza about the finances of Chinese Premier Wen Jiabao's family.[157] In retaliation for the article, the Chinese government blocked access to both nytimes.com and cn.nytimes.com inside the People's Republic of China (PRC).		Despite Chinese government interference, however, the Chinese-language operations have continued to develop, adding a second site, cn.nytstyle.com, iOS and Android apps and newsletters, all of which are accessible inside the PRC. The China operations also produce three print publications in Chinese. Traffic to cn.nytimes.com, meanwhile, has risen due to the widespread use of VPN technology in the PRC and to a growing Chinese audience outside mainland China.[158] New York Times articles are also available to users in China via the use of mirror websites, apps, domestic newspapers, and social media.[158][159] The Chinese platforms now represent one of The New York Times' top five digital markets globally. The editor-in-chief of the Chinese platforms is Ching-Ching Ni.[160]		The website's "Newsroom Navigator" collects online resources for use by reporters and editors. It is maintained by Rich Meislin.[161][162][163] Further specific collections are available to cover the subjects of business, politics and health.[161][164][165] In 1998, Meislin was editor-in-chief of electronic media at the newspaper.[166]		Because of holidays, no editions were printed on November 23, 1851; January 2, 1852; July 4, 1852; January 2, 1853; and January 1, 1854.[167]		Because of strikes, the regular edition of The New York Times was not printed during the following periods:[168]		The New York Times editorial page is often regarded as liberal.[171][172] In mid-2004, the newspaper's then public editor (ombudsman), Daniel Okrent, wrote that "the Op-Ed page editors do an evenhanded job of representing a range of views in the essays from outsiders they publish – but you need an awfully heavy counterweight to balance a page that also bears the work of seven opinionated columnists, only two of whom could be classified as conservative (and, even then, of the conservative subspecies that supports legalization of gay unions and, in the case of William Safire, opposes some central provisions of the Patriot Act."[173]		The New York Times has not endorsed a Republican for president since Dwight D. Eisenhower in 1956; since 1960, it has endorsed the Democratic nominee in every presidential election (see New York Times presidential endorsements).[174] However, the Times did endorse incumbent Republican Mayors of New York City Rudy Giuliani in 1997[175] and Michael Bloomberg in 2005[176] and 2009.[177] The Times also endorsed Republican Governor George Pataki in 2002.[178]		The New York Times supported the 2003 invasion of Iraq.[179] On May 26, 2004, a year after the war started, the newspaper asserted that some of its articles had not been as rigorous as they should have been, and were insufficiently qualified, frequently overly dependent upon information from Iraqi exiles desiring regime change.[180] Reporter Judith Miller retired after criticisms that her reporting of the lead-up to the Iraq War was factually inaccurate and overly favorable to the Bush administration's position, for which The New York Times later apologized.[181][182] One of Miller's prime sources was Ahmed Chalabi, an Iraqi expatriate who returned to Iraq after the U.S. invasion and held a number of governmental positions culminating in acting oil minister and deputy prime minister from May 2005 until May 2006.[183][184]		A 2015 study found that The New York Times fed into an overarching tendency towards national bias. During the Iranian nuclear crisis the newspaper minimized the "negative processes" of the United States while overemphasizing similar processes of Iran. This tendency was shared by other papers such as The Guardian, Tehran Times, and the Fars News Agency, while Xinhua News Agency was found to be more neutral while at the same time mimicking the foreign policy of the Peoples' Republic of China.[185]		A 2003 study in The Harvard International Journal of Press/Politics concluded that The New York Times reporting was more favorable to Israelis than to Palestinians.[186] A 2002 study published in the journal Journalism examined Middle East coverage of the Second Intifada over a one-month period in the Times, Washington Post and Chicago Tribune. The study authors said that the Times was "the most slanted in a pro-Israeli direction" with a bias "reflected ... in its use of headlines, photographs, graphics, sourcing practices and lead paragraphs."[187]		For its coverage of the Israeli–Palestinian conflict, some (such as Ed Koch) have claimed that the paper is pro-Palestinian, while others (such as As`ad AbuKhalil) have insisted that it is pro-Israel.[188][189] The Israel Lobby and U.S. Foreign Policy, by political science professors John Mearsheimer and Stephen Walt, alleges that The New York Times sometimes criticizes Israeli policies but is not even-handed and is generally pro-Israel.[190] On the other hand, the Simon Wiesenthal Center has criticized The New York Times for printing cartoons regarding the Israeli-Palestinian conflict that were claimed to be anti-Semitic.[191]		Israeli Prime Minister Benjamin Netanyahu rejected a proposal to write an article for the paper on grounds of lack of objectivity. A piece in which Thomas Friedman commented that praise awarded to Netanyahu during a speech at congress was "paid for by the Israel lobby" elicited an apology and clarification from its writer.[192]		The New York Times' public editor Clark Hoyt concluded in his January 10, 2009, column, "Though the most vociferous supporters of Israel and the Palestinians do not agree, I think The New York Times, largely barred from the battlefield and reporting amid the chaos of war, has tried its best to do a fair, balanced and complete job — and has largely succeeded."[193]		On November 14, 2001, in The New York Times' 150th anniversary issue, former executive editor Max Frankel wrote that before and during World War II, the NY Times had maintained a consistent policy to minimize reports on the Holocaust in their news pages.[194] Laurel Leff, associate professor of journalism at Northeastern University, concluded that the newspaper had downplayed the Third Reich targeting of Jews for genocide. Her 2005 book Buried by the Times documents the paper's tendency before, during and after World War II to place deep inside its daily editions the news stories about the ongoing persecution and extermination of Jews, while obscuring in those stories the special impact of the Nazis' crimes on Jews in particular. Leff attributes this dearth in part to the complex personal and political views of the newspaper's Jewish publisher, Arthur Hays Sulzberger, concerning Jewishness, antisemitism, and Zionism.[195]		During the war, The New York Times journalist William L. Laurence was "on the payroll of the War Department".[196][197]		The New York Times was criticized for the work of reporter Walter Duranty, who served as its Moscow bureau chief from 1922 through 1936. Duranty wrote a series of stories in 1931 on the Soviet Union and won a Pulitzer Prize for his work at that time; however, he has been criticized for his denial of widespread famine, most particularly the Ukrainian famine in the 1930s.[198][199][200][201] In 2003, after the Pulitzer Board began a renewed inquiry, the Times hired Mark von Hagen, professor of Russian history at Columbia University, to review Duranty's work. Von Hagen found Duranty's reports to be unbalanced and uncritical, and that they far too often gave voice to Stalinist propaganda. In comments to the press he stated, "For the sake of The New York Times' honor, they should take the prize away."[202]		In the mid to late 1950s, "fashion writer[s]... were required to come up every month with articles whose total column-inches reflected the relative advertising strength of every ["department" or "specialty"] store ["assigned" to a writer]... The monitor of all this was... the advertising director [of the NYT]... " However, within this requirement, story ideas may have been the reporters' and editors' own.[203]		In May 2003, The New York Times reporter Jayson Blair was forced to resign from the newspaper after he was caught plagiarizing and fabricating elements of his stories. Some critics contended that African-American Blair's race was a major factor in his hiring and in The New York Times' initial reluctance to fire him.[204]		The newspaper was criticized for largely reporting the prosecutors' version of events in the 2006 Duke lacrosse case.[205][206] Suzanne Smalley of Newsweek criticized the newspaper for its "credulous"[207] coverage of the charges of rape against Duke University lacrosse players. Stuart Taylor, Jr. and KC Johnson, in their book Until Proven Innocent: Political Correctness and the Shameful Injustices of the Duke Lacrosse Rape Case, write: "at the head of the guilt-presuming pack, The New York Times vied in a race to the journalistic bottom with trash-TV talk shows."[206]		In February 2009, a Village Voice music blogger accused the newspaper of using "chintzy, ad-hominem allegations" in an article on British Tamil music artist M.I.A. concerning her activism against the Sinhala-Tamil conflict in Sri Lanka.[208][209] M.I.A. criticized the paper in January 2010 after a travel piece rated post-conflict Sri Lanka the "#1 place to go in 2010".[210][211] In June 2010, The New York Times Magazine published a correction on its cover article of M.I.A., acknowledging that the interview conducted by current W editor and then-Times Magazine contributor Lynn Hirschberg contained a recontextualization of two quotes.[212][213] In response to the piece, M.I.A. broadcast Hirschberg's phone number and secret audio recordings from the interview via her Twitter and website.[214][215]		The New York Times was criticized for the 13-month delay of the December 2005 story revealing the U.S. National Security Agency warrantless surveillance program.[216] Ex-NSA officials blew the whistle on the program to journalists James Risen and Eric Lichtblau, who presented an investigative article to the newspaper in November 2004, weeks before America's presidential election. Contact with former agency officials began the previous summer.[217]		Former The New York Times executive editor Bill Keller decided not to report the piece after being pressured by the Bush administration and being advised not to do so by New York Times Washington bureau chief Philip Taubman. Keller explained the silence's rationale in an interview with the newspaper in 2013, stating "Three years after 9/11, we, as a country, were still under the influence of that trauma, and we, as a newspaper, were not immune".[218]		In 2014, PBS Frontline interviewed Risen and Lichtblau, who said that the newspaper's plan was to not publish the story at all. "The editors were furious at me", Risen said to the program. "They thought I was being insubordinate." Risen wrote a book about the mass surveillance revelations after The New York Times declined the piece's publication, and only released it after Risen told them that he would publish the book. Another reporter told NPR that the newspaper "avoided disaster" by ultimately publishing the story.[219]		In September 2014, around the time when India's Mars Orbiter Mission probe was to go into Mars orbit, the International New York Times published an editorial cartoon by Singapore cartoonist Heng Kim Song depicting a turban-wearing man with a cow knocking at the door of the "Elite Space Club" with members inside reading a newspaper with a headline about India's Mars mission.[220][221]		The cartoon drew criticism, with critics saying that the cartoon was racist.[222][223][224] Times editorial page editor Andrew Rosenthal, apologized, writing in a Facebook post: "A large number of readers have complained about a recent editorial cartoon in The International New York Times, about India's foray into space exploration. The intent of the cartoonist ... was to highlight how space exploration is no longer the exclusive domain of rich, Western countries. Mr. Heng, who is based in Singapore, uses images and text – often in a provocative way – to make observations about international affairs. We apologize to readers who were offended by the choice of images in this cartoon. Mr. Heng was in no way trying to impugn India, its government or its citizens."[225][221]		On June 16, 2015, The New York Times published an article reporting the deaths of six Irish students staying in Berkeley, California when the balcony they were standing on collapsed, the paper's story insinuating that they were to blame for the collapse. The paper stated that the behavior of Irish students coming to the US on J1 visas was an "embarrassment to Ireland".[226] The Irish Taoiseach and former President of Ireland criticized the newspaper for "being insensitive and inaccurate" in its handling of the story.[227]		In May 2015, a New York Times exposé by Sarah Maslin Nir on the working conditions of manicurists in New York City and elsewhere[228] and the health hazards to which they are exposed[229] attracted wide attention, resulting in emergency workplace enforcement actions by New York governor Andrew M. Cuomo.[230] In July 2015, the story's claims of widespread illegally low wages were challenged by former New York Times reporter Richard Bernstein, in the New York Review of Books. Bernstein, whose wife owns two nail salons, asserted that such illegally low wages were inconsistent with his personal experience, and were not evidenced by ads in the Chinese-language papers cited by the story.[231] The New York Times editorial staff subsequently answered Bernstein's criticisms with examples of several published ads and stating that his response was industry advocacy.[232] The independent NYT Public Editor also reported that she had previously corresponded with Bernstein and looked into his complaints, and expressed her belief that the story's reporting was sound.[233]		In September and October 2015, nail salon owners and workers protested at The New York Times offices several times, in response to the story and the ensuing New York State crackdown.[234][235] In October 2015, Reason magazine published a three part re-reporting of the story by Jim Epstein, charging that the series was filled with misquotes and factual errors respecting both its claims of illegally low wages and health hazards. Epstein additionally argued that The New York Times had mistranslated the ads cited in its answer to Bernstein, and that those ads actually validated Bernstein's argument.[236][237][238] In November 2015, The New York Times' public editor concluded that the exposé's "findings, and the language used to express them, should have been dialed back — in some instances substantially" and recommended that "The Times write further follow-up stories, including some that re-examine its original findings and that take on the criticism from salon owners and others — not defensively but with an open mind."[239]		In April 2016, two black female employees in their sixties filed a federal class action lawsuit against The New York Times Company CEO Mark Thompson and chief revenue officer Meredith Levien, claiming age, gender, and racial discrimination. The plaintiffs claim that the Times advertising department favored younger white employees over older black employees in making firing and promotion decisions.[240][241] The Times said that the suit was "entirely without merit" and was "a series of recycled, scurrilous and unjustified attacks."[241]		New York Times public editor (ombudsman) Liz Spayd wrote in 2016 that "Conservatives and even many moderates, see in The Times a blue-state worldview" and accuse it of harboring a liberal bias. Spayd did not analyze the substance of the claim, but did opine that the Times is "part of a fracturing media environment that reflects a fractured country. That in turn leads liberals and conservatives toward separate news sources."[242] Times executive editor Dean Baquet stated that he does not believe coverage has a liberal bias, but that: "We have to be really careful that people feel like they can see themselves in The New York Times. I want us to be perceived as fair and honest to the world, not just a segment of it. It's a really difficult goal. Do we pull it off all the time? No."[242]		Times public editor Arthur Brisbane wrote in 2012: "When The Times covers a national presidential campaign, I have found that the lead editors and reporters are disciplined about enforcing fairness and balance, and usually succeed in doing so. Across the paper's many departments, though, so many share a kind of political and cultural progressivism — for lack of a better term — that this worldview virtually bleeds through the fabric of The Times."[243]		In mid-2004, the newspaper's then public editor Daniel Okrent, wrote an opinion piece in which he said that The New York Times did have a liberal bias in news coverage of certain social issues such as abortion and same-sex marriage. He stated that this bias reflected the paper's cosmopolitanism, which arose naturally from its roots as a hometown paper of New York City.[173] He wrote, "if you're examining the paper's coverage of these subjects from a perspective that is neither urban nor Northeastern nor culturally seen-it-all; if you are among the groups The Times treats as strange objects to be examined on a laboratory slide (devout Catholics, gun owners, Orthodox Jews, Texans); if your value system wouldn't wear well on a composite New York Times journalist, then a walk through this paper can make you feel you're traveling in a strange and forbidding world."[173] Okrent wrote that the Time's Arts & Leisure, Sunday Times Magazine, and Culture coverage trend to the left.[244]		In December 2004, a University of California, Los Angeles study by former fellows of a conservative think tank gave The New York Times a score of 73.7 on a 100-point scale, with 0 being most conservative and 100 being most liberal, making it the second-most liberal major newspaper in the study after The Wall Street Journal (85.1).[245] The validity of the study has been questioned, however. The liberal watchdog group Media Matters for America pointed out potential conflicts of interest with the author's funding, and political scientists, such as Brendan Nyhan, cited flaws in the study's methodology.[246][247]		Donald Trump has frequently criticized the New York Times on his Twitter account before and during his presidency; since November 2015, Trump has referred to the Times as "the failing New York Times" in a series of tweets.[248] Despite Trump's criticism, New York Times editor Mark Thompson noted that the paper had enjoyed soaring digital readership, with the fourth quarter of 2016 seeing the highest number of new digital subscribers to the newspaper since 2011.[249][250][251]		Critic Matt Taibbi accused The New York Times of favoring Hillary Clinton over Bernie Sanders in the paper's news coverage of the 2016 Democratic presidential primaries.[252] Responding to the complaints of many readers, New York Times public editor Margaret Sullivan wrote that, "The Times has not ignored Mr. Sanders's campaign, but it hasn't always taken it very seriously. The tone of some stories is regrettably dismissive, even mocking at times. Some of that is focused on the candidate's age, appearance and style, rather than what he has to say."[253] Times senior editor Carolyn Ryan defended both the volume of New York Times coverage (noting that Sanders had received about the same amount of article coverage as Jeb Bush and Marco Rubio) and its tone.[254]		The Times has developed a national and international "reputation for thoroughness" over time.[255] Among journalists, the paper is held in high regard; a 1999 survey of newspaper editors conducted by the Columbia Journalism Review found that the Times was the "best" American paper, ahead of the Washington Post, Wall Street Journal, and Los Angeles Times.[256] The Times also was ranked #1 in a 2011 "quality" ranking of U.S. newspapers by Daniel de Vise of the Washington Post; the objective ranking took into account the number of recent Pulitzer Prizes won, circulation, and perceived Web site quality.[256] A 2012 report in WNYC called the Times "the most respected newspaper in the world."[257]		Nevertheless, like many other U.S. media sources, the Times had suffered from a decline in public perceptions of credibility in the U.S. from 2004 to 2012.[258] A Pew Research Center survey in 2012 asked respondents about their views on credibility of various news organizations. Among respondents who gave a rating, 49% said that they believed "all or most" of the Times's reporting, while 50% disagreed. A large percentage (19%) of respondents were unable to rate believability. The Times's score was comparable to that of USA Today.[258] Media analyst Brooke Gladstone of WNYC's On the Media writing for the New York Times says that the decline in U.S. public trust of the mass media can be explained (1) by the rise of the polarized Internet-driven news; (2) by a decline in trust in U.S. institutions more generally; and (3) by the fact that "Americans say they want accuracy and impartiality, but the polls suggest that, actually, most of us are seeking affirmation."[259]		The TimesMachine is a web-based archive of scanned issues of The New York Times from 1851 through 2002.[260]		Unlike The New York Times online archive, the TimesMachine presents scanned images of the actual newspaper. All non-advertising content can be displayed on a per-story basis in a separate PDF display page and saved for future reference.[261]		The archive is available to New York Times subscribers, home delivery and/or digital.[260]		The position of public editor was established in 2003 to "investigate matters of journalistic integrity"; each public editor was to serve a two-year term.[262] The post "was established to receive reader complaints and question Times journalists on how they make decisions."[263] The impetus for the creation of the public editor position was the Jayson Blair affair. Public editors were: Daniel Okrent (2003–2005), Byron Calame (2005–2007), Clark Hoyt (2007–2010) (served an extra year), Arthur S. Brisbane (2010–2012), Margaret Sullivan (2012–2016) (served a four-year term), and Elizabeth Spayd (2016–2017). In 2017, the Times eliminated the position of public editor.[263][264]		Official New York Times web sites		Unofficial New York Times related web sites		1 2 3 4 5 6 7		NBC Wall Street Journal Agence France-Presse MSNBC Bloomberg BNA Washington Examiner Talk Media News/Univision		Fox News CBS Radio AP Radio Foreign Pool Time Yahoo! News Dallas Morning News		CBS News Bloomberg McClatchy Washington Times SiriusXM Salem Radio Globe/Roll Call		AP NPR AURN The Hill Regionals Newsmax CBN		ABC News Washington Post Politico Fox News Radio CSM/NY Post Daily Mail BBC/OAN		Reuters NY Times Chicago Tribune VOA RealClearPolitics HuffPost/NY Daily News BuzzFeed/Daily Beast		CNN USA Today ABC Radio National Journal Al Jazeera/PBS Westwood One Financial Times/Guardian		* From 1985 to 1990: Pulitzer Prize for General News Reporting; From 1991 to 1997: Pulitzer Prize for Spot News Reporting; From 1998 to present: Pulitzer Prize for Breaking News Reporting		
Joseph John Campbell (March 26, 1904 – October 30, 1987) was an American mythologist, writer, and lecturer, best known for his work in comparative mythology and comparative religion. His work covers many aspects of the human experience. Campbell's magnum opus is his book The Hero with a Thousand Faces (1949), in which he discusses his theory of the journey of the archetypal hero found in world mythologies. Since the book's publication, Campbell's theory has been consciously applied by a wide variety of modern writers and artists. His philosophy has been summarized by his own often repeated phrase: "Follow your bliss."[1]						Joseph Campbell was born in White Plains, New York,[2] the son of Josephine (née Lynch) and Charles William Campbell.[3] He was from an upper-middle-class Irish Catholic family. During his childhood, he moved with his family to nearby New Rochelle, New York. In 1919 a fire destroyed the family home in New Rochelle, killing his grandmother.[4]		In 1921 Campbell graduated from the Canterbury School in New Milford, Connecticut.		While at Dartmouth College he studied biology and mathematics, but decided that he preferred the humanities. He transferred to Columbia University, where he received a BA in English literature in 1925 and an MA in Medieval literature in 1927. At Dartmouth he had joined Delta Tau Delta. An accomplished athlete, he received awards in track and field events, and, for a time, was among the fastest half-mile runners in the world.[5]		In 1924 Campbell traveled to Europe with his family. On the ship during his return trip he encountered the messiah elect of the Theosophical Society, Jiddu Krishnamurti; they discussed Indian philosophy, sparking in Campbell an interest in Hindu and Indian thought.[6][7]		In 1927 Campbell received a fellowship from Columbia University to study in Europe. Campbell studied Old French, Provençal and Sanskrit at the University of Paris in France and the University of Munich in Germany. He learned to read and speak French and German.[8]		On his return to Columbia University in 1929, Campbell expressed a desire to pursue the study of Sanskrit and Modern Art in addition to Medieval literature. Lacking faculty approval, Campbell withdrew from graduate studies. Later in life he said while laughing but not in jest that it is a sign of incompetence to have a PhD in the liberal arts, the discipline covering his work.[9]		With the arrival of the Great Depression a few weeks later, Campbell spent the next five years (1929–34) living in a rented shack on some land in Woodstock, New York.[10] There, he contemplated the next course of his life[11] while engaged in intensive and rigorous independent study. He later said that he "would divide the day into four four-hour periods, of which I would be reading in three of the four-hour periods, and free one of them ... I would get nine hours of sheer reading done a day. And this went on for five years straight."[12]		Campbell traveled to California for a year (1931–32), continuing his independent studies and becoming close friends with the budding writer John Steinbeck and his wife Carol. Campbell was introduced to the Steinbecks by author and early nutritionist Adelle Davis whom he met and developed a close relationship with on a cruise to the Caribbean with his father in December 1929.[13][14] On the Monterey Peninsula, Campbell, like John Steinbeck, fell under the spell of the marine biologist Ed Ricketts (the model for "Doc" in Steinbeck's novel Cannery Row as well as central characters in several other novels).[15] Campbell lived for a while next door to Ricketts, participated in professional and social activities at his neighbor's, and accompanied him, along with Xenia and Sasha Kashevaroff, on a 1932 journey to Juneau, Alaska on the Grampus.[16] Campbell began writing a novel centered on Ricketts as a hero but, unlike Steinbeck, did not complete his book.[17]		Bruce Robison writes that "Campbell would refer to those days as a time when everything in his life was taking shape.... Campbell, the great chronicler of the 'hero's journey' in mythology, recognized patterns that paralleled his own thinking in one of Ricketts's unpublished philosophical essays. Echoes of Carl Jung, Robinson Jeffers and James Joyce can be found in the work of Steinbeck and Ricketts as well as Campbell."[18]		Campbell continued his independent reading while teaching for a year in 1933 at the Canterbury School, during which time he also attempted to publish works of fiction. While teaching at the Canterbury School, Campbell sold his first short story Strictly Platonic to Liberty magazine.[19][20]		In 1934 Campbell accepted a position as professor at Sarah Lawrence College.		In 1938 Campbell married one of his former students, the dancer-choreographer Jean Erdman. For most of their 49 years of marriage they shared a two-room apartment in Greenwich Village in New York City. In the 1980s they also purchased an apartment in Honolulu and divided their time between the two cities. They did not have any children.		Early in World War II, Campbell attended a lecture by the Indologist Heinrich Zimmer; the two men became good friends. After Zimmer's death, Campbell was given the task of editing and posthumously publishing Zimmer's papers, which he would do over the following decade.		In 1955–56, as the last volume of Zimmer's posthuma (The Art of Indian Asia, its Mythology and Transformations) was finally about to be published, Campbell took a sabbatical from Sarah Lawrence College and traveled, for the first time, to Asia. He spent six months in southern Asia (mostly India) and another six in East Asia (mostly Japan).		This year had a profound influence on his thinking about Asian religion and myth, and also on the necessity for teaching comparative mythology to a larger, non-academic audience.[21]		In 1972 Campbell retired from Sarah Lawrence College, after having taught there for 38 years.		Campbell died at his home in Honolulu, Hawaii, on 30 October 1987, from complications of esophageal cancer.[22][23] Before his death he had completed filming the series of interviews with Bill Moyers that aired the following spring as The Power of Myth.		Campbell often referred to the work of modern writers James Joyce and Thomas Mann in his lectures and writings, as well as to the art of Pablo Picasso. He was introduced to their work during his stay as a graduate student in Paris. Campbell eventually corresponded with Mann.[24]		The works of Arthur Schopenhauer and Friedrich Nietzsche had a profound effect on Campbell's thinking; he quoted their writing frequently.		The "follow your bliss" philosophy attributed to Campbell following the original broadcast of The Power of Myth (see below) derives from the Hindu Upanishads; however, Campbell was possibly also influenced by the 1922 Sinclair Lewis novel Babbitt. In The Power of Myth, Campbell quotes from the novel:		The anthropologist Leo Frobenius and his disciple Adolf Ellegard Jensen were important to Campbell's view of cultural history. Campbell was also influenced by the psychological work of Abraham Maslow and Stanislav Grof.		Campbell's ideas regarding myth and its relation to the human psyche are dependent in part on the pioneering work of Sigmund Freud, but in particular on the work of Carl Jung, whose studies of human psychology greatly influenced Campbell. Campbell's conception of myth is closely related to the Jungian method of dream interpretation, which is heavily reliant on symbolic interpretation.		Jung's insights into archetypes were heavily influenced by the Bardo Thodol (also known as The Tibetan Book of the Dead). In his book The Mythic Image, Campbell quotes Jung's statement about the Bardo Thodol, that it "belongs to that class of writings which not only are of interest to specialists in Mahayana Buddhism, but also, because of their deep humanity and still deeper insight into the secrets of the human psyche, make an especial appeal to the layman seeking to broaden his knowledge of life... For years, ever since it was first published, the Bardo Thodol has been my constant companion, and to it I owe not only many stimulating ideas and discoveries, but also many fundamental insights."[26]		Campbell's concept of monomyth (one myth) refers to the theory that sees all mythic narratives as variations of a single great story. The theory is based on the observation that a common pattern exists beneath the narrative elements of most great myths, regardless of their origin or time of creation. Campbell often referred to the ideas of Adolf Bastian and his distinction between what he called "folk" and "elementary" ideas, the latter referring to the prime matter of monomyth while the former to the multitude of local forms the myth takes in order to remain an up-to-date carrier of sacred meanings. The central pattern most studied by Campbell is often referred to as the hero's journey and was first described in The Hero with a Thousand Faces (1949).[27] An enthusiast of novelist James Joyce,[28] Campbell borrowed the term "monomyth" from Joyce's Finnegans Wake.[29] Campbell also made heavy use of Carl Jung's theories on the structure of the human psyche, and he often used terms such as "anima/animus" and "ego consciousness".		As a strong believer in the psychic unity of mankind and its poetic expression through mythology, Campbell made use of the concept to express the idea that the whole of the human race can be seen as engaged in the effort of making the world "transparent to transcendence" by showing that underneath the world of phenomena lies an eternal source which is constantly pouring its energies into this world of time, suffering, and ultimately death. To achieve this task one needs to speak about things that existed before and beyond words, a seemingly impossible task, the solution to which lies in the metaphors found in myths. These metaphors are statements that point beyond themselves into the transcendent. The Hero's Journey was the story of the man or woman who, through great suffering, reached an experience of the eternal source and returned with gifts powerful enough to set their society free.		As this story spread through space and evolved through time, it was broken down into various local forms (masks), depending on the social structures and environmental pressures that existed for the culture that interpreted it. The basic structure, however, has remained relatively unchanged and can be classified using the various stages of a hero's adventure through the story, stages such as the Call to Adventure, Receiving Supernatural Aid, Meeting with the Goddess/Atonement with the Father and Return. These stages, as well as the symbols one encounters throughout the story, provide the necessary metaphors to express the spiritual truths the story is trying to convey. Metaphor for Campbell, in contrast with comparisons which make use of the word like, pretend to a literal interpretation of what they are referring to, as in the sentence "Jesus is the Son of God" rather than "the relationship of man to God is like that of a son to a father".[30]		In the 2000 documentary Joseph Campbell: A Hero's Journey, he explains God in terms of a metaphor:		God is a metaphor for a mystery that absolutely transcends all human categories of thought, even the categories of being and non-being. Those are categories of thought. I mean it's as simple as that. So it depends on how much you want to think about it. Whether it's doing you any good. Whether it is putting you in touch with the mystery that's the ground of your own being. If it isn't, well, it's a lie. So half the people in the world are religious people who think that their metaphors are facts. Those are what we call theists. The other half are people who know that the metaphors are not facts. And so, they're lies. Those are the atheists.[31]		Some scholars have disagreed with the concept of the "monomyth" because of its oversimplification of different cultures. According to Robert Ellwood, "A tendency to think in generic terms of people, races... is undoubtedly the profoundest flaw in mythological thinking."[32]		Campbell often described mythology as having a fourfold function within human society. These appear at the end of his work The Masks of God: Creative Mythology, as well as various lectures.[33]		Campbell's view of mythology was by no means static and his books describe in detail how mythologies evolved through time, reflecting the realities in which each society had to adjust.[36] Various stages of cultural development have different yet identifiable mythological systems. In brief these are:		In 1991, Campbell's widow, choreographer Jean Erdman, worked with Campbell's longtime friend and editor, Robert Walter, to create the Joseph Campbell Foundation.		Initiatives undertaken by the JCF include: The Collected Works of Joseph Campbell, a series of books and recordings that aims to pull together Campbell's myriad-minded work; the Erdman Campbell Award; the Mythological RoundTables, a network of local groups around the globe that explore the subjects of comparative mythology, psychology, religion and culture; and the collection of Campbell's library and papers housed at the OPUS Archives and Research Center (see below).[43]		George Lucas was the first Hollywood filmmaker to credit Campbell's influence. Lucas stated, following the release of the first Star Wars film in 1977, that its story was shaped, in part, by ideas described in The Hero with a Thousand Faces and other works of Campbell's. The linkage between Star Wars and Campbell was further reinforced when later reprints of Campbell's book used the image of Mark Hamill as Luke Skywalker on the cover.[44] Lucas discusses this influence at great length in the authorized biography of Joseph Campbell, A Fire in the Mind:		I [Lucas] came to the conclusion after American Graffiti that what's valuable for me is to set standards, not to show people the world the way it is...around the period of this realization...it came to me that there really was no modern use of mythology...The Western was possibly the last generically American fairy tale, telling us about our values. And once the Western disappeared, nothing has ever taken its place. In literature we were going off into science fiction...so that's when I started doing more strenuous research on fairy tales, folklore, and mythology, and I started reading Joe's books. Before that I hadn't read any of Joe's books...It was very eerie because in reading The Hero with a Thousand Faces I began to realize that my first draft of Star Wars was following classic motifs...so I modified my next draft [of Star Wars] according to what I'd been learning about classical motifs and made it a little bit more consistent...I went on to read 'The Masks of God' and many other books.[45]		It was not until after the completion of the original Star Wars trilogy in 1983, however, that Lucas met Campbell or heard any of his lectures.[46] The 1988 documentary The Power of Myth was filmed at Lucas' Skywalker Ranch. During his interviews with Bill Moyers, Campbell discusses the way in which Lucas used The Hero's Journey in the Star Wars films (IV, V, and VI) to re-invent the mythology for the contemporary viewer. Moyers and Lucas filmed an interview 12 years later in 1999 called the Mythology of Star Wars with George Lucas & Bill Moyers to further discuss the impact of Campbell's work on Lucas' films.[47] In addition, the National Air and Space Museum of the Smithsonian Institution sponsored an exhibit during the late 1990s called Star Wars: The Magic of Myth, which discussed the ways in which Campbell's work shaped the Star Wars films.[48]		Christopher Vogler, a Hollywood screenwriter, created a seven-page company memo based on Campbell's work, A Practical Guide to The Hero With a Thousand Faces,[49] which led to the development of Disney's 1994 film The Lion King.		Many filmmakers of the late twentieth and early twenty-first centuries have acknowledged the influence of Campbell's work on their own craft. Among films that many viewers have recognized as closely following the pattern of the monomyth are The Matrix series, the Batman series and the Indiana Jones series.[50]		The creator of the TV show Community, Dan Harmon, often references Campbell as a major influence. According to him, his process of writing with his "Story Circle", which he uses to break every single story he writes, is a formulation of Campbell's work.[51]		After the explosion of popularity brought on by the Star Wars films and The Power of Myth, creative artists in many media recognized the potential to use Campbell's theories to try to unlock human responses to narrative patterns. Novelists,[52] songwriters,[53][54] video game designers[55] have studied Campbell's work in order to better understand mythology — in particular, the monomyth — and its impact.[citation needed]		Novelist Richard Adams acknowledges a debt to Campbell's work, and specifically to the concept of the monomyth.[56] In his best known work, Watership Down, Adams uses extracts from The Hero with a Thousand Faces as chapter epigrams.[57]		Dan Brown mentioned in a New York Times interview that Joseph Campbell's works, particularly The Power of Myth and The Hero with a Thousand Faces, inspired him to create the character of Robert Langdon.[58]		One of Campbell's most identifiable, most quoted and arguably most misunderstood sayings was his admonition to "follow your bliss". He derived this idea from the Upanishads:		Now, I came to this idea of bliss because in Sanskrit, which is the great spiritual language of the world, there are three terms that represent the brink, the jumping-off place to the ocean of transcendence: Sat-Chit-Ananda. The word "Sat" means being. "Chit" means consciousness. "Ananda" means bliss or rapture. I thought, "I don't know whether my consciousness is proper consciousness or not; I don't know whether what I know of my being is my proper being or not; but I do know where my rapture is. So let me hang on to rapture, and that will bring me both my consciousness and my being." I think it worked.[59]		He saw this not merely as a mantra, but as a helpful guide to the individual along the hero journey that each of us walks through life:		If you follow your bliss, you put yourself on a kind of track that has been there all the while, waiting for you, and the life that you ought to be living is the one you are living. Wherever you are—if you are following your bliss, you are enjoying that refreshment, that life within you, all the time.[60]		Campbell began sharing this idea with students during his lectures in the 1970s. By the time that The Power of Myth was aired in 1988, six months following Campbell's death, "Follow your bliss" was a philosophy that resonated deeply with the American public—both religious and secular.[61]		During his later years, when some students took him to be encouraging hedonism, Campbell is reported to have grumbled, "I should have said, 'Follow your blisters.'"[62]		Campbell has been accused of antisemitism by some authors. Brendan Gill, in an article published in The New York Review of Books in 1989, accused Campbell of both antisemitism and prejudice against blacks. Gill's article resulted in a series of letters to the editor, some supporting the charge of antisemitism or accusing Campbell of having various other right-wing biases, others defending him. Robert Ellwood writes that Gill relied on "scraps of evidence, largely anecdotal" to support his charges against Campbell.[63] The former psychoanalyst Jeffrey Moussaieff Masson accused Campbell of "hidden anti-Semitism" and "fascination with conservative, semifascistic views".[64]		Campbell's scholarship and understanding of Sanskrit has also been questioned. Masson, a Sanskrit scholar, said that he once met Campbell, and that the two "hated each other at sight". Masson commented that, "When I met Campbell at a public gathering, he was quoting Sanskrit verses. He had no clue as to what he was talking about; he had the most superficial knowledge of India but he could use it for his own aggrandizement. I remember thinking: this man is corrupt. I know that he was simply lying about his understanding".[65] According to Richard Buchen, the editor of the third edition of The Hero With a Thousand Faces, Campbell could not translate Sanskrit well.[66]		Ellwood observes that The Masks of God series "impressed literate laity more than specialists"; he quotes Stephen P. Dunn as remarking that in Occidental Mythology Campbell "writes in a curiously archaic style – full of rhetorical questions, exclamations of wonder and delight, and expostulations directed at the reader, or perhaps at the author's other self – which is charming about a third of the time and rather annoying the rest." Ellwood notes that "Campbell was not really a social scientist, and those in the latter camp could tell" and records a concern about Campbell's "oversimpification of historical matters and tendency to make myth mean whatever he wanted it to mean".[63] The critic Camille Paglia, writing in Sexual Personae (1990), expressed disagreement with Campbell's "negative critique of fifth-century Athens" in Occidental Mythology, arguing that Campbell missed the "visionary and exalted" androgyny in Greek statues of nude boys.[67] Paglia has written that while Campbell is "a seminal figure for many American feminists", she loathes him for his "mawkishness and bad research." Paglia has called Campbell "mushy" and a "false teacher",[68] and described his work as a "fanciful, showy mishmash".[69]		The religious studies scholar Russell T. McCutcheon characterised Campbell's work as "spiritual and psychological legitimation" for "Reaganomics".[70]		The first published work that bore Campbell's name was Where the Two Came to Their Father (1943), a Navajo ceremony that was performed by singer (medicine man) Jeff King and recorded by artist and ethnologist Maud Oakes, recounting the story of two young heroes who go to the hogan of their father, the Sun, and return with the power to destroy the monsters that are plaguing their people. Campbell provided a commentary. He would use this tale through the rest of his career to illustrate both the universal symbols and structures of human myths and the particulars ("folk ideas") of Native American stories.		As noted above, James Joyce was an important influence on Campbell. Campbell's first important book (with Henry Morton Robinson), A Skeleton Key to Finnegans Wake (1944), is a critical analysis of Joyce's final text Finnegans Wake. In addition, Campbell's seminal work, The Hero with a Thousand Faces (1949), discusses what Campbell called the monomyth – the cycle of the journey of the hero – a term that he borrowed directly from Joyce's Finnegans Wake.[71]		From his days in college through the 1940s, Joseph Campbell turned his hand to writing fiction.[72] In many of his later stories (published in the posthumous collection Mythic Imagination) he began to explore the mythological themes that he was discussing in his Sarah Lawrence classes. These ideas turned him eventually from fiction to non-fiction.		Originally titled How to Read a Myth, and based on the introductory class on mythology that he had been teaching at Sarah Lawrence College, The Hero with a Thousand Faces was published in 1949 as Campbell's first foray as a solo author; it established his name outside of scholarly circles and remains, arguably, his most influential work to this day. The book argues that hero stories such as Krishna, Buddha, Apollonius of Tyana, and Jesus all share a similar mythological basis.[73] Not only did it introduce the concept of the hero's journey to popular thinking, but it also began to popularize the very idea of comparative mythology itself—the study of the human impulse to create stories and images that, though they are clothed in the motifs of a particular time and place, draw nonetheless on universal, eternal themes. Campbell asserted:		Wherever the poetry of myth is interpreted as biography, history, or science, it is killed. The living images become only remote facts of a distant time or sky. Furthermore, it is never difficult to demonstrate that as science and history, mythology is absurd. When a civilization begins to reinterpret its mythology in this way, the life goes out of it, temples become museums, and the link between the two perspectives becomes dissolved.[74]		Published between 1959 and 1968, Campbell's four-volume work The Masks of God covers mythology from around the world, from ancient to modern. Where The Hero with a Thousand Faces focused on the commonality of mythology (the "elementary ideas"), the Masks of God books focus upon historical and cultural variations the monomyth takes on (the "folk ideas"). In other words, where The Hero with a Thousand Faces draws perhaps more from psychology, the Masks of God books draw more from anthropology and history. The four volumes of Masks of God are as follows: Primitive Mythology, Oriental Mythology, Occidental Mythology, and Creative Mythology.		The book is quoted by proponents of the Christ myth theory. Campbell writes, "It is clear that, whether accurate or not as to biographical detail, the moving legend of the Crucified and Risen Christ was fit to bring a new warmth, immediacy, and humanity, to the old motifs of the beloved Tammuz, Adonis, and Osiris cycles."[75]		At the time of his death, Campbell was in the midst of working upon a large-format, lavishly illustrated series entitled Historical Atlas of World Mythology. This series was to build on Campbell's idea, first presented in The Hero with a Thousand Faces, that myth evolves over time through four stages:		Only the first volume was completed at the time of Campbell's death. Campbell's editor Robert Walter completed the publication of the first three of five parts of the second volume after Campbell's death. The works are now out of print. As of 2014[update], Joseph Campbell Foundation is currently undertaking to create a new, ebook edition.[76]		Campbell's widest popular recognition followed his collaboration with Bill Moyers on the PBS series The Power of Myth, which was first broadcast in 1988, the year following Campbell's death. The series discusses mythological, religious, and psychological archetypes. A book, The Power of Myth, containing expanded transcripts of their conversations, was released shortly after the original broadcast.		The Collected Works of Joseph Campbell series is a project initiated by the Joseph Campbell Foundation to release new, authoritative editions of Campbell's published and unpublished writing, as well as audio and video recordings of his lectures.[77] Working with New World Library and Acorn Media UK, as well as publishing audio recordings and ebooks under its own banner, as of 2014[update] the project has produced over seventy-five titles. The series's executive editor is Robert Walter, and the managing editor is David Kudler.		Books		Articles		Books		
A knight-errant[1] (or knight errant[2] and white knight) is a figure of medieval chivalric romance literature. The adjective errant (meaning "wandering, roving") indicates how the knight-errant would wander the land in search of adventures to prove his chivalric virtues, either in knightly duels (pas d'armes) or in some other pursuit of courtly love.		In medieval Europe, knight-errantry existed only in literature, although many fictional works from this time period present themselves as historical non-fiction.[3] The handful of knights-errant that existed were well-to-do young men inspired to enact what they had read about in romances.[4][5]		The template of the knight-errant are the heroes of the Round Table of the Arthurian cycle such as Gawain, Lancelot and Percival. The quest par excellence in pursuit of which these knights wander the lands is that of the Holy Grail, such as in Perceval, the Story of the Grail written by Chrétien de Troyes in the 1180s.		Although the character is part of the romance genre as it developed during the late 12th century, the term "knight-errant" itself is younger, for the first time recorded (as knygt erraunt) in the 14th-century poem Sir Gawain and the Green Knight.[6] Knight-errantry tales remain popular with courtly audiences throughout the Late Middle Ages. They are written in Middle French, in Middle English and in Middle German. In the 16th century, the genre becomes highly popular in the Iberian Peninsula, Amadis de Gaula was one of the most successful knight-errantry tales of this period. In Don Quixote (1605), Cervantes burlesqued the romances and their popularity. Tales of knight-errantry then fell out of fashion for two centuries, until they re-emerged in the form of the historical novel in Romanticism.						A knight-errant typically performed all his deeds in the name of a lady, and invoked her name before performing an exploit.[citation needed] In more sublimated forms of knight-errantry, pure moralist idealism rather than romantic inspiration motivated the knight-errant (as in the case of Sir Galahad). Such a knight might well be outside the structure of feudalism, wandering solely to perform noble exploits (and perhaps to find a lord to give his service to), but might also be in service to a king or lord, traveling either in pursuit of a specific duty that his overlord charged him with, or to put down evildoers in general. This quest sends a knight on adventures much like the ones of a knight in search of them, as he happens on the same marvels. In The Faerie Queene, St. George is sent to rescue Una's parents' kingdom from a dragon, and Guyon has no such quest, but both knights encounter perils and adventures.		In the romances, his adventures frequently included greater foes than other knights, including giants, enchantresses, or dragons. They may also gain help that is out of ordinary. Sir Ywain assisted a lion against a serpent, and was thereafter accompanied by it, becoming the Knight of the Lion. Other knights-errant have been assisted by wild men of the woods, as in Valentine and Orson, or, like Guillaume de Palerme, by wolves that were, in fact, enchanted princes.		Don Quixote is an early 17th-century parody of the genre, in reaction to the extreme popularity which late medieval romances such as Amadis de Gaula came to enjoy in the Iberian Peninsula in the 16th century.		A depiction of knight-errantry in the modern historical novel is found e.g. in Sir Nigel by Arthur Conan Doyle (1906).		The knight-errant stock character became the trope of the "knight in shining armour" in depiction of the Middle Ages in popular culture, and the term came to be used also outside of medieval drama, as in e.g. The Dark Knight as a title of Batman.		Lee Child has said Jack Reacher is a knight-errant.		In the epic fantasy series A Song of Ice and Fire, there is a class of knights referred to as Hedge Knights. A Hedge Knight is a wandering knight without a master, many are quite poor. Hedge knights travel the length and breadth of Westeros looking for gainful employment, and their name comes from the propensity to sleep out in the open air or in forests when they cannot afford lodging.		East Slavic bylina (epic poetry) feature bogatyrs, knights-errant who served as protectors of their homeland, and occasionally as adventurers. Some of them are presumed to be historical figures, while others are fictional and possibly descend from Slavic mythology. Most tales about bogatyrs revolve around the court of Vladimir I of Kiev. Three popular bogatyrs—Ilya Muromets, Dobrynya Nikitich and Alyosha Popovich (famously painted by Victor Vasnetsov)—are said to have served him.		Youxia, Chinese knights-errant, traveled solo protecting common folk from oppressive regimes. Unlike their European counterpart, they did not come from any particular social caste and were anything from soldiers to poets. There is even a popular literary tradition that arose during the Tang Dynasty which centered on Negrito-slaves who used supernatural physical abilities to save kidnapped damsels in distress and to swim to the bottom of raging rivers to retrieve treasures for their Feudal Lords (see Kunlun Nu).[7][8] A youxia who excels or is renowned for martial prowess or skills is usually called wuxia.		In Japan, the expression Musha shugyō described a Samurai who wanted to test his abilities in real life conditions would travel the land and engage in duels along the way.		
Parachuting, or skydiving, is a method of transiting from a high point to Earth with the aid of gravity, involving the control of speed during the descent with the use of a parachute. It may involve more or less free-falling which is a period during the parachute has not been deployed and the body gradually accelerates to terminal velocity.		The first parachute jump in history was made by Andre-Jacques Garnerin, the inventor of the parachute, on October 22, 1797. Garnerin tested his contraption by leaping from a hydrogen balloon 3,200 feet above Paris. Garnerin's parachute bore little resemblance to today's parachutes, however, as it was not packed into any sort of container and did not feature a ripcord.[1] The first intentional freefall jump with a ripcord-operated deployment was not made until over a century later by Leslie Irvin in 1919.[2] While Georgia Broadwick made an earlier freefall in 1914 when her static line became entangled with her jump aircraft's tail assembly, her freefall descent was not planned. Broadwick cut her static line and deployed her parachute manually, only as a means of freeing herself from the aircraft to which she had become entangled.[3]		The military developed parachuting technology as a way to save aircrews from emergencies aboard balloons and aircraft in flight, and later as a way of delivering soldiers to the battlefield. Early competitions date back to the 1930s, and it became an international sport in 1952.[how?]						Parachuting is performed as a recreational activity and a competitive sport, widely considered an extreme sport due to the risks involved. Modern militaries utilize parachuting for the deployment of airborne forces and supplies, and special operations forces commonly employ parachuting, especially free-fall parachuting, as a method of insertion. Occasionally forest firefighters, known as "smokejumpers" in the United States, use parachuting as a means of rapidly inserting themselves near forest fires in especially remote or otherwise inaccessible areas.		Manually exiting an aircraft and parachuting to safety has been widely used by aviators (especially military aviators and aircrew), and passengers to escape an aircraft that could not otherwise land safely. While this method of escape is relatively rare in modern times, it was commonly used in World War I by military aviators, and utilized extensively throughout the air wars of World War II. In modern times, the most common means of escape from an aircraft in distress is via an ejection seat. Said system is usually operated by the pilot, aircrew member, or passenger, by engaging an activation device manually. In most designs, this will lead to the seat being propelled out of and away from the aircraft carrying the occupant with it, by means of either an explosive charge or a rocket propulsion system. Once clear of the aircraft, the ejection seat will deploy a parachute, although some older models entrusted this step to manual activation by the seat's occupant.		Despite the perception of danger, fatalities are rare. About 21 skydivers are confirmed killed each year in the US, roughly one death for every 150,000 jumps (about 0.0007%).[4][5]		In the US and in most of the western world, skydivers are required to carry two parachutes. The reserve parachute must be periodically inspected and re-packed (whether used or not) by a certified parachute rigger (in the US, an FAA certificated parachute rigger). Many skydivers use an automatic activation device (AAD) that opens the reserve parachute at a pre-determined altitude if it detects that the skydiver is still in free fall. Depending on the country, AADs are often mandatory for new jumpers, and/or required for all jumpers regardless of their experience level.[6] Most skydivers wear a visual altimeter, and an increasing number also use audible altimeters fitted to their helmets.		Injuries and fatalities occurring under a fully functional parachute usually happen because the skydiver performed unsafe maneuvers or made an error in judgment while flying their canopy, typically resulting in a high-speed impact with the ground or other hazards on the ground.[7] One of the most common sources of injury is a low turn under a high-performance canopy and while swooping. Swooping is the advanced discipline of gliding at high-speed parallel to the ground during landing.		Changing wind conditions are another risk factor. In conditions of strong winds, and turbulence during hot days the parachutist can be caught in downdrafts close to the ground. Shifting winds can cause a crosswind or downwind landing which have a higher potential for injury due to the wind speed adding to the landing speed.		Another risk factor is that of "canopy collisions", or collisions between two or more skydivers under fully inflated parachutes. Canopy collisions can cause the jumpers' inflated parachutes to entangle with each other, often resulting in a sudden collapse (deflation) of one or more of the involved parachutes. When this occurs, the jumpers often must quickly perform emergency procedures (if there is sufficient altitude to do so) to "cut-away" (jettison) from their main canopies and deploy their reserve canopies. Canopy collisions are particularly dangerous when occurring at altitudes too low to allow the jumpers adequate time to safely jettison their main parachutes and fully deploy their reserve parachutes.		Equipment failure rarely causes fatalities and injuries. Approximately one in 750 deployments of a main parachute result in a malfunction.[8] Ram-air parachutes typically spin uncontrollably when malfunctioning, and must be jettisoned before deploying the reserve parachute. Reserve parachutes are packed and deployed differently; they are also designed more conservatively and built and tested to more exacting standards so they are more reliable than main parachutes, but the real safety advantage comes from the probability of an unlikely main malfunction multiplied by the even less likely probability of a reserve malfunction. This yields an even smaller probability of a double malfunction although the possibility of a main malfunction that cannot be cutaway causing a reserve malfunction is a very real risk.		Parachuting disciplines such as BASE jumping or those that involve equipment such as wing suit flying and sky surfing have a higher risk factor due to the lower mobility of the jumper and the greater risk of entanglement. For this reason,[tone] these disciplines are generally practiced by experienced jumpers.[citation needed]		Depictions in commercial films – notably Hollywood action movies[citation needed] – usually overstate the dangers of the sport.[tone] Often, the characters in such films are shown performing feats that are physically impossible without special effects assistance. In other cases, their practices would cause them to be grounded or shunned at any safety-conscious drop zone or club. USPA member drop zones in the US and Canada are required to have an experienced jumper act as a "safety officer" (in Canada DSO – Drop Zone Safety Officer; in the U.S. S&TA – Safety and Training Advisor) who is responsible for dealing with jumpers who violate rules, regulations, or otherwise act in a fashion deemed unsafe by the appointed individual.		In many countries, either the local regulations or the liability-conscious prudence of the drop zone owners require that parachutists must have attained the age of majority before engaging in the sport.		The first skydive performed without a parachute was by stuntman Gary Connery on 23 May 2012 at 732 m.[9]		Due to the hazardous nature of skydiving, the greatest of precautions are taken to avoid parachuting injuries and death. For first time solo-parachutists, this includes anywhere from 4 to 8 hours of ground instruction.[10] Since the majority of parachute injuries occur upon landing (approximately 85%),[11] the greatest emphasis within ground training is usually on the proper parachute landing fall (PLF), which seeks to orient the body as to evenly disperse the impact through flexion of several large, insulating muscles (such as the medial gastrocnemius, tibialis anterior, rectus femoris, vastus medialis, biceps femoris, and semitendinosus ),[12] as opposed to individual bones, tendons, and ligaments which break and tear more easily.		Parachutists, especially those flying smaller sport canopies, often land with dangerous amounts of kinetic energy, and for this reason, improper landings are the cause of more than 30% of all skydiving related injuries and deaths.[11] Often, injuries sustained during parachute landing are caused when a single outstretched limb, such as a hand or foot, is extended separately from the rest of the body, causing it to sustain forces disproportional to the support structures within. This tendency is displayed in the accompanying chart, which shows the significantly higher proportion of wrist and ankle injuries among the 186 injured in a 110,000 parachute jump study.		Due to the possibility of fractures (commonly occurring on the tibia and the ankle mortise), it is recommended that parachutists wear supportive footwear.[11] Supportive footwear prevents inward and outward ankle rolling, allowing the PLF to safely transfer impact energy through the true ankle joint, and dissipate it via the medial gastrocnemius and tibialis anterior muscles.		Parachuting in poor weather, especially with thunderstorms, high winds, and dust devils can be a dangerous activity. Reputable drop zones will suspend normal operations during inclement weather. In the United States, the USPA's Basic Safety Requirements prohibit solo student skydivers from jumping in winds exceeding 14 mph while using ram-air equipment. However, maximum ground winds are unlimited for licensed skydivers.[13]		As parachuting is an aviation activity under the visual flight rules,[14] it is generally illegal to jump in or through clouds, according to the relevant rules governing the airspace, such as FAR105[15] in the US or Faldskærmsbestemmelser (Parachuting Ordinances)[16] in Denmark. Jumpers and pilots of the dropping aircraft similarly bear responsibility of following the other VFR elements,[14] in particular ensuring that the air traffic at the moment of jump does not create a hazard.		A collision with another canopy is a statistical hazard, and may be avoided by observing simple principles, including knowing upper wind speeds, the number of party members and exit groups, and having sufficient exit separation between jumpers.[17] In 2013, 17% of all skydiving fatalities in the United States resulted from mid-air collisions.[18]				Skydiving can be practised without jumping. Vertical wind tunnels are used to practise for free fall ("indoor skydiving" or "bodyflight"), while virtual reality parachute simulators are used to practise parachute control.		Beginning skydivers seeking training have the following options:		At a sport skydiver's deployment altitude, the individual manually deploys a small pilot-chute which acts as a drogue, catching air and pulling out the main parachute or the main canopy. There are two principal systems in use: the "throw-out", where the skydiver pulls a toggle attached to the top of the pilot-chute stowed in a small pocket outside the main container: and the "pull-out", where the skydiver pulls a small pad attached to the pilot-chute which is stowed inside the container.		Throw-out pilot-chute pouches are usually positioned at the bottom of the container – the B.O.C. deployment system – but older harnesses often have leg-mounted pouches. The latter are safe for flat-flying, but often unsuitable for freestyle or head-down flying.		In a typical civilian sport parachute system, the pilot-chute is connected to a line known as the "bridle", which in turn is attached to a small deployment bag that contains the folded parachute and the canopy suspension lines, which are stowed with rubber bands. At the bottom of the container that holds the deployment bag is a closing loop which, during packing, is fed through the grommets of the four flaps that are used to close the container. At that point, a curved pin that is attached to the bridle is inserted through the closing loop. The next step involves folding the pilot-chute and placing it in a pouch (e.g., B.O.C pouch).		Activation begins when the pilot-chute is thrown out. It inflates and creates drag, pulling the pin out of the closing loop and allowing the pilot-chute to pull the deployment bag from the container. The parachute lines are pulled loose from the rubber bands and extend as the canopy starts to open. A rectangular piece of fabric called the "slider" (which separates the parachute lines into four main groups fed through grommets in the four respective corners of the slider) slows the opening of the parachute and works its way down until the canopy is fully open and the slider is just above the head of the skydiver. The slider slows and controls the deployment of the parachute. Without a slider, the parachute would inflate fast, potentially damaging the parachute fabric and/or suspension lines, as well as causing discomfort, injury or even death of the jumper.[19] During a normal deployment, a skydiver will generally experience a few seconds of intense deceleration, in the realm of 3 to 4 g, while the parachute slows the descent from 190 km/h (120 mph) to approximately 28 km/h (17 mph).		If a skydiver experiences a malfunction of their main parachute which they cannot correct, they pull a "cut-away" handle on the front right-hand side of their harness (on the chest) which will release the main canopy from the harness/container. Once free from the malfunctioning main canopy, the reserve canopy can be activated manually by pulling a second handle on the front left harness. Some containers are fitted with a connecting line from the main to reserve parachutes – known as a reserve static line (RSL) – which pulls open the reserve container faster than a manual release could. Whichever method is used, a spring-loaded pilot-chute then extracts the reserve parachute from the upper half of the container.		One example of this is "Hit and Rock", a variant of accuracy landing devised to let people of varying skill levels compete for fun. "Hit and Rock" is originally from POPS (Parachutists Over Phorty Society). The object is to land as close as possible to the chair, remove the parachute harness, sprint to the chair, sit fully in the chair and rock back and forth at least one time. The contestant is timed from the moment that feet touch the ground until that first rock is completed. This event is considered a race.		Angle Flying was presented for the first time in 2000 at the World Freestyle Competitions, the European Espace Boogie, and the Eloy Freefly Festival.		The technique consists of flying diagonally with a determinate relation between angle and trajectory speed of the body, to obtain an air stream that allows for control of flight. The aim is to fly in formation at the same level and angle, and to be able to perform different aerial games, such as freestyle, three-dimensional flight formation with grip, or acrobatic free-flying.[20]		Classic accuracy is running with opened parachute, in individual or team contest. The aim is to touch down on a target whose center is 2 cm in diameter. The target can be a deep foam mattress or an air-filled landing pad. An electronic recording pad of 32 cm in diameter is set in the middle. It measures score in 1 cm increments up to 16 cm and displays result just after landing.		The first part of any competition take place over 8 rounds. Then in the individual competition, after this 8 selective rounds, the top 25% jump a semi-final round. After semi-final round, the top 50% are selected for the final round. The competitor with the lowest cumulative score is declared the winner.		Competitors jump in teams of 5 maximum, exiting the aircraft at 1000 or 1200 meters and opening their parachutes sequentially to allow each competitor a clear approach to the target.		This sport is unpredictable because weather conditions play a very important part. So classic accuracy requires high adaptability to aerology and excellent steering control.		It is also the most interesting discipline for spectator due to the closeness of action (a few metres) and the possibility to be practised everywhere (sport ground, stadium, urban place...). Today, classic accuracy is the most practiced (in competition) discipline of skydiving in the world.		A cross-country jump is a skydive where the participants open their parachutes immediately after jumping, with the intention of covering as much ground under canopy as possible. Usual distance from jump run to the dropzone can be as much as several miles.		There are two variations of a cross-country jump:		The more popular one is to plan the exit point upwind of the drop zone. A map and information about the wind direction and velocity at different altitudes are used to determine the exit point. This is usually set at a distance from where all the participants should be able to fly back to the drop zone.		The other variation is to jump out directly above the drop zone and fly down wind as far as possible. This increases the risks of the jump substantially, as the participants must be able to find a suitable landing area before they run out of altitude.		Two-way radios and cell-phones are often used to make sure everyone has landed safely, and, in case of a landing off the drop zone, to find out where the parachutist is so that ground crew can pick them up.		Formation Skydiving (FS) was born in California, USA during the 1960‘s. The first documented skydiving formation occurred over Arvin, California in March 1964 when Mitch Poteet, Don Henderson, Andy Keech and Lou Paproski successfully formed a 4-man star formation, photographed by Bob Buquor. This discipline was formerly referred to in the skydiving community as Relative Work, often abbreviated to RW, Relly or Rel.[21]		Parachuting is not always restricted to daytime hours; experienced skydivers sometimes perform night jumps. For safety reasons, this requires more equipment than a usual daytime jump and in most jurisdictions, it requires both an advanced skydiving license (at least a B-License in the U.S.) and a meeting with the local safety official covering who will be doing what on the load. A lit altimeter (preferably accompanied with an audible altimeter) is a must. Skydivers performing night jumps often take flashlights up with them so that they can check their canopies have properly deployed.		Visibility to other skydivers and other aircraft is also a consideration; FAA regulations require skydivers jumping at night to be wearing a light visible for three miles (5 km) in every direction, and to turn it on once they are under canopy. A chem-light(glowstick) is a good idea on a night jump.		Night jumpers should be made aware of the dark zone, when landing at night. Above 30 meters (100 feet) jumpers flying their canopy have a good view of the landing zone normally because of reflected ambient light/moon light. Once they get close to the ground, this ambient light source is lost, because of the low angle of reflection. The lower they get, the darker the ground looks. At about 100 feet and below it may seem that they are landing in a black hole. Suddenly it becomes very dark, and the jumper hits the ground soon after. This ground rush should be explained to, and anticipated by, the first time night jumper. Recommendations should be made to the jumper to utilize a canopy that is larger than they typically use on a day jump and to attempt to schedule their first night jump as close to a full moon as possible to make it easier to see the ground.		While more dangerous than regular skydiving and more difficult to schedule, two night jumps are required by the USPA for a jumper to obtain their D license.		Pond swooping is a form of competitive parachuting wherein canopy pilots attempt to touch down and glide across a small body of water, and onto the shore. Events provide lighthearted competition, rating accuracy, speed, distance and style. Points and peer approval are reduced when a participant "chows", or fails to reach shore and sinks into the water. Swoop ponds are not deep enough to drown in under ordinary circumstances, their main danger being from the concussive force of an incorrectly executed maneuver. In order to gain distance, swoopers increase their speed by executing a "hook turn", wherein both speed and difficulty increase with the angle of the turn. Hook turns are most commonly measured in increments of 90 degrees. As the angle of the turn increases, both horizontal and vertical speed are increased, such that a misjudgement of altitude or imprecise manipulation of the canopy's control structures (front risers, rear risers, and toggles) can lead to a high speed impact with the pond or Earth. Prevention of injury is the main reason why a pond is used for swooping rather than a grass landing area.		This is when skydivers have a ball which weighs 100-200 grams and release it in free fall. The ball maintains the same fall rate as the skydivers. The skydivers can pass the ball around to each other whilst in free fall. At a predetermined altitude, the "ball master" will catch the ball and hold on to it to ensure it does not impact the ground.		Style can be considered as sprint of parachuting. This individual discipline is played in free fall.		The idea is to take maximum speed and complete a pre-designated series of maneuvers as fast and cleanly as possible (speed can exceed 400 km/h / 250 mph)		Jumps are filmed using a ground based camera (with an exceptional lens to record the performance).		Performance is timed (from the start of the maneuver until its completion) and then judged in public at the end of the jump. Competition includes 4 qualifying rounds and a final for the top 8. Competitors jump from a height of 2200 m to 2500 m.		They rush into an acceleration stage for 15 to 20 seconds and then run their series of maneuvers benefiting to the maximum of the stored speed.		Those series consist of Turns and Back-Loops to achieve in a pre-designated order. The incorrect performance of the maneuvers gives rise to penalties that are added at runtime.		The performance of the athlete is defined in seconds and hundredths of a second. The competitor with the lowest cumulative time is declared the winner.		Notice the complete sequence is performed by leading international experts in just over 6 seconds, penalties included.		Thanks to large unpopulated areas to jump over, 'stuff' jumps become possible. Also known as "zoo jumps", in these jumps the skydivers jump out with some object. Rubber raft jumps are popular; where the jumpers sit in a rubber raft. Cars, bicycles, motorcycles, vacuum cleaners, water tanks, and inflatable companions have also been thrown out the back of an aircraft. At a certain altitude, the jumpers break off from the object and deploy their parachutes, leaving it to smash into the ground at terminal velocity.		A tradition at many drop zones is the swoop and chug. As parachutists land from the last load of the day, other skydivers often hand the landing skydivers a beer that is customarily chugged in the landing area. This is sometimes timed as a friendly competition but is usually an informal, untimed, kick-off for the night's festivities.[22]		Tracking is where skydivers take a body position to achieve a high forward speed, allowing them to cover a great distance over the ground. Tracking is also used at the end of group jumps to achieve separation from other jumpers before parachute deployment. The tracking position involves sweeping the arms out to the side of the body and straightening the legs with the toes pointed.		Using a vertical wind tunnel to simulate free fall has become a discipline of its own and is not only used for training but has its own competitions, teams, and figures.		'Wingsuit flying' or 'wingsuiting' is the sport of flying through the air using a wingsuit, which adds surface area to the human body to enable a significant increase in lift. The common type of wingsuit creates an extra surface area with fabric between the legs and under the arms.		National parachuting associations exist in many countries, many affiliated with the Fédération Aéronautique Internationale (FAI), to promote their sport. In most cases, national representative bodies, as well as local drop zone operators, require that participants carry certification attesting to their training their level of experience in the sport, and their proven competence. Anyone who cannot produce such bona-fides is treated as a student, requiring close supervision.[citation needed]		The sole organization in the United States is the United States Parachute Association (USPA), which issues licenses and ratings, governs skydiving, and represents skydiving to government agencies. USPA publishes the Skydivers Information Manual (SIM) and many other resources. In Canada, the Canadian Sport Parachuting Association is the lead organization. In South Africa, the sport is managed by the Parachute Association of South Africa, and in the United Kingdom by the British Parachute Association. In Brazil the CNP (Centro Nacional de Paraquedismo) sets in Boituva, where many records have been broken and where it is known for being the 2nd largest center in the world and the largest in the Southern Hemisphere.[citation needed]		Within the sport, associations promote safety, technical advances, training-and-certification, competition and other interests of their members. Outside their respective communities, they promote their sport to the public and often intercede with government regulators.		Competitions are organized at regional, national and international levels in most these disciplines. Some of them offer amateur competition.		Many of the more photogenic/videogenic variants also enjoy sponsored events with prize money for the winners.		The majority of jumpers tend to be non-competitive, enjoying the opportunity to skydive with their friends on weekends and holidays. The atmosphere of their gatherings is relaxed, sociable and welcoming to newcomers.[citation needed] Skydiving events, called "boogies", are arranged at local, national and international scale each year, which attracting both young jumpers and their elders – Parachutists Over Phorty (POPs), Skydivers Over Sixty (SOS) and even older groups.		In parachuting, a drop zone or DZ is the area above and around a location where a parachutist freefalls and expects to land. It is usually situated beside a small airport, often sharing the facility with other general aviation activities. There is generally a landing area designated for parachute landings. Drop zone staff includes the DZO (drop zone operator or owner), manifestors, pilots, instructors, coaches, cameramen, packers, riggers and other general staff.		Costs in the sport are not trivial. As new technological advances or performance enhancements are introduced, equipment tends to increase in price.[citation needed] Similarly, the average skydiver carries more equipment than in earlier years, with safety devices (such as an AAD) contributing to a significant portion of the cost.		A full set of brand-new equipment can easily cost as much as a new motorcycle or half a small car.[citation needed] The market is not large enough to permit the steady lowering of prices that is seen with some other equipment like computers. A new main canopy for the experienced parachutist can cost between $2,000 and $3,000 US [23][24][25] Higher performance and Tandem Parachutes cost significantly more, whilst large docile student parachutes often cost less.		In many countries, the sport supports a used-equipment market.[citation needed] For beginners, that is the preferred way to acquire "gear", and has two advantages because users can:		Novices generally start with parachutes that are large and docile relative to the jumper's body weight. As they improve in skill and confidence, they can graduate to smaller, faster, more responsive parachutes. An active jumper might change parachutes several times in the space of a few years while retaining his or her first harness/container and peripheral equipment.[citation needed]		Older jumpers, especially those who jump only on weekends in summer, sometimes tend in the other direction, selecting slightly larger, more gentle parachutes that do not demand youthful intensity and reflexes on each jump. They may be adhering to the maxim that: "There are old jumpers and there are bold jumpers, but there are no old, bold jumpers." (Pilots have much the same saying.)[citation needed]		Most parachuting equipment is ruggedly designed and is enjoyed by several owners before being retired. Purchasers are always advised to have any potential purchases examined by a qualified parachute rigger.[citation needed] A rigger is trained to spot signs of damage or misuse. Riggers also keep track of industry product and safety bulletins, and can, therefore, determine if a piece of equipment is up-to-date and serviceable.		
The Right Stuff is a 1979 book by Tom Wolfe about the pilots engaged in U.S. postwar research with experimental rocket-powered, high-speed aircraft as well as documenting the stories of the first Project Mercury astronauts selected for the NASA space program. The Right Stuff is based on extensive research by Wolfe, who interviewed test pilots, the astronauts and their wives, among others. The story contrasts the "Mercury Seven"[1] and their families with test pilots such as Chuck Yeager, who was considered by many contemporaries as the best of them all, but who was never selected as an astronaut.		Wolfe wrote that the book was inspired by the desire to find out why the astronauts accepted the danger of space flight. He recounts the enormous risks that test pilots were already taking, and the mental and physical characteristics—the titular "right stuff"—required for and reinforced by their jobs. Wolfe likens the astronauts to "single combat warriors" from an earlier era who received the honor and adoration of their people before going forth to fight on their behalf.		The 1983 film The Right Stuff is adapted from the book.						In 1972 Jann Wenner, the editor of Rolling Stone assigned Wolfe to cover the launch of NASA's last moon mission, Apollo 17. Wolfe became fascinated with the astronauts, and his competitive spirit compelled him to try to outdo Norman Mailer's nonfiction book about the first moon mission, Of a Fire on the Moon. He published a four-part series for Rolling Stone in 1973 titled "Post-Orbital Remorse", about the depression that some astronauts experienced after having been in space. After the series, Wolfe began researching the whole of the space program, in what became a seven-year project from which he took time to write The Painted Word, a book on art, and to complete Mauve Gloves & Madmen, Clutter & Vine, a collection of shorter pieces.[3]		In 1977 he returned to his astronaut book full-time. Wolfe originally planned to write a complete history of the space program, though after writing through the Mercury program, he felt that his work was complete and that it captured the astronauts' ethos — the "right stuff" that astronauts and test pilots of the 1940s and 1950s shared — the unspoken code of bravery and machismo that compelled these men to ride on top of dangerous rockets. While conducting research, he consulted with General Chuck Yeager and, after receiving a comprehensive review of his manuscript, was convinced that test pilots like Yeager should form the backdrop of the period. In the end, Yeager becomes a personification of the many postwar test pilots and their "right stuff".[4] The phrase itself may have originated in the Joseph Conrad story "Youth", where it was used.		The Right Stuff was published in 1979 by Farrar, Straus and Giroux and became Wolfe's best selling book yet.[citation needed] It was praised by most critics, was a finalist for the National Book Critics Circle Award, and won the National Book Award for Nonfiction.[5][6]		In the foreword to a new edition, published in 1983 when the film adaptation was released, Wolfe wrote that his "book grew out of some ordinary curiosity" about what "makes a man willing to sit up on top of an enormous Roman candle… and wait for someone to light the fuse".[7]		The story is more about the space race than space exploration in general. The Soviet Union's early space efforts are mentioned only as background, focusing entirely on an early portion of the U.S. space program. Only Project Mercury, the first operational manned space-flight program, is covered. The Mercury Seven were Scott Carpenter, Gordon Cooper, John Glenn, Gus Grissom, Wally Schirra, Alan Shepard, and Deke Slayton. Emphasis is given to the personal stories of the astronauts and their wives rather than the technical aspects of space travel and the flights themselves.		The storyline also involves the political reasons for putting people into space, asserting that the Mercury astronauts were actually a burden to the program and were only sent up for promotional reasons. Reasons for including living beings in spacecraft are barely touched upon, but the first option considered was to use a chimpanzee (and, indeed, chimpanzees were sent up first).		Another option considered were athletes already accustomed to physical stress, such as circus trapeze artists. Wolfe states that President Dwight D. Eisenhower, however, insisted on pilots, even though the first crew members would not actually fly the spacecraft. When Gus Grissom lands at sea and exits his space capsule, saving the capsule seems more important to the recovery team than saving the pilot because of the value of the data.		Wolfe contrasts the Seven with the Edwards AFB test pilots, among whom was Chuck Yeager, who was shut out of the astronaut program after NASA officials decided to use college-degreed pilots, not ones who gained their commissions as enlisted men, such as participants in the USAAF Flying Sergeants Program in World War II. Chuck Yeager spent time with Tom Wolfe explaining accident reports "that Wolfe kept getting all wrong". Publishing insiders say these sessions between Wolfe and Yeager led Wolfe to highlight Yeager's character, presence, thoughts, and anecdotes throughout the book. As an example, Yeager prides his speech to the Society of Test Pilots that the first rider in the Mercury development program would be a monkey, not a real test pilot, and Wolfe plays this drama out on the angst felt by the Mercury Astronauts over those remarks. Yeager himself downplayed the theory of "the right stuff", attributing his survival of potential catastrophes to simply knowing his airplane thoroughly, along with some good luck.		Another test pilot highlighted in the book is Scott Crossfield. Crossfield and Yeager were fierce but friendly rivals for speed and altitude records.		A 3-hour, 13-minute film stars Sam Shepard, Scott Glenn, Ed Harris, Dennis Quaid, Fred Ward, Barbara Hershey, Kim Stanley, Levon Helm, Veronica Cartwright, Pamela Reed, Lance Henriksen, and the real Chuck Yeager in a cameo appearance. NFL Hall of Famer Anthony Muñoz also has a small role, playing "Gonzalez". It features a score by composer Bill Conti.		The screenplay was adapted by Philip Kaufman from the book, with some contributions from screenwriter William Goldman (Goldman dissociated himself with the film after quarrelling with Kaufman about the story). The film was also directed by Kaufman.		While the movie took liberties with certain historical facts as part of "dramatic license", criticism focused on one: the portrayal of Gus Grissom panicking when his Liberty Bell 7 spacecraft sank following splashdown. Most historians, as well as engineers working for or with NASA and many of the related contractor agencies within the aerospace industry, are now convinced that the premature detonation of the spacecraft hatch's explosive bolts was caused by failure not associated with direct human error or deliberate detonation at the hands of Grissom.[citation needed]		This determination had, in fact, been made long before the movie was filmed, and even Tom Wolfe's book only states that this possibility was considered, not that it was actually judged as being the cause of the accident. In fact, Grissom was assigned to command the first flights of both Gemini and Apollo. Ironically, Grissom died in the Apollo 1 fire because there was no quick-opening hatch on the Block 1 Apollo Command Module — a design choice made because NASA had determined that the explosion in the hatch on Grissom's Liberty Bell 7 had been most likely self-initiated.[citation needed]		Another fact that had been altered in the film was the statement by Trudy Cooper, who commented that she "wondered how they would've felt if every time their husband went in to make a deal, there was a one-in-four chance he wouldn't come out of that meeting". According to the book, this actually reflected the 23% chance of dying during a 20-year career as a normal pilot. For a test pilot, these odds were higher, at 53%, but were still considerably less than the movie implied. In addition, the movie merely used the fictional Mrs. Cooper as a vehicle for the statement; the real Mrs. Cooper is not known to have said this.[8]		Wolfe made no secret that he disliked the film, especially because of changes from his original book. William Goldman, involved in early drafts of the script, also disliked the choices made by Kaufman, saying in his book Adventures in the Screen Trade that "Phil [Kaufman]'s heart was with Yeager. And not only that, he felt the astronauts, rather than being heroic, were really minor leaguers, mechanical men of no particular quality, not great pilots at all, simply the product of hype”.[9] Critics, however, generally were favorable toward the film.		
An adventurer is a person who adventures.		Adventurer may also refer to:		
Adventure education is the promotion of learning through adventure centered experiences.		Adventure centered experiences can include a wide variety of activities, due to the different ways people experience adventure. Outdoor sports, challenge courses, races, and even indoor activities can be used in adventure education. Adventure education is related to adventure programming, adventure therapy, and outdoor education. It is an active process rather than a passive process of learning that requires active engagement from the learners as well as the instructors.[1] Often adventure education is linked to an incorporation of all five senses within the experiences which can heighten the opportunities for learning and retaining information. The learning experiences within adventure education programs are structured for a potential increase in human performance and capacity. Sometimes the adventure lies more in the journey than the destination. The venture lies in the struggle, not in the prize.[2]						Merriam-Webster defines adventure as "an undertaking usually involving danger and unknown risks". Danger is defined as "exposure or liability to injury, pain, harm, or loss." Danger involves two factors which are perils- the origins of injury or the causes of loss, and hazards- the conditions that emphasize the chance of injury or loss.[3] Risk is defined as "potential loss or injury". Risk can be described as "real risk" or "perceived risk" [4] such as bungee jumping; it seems as though there is a high level of risk, but with proper equipment it can be relatively safe. Danger, then is the exposure, or magnitude, of the harm a person may encounter; risk is the probability of that harm. These two variables are filtered through a person's perceptions, which may or may not be accurate.		Consequently, adventure is created through a person's perception of the magnitude of the potential risk, and the probability of loss. An activity with relatively low magnitude but high probability of harm (such as adventure racing or slacklining) may be just as much of an adventure as an activity with relatively high magnitude and low probability of harm (such as sport rock climbing, skydiving, or riding a roller coaster).		Adventure education has many positive outcomes. A meta-analysis of adventure education studies identified forty major outcomes, grouped into the following six categories: leadership, self-concept, academic, personality, interpersonal, and adventuresomeness.[5] Adventure education often employs practical skills that will benefit an individual in areas beyond the activities in an adventure program. There are three theories of transfer in adventure education in which the participant may apply what they learned into future experiences.[6] The first of these theories is "specific transfer"- the learner applies the habits and skills learned during an experience to a new and similar experience (e.g. when an individual learns how to belay during a rock climbing experience and then applies that knowledge to rappelling). The second theory is "nonspecific transfer"- the learner establishes some common principles acquired through previous experiences and applies them in a new learning situation (e.g. when an individual develops trust through a trust building activity). The third theory is "metaphoric transfer"- the learner applies similar underlying principles to other areas and situations (e.g. when individuals utilize teamwork during an activity such as canoeing and later applies it to the workplace or other group experiences).[7]		There are six categories of program characteristics that contribute to achieving the program outcomes described above. These are the physical environment, activities, processing, the group, instructors, and the participant.[8]		Unfamiliar environments contribute a great deal to program outcomes experienced by adventure education participants. Being in a new environment allows participants to gain new perspectives on familiar environments[9] and gives them the freedom to experiment.[10] An unfamiliar environment also creates some level of anxiety for the participant, as well as creating the perception of risk. Overcoming the challenges presented by unfamiliar environments through the mastery of specific tasks results in positive benefits to the individual, such as increased self-esteem.[11] Positive outcomes are offered by several types of environments, including wilderness, non-wilderness (e.g. ropes-course), or a traditional classroom. However, wilderness is often considered as providing additional benefits to participants, thus being the optimal environmental setting for adventure education programs.[12]		Rather than activities themselves, it is the qualities of activities that are responsible for achieving program outcomes.[13] The combination of challenge, mastery, and success in activities is what led to participant growth. Challenges should be holistic in order to maximize positive outcomes. Programs should include mental, emotional, and physical challenges, and encourage concurrent mastery in all three domains.[14] Challenges should also increase incrementally, so as not to overwhelm participants early on in the program but allow them to grow and develop throughout. Activities should be well organized and matched to suit the particular needs and requirements of the participants. The GRABBS model[15] (Goals, Readiness, Affect, Behavior, Body, and Stage of Development) is a good method for matching activities and participants. Success in the activities must be achievable. However, some failure may also be good for participant development.[16] Program participants can learn from their failures to achieve success. Goal-setting is critical to achieving program outcomes, at both the individual and group levels. It is also important to allow participants to have personal choice related to activities. The "challenge-by-choice" philosophy of adventure programming allows the participant to have some autonomy related to the activities s/he participates in.		While the qualities of activities are most important in achieving program outcomes, there are also specific activities that are well-suited to adventure programming.[17] These include activities related to trust and empathy (e.g. trust falls), communication, decision-making and problem solving, social responsibility, and personal responsibility.		Processing is defined as "the sorting and ordering of information" that enables program participants to internalize meaning gained from an adventure education experience[18] Three models have been identified by which participants process meaning.[19] In the "Mountains Speak for Themselves" model, participants are responsible for reflecting on their experiences on their own, without facilitation from the instructor. In the "Outward Bound Plus" model, the instructor serves as a counselor, facilitator, and discussion leader. In the metaphoric model, activities are consciously framed so that they become experiential metaphors that can be applied to challenges in participants' daily lives.		Several characteristics of the group also contribute to achieving program outcomes. In terms of the size of the group, small groups of seven to fifteen individuals are usually more conducive to achieving desired outcomes.[20] Reciprocity within the group is also important. This refers to group members learning to cooperate with one another and capitalize on the strengths of each individual.[21] Autonomy of individuals and personal relationships are other aspects of the group that contribute to achievement.[22]		Certain aspects of program instructors such as biographical characteristics, personality, and interpersonal interactions can have a large influence on participants' achievement of desired program outcomes.[23]		The age, gender, background, and expectations of program participants have also been shown to be related to the achievement of program outcomes.[24]		Adventure education programming can be implemented in several contexts, including therapy for youth at risk,[25] survivors of sexual assault,[26] families in distress,[27] and persons with medical conditions.[28]		
Tourism is travel for pleasure or business; also the theory and practice of touring, the business of attracting, accommodating, and entertaining tourists, and the business of operating tours.[1] Tourism may be international, or within the traveller's country. The World Tourism Organization defines tourism more generally, in terms which go "beyond the common perception of tourism as being limited to holiday activity only", as people "traveling to and staying in places outside their usual environment for not more than one consecutive year for leisure, business and other purposes".[2]		Tourism can be domestic or international, and international tourism has both incoming and outgoing implications on a country's balance of payments. Today, tourism is a major source of income for many countries, and affects the economy of both the source and host countries, in some cases being of vital importance.[3]		Tourism suffered as a result of a strong economic slowdown of the late-2000s recession, between the second half of 2008 and the end of 2009, and the outbreak of the H1N1 influenza virus,[4][5] but slowly recovered. International tourism receipts (the travel item in the balance of payments) grew to US$1.03 trillion (€740 billion) in 2011, corresponding to an increase in real terms of 3.8% from 2010.[6] International tourist arrivals surpassed the milestone of 1 billion tourists globally for the first time in 2012,[7] emerging markets such as China, Russia and Brazil had significantly increased their spending over the previous decade.[8] The ITB Berlin is the world's leading tourism trade fair.[9]						The word tourist was used in 1772[10] and tourism in 1811.[11] It is formed from the word tour, which is derived from Old English turian, from Old French torner, from Latin tornare; 'to turn on a lathe,' which is itself from Ancient Greek tornos; 'lathe'.[12]		Tourism is an important, even vital, source of income for many regions and countries. Its importance was recognized in the Manila Declaration on World Tourism of 1980 as "an activity essential to the life of nations because of its direct effects on the social, cultural, educational, and economic sectors of national societies and on their international relations."[2][13]		Tourism brings in large amounts of income into a local economy in the form of payment for goods and services needed by tourists, accounting for 30% of the world's trade of services, and 6% of overall exports of goods and services.[6] It also creates opportunities for employment in the service sector of the economy associated with tourism.[14]		The service industries which benefit from tourism include transportation services, such as airlines, cruise ships, and taxicabs; hospitality services, such as accommodations, including hotels and resorts; and entertainment venues, such as amusement parks, casinos, shopping malls, music venues, and theatres. This is in addition to goods bought by tourists, including souvenirs.		In 1936, the League of Nations defined a foreign tourist as "someone traveling abroad for at least twenty-four hours". Its successor, the United Nations, amended this definition in 1945, by including a maximum stay of six months.[15]		In 1941, Hunziker and Kraft defined tourism as "the sum of the phenomena and relationships arising from the travel and stay of non-residents, insofar as they do not lead to permanent residence and are not connected with any earning activity."[16][17] In 1976, the Tourism Society of England's definition was: "Tourism is the temporary, short-term movement of people to destinations outside the places where they normally live and work and their activities during the stay at each destination. It includes movements for all purposes."[18] In 1981, the International Association of Scientific Experts in Tourism defined tourism in terms of particular activities chosen and undertaken outside the home.[19]		In 1994, the United Nations identified three forms of tourism in its Recommendations on Tourism Statistics:[20]		The terms tourism and travel are sometimes used interchangeably. In this context, travel has a similar definition to tourism, but implies a more purposeful journey. The terms tourism and tourist are sometimes used pejoratively, to imply a shallow interest in the cultures or locations visited. By contrast, traveler is often used as a sign of distinction. The sociology of tourism has studied the cultural values underpinning these distinctions and their implications for class relations.[21]		International tourist arrivals reached 1.035 billion in 2012, up from over 996 million in 2011, and 952 million in 2010.[7] In 2011 and 2012, international travel demand continued to recover from the losses resulting from the late-2000s recession, where tourism suffered a strong slowdown from the second half of 2008 through the end of 2009. After a 5% increase in the first half of 2008, growth in international tourist arrivals moved into negative territory in the second half of 2008, and ended up only 2% for the year, compared to a 7% increase in 2007.[4] The negative trend intensified during 2009, exacerbated in some countries due to the outbreak of the H1N1 influenza virus, resulting in a worldwide decline of 4.2% in 2009 to 880 million international tourists arrivals, and a 5.7% decline in international tourism receipts.[5]		The World Tourism Organization reports the following ten destinations as the most visited in terms of the number of international travelers in 2016.		International tourism receipts grew to US$1.2 trillion in 2014, corresponding to an increase in real terms of 3.7% from 2013.[6][not in citation given] The World Tourism Organization reports the following entities as the top ten tourism earners for the year 2015:		The World Tourism Organization reports the following countries as the ten biggest spenders on international tourism for the year 2015.		Based upon air traffic, the MasterCard Global Destination Cities Index rates the following as the world's ten most popular cities for international tourism.		MasterCard rates the following cities as the world's ten biggest earners from international tourism in 2015.[25]		Euromonitor International rated these the world's cities most visited by international tourists in January 2015:[26][27]		Travel outside a person's local area for leisure was largely confined to wealthy classes, who at times travelled to distant parts of the world, to see great buildings and works of art, learn new languages, experience new cultures, and to taste different cuisines. As early as Shulgi, however, kings praised themselves for protecting roads and building waystations for travelers.[31] During the Roman Republic, spas and coastal resorts such as Baiae were popular among the rich. Pausanias wrote his Description of Greece in the 2nd century AD. In ancient China, nobles sometimes made a point of visiting Mount Tai and, on occasion, all five Sacred Mountains.		By the Middle Ages, Christianity, Buddhism, and Islam all had traditions of pilgrimage that motivated even the lower classes to undertake distant journeys for health or spiritual improvement, seeing the sights along the way. The Islamic hajj is still central to its faith and Chaucer's Canterbury Tales and Wu Cheng'en's Journey to the West remain classics of English and Chinese literature.		The 10th- to 13th-century Song dynasty also saw secular travel writers such as Su Shi (11th century) and Fan Chengda (12th century) become popular in China. Under the Ming, Xu Xiake continued the practice.[32] In medieval Italy, Francesco Petrarch also wrote an allegorical account of his 1336 ascent of Mount Ventoux that praised the act of traveling and criticized frigida incuriositas ("cold lack of curiosity"). The Burgundian poet Michault Taillevent (fr) later composed his own horrified recollections of a 1430 trip through the Jura Mountains.[33]		Modern tourism can be traced to what was known as the Grand Tour, which was a traditional trip around Europe (especially Germany and Italy), undertaken by mainly upper-class European young men of means, mainly from Western and Northern European countries. In 1624, young Prince of Poland, Ladislaus Sigismund Vasa, the eldest son and heir of Sigismund III, embarked for a journey across Europe, as was in custom among Polish nobility.[34] He travelled through territories of today's Germany, Belgium, Netherlands, where he admired the Siege of Breda by Spanish forces, France, Switzerland to Italy, Austria and Czechia.[34] It was an educational journey[35] and one of the outcomes was introduction of Italian opera in the Polish–Lithuanian Commonwealth.[36]		The custom flourished from about 1660 until the advent of large-scale rail transit in the 1840s, and generally followed a standard itinerary. It was an educational opportunity and rite of passage. Though primarily associated with the British nobility and wealthy landed gentry, similar trips were made by wealthy young men of Protestant Northern European nations on the Continent, and from the second half of the 18th century some South American, US, and other overseas youth joined in. The tradition was extended to include more of the middle class after rail and steamship travel made the journey easier, and Thomas Cook made the "Cook's Tour" a byword.		The Grand Tour became a real status symbol for upper class students in the 18th and 19th centuries. In this period, Johann Joachim Winckelmann's theories about the supremacy of classic culture became very popular and appreciated in the European academic world. Artists, writers and travellers (such as Goethe) affirmed the supremacy of classic art of which Italy, France and Greece provide excellent examples. For these reasons, the Grand Tour's main destinations were to those centres, where upper-class students could find rare examples of classic art and history.		The New York Times recently described the Grand Tour in this way:		Three hundred years ago, wealthy young Englishmen began taking a post-Oxbridge trek through France and Italy in search of art, culture and the roots of Western civilization. With nearly unlimited funds, aristocratic connections and months (or years) to roam, they commissioned paintings, perfected their language skills and mingled with the upper crust of the Continent.		The primary value of the Grand Tour, it was believed, laid in the exposure both to the cultural legacy of classical antiquity and the Renaissance, and to the aristocratic and fashionably polite society of the European continent.		Leisure travel was associated with the Industrial Revolution in the United Kingdom – the first European country to promote leisure time to the increasing industrial population.[37] Initially, this applied to the owners of the machinery of production, the economic oligarchy, factory owners and traders. These comprised the new middle class.[37] Cox & Kings was the first official travel company to be formed in 1758.[38]		The British origin of this new industry is reflected in many place names. In Nice, France, one of the first and best-established holiday resorts on the French Riviera, the long esplanade along the seafront is known to this day as the Promenade des Anglais; in many other historic resorts in continental Europe, old, well-established palace hotels have names like the Hotel Bristol, Hotel Carlton, or Hotel Majestic – reflecting the dominance of English customers.		A pioneer of the travel agency business, Thomas Cook's idea to offer excursions came to him while waiting for the stagecoach on the London Road at Kibworth. With the opening of the extended Midland Counties Railway, he arranged to take a group of 540 temperance campaigners from Leicester Campbell Street station to a rally in Loughborough, eleven miles (18 km) away. On 5 July 1841, Thomas Cook arranged for the rail company to charge one shilling per person; this included rail tickets and food for the journey. Cook was paid a share of the fares charged to the passengers, as the railway tickets, being legal contracts between company and passenger, could not have been issued at his own price.[clarification needed] This was the first privately chartered excursion train to be advertised to the general public; Cook himself acknowledged that there had been previous, unadvertised, private excursion trains.[39] During the following three summers he planned and conducted outings for temperance societies and Sunday school children. In 1844 the Midland Counties Railway Company agreed to make a permanent arrangement with him, provided he found the passengers. This success led him to start his own business running rail excursions for pleasure, taking a percentage of the railway fares.[40]		In 1855, he planned his first excursion abroad, when he took a group from Leicester to Calais to coincide with the Paris Exhibition. The following year he started his "grand circular tours" of Europe.[41] During the 1860s he took parties to Switzerland, Italy, Egypt and the United States. Cook established "inclusive independent travel", whereby the traveller went independently but his agency charged for travel, food and accommodation for a fixed period over any chosen route. Such was his success that the Scottish railway companies withdrew their support between 1862 and 1863 to try the excursion business for themselves.		Cruising is a popular form of water tourism. Leisure cruise ships were introduced by the Peninsular & Oriental Steam Navigation Company (P&O) in 1844,[42] sailing from Southampton to destinations such as Gibraltar, Malta and Athens.[43] In 1891, German businessman Albert Ballin sailed the ship Augusta Victoria from Hamburg into the Mediterranean Sea. In 1900, one of the first purpose-built cruise ships was Prinzessin Victoria Luise, built in Hamburg.		Many leisure-oriented tourists travel to seaside resorts on their nearest coast or further afield. Coastal areas in the tropics are popular in both summer and winter.		St. Moritz, Switzerland became the cradle of the developing winter tourism in the 1860s: hotel manager Johannes Badrutt invited some summer guests from England to return in the winter to see the snowy landscape, thereby inaugurating a popular trend.[44][45] It was, however, only in the 1970s when winter tourism took over the lead from summer tourism in many of the Swiss ski resorts. Even in winter, up to one third of all guests (depending on the location) consist of non-skiers.[46]		Major ski resorts are located mostly in the various European countries (e.g. Andorra, Austria], Bulgaria, Bosnia-Herzegovina, Croatia, Czech Republic, Cyprus, Finland, France, Germany, Greece, Iceland, Italy, Norway, Latvia, Lithuania, Poland, Romania, Serbia, Sweden, Slovakia, Slovenia, Spain, Switzerland, Turkey), Canada, the United States (e.g. Montana, Utah, Colorado, California, Wyoming, Vermont, New Hampshire, New York) Lebanon, New Zealand, Japan, South Korea, Chile, and Argentina.		Academics have defined mass tourism as travel by groups on pre-scheduled tours, usually under the organization of tourism professionals.[47] This form of tourism developed during the second half of the 19th century in the United Kingdom and was pioneered by Thomas Cook. Cook took advantage of Europe's rapidly expanding railway network and established a company that offered affordable day trip excursions to the masses, in addition to longer holidays to Continental Europe, India, Asia and the Western Hemisphere which attracted wealthier customers. By the 1890s over 20,000 tourists per year used Thomas Cook & Son.[48]		The relationship between tourism companies, transportation operators and hotels is a central feature of mass tourism. Cook was able to offer prices that were below the publicly advertised price because his company purchased large numbers of tickets from railroads.[48] One contemporary form of mass tourism, package tourism, still incorporates the partnership between these three groups.		Travel developed during the early 20th century and was facilitated by the development of the automobiles and later by airplanes. Improvements in transport allowed many people to travel quickly to places of leisure interest, so that more people could begin to enjoy the benefits of leisure time.		In Continental Europe, early seaside resorts included: Heiligendamm, founded in 1793 at the Baltic Sea, being the first seaside resort; Ostend, popularised by the people of Brussels; Boulogne-sur-Mer and Deauville for the Parisians; Taormina in Sicily. In the United States, the first seaside resorts in the European style were at Atlantic City, New Jersey and Long Island, New York.		By the mid-20th century the Mediterranean Coast became the principal mass tourism destination. The 1960s and 1970s saw mass tourism play a major role in the Spanish economic "miracle".		Niche tourism refers to the numerous specialty forms of tourism that have emerged over the years, each with its own adjective. Many of these terms have come into common use by the tourism industry and academics.[49] Others are emerging concepts that may or may not gain popular usage. Examples of the more common niche tourism markets are:		Other terms used for niche or specialty travel forms include the term "destination" in the descriptions, such as destination weddings, and terms such as location vacation.		There has been an up-trend in tourism over the last few decades,[vague] especially in Europe, where international travel for short breaks is common. Tourists have a wide range of budgets and tastes, and a wide variety of resorts and hotels have developed to cater for them. For example, some people prefer simple beach vacations, while others want more specialised holidays, quieter resorts, family-oriented holidays, or niche market-targeted destination hotels.		The developments in technology and transport infrastructure, such as jumbo jets, low-cost airlines, and more accessible airports have made many types of tourism more affordable. The WHO estimated in 2009 that there are around half a million people on board aircraft at any given time.[50] There have also been changes in lifestyle, for example some retirement-age people sustain year round tourism. This is facilitated by internet sales of tourist services. Some sites have now started to offer dynamic packaging, in which an inclusive price is quoted for a tailor-made package requested by the customer upon impulse.		There have been a few setbacks in tourism, such as the September 11 attacks and terrorist threats to tourist destinations, such as in Bali and several European cities. Also, on 26 December 2004, a tsunami, caused by the 2004 Indian Ocean earthquake, hit the Asian countries on the Indian Ocean, including the Maldives. Thousands of lives were lost including many tourists. This, together with the vast clean-up operations, stopped or severely hampered tourism in the area for a time.[51]		Individual low-price or even zero-price overnight stays have become more popular in the 2000s, especially with a strong growth in the hostel market and services like CouchSurfing and airbnb being established.[52] There has also been examples of jurisdictions wherein a significant portion of GDP is being spent on altering the primary sources of revenue towards tourism, as has occurred for instance in Dubai.[53]		"Sustainable tourism is envisaged as leading to management of all resources in such a way that economic, social and aesthetic needs can be fulfilled while maintaining cultural integrity, essential ecological processes, biological diversity and life support systems." (World Tourism Organization)[54]		Sustainable development implies "meeting the needs of the present without compromising the ability of future generations to meet their own needs." (World Commission on Environment and Development, 1987)[55]		Sustainable tourism can be seen as having regard to ecological and social-cultural carrying capacities and includes involving the community of the destination in tourism development planning. It also involves integrating tourism to match current economic and growth policies so as to mitigate some of the negative economic and social impacts of 'mass tourism'. Murphy (1985) advocates the use of an 'ecological approach', to consider both 'plants' and 'people' when implementing the sustainable tourism development process. This is in contrast to the 'boosterism' and 'economic' approaches to tourism planning, neither of which consider the detrimental ecological or sociological impacts of tourism development to a destination.		However, Butler questions the exposition of the term 'sustainable' in the context of tourism, citing its ambiguity and stating that "the emerging sustainable development philosophy of the 1990s can be viewed as an extension of the broader realization that a preoccupation with economic growth without regard to its social and environmental consequences is self-defeating in the long term." Thus 'sustainable tourism development' is seldom considered as an autonomous function of economic regeneration as separate from general economic growth.		Ecotourism, also known as ecological tourism, is responsible travel to fragile, pristine, and usually protected areas that strives to be low-impact and (often) small-scale. It helps educate the traveler; provides funds for conservation; directly benefits the economic development and political empowerment of local communities; and fosters respect for different cultures and for human rights.Take only memories and leave only footprints is a very common slogan in protected areas.[56] Tourist destinations are shifting to low carbon emissions following the trend of visitors more focused in being environmentally responsible adopting a sustainable behavior.[57]		Volunteer tourism (or voluntourism) is growing as a largely Western phenomenon, with volunteers travelling to aid those less fortunate than themselves in order to counter global inequalities. Wearing (2001) defines volunteer tourism as applying “to those tourists who, for various reasons, volunteer in an organised way to undertake holidays that might involve aiding or alleviating the material poverty of some groups in society”[58]. VSO was founded in the UK in 1958 and the US Peace Corps was subsequently founded in 1960. These were the first large scale voluntary sending organisations, initially arising to modernise less economically developed countries, which it was hoped would curb the influence of communism[59].		This form of tourism is largely praised for its more sustainable approach to travel, with tourists attempting to assimilate into local cultures, and avoiding the criticisms of consumptive and exploitative mass tourism[60]. However, increasingly voluntourism is being criticised by scholars who suggest it may have negative effects as it begins to undermine local labour, and force unwilling host communities to adopt Western initiatives[61], while host communities without a strong heritage fail to retain volunteers who become dissatisfied with experiences and volunteer shortages persist[62]. Increasingly organisations such as VSO have been concerned with community-centric volunteer programmes where power to control the future of the community is in the hands of local people[63].		Pro-poor tourism, which seeks to help the poorest people in developing countries, has been receiving increasing attention by those involved in development; the issue has been addressed through small-scale projects in local communities and through attempts by Ministries of Tourism to attract large numbers of tourists. Research by the Overseas Development Institute suggests that neither is the best way to encourage tourists' money to reach the poorest as only 25% or less (far less in some cases) ever reaches the poor; successful examples of money reaching the poor include mountain-climbing in Tanzania and cultural tourism in Luang Prabang, Laos.[64] There is also the possibilty of pro-poor tourism principles being adopted in centre sites of regeneration in the developed world[65].		Recession tourism is a travel trend which evolved by way of the world economic crisis. Recession tourism is defined by low-cost and high-value experiences taking place of once-popular generic retreats. Various recession tourism hotspots have seen business boom during the recession thanks to comparatively low costs of living and a slow world job market suggesting travelers are elongating trips where their money travels further. This concept is not widely used in tourism research. It is related to the short-lived phenomenon that is more widely known as staycation.		When there is a significant price difference between countries for a given medical procedure, particularly in Southeast Asia, India, Eastern Europe, Cuba[66] and Canada[67] where there are different regulatory regimes, in relation to particular medical procedures (e.g. dentistry), traveling to take advantage of the price or regulatory differences is often referred to as "medical tourism".		Educational tourism is developed because of the growing popularity of teaching and learning of knowledge and the enhancing of technical competency outside of classroom environment. In educational tourism, the main focus of the tour or leisure activity includes visiting another country to learn about the culture, study tours, or to work and apply skills learned inside the classroom in a different environment, such as in the International Practicum Training Program.		Creative tourism has existed as a form of cultural tourism, since the early beginnings of tourism itself. Its European roots date back to the time of the Grand Tour, which saw the sons of aristocratic families traveling for the purpose of mostly interactive, educational experiences. More recently, creative tourism has been given its own name by Crispin Raymond and Greg Richards,[68] who as members of the Association for Tourism and Leisure Education (ATLAS), have directed a number of projects for the European Commission, including cultural and crafts tourism, known as sustainable tourism. They have defined "creative tourism" as tourism related to the active participation of travellers in the culture of the host community, through interactive workshops and informal learning experiences.[68]		Meanwhile, the concept of creative tourism has been picked up by high-profile organizations such as UNESCO, who through the Creative Cities Network, have endorsed creative tourism as an engaged, authentic experience that promotes an active understanding of the specific cultural features of a place.[citation needed]		More recently, creative tourism has gained popularity as a form of cultural tourism, drawing on active participation by travelers in the culture of the host communities they visit. Several countries offer examples of this type of tourism development, including the United Kingdom, Austria, France, the Bahamas, Jamaica, Spain, Italy and New Zealand.[citation needed]		The growing interest of tourists[69] in this new way to discover a culture regards particularly the operators and branding managers, attentive to the possibility of attracting a quality tourism, highlighting the intangible heritage (craft workshops, cooking classes, etc.) and optimizing the use of existing infrastructure (for example, through the rent of halls and auditorium).		Experiential travel (or "immersion travel") is one of the major market trends in the modern tourism industry. It is an approach to travelling which focuses on experiencing a country, city or particular place by connecting to its history, people, food and culture.[70]		The term “Experiential travel” is already mentioned in publications from 1985[71] – however it was discovered as a meaningful market trend much later.		One emerging area of special interest has been identified by Lennon and Foley (2000)[72][73] as "dark" tourism. This type of tourism involves visits to "dark" sites, such as battlegrounds, scenes of horrific crimes or acts of genocide, for example concentration camps. Dark tourism remains a small niche market, driven by varied motivations, such as mourning, remembrance, education, macabre curiosity or even entertainment.[citation needed] Its origins are rooted in fairgrounds and medieval fairs.[74]		Philip Stone argues that dark tourism is a way of imagining one's own death through the real death of others.[75] Erik H Cohen introduces the term "populo sites" to evidence the educational character of dark tourism. Populo sites transmit the story of vicitimized people to visitors. Based on a study at Yad Vashem, the Shoah (Holocaust) memorial museum in Jerusalem, a new term—in populo—is proposed to describe dark tourism sites at a spiritual and population center of the people to whom a tragedy befell. Learning about the Shoah in Jerusalem offers an encounter with the subject which is different from visits to sites in Europe, but equally authentic. It is argued that a dichotomy between "authentic" sites at the location of a tragedy and "created" sites elsewhere is insufficient. Participants' evaluations of seminars for European teachers at Yad Vashem indicate that the location is an important aspect of a meaningful encounter with the subject. Implications for other cases of dark tourism at in populo locations are discussed.[76] In this vein, Peter Tarlow defines dark tourism as the tendency to visit the scenes of tragedies or historically noteworthy deaths, which continue to impact our lives. This issue cannot be understood without the figure of trauma.[77] Following this, Maximiliano Korstanje explains that tourism serves as an scapegoat mechanism used in order for society does not collapse.[clarification needed] This is the reason why tourists look for something special, something new beyond their nearest residential home.[clarification needed] The quest for "Otherness" leads not only to maximize pleasure but also provides a pedagogical message to the us.[clarification needed] In the context of disasters and tragedies, dark tourism may revitalize the lost trust giving a positive value that helps community in the process of recovery. Tourism is in fact an instrument of resiliency that paves the ways for the society to be united[clarification needed].[78][79][80]		Social tourism is making tourism available to poor people who otherwise could not afford to travel for their education or recreation. It includes youth hostels and low-priced holiday accommodation run by church and voluntary organisations, trade unions, or in Communist times publicly owned enterprises. In May 1959, at the second Congress of Social Tourism in Austria, Walter Hunziker proposed the following definition: "Social tourism is a type of tourism practiced by low income groups, and which is rendered possible and facilitated by entirely separate and therefore easily recognizable services".[citation needed]		Also known as "Tourism of Doom," or "Last Chance Tourism" this emerging trend involves traveling to places that are environmentally or otherwise threatened (such as the ice caps of Mount Kilimanjaro, the melting glaciers of Patagonia, or the coral of the Great Barrier Reef) before it is too late. Identified by travel trade magazine Travel Age West[81] editor-in-chief Kenneth Shapiro in 2007 and later explored in The New York Times,[82] this type of tourism is believed to be on the rise. Some see the trend as related to sustainable tourism or ecotourism due to the fact that a number of these tourist destinations are considered threatened by environmental factors such as global warming, overpopulation or climate change. Others worry that travel to many of these threatened locations increases an individual’s carbon footprint and only hastens problems threatened locations are already facing.[83][84][85][86][87][88]		Religious tourism, in particular religious travel, is used to strengthen faith and show devotion both of which are central tenets of many major religions[89]. Religious tourists seek destinations whose image encourages them to believe that they can strengthen the religious elements of their self-identity in a positive manner. Given this, the perceived image of a destination may be positively influenced by whether it conforms to the requirements of their religious self-identity or not[90]. 		The World Tourism Organization (UNWTO) forecasts that international tourism will continue growing at the average annual rate of 4%.[91] With the advent of e-commerce, tourism products have become one of the most traded items on the internet.[citation needed] Tourism products and services have been made available through intermediaries, although tourism providers (hotels, airlines, etc.), including small-scale operators, can sell their services directly.[92][93] This has put pressure on intermediaries from both on-line and traditional shops.		It has been suggested there is a strong correlation between tourism expenditure per capita and the degree to which countries play in the global context.[94] Not only as a result of the important economic contribution of the tourism industry, but also as an indicator of the degree of confidence with which global citizens leverage the resources of the globe for the benefit of their local economies. This is why any projections of growth in tourism may serve as an indication of the relative influence that each country will exercise in the future.		There has been a limited amount of orbital space tourism, with only the Russian Space Agency providing transport to date. A 2010 report into space tourism anticipated that it could become a billion dollar market by 2030.[95]		Since the late 1980s, sports tourism has become increasingly popular. Events such as rugby, Olympics, Commonwealth games, Asian Games and football World Cups have enabled specialist travel companies to gain official ticket allocation and then sell them in packages that include flights, hotels and excursions.		The focus on sport and spreading knowledge on the subject, especially more so recently, led to the increase in the sport tourism. Most notably, the international event such as the Olympics caused a shift in focus in the audience who now realize the variety of sports that exist in the world. In the United States, one of the most popular sports that usually are focused on was Football. This popularity was increased through major events like the World Cups. In Asian countries, the numerous football events also increased the popularity of football. But, it was the Olympics that brought together the different sports that led to the increase in sport tourism. The drastic interest increase in sports in general and not just one sport caught the attention of travel companies, who then began to sell flights in packages. Due to the low number of people who actually purchase these packages than predicted, the cost of these packages plummeted initially. As the number start to rise slightly the packages increased to regain the lost profits. With the certain economic state, the number of purchases decreased once again. The fluctuation in the number of packages sold was solely dependent on the economic situation, therefore, most travel companies were forced to set aside the plan to execute the marketing of any new package features.		As a result of the late-2000s recession, international arrivals suffered a strong slowdown beginning in June 2008. Growth from 2007 to 2008 was only 3.7% during the first eight months of 2008. This slowdown on international tourism demand was also reflected in the air transport industry, with a negative growth in September 2008 and a 3.3% growth in passenger traffic through September. The hotel industry also reported a slowdown, with room occupancy declining. In 2009 worldwide tourism arrivals decreased by 3.8%.[96] By the first quarter of 2009, real travel demand in the United States had fallen 6% over six quarters. While this is considerably milder than what occurred after the 9/11 attacks, the decline was at twice the rate as real GDP has fallen.[97][98]		However, evidence suggests that tourism as a global phenomenon shows no signs of substantially abating in the long term. It has been suggested that travel is necessary in order to maintain relationships, as social life is increasingly networked and conducted at a distance.[99] For many people vacations and travel are increasingly being viewed as a necessity rather than a luxury, and this is reflected in tourist numbers recovering some 6.6% globally over 2009, with growth up to 8% in emerging economies.[96]		
Rafting and white water rafting are recreational outdoor activities which use an inflatable raft to navigate a river or other body of water. This is often done on whitewater or different degrees of rough water. Dealing with risk and the need for teamwork is often a part of the experience.[1] This activity as a leisure sport has become popular since the 1950s, if not earlier, evolving from individuals paddling 10 feet (3.0 m) to 14 feet (4.3 m) rafts with double-bladed paddles or oars to multi-person rafts propelled by single-bladed paddles and steered by a person at the stern, or by the use of oars. [2] Rafting on some sections of rivers is considered an extreme sport, and can be fatal, while other sections are not so extreme or difficult. The International Rafting Federation, often referred to as the IRF, is the worldwide body which oversees all aspects of the sport.[3]						Otherwise known as the International Scale of River Difficulty, below are the six grades of difficulty in white water rafting. They range from simple to very dangerous and potential death or serious injuries.		Class 1: Very small rough areas, might require slight maneuvering. (Skill level: Very basic) Class 2: Some rough water, maybe some rocks, might require some maneuvering. (Skill level: Basic paddling skill) Class 3: Small waves, maybe a small drop, but no considerable danger. May require significant maneuvering. (Skill level: Some experience in rafting) Class 4: Whitewater, medium waves, maybe rocks, maybe a considerable drop, sharp maneuvers may be needed.(Skill level: Exceptional rafting experience) Class 5: Whitewater, large waves, large volume, possibility of large rocks and hazards, possibility of a large drop, requires precise maneuvering. (Skill level: Full mastery of rafting) Class 6: Class 6 rapids are considered to be so dangerous that they are effectively unnavigable on a reliably safe basis. Rafters can expect to encounter substantial whitewater, huge waves, huge rocks and hazards, and/or substantial drops that will impart severe impacts beyond the structural capacities and impact ratings of almost all rafting equipment. Traversing a Class 6 rapid has a dramatically increased likelihood of ending in serious injury or death compared to lesser classes. (Skill level: Full mastery of rafting, and even then it may not be safe)		Rafts in white water are very different vehicles than canoes or kayaks and have their own specific techniques to maneuver through whitewater obstacles. Examples of these techniques include.		White water rafting can be a dangerous sport, especially if basic safety precautions are not observed. That said, fatalities are rare in both commercial and do-it-yourself rafting.[4] Meta-analyses have calculated fatalities fell between 0.55[5] - 0.86%[6] per 100,000 user days. Studies have shown that injury rates in rafting are relatively low,[7] however may be skewed due to a large number of unreported incidents.[8] Typical rafting injuries include trauma from striking an object, traumatic stress from the interaction of the paddler’s positioning and equipment and the force of the water, overuse injuries, and submersion/environmental injuries, non environmental, undisclosed medical conditions (such as heart problems).[4]		Depending on the area, safety regulations covering rafting, both for the general do-it-yourself public as well as commercial operators, may exist in legislation. These range from the mandatory wearing of lifejackets, carrying certain equipment such as whistles and throwable flotation devices, to certification of commercial outfitters and their employees. It is generally advisable to discuss safety measures with a commercial rafting operator before signing on for that type of trip. The required equipment needed is essential information to be considered.		Like most outdoor sports, rafting in general has become safer over the years. Expertise in the sport has increased, and equipment has become more specialized and improved in quality. As a result, the difficulty rating of most river runs has changed. A classic example would be the Colorado River in the Grand Canyon, which historically had a reputation far exceeding its actual safety statistics. Today the Grand Canyon sees hundreds of safe rafting trips by both do-it-yourself rafters and commercial river concessionaires.[9]		Risks in white water rafting stem from both environmental dangers and from improper behavior. Certain features on rivers are inherently unsafe and have remained consistently so despite the passage of time. These would include ‘keeper hydraulics’, ‘strainers’ (e.g. fallen trees), dams (especially low-head dams, which tend to produce river-wide keeper hydraulics), undercut rocks, and of course dangerously high waterfalls. Even in safe areas, however, moving water can always present risks—such as when a swimmer attempts to stand up on a rocky riverbed in strong current, risking foot entrapment. Irresponsible behavior related to rafting while intoxicated has also contributed to many accidents. [10]		Rafting is not an amusement park ride. Rafting companies generally require customers to sign waiver forms indicating understanding and acceptance of potential serious risks. Both do-it-yourself and commercial rafting trips often begin with safety presentations to educate rafting participants about problems that may arise.		The overall risk level on a rafting trip using proper precautions is low.[11] Thousands of people safely enjoy raft trips every year.		Like all outdoor activities, rafting must balance its use of nature with the conservation of rivers as a natural resource and habitat. Because of these issues, some rivers now have regulations restricting the annual and daily operating times or numbers of rafters.		Conflicts have arisen when commercial rafting operators, often in co-operation with municipalities and tourism associations, alter the riverbed by dredging and/or blasting in order to eliminate safety hazards or create more interesting whitewater features in the river. Environmentalists argue that this may have negative impacts to riparian and aquatic ecosystems, while proponents claim these measures are usually only temporary, since a riverbed is naturally subject to permanent changes during large floods and other events. Another conflict involves the distribution of scarce river permits to either the do-it-yourself public or commercial rafting companies.[12]		Rafting by do-it-yourself rafters and commercial rafting companies contributes to the economy of many regions which in turn may contribute to the protection of rivers from hydroelectric power generation, diversion for irrigation, and other development. Additionally, white water rafting trips can promote environmentalism. Multi-day rafting trips by do-it-yourself rafters and commercial rafting companies through the National Wild and Scenic Rivers System have the potential to develop environmental stewardship and general environmental behavior. Studies suggest that environmental efficacy increases when there is an increase in the length of the trip, daily immersion, and the amount of resource education by trip participants. [13]		
Captain James Cook FRS (7 November 1728[NB 1] – 14 February 1779) was a British explorer, navigator, cartographer, and captain in the Royal Navy. Cook made detailed maps of Newfoundland prior to making three voyages to the Pacific Ocean, during which he achieved the first recorded European contact with the eastern coastline of Australia and the Hawaiian Islands, and the first recorded circumnavigation of New Zealand.		Cook joined the British merchant navy as a teenager and joined the Royal Navy in 1755. He saw action in the Seven Years' War, and subsequently surveyed and mapped much of the entrance to the Saint Lawrence River during the siege of Quebec. This helped bring Cook to the attention of the Admiralty and Royal Society. This notice came at a crucial moment in both Cook's career and the direction of British overseas exploration, and led to his commission in 1766 as commander of HM Bark Endeavour for the first of three Pacific voyages.		In three voyages Cook sailed thousands of miles across largely uncharted areas of the globe. He mapped lands from New Zealand to Hawaii in the Pacific Ocean in greater detail and on a scale not previously achieved. As he progressed on his voyages of discovery he surveyed and named features, and recorded islands and coastlines on European maps for the first time. He displayed a combination of seamanship, superior surveying and cartographic skills, physical courage and an ability to lead men in adverse conditions.		Cook was attacked and killed while attempting to kidnap the native chief of Hawaii during his third exploratory voyage in the Pacific in 1779.[1] He left a legacy of scientific and geographical knowledge which was to influence his successors well into the 20th century, and numerous memorials worldwide have been dedicated to him.						James Cook was born on 7 November 1728 (N.S.) in the village of Marton in Yorkshire and baptised on 14 November (N.S.) in the parish church of St Cuthbert, where his name can be seen in the church register.[2][3] He was the second of eight children of James Cook, a Scottish farm labourer from Ednam in Roxburghshire, and his locally born wife, Grace Pace, from Thornaby-on-Tees.[2][4][5] In 1736, his family moved to Airey Holme farm at Great Ayton, where his father's employer, Thomas Skottowe, paid for him to attend the local school. In 1741, after five years' schooling, he began work for his father, who had been promoted to farm manager. For leisure, he would climb a nearby hill, Roseberry Topping, enjoying the opportunity for solitude.[6] Cooks' Cottage, his parents' last home, which he is likely to have visited, is now in Melbourne, having been moved from England and reassembled, brick by brick, in 1934.[7]		In 1745, when he was 16, Cook moved 20 miles (32 km) to the fishing village of Staithes, to be apprenticed as a shop boy to grocer and haberdasher William Sanderson.[2] Historians have speculated that this is where Cook first felt the lure of the sea while gazing out of the shop window.[5]		After 18 months, not proving suitable for shop work, Cook travelled to the nearby port town of Whitby to be introduced to friends of Sanderson's, John and Henry Walker.[7] The Walkers, who were Quakers, were prominent local ship-owners in the coal trade. Their house is now the Captain Cook Memorial Museum. Cook was taken on as a merchant navy apprentice in their small fleet of vessels, plying coal along the English coast. His first assignment was aboard the collier Freelove, and he spent several years on this and various other coasters, sailing between the Tyne and London. As part of his apprenticeship, Cook applied himself to the study of algebra, geometry, trigonometry, navigation and astronomy—all skills he would need one day to command his own ship.[5]		His three-year apprenticeship completed, Cook began working on trading ships in the Baltic Sea. After passing his examinations in 1752, he soon progressed through the merchant navy ranks, starting with his promotion in that year to mate aboard the collier brig Friendship.[8] In 1755, within a month of being offered command of this vessel, he volunteered for service in the Royal Navy, when Britain was re-arming for what was to become the Seven Years' War. Despite the need to start back at the bottom of the naval hierarchy, Cook realised his career would advance more quickly in military service and entered the Navy at Wapping on 17 June 1755.[9]		Cook married Elizabeth Batts (1742–1835), the daughter of Samuel Batts, keeper of the Bell Inn, Wapping[10] and one of his mentors, on 21 December 1762 at St Margaret's Church, Barking, Essex.[11] The couple had six children: James (1763–94), Nathaniel (1764–80, lost aboard HMS Thunderer which foundered with all hands in a hurricane in the West Indies), Elizabeth (1767–71), Joseph (1768–68), George (1772–72) and Hugh (1776–93), the last of whom died of scarlet fever while a student at Christ's College, Cambridge. When not at sea, Cook lived in the East End of London. He attended St Paul's Church, Shadwell, where his son James was baptised. Cook has no known direct descendants—all his recorded children either pre-deceased him or died without issue.[12]		Cook's first posting was with HMS Eagle, serving as able seaman and master's mate under Captain Joseph Hamar for his first year aboard, and Captain Hugh Palliser thereafter.[13] In October and November 1755 he took part in Eagle's capture of one French warship and the sinking of another, following which he was promoted to boatswain in addition to his other duties.[9] His first temporary command was in March 1756 when he was briefly master of the Cruizer, a small cutter attached to the Eagle while on patrol.[9][14]		In June 1757 Cook passed his master's examinations at Trinity House, Deptford, which qualified him to navigate and handle a ship of the King's fleet.[15] He then joined the frigate HMS Solebay as master under Captain Robert Craig.[16]		During the Seven Years' War, Cook served in North America as master of Pembroke (1757).[17] In 1758 he took part in the major amphibious assault that captured the Fortress of Louisbourg from the French, after which he participated in the siege of Quebec City and then the Battle of the Plains of Abraham in 1759. He showed a talent for surveying and cartography, and was responsible for mapping much of the entrance to the Saint Lawrence River during the siege, thus allowing General Wolfe to make his famous stealth attack on the Plains of Abraham.[18]		Cook's surveying ability was put to good use mapping the jagged coast of Newfoundland in the 1760s, aboard HMS Grenville. He surveyed the north-west stretch in 1763 and 1764, the south coast between the Burin Peninsula and Cape Ray in 1765 and 1766, and the west coast in 1767. At this time Cook employed local pilots to point out the "rocks and hidden dangers" along the south and west coasts. During the 1765 season, four pilots were engaged at a daily pay of 4 shillings each: John Beck for the coast west of "Great St Lawrence", Morgan Snook for Fortune Bay, John Dawson for Connaigre and Hermitage Bay, and John Peck for the "Bay of Despair".[19]		His five seasons in Newfoundland produced the first large-scale and accurate maps of the island's coasts and were the first scientific, large scale, hydrographic surveys to use precise triangulation to establish land outlines.[20] They also gave Cook his mastery of practical surveying, achieved under often adverse conditions, and brought him to the attention of the Admiralty and Royal Society at a crucial moment both in his career and in the direction of British overseas discovery. Cook's map would be used into the 20th century—copies of it being referenced by those sailing Newfoundland's waters for 200 years.[21]		Following on from his exertions in Newfoundland, it was at this time that Cook wrote that he intended to go not only "farther than any man has been before me, but as far as I think it is possible for a man to go."[15]		In 1766, Admiralty engaged Cook to command a scientific voyage to the Pacific Ocean. The purpose of the voyage was to observe and record the transit of Venus across the Sun for the benefit of a Royal Society inquiry into a means of determining longitude.[22] Cook, at the age of 39, was promoted to lieutenant to grant him sufficient status to take the command.[23][24] For its part the Royal Society agreed that Cook would receive a one hundred guinea gratuity in addition to his Naval pay.[25]		The expedition sailed aboard the HMS Endeavour, departing England on 26 August 1768.[26] Cook and his crew rounded Cape Horn and continued westward across the Pacific to arrive at Tahiti on 13 April 1769, where the observations of the Venus Transit were made.[27] However, the result of the observations was not as conclusive or accurate as had been hoped. Once the observations were completed, Cook opened the sealed orders which were additional instructions from the Admiralty for the second part of his voyage: to search the south Pacific for signs of the postulated rich southern continent of Terra Australis.[28] Cook then sailed to New Zealand and mapped the complete coastline, making only some minor errors. He then voyaged west, reaching the south-eastern coast of Australia on 19 April 1770, and in doing so his expedition became the first recorded Europeans to have encountered its eastern coastline.[NB 2]		On 23 April he made his first recorded direct observation of indigenous Australians at Brush Island near Bawley Point, noting in his journal: "...and were so near the Shore as to distinguish several people upon the Sea beach they appear'd to be of a very dark or black Colour but whether this was the real colour of their skins or the C[l]othes they might have on I know not."[29] On 29 April Cook and crew made their first landfall on the mainland of the continent at a place now known as the Kurnell Peninsula. Cook originally christened the area as "Stingray Bay", but he later crossed it out and named it "Botany Bay"[30] after the unique specimens retrieved by the botanists Joseph Banks and Daniel Solander. It is here that James Cook made first contact with an aboriginal tribe known as the Gweagal.[31]		After his departure from Botany Bay he continued northwards. On 11 June a mishap occurred when HMS Endeavour ran aground on a shoal of the Great Barrier Reef, and then "nursed into a river mouth on 18 June 1770".[32] The ship was badly damaged and his voyage was delayed almost seven weeks while repairs were carried out on the beach (near the docks of modern Cooktown, Queensland, at the mouth of the Endeavour River).[5] The voyage then continued, sailing through Torres Strait and on 22 August Cook landed on Possession Island, where he claimed the entire coastline that he had just explored as British territory. He returned to England via Batavia (modern Jakarta, Indonesia where many in his crew succumbed to malaria), the Cape of Good Hope, arriving on the island of Saint Helena on 12 July 1771.[33]		Cook's journals were published upon his return, and he became something of a hero among the scientific community. Among the general public, however, the aristocratic botanist Joseph Banks was a greater hero.[5] Banks even attempted to take command of Cook's second voyage, but removed himself from the voyage before it began, and Johann Reinhold Forster and his son Georg Forster were taken on as scientists for the voyage. Cook's son George was born five days before he left for his second voyage.[34]		Shortly after his return from the first voyage, Cook was promoted in August 1771, to the rank of commander.[35][36] In 1772 he was commissioned to lead another scientific expedition on behalf of the Royal Society, to search for the hypothetical Terra Australis. On his first voyage, Cook had demonstrated by circumnavigating New Zealand that it was not attached to a larger landmass to the south. Although he charted almost the entire eastern coastline of Australia, showing it to be continental in size, the Terra Australis was believed to lie further south. Despite this evidence to the contrary, Alexander Dalrymple and others of the Royal Society still believed that a massive southern continent should exist.[37]		Cook commanded HMS Resolution on this voyage, while Tobias Furneaux commanded its companion ship, HMS Adventure. Cook's expedition circumnavigated the globe at an extreme southern latitude, becoming one of the first to cross the Antarctic Circle (17 January 1773). In the Antarctic fog, Resolution and Adventure became separated. Furneaux made his way to New Zealand, where he lost some of his men during an encounter with Māori, and eventually sailed back to Britain, while Cook continued to explore the Antarctic, reaching 71°10'S on 31 January 1774.[15]		Cook almost encountered the mainland of Antarctica, but turned towards Tahiti to resupply his ship. He then resumed his southward course in a second fruitless attempt to find the supposed continent. On this leg of the voyage he brought a young Tahitian named Omai, who proved to be somewhat less knowledgeable about the Pacific than Tupaia had been on the first voyage. On his return voyage to New Zealand in 1774, Cook landed at the Friendly Islands, Easter Island, Norfolk Island, New Caledonia, and Vanuatu.		Before returning to England, Cook made a final sweep across the South Atlantic from Cape Horn and surveyed, mapped and took possession for Britain of South Georgia, which had been explored by Anthony de la Roché in 1675. Cook also discovered and named Clerke Rocks and the South Sandwich Islands ("Sandwich Land"). He then turned north to South Africa, and from there continued back to England. His reports upon his return home put to rest the popular myth of Terra Australis.[38]		Cook's second voyage marked a successful employment of Larcum Kendall's K1 copy of John Harrison's H4 marine chronometer, which enabled Cook to calculate his longitudinal position with much greater accuracy. Cook's log was full of praise for this time-piece which he used to make charts of the southern Pacific Ocean that were so remarkably accurate that copies of them were still in use in the mid-20th century.[39]		Upon his return, Cook was promoted to the rank of post-captain and given an honorary retirement from the Royal Navy, with a posting as an officer of the Greenwich Hospital. He reluctantly accepted, insisting that he be allowed to quit the post if an opportunity for active duty should arise.[40] His fame extended beyond the Admiralty; he was made a Fellow of the Royal Society, and awarded the Copley Gold Medal for completing his second voyage without losing a man to scurvy.[41] Nathaniel Dance-Holland painted his portrait; he dined with James Boswell; he was described in the House of Lords as "the first navigator in Europe".[15] But he could not be kept away from the sea. A third voyage was planned and Cook volunteered to find the Northwest Passage. He travelled to the Pacific and hoped to travel east to the Atlantic, while a simultaneous voyage travelled the opposite route.[42]		On his last voyage, Cook again commanded HMS Resolution, while Captain Charles Clerke commanded HMS Discovery. The voyage was ostensibly planned to return the Pacific Islander, Omai to Tahiti, or so the public were led to believe. The trip's principal goal was to locate a Northwest Passage around the American continent.[43] After dropping Omai at Tahiti, Cook travelled north and in 1778 became the first European to begin formal contact with the Hawaiian Islands.[44] After his initial landfall in January 1778 at Waimea harbour, Kauai, Cook named the archipelago the "Sandwich Islands" after the fourth Earl of Sandwich—the acting First Lord of the Admiralty.[45]		From the Sandwich Islands Cook sailed north and then north-east to explore the west coast of North America north of the Spanish settlements in Alta California. He made landfall on the Oregon coast at approximately 44°30′ north latitude, naming his landing point Cape Foulweather. Bad weather forced his ships south to about 43° north before they could begin their exploration of the coast northward.[46] He unknowingly sailed past the Strait of Juan de Fuca, and soon after entered Nootka Sound on Vancouver Island. He anchored near the First Nations village of Yuquot. Cook's two ships remained in Nootka Sound from 29 March to 26 April 1778, in what Cook called Ship Cove, now Resolution Cove,[47] at the south end of Bligh Island, about 5 miles (8 km) east across Nootka Sound from Yuquot, lay a Nuu-chah-nulth village (whose chief Cook did not identify but may have been Maquinna). Relations between Cook's crew and the people of Yuquot were cordial if sometimes strained. In trading, the people of Yuquot demanded much more valuable items than the usual trinkets that had worked in Hawaii. Metal objects were much desired, but the lead, pewter, and tin traded at first soon fell into disrepute. The most valuable items which the British received in trade were sea otter pelts. During the stay, the Yuquot "hosts" essentially controlled the trade with the British vessels; the natives usually visited the British vessels at Resolution Cove instead of the British visiting the village of Yuquot at Friendly Cove.[48]		After leaving Nootka Sound, Cook explored and mapped the coast all the way to the Bering Strait, on the way identifying what came to be known as Cook Inlet in Alaska. In a single visit, Cook charted the majority of the North American north-west coastline on world maps for the first time, determined the extent of Alaska, and closed the gaps in Russian (from the West) and Spanish (from the South) exploratory probes of the Northern limits of the Pacific.[15]		By the second week of August 1778 Cook was through the Bering Strait, sailing into the Chukchi Sea. He headed north-east up the coast of Alaska until he was blocked by sea ice. His furthest north was 70 degrees 44 minutes. Cook then sailed west to the Siberian coast, and then south-east down the Siberian coast back to the Bering Strait. By early September 1778 he was back in the Bering Sea to begin the trip to the Sandwich (Hawaiian) Islands.[49] He became increasingly frustrated on this voyage, and perhaps began to suffer from a stomach ailment; it has been speculated that this led to irrational behaviour towards his crew, such as forcing them to eat walrus meat, which they had pronounced inedible.[50]		Cook returned to Hawaii in 1779. After sailing around the archipelago for some eight weeks, he made landfall at Kealakekua Bay, on 'Hawaii Island', largest island in the Hawaiian Archipelago. Cook's arrival coincided with the Makahiki, a Hawaiian harvest festival of worship for the Polynesian god Lono. Coincidentally the form of Cook's ship, HMS Resolution, or more particularly the mast formation, sails and rigging, resembled certain significant artefacts that formed part of the season of worship.[5][50] Similarly, Cook's clockwise route around the island of Hawaii before making landfall resembled the processions that took place in a clockwise direction around the island during the Lono festivals. It has been argued (most extensively by Marshall Sahlins) that such coincidences were the reasons for Cook's (and to a limited extent, his crew's) initial deification by some Hawaiians who treated Cook as an incarnation of Lono.[51] Though this view was first suggested by members of Cook's expedition, the idea that any Hawaiians understood Cook to be Lono, and the evidence presented in support of it, were challenged in 1992.[50][52]		After a month's stay, Cook attempted to resume his exploration of the Northern Pacific. Shortly after leaving Hawaii Island, however, the Resolution's foremast broke, so the ships returned to Kealakekua Bay for repairs.		Tensions rose, and a number of quarrels broke out between the Europeans and Hawaiians at Kealakekua Bay. An unknown group of Hawaiians took one of Cook's small boats. The evening when the cutter was taken, the people had become "insolent" even with threats to fire upon them. Cook was forced into a wild goose chase that ended with his return to the ship frustrated.[54] He attempted to kidnap and ransom the King of Hawaiʻi, Kalaniʻōpuʻu.		That following day, 14 February 1779, Cook marched through the village to retrieve the King. Cook took the King (aliʻi nui) by his own hand and led him willingly away. One of Kalaniʻōpuʻu's favorite wives, Kanekapolei and two chiefs approached the group as they were heading to boats. They pleaded with the king not to go until he stopped and sat where he stood. An old kahuna (priest), chanting rapidly while holding out a coconut, attempted to distract Cook and his men as a large crowd began to form at the shore. The king began to understand that Cook was his enemy.[54] As Cook turned his back to help launch the boats, he was struck on the head by the villagers and then stabbed to death as he fell on his face in the surf.[55] He was first struck on the head with a club by a chief named Kalaimanokahoʻowaha or Kanaʻina (namesake of Charles Kana'ina) and then stabbed by one of the king's attendants, Nuaa.[56][57] The Hawaiians carried his body away towards the back of the town, still visible to the ship through their spyglass. Four marines, Corporal James Thomas, Private Theophilus Hinks, Private Thomas Fatchett and Private John Allen, were also killed and two others were wounded in the confrontation.[56][58]		The esteem which the islanders nevertheless held for Cook caused them to retain his body. Following their practice of the time, they prepared his body with funerary rituals usually reserved for the chiefs and highest elders of the society. The body was disembowelled, baked to facilitate removal of the flesh, and the bones were carefully cleaned for preservation as religious icons in a fashion somewhat reminiscent of the treatment of European saints in the Middle Ages. Some of Cook's remains, thus preserved, were eventually returned to his crew for a formal burial at sea.[59]		Clerke assumed leadership of the expedition, and made a final attempt to pass through the Bering Strait.[60] He died from tuberculosis on 22 August 1779 and John Gore, a veteran of Cook's first voyage, took command of Resolution and of the expedition. James King replaced Gore in command of Discovery.[61] The expedition returned home, reaching England in October 1780. After their arrival in England, King completed Cook's account of the voyage.[citation needed]		David Samwell, who sailed with Cook on Resolution, wrote of him: "He was a modest man, and rather bashful; of an agreeable lively conversation, sensible and intelligent. In temper he was somewhat hasty, but of a disposition the most friendly, benevolent and humane. His person was above six feet high: and, though a good looking man, he was plain both in dress and appearance. His face was full of expression: his nose extremely well shaped: his eyes which were small and of a brown cast, were quick and piercing; his eyebrows prominent, which gave his countenance altogether an air of austerity."[62]		The Australian Museum acquired its Cook Collection in 1894 from the Government of New South Wales. At that time the collection consisted of 115 artefacts collected on Cook's three voyages throughout the Pacific Ocean, during the period 1768–80, along with documents and memorabilia related to these voyages. Many of the ethnographic artifacts were collected at a time of first contact between Pacific Peoples and Europeans. In 1935 most of the documents and memorabilia were transferred to the Mitchell Library in the State Library of New South Wales. The provenance of the collection shows that the objects remained in the hands of Cook's widow Elizabeth Cook, and her descendants, until 1886. In this year John Mackrell, the great-nephew of Isaac Smith, Elizabeth Cook's cousin, organised the display of this collection at the request of the NSW Government at the Colonial and Indian Exhibition in London. In 1887 the London-based Agent-General for the New South Wales Government, Saul Samuel, bought John Mackrell's items and also acquired items belonging to the other relatives Reverend Canon Frederick Bennett, Mrs Thomas Langton, H. M. C. Alexander, and William Adams. The collection remained with the Colonial Secretary of NSW until 1894, when it was transferred to the Australian Museum.[63]		Cook's 12 years sailing around the Pacific Ocean contributed much to European knowledge of the area. Several islands such as the Sandwich Islands (Hawaii) were encountered for the first time by Europeans, and his more accurate navigational charting of large areas of the Pacific was a major achievement.[64]		To create accurate maps, latitude and longitude must be accurately determined. Navigators had been able to work out latitude accurately for centuries by measuring the angle of the sun or a star above the horizon with an instrument such as a backstaff or quadrant. Longitude was more difficult to measure accurately because it requires precise knowledge of the time difference between points on the surface of the earth. The Earth turns a full 360 degrees relative to the sun each day. Thus longitude corresponds to time: 15 degrees every hour, or 1 degree every 4 minutes.[65]		Cook gathered accurate longitude measurements during his first voyage due to his navigational skills, the help of astronomer Charles Green and by using the newly published Nautical Almanac tables, via the lunar distance method—measuring the angular distance from the moon to either the sun during daytime or one of eight bright stars during night-time to determine the time at the Royal Observatory, Greenwich, and comparing that to his local time determined via the altitude of the sun, moon, or stars. On his second voyage Cook used the K1 chronometer made by Larcum Kendall, which was the shape of a large pocket watch, 5 inches (13 cm) in diameter. It was a copy of the H4 clock made by John Harrison, which proved to be the first to keep accurate time at sea when used on the ship Deptford's journey to Jamaica, 1761–62.[66]		Cook succeeded in circumnavigating the world on his first voyage without losing a single man to scurvy, an unusual accomplishment at the time. He tested several preventive measures but the most important was frequent replenishment of fresh food.[67] It was for presenting a paper on this aspect of the voyage to the Royal Society that he was presented with the Copley Medal in 1776.[68][69] Ever the observer, Cook was the first European to have extensive contact with various people of the Pacific. He correctly postulated a link among all the Pacific peoples, despite their being separated by great ocean stretches (see Malayo-Polynesian languages). Cook theorised that Polynesians originated from Asia, which scientist Bryan Sykes later verified.[70] In New Zealand the coming of Cook is often used to signify the onset of colonisation.[5][7]		Cook carried several scientists on his voyages; they made significant observations and discoveries. Two botanists, Joseph Banks and Swede Daniel Solander, were on the first voyage. The two collected over 3,000 plant species.[71] Banks subsequently strongly promoted British settlement of Australia.[72][73]		Artists also sailed on Cook's first voyage. Sydney Parkinson was heavily involved in documenting the botanists' findings, completing 264 drawings before his death near the end of the voyage. They were of immense scientific value to British botanists.[5][74] Cook's second expedition included William Hodges, who produced notable landscape paintings of Tahiti, Easter Island, and other locations.		Several officers who served under Cook went on to distinctive accomplishments. William Bligh, Cook's sailing master, was given command of HMS Bounty in 1787 to sail to Tahiti and return with breadfruit. Bligh is most known for the mutiny of his crew which resulted in his being set adrift in 1789. He later became governor of New South Wales, where he was subject of another mutiny—the Rum Rebellion was the only successful armed takeover of an Australian government.[75] George Vancouver, one of Cook's midshipmen, led a voyage of exploration to the Pacific Coast of North America from 1791 to 1794.[76] In honour of his former commander, Vancouver's ship was named HMS Discovery (1789). George Dixon, who sailed under Cook on his third expedition, later commanded his own.[77] A lieutenant under Cook, Henry Roberts, spent many years after that voyage preparing the detailed charts that went into Cook's posthumous Atlas, published around 1784.		Cook's contributions to knowledge were internationally recognised during his lifetime. In 1779, while the American colonies were fighting Britain for their independence, Benjamin Franklin wrote to captains of colonial warships at sea, recommending that if they came into contact with Cook's vessel, they were to "not consider her an enemy, nor suffer any plunder to be made of the effects contained in her, nor obstruct her immediate return to England by detaining her or sending her into any other part of Europe or to America; but that you treat the said Captain Cook and his people with all civility and kindness, ... as common friends to mankind."[78] Unknown to Franklin, Cook had met his death a month before this safe conduct "passport" was written.		Cook's voyages were involved in another unusual first. The first recorded circumnavigation of the world by an animal was by Cook's goat, who made that memorable journey twice; the first time on HMS Dolphin, under Samuel Wallis, and then aboard Endeavour. When they returned to England, Cook had the goat presented with a silver collar engraved with lines from Samuel Johnson: Perpetui, ambita bis terra, praemia lactis Haec habet altrici Capra secunda Jovis. She was put to pasture on Cook's farm outside London, and was reportedly admitted to the privileges of the Royal Naval hospital at Greenwich. Cook's journal recorded the date of the goat's death: 28 March 1772.[79]		A US coin, the 1928 Hawaiian Sesquicentennial half dollar carries Cook's image. Minted for the 150th anniversary of his discovery of the islands, its low mintage (10,008) has made this example of Early United States commemorative coins both scarce and expensive.[80] The site where he was killed in Hawaii was marked in 1874 by a white obelisk set on 25 square feet (2.3 m2) of chained-off beach. This land, although in Hawaii, was deeded to the United Kingdom.[81][not in citation given] A nearby town is named Captain Cook, Hawaii; several Hawaiian businesses also carry his name. The Apollo 15 Command/Service Module Endeavour was named after Cook's ship, HMS Endeavour,[82] as was the space shuttle Space Shuttle Endeavour.[83] Another shuttle, Discovery, was named after Cook's HMS Discovery.[84]		The first institution of higher education in North Queensland, Australia was named after him, with James Cook University opening in Townsville in 1970.[85] In Australian rhyming slang the expression "Captain Cook" means "look".[86] Numerous institutions, landmarks and place names reflect the importance of Cook's contributions, including the Cook Islands, the Cook Strait, Cook Inlet, and the Cook crater on the Moon.[87] Aoraki/Mount Cook, the highest summit in New Zealand, is named for him.[88] Another Mount Cook is on the border between the US state of Alaska and the Canadian Yukon Territory, and is designated Boundary Peak 182 as one of the official Boundary Peaks of the Hay–Herbert Treaty.[89]		One of the earliest monuments to Cook in the United Kingdom is located at The Vache, erected in 1780 by Admiral Hugh Palliser, a contemporary of Cook and one-time owner of the estate.[90] A huge obelisk was built in 1827 as a monument to Cook on Easby Moor overlooking his boyhood village of Great Ayton,[91] along with a smaller monument at the former location of Cook's cottage.[92] There is also a monument to Cook in the church of St Andrew the Great, St Andrew's Street, Cambridge, where his son Hugh, a student at Christ's College, was buried. Cook's widow Elizabeth was also buried in the church and in her will left money for the memorial's upkeep. The 250th anniversary of Cook's birth was marked at the site of his birthplace in Marton, by the opening of the Captain Cook Birthplace Museum, located within Stewart Park (1978). A granite vase just to the south of the museum marks the approximate spot where he was born.[93] Tributes also abound in post-industrial Middlesbrough, including a primary school,[94] shopping square[95] and the Bottle 'O Notes, a public artwork by Claes Oldenburg, that was erected in the town's Central Gardens in 1993. Also named after Cook is the James Cook University Hospital, a major teaching hospital which opened in 2003 with a railway station serving it called James Cook opening in 2014.[96] The Royal Research Ship RRS James Cook was built in 2006 to replace the RRS Charles Darwin in the UK's Royal Research Fleet,[97] and Stepney Historical Trust placed a plaque on Free Trade Wharf in the Highway, Shadwell to commemorate his life in the East End of London. In 2002 Cook was placed at number 12 in the BBC's poll of the 100 Greatest Britons.[98]		In the 19th century, the death of Cook was of interest to artists and writers because its brutality was a contrast to the idea that primitive humans were naturally good-natured – the "noble savage". There were no eye-witness accounts of the death and so the incident was open to artistic interpretation and myth-making.[99]		Later, in the 20th century Captain Cook was memorialized in Gene Roddenberry's television series, Star Trek: The Original Series. The lead character, James T. Kirk, played the role of the captain of a scientific ship of exploration, the starship USS Enterprise. The premise of the series was somewhat loosely based on the 18th-century exploits of Captain James Cook aboard the HMS Endeavour. This same series had been previously attempted by Roddenberry, having reached the production stage of a pilot episode called "The Cage", but had been turned down by Desilu Productions after its first pilot episode presentation. It was only after Roddenberry added the clear modeling of the series on the exploits of Captain Cook in its second pilot episode presentation (called "Where No Man Has Gone Before") that the series was given the "go ahead" by the studio.[100]		
Sport (British English) or sports (American English) includes all forms of competitive physical activity or games which,[1] through casual or organised participation, aim to use, maintain or improve physical ability and skills while providing enjoyment to participants, and in some cases, entertainment for spectators.[2] Usually the contest or game is between two sides, each attempting to exceed the other. Some sports allow a tie game; others provide tie-breaking methods, to ensure one winner and one loser. A number of such two-sided contests may be arranged in a tournament producing a champion. Many sports leagues make an annual champion by arranging games in a regular sports season, followed in some cases by playoffs. Hundreds of sports exist, from those between single contestants, through to those with hundreds of simultaneous participants, either in teams or competing as individuals. In certain sports such as racing, many contestants may compete, each against each other, with one winner.		Sport is generally recognised as system of activities which are based in physical athleticism or physical dexterity, with the largest major competitions such as the Olympic Games admitting only sports meeting this definition,[3] and other organisations such as the Council of Europe using definitions precluding activities without a physical element from classification as sports.[2] However, a number of competitive, but non-physical, activities claim recognition as mind sports. The International Olympic Committee (through ARISF) recognises both chess and bridge as bona fide sports, and SportAccord, the international sports federation association, recognises five non-physical sports: bridge, chess, draughts (checkers), Go and xiangqi,[4][5] and limits the number of mind games which can be admitted as sports.[1]		Sports are usually governed by a set of rules or customs, which serve to ensure fair competition, and allow consistent adjudication of the winner. Winning can be determined by physical events such as scoring goals or crossing a line first. It can also be determined by judges who are scoring elements of the sporting performance, including objective or subjective measures such as technical performance or artistic impression.		Records of performance are often kept, and for popular sports, this information may be widely announced or reported in sport news. Sport is also a major source of entertainment for non-participants, with spectator sport drawing large crowds to sport venues, and reaching wider audiences through broadcasting. Sports betting is in some cases severely regulated, and in some cases is central to the sport.		According to A.T. Kearney, a consultancy, the global sporting industry is worth up to $620 billion as of 2013.[6] The world's most accessible and practiced sport is running, while association football is its most popular spectator sport.[7][8]						The word "Sport" comes from the Old French desport meaning "leisure", with the oldest definition in English from around 1300 being "anything humans find amusing or entertaining".[9]		Other meanings include gambling and events staged for the purpose of gambling; hunting; and games and diversions, including ones that require exercise.[10] Roget's defines the noun sport as an "activity engaged in for relaxation and amusement" with synonyms including diversion and recreation.[11]		The singular term "sport" is used in most English dialects to describe the overall concept (e.g. "children taking part in sport"), with "sports" used to describe multiple activities (e.g. "football and rugby are the most popular sports in England"). American English uses "sports" for both terms.		The precise definition of what separates a sport from other leisure activities varies between sources. The closest to an international agreement on a definition is provided by SportAccord, which is the association for all the largest international sports federations (including association football, athletics, cycling, tennis, equestrian sports, and more), and is therefore the de facto representative of international sport.		SportAccord uses the following criteria, determining that a sport should:[1]		They also recognise that sport can be primarily physical (such as rugby or athletics), primarily mind (such as chess or go), predominantly motorised (such as Formula 1 or powerboating), primarily co-ordination (such as billiard sports), or primarily animal-supported (such as equestrian sport).[1]		The inclusion of mind sports within sport definitions has not been universally accepted, leading to legal challenges from governing bodies in regards to being denied funding available to sports.[12] Whilst SportAccord recognises a small number of mind sports, it is not open to admitting any further mind sports.		There has been an increase in the application of the term "sport" to a wider set of non-physical challenges such as video games, also called esports, especially due to the large scale of participation and organised competition, but these are not widely recognised by mainstream sports organisations. According to Council of Europe, European Sports Charter, article 2.i, " "Sport" means all forms of physical activity which, through casual or organised participation, aim at expressing or improving physical fitness and mental well-being, forming social relationships or obtaining results in competition at all levels.".[13]		There are opposing views on the necessity of competition as a defining element of a sport, with almost all professional sport involving competition, and governing bodies requiring competition as a prerequisite of recognition by the International Olympic Committee (IOC) or SportAccord.[1]		Other bodies advocate widening the definition of sport to include all physical activity. For instance, the Council of Europe include all forms of physical exercise, including those competed just for fun.		In order to widen participation, and reduce the impact of losing on less able participants, there has been an introduction of non-competitive physical activity to traditionally competitive events such as school sports days, although moves like this are often controversial.[14][15]		In competitive events, participants are graded or classified based on their "result" and often divided into groups of comparable performance, (e.g. gender, weight and age). The measurement of the result may be objective or subjective, and corrected with "handicaps" or penalties. In a race, for example, the time to complete the course is an objective measurement. In gymnastics or diving the result is decided by a panel of judges, and therefore subjective. There are many shades of judging between boxing and mixed martial arts, where victory is assigned by judges if neither competitor has lost at the end of the match time.		Artifacts and structures suggest sport in China as early as 2000 BC.[16] Gymnastics appears to have been popular in China's ancient past. Monuments to the Pharaohs indicate that a number of sports, including swimming and fishing, were well-developed and regulated several thousands of years ago in ancient Egypt.[17] Other Egyptian sports included javelin throwing, high jump, and wrestling. Ancient Persian sports such as the traditional Iranian martial art of Zourkhaneh had a close connection to warfare skills.[18] Among other sports that originate in ancient Persia are polo and jousting.		A wide range of sports were already established by the time of Ancient Greece and the military culture and the development of sports in Greece influenced one another considerably. Sports became such a prominent part of their culture that the Greeks created the Olympic Games, which in ancient times were held every four years in a small village in the Peloponnesus called Olympia.[19]		Sports have been increasingly organised and regulated from the time of the ancient Olympics up to the present century. Industrialisation has brought increased leisure time, letting people attend and follow spectator sports and participate in athletic activities. These trends continued with the advent of mass media and global communication. Professionalism became prevalent, further adding to the increase in sport's popularity, as sports fans followed the exploits of professional athletes — all while enjoying the exercise and competition associated with amateur participation in sports. Since the turn of the 21st century, there has been increasing debate about whether transgender sportpersons should be able to participate in sport events that conform with their post-transition gender identity.[20]		Sportsmanship is an attitude that strives for fair play, courtesy toward teammates and opponents, ethical behaviour and integrity, and grace in victory or defeat.[21][22][23]		Sportsmanship expresses an aspiration or ethos that the activity will be enjoyed for its own sake. The well-known sentiment by sports journalist Grantland Rice, that it's "not that you won or lost but how you played the game", and the modern Olympic creed expressed by its founder Pierre de Coubertin: "The most important thing... is not winning but taking part" are typical expressions of this sentiment.		Key principles of sport include that the result should not be predetermined, and that both sides should have equal opportunity to win. Rules are in place to ensure that fair play to occur, but participants can break these rules in order to gain advantage.		Participants may choose to cheat in order to satisfy their desire to win, or in order to achieve an ulterior motive. The widespread existence of gambling on the results of sports fixtures creates the motivation for match fixing, where a participant or participants deliberately work to ensure a given outcome.		The competitive nature of sport encourages some participants to attempt to enhance their performance through the use of medicines, or through other means such as increasing the volume of blood in their bodies through artificial means.		All sports recognised by the IOC or SportAccord are required to implement a testing programme, looking for a list of banned drugs, with suspensions or bans being placed on participants who test positive for banned substances.		Violence in sports involves crossing the line between fair competition and intentional aggressive violence. Athletes, coaches, fans, and parents sometimes unleash violent behaviour on people or property, in misguided shows of loyalty, dominance, anger, or celebration. Rioting or hooliganism by fans in particular is a problem at some national and international sporting contests.		Female participation in sports continues to rise alongside the opportunity for involvement and the value of sports for child development and physical fitness. Despite gains during the last three decades, a gap persists in the enrollment figures between male and female players. Female players account for 39% of the total participation in US interscholastic athletics. Gender balance has been accelerating from a 32% increase in 1973–74 to a 63% increase in 1994–95. Hessel (2000).[full citation needed]		Youth sports present children with opportunities for fun, socialization, forming peer relationships, physical fitness, and athletic scholarships. Activists for education and the war on drugs encourage youth sports as a means to increase educational participation and to fight the illegal drug trade. According to the Center for Injury Research and Policy at Nationwide Children's Hospital, the biggest risk for youth sports is death or serious injury including concussion. These risks come from running, basketball, association football, volleyball, gridiron, gymnastics, and ice hockey.[24]		Disabled sports also adaptive sports or parasports, are sports played by persons with a disability, including physical and intellectual disabilities. As many of these based on existing sports modified to meet the needs of persons with a disability, they are sometimes referred to as adapted sports. However, not all disabled sports are adapted; several sports that have been specifically created for persons with a disability have no equivalent in able-bodied sports.		The competition element of sport, along with the aesthetic appeal of some sports, result in the popularity of people attending to watch sport being played. This has led to the specific phenomenon of spectator sport.		Both amateur and professional sports attract spectators, both in person at the sport venue, and through broadcast mediums including radio, television and internet broadcast. Both attendance in person and viewing remotely can incur a sometimes substantial charge, such as an entrance ticket, or pay-per-view television broadcast.		It is common for popular sports to attract large broadcast audiences, leading to rival broadcasters bidding large amounts of money for the rights to show certain fixtures. The football World Cup attracts a global television audience of hundreds of millions; the 2006 final alone attracted an estimated worldwide audience of well over 700 million and the 2011 Cricket World Cup Final attracted an estimated audience of 135 million in India alone .[25]		In the United States, the championship game of the NFL, the Super Bowl, has become one of the most watched television broadcasts of the year.[26][27] Super Bowl Sunday is a de facto national holiday in America;[28][29] the viewership being so great that in 2015, advertising space was reported as being sold at $4.5m for a 30-second slot.[26]		Sport can be undertaken on an amateur, professional or semi-professional basis, depending on whether participants are incentivised for participation (usually through payment of a wage or salary). Amateur participation in sport at lower levels is often called "grassroots sport".[2][30]		The popularity of spectator sport as a recreation for non-participants has led to sport becoming a major business in its own right, and this has incentivised a high paying professional sport culture, where high performing participants are rewarded with pay far in excess of average wages, which can run into millions of dollars.[31]		Some sports, or individual competitions within a sport, retain a policy of allowing only amateur sport. The Olympic Games started with a principle of amateur competition with those who practiced a sport professionally considered to have an unfair advantage over those who practiced it merely as a hobby.[32] From 1971, Olympic athletes were allowed to receive compensation and sponsorship,[33] and from 1986, the IOC decided to make all professional athletes eligible for the Olympics,[33][34] with the exceptions of boxing,[35][36] and wrestling.[37][38]		Technology plays an important part in modern sports. With it being a necessary part of some sports (such as motorsport), it is used in others to improve performance. Some sports also use it to allow off-field decision making.		Sports science is a widespread academic discipline, and can be applied to areas including athlete performance, such as the use of video analysis to fine-tune technique, or to equipment, such as improved running shoes or competitive swimwear. Sports engineering emerged as a discipline in 1998 with an increasing focus not just on materials design but also the use of technology in sport, from analytics and big data to wearable technology.[39] In order to control the impact of technology on fair play, governing bodies frequently have specific rules that are set to control the impact of technical advantage between participants. For example, in 2010, full-body, non-textile swimsuits were banned by FINA, as they were enhancing swimmers' performances.[40][41]		The increase in technology has also allowed many decisions in sports matches to be taken, or reviewed, off-field, with another official using instant replays to make decisions. In some sports, players can now challenge decisions made by officials. In football, Goal-line technology makes decisions on whether a ball has crossed the goal line or not.[42] The technology is not compulsory,[43] but was used in the 2014 FIFA World Cup in Brazil,[44] and the 2015 FIFA Women's World Cup in Canada,[45] as well as in the Premier League from 2013–14,[46] and the Bundesliga from 2015–16.[47] In the NFL, a referee can ask for a review from the replay booth, or a head coach can issue a challenge to review the play using replays. The final decision rests with the referee.[48] A video referee (commonly known as a Television Match Official or TMO) can also use replays to help decision-making in rugby (both league and union).[49][50] In international cricket, an umpire can ask the Third umpire for a decision, and the third umpire makes the final decision.[51][52] Since 2008, a decision review system for players to review decisions has been introduced and used in ICC-run tournaments, and optionally in other matches.[51][53] Depending on the host broadcaster, a number of different technologies are used during an umpire or player review, including instant replays, Hawk-Eye, Hot Spot and Real Time Snickometer.[54][55] Hawk-Eye is also used in tennis to challenge umpiring decisions.[56][57]		Sports and politics can influence each other greatly.		Benito Mussolini used the 1934 FIFA World Cup, which was held in Italy, to showcase Fascist Italy.[58][59] Adolf Hitler also used the 1936 Summer Olympics held in Berlin, and the 1936 Winter Olympics held in Garmisch-Partenkirchen, to promote the Nazi ideology of the superiority of the Aryan race, and inferiority of the Jews and other "undesirables".[59][60] Germany used the Olympics to give of itself a peaceful image while it was very actively preparing the war.[61]		When apartheid was the official policy in South Africa, many sports people, particularly in rugby union, adopted the conscientious approach that they should not appear in competitive sports there. Some feel this was an effective contribution to the eventual demolition of the policy of apartheid, others feel that it may have prolonged and reinforced its worst effects.[62]		In the history of Ireland, Gaelic sports were connected with cultural nationalism. Until the mid 20th century a person could have been banned from playing Gaelic football, hurling, or other sports administered by the Gaelic Athletic Association (GAA) if she/he played or supported football, or other games seen to be of British origin. Until recently the GAA continued to ban the playing of football and rugby union at Gaelic venues. This ban, also known as Rule 42,[63] is still enforced, but was modified to allow football and rugby to be played in Croke Park while Lansdowne Road was redeveloped into Aviva Stadium. Until recently, under Rule 21, the GAA also banned members of the British security forces and members of the RUC from playing Gaelic games, but the advent of the Good Friday Agreement in 1998 led to the eventual removal of the ban.		Nationalism is often evident in the pursuit of sports, or in its reporting: people compete in national teams, or commentators and audiences can adopt a partisan view. On occasion, such tensions can lead to violent confrontation among players or spectators within and beyond the sporting venue, as in the Football War. These trends are seen by many as contrary to the fundamental ethos of sports being carried on for its own sake and for the enjoyment of its participants.		A very famous case when sports and politics collided was the 1972 Olympics in Munich. Masked men entered the hotel of the Israeli olympic team and killed many of their men. This was known as the Munich massacre.		A study of US elections has shown that the result of sports events can affect the results. A study published in the Proceedings of the National Academy of Sciences showed that when the home team wins the game before the election, the incumbent candidates can increase their share of the vote by 1.5 percent. A loss had the opposite effect, and the effect is greater for higher-profile teams or unexpected wins and losses.[64] Also, when Washington Redskins win their final game before an election, then the incumbent President is more likely to win, and if the Redskins lose, then the opposition candidate is more likely to win; this has become known as the Redskins Rule.[65][66]		Étienne de La Boétie, in his essay Discourse on Voluntary Servitude describes athletic spectacles as means for tyrants to control their subjects by distracting them.		Do not imagine that there is any bird more easily caught by decoy, nor any fish sooner fixed on the hook by wormy bait, than are all these poor fools neatly tricked into servitude by the slightest feather passed, so to speak, before their mouths. Truly it is a marvelous thing that they let themselves be caught so quickly at the slightest tickling of their fancy. Plays, farces, spectacles, gladiators, strange beasts, medals, pictures, and other such opiates, these were for ancient peoples the bait toward slavery, the price of their liberty, the instruments of tyranny. By these practices and enticements the ancient dictators so successfully lulled their subjects under the yoke, that the stupefied peoples, fascinated by the pastimes and vain pleasures flashed before their eyes, learned subservience as naïvely, but not so creditably, as little children learn to read by looking at bright picture books.[67]		The practice of athletic competitions has been criticized by some Christian thinkers as a form of idolatry, in which "human beings extol themselves, adore themselves, sacrifice themselves and reward themselves."[68] Sports are seen by these critics as a manifestation of "collective pride" and "national self-deification" in which feats of human power are idolized at the expense of divine worship.[68]		Tertullian condemns the athletic performances of his day, insisting "the entire apparatus of the shows is based upon idolatry."[69] The shows, says Tertullian, excite passions foreign to the calm temperament cultivated by the Christian:		God has enjoined us to deal calmly, gently, quietly, and peacefully with the Holy Spirit, because these things are alone in keeping with the goodness of His nature, with His tenderness and sensitiveness. ... Well, how shall this be made to accord with the shows? For the show always leads to spiritual agitation, since where there is pleasure, there is keenness of feeling giving pleasure its zest; and where there is keenness of feeling, there is rivalry giving in turn its zest to that. Then, too, where you have rivalry, you have rage, bitterness, wrath and grief, with all bad things which flow from them—the whole entirely out of keeping with the religion of Christ.[70]		
An adventure is an exciting or unusual experience.		Adventures or the Adventures may also refer to:		
The Odyssey (/ˈɒdəsi/;[1] Greek: Ὀδύσσεια Odýsseia, pronounced [o.dýs.sej.ja] in Classical Attic) is one of two major ancient Greek epic poems attributed to Homer. It is, in part, a sequel to the Iliad, the other work ascribed to Homer. The Odyssey is fundamental to the modern Western canon, and is the second-oldest extant work of Western literature; the Iliad is the oldest. Scholars believe the Odyssey was composed near the end of the 8th century BC, somewhere in Ionia, the Greek coastal region of Anatolia.[2]		The poem mainly focuses on the Greek hero Odysseus (known as Ulysses in Roman myths), king of Ithaca, and his journey home after the fall of Troy. It takes Odysseus ten years to reach Ithaca after the ten-year Trojan War.[3] In his absence, it is assumed Odysseus has died, and his wife Penelope and son Telemachus must deal with a group of unruly suitors, the Mnesteres (Greek: Μνηστῆρες) or Proci, who compete for Penelope's hand in marriage.		The Odyssey continues to be read in the Homeric Greek and translated into modern languages around the world. Many scholars believe the original poem was composed in an oral tradition by an aoidos (epic poet/singer), perhaps a rhapsode (professional performer), and was more likely intended to be heard than read.[2] The details of the ancient oral performance and the story's conversion to a written work inspire continual debate among scholars. The Odyssey was written in a poetic dialect of Greek—a literary amalgam of Aeolic Greek, Ionic Greek, and other Ancient Greek dialects—and comprises 12,110 lines of dactylic hexameter.[4][5] Among the most noteworthy elements of the text are its non-linear plot, and the influence on events of choices made by women and slaves, besides the actions of fighting men. In the English language as well as many others, the word odyssey has come to refer to an epic voyage.		The Odyssey has a lost sequel, the Telegony, which was not written by Homer. It was usually attributed in antiquity to Cinaethon of Sparta. In one source,[which?] the Telegony was said to have been stolen from Musaeus by either Eugamon or Eugammon of Cyrene (see Cyclic poets).						The Odyssey begins ten years after the end of the ten-year Trojan War (the subject of the Iliad), and Odysseus has still not returned home from the war. Odysseus' son Telemachus is about 20 years old and is sharing his absent father's house on the island of Ithaca with his mother Penelope and a crowd of 108 boisterous young men, "the Suitors", whose aim is to persuade Penelope to marry one of them, all the while reveling in Odysseus' palace and eating up his wealth.		Odysseus' protectress, the goddess Athena, requests to Zeus, king of the gods, to finally allow Odysseus to return home when Odysseus' enemy, the god of the sea Poseidon, is absent from Mount Olympus. Then, disguised as a Taphian chieftain named Mentes, she visits Telemachus to urge him to search for news of his father. He offers her hospitality; they observe the suitors dining rowdily while the bard Phemius performs a narrative poem for them. Penelope objects to Phemius' theme, the "Return from Troy",[6] because it reminds her of her missing husband, but Telemachus rebuts her objections, asserting his role as head of the household.		That night Athena, disguised as Telemachus, finds a ship and crew for the true prince. The next morning, Telemachus calls an assembly of citizens of Ithaca to discuss what should be done with the suitors. Accompanied by Athena (now disguised as Mentor), he departs for the Greek mainland and the household of Nestor, most venerable of the Greek warriors at Troy, now at home in Pylos.		From there, Telemachus rides overland, accompanied by Nestor's son Peisistratus, to Sparta, where he finds Menelaus and Helen, who have somewhat reconciled. While Helen laments the fit of lust brought on by Aphrodite that sent her to Troy with Paris, Menelaus recounts how she betrayed the Greeks by attempting to imitate the voices of the soldiers' wives while they were inside the Trojan Horse. Telemachus also hears from Helen, who is the first to recognize him, that she pities him because Odysseus was not there for him in his childhood because he went to Troy to fight for her and also about his exploit of stealing the Palladium, or the Luck of Troy, where she was the only one to recognize him. Menelaus, meanwhile, also praises Odysseus as an irreproachable comrade and friend, lamenting the fact that they were not only unable to return together from Troy but that Odysseus is yet to return.		Both Helen and Menelaus also say that they returned to Sparta after a long voyage by way of Egypt. There, on the island of Pharos, Menelaus encountered the old sea-god Proteus, who told him that Odysseus was a captive of the nymph Calypso. Incidentally, Telemachus learns the fate of Menelaus' brother Agamemnon, king of Mycenae and leader of the Greeks at Troy: he was murdered on his return home by his wife Clytemnestra and her lover Aegisthus. The story briefly shifts to the suitors, who have only just now realized that Telemachus is gone; angry, they formulate a plan to ambush his ship and kill him as he sails back home. Penelope overhears their plot and worries for her son's safety.		The second part recounts the story of Odysseus. After he has spent seven years in captivity on Ogygia, the island of Calypso, she falls deeply in love with him, even though he has consistently spurned her advances. She is persuaded to release him by Odysseus' great-grandfather, the messenger god Hermes, who has been sent by Zeus in response to Athena's plea. Odysseus builds a raft and is given clothing, food, and drink by Calypso. When Poseidon learns that Odysseus has escaped, he wrecks the raft, but, helped by a veil given by the sea nymph Ino, Odysseus swims ashore on Scherie, the island of the Phaeacians. Naked and exhausted, he hides in a pile of leaves and falls asleep. The next morning, awakened by the laughter of girls, he sees the young Nausicaa, who has gone to the seashore with her maids to wash clothes after Athena told her in a dream to do so. He appeals to her for help. She encourages him to seek the hospitality of her parents, Arete and Alcinous (or Alkinous). Odysseus is welcomed and is not at first asked for his name. He remains for several days, takes part in a pentathlon, and hears the blind singer Demodocus perform two narrative poems. The first is an otherwise obscure incident of the Trojan War, the "Quarrel of Odysseus and Achilles"; the second is the amusing tale of a love affair between two Olympian gods, Ares and Aphrodite. Finally, Odysseus asks Demodocus to return to the Trojan War theme and tell of the Trojan Horse, a stratagem in which Odysseus had played a leading role. Unable to hide his emotion as he relives this episode, Odysseus at last reveals his identity. He then begins to tell the story of his return from Troy.		After a failed piratical raid on Ismaros in the land of the Cicones, Odysseus and his twelve ships were driven off course by storms. Odysseus visited the lethargic Lotus-Eaters who gave his men their fruit that would have caused them to forget their homecoming had Odysseus not dragged them back to the ship by force. Then, they entered the cave of the Cyclops Polyphemus on the underbellies of sheep, escaping by blinding him with a wooden stake. While they were escaping, however, Odysseus foolishly told Polyphemus his identity, and Polyphemus told his father, Poseidon, that Odysseus had blinded him. Poseidon then cursed Odysseus to wander the sea for ten years, during which he would lose all his crew and return home through the aid of others. After the escape, Odysseus and his crew stayed with Aeolus, a king endowed by the gods with the winds. He gave Odysseus a leather bag containing all the winds, except the west wind, a gift that should have ensured a safe return home. Just as Ithaca came into sight, the greedy sailors naively opened the bag while Odysseus slept, thinking it contained gold. All of the winds flew out and the resulting storm drove the ships back the way they had come.		After unsuccessfully pleading with Aeolus to help them again, they re-embarked and encountered the cannibalistic Laestrygonians. All of Odysseus' ships except his own entered the harbor of the Laestrygonians' Island and were immediately destroyed. He sailed on and visited the witch-goddess Circe. She turned half of his men into swine after feeding them cheese and wine. Hermes warned Odysseus about Circe and gave Odysseus a drug called moly which gave him resistance to Circe's magic. Odysseus forced the now-powerless Circe to change his men back to their human form. They remained with her on the island for one year, while they feasted and drank. Finally, guided by Circe's instructions, Odysseus and his crew crossed the ocean and reached a harbor at the western edge of the world, where Odysseus sacrificed to the dead. He first encountered the spirit of Elpenor, a crewman who had gotten drunk and fallen from a roof to his death, which had gone unnoticed by others, before Odysseus and the rest of his crew had left Circe. Elpenor's ghost told Odysseus to bury his body, which Odysseus promised to do. Odysseus then summoned the spirit of the prophet Tiresias for advice on how to appease Poseidon upon his return home. Next Odysseus met the spirit of his own mother, who had died of grief during his long absence. From her, he got his first news of his own household, threatened by the greed of the Suitors. Finally, he met the spirits of famous men and women. Notably, he encountered the spirit of Agamemnon, of whose murder he now learned, and Achilles, who told him about the woes of the land of the dead (for Odysseus' encounter with the dead, see also Nekuia).		Returning to Circe's island, they were advised by her on the remaining stages of the journey. They skirted the land of the Sirens, who sang an enchanting song that normally caused passing sailors to steer toward the rocks, only to hit them and sink. All of the sailors had their ears plugged up with beeswax, except for Odysseus, who was tied to the mast as he wanted to hear the song. He told his sailors not to untie him as it would only make him want to drown himself. They then passed between the six-headed monster Scylla and the whirlpool Charybdis, narrowly avoiding death, even though Scylla snatched up six men. Next, they landed on the island of Thrinacia. Zeus caused a storm which prevented them leaving. While Odysseus was away praying, his men ignored the warnings of Tiresias and Circe and hunted the sacred cattle of the sun god Helios as their food had run short. The Sun God insisted that Zeus punish the men for this sacrilege. They suffered a shipwreck as they were driven towards Charybdis. All but Odysseus were drowned; he clung to a fig tree above Charybdis. Washed ashore on the island of Ogygia, he was compelled to remain there as Calypso's lover, bored, homesick and trapped on her small island, until she was ordered by Zeus, via Hermes, to release Odysseus. Odysseus did not realise how long it would take to get home to his family.		Having listened with rapt attention to his story, the Phaeacians, who are skilled mariners, agree to help Odysseus get home. They deliver him at night, while he is fast asleep, to a hidden harbour on Ithaca. He finds his way to the hut of one of his own slaves, the swineherd Eumaeus. Athena disguises Odysseus as a wandering beggar so he can see how things stand in his household. After dinner, he tells the farm laborers a fictitious tale of himself: He was born in Crete, had led a party of Cretans to fight alongside other Greeks in the Trojan War, and had then spent seven years at the court of the king of Egypt; finally he had been shipwrecked in Thesprotia and crossed from there to Ithaca.		Meanwhile, Telemachus sails home from Sparta, evading an ambush set by the Suitors. He disembarks on the coast of Ithaca and makes for Eumaeus's hut. Father and son meet; Odysseus identifies himself to Telemachus (but still not to Eumaeus), and they decide that the Suitors must be killed. Telemachus goes home first. Accompanied by Eumaeus, Odysseus returns to his own house, still pretending to be a beggar. When Odysseus' dog (who was a puppy before he left) saw him, he becomes so excited that he dies.[7] He is ridiculed by the Suitors in his own home, especially by one extremely impertinent man named Antinous. Odysseus meets Penelope and tests her intentions by saying he once met Odysseus in Crete. Closely questioned, he adds that he had recently been in Thesprotia and had learned something there of Odysseus's recent wanderings.		Odysseus's identity is discovered by the housekeeper, Eurycleia, when she recognizes an old scar as she is washing his feet. Eurycleia tries to tell Penelope about the beggar's true identity, but Athena makes sure that Penelope cannot hear her. Odysseus then swears Eurycleia to secrecy.		The next day, at Athena's prompting, Penelope maneuvers the Suitors into competing for her hand with an archery competition using Odysseus' bow. The man who can string the bow and shoot it through a dozen axe heads would win. Odysseus takes part in the competition himself: he alone is strong enough to string the bow and shoot it through the dozen axe heads, making him the winner. He then throws off his rags and kills Antinous with his next arrow. Then, with the help of Athena, Odysseus, Telemachus, Eumaeus, and Philoetius the cowherd kill the rest of the Suitors, first using the rest of the arrows and then by swords and spears once both sides have armed themselves. Once the battle is won, Odysseus and Telemachus also hang twelve of their household maids whom Eurycleia identifies as guilty of betraying Penelope, having sex with the Suitors, or both; they mutilate and kill the goatherd Melanthius, who had mocked and abused Odysseus and also brought weapons and armor to the suitors. Now, at last, Odysseus identifies himself to Penelope. She is hesitant but recognizes him when he mentions that he made their bed from an olive tree still rooted to the ground. Many modern and ancient scholars take this to be the original ending of the Odyssey, and the rest to be an interpolation.		The next day he and Telemachus visit the country farm of his old father Laertes, who likewise accepts his identity only when Odysseus correctly describes the orchard that Laertes had previously given him.		The citizens of Ithaca have followed Odysseus on the road, planning to avenge the killing of the Suitors, their sons. Their leader points out that Odysseus has now caused the deaths of two generations of the men of Ithaca: his sailors, not one of whom survived; and the Suitors, whom he has now executed (albeit rightly). Athena intervenes as a "dea" ex machina, as it were, and persuades both sides to give up the vendetta. After this, Ithaca is at peace once more, concluding the Odyssey.		Odysseus' name means "trouble" in Greek, referring to both the giving and receiving of trouble—as is often the case in his wanderings. An early example of this is the boar hunt that gave Odysseus the scar by which Eurycleia recognizes him; Odysseus is injured by the boar and responds by killing it. Odysseus' heroic trait is his mētis, or "cunning intelligence": he is often described as the "Peer of Zeus in Counsel". This intelligence is most often manifested by his use of disguise and deceptive speech. His disguises take forms both physical (altering his appearance) and verbal, such as telling the Cyclops Polyphemus that his name is Οὖτις, "Nobody", then escaping after blinding Polyphemus. When asked by other Cyclopes why he is screaming, Polyphemus replies that "Nobody" is hurting him, so the others assume that "If alone as you are [Polyphemus] none uses violence on you, why, there is no avoiding the sickness sent by great Zeus; so you had better pray to your father, the lord Poseidon".[8] The most evident flaw that Odysseus sports is that of his arrogance and his pride, or hubris. As he sails away from the island of the Cyclopes, he shouts his name and boasts that nobody can defeat the "Great Odysseus". The Cyclops then throws the top half of a mountain at him and prays to his father, Poseidon, saying that Odysseus has blinded him. This enrages Poseidon, causing the god to thwart Odysseus' homecoming for a very long time.		The Odyssey was written in dactylic hexameter. It opens in medias res, in the middle of the overall story, with prior events described through flashbacks or storytelling. This device is also used by later authors of literary epics, such as Virgil in the Aeneid, Luís de Camões in Os Lusíadas[9] and Alexander Pope in The Rape of the Lock.		The first four books of the poem trace Telemachus' efforts to assert control of the household, and then, at Athena's advice, his efforts to search for news of his long-lost father. Then the scene shifts: Odysseus has been a captive of the beautiful nymph Calypso, with whom he has spent seven of his ten lost years. Released by the intercession of his patroness Athena, through the aid of Hermes, he departs, but his raft is destroyed by his divine enemy Poseidon, who is angry because Odysseus blinded his son, Polyphemus. When Odysseus washes up on Scherie, home to the Phaeacians, he is assisted by the young Nausicaa and is treated hospitably. In return, he satisfies the Phaeacians' curiosity, telling them, and the reader, of all his adventures since departing from Troy. The shipbuilding Phaeacians then loan him a ship to return to Ithaca, where he is aided by the swineherd Eumaeus, meets Telemachus, regains his household, kills the Suitors, and is reunited with his faithful wife, Penelope.		All ancient and nearly all modern editions and translations of the Odyssey are divided into 24 books. This division is convenient, but it may not be original. Many scholars[who?] believe it was developed by Alexandrian editors of the 3rd century BC. In the Classical period, moreover, several of the books (individually and in groups) were given their own titles: the first four books, focusing on Telemachus, are commonly known as the Telemachy. Odysseus' narrative, Book 9, featuring his encounter with the cyclops Polyphemus, is traditionally called the Cyclopeia. Book 11, the section describing his meeting with the spirits of the dead is known as the Nekuia. Books 9 through 12, wherein Odysseus recalls his adventures for his Phaeacian hosts, are collectively referred to as the Apologoi: Odysseus' "stories". Book 22, wherein Odysseus kills all the Suitors, has been given the title Mnesterophonia: "slaughter of the Suitors". This concludes the Greek Epic Cycle, though fragments remain of the "alternative ending" of sorts known as the Telegony.		This Telegony aside, the last 548 lines of the Odyssey, corresponding to Book 24, are believed by many scholars to have been added by a slightly later poet.[10] Several passages in earlier books seem to be setting up the events of Book 24, so if it were indeed a later addition, the offending editor would seem to have changed earlier text as well. For more about varying views on the origin, authorship and unity of the poem see Homeric scholarship.		The events in the main sequence of the Odyssey (excluding Odysseus' embedded narrative of his wanderings) take place in the Peloponnese and in what are now called the Ionian Islands.[citation needed] There are difficulties in the apparently simple identification of Ithaca, the homeland of Odysseus, which may or may not be the same island that is now called Ithake. The wanderings of Odysseus as told to the Phaeacians, and the location of the Phaeacians' own island of Scheria, pose more fundamental problems, if geography is to be applied: scholars, both ancient and modern, are divided as to whether or not any of the places visited by Odysseus (after Ismaros and before his return to Ithaca) are real.[citation needed]		Scholars have seen strong influences from Near Eastern mythology and literature in the Odyssey. Martin West has noted substantial parallels between the Epic of Gilgamesh and the Odyssey.[11] Both Odysseus and Gilgamesh are known for traveling to the ends of the earth, and on their journeys go to the land of the dead. On his voyage to the underworld, Odysseus follows instructions given to him by Circe, a goddess who is the daughter of the sun-god Helios. Her island, Aeaea, is located at the edges of the world and seems to have close associations with the sun. Like Odysseus, Gilgamesh gets directions on how to reach the land of the dead from a divine helper: in this case, the goddess Siduri, who, like Circe, dwells by the sea at the ends of the earth. Her home is also associated with the sun: Gilgamesh reaches Siduri's house by passing through a tunnel underneath Mt. Mashu, the high mountain from which the sun comes into the sky. West argues that the similarity of Odysseus' and Gilgamesh's journeys to the edges of the earth are the result of the influence of the Gilgamesh epic upon the Odyssey.		In 1914, paleontologist Othenio Abel surmised the origins of the cyclops to be the result of ancient Greeks finding an elephant skull. The enormous nasal passage in the middle of the forehead could have looked like the eye socket of a giant, to those who had never seen a living elephant.[12] Classical scholars, on the other hand, have long realized that the story of the cyclops was originally a Greek folk tale, which existed independently of The Odyssey and which only became embedded in it at a later date. Similar stories are found in cultures across Europe and the Middle East.[13] According to this explanation, the cyclops was originally simply a giant or ogre, much like Humbaba in the Epic of Gilgamesh;[13] the detail about it having one eye was simply invented in order to explain how the creature was so easily blinded.[14]		An important factor to consider about Odysseus' homecoming is the hint at potential endings to the epic by using other characters as parallels for his journey.[15] For instance, one example is that of Agamemnon's homecoming versus Odysseus' homecoming. Upon Agamemnon's return, his wife, Clytemnestra, and her lover, Aegisthus, kill Agamemnon. Agamemnon's son, Orestes, out of vengeance for his father's death, kills Aegisthus. This parallel compares the death of the suitors to the death of Aegisthus and sets Orestes up as an example for Telemachus.[15] Also, because Odysseus knows about Clytemnestra's betrayal, Odysseus returns home in disguise in order to test the loyalty of his own wife, Penelope.[15] Later, Agamemnon praises Penelope for not killing Odysseus. It is because of Penelope that Odysseus has fame and a successful homecoming. This successful homecoming is unlike Achilles, who has fame but is dead, and Agamemnon, who had an unsuccessful homecoming resulting in his death.[15]		Only two of Odysseus's adventures are described by the poet. The rest of Odysseus' adventures are recounted by Odysseus himself. The two scenes that the poet describes are Odysseus on Calypso's island and Odysseus' encounter with the Phaeacians. These scenes are told by the poet to represent an important transition in Odysseus' journey: being concealed to returning home.[16] Calypso's name means "concealer" or "one who conceals," and that is exactly what she does with Odysseus.[17] Calypso keeps Odysseus concealed from the world and unable to return home. After leaving Calypso's island, the poet describes Odysseus' encounters with the Phaeacians—those who "convoy without hurt to all men"[18]—which represents his transition from not returning home to returning home.[16] Also, during Odysseus' journey, he encounters many beings that are close to the gods. These encounters are useful in understanding that Odysseus is in a world beyond man and that influences the fact he cannot return home.[16] These beings that are close to the gods include the Phaeacians who lived near Cyclopes,[19] whose king, Alcinous, is the great-grandson of the king of the giants, Eurymedon, and the grandson of Poseidon.[16] Some of the other characters that Odysseus encounters are Polyphemus who is the cyclops son of Poseidon; God of Oceans, Circe who is the sorceress daughter of the Sun that turns men into animals, Calypso who is a goddess, and the Laestrygonians who are cannibalistic giants.[16]		Throughout the course of the epic, Odysseus encounters several examples of guest-friendship which provide examples of how hosts should and should not act.[20] One example of good guest-friendship is that of the Phaeacians. The Phaeacians feed Odysseus, give him a place to sleep, and give him a safe voyage home, which are all things a good host should do. He also encounters some bad hosts. For instance, the cyclops's "gift" to Odysseus was that he would eat him last.[20] He was not a very good host. Another host that was not well versed in guest-friendship was Calypso, who did not allow Odysseus to leave her island.[20] Another important factor to guest-friendship is that kingship implies generosity. It is assumed that a king has the means to be a generous host and is more generous with his own property.[20] This is best seen when Odysseus, disguised as a beggar, begs Antinous, one of the suitors, for food and Antinous denies his request. Odysseus essentially says that while Antinous may look like a king, he is far from a king since he is not generous.[21]		Another theme throughout the Odyssey is testing.[22] This occurs in two distinct ways. Odysseus tests the loyalty of others and others test Odysseus' identity. An example of Odysseus testing the loyalties of others is when he returns home.[22] Instead of immediately revealing his identity, he arrives disguised as a beggar and then proceeds to determine who in his house has remained loyal to him and who has helped the suitors. After Odysseus reveals his true identity, the characters test Odysseus' identity to see if he really is who he says he is.[22] For instance, Penelope tests Odysseus' identity by saying that she will move the bed into the other room for him. This is a difficult task since it is made out of a living tree that would require being cut down, a fact that only the real Odysseus would know, thus proving his identity. For more information on the progression of testing type scenes, read more below.[22]		Omens occur frequently throughout the Odyssey, as well as many other epics. Within the Odyssey, omens frequently involve birds.[23] It is important to note who receives the omens and what these omens mean to the characters and to the epic as a whole. For instance, bird omens are shown to Telemachus, Penelope, Odysseus, and the suitors.[23] Telemachus and Penelope receive their omens as well in the form of words, sneezes, and dreams.[23] However, Odysseus is the only character that receives thunder or lightning as an omen.[24][25] This is important to note because the thunder came from Zeus, the king of the gods. This direct relationship between Zeus and Odysseus represents the kingship of Odysseus.[23]		Finding scenes occur in the Odyssey when a character discovers another character within the epic. Finding scenes proceed as followed:[16]		These finding scenes can be identified several times throughout the epic including when Telemachus and Pisistratus find Menelaus when Calypso finds Odysseus on the beach, and when the suitor Amphimedon finds Agamemnon in Hades.[16][26]		Omens are another example of a type scene in the Odyssey. Two important parts of an omen type scene are the recognition of the omen and then the interpretation.[23] In the Odyssey specifically, there are several omens involving birds. All of the bird omens—with the exception of the first one in the epic—show large birds attacking smaller bird.[23][26] Accompanying each omen is a wish; this wish can be either explicitly stated or implicitly implied.[23] For example, Telemachus wishes for vengeance[27] and for Odysseus to be home,[28] Penelope wishes for Odysseus' return,[29] and the suitors wish for the death of Telemachus.[30] The omens seen in the Odyssey are also a recurring theme throughout the epic.[23][26]		While testing is a theme with the epic, it also has a very specific type scene that accompanies it as well. Throughout the epic, the testing of others follows a typical pattern. This pattern is:		Guest-Friendship is also a theme in the Odyssey, but it too follows a very specific pattern. This pattern is:		Another important factor of guest-friendship is not keeping the guest longer than they wish and also promising their safety while they are a guest within the host's home.[20][26]		The Odyssey is regarded as one of the most important foundational works of western literature.[31] It is widely regarded by western literary critics as a timeless classic.[32]		Straightforward retellings of The Odyssey have flourished ever since the Middle Ages. Merugud Uilix maicc Leirtis ("On the Wandering of Ulysses, son of Laertes") is an eccentric Old Irish version of the material; the work exists in a 12th-century AD manuscript, which linguists believe is based on an 8th-century original.[33][34] Il ritorno d'Ulisse in patria, first performed in 1640, is an opera by Claudio Monteverdi based on the second half of Homer's Odyssey.[35] The first canto of Ezra Pound's The Cantos (1917) is both a translation and a retelling of Odysseus' journey to the underworld.[36] The poem "Ulysses" by Alfred, Lord Tennyson is narrated by an aged Ulysses who is determined to continue to live life to the fullest. The Odyssey (1997), a made-for-TV movie directed by Andrei Konchalovsky, is a slightly abbreviated version of the epic.		Other authors have composed more creative reworkings of the poem, often updated to address contemporary themes and concerns. Cyclops by Euripides, the only fully extant satyr play,[37] retells the episode involving Polyphemus with a humorous twist.[38] A True Story, written by Lucian of Samosata in the 2nd century AD, is a satire on the Odyssey and on ancient travel tales, describing a journey sailing westward, beyond the Pillars of Hercules and to the Moon, the first known text that could be called science fiction.[39]		James Joyce's modernist novel Ulysses (1922) is a retelling of The Odyssey set in modern-day Dublin. Each chapter in the book has an assigned theme, technique, and correspondences between its characters and those of Homer's Odyssey.[40] Homer's Daughter by Robert Graves is a novel imagining how the version we have might have been invented out of older tales. The Japanese-French anime Ulysses 31 (1981) updates the ancient setting into a 31st-century space opera. Omeros (1991), an epic poem by Derek Walcott, is in part a retelling of the Odyssey, set on the Caribbean island of St. Lucia. The film Ulysses' Gaze (1995) directed by Theo Angelopoulos has many of the elements of the Odyssey set against the backdrop of the most recent and previous Balkan Wars.[40]		Similarly, Daniel Wallace's Big Fish: A Novel of Mythic Proportions (1998) adapts the epic to the American South, while also incorporating tall tales into its first-person narrative much as Odysseus does in the Apologoi (Books 9-12). The Coen Brothers' 2000 film O Brother, Where Art Thou? is loosely based on Homer's poem. Margaret Atwood's 2005 novella The Penelopiad is an ironic rewriting of The Odyssey from Penelope's perspective. Zachary Mason's The Lost Books of the Odyssey (2007) is a series of short stories that rework Homer's original plot in a contemporary style reminiscent of Italo Calvino. The Heroes of Olympus, by Rick Riordan, is based entirely off of Greek mythology and includes many aspects and characters from the Odyssey.[41]		Ever since the ancient times, various authors have sought to imagine new endings for The Odyssey. In canto XXVI of the Inferno, Dante Alighieri meets Odysseus in the eighth circle of hell, where Odysseus himself appends a new ending to The Odyssey in which he never returns to Ithaca and instead continues his restless adventuring.[42][43] Nikos Kazantzakis aspires to continue the poem and explore more modern concerns in his epic poem The Odyssey: A Modern Sequel, which was first published in 1938 in modern Greek.[44]		This is a partial list of translations into English of Homer's Odyssey.		Lucian of Samosata, the Greco-Syrian satirist of the second century, appears today as an exemplar of the science-fiction artist. There is little, if any, need to argue that his mythopoeic Milesian Tales and his literary fantastic voyages and utopistic hyperbole comport with the genre of science fiction; ...		
Hatchet is a 1987 Newbery Honor-winning young-adult wilderness survival novel written by American writer Gary Paulsen.[1] It is the first novel of five in the Hatchet series.						Brian Robeson is a thirteen-year-old son of divorced parents. As he travels from Hampton, New York on a Cessna 406 bush plane to visit his father in the oil fields in Northern Canada for the summer, the pilot suffers a massive heart attack and dies. Brian tries to land the plane, but ends up crash-landing into a lake in the forest. He must learn to survive on his own with nothing but his hatchet—a gift his mother gave him shortly before his plane departed.[2]		Throughout the summer, Brian learns how to survive in the vast wilderness with only his hatchet. He discovers how to make fire with the hatchet and eats whatever food he can find, such as rabbits, birds, turtle eggs, fish, berries, and fruit. He deals with various threats of nature, including mosquitos, quail, a porcupine, bear, skunk, moose, wolves, and even a tornado. Over time, Brian develops his survival skills and becomes a fine woodsman. He crafts a bow, arrows, and a fishing spear to aid in his hunting. He also fashions a shelter out of the underside of a rock overhang. During his time alone, Brian struggles with memories of home and the bittersweet memory of his mother, whom Brian had caught cheating on his father prior to their divorce.[2]		When a sudden tornado hits the area, it draws the tail of the plane toward the shore of the lake. This triggered his thoughts that there may be a survival pack of some sorts on the plane. Brian makes a raft from a few broken off tree tops to get to the plane. When Brian is cutting his way into the tail of the plane, he drops his hatchet in the lake and dives in to get it. Once inside the plane, Brian finds a survival pack that includes additional food, an emergency transmitter, and a .22 rifle. Back on shore, Brian activates the transmitter, but not knowing how to use it, he thinks it is broken and throws it aside. However, his distress call is heard by a passing airplane, and he is rescued. Brian spends the remainder of the summer with his father but does not disclose his mother's affair.[2]		A film adaptation titled A Cry in the Wild was released in 1990.[3]		Paulsen continued the story of Brian Robeson with four more novels, beginning with The River in 1991.		Hatchet was a recipient of the 1988 Newbery Honor.[4]		Paulsen, Gary (1999). Hatchet. Simon & Schuster. ISBN 9781416936473. 		Salvner, Gary M. (2011). "Lessons and Lives: Why Young Adult Literature Matters". The ALAN Review. 28 (3): 9.  |access-date= requires |url= (help)		Sturm, Brian W. (Winter 2009). "The Structure of Power in Young Adult Problem Novels". Young Adult Library Services. 7 (2): 39–47.  |access-date= requires |url= (help)		Unwin, Cynthia G. (1999). "Survival as a Bridge to Resistant Readers: Applications of Gary Paulsen's Hatchet to an Integrated Curriculum". The ALAN Review. 26 (3): 9–12.  |access-date= requires |url= (help)				
Exploration is the act of searching for the purpose of discovery of information or resources. Exploration occurs in all non-sessile animal species, including humans. In human history, its most dramatic rise was during the Age of Discovery when European explorers sailed and charted much of the rest of the world for a variety of reasons. Since then, major explorations after the Age of Discovery have occurred for reasons mostly aimed at information discovery.		In scientific research, exploration is one of three purposes of empirical research (the other two being description and explanation). The term is commonly used metaphorically. For example, an individual may speak of exploring the Internet, sexuality, etc.						The Phoenicians (1550 BCE–300 BCE) traded throughout the Mediterranean Sea and Asia Minor though many of their routes are still unknown today. The presence of tin in some Phoenician artifacts suggests that they may have traveled to Britain. According to Virgil's Aeneid and other ancient sources, the legendary Queen Dido was a Phoenician from Tyre who sailed to North Africa and founded the city of Carthage.		Hanno the Navigator (500 BC), a Carthaginean navigator explored the Western Coast of Africa.		The Greek explorer from Marseille, Pytheas (380 – c. 310 BC) was the first to circumnavigate Great Britain, explore Germany, and reach Thule (most commonly thought to be the Shetland Islands or Iceland).		The Romans organized expeditions to cross the Sahara desert with five different routes:		All these expeditions were supported by legionaries and had mainly a commercial purpose. Only the one done by emperor Nero seemed to be a preparative for the conquest of Ethiopia or Nubia: in 62 AD two legionaries explored the sources of the Nile river.		One of the main reasons of the explorations was to get gold using the camel to transport it.[1]		The explorations near the African western and eastern coasts were supported by Roman ships and deeply related to the naval commerce (mainly toward the Indian Ocean). Romans organized several explorations also in Northern Europe, and as far as Asia up to China .		During the 2nd century BC, the Han dynasty explored much of the Eastern Northern Hemisphere. Starting in 139 BC, the Han diplomat Zhang Qian traveled west in an unsuccessful attempt to secure an alliance with the Da Yuezhi against the Xiongnu (the Yuezhi had been evicted from Gansu by the Xiongnu in 177 BC); however, Zhang's travels discovered entire countries which the Chinese were unaware of, including the remnants of the conquests of Alexander the Great (r. 336–323 BC).[2] When Zhang returned to China in 125 BC, he reported on his visits to Dayuan (Fergana), Kangju (Sogdiana), and Daxia (Bactria, formerly the Greco-Bactrian Kingdom which had just been subjugated by the Da Yuezhi).[3] Zhang described Dayuan and Daxia as agricultural and urban countries like China, and although he did not venture there, described Shendu (the Indus River valley of Northwestern India) and Anxi (Arsacid territories) further west.[4]		From about 800 Ad to 1040 AD, the Vikings explored Europe and much of the Western Northern Hemisphere via rivers and oceans. For example, it is known that the Norwegian Viking explorer, Erik the Red (950–1003), sailed to and settled in Greenland after being expelled from Iceland, while his son, the Icelandic explorer Leif Ericson (980–1020), reached Newfoundland and the nearby North American coast, and is believed to be the first European to land in North America.		Polynesians were a maritime people, who populated and explored the central and south Pacific for around 5,000 years, up to about 1280 when they discovered New Zealand. The key invention to their exploration was the outrigger canoe, which provided a swift and stable platform for carrying goods and people. Based on limited evidence, it is thought that the voyage to New Zealand was deliberate. It is unknown if one or more boats went to New Zealand, or the type of boat, or the names of those who migrated. 2011 studies at Wairau Bar in New Zealand show a high probability that one origin was Ruahine Island in the Society Islands. Polynesians may have used the prevailing north easterly trade winds to reach New Zealand in about three weeks. The Cook Islands are in direct line along the migration path and may have been an intermediate stopping point. There are cultural and language similarities between Cook Islanders and New Zealand Maori. Early Maori had different legends of their origins, but the stories were misunderstood and reinterpreted in confused written accounts by early European historians in New Zealand trying to present a coherent pattern of Maori settlement in New Zealand.		Mathematical modelling based on DNA genome studies, using state-of-the-art techniques, have shown that a large number of Polynesian migrants (100–200), including women, arrived in New Zealand around the same time, in about 1280. Otago University studies have tried to link distinctive DNA teeth patterns, which show special dietary influence, with places in or nearby the Society Islands.[5]		The Chinese explorer, Wang Dayuan (fl. 1311–1350) made two major trips by ship to the Indian Ocean. During 1328–1333, he sailed along the South China Sea and visited many places in Southeast Asia and reached as far as South Asia, landing in Sri Lanka and India. Then in 1334–1339, he visited North Africa and East Africa. Later, the Chinese admiral Zheng He (1371–1433) made seven voyages to Arabia, East Africa, India, Indonesia and Thailand.		The Age of Discovery, also known as the Age of Exploration, is one of the most important periods of geographical exploration in human history. It started in the early 15th century and lasted until the 17th century. In that period, Europeans discovered and/or explored vast areas of the Americas, Africa, Asia and Oceania. Portugal and Spain dominated the first stages of exploration, while other European nations followed, such as England, Netherlands, and France.		The most important explorers of this period include: Diogo Cão (c.1452 –c.1486) who discovered and ascended the Congo River and reached the coasts of the present-day Angola and Namibia; Bartolomeu Dias (c. 1450–1500), who was the first European to reach the Cape of Good Hope and other parts of the South African coast; Christopher Columbus (1451–1506), who led a Castilian (Spanish) expedition across the Atlantic, discovering America; Vasco da Gama (1460–1524), a navigator who made the first trip from Europe to India and back by the Cape of Good Hope, discovering the ocean route to the East; Pedro Alvares Cabral (c. 1467/68–c.1520) who, following the path of Gama, claimed Brazil and led the first expedition that linked Europe, Africa, America, and Asia; Diogo Dias, who discovered the eastern coast of Madagascar and rounded the corner of Africa; explorers such as Diogo Fernandes Pereira and Pedro Mascarenhas (1470–1555), among others, who discovered and mapped the Mascarene Islands and other archipelagos; António de Abreu (c.1480–c.1514) and Francisco Serrão (14?–1521), who led the first direct European fleet into the Pacific Ocean (on its western edges), through the Sunda Islands, reaching the Moluccas; Juan Ponce de León (1474–1521), who discovered and mapped the coast of Florida; Vasco Núñez de Balboa (c. 1475–1519), who was the first European to view the Pacific Ocean from American shores (after crossing the Isthmus of Panama) confirming that America was a separate continent from Asia; Ferdinand Magellan (1480–1521), who was the first navigator to cross the Pacific Ocean, discovering the Strait of Magellan, the Tuamotus and Mariana Islands, achieving a nearly complete circumnavigation of the Earth, in multiple voyages, for the first time; Juan Sebastian Elcano (1476–1526), who completed the first global circumnavigation; Aleixo Garcia (14?–1527), who explored the territories of present-day southern Brazil, Paraguay and Bolivia, crossing the Chaco and reaching the Andes (near Sucre); Jorge de Menezes (c. 1498–?), who discovered Papua New Guinea; García Jofre de Loaísa (1490–1526), who discovered the Marshall Islands; Álvar Núñez Cabeza de Vaca (1490–1558), who discovered the Mississippi River and was the first European to sail the Gulf of Mexico and cross Texas; Jacques Cartier (1491–1557), who drew the first maps of part of central and maritime Canada; Andres de Urdaneta (1498–1568), who discovered the maritime route from Asia to the Americas; Francisco Vázquez de Coronado (1510–1554), who discovered the Grand Canyon and the Colorado River; Francisco de Orellana (1511–1546), who was the first European to navigate the length of the Amazon River.		Continuing in the second half of the 16th century and the 17th century with explorers such as Andrés de Urdaneta (1498–1568), who discovered the maritime route from Asia to the Americas; Álvaro de Mendaña (1542–1595), who discovered the Tuvalu archipelago, the Marquesas, the Solomon Islands and Wake Island; Willem Janszoon (1570–1630), who made the first recorded European landing in Australia; Pedro Fernandes de Queirós (1565–1614), who discovered the Pitcairn Islands and the Vanuatu archipelago; Yñigo Ortiz de Retez, who discovered and reached eastern and northern New Guinea; Luis Váez de Torres (1565–1613), who discovered the Torres Strait between Australia and New Guinea; Henry Hudson (156?–1611), who explored the Hudson Bay in Canada; Samuel de Champlain (1574–1635), who explored St. Lawrence River and the Great Lakes (in Canada and northern United States); Abel Tasman (1603–1659), who explored North Australia, discovered Tasmania and New Zealand; and René-Robert Cavelier, Sieur de La Salle (1643–1687), who explored the Great Lakes region of the United States and Canada, and the entire length of the Mississippi River.		Long after the golden age of discovery, other explorers completed the world map, such as various Russians explorers, reaching the Siberian Pacific coast and the Bering Strait, at the extreme edge of Asia and Alaska (North America); Vitus Bering (1681–1741) who in the service of the Russian Navy, explored the Bering Strait, the Bering Sea, the North American coast of Alaska, and some other northern areas of the Pacific Ocean; and James Cook, who explored the east coast of Australia, the Hawaiian Islands, and circumnavigated the Antarctic continent.		Humanity is continuing to follow the impulse to explore, moving beyond Earth. Space exploration started in the 20th century with the invention of exo-atmospheric rockets. This has given humans the opportunity to travel to the Moon, and to send robotic explorers to other planets and far beyond.		Both of the Voyager probes have left the Solar System, bearing imprinted gold discs with multiple data types.		A recent scientific study, performed on mobile phone data of an entire European country and on GPS tracks of private vehicles in Italy, demonstrated that even today individuals naturally split into two well-defined categories according to their mobility habits: returners and explorers.[6] According to this research published on Nature Communications,[6] today's explorers have a tendency to wander between a large number of different locations showing a star-like mobility pattern: they have a central core of locations (composed by home and work places) around which distant core of locations gravitates.[6][7] Interestingly, two features characterize today's explorers: 1) they are more likely to spread infectious diseases when traveling, due to their mobility patterns; 2) they tend to communicate preferably with other explorers, i.e. they show a communication homophily with people in the same mobility category.[6]		Humans have developed specialized tools and strategies to explore specific areas of the Earth, including the arctic, caves, deserts, oceans, urban environments, as well as the Moon. With robotic machines, humans have also explored many parts of the heliosphere, and through measurements, beyond the Solar System and the Milky Way as part of an ongoing global space exploration initiative.		
André Malraux DSO (French: [ɑ̃dʁe malʁo]; 3 November 1901 – 23 November 1976) was a French novelist, art theorist and Minister of Cultural Affairs. Malraux's novel La Condition Humaine (Man's Fate) (1933) won the Prix Goncourt. He was appointed by President Charles de Gaulle as Minister of Information (1945–1946) and subsequently as France's first Minister of Cultural Affairs during de Gaulle's presidency (1959–1969).						Malraux was born in Paris in 1901, the son of Fernand-Georges Malraux and Berthe Lamy (Malraux). His parents separated in 1905 and eventually divorced. There are suggestions that Malraux's paternal grandfather committed suicide in 1909.[1]		Malraux was raised by his mother, maternal aunt Marie and maternal grandmother, Adrienne Lamy-Romagna, who had a grocery store in the small town of Bondy.[1][2] His father, a stockbroker, committed suicide in 1930 after the international crash of the stock market and onset of the Great Depression.[3] From his childhood, associates noticed that André had marked nervousness and motor and vocal tics. The recent biographer Olivier Todd, who published a book on Malraux in 2005, suggests that he had Tourette's syndrome, although that has not been confirmed.[4] Either way, most critics have not seen this as a significant factor in Malraux's life or literary works.		The young Malraux left formal education early, but he followed his curiosity through the booksellers and museums in Paris, and explored its rich libraries as well.		Malraux's first published work, an article entitled "The Origins of Cubist Poetry", appeared in the magazine Action in 1920. This was followed in 1921 by three semi-surrealist tales, one of which, "Paper Moons", was illustrated by Fernand Léger. Malraux also frequented the Parisian artistic and literary milieux of the period, meeting figures such as Demetrios Galanis, Max Jacob, François Mauriac, Guy de Pourtalès, André Salmon, Jean Cocteau, Raymond Radiguet, Florent Fels, Pascal Pia, Marcel Arland, Edmond Jaloux, and Pierre Mac Orlan.[5] In 1922, Malraux married Clara Goldschmidt. Malraux and his first wife separated in 1938 but didn't divorce until 1947. His daughter from this marriage, Florence (b. 1933), married the filmmaker Alain Resnais.[6] By the age of twenty, Malraux was reading the work of the German philosopher Friedrich Nietzsche who was to remain a major influence on him for the rest of his life.[7] Malraux was especially impressed with Nietzsche's theory of a world in continuous turmoil and his statement "that the individual himself is still the most recent creation" who was completely responsible for all of his actions.[7] Most of all, Malraux embraced Nietzsche's theory of the Übermensch, the heroic, exalted man who would create great works of art and whose will would allow him to triumph over anything.[8]		The British Colonel T. E. Lawrence, aka "Lawrence of Arabia" holds a sinister reputation in France as the man who was supposedly responsible for France's troubles in Syria in the 1920s. An exception was Malraux who regarded Lawrence as a role model, the intellectual-cum-man of action and the romantic, enigmatic hero.[9] Malraux often admitted to having a "certain fascination" with Lawrence, and it has been suggested that Malraux's sudden decision to abandon the Surrealist literary scene in Paris for adventure in the Far East was prompted by a desire to emulate Lawrence who began his career as an archaeologist in the Ottoman empire excavating the ruins of the ancient city of Carchemish in the vilayet of Aleppo in what is now modern Syria.[10] As Lawrence had first made his reputation in the Near East digging up the ruins of an ancient civilization, it was only natural that Malraux should go to the Far East to likewise make his reputation in Asia digging up ancient ruins.[11] Lawrence considered himself a writer first and foremost while also presenting himself as a man of action, the Nietzschean hero who triumphs over both the environment and men through the force of his will, a persona that Malraux consciously imitated.[12] Malraux often wrote about Lawrence, whom he described admiringly as a man with a need for "the absolute", for whom no compromises were possible and for whom going all the way was the only way.[13] Along the same lines, Malraux argued that Lawrence should not be remembered mainly as a guerrilla leader in the Arab Revolt and the British liaison officer with the Emir Faisal, but rather as a romantic, lyrical writer as writing was Lawrence's first passion, which also described Malraux very well.[14] Although Malraux courted fame through his novels, poems and essays on art in combination with his adventures and political activism, he was an intensely shy and private man who kept to himself, maintaining a distance between himself and others.[15] Malraux's reticence led his first wife Clara to later say she barely knew him during their marriage.[15]		In 1923, aged 22, Malraux and Clara left for the French Protectorate of Cambodia.[16] Angkor Wat is a huge 12th century Hindu temple situated in the old capital of the Khmer empire. Angkor (Yasodharapura) was "the world’s largest urban settlement" in the 11th and 12th centuries supported by an elaborate network of canals and roads across mainland Southeast Asia before decaying and falling into the jungle.[17] The rediscovery of the ruins of Angkor Wat (the Khmers had never fully abandoned the temples of Angkor) in the jungle by the French explorer Henri Mouhot in 1861 had given Cambodia a romantic reputation in France, as the home of the vast, mysterious ruins of the Khmer empire. Upon reaching Cambodia, Malraux, Clara and friend Louis Chevasson undertook an expedition into unexplored areas of the former imperial settlements in search of hidden temples, hoping to find artifacts and items that could be sold to art collectors and museums. At about the same time archaeologists, with the approval of the French government, were removing large numbers of items from Angkor - many of which are now housed in the Guimet Museum in Paris. On his return, Malraux was arrested and charged by French colonial authorities for removing a bas-relief from the exquisite Banteay Srei temple. Malraux, who believed he had acted within the law as it then stood, contested the charges but was unsuccessful.[18]		Malraux's experiences in Indochina led him to become highly critical of the French colonial authorities there. In 1925, with Paul Monin,[19] a progressive lawyer, he helped to organize the Young Annam League and founded a newspaper L'Indochine to champion Vietnamese independence.[20] After falling afoul of the French authorities, Malraux claimed to have crossed over to China where he was involved with the Kuomintang and their then allies, the Chinese Communists in their struggle against the warlords in the Great Northern Expedition before they turned on each other in 1927, which marked the beginning of the Chinese Civil War that was to last on and off until 1949.[21] In fact, Malraux did not first visit China until 1931 and he did not see the bloody suppression of the Chinese Communists by the Kuomintang in 1927 first-hand as he often implied that he did, although he did do much reading on the subject.[22]		On his return to France, Malraux published The Temptation of the West (1926). The work was in the form of an exchange of letters between a Westerner and an Asian, comparing aspects of the two cultures. This was followed by his first novel The Conquerors (1928), and then by The Royal Way (1930) which reflected some of his Cambodian experiences.[23] The American literary critic Dennis Roak described Les Conquérants as influenced by The Seven Pillars of Wisdom as it was narrated in the present tense "...with its staccato snatches of dialogue and the images of sound and sight, light and darkness, which create a compellingly haunting atmosphere."[14] Les Conquérants was set in the summer of 1925 against the backdrop of the general strike called by the Chinese Communist Party (CCP) and Kuomintang in Hong Kong and Canton, the novel concerns political intrigue amongst the "anti-imperialist" camp.[24] The novel is narrated by an unnamed Frenchman who travels from Saigon to Hong Kong to Canton to meet an old friend named Garine who is a professional revolutionary working with Mikhail Borodin, who in real life was the Comintern's principle agent in China.[24] The Kuomintang are depicted rather unflatteringly as conservative Chinese nationalists uninterested in social reform, another fraction is led by Hong, a Chinese assassin committed to revolutionary violence for the sake of violence, and only the Communists are portrayed relatively favorably.[25] Much of the dramatic tension between the novel concerns a three-way struggle between the hero, Garine and Borodin who is only interested in using the revolution in China to achieve Soviet foreign policy goals.[25] The fact that the European characters are considerably better drawn than the Asian characters reflected Malraux's understanding of China at the time more of an exotic place where Europeans played out their own dramas rather than a place to be understood in its own right. Initially, Malraux's writings on Asia reflected the influence of "Orientalism" presenting the Far East as strange, exotic, decadent, mysterious, sensuous and violent, but Malraux's picture of China grew somewhat more humanized and understanding as Malraux disregarded his Orientalist and Eurocentric viewpoint in favor of one that presented the Chinese as fellow human beings.[26]		The second of Malraux's Asian novels was the semi-autobiographical La Voie Royale which relates the adventures of a Frenchman Claude Vannec who together with his Danish friend Perken head down the royal road of the title into the jungle of Cambodia with the intention of stealing bas-relief sculptures from the ruins of Hindu temples.[27] After many perilous adventures, Vannec and Perken are captured by hostile tribesmen and find an old friend of Perken's Grabot who has already been captured for some time.[28] Grabot, a deserter from the French Foreign Legion has been reduced to nothing as his captors have blinded him and left him tied to a stake starving, a stark picture of human degradation.[28] The three Europeans escape, but Perken is wounded and dies of an infection.[28] Through ostensibly an adventure novel, La Voie Royale is in fact a philosophical novel concerned with existential questions about the meaning of life.[28] The book was a failure at the time as the publishers marketed it as a stirring adventure story set in far-off, exotic Cambodia which confused many readers who instead found a novel pondering deep philosophical questions.[29]		In his Asian novels, Malraux used Asia as a stick to beat Europe with, as he argued that after World War I, the ideal of progress, of a Europe getting better and better for the general advancement of humanity was dead.[30] As such, Malraux now argued that European civilization was faced with a Nietzschean void, a twilight world without God or progress, in which the old values had proven worthless and a sense of spirituality that had once existed was gone.[30] An agnostic, but an intensely spiritual man, Malraux maintained what was needed was an "aesthetic spirituality" in which love of art and civilization would allow one to appreciate le sacré in life, a sensibility that was both tragic and awe-inspiring as one surveyed all of the cultural treasures of the world, a mystical sense of humanity's place in a universe that was as astonishingly beautiful as it was mysterious.[30] Malraux argued that as death is inevitable and in a world devoid of meaning, which thus was "absurd", only art could offer meaning in an "absurd" world.[31] Malraux argued that art transcended time as art allowed one to connect with the past, and the very act of appreciating art was itself an act of art as the love of art was part of a continuation of endless artistic metamorphosis that constantly creating something new.[31] Malraux argued that as different types of art went in and out of style, the revival of a style was a metamorphosis as art could never be appreciated in exactly the same way as it was in the past.[31] As art was timeless, it conquered time and death as artworks lived on after the death of the artist.[31] The American literary critic Jean-Pierre Hérubel wrote that Malraux never entirely worked out a coherent philosophy as his mystical Weltanschauung (world view) was based more upon emotion than logic.[30] In Malraux's viewpoint, of all the professions, the artist was the most important as artists were the explorers and voyagers of the human spirit as artistic creation was the highest form of human achievement for only art could illustrate humanity's relationship with the universe as Malraux wrote "there is something far greater than history and it is the persistence of genius".[30] Hérubel argued that it is fruitless to attempt to criticize Malraux for his lack of methodological consistency as Malraux cultivated a poetical sensibility, a certain lyrical style that appealed more to the heart than to the brain.[32] Malraux was a proud Frenchman, but he also saw himself as a citizen of the world, a man who loved the cultural achievements of all of the civilizations across the globe.[32] At the same time, Malraux criticized those intellectuals who wanted to retreat into the ivory tower, instead arguing that it was the duty of intellectuals to participate and fight (both metaphorically and literally) in the great political causes of the day, that the only truly great causes were the ones that one was willing to die for.[15]		In 1933 Malraux published Man's Fate (La Condition Humaine), a novel about the 1927 failed Communist rebellion in Shanghai. Despite Malraux's attempts to present his Chinese characters as more three dimensional and developed than he did in Les Conquérants , his biographer Oliver Todd wrote he could not "quite break clear of a conventional idea of China with coolies, bamboo shoots, opium smokers, destitutes, and prostitutes", which were the standard French stereotypes of China at the time.[33] The work was awarded the 1933 Prix Goncourt.[34] After the breakdown of his marriage with Clara, Malraux lived with journalist and novelist Josette Clotis, starting in 1933. Malraux and Josette had two sons: Pierre-Gauthier (1940–1961) and Vincent (1943–1961). During 1944, while Malraux was fighting in Alsace, Josette died, aged 34, when she slipped while boarding a train. His two sons died together in 1961 in an automobile accident.		On 22 February 1934, Malraux together with Édouard Corniglion-Molinier embarked on a much publicized expedition to find the lost capital of the Queen of Sheba mentioned in the Old Testament.[35] Saudi Arabia and Yemen were both remote, dangerous places that few Westerners visited at the time, and what made the expedition especially dangerous was while Malraux was searching for the lost cities of Sheba, King Ibn Saud of Saudi Arabia invaded Yemen, and the ensuring Saudi-Yemeni war greatly complicated Malraux's search.[36] After several weeks of flying over the deserts in Saudi Arabia and Yemen, Malraux returned to France to announce that ruins he found up in the mountains of Yemen were the capital of the Queen of Sheba.[35] Through Malraux's claim is not generally accepted by archeologists, the expedition bolstered Malraux's fame and provided the material for several of his later essays.[35]		During the 1930s, Malraux was active in the anti-fascist Popular Front in France. At the beginning of the Spanish Civil War he joined the Republican forces in Spain, serving in and helping to organize the small Spanish Republican Air Force.[37] Curtis Cate, one of his biographers, claims that Malraux was slightly wounded twice during efforts to stop the Battle of Madrid in 1936 as the Spanish Nationalists attempted to take Madrid, but the historian Hugh Thomas claims otherwise.		The French government sent aircraft to Republican forces in Spain, but they were obsolete by the standards of 1936. They were mainly Potez 540 bombers and Dewoitine D.372 fighters. The slow Potez 540 rarely survived three months of air missions, flying at 160 knots against enemy fighters flying at more than 250 knots. Few of the fighters proved to be airworthy, and they were delivered intentionally without guns or gunsights. The Ministry of Defense of France had feared that modern types of planes would easily be captured by the German Condor Legion fighting with General Francisco Franco, and the lesser models were a way of maintaining official "neutrality".[38] The planes were surpassed by more modern types introduced by the end of 1936 on both sides.		The Republic circulated photos of Malraux standing next to some Potez 540 bombers suggesting that France was on their side, at a time when France and the United Kingdom had declared official neutrality. But Malraux's commitment to the Republicans was personal, like that of many other foreign volunteers, and there was never any suggestion that he was there at the behest of the French Government. Malraux himself was not a pilot, and never claimed to be one, but his leadership qualities seem to have been recognized because he was made Squadron Leader of the 'España' squadron. Acutely aware of the Republicans' inferior armaments, of which outdated aircraft were just one example, he toured the United States to raise funds for the cause. In 1938 he published L'Espoir (Man's Hope), a novel influenced by his Spanish war experiences.[39]		Malraux's participation in major historical events such as the Spanish Civil War inevitably brought him determined adversaries as well as strong supporters, and the resulting polarization of opinion has colored, and rendered questionable, much that has been written about his life. Fellow combatants praised Malraux's leadership and sense of camaraderie[40] While André Marty of the Comintern called him as an "adventurer" for his high profile and demands on the Spanish Republican government.[41] The British historian Antony Beevor also claims that "Malraux stands out, not just because he was a mythomaniac in his claims of martial heroism – in Spain and later in the French Resistance – but because he cynically exploited the opportunity for intellectual heroism in the legend of the Spanish Republic."[41] Both these claims are highly questionable. Malraux’s only work about the Spanish Revolution was a novel – in which he does not figure at all. His later works occasionally describe episodes during the Resistance[42] but in none of them does he make anything resembling "claims of martial heroism”. Indeed, in one episode, he portrays himself in a somewhat unflattering light. The Comintern's remarks are, of course, very unsurprising. During the Spanish Civil War it was normal Communist practice to denigrate anyone who "deviated"; indeed many suffered far worse fates than that as Orwell makes clear in Homage to Catalonia.		In any case, Malraux's participation in events such as the Spanish Civil War has tended to distract attention from his important literary achievement. Malraux saw himself first and foremost as a writer and thinker (and not a "man of action" as biographers so often portray him) but his extremely eventful life – a far cry from the stereotype of the French intellectual confined to his study or a Left Bank café – has tended to obscure this fact. As a result, his literary works, including his important works on the theory of art, have received less attention than one might expect, especially in Anglophone countries.[43]		At the beginning of the Second World War, Malraux joined the French Army. He was captured in 1940 during the Battle of France but escaped and later joined the French Resistance.[44] In 1944, he was captured by the Gestapo.[45] He later commanded the tank unit Brigade Alsace-Lorraine in defence of Strasbourg and in the attack on Stuttgart.[46]		Otto Abetz was the German Ambassador, and produced a series of "black lists" of authors forbidden to be read, circulated or sold in Nazi Occupied France. These included anything written by a Jew, a communist, an Anglo-Saxon or anyone else who was anti-Germanic or anti-fascist. Louis Aragon and André Malraux were both on these "Otto Lists" of forbidden authors.[47]		After the war, Malraux was awarded the Médaille de la Résistance and the Croix de guerre. The British awarded him the Distinguished Service Order, for his work with British liaison officers in Corrèze, Dordogne and Lot. After Dordogne was liberated, Malraux led a battalion of former resistance fighters to Alsace-Lorraine, where they fought alongside the First Army.[48]		During the war, he worked on his last novel, The Struggle with the Angel, the title drawn from the story of the Biblical Jacob. The manuscript was destroyed by the Gestapo after his capture in 1944. A surviving first section, titled The Walnut Trees of Altenburg, was published after the war.		Shortly after the war, General Charles de Gaulle appointed Malraux as his Minister for Information (1945–1946). Soon after, he completed his first book on art, The Psychology of Art, published in three volumes (1947–1949). The work was subsequently revised and republished in one volume as The Voices of Silence (Les Voix du Silence), the first part of which has been published separately as The Museum without Walls. Other important works on the theory of art were to follow. These included the three-volume Metamorphosis of the Gods and Precarious Man and Literature, the latter published posthumously in 1977. In 1948, Malraux married a second time, to Marie-Madeleine Lioux, a concert pianist and the widow of his half-brother, Roland Malraux. They separated in 1966. Subsequently, Malraux lived with Louise de Vilmorin in the Vilmorin family château at Verrières-le-Buisson, Essonne, a suburb southwest of Paris. Vilmorin was best known as a writer of delicate but mordant tales, often set in aristocratic or artistic milieu. Her most famous novel was Madame de..., published in 1951, which was adapted into the celebrated film The Earrings of Madame de… (1953), directed by Max Ophüls and starring Charles Boyer, Danielle Darrieux and Vittorio de Sica. Vilmorin's other works included Juliette, La lettre dans un taxi, Les belles amours, Saintes-Unefois, and Intimités. Her letters to Jean Cocteau were published after the death of both correspondents. After Louise's death, Malraux spent his final years with her relative, Sophie de Vilmorin.		In 1957, Malraux published the first volume of his trilogy on art entitled The Metamorphosis of the Gods. The second two volumes (not yet translated into English) were published shortly before he died. They are entitled L’Irréel and L'Intemporel and discuss artistic developments from the Renaissance to modern times. Malraux also initiated the series Arts of Mankind, an ambitious survey of world art that generated more than thirty large, illustrated volumes.		When de Gaulle returned to the French presidency in 1958, Malraux became France's first Minister of Cultural Affairs, a post he held from 1958 to 1969. On 7 February 1962, Malraux was the target of assassination attempt by the Organisation armée secrète (OAS) who set off a bomb to his apartment building that failed to kill its intended target, but did leave a four-year girl living in the adjudging apartment blinded by the shrapnel.[50] Ironically, Malraux was a lukewarm supporter of de Gaulle's decision to grant independence to Algeria, but the OAS was not aware of this, and had decided to assassinate Malraux as a high profile minister.		Among many initiatives, Malraux launched an innovative (and subsequently widely imitated) program to clean the blackened façades of notable French buildings, revealing the natural stone underneath.[51] He also created a number of maisons de la culture in provincial cities and worked to preserve France's national heritage by promoting industrial archaeology.[52] An intellectual who took the arts very seriously, Malraux saw his mission as Cultural Minister to preserve France's heritage and to improve the cultural levels of the masses.[53] Malraux's efforts to promote French culture mostly concerned renewing old or building new libraries, art galleries, museums, theaters, opera houses, and maisons de la culture (centers built in provincial cities that were a mixture of a library, art gallery and theater).[52] Film, television and music took less of Malraux's time, and the changing demographics caused by immigration from the Third World stymied his efforts to promote French high culture as many immigrants from Muslim and African nations did not find French high culture that compelling.[52] A passionate bibliophile, Malraux built up a huge collections of books both as a cultural minister for the nation and as a man for himself.[54]		Malraux was an outspoken supporter of the Bangladesh liberation movement during the 1971 Pakistani Civil War and despite his age seriously considered joining the struggle. When Indira Gandhi came to Paris in November 1971, there was extensive discussion between them about the situation in Bangladesh.		During this post-war period, Malraux also published a series of semi-autobiographical works, the first entitled Antimémoires (1967). A later volume in the series, Lazarus, is a reflection on death occasioned by his experiences during a serious illness. La Tête d'obsidienne (1974) (translated as Picasso's Mask) concerns Picasso, and visual art more generally. In his last book published posthumously in 1977, L'Homme précaire et la littérature, Malraux propounded the theory that there was a bibliothéque imaginarie where writers created works that influenced subsequent writers much as painters learned their craft by studying the old masters, and once have mastered the work of the old masters, writers would sally fourth with the knowledge gained to create new works that added to the growing and never-ending bibliothéque imaginarie.[52] An elitist who appreciated what he saw as the high culture of all the nations of the world, Malraux was especially interested in art history and archeology, and saw his duty as an artist to share what he knew with ordinary people.[52] An aesthete, Malraux believed that art was spiritually-enriching and necessary for humanity.[55]		Malraux died in Créteil, near Paris, on 23 November 1976. He was buried in the Verrières-le-Buisson (Essonne) cemetery. In recognition of his contributions to French culture, his ashes were moved to the Panthéon in Paris during 1996, on the twentieth anniversary of his death.		There is now a large and steadily growing body of critical commentary on Malraux's literary œuvre, including his very extensive writings on art. Unfortunately, some of his works, including the last two volumes of The Metamorphosis of the Gods (L'Irréel and L'Intemporel) are not yet available in English translation. Malraux's works on the theory of art contain a revolutionary approach to art that challenges the Enlightenment tradition that treats art simply as a source of "aesthetic pleasure". However, as French writer André Brincourt has commented, Malraux's books on art have been "skimmed a lot but very little read"[56] (this is especially true in Anglophone countries) and the radical implications of his thinking are often missed. A particularly important aspect of Malraux's thinking about art is his explanation of the capacity of art to transcend time. In contrast to the traditional notion that art endures because it is timeless ("eternal"), Malraux argues that art lives on through metamorphosis – a process of resuscitation (where the work had fallen into obscurity) and transformation in meaning.[57]		"Man is dead, after God". Malraux, The Temptation of the West. (1926)		‘The artist is not the transcriber of the world, he is its rival.’ Malraux, L'Intemporel (3rd volume of The Metamorphosis of the Gods.)		"What is a man? A miserable little pile of secrets" Antimémoires, preface (1967)		'In a world in which everything is subject to the passing of time, art alone is both subject to time and yet victorious over it'. Malraux in a television program about art, 1975.		"Art is an object lesson for the gods." The Voices of Silence		"The art museum is one of the places that give us the highest idea of man." The Voices of Silence		"Humanism does not consist in saying: ‘No animal could have done what I have done,’ but in declaring: ‘We have refused what the beast within us willed to do, and we seek to reclaim man wherever we find that which crushes him.’" The Voices of Silence		"The greatest mystery is not that we have been flung at random between this profusion of matter and the stars, but that within this prison we can draw from ourselves images powerful enough to deny our nothingness." Les Noyers de l'Altenburg		For a more complete bibliography, see site littéraire André Malraux.[60]		
Alice's Adventures in Wonderland (commonly shortened to Alice in Wonderland) is an 1865 fantasy novel written by English mathematician Charles Lutwidge Dodgson under the pseudonym Lewis Carroll. It tells of a girl named Alice falling through a rabbit hole into a fantasy world populated by peculiar, anthropomorphic creatures. The tale plays with logic, giving the story lasting popularity with adults as well as with children.[1] It is considered to be one of the best examples of the literary nonsense genre.[1][2] Its narrative course and structure, characters and imagery have been enormously influential[2] in both popular culture and literature, especially in the fantasy genre.						Alice was published in 1865, three years after Charles Lutwidge Dodgson and the Reverend Robinson Duckworth rowed a boat up the Isis on 4 July 1862[3] (this popular date of the "golden afternoon"[4] might be a confusion or even another Alice-tale, for that particular day was cool, cloudy, and rainy[5]) with the three young daughters of Henry Liddell (the Vice-Chancellor of Oxford University and Dean of Christ Church): Lorina Charlotte Liddell (aged 13, born 1849, "Prima" in the book's prefatory verse); Alice Pleasance Liddell (aged 10, born 1852, "Secunda" in the prefatory verse); Edith Mary Liddell (aged 8, born 1853, "Tertia" in the prefatory verse).[6]		The journey began at Folly Bridge in Oxford and ended 3 miles (5 km) north-west in the village of Godstow. During the trip, Dodgson told the girls a story that featured a bored little girl named Alice who goes looking for an adventure. The girls loved it, and Alice Liddell asked Dodgson to write it down for her. He began writing the manuscript of the story the next day, although that earliest version no longer exists. The girls and Dodgson took another boat trip a month later when he elaborated the plot to the story of Alice, and in November he began working on the manuscript in earnest.[7]		To add the finishing touches, he researched natural history for the animals presented in the book, and then had the book examined by other children—particularly the children of George MacDonald. It was also MacDonald, and Henry Kingsley, who encouraged him to publish the story.[8] Carroll added his own illustrations but approached John Tenniel to illustrate the book for publication, telling him that the story had been well liked by children.[7]		On 26 November 1864, he gave Alice the handwritten manuscript of Alice's Adventures Under Ground, with illustrations by Dodgson himself, dedicating it as "A Christmas Gift to a Dear Child in Memory of a Summer's Day".[9] Some, including Martin Gardner, speculate that there was an earlier version that was destroyed later by Dodgson when he wrote a more elaborate copy by hand.[10]		But before Alice received her copy, Dodgson was already preparing it for publication and expanding the 15,500-word original to 27,500 words,[11] most notably adding the episodes about the Cheshire Cat and the Mad Tea-Party.		Chapter One – Down the Rabbit Hole: Alice is feeling bored and drowsy while sitting on the riverbank with her older sister, who is reading a book with no pictures or conversations. She then notices a White Rabbit wearing a waistcoat and pocket watch, talking to itself as it runs past. She follows it down a rabbit hole, but suddenly falls a long way to a curious hall with many locked doors of all sizes. She finds a small key to a door too small for her to fit through, but through it she sees an attractive garden. She then discovers a bottle on a table labelled "DRINK ME", the contents of which cause her to shrink too small to reach the key, which she has left on the table. She eats a cake with "EAT ME" written on it in currants as the chapter closes.		Chapter Two – The Pool of Tears: Chapter Two opens with Alice growing to such a tremendous size that her head hits the ceiling. Alice is unhappy and, as she cries, her tears flood the hallway. After shrinking down again due to a fan she had picked up, Alice swims through her own tears and meets a Mouse, who is swimming as well. She tries to make small talk with him in elementary French (thinking he may be a French mouse) but her opening gambit "Où est ma chatte?" ("Where is my cat?") offends the mouse and he tries to escape her.		Chapter Three – The Caucus Race and a Long Tale: The sea of tears becomes crowded with other animals and birds that have been swept away by the rising waters. Alice and the other animals convene on the bank and the question among them is how to get dry again. The Mouse gives them a very dry lecture on William the Conqueror. A Dodo decides that the best thing to dry them off would be a Caucus-Race, which consists of everyone running in a circle with no clear winner. Alice eventually frightens all the animals away, unwittingly, by talking about her (moderately ferocious) cat.		Chapter Four – The Rabbit Sends a Little Bill: The White Rabbit appears again in search of the Duchess's gloves and fan. Mistaking her for his maidservant, Mary Ann, he orders Alice to go into the house and retrieve them, but once she gets inside she starts growing. The horrified Rabbit orders his gardener, Bill the Lizard, to climb on the roof and go down the chimney. Outside, Alice hears the voices of animals that have gathered to gawk at her giant arm. The crowd hurls pebbles at her, which turn into little cakes. Alice eats them, and they make her smaller again.		Chapter Five – Advice from a Caterpillar: Alice comes upon a mushroom; sitting on it is a blue Caterpillar smoking a hookah. The Caterpillar questions Alice and she admits to her current identity crisis, compounded by her inability to remember a poem. Before crawling away, the caterpillar tells Alice that one side of the mushroom will make her taller and the other side will make her shorter. She breaks off two pieces from the mushroom. One side makes her shrink smaller than ever, while another causes her neck to grow high into the trees, where a pigeon mistakes her for a serpent. With some effort, Alice brings herself back to her normal height. She stumbles upon a small estate and uses the mushroom to reach a more appropriate height.		Chapter Six – Pig and Pepper: A Fish-Footman has an invitation for the Duchess of the house, which he delivers to a Frog-Footman. Alice observes this transaction and, after a perplexing conversation with the frog, lets herself into the house. The Duchess's Cook is throwing dishes and making a soup that has too much pepper, which causes Alice, the Duchess, and her baby (but not the cook or grinning Cheshire Cat) to sneeze violently. Alice is given the baby by the Duchess and to her surprise, the baby turns into a pig. The Cheshire Cat appears in a tree, directing her to the March Hare's house. He disappears, but his grin remains behind to float on its own in the air, prompting Alice to remark that she has often seen a cat without a grin but never a grin without a cat.		Chapter Seven – A Mad Tea-Party: Alice becomes a guest at a "mad" tea party along with the March Hare, the Hatter, and a very tired Dormouse who falls asleep frequently, only to be violently woken up moments later by the March Hare and the Hatter. The characters give Alice many riddles and stories, including the famous "Why is a raven like a writing desk?". The Hatter reveals that they have tea all day because Time has punished him by eternally standing still at 6 pm (tea time). Alice becomes insulted and tired of being bombarded with riddles and she leaves, claiming that it was the stupidest tea party that she had ever been to.		Chapter Eight – The Queen's Croquet Ground: Alice leaves the tea party and enters the garden, where she comes upon three living playing cards painting the white roses on a rose tree red because The Queen of Hearts hates white roses. A procession of more cards, kings and queens and even the White Rabbit enters the garden. Alice then meets the King and Queen. The Queen, a figure difficult to please, introduces her trademark phrase "Off with her head!", which she utters at the slightest dissatisfaction with a subject. Alice is invited (or some might say ordered) to play a game of croquet with the Queen and the rest of her subjects, but the game quickly descends into chaos. Live flamingos are used as mallets and hedgehogs as balls, and Alice once again meets the Cheshire Cat. The Queen of Hearts then orders the Cat to be beheaded, only to have her executioner complain that this is impossible since the head is all that can be seen of him. Because the cat belongs to the Duchess, the Queen is prompted to release the Duchess from prison to resolve the matter.		Chapter Nine – The Mock Turtle's Story: The Duchess is brought to the croquet ground at Alice's request. She ruminates on finding morals in everything around her. The Queen of Hearts dismisses her with the threat of execution and she introduces Alice to the Gryphon, who takes her to the Mock Turtle. The Mock Turtle is very sad, even though he has no sorrow. He tries to tell his story about how he used to be a real turtle in school, which the Gryphon interrupts so that they can play a game.		Chapter Ten – Lobster Quadrille: The Mock Turtle and the Gryphon dance to the Lobster Quadrille, while Alice recites (rather incorrectly) "'Tis the Voice of the Lobster". The Mock Turtle sings them "Beautiful Soup" during which the Gryphon drags Alice away for an impending trial.		Chapter Eleven – Who Stole the Tarts?: Alice attends a trial in which the Knave of Hearts is accused of stealing the Queen's tarts. The jury is composed of various animals, including Bill the Lizard; the White Rabbit is the court's trumpeter; and the judge is the King of Hearts. During the proceedings, Alice finds that she is steadily growing larger. The dormouse scolds Alice and tells her she has no right to grow at such a rapid pace and take up all the air. Alice scoffs and calls the dormouse's accusation ridiculous because everyone grows and she cannot help it. Meanwhile, witnesses at the trial include the Hatter, who displeases and frustrates the King through his indirect answers to the questioning, and the Duchess's cook.		Chapter Twelve – Alice's Evidence: Alice is then called up as a witness. She accidentally knocks over the jury box with the animals inside, and the King orders the animals to be placed back into their seats before the trial continues. The King and Queen order Alice to be gone, citing Rule 42 ("All persons more than a mile high must leave the court"), but Alice disputes their judgement and refuses to leave. She argues with the King and Queen of Hearts over the ridiculous proceedings, eventually refusing to hold her tongue. The Queen shouts her familiar "Off with her head!" but Alice is unafraid, calling them out as just a pack of cards, just as they start to swarm over her. Alice's sister wakes her up from a dream, brushing what turns out to be some leaves, and not a shower of playing cards, from Alice's face. Alice leaves her sister on the bank to imagine all the curious happenings for herself.		The following is a list of main characters in Alice's Adventures in Wonderland.		In The Annotated Alice, Martin Gardner provides background information for the characters. The members of the boating party that first heard Carroll's tale show up in Chapter 3 ("A Caucus-Race and a Long Tale"):[12]		Carroll wrote multiple poems and songs for Alice's Adventures in Wonderland, including:		Some of the book's adventures may have been based on or influenced by people, situations, and buildings in Oxford and at Christ Church. For example, the "Rabbit Hole" might have been inspired by the actual stairs in the back of the main hall in Christ Church. A carving of a griffon and rabbit may have provided inspiration for the tale, as seen in Ripon Cathedral, where Carroll's father was a canon.[21]		Carroll was a mathematician at Christ Church, and it has been suggested[22][23] that there are many references and mathematical concepts in both this story and Through the Looking-Glass; examples include:		There have been attempts to reinterpret Alice's Adventures in Wonderland as having a coded submersive layer of meaning, as is common in other examples of children's literature (such as the gold standard reading of The Wonderful Wizard of Oz (1900)). For example, literary scholar Melanie Bayley asserted in the magazine New Scientist that Dodgson wrote Alice in Wonderland in its final form as a scathing satire on new modern mathematics that were emerging in the mid-19th century.[24] However, such views are not widely held.		Several people (e.g., Martin Gardner and Selwyn Goodacre,[22]) have suggested that Dodgson had an interest in the French language, choosing to make references and puns about it in the story. It is most likely that these are references to French lessons—a common feature of a Victorian middle-class girl's upbringing. For example, in the second chapter, Alice posits that the mouse may be French. She therefore chooses to speak the first sentence of her French lesson-book to it: "Où est ma chatte?" ("Where is my cat?"). In Henri Bué's French translation, Alice posits that the mouse may be Italian and speaks Italian to it.		Pat's "Digging for apples" could be a cross-language pun, as pomme de terre (literally; "apple of the earth") means potato and pomme means apple, which little English girls studying French would easily guess.[25]		In the second chapter, Alice initially addresses the mouse as "O Mouse", based on her memory of the noun declensions "in her brother's Latin Grammar, 'A mouse – of a mouse – to a mouse – a mouse – O mouse!'" These words correspond to the first five of Latin's six cases, in a traditional order established by medieval grammarians: mus (nominative), muris (genitive), muri (dative), murem (accusative), (O) mus (vocative). The sixth case mure (ablative) is absent from Alice's recitation.		In the eighth chapter, three cards are painting the roses red on a rose tree, because they had accidentally planted a white-rose tree that The Queen of Hearts hates. Red roses symbolised the English House of Lancaster, while white roses were the symbol for their rival, the House of York. This scene is an allusion to the Wars of the Roses.[26]		Carina Garland notes how the world is "expressed via representations of food and appetite", naming Alice's frequent desire for consumption (of both food and words) her "Curious Appetites".[27] Often, the idea of eating coincides to make gruesome images. After the riddle "Why is a raven like a writing-desk?", the Hatter claims that Alice might as well say, "I see what I eat… I eat what I see" and so the riddle's solution, put forward by Boe Birns,[28] could be that "A raven eats worms; a writing desk is worm-eaten"; this idea of food encapsulates the idea of life feeding on life, for the worm is being eaten and then becomes the eater  – a horrific image of mortality.		Nina Auerbach discusses how the novel revolves around eating and drinking, which "motivates much of [Alice's] behaviour", for the story is essentially about things "entering and leaving her mouth".[29] The animals of Wonderland are of particular interest, for Alice's relation to them shifts constantly because, as Lovell-Smith states, Alice's size-changes continually reposition her in the food chain, serving as a way to make her acutely aware of the "eat or be eaten" attitude that permeates Wonderland.[30]		The manuscript was illustrated by Dodgson himself who added 37 illustrations—printed in a facsimile edition in 1887.[9] John Tenniel provided 42 wood engraved illustrations for the published version of the book. The first print run was destroyed (or sold to the United States[31]) at Carroll's request because he was dissatisfied with the quality. The book was reprinted and published in 1866.[9]		Neither Dodgson's nor John Tenniel's illustrations of Alice portray the real Alice Liddell, who had dark hair and a short fringe as in the Charles Robinson illustration pictured above.		Alice has provided a challenge for other illustrators, including those of 1907 by Charles Pears and the full series of colour plates and line-drawings by Harry Rountree published in the (inter-War) Children's Press (Glasgow) edition. Other significant illustrators include: Arthur Rackham (1907), Charles Robinson (1907) Willy Pogany (1929), Mervyn Peake (1946), Ralph Steadman (1967), Salvador Dalí (1969), Graham Overden (1969), Max Ernst (1970), Peter Blake (1970), Tove Jansson (1977), Anthony Browne (1988), Helen Oxenbury (1999), Lisbeth Zwerger (1999), DeLoss McGraw (2001), Robert Ingpen (2009) and Yayoi Kusama (2012).		The book Alice in Wonderland failed to be named in an 1888 poll of the most popular children's stories. Generally, it received poor reviews, with reviewers giving more credit to Tenniel's illustrations than to Carroll's story. At the release of Through the Looking-Glass, the first Alice tale gained in popularity and, by the end of the 19th century, Sir Walter Besant wrote that Alice in Wonderland "was a book of that extremely rare kind which will belong to all the generations to come until the language becomes obsolete".[32]		Dodgson's tale was published in 1865 as Alice's Adventures in Wonderland by "Lewis Carroll" with illustrations by John Tenniel. The first print run of 2,000 was held back because Tenniel objected to the print quality.[33] A new edition was quickly printed, released in December of the same year but carrying an 1866 date. The text blocks of the original edition were removed from the binding and sold with Dodgson's permission to the New York publishing house of D. Appleton & Company. The binding for the Appleton Alice was virtually identical to the 1866 Macmillan Alice, except for the publisher's name at the foot of the spine. The title page of the Appleton Alice was an insert cancelling the original Macmillan title page of 1865, and bearing the New York publisher's imprint and the date 1866.		The entire print run sold out quickly. Alice was a publishing sensation, beloved by children and adults alike. Among its first avid readers were Queen Victoria[34] and the young Oscar Wilde.[35] The book has never been out of print. Alice's Adventures in Wonderland has been translated into at least 174 languages.[36] There have now been over a hundred English-language editions of the book, as well as countless adaptations in other media, especially theatre and film.		The book is commonly referred to by the abbreviated title Alice in Wonderland, which has been popularised by the numerous stage, film, and television adaptations of the story produced over the years. Some printings of this title contain both Alice's Adventures in Wonderland and its sequel Through the Looking-Glass, and What Alice Found There.		The following list is a timeline of major publication events related to Alice's Adventures in Wonderland:		The book has inspired numerous film and television adaptations which have multiplied as the original work is now in the public domain in all jurisdictions. The following list is of direct adaptations of Adventures in Wonderland (sometimes merging it with Through the Looking-Glass), not other sequels or works otherwise inspired by the works (such as Tim Burton's 2010 film Alice in Wonderland):		The book has also inspired numerous comic book adaptations:		As the book and its sequel are Carroll's most widely recognised works, they have also inspired numerous live performances, including ballets, musicals, operas, plays, and traditional English pantomimes. These works range from fairly faithful adaptations to those that use the story as a basis for new works. Additionally, over the years, many notable people in the performing arts have been involved in Alice productions.		Alice has inspired numerous songs and albums, including:		The book has inspired several parodies, including:		Several radio adaptations have been aired, including:		Alice and the rest of Wonderland continue to inspire or influence many other works of art to this day, sometimes indirectly via the 1951 Disney movie, for example. The character of the plucky yet proper Alice has proven immensely popular and inspired similar heroines in literature and pop culture, many also named Alice in homage.		The cover illustration, by E. Gertrude Thomson		The White Rabbit by John Tenniel, coloured		Alice's Adventures in Wonderland, John Tenniel, 1865		Alice's Adventures in Wonderland by Arthur Rackham		Alice's Adventures in Wonderland by Gertrude Kay		An illustration by Karl Beutel		The Pool of Tears by Arthur Rackham		The Pool of Tears by Milo Winter		
Book of the Marvels of the World (French: Livre des Merveilles du Monde) or Description of the World (Devisement du Monde), in Italian Il Milione (The Million) or Oriente Poliano and in English commonly called The Travels of Marco Polo, is a 13th-century travelogue written down by Rustichello da Pisa from stories told by Marco Polo, describing Polo's travels through Asia between 1276 and 1291, and his experiences at the court of Kublai Khan.[1][2]		The book was written in Old French by romance writer Rustichello da Pisa, who worked from accounts which he had heard from Marco Polo when they were imprisoned together in Genoa.[3] From the beginning, there has been incredulity over Polo's sometimes fabulous stories, as well as a scholarly debate in recent times. Some have questioned whether Marco had actually travelled to China or was just repeating stories that he had heard from other travellers.[4]		Economic historian Mark Elvin concludes that recent work "demonstrates by specific example the ultimately overwhelming probability of the broad authenticity" of Polo's account, and that the book is, "in essence, authentic, and, when used with care, in broad terms to be trusted as a serious though obviously not always final, witness."[5]						The source of the title Il Milione is debated. One view is it comes from the Polo family's use of the name Emilione to distinguish themselves from the numerous other Venetian families bearing the name Polo.[6] A more common view is that the name refers to medieval reception of the travelog, namely that it was full of "a million" lies.[7]		Modern assessments of the text usually consider it to be the record of an observant rather than imaginative or analytical traveller. Marco Polo emerges as being curious and tolerant, and devoted to Kublai Khan and the dynasty that he served for two decades. The book is Polo's account of his travels to China, which he calls Cathay (north China) and Manji (south China). The Polo party left Venice in 1271. The journey took 3 years after which they arrived in Cathay as it was then called and met the Grandson of Genghis Khan, Kublai Khan. They left China in late 1290 or early 1291[8] and were back in Venice in 1295. The tradition is that Polo dictated the book to a romance writer, Rustichello da Pisa, while in prison in Genoa between 1298–1299. Rustichello may have worked up his first Franco-Italian version from Marco's notes. The book was then named Devisement du Monde and Livres des Merveilles du Monde in French, and De Mirabilibus Mundi in Latin.[9]		The Travels is divided into four books. Book One describes the lands of the Middle East and Central Asia that Marco encountered on his way to China. Book Two describes China and the court of Kublai Khan. Book Three describes some of the coastal regions of the East: Japan, India, Sri Lanka, Southeast Asia, and the east coast of Africa. Book Four describes some of the then-recent wars among the Mongols and some of the regions of the far north, like Russia. Polo's writings included descriptions of cannibals and spice growers.		The Travels was a rare popular success in an era before printing.		The impact of Polo's book on cartography was delayed: the first map in which some names mentioned by Polo appear was in the Catalan Atlas of Charles V (1375), which included thirty names in China and a number of other Asian toponyms.[10] In the mid-fifteenth century the cartographer of Murano, Fra Mauro, meticulously included all of Polo's toponyms in his 1450 map of the world.		Marco Polo's description of the Far East and its riches inspired Christopher Columbus's decision to try to reach Asia by sea,[citation needed] in a westward route. A heavily annotated copy of Polo's book was among the belongings of Columbus.		Marco Polo was accompanied on his trips by his father and uncle (both of whom had been to China previously), though neither of them published any known works about their journeys. The book was translated into many European languages in Marco Polo's own lifetime, but the original manuscripts are now lost.		The oldest surviving Polo manuscript is in Old French heavily flavoured with Italian;[11] for Luigi Foscolo Benedetto, this "F" text is the basic original text, which he corrected by comparing it with the somewhat more detailed Italian of Ramusio, together with a Latin manuscript in the Biblioteca Ambrosiana. Other early important sources are R (Ramusio's Italian translation first printed in 1559), and Z (a fifteenth-century Latin manuscript kept at Toledo, Spain). Another Old French Polo manuscript, dating to around 1350, is held by the National Library of Sweden.[12]		A total of about 150 copies in various languages are known to exist. During copying and translating many errors were made, so there are many differences between the various copies.[13] The first English translation is the Elizabethan version by John Frampton, The most noble and famous travels of Marco Polo, based on Santaella's Castilian translation of 1503 (the first version in that language).[14]		The first attempt to collate manuscripts and provide a critical edition was in a volume of collected travel narratives printed at Venice in 1559.[15]		The editor, Giovan Battista Ramusio, collated manuscripts from the first part of the fourteenth century,[16] which he considered to be "perfettamente corretto" ("perfectly correct"). He was of the opinion, not shared by modern scholars, that Marco had first written in Latin, quickly translated into Italian: he had apparently been able to use a Latin version "of marvelous antiquity" lent him by a friend in the Ghisi family of Venice.		The edition of Benedetto, Marco Polo, Il Milione, under the patronage of the Comitato Geografico Nazionale Italiano (Florence: Olschki, 1928), collated sixty additional manuscript sources, in addition to some eighty that had been collected by Henry Yule, for his 1871 edition. It was Benedetto who identified Rustichello da Pisa,[17] as the original compiler or amanuensis, and his established text has provided the basis for many modern translations: his own in Italian (1932), and Aldo Ricci's The Travels of Marco Polo (London, 1931).		A. C. Moule and Paul Pelliot published a translation under the title Description of the World that uses manuscript F as its base and attempts to combine the several versions of the text into one continuous narrative while at the same time indicating the source for each section (London, 1938). ISBN 4871873080		An introduction to Marco Polo is Leonard Olschki, Marco Polo's Asia: An Introduction to His "Description of the World" Called "Il Milione", translated by John A. Scott (Berkeley: University of California) 1960; it had its origins in the celebrations of the seven hundredth anniversary of Marco Polo's birth.		Since the book's publication, many have viewed the book with skepticism. Some in the Middle Ages viewed the book simply as a romance or fable, due largely to the sharp difference of its descriptions of a sophisticated civilisation in China to other early accounts by Giovanni da Pian del Carpine and William of Rubruck who portrayed the Mongols as 'barbarians' who appeared to belong to 'some other world'.[18] Doubts have also been raised in later centuries about Marco Polo's narrative of his travels in China, for example for his failure to mention a number of things and practices commonly associated with China, such as the Chinese characters, tea, chopsticks, and footbinding.[19] In particular, his failure to mention the Great Wall of China had been noted as early as the middle of the seventeenth century.[20] In addition, the difficulties in identifying many of the place names he used also raised suspicion about Polo's accounts.[20] Many have questioned if he had visited the places he mentioned in his itinerary, if he had appropriated the accounts of his father and uncle or other travelers, or doubted if he even reached China, and that if he did, perhaps never went beyond Khanbaliq (Beijing).[20][21]		Historian Stephen G. Haw however argued that many of the "omissions" could be explained. For example, none of the Western travelers to Yuan dynasty China at that time, such as Giovanni de' Marignolli and Odoric of Pordenone, mentioned the Great Wall, and that while remnants of the Wall would have existed at that time, it would not have been significant or noteworthy as it had not been maintained for a long time. The Great Walls were built to keep out northern invaders, whereas the ruling dynasty during Marco Polo's visit were those very northern invaders. The Mongol rulers whom Polo served also controlled territories both north and south of today's wall, and would have no reasons to maintain any fortifications that may have remained there from the earlier dynasties. He noted the Great Wall familiar to us today is a Ming structure built some two centuries after Marco Polo's travels.[22] The Muslim traveler Ibn Batutta did mention the Great Wall, but when he asked about the wall while in China during the Yuan Dynasty, he could find no one who had either seen it or knew of anyone who had seen it.[22] Haw also argued that practices such as footbinding were not common even among Chinese during Polo's time and almost unknown among the Mongols. While the Italian missionary Odoric of Pordenone who visited Yuan China mentioned footbinding (it is however unclear whether he was only relaying something he heard as his description is inaccurate),[23] no other foreign visitors to Yuan China mentioned the practice, perhaps an indication that the footbinding was not widespread or was not practiced in an extreme form at that time.[24] Marco Polo himself noted (in the Toledo manuscript) the dainty walk of Chinese women who took very short steps.[22]		It has also been pointed out that Polo's accounts are more accurate and detailed than other accounts of the periods. Polo had at times denied the 'marvelous' fables and legends given in other European accounts, and also omitted descriptions of strange races of people then believed to inhibit eastern Asia and given in such accounts. For example, Odoric of Pordenone said that the Yangtze river flows through the land of pygmies only three spans high and gave other fanciful tales, while Giovanni da Pian del Carpine spoke of "wild men, who do not speak at all and have no joints in their legs", monster who looked like women but whose menfolk were dogs, and other equally fantastic accounts. Despite a few exaggerations and errors, Polo's accounts are relatively free of the descriptions of irrational marvels, and in many cases where present (mostly given in the first part before he reached China, such as mentions of Christian miracles), he made a clear distinction that they are what he had heard rather than what he had seen. It is also largely free of the gross errors in other accounts such as those given by the Moroccan traveler Ibn Battuta who had confused the Yellow River with the Grand Canal and other waterways, and believed that porcelain was made from coal.[25]		Many of the details in Polo's accounts have been verified. For example, when visiting Zhenjiang in Jiangsu, China, Marco Polo noted that a large number of Christian churches had been built there. His claim is confirmed by a Chinese text of the 14th century explaining how a Sogdian named Mar-Sargis from Samarkand founded six Nestorian Christian churches there in addition to one in Hangzhou during the second half of the 13th century.[26] Nestorian Christianity had existed in China since the Tang Dynasty (618 - 907 AD) when a Persian monk named Alopen came to the capital Chang'an in 653 to proselytize, as described in a dual Chinese and Syriac language inscription from Chang'an (modern Xi'an) dated to the year 781.[27]		In 2012, the University of Tübingen Sinologist and historian Hans Ulrich Vogel released a detailed analysis of Polo's description of currencies, salt production and revenues, and argued that the evidence supports his presence in China because he included details which he could not have otherwise known.[28][29] Vogel noted that no other Western, Arab, or Persian sources have given such accurate and unique details about the currencies of China, for example, the shape and size of the paper, the use of seals, the various denominations of paper money as well as variations in currency usage in different regions of China, such as the use of cowry shells in Yunnan, details supported by archaeological evidence and Chinese sources compiled long after Polo's had left China.[30] His accounts of salt production and revenues from the salt monopoly are also accurate, and accord with Chinese documents of the Yuan era.[31] Economic historian Mark Elvin, in his preface to Vogel's 2013 monograph, concludes that Vogel "demonstrates by specific example after specific example the ultimately overwhelming probability of the broad authenticity" of Polo's account. Many problems were caused by the oral transmission of the original text and the proliferation of significantly different hand-copied manuscripts. For instance, did Polo exert "political authority" (seignora) in Yangzhou or merely "sojourn" (sejourna) there. Elvin concludes that "those who doubted, although mistaken, were not always being casual or foolish," but "the case as a whole had now been closed": the book is, "in essence, authentic, and, when used with care, in broad terms to be trusted as a serious though obviously not always final, witness."[32]		Although Marco Polo was certainly the most famous, he was not the only nor the first European traveller to the Mongol Empire who subsequently wrote an account of his experiences. Earlier thirteenth-century European travellers who journeyed to the court of the Great Khan were André de Longjumeau, William of Rubruck and Giovanni da Pian del Carpine with Benedykt Polak. None of them however reached China itself. Later travelers such as Odoric of Pordenone and Giovanni de' Marignolli reached China during the Yuan dynasty and wrote accounts of their travels.[23][24]		The Moroccan merchant Ibn Battuta travelled through the Golden Horde and China subsequently in the early-to-mid-14th century. The 14th-century author John de Mandeville wrote an account of journeys in the East, but this was probably based on second-hand information and contains much apocryphal information.		
Samuel Langhorne Clemens (November 30, 1835 – April 21, 1910),[1] better known by his pen name Mark Twain, was an American writer, humorist, entrepreneur, publisher, and lecturer. Among his novels are The Adventures of Tom Sawyer (1876) and its sequel, the Adventures of Huckleberry Finn (1885),[2] the latter often called "The Great American Novel".		Twain was raised in Hannibal, Missouri, which later provided the setting for Tom Sawyer and Huckleberry Finn. He served an apprenticeship with a printer and then worked as a typesetter, contributing articles to the newspaper of his older brother Orion Clemens. He later became a riverboat pilot on the Mississippi River before heading west to join Orion in Nevada. He referred humorously to his lack of success at mining, turning to journalism for the Virginia City Territorial Enterprise.[3] His humorous story, "The Celebrated Jumping Frog of Calaveras County", was published in 1865, based on a story that he heard at Angels Hotel in Angels Camp, California where he had spent some time as a miner. The short story brought international attention and was even translated into classic Greek.[4] His wit and satire, in prose and in speech, earned praise from critics and peers, and he was a friend to presidents, artists, industrialists, and European royalty.		Twain earned a great deal of money from his writings and lectures, but he invested in ventures that lost most of it—notably the Paige Compositor, a mechanical typesetter that failed because of its complexity and imprecision. He filed for bankruptcy in the wake of these financial setbacks, but he eventually overcame his financial troubles with the help of Henry Huttleston Rogers. He chose to pay all his pre-bankruptcy creditors in full, even after he had no legal responsibility to do so.		Twain was born shortly after an appearance of Halley's Comet, and he predicted that he would "go out with it" as well; he died the day after the comet returned. He was lauded as the "greatest American humorist of his age",[5] and William Faulkner called him "the father of American literature".[6]						Mark Twain was born Samuel Langhorne Clemens on November 30, 1835 in Florida, Missouri, the sixth of seven children born to Jane (née Lampton; 1803–1890), a native of Kentucky, and John Marshall Clemens (1798–1847), a native of Virginia. His parents met when his father moved to Missouri, and they were married in 1823.[7][8] Twain was of Cornish, English, and Scots-Irish descent.[9][10][11][12] Only three of his siblings survived childhood: Orion (1825–1897), Henry (1838–1858), and Pamela (1827–1904). His sister Margaret (1833–1839) died when Twain was three, and his brother Benjamin (1832–1842) died three years later. His brother Pleasant (1828–1829) died at six months of age.[13]		When he was four, Twain's family moved to Hannibal, Missouri,[14] a port town on the Mississippi River that inspired the fictional town of St. Petersburg in The Adventures of Tom Sawyer and the Adventures of Huckleberry Finn.[15] Slavery was legal in Missouri at the time, and it became a theme in these writings. His father was an attorney and judge, who died of pneumonia in 1847, when Twain was 11.[16] The next year, Twain left school after the fifth grade[17] to become a printer's apprentice. In 1851 he began working as a typesetter, contributing articles and humorous sketches to the Hannibal Journal, a newspaper that Orion owned. When he was 18, he left Hannibal and worked as a printer in New York City, Philadelphia, St. Louis, and Cincinnati, joining the newly formed International Typographical Union, the printers trade union. He educated himself in public libraries in the evenings, finding wider information than at a conventional school.[18]		Twain describes his boyhood in Life on the Mississippi, stating that "there was but one permanent ambition" among his comrades: to be a steamboatman.		Pilot was the grandest position of all. The pilot, even in those days of trivial wages, had a princely salary – from a hundred and fifty to two hundred and fifty dollars a month, and no board to pay.		As Twain describes it, the pilot's prestige exceeded that of the captain. The pilot had to:		…get up a warm personal acquaintanceship with every old snag and one-limbed cottonwood and every obscure wood pile that ornaments the banks of this river for twelve hundred miles; and more than that, must… actually know where these things are in the dark		Steamboat pilot Horace E. Bixby took Twain on as a cub pilot to teach him the river between New Orleans and St. Louis for $500, payable out of Twain's first wages after graduating. Twain studied the Mississippi, learning its landmarks, how to navigate its currents effectively, and how to read the river and its constantly shifting channels, reefs, submerged snags, and rocks that would "tear the life out of the strongest vessel that ever floated".[19] It was more than two years before he received his pilot's license. Piloting also gave him his pen name from "mark twain", the leadsman's cry for a measured river depth of two fathoms (12 feet), which was safe water for a steamboat.		While training, Samuel convinced his younger brother Henry to work with him. Henry was killed on June 21, 1858 when their steamboat Pennsylvania exploded. Twain claimed to have foreseen this death in a dream a month earlier,[20]:275 which inspired his interest in parapsychology; he was an early member of the Society for Psychical Research.[21] Twain was guilt-stricken and held himself responsible for the rest of his life. He continued to work on the river and was a river pilot until the Civil War broke out in 1861, when traffic was curtailed along the Mississippi River. At the start of hostilities, he enlisted briefly in a local Confederate unit. He later wrote the sketch "The Private History of a Campaign That Failed", describing how he and his friends had been Confederate volunteers for two weeks before disbanding.[22]		He then left for Nevada to work for Orion, who was Secretary of the Nevada Territory. Twain describes the episode in his book Roughing It.[23][24]		Orion became secretary to Nevada Territory governor James W. Nye in 1861, and Twain joined him when he moved west. The brothers traveled more than two weeks on a stagecoach across the Great Plains and the Rocky Mountains, visiting the Mormon community in Salt Lake City.		Twain's journey ended in the silver-mining town of Virginia City, Nevada where he became a miner on the Comstock Lode.[22] He failed as a miner and went to work at the Virginia City newspaper Territorial Enterprise,[25] working under a friend, the writer Dan DeQuille. He first used his pen name here on February 3, 1863, when he wrote a humorous travel account entitled "Letter From Carson – re: Joe Goodman; party at Gov. Johnson's; music" and signed it "Mark Twain".[26][27]		His experiences in the American West inspired Roughing It, written during 1870–71 and published in 1872. His experiences in Angels Camp (in Calaveras County, California) provided material for "The Celebrated Jumping Frog of Calaveras County" (1865).		Twain moved to San Francisco in 1864, still as a journalist, and met writers such as Bret Harte and Artemus Ward. He may have been romantically involved with the poet Ina Coolbrith.[28]		His first success as a writer came when his humorous tall tale "The Celebrated Jumping Frog of Calaveras County" was published on November 18, 1865 in the New York weekly The Saturday Press, bringing him national attention. A year later, he traveled to the Sandwich Islands (present day Hawaii) as a reporter for the Sacramento Union. His letters to the Union were popular and became the basis for his first lectures.[29]		In 1867, a local newspaper funded his trip to the Mediterranean aboard the Quaker City, including a tour of Europe and the Middle East. He wrote a collection of travel letters which were later compiled as The Innocents Abroad (1869). It was on this trip that he met fellow passenger Charles Langdon, who showed him a picture of his sister Olivia. Twain later claimed to have fallen in love at first sight.[citation needed]		Upon returning to the United States, Twain was offered honorary membership in Yale University's secret society Scroll and Key in 1868.[30] Its devotion to "fellowship, moral and literary self-improvement, and charity" suited him well.		Twain and Olivia Langdon corresponded throughout 1868. She rejected his first marriage proposal, but they were married in Elmira, New York in February 1870,[29] where he courted her and managed to overcome her father's initial reluctance.[31] She came from a "wealthy but liberal family"; through her, he met abolitionists, "socialists, principled atheists and activists for women's rights and social equality", including Harriet Beecher Stowe (his next-door neighbor in Hartford, Connecticut), Frederick Douglass, and writer and utopian socialist William Dean Howells,[32] who became a long-time friend.		The couple lived in Buffalo, New York, from 1869 to 1871. He owned a stake in the Buffalo Express newspaper and worked as an editor and writer. While they were living in Buffalo, their son Langdon died of diphtheria at the age of 19 months. They had three daughters: Susy (1872–1896), Clara (1874–1962),[33] and Jean (1880–1909).		Twain moved his family to Hartford, Connecticut, where he arranged the building of a home starting in 1873. In the 1870s and 1880s, the family summered at Quarry Farm in Elmira, the home of Olivia's sister, Susan Crane.[34][35] In 1874,[34] Susan had a study built apart from the main house so that Twain would have a quiet place in which to write. Also, he smoked cigars constantly, and Susan did not want him to do so in her house.		Twain wrote many of his classic novels during his 17 years in Hartford (1874–1891) and over 20 summers at Quarry Farm. They include The Adventures of Tom Sawyer (1876), The Prince and the Pauper (1881), Life on the Mississippi (1883), Adventures of Huckleberry Finn (1885), and A Connecticut Yankee in King Arthur's Court (1889).[citation needed]		The couple's marriage lasted 34 years until Olivia's death in 1904. All of the Clemens family are buried in Elmira's Woodlawn Cemetery.		Twain was fascinated with science and scientific inquiry. He developed a close and lasting friendship with Nikola Tesla, and the two spent much time together in Tesla's laboratory.		Twain patented three inventions, including an "Improvement in Adjustable and Detachable Straps for Garments" (to replace suspenders) and a history trivia game.[36][37] Most commercially successful was a self-pasting scrapbook; a dried adhesive on the pages needed only to be moistened before use.[36] Over 25,000 were sold.[36]		Twain's novel A Connecticut Yankee in King Arthur's Court (1889) features a time traveler from the contemporary U.S., using his knowledge of science to introduce modern technology to Arthurian England. This type of storyline became a common feature of the science fiction subgenre alternate history.		In 1909, Thomas Edison visited Twain at his home in Redding, Connecticut and filmed him. Part of the footage was used in The Prince and the Pauper (1909), a two-reel short film. It is said to be the only known existing film footage of Twain.[38]		Twain made a substantial amount of money through his writing, but he lost a great deal through investments. He invested mostly in new inventions and technology, particularly in the Paige typesetting machine. It was a beautifully engineered mechanical marvel that amazed viewers when it worked, but it was prone to breakdowns. Twain spent $300,000 (equal to $8,000,000 in inflation-adjusted terms [39]) on it between 1880 and 1894,[40] but before it could be perfected it was rendered obsolete by the Linotype. He lost the bulk of his book profits, as well as a substantial portion of his wife's inheritance.[41]		Twain also lost money through his publishing house of Charles L. Webster and Company, which enjoyed initial success selling the memoirs of Ulysses S. Grant but failed soon afterward, losing money on a biography of Pope Leo XIII. Fewer than 200 copies were sold.[41]		Twain and his family closed down their expensive Hartford home in response to the dwindling income and moved to Europe in June 1891. William M. Laffan of The New York Sun and the McClure Newspaper Syndicate offered him the publication of a series of six European letters. Twain, Olivia, and their daughter Susy were all faced with health problems, and they believed that it would be of benefit to visit European baths.[42] The family stayed mainly in France, Germany, and Italy until May 1895, with longer spells at Berlin (winter 1891/92), Florence (fall and winter 1892/93), and Paris (winters and springs 1893/94 and 1894/95). During that period, Twain returned four times to New York due to his enduring business troubles. He took "a cheap room" in September 1893 at $1.50 per day at The Players Club, which he had to keep until March 1894; meanwhile, he became "the Belle of New York," in the words of biographer Albert Bigelow Paine.[43]		Twain's writings and lectures enabled him to recover financially, combined with the help of a new friend.[44] In fall 1893, he began a friendship with financier Henry Huttleston Rogers, a principal of Standard Oil, that lasted the remainder of his life. Rogers first made him file for bankruptcy in April 1894, then had him transfer the copyrights on his written works to his wife to prevent creditors from gaining possession of them. Finally, Rogers took absolute charge of Twain's money until all his creditors were paid.[45]		Twain accepted an offer from Robert Sparrow Smythe[46] and embarked on a year-long, around the world lecture tour in July 1895[47] to pay off his creditors in full, although he was no longer under any legal obligation to do so.[48] It was a long, arduous journey and he was sick much of the time, mostly from a cold and a carbuncle. The first part of the itinerary took him across northern America to British Columbia, Canada, until the second half of August. For the second part, he sailed across the Pacific Ocean. His scheduled lecture in Honolulu, Hawaii had to be cancelled due to a cholera epidemic.[49][50] Twain went on to Fiji, Australia, New Zealand, Sri Lanka, India, Mauritius, and South Africa. His three months in India became the centerpiece of his 712-page book Following the Equator. In the second half of July 1896, he sailed back to England, completing his circumnavigation of the world begun 14 months before.[51]		Twain and his family spent four more years in Europe, mainly in England and Austria (October 1897 to May 1899), with longer spells in London and Vienna. Clara had wished to study the piano under Theodor Leschetizky in Vienna.[52] Unfortunately, Jean's health did not benefit from consulting with specialists in Vienna, the "City of Doctors".[citation needed] The family moved to London in spring 1899, following a lead by Poultney Bigelow who had a good experience being treated by Dr. Jonas Henrik Kellgren, a Swedish osteopathic practitioner in Belgravia. They were persuaded to spend the summer at Kellgren's sanatorium by the lake in the Swedish village of Sanna. Coming back in fall, they continued the treatment in London, until Twain was convinced by lengthy inquiries in America that similar osteopathic expertise was available there.[53]		In mid-1900, he was the guest of newspaper proprietor Hugh Gilzean-Reid at Dollis Hill House, located on the north side of London. Twain wrote that he had "never seen any place that was so satisfactorily situated, with its noble trees and stretch of country, and everything that went to make life delightful, and all within a biscuit's throw of the metropolis of the world."[54] He then returned to America in October 1900, having earned enough to pay off his debts. In winter 1900/01, he became his country's most prominent opponent of imperialism, raising the issue in his speeches, interviews, and writings. In January 1901, he began serving as vice-president of the Anti-Imperialist League of New York.[55]		Twain was in great demand as a featured speaker, performing solo humorous talks similar to modern stand-up comedy.[56] He gave paid talks to many men's clubs, including the Authors' Club, Beefsteak Club, Vagabonds, White Friars, and Monday Evening Club of Hartford.		In the late 1890s, he spoke to the Savage Club in London and was elected an honorary member. He was told that only three men had been so honored, including the Prince of Wales, and he replied: "Well, it must make the Prince feel mighty fine."[57] He visited Melbourne and Sydney in 1895 as part of a world lecture tour. In 1897, he spoke to the Concordia Press Club in Vienna as a special guest, following the diplomat Charlemagne Tower, Jr. He delivered the speech "Die Schrecken der deutschen Sprache" ("The Horrors of the German Language")—in German—to the great amusement of the audience.[58] In 1901, he was invited to speak at Princeton University's Cliosophic Literary Society, where he was made an honorary member.[59]		In 1881, Twain was honored at a banquet in Montreal, Canada where he made reference to securing a copyright.[60] In 1883, he paid a brief visit to Ottawa,[61] and he visited Toronto twice in 1884 and 1885 on a reading tour with George Washington Cable, known as the "Twins of Genius" tour.[62][61][63]		The reason for the Toronto visits was to secure Canadian and British copyrights for his upcoming book Adventures of Huckleberry Finn,[63][61] to which he had alluded in his Montreal visit. The reason for the Ottawa visit had been to secure Canadian and British copyrights for Life on the Mississippi.[61] Publishers in Toronto had printed unauthorized editions of his books at the time, before an international copyright agreement was established in 1891.[61] These were sold in the United States as well as in Canada, depriving him of royalties. He estimated that Belford Brothers' edition of The Adventures of Tom Sawyer alone had cost him ten thousand dollars.[61] He had unsuccessfully attempted to secure the rights for The Prince and the Pauper in 1881, in conjunction with his Montreal trip.[61] Eventually, he received legal advice to register a copyright in Canada (for both Canada and Britain) prior to publishing in the United States, which would restrain the Canadian publishers from printing a version when the American edition was published.[63][61] There was a requirement that a copyright be registered to a Canadian resident; he addressed this by his short visits to the country.[63][61]		Twain lived in his later years at 14 West 10th Street in Manhattan. [65]		Twain passed through a period of deep depression which began in 1896 when his daughter Susy died of meningitis. Olivia's death in 1904 and Jean's on December 24, 1909 deepened his gloom.[66] On May 20, 1909, his close friend Henry Rogers died suddenly. In 1906, Twain began his autobiography in the North American Review. In April, he heard that his friend Ina Coolbrith had lost nearly all that she owned in the 1906 San Francisco earthquake, and he volunteered a few autographed portrait photographs to be sold for her benefit. To further aid Coolbrith, George Wharton James visited Twain in New York and arranged for a new portrait session. He was resistant initially, but he eventually admitted that four of the resulting images were the finest ones ever taken of him.[67]		Twain formed a club in 1906 for girls whom he viewed as surrogate granddaughters called the Angel Fish and Aquarium Club. The dozen or so members ranged in age from 10 to 16. He exchanged letters with his "Angel Fish" girls and invited them to concerts and the theatre and to play games. Twain wrote in 1908 that the club was his "life's chief delight".[68] In 1907, he met Dorothy Quick (aged 11) on a transatlantic crossing, beginning "a friendship that was to last until the very day of his death".[69]		Oxford University awarded Twain an honorary doctorate in letters in 1907.		Twain was born two weeks after Halley's Comet's closest approach in 1835; he said in 1909:[70]		I came in with Halley's Comet in 1835. It is coming again next year, and I expect to go out with it. It will be the greatest disappointment of my life if I don't go out with Halley's Comet. The Almighty has said, no doubt: "Now here are these two unaccountable freaks; they came in together, they must go out together".		Twain's prediction was accurate; he died of a heart attack on April 21, 1910 in Redding, Connecticut, one day after the comet's closest approach to Earth.		Upon hearing of Twain's death, President William Howard Taft said:[71][72]		Mark Twain gave pleasure — real intellectual enjoyment — to millions, and his works will continue to give such pleasure to millions yet to come … His humor was American, but he was nearly as much appreciated by Englishmen and people of other countries as by his own countrymen. He has made an enduring part of American literature.		Twain's funeral was at the "Old Brick" Presbyterian Church in New York.[73] He is buried in his wife's family plot at Woodlawn Cemetery in Elmira, New York. The Langdon family plot is marked by a 12-foot monument (two fathoms, or "mark twain") placed there by his surviving daughter Clara.[74] There is also a smaller headstone. He expressed a preference for cremation (for example, in Life on the Mississippi), but he acknowledged that his surviving family would have the last word.		Officials in Connecticut and New York estimated the value of Twain's estate at $471,000 ($12,000,000 today).[75]		Twain began his career writing light, humorous verse, but he became a chronicler of the vanities, hypocrisies, and murderous acts of mankind. At mid-career, he combined rich humor, sturdy narrative, and social criticism in Huckleberry Finn. He was a master of rendering colloquial speech and helped to create and popularize a distinctive American literature built on American themes and language.		Many of his works have been suppressed at times for various reasons. The Adventures of Huckleberry Finn has been repeatedly restricted in American high schools, not least for its frequent use of the word "nigger", which was in common usage in the pre-Civil War period in which the novel was set.		A complete bibliography of Twain's works is nearly impossible to compile because of the vast number of pieces he wrote (often in obscure newspapers) and his use of several different pen names. Additionally, a large portion of his speeches and lectures have been lost or were not recorded; thus, the compilation of Twain's works is an ongoing process. Researchers rediscovered published material as recently as 1995 and 2015.[41][76]		Twain was writing for the Virginia City newspaper the Territorial Enterprise in 1863 when he met lawyer Tom Fitch, editor of the competing newspaper Virginia Daily Union and known as the "silver-tongued orator of the Pacific".[77]:51 He credited Fitch with giving him his "first really profitable lesson" in writing. "When I first began to lecture, and in my earlier writings," Twain later commented, "my sole idea was to make comic capital out of everything I saw and heard."[78] In 1866, he presented his lecture on the Sandwich Islands to a crowd in Washoe City, Nevada.[79] Afterwards, Fitch told him:		Clemens, your lecture was magnificent. It was eloquent, moving, sincere. Never in my entire life have I listened to such a magnificent piece of descriptive narration. But you committed one unpardonable sin – the unpardonable sin. It is a sin you must never commit again. You closed a most eloquent description, by which you had keyed your audience up to a pitch of the intensest interest, with a piece of atrocious anti-climax which nullified all the really fine effect you had produced.[80]		It was in these days that Twain became a writer of the Sagebrush School; he was known later as the most notable within the genre.[81] His first important work was "The Celebrated Jumping Frog of Calaveras County," published in the New York Saturday Press on November 18, 1865. After a burst of popularity, the Sacramento Union commissioned him to write letters about his travel experiences. The first journey that he took for this job was to ride the steamer Ajax on its maiden voyage to the Sandwich Islands (Hawaii). All the while, he was writing letters to the newspaper that were meant for publishing, chronicling his experiences with humor. These letters proved to be the genesis to his work with the San Francisco Alta California newspaper, which designated him a traveling correspondent for a trip from San Francisco to New York City via the Panama isthmus.		On June 8, 1867, he set sail on the pleasure cruiser Quaker City for five months, and this trip resulted in The Innocents Abroad or The New Pilgrims' Progress. In 1872, he published his second piece of travel literature, Roughing It, as an account of his journey from Missouri to Nevada, his subsequent life in the American West, and his visit to Hawaii. The book lampoons American and Western society in the same way that Innocents critiqued the various countries of Europe and the Middle East. His next work was The Gilded Age: A Tale of Today, his first attempt at writing a novel. The book is also notable because it is his only collaboration, written with his neighbor Charles Dudley Warner.		Twain's next work drew on his experiences on the Mississippi River. Old Times on the Mississippi was a series of sketches published in the Atlantic Monthly in 1875 featuring his disillusionment with Romanticism.[82] Old Times eventually became the starting point for Life on the Mississippi.		Twain's next major publication was The Adventures of Tom Sawyer, which draws on his youth in Hannibal. Tom Sawyer was modeled on Twain as a child, with traces of schoolmates John Briggs and Will Bowen. The book also introduces Huckleberry Finn in a supporting role, based on Twain's boyhood friend Tom Blankenship.		The Prince and the Pauper was not as well received, despite a storyline that is common in film and literature today. The book tells the story of two boys born on the same day who are physically identical, acting as a social commentary as the prince and pauper switch places. Twain had started Adventures of Huckleberry Finn (which he consistently had problems completing)[83] and had completed his travel book A Tramp Abroad, which describes his travels through central and southern Europe.		Twain's next major published work was the Adventures of Huckleberry Finn, which confirmed him as a noteworthy American writer. Some have called it the first Great American Novel, and the book has become required reading in many schools throughout the United States. Huckleberry Finn was an offshoot from Tom Sawyer and had a more serious tone than its predecessor. Four hundred manuscript pages were written in mid-1876, right after the publication of Tom Sawyer. The last fifth of Huckleberry Finn is subject to much controversy. Some say that Twain experienced a "failure of nerve," as critic Leo Marx puts it. Ernest Hemingway once said of Huckleberry Finn:		If you read it, you must stop where the Nigger Jim is stolen from the boys. That is the real end. The rest is just cheating.		Hemingway also wrote in the same essay:		All modern American literature comes from one book by Mark Twain called Huckleberry Finn.[84]		Near the completion of Huckleberry Finn, Twain wrote Life on the Mississippi, which is said to have heavily influenced the novel.[41] The travel work recounts Twain's memories and new experiences after a 22-year absence from the Mississippi River. In it, he also explains that "Mark Twain" was the call made when the boat was in safe water, indicating a depth of two fathoms (12 feet or 3.7 metres).		Twain produced President Ulysses S. Grant's Memoirs through his fledgling publishing house, Charles L. Webster & Company, which he co-owned with Charles L. Webster, his nephew by marriage.[85]		At this time he also wrote "The Private History of a Campaign That Failed" for The Century Magazine. This piece detailed his two-week stint in a Confederate militia during the Civil War. He next focused on A Connecticut Yankee in King Arthur's Court, written with the same historical fiction style as The Prince and the Pauper. A Connecticut Yankee showed the absurdities of political and social norms by setting them in the court of King Arthur. The book was started in December 1885, then shelved a few months later until the summer of 1887, and eventually finished in the spring of 1889.[citation needed]		His next large-scale work was Pudd'nhead Wilson, which he wrote rapidly, as he was desperately trying to stave off bankruptcy. From November 12 to December 14, 1893, Twain wrote 60,000 words for the novel.[41] Critics[who?] have pointed to this rushed completion as the cause of the novel's rough organization and constant disruption of the plot. This novel also contains the tale of two boys born on the same day who switch positions in life, like The Prince and the Pauper. It was first published serially in Century Magazine and, when it was finally published in book form, Pudd'nhead Wilson appeared as the main title; however, the "subtitles" make the entire title read: The Tragedy of Pudd'nhead Wilson and the Comedy of The Extraordinary Twins.[41]		Twain's next venture was a work of straight fiction that he called Personal Recollections of Joan of Arc and dedicated to his wife. He had long said[where?] that this was the work that he was most proud of, despite the criticism that he received for it. The book had been a dream of his since childhood, and he claimed that he had found a manuscript detailing the life of Joan of Arc when he was an adolescent.[41] This was another piece that he was convinced would save his publishing company. His financial adviser Henry Huttleston Rogers quashed that idea and got Twain out of that business altogether, but the book was published nonetheless.[citation needed]		To pay the bills and keep his business projects afloat, Twain had begun to write articles and commentary furiously, with diminishing returns, but it was not enough. He filed for bankruptcy in 1894. During this time of dire financial straits, he published several literary reviews in newspapers to help make ends meet. He famously derided James Fenimore Cooper in his article detailing Cooper's "Literary Offenses". He became an extremely outspoken critic of other authors and other critics; he suggested that, before praising Cooper's work, Thomas Lounsbury, Brander Matthews, and Wilkie Collins "ought to have read some of it".[86]		George Eliot, Jane Austen, and Robert Louis Stevenson also fell under Twain's attack during this time period, beginning around 1890 and continuing until his death.[87] He outlines what he considers to be "quality writing" in several letters and essays, in addition to providing a source for the "tooth and claw" style of literary criticism. He places emphasis on concision, utility of word choice, and realism; he complains, for example, that Cooper's Deerslayer purports to be realistic but has several shortcomings. Ironically, several of his own works were later criticized for lack of continuity (Adventures of Huckleberry Finn) and organization (Pudd'nhead Wilson).		Twain's wife died in 1904 while the couple were staying at the Villa di Quarto in Florence. After some time had passed he published some works that his wife, his de facto editor and censor throughout her married life, had looked down upon. The Mysterious Stranger is perhaps the best known, depicting various visits of Satan to earth. This particular work was not published in Twain's lifetime. His manuscripts included three versions, written between 1897 and 1905: the so-called Hannibal, Eseldorf, and Print Shop versions. The resulting confusion led to extensive publication of a jumbled version, and only recently have the original versions become available as Twain wrote them.		Twain's last work was his autobiography, which he dictated and thought would be most entertaining if he went off on whims and tangents in non-chronological order. Some archivists and compilers have rearranged the biography into a more conventional form, thereby eliminating some of Twain's humor and the flow of the book. The first volume of the autobiography, over 736 pages, was published by the University of California in November 2010, 100 years after his death, as Twain wished.[88][89] It soon became an unexpected[90] best seller,[91] making Twain one of a very few authors publishing new best-selling volumes in the 19th, 20th, and 21st centuries.		Twain's works have been subjected to censorship efforts. According to Stuart (2013), "Leading these banning campaigns, generally, were religious organizations or individuals in positions of influence – not so much working librarians, who had been instilled with that American "library spirit" which honored intellectual freedom (within bounds of course)". In 1905, the Brooklyn Public Library banned both The Adventures of Huckleberry Finn and The Adventures of Tom Sawyer from the children's department because of their language.[92]		Twain's views became more radical as he grew older. In a letter to friend and fellow writer William Dean Howells in 1887 he acknowledged that his views had changed and developed over his lifetime, referring to one of his favorite works:		When I finished Carlyle's French Revolution in 1871, I was a Girondin; every time I have read it since, I have read it differently – being influenced and changed, little by little, by life and environment ... and now I lay the book down once more, and recognize that I am a Sansculotte! And not a pale, characterless Sansculotte, but a Marat.[93][94]		Before 1899, Twain was an ardent imperialist. In the late 1860s and early 1870s, he spoke out strongly in favor of American interests in the Hawaiian Islands.[95] He said the war with Spain in 1898 was "the worthiest" war ever fought.[96] In 1899, however, he reversed course. In the New York Herald, October 16, 1900, Twain describes his transformation and political awakening, in the context of the Philippine–American War, to anti-imperialism:		I wanted the American eagle to go screaming into the Pacific ... Why not spread its wings over the Philippines, I asked myself? ... I said to myself, Here are a people who have suffered for three centuries. We can make them as free as ourselves, give them a government and country of their own, put a miniature of the American Constitution afloat in the Pacific, start a brand new republic to take its place among the free nations of the world. It seemed to me a great task to which we had addressed ourselves.		But I have thought some more, since then, and I have read carefully the treaty of Paris [which ended the Spanish–American War], and I have seen that we do not intend to free, but to subjugate the people of the Philippines. We have gone there to conquer, not to redeem.		It should, it seems to me, be our pleasure and duty to make those people free, and let them deal with their own domestic questions in their own way. And so I am an anti-imperialist. I am opposed to having the eagle put its talons on any other land.[97][98]		During the Boxer rebellion, Mark Twain said that "the Boxer is a patriot. He loves his country better than he does the countries of other people. I wish him success."[99]		From 1901, soon after his return from Europe, until his death in 1910, Twain was vice-president of the American Anti-Imperialist League,[100] which opposed the annexation of the Philippines by the United States and had "tens of thousands of members".[32] He wrote many political pamphlets for the organization. The Incident in the Philippines, posthumously published in 1924, was in response to the Moro Crater Massacre, in which six hundred Moros were killed.[101] Many of his neglected and previously uncollected writings on anti-imperialism appeared for the first time in book form in 1992.[100]		Twain was critical of imperialism in other countries as well. In Following the Equator, Twain expresses "hatred and condemnation of imperialism of all stripes".[32] He was highly critical of European imperialists, notably Cecil Rhodes, who greatly expanded the British Empire, and Leopold II, King of the Belgians.[32] King Leopold's Soliloquy is a stinging political satire about his private colony, the Congo Free State. Reports of outrageous exploitation and grotesque abuses led to widespread international protest in the early 1900s, arguably the first large-scale human rights movement. In the soliloquy, the King argues that bringing Christianity to the country outweighs a little starvation. Leopold's rubber gatherers were tortured, maimed and slaughtered until the movement forced Brussels to call a halt.[102][103]		During the Philippine–American War, Twain wrote a short pacifist story titled The War Prayer, which makes the point that humanism and Christianity's preaching of love are incompatible with the conduct of war. It was submitted to Harper's Bazaar for publication, but on March 22, 1905, the magazine rejected the story as "not quite suited to a woman's magazine". Eight days later, Twain wrote to his friend Daniel Carter Beard, to whom he had read the story, "I don't think the prayer will be published in my time. None but the dead are permitted to tell the truth." Because he had an exclusive contract with Harper & Brothers, Twain could not publish The War Prayer elsewhere; it remained unpublished until 1923. It was republished as campaigning material by Vietnam War protesters.[32]		Twain acknowledged that he had originally sympathized with the more moderate Girondins of the French Revolution and then shifted his sympathies to the more radical Sansculottes, indeed identifying himself as "a Marat". Twain supported the revolutionaries in Russia against the reformists, arguing that the Tsar must be got rid of by violent means, because peaceful ones would not work.[104] He summed up his views of revolutions in the following statement:		I am said to be a revolutionist in my sympathies, by birth, by breeding and by principle. I am always on the side of the revolutionists, because there never was a revolution unless there were some oppressive and intolerable conditions against which to revolute.[105]		Twain was an adamant supporter of the abolition of slavery and emancipation of slaves, even going so far as to say, "Lincoln's Proclamation ... not only set the black slaves free, but set the white man free also".[106] He argued that non-whites did not receive justice in the United States, once saying, "I have seen Chinamen abused and maltreated in all the mean, cowardly ways possible to the invention of a degraded nature ... but I never saw a Chinaman righted in a court of justice for wrongs thus done to him".[107] He paid for at least one black person to attend Yale Law School and for another black person to attend a southern university to become a minister.[108]		Twain's sympathetic views on race were not reflected in his early writings on Native Americans. Of them, Twain wrote in 1870:		His heart is a cesspool of falsehood, of treachery, and of low and devilish instincts. With him, gratitude is an unknown emotion; and when one does him a kindness, it is safest to keep the face toward him, lest the reward be an arrow in the back. To accept of a favor from him is to assume a debt which you can never repay to his satisfaction, though you bankrupt yourself trying. The scum of the earth![109]		As counterpoint, Twain's essay on "The Literary Offenses of Fenimore Cooper" offers a much kinder view of Indians.[86] "No, other Indians would have noticed these things, but Cooper's Indians never notice anything. Cooper thinks they are marvelous creatures for noticing, but he was almost always in error about his Indians. There was seldom a sane one among them."[110] In his later travelogue Following the Equator (1897), Twain observes that in colonized lands all over the world, "savages" have always been wronged by "whites" in the most merciless ways, such as "robbery, humiliation, and slow, slow murder, through poverty and the white man's whiskey"; his conclusion is that "there are many humorous things in this world; among them the white man's notion that he is less savage than the other savages".[111] In an expression that captures his Indian experiences, he wrote, "So far as I am able to judge nothing has been left undone, either by man or Nature, to make India the most extraordinary country that the sun visits on his rounds. Where every prospect pleases, and only man is vile."[112]		Twain was also a staunch supporter of women's rights and an active campaigner for women's suffrage. His "Votes for Women" speech, in which he pressed for the granting of voting rights to women, is considered one of the most famous in history.[113]		Helen Keller benefited from Twain's support as she pursued her college education and publishing despite her disabilities and financial limitations.		Twain wrote glowingly about unions in the river boating industry in Life on the Mississippi, which was read in union halls decades later.[114] He supported the labor movement, especially one of the most important unions, the Knights of Labor.[32] In a speech to them, he said:		Who are the oppressors? The few: the King, the capitalist, and a handful of other overseers and superintendents. Who are the oppressed? The many: the nations of the earth; the valuable personages; the workers; they that make the bread that the soft-handed and idle eat.[115]		Twain was a Presbyterian.[116] He was critical of organized religion and certain elements of Christianity through his later life. He wrote, for example, "Faith is believing what you know ain't so", and "If Christ were here now there is one thing he would not be – a Christian".[117] With anti-Catholic sentiment rampant in 19th century America, Twain noted he was “educated to enmity toward everything that is Catholic”.[118] As an adult, he engaged in religious discussions and attended services, his theology developing as he wrestled with the deaths of loved ones and with his own mortality.[119]		Twain generally avoided publishing his most controversial[120] opinions on religion in his lifetime, and they are known from essays and stories that were published later. In the essay Three Statements of the Eighties in the 1880s, Twain stated that he believed in an almighty God, but not in any messages, revelations, holy scriptures such as the Bible, Providence, or retribution in the afterlife. He did state that "the goodness, the justice, and the mercy of God are manifested in His works", but also that "the universe is governed by strict and immutable laws", which determine "small matters", such as who dies in a pestilence.[121] At other times, he wrote or spoke in ways that contradicted a strict deist view, for example, plainly professing a belief in Providence.[122] In some later writings in the 1890s, he was less optimistic about the goodness of God, observing that "if our Maker is all-powerful for good or evil, He is not in His right mind". At other times, he conjectured sardonically that perhaps God had created the world with all its tortures for some purpose of His own, but was otherwise indifferent to humanity, which was too petty and insignificant to deserve His attention anyway.[123]		In 1901, Twain criticized the actions of the missionary Dr. William Scott Ament (1851–1909) because Ament and other missionaries had collected indemnities from Chinese subjects in the aftermath of the Boxer Uprising of 1900. Twain's response to hearing of Ament's methods was published in the North American Review in February 1901: To the Person Sitting in Darkness, and deals with examples of imperialism in China, South Africa, and with the U.S. occupation of the Philippines.[124] A subsequent article, "To My Missionary Critics" published in The North American Review in April 1901, unapologetically continues his attack, but with the focus shifted from Ament to his missionary superiors, the American Board of Commissioners for Foreign Missions.[125]		After his death, Twain's family suppressed some of his work that was especially irreverent toward conventional religion, notably Letters from the Earth, which was not published until his daughter Clara reversed her position in 1962 in response to Soviet propaganda about the withholding.[126] The anti-religious The Mysterious Stranger was published in 1916. Little Bessie, a story ridiculing Christianity, was first published in the 1972 collection Mark Twain's Fables of Man.[127]		He raised money to build a Presbyterian Church in Nevada in 1864.[128]		Twain created a reverent portrayal of Joan of Arc, a subject over which he had obsessed for forty years, studied for a dozen years and spent two years writing about.[129] In 1900 and again in 1908 he stated, "I like Joan of Arc best of all my books, it is the best".[129][130]		Those who knew Twain well late in life recount that he dwelt on the subject of the afterlife, his daughter Clara saying: "Sometimes he believed death ended everything, but most of the time he felt sure of a life beyond."[131]		Twain's frankest views on religion appeared in his final work Autobiography of Mark Twain, the publication of which started in November 2010, 100 years after his death. In it, he said:[132]		There is one notable thing about our Christianity: bad, bloody, merciless, money-grabbing, and predatory as it is – in our country particularly and in all other Christian countries in a somewhat modified degree – it is still a hundred times better than the Christianity of the Bible, with its prodigious crime – the invention of Hell. Measured by our Christianity of to-day, bad as it is, hypocritical as it is, empty and hollow as it is, neither the Deity nor his Son is a Christian, nor qualified for that moderately high place. Ours is a terrible religion. The fleets of the world could swim in spacious comfort in the innocent blood it has spilled.		Twain was a Freemason.[133][134] He belonged to Polar Star Lodge No. 79 A.F.&A.M., based in St. Louis. He was initiated an Entered Apprentice on May 22, 1861, passed to the degree of Fellow Craft on June 12, and raised to the degree of Master Mason on July 10.		Twain visited Salt Lake City for two days and met there members of The Church of Jesus Christ of Latter-day Saints. They also gave him a Book of Mormon.[135] He later wrote in Roughing It about that book:[136][137]		The book seems to be merely a prosy detail of imaginary history, with the Old Testament for a model; followed by a tedious plagiarism of the New Testament.		Twain was opposed to the vivisection practices of his day. His objection was not on a scientific basis but rather an ethical one. He specifically cited the pain caused to the animal as his basis of his opposition.[138][139]		I am not interested to know whether Vivisection produces results that are profitable to the human race or doesn't. ... The pains which it inflicts upon unconsenting animals is the basis of my enmity towards it, and it is to me sufficient justification of the enmity without looking further.		Twain used different pen names before deciding on "'Mark Twain". He signed humorous and imaginative sketches as "Josh" until 1863. Additionally, he used the pen name "Thomas Jefferson Snodgrass" for a series of humorous letters.[140]		He maintained that his primary pen name came from his years working on Mississippi riverboats, where two fathoms, a depth indicating water safe for the passage of boat, was a measure on the sounding line. Twain is an archaic term for "two", as in "The veil of the temple was rent in twain."[141] The riverboatman's cry was "mark twain" or, more fully, "by the mark twain", meaning "according to the mark [on the line], [the depth is] two [fathoms]", that is, "The water is 12 feet (3.7 m) deep and it is safe to pass."		Twain claimed that his famous pen name was not entirely his invention. In Life on the Mississippi, he wrote:		Captain Isaiah Sellers was not of literary turn or capacity, but he used to jot down brief paragraphs of plain practical information about the river, and sign them "MARK TWAIN", and give them to the New Orleans Picayune. They related to the stage and condition of the river, and were accurate and valuable; ... At the time that the telegraph brought the news of his death, I was on the Pacific coast. I was a fresh new journalist, and needed a nom de guerre; so I confiscated the ancient mariner's discarded one, and have done my best to make it remain what it was in his hands – a sign and symbol and warrant that whatever is found in its company may be gambled on as being the petrified truth; how I have succeeded, it would not be modest in me to say.[142]		Twain's story about his pen name has been questioned by some[143] with the suggestion that "mark twain" refers to a running bar tab that Twain would regularly incur while drinking at John Piper's saloon in Virginia City, Nevada. Samuel Clemens himself responded to this suggestion by saying, "Mark Twain was the nom de plume of one Captain Isaiah Sellers, who used to write river news over it for the New Orleans Picayune. He died in 1869 and as he could no longer need that signature, I laid violent hands upon it without asking permission of the proprietor's remains. That is the history of the nom de plume I bear."[144]		In his autobiography, Twain writes further of Captain Sellers' use of "Mark Twain":		I was a cub pilot on the Mississippi River then, and one day I wrote a rude and crude satire which was leveled at Captain Isaiah Sellers, the oldest steamboat pilot on the Mississippi River, and the most respected, esteemed, and revered. For many years he had occasionally written brief paragraphs concerning the river and the changes which it had undergone under his observation during fifty years, and had signed these paragraphs "Mark Twain" and published them in the St. Louis and New Orleans journals. In my satire I made rude game of his reminiscences. It was a shabby poor performance, but I didn't know it, and the pilots didn't know it. The pilots thought it was brilliant. They were jealous of Sellers, because when the gray-heads among them pleased their vanity by detailing in the hearing of the younger craftsmen marvels which they had seen in the long ago on the river, Sellers was always likely to step in at the psychological moment and snuff them out with wonders of his own which made their small marvels look pale and sick. However, I have told all about this in "Old Times on the Mississippi." The pilots handed my extravagant satire to a river reporter, and it was published in the New Orleans True Delta. That poor old Captain Sellers was deeply wounded. He had never been held up to ridicule before; he was sensitive, and he never got over the hurt which I had wantonly and stupidly inflicted upon his dignity. I was proud of my performance for a while, and considered it quite wonderful, but I have changed my opinion of it long ago. Sellers never published another paragraph nor ever used his nom de guerre again.[145]		While Twain is often depicted wearing a white suit, modern representations suggesting that he wore them throughout his life are unfounded. Evidence suggests that Twain began wearing white suits on the lecture circuit, after the death of his wife Olivia ("Livy") in 1904. However, there is also evidence showing him wearing a white suit before 1904. In 1882, he sent a photograph of himself in a white suit to 18-year-old Edward W. Bok, later publisher of the Ladies Home Journal, with a handwritten dated note on verso. It did eventually become his trademark, as illustrated in anecdotes about this eccentricity (such as the time he wore a white summer suit to a Congressional hearing during the winter).[41] McMasters' The Mark Twain Encyclopedia states that Twain did not wear a white suit in his last three years, except at one banquet speech.[146]		In his autobiography, Twain writes of his early experiments with wearing white out-of-season:		Next after fine colors, I like plain white. One of my sorrows, when the summer ends, is that I must put off my cheery and comfortable white clothes and enter for the winter into the depressing captivity of the shapeless and degrading black ones. It is mid-October now, and the weather is growing cold up here in the New Hampshire hills, but it will not succeed in freezing me out of these white garments, for here the neighbors are few, and it is only of crowds that I am afraid. I made a brave experiment, the other night, to see how it would feel to shock a crowd with these unseasonable clothes, and also to see how long it might take the crowd to reconcile itself to them and stop looking astonished and outraged. On a stormy evening I made a talk before a full house, in the village, clothed like a ghost, and looking as conspicuous, all solitary and alone on that platform, as any ghost could have looked; and I found, to my gratification, that it took the house less than ten minutes to forget about the ghost and give its attention to the tidings I had brought. I am nearly seventy-one, and I recognize that my age has given me a good many privileges; valuable privileges; privileges which are not granted to younger persons. Little by little I hope to get together courage enough to wear white clothes all through the winter, in New York. It will be a great satisfaction to me to show off in this way; and perhaps the largest of all the satisfactions will be the knowledge that every scoffer, of my sex, will secretly envy me and wish he dared to follow my lead.[147]		
My Side of the Mountain is a children or young adult adventure novel written and illustrated by American writer Jean Craighead George published by E. P. Dutton in 1959.[1] It features a boy who learns about courage, independence, and the need for companionship while attempting to live in a forested area of New York state. In 1960, it was one of three Newbery Medal Honor Books (runners-up)[2] and in 1969 it was loosely adapted as a film of the same name. George continued the story in print, decades later.						The book is about Sam Gribley, a 15 year old boy who intensely dislikes living in his parents' cramped New York City apartment with his eight brothers and sisters. He decides to run away to his great-grandfather's abandoned farm in the Catskill Mountains to live in the wilderness. The reader meets Frightful, Sam's pet peregrine falcon, and The Baron, a weasel that Sam befriends. Roughly the first 80 percent of the novel is Sam's reminiscences during a snowstorm about how he came to be in a home made out of a hollowed-out tree, while the remainder of the novel is a traditional linear narrative about what happens after the snowstorm.		The second chapter opens with Sam Gribley remembering how he came to dislike living in New York City; how he learned of his grandfather's abandoned farm near Delhi, New York; how he learned wilderness survival skills by reading a book at the New York City Public Library; and about his trip to the small town of Delhi using $40 he earned by selling magazine subscriptions. Realizing his son will run away from home no matter what he does, Sam's father permits him to go to Delhi as long as Sam lets people in the town know that he is staying at the farm. Sam enters the forest near the town, builds a tent out of hemlock evergreen tree branches, and catches five trout in a nearby stream. But his survival skills are incomplete, and he is unable to build a fire. The next day, Sam searches for his grandfather's farm and fails to find it. However, he does meet Bill, a man living in a cabin in the woods. Bill teaches him how to make a fire. Sam is forced to go into town to learn where his grandfather's land is. He tells the local librarian who he is and where he is going, then journeys to the farm. Sam discovers the stone foundation for the long-destroyed farmhouse, but little else remains of the homestead.		Over the next several chapters, Sam continues to reminisce about how he came to be self-sufficient by living off the plants and animals he finds on his grandfather's abandoned farm. He finds a hollow tree and decides to make it his home. Remembering how Native Americans used fire to create dugout canoes, he uses fire to make the interior of the hollow tree bigger. One day, while Sam is chopping an ash tree to make a bed, an old woman named Mrs. Thomas Fiedler forces him to help her pick strawberries. Seeing a peregrine falcon hunting for its prey, Sam decides he wants a falcon as a hunting bird. Sam returns to town to get a haircut, and reads up on falconry at the local public library. He camps near a cliff for several days to learn the location of a peregrine falcon nest, and steals a chick from the nest while the mother bird attacks him. He names the bird Frightful, because of the difficult time he had getting the nestling. A short time later, Sam is forced to hide in the woods for two days. A forest ranger, spotting the smoke from Sam's cooking fire, came to investigate what he believed was a forest fire. The ranger lingers near Sam's home overnight, but leaves after believing that whoever started the fire must have left the place. Sam also relates to the reader his memories about his adventures in the fall. He makes a box trap to catch animals to eat, but ends up catching a weasel instead. Sam calls the weasel The Baron for the fearless way the animal moves about the hollowed-out treehouse. Realizing winter is coming, Sam wants to kill a deer so he can make a door for his home. He learns how to smoke meat to preserve it for winter, and how to tan hides. When a poacher illegally kills a deer, Sam hides the carcass from the hunter so he can use it for food, a door, and a new pair of clothes. Sam remembers how he tanned the hide using a hollow tree stump and various plants. He also avoids townspeople who wander near his home by hiding in the woods. Sam trains Frightful to hunt, and the bird proves very good at it. Sam prepares for winter by hunting frogs, pheasants, rabbits, and sparrows; preserving wild grains and tubers; smoking fish and meat; and preparing storage spaces by hollowing out the trunks of trees. Finding another poached deer, Sam makes himself deerskin clothing to replace his worn-out city clothes. Sam notices a raccoon digging for mussels in the creek, and he learns how to hunt for shellfish. Sam names the raccoon Jesse Coon James, because it looks like a bandit and reminds him of the legendary outlaw Jesse James.		Shortly after befriending the raccoon, Sam hears sirens nearby. When he returns to his hollowed out tree home, he finds a man there. At first, Sam believes the man is a criminal, and nicknames him "Bando" (a shortened version of "bandit"). But the man is a professor of English literature, and is merely lost. He is surprised to find Sam, and gives Sam the nickname "Thoreau". Bando spends 10 days with Sam, building a raft to take them downstream to catch fish. He gives Sam 10 pounds of sugar and teaches him to make jam. He also shows Sam how to make a whistle out of a willow branch. Bando also tries to make clay pots. Bando departs, and they agree that Bando will come back at Christmas to visit with Sam.		Sam remembers how, as winter came closer, he realized he needed to make a clay fireplace to keep his home warm. Sam steals two more dead deer from local hunters to make winter clothes, begins rapidly storing as many fruits and nuts as he can (trying desperately to get to them before the squirrels do), and builds his fireplace. Sam insulates his treehouse home too well, however: his fire generates too much carbon monoxide and not enough oxygen can get inside the treehouse. Frightful becomes sick with CO poisoning, which warns Sam. Sam puts ventilation holes in the walls of his treehouse to admit more fresh air. Sam feels lonely during Halloween, and makes a party for his animal friends—which goes badly when the animals start stealing his provisions. He tries to go into town to visit the library again, but is forced to climb a tree and stay there all day to avoid being discovered by hunters. He obtains two more deer; their carcasses freeze in the winter cold, so he does not need to smoke them. Sam feels lonely and visits the town again, where he gets a haircut and meets another teenage boy (Tom Sidler). On Christmas Eve, Bando finally arrives back at the mountain, showing Sam many newspaper articles about the "wild boy" living in the forest. On Christmas Day, Sam gets a surprise: Sam's father has come to visit. Sam is overjoyed to see his father again, and the three have a Christmas dinner of venison together. Sam's father is greatly relieved to find that Sam is doing just fine.		The novel ceases to be a flashback in Chapter 18, and becomes a straightforward narrative. Sam learns many things about how animals behave in winter, even during terrible storms. After the blizzard ends, Sam must still forage for food. He is happy that a great horned owl has taken up residence on the farm, for it means that no people or building developments are nearby. Sam learns how Frightful and The Baron manage to survive during winter, helps the local deer find nourishment by cutting down tree branches for them to eat, and overcomes his own vitamin deficiency by eating the right foods.		After spring arrives, Matt Spell, a teenager who wants to become a reporter for the local newspaper, arrives at Sam's treehouse home. Matt wants to write about Sam's presence on the Gribley farm. At first, Sam lies to Matt and says the "wild boy" is someone who lives in a nearby cave. But Matt doesn't believe him. Sam then offers Matt a deal: Matt can come live with him for a week during school spring break, if Matt will not reveal his location. Matt agrees. After Matt leaves, Sam realizes he is very lonely and debates with himself whether he wants to be "caught" or not. A few weeks later, Sam encounters Aaron, a Jewish songwriter who is visiting the forest for inspiration and singing a song. He tells Sam it is close to Passover, which makes Sam realize Matt will be visiting soon. Matt spends a week with Sam, mostly gathering food during this time. Matt is thrilled to be there, but Sam is sad because he realizes he is beginning to replicate his old life in New York City. Matt makes Sam even more unhappy by confessing that he told newspaper photographers where to find Sam. Bando arrives while Matt is visiting, and the two work on making one of the other trees into a guest house. A short time later, Tom Sidler discovers Sam living at the farm. Sam calls him "Mr. Jacket," and the two boys play for a while. Tom's visit makes Sam realize he is desperate for human companionship.		Bando returns to check on Sam, and Sam asks Bando to bring him some jeans and a shirt next time so he can visit his new friend Tom in town. In June, Sam is surprised one day to find that his father, mother, and all his siblings have arrived at the farm. His father announces that the entire family is moving to the farm. At first, Sam (now 15 years old) is overjoyed that his family has come to see him. But he is also upset, because it means the end of his life living off the land alone. Sam argues with his father about the family's decision. But his father says the family is as loyal to Sam as Sam has been to them, and that he will build a proper house for the family on the farm. Sam is especially upset about the decision to build a traditional home. The novel ends as Sam meditates on the fact that, even if he went across the Pacific Ocean to get away from people, he still craves friendship and family. His journey in life, he decides, is about balancing his desire to live off the land with his desire to be with the people he loves.		My Side of the Mountain won critical plaudits upon its release. Numerous reviewers praised the novel for its detailed depiction of the wilderness and animals, its unsentimental treatment of animals and nature, and its characters, their maturation, and development.[3] The New York Times in 1959 gave the novel a solid review, calling it "a delightful flight from civilization, written with real feeling for the woods."[4] Children's author Zena Sutherland, writing in Children & Books at the time, called Sam's development from immature, impulsive child into a mature young adult "wholly convincing".[3] Ruth Hill Viguers, reviewing the book in The Horn Book Magazine, concluded in 1959, "I believe it will be read year after year, linking together many generations in a chain of well-remembered joy and refreshment."[5]		In addition to being named to the Newbery Award Honors list, the book was also an American Library Association's Notable Book for 1959, was placed on the Hans Christian Andersen Award 1959 honors list, was given a Lewis Carroll Shelf Award citation (in 1965), and won the 1959 George G. Stone Center for Children's Books Award.[3]		The book continued to be praised in the 1990s and 2000s. Book critic Eden Ross Lipson included it in her 2000 list of the best children's books, and said it "skillfully blends themes of nature, courage, curiosity, and independence".[6] Librarians and authors Janice DeLong and Rachel Schwedt listed the book as one of a "core collection" of noveave in thdult fiction section.[7] Author Charles Wohlforth, writing in 2004, agreed that it was a classic of contemporary children's literature.[8] By 1998, the book had been translated into numerous foreign languages, and visitors to the Cannon Free Library in Delhi, New York, often asked to see the abandoned farm where the novel was set.[9] (The abandoned farm does not actually exist; the Gribley farm is entirely fictional.)		The book has not always won uncritical praise. In 1999, reviewer Mary Harris Russell noted that "the narrator, Sam, speaks with a tone more measured than that of most teenagers. That tone grates on some readers."[10]		Robert F. Kennedy Jr. has cited My Side of the Mountain with inspiring him to become a falconer, which led him into a career in environmental law and environmental activism.[11] Television host and pet advice author Marc Morrone and award-winning natural history author Ken Lamberton also credit the book with generating their interest in falconry.[12]		Based on a 2007 online poll, the National Education Association named the book one of its "Teachers' Top 100 Books for Children".[13] In 2012 it was ranked number 77 among all-time children's novels in a survey published by School Library Journal.[14]		A film adaptation directed by James B. Clark was released by Paramount Pictures in 1969.[15] The film My Side of the Mountain is set in Toronto and the Notre Dame Mountains, a Quebec province section of the Appalachians, rather than in New York City and a New York state section.		A sequel written and illustrated by George was published in 1990, more than three decades after the original. Over the next 16 years there were three more sequels, a third novel illustrated by George and two picture books illustrated by Daniel San Souci. All the sequels were published by Dutton Children's Books, an imprint of Penguin Books since its acquisition of the original publisher E. P. Dutton in 1986.		The three novels were issued in an omnibus edition that retains the original pagination, about 600 pages in sum: My Side of the Mountain Trilogy (2000). OCLC 45610215		In 2009, Dutton published A Pocket Guide to the Outdoors: Based on 'My Side of the Mountain', by George and her daughter Twig C. George. According to a library summary: "This guide to the outdoors provides advice and instructions on camping, building shelters, finding water, and cooking outdoors. Some activities may require adult supervision." Inside responsibility credits John C. George and T. Luke George as well. OCLC 311783530		
Scuba diving is a form of underwater diving where the diver uses a self-contained underwater breathing apparatus (scuba) which is completely independent of surface supply, to breathe underwater.[1] Scuba divers carry their own source of breathing gas, usually compressed air,[2] allowing them greater independence and freedom of movement than surface-supplied divers, and longer underwater endurance than breath-hold divers.[1] Open circuit scuba systems discharge the breathing gas into the environment as it is exhaled, and consist of one or more diving cylinders containing breathing gas at high pressure which is supplied to the diver through a regulator. They may include additional cylinders for decompression gas or emergency breathing gas.[3] Closed-circuit or semi-closed circuit rebreather scuba systems allow recycling of exhaled gases. The volume of gas used is reduced compared to that of open circuit; therefore, a smaller cylinder or cylinders, may be used for an equivalent dive duration. Rebreathers extend the time spent underwater compared to open circuit for the same gas consumption, they produce fewer bubbles and less noise than scuba which makes them attractive to covert military divers to avoid detection, scientific divers to avoid disturbing marine animals, and media divers to avoid bubble interference.[1]		Scuba diving may be done recreationally or professionally in a number of applications, including scientific, military and public safety roles, but most commercial diving uses surface-supplied diving equipment when this is practicable. Scuba divers engaged in armed forces covert operations may be referred to as frogmen, combat divers or attack swimmers.[4]		A scuba diver primarily moves underwater by using fins attached to the feet, but external propulsion can be provided by a diver propulsion vehicle, or a sled pulled from the surface. Other equipment includes a mask to improve underwater vision, exposure protection, equipment to control buoyancy, and equipment related to the specific circumstances and purpose of the dive. Scuba divers are trained in the procedures and skills appropriate to their level of certification by instructors affiliated to the diver certification organisations which issue these certifications. These include standard operating procedures for using the equipment and dealing with the general hazards of the underwater environment, and emergency procedures for self-help and assistance of a similarly equipped diver experiencing problems. A minimum level of fitness and health is required by most training organisations, but a higher level of fitness may be appropriate for some applications.						The history of scuba diving is closely linked with the history of scuba equipment. By the turn of the twentieth century, two basic architectures for underwater breathing apparatus had been pioneered; open-circuit surface supplied equipment where the diver's exhaled gas is vented directly into the water, and closed-circuit breathing apparatus where the diver's carbon dioxide is filtered from unused oxygen, which is then recirculated. Closed circuit equipment was more easily adapted to scuba in the absence of reliable, portable, and economical high pressure gas storage vessels. By the mid twentieth century, high pressure cylinders were available and two systems for scuba had emerged: open-circuit scuba where the diver's exhaled breath is vented directly into the water, and closed-circuit scuba where the carbon dioxide is removed from the diver's exhaled breath which has oxygen added and is recirculated.		These were the first systems that became popular with recreational divers. They were safer than early rebreather systems, less expensive to operate, and allowed dives to greater depths.		An important step for the development of open circuit scuba technology was the invention of the demand regulator. In 1864, the French engineers Auguste Denayrouze and Benoît Rouquayrol designed and patented their "Rouquayrol-Denayrouze diving suit" after adapting a pressure regulator and developing it for underwater use. This would be the first diving suit that could supply air to the diver on demand by adjusting the flow of air from the tank to meet the diver’s breathing and pressure requirements. The system still had to use surface supply, as the cylinders of the 1860s would not have been able to withstand the necessary high pressures.[citation needed]		The first open-circuit scuba system was devised in 1925 by Yves Le Prieur in France. Inspired by the simple apparatus of Maurice Fernez and the freedom it allowed the diver, he conceived an idea to make it free of the tube to the surface pump by using Michelin cylinders as the air supply, containing three litres of air compressed to 150 kilograms per square centimetre (2,100 psi; 150 bar). The "Fernez-Le Prieur" diving apparatus was demonstrated at the swimming pool of Tourelles in Paris in 1926. The unit consisted of a cylinder of compressed air carried on the back of the diver, connected to a pressure regulator designed by Le Prieur adjusted manually by the diver, with two gauges, one for tank pressure and one for output (supply) pressure. Air was supplied continually to the mouthpiece and ejected through a short exhaust pipe fitted with a valve as in the Fernez design,[5] however, the lack of a demand regulator and the consequent low endurance of the apparatus limited the practical use of LePrieur’s device.		Fernez had previously invented the noseclip, a mouthpiece (equipped with a one-way valve for exhalation) and diving goggles, and Yves le Prieur just joined to those three Fernez elements a hand-controlled regulator and a compressed-air cylinder. Fernez's goggles didn't allow a dive deeper than ten metres due to "mask squeeze", so, in 1933, Le Prieur replaced all the Fernez equipment (goggles, noseclip and valve) by a full face mask, directly supplied with constant flow air from the cylinder.[5]		In 1942, during the German occupation of France, Jacques-Yves Cousteau and Émile Gagnan designed the first successful and safe open-circuit scuba, known as the Aqua-Lung. Their system combined an improved demand regulator with high-pressure air tanks. Émile Gagnan, an engineer employed by the Air Liquide company, miniaturized and adapted the regulator to use with gas generators, in response to constant fuel shortage that was a consequence of German requisitioning. Gagnan's boss, Henri Melchior, knew that his son-in-law Jacques-Yves Cousteau was looking for an automatic demand regulator to increase the useful period of the underwater breathing apparatus invented by Commander le Prieur,[6] so he introduced Cousteau to Gagnan in December 1942. On Cousteau's initiative, the Gagnan's regulator was adapted to diving, and the new Cousteau-Gagnan patent was registered some weeks later in 1943.[7]		The alternative concept, developed in roughly the same time frame was closed-circuit scuba. The body consumes and metabolises only a small fraction of inhaled oxygen—the situation is even more wasteful of oxygen when the breathing gas is compressed as it is in ambient pressure breathing systems underwater. The rebreather recycles the exhaled breathing gas, while constantly replenishing it from the supply so that the oxygen level does not get depleted. The apparatus also has to remove the exhaled carbon dioxide, as a buildup of CO2 levels would result in respiratory distress and hypercapnia.		The first commercially practical scuba rebreather was designed and built by the diving engineer Henry Fleuss in 1878, while working for Siebe Gorman in London.[8] His self contained breathing apparatus consisted of a rubber mask connected to a breathing bag, with (estimated) 50-60% O2 supplied from a copper tank and CO2 scrubbed by rope yarn soaked in a solution of caustic potash; the system giving a duration of about three hours.[8][9] This apparatus was first used under operational conditions in 1880 by the lead diver on the Severn Tunnel construction project, who was able to travel 1,000 feet (300 m) in the darkness to close several submerged sluice doors in the tunnel; this had defeated the best efforts of hard hat divers due to the danger of their air supply hoses becoming fouled on submerged debris, and the strong water currents in the workings.[8]		Fleuss continually improved his apparatus, adding a demand regulator and tanks capable of holding greater amounts of oxygen at higher pressure. Sir Robert Davis, head of Siebe Gorman, improved the oxygen rebreather in 1910[8][9] with his invention of the Davis Submerged Escape Apparatus, the first rebreather to be made in quantity. While intended primarily as an emergency escape apparatus for submarine crews, it was soon also used for diving, being a handy shallow water diving apparatus with a thirty-minute endurance,[9] and as an industrial breathing set.		The rig comprised a rubber breathing/buoyancy bag containing a canister of barium hydroxide to scrub exhaled CO2 and, in a pocket at the lower end of the bag, a steel pressure cylinder holding approximately 56 litres (2.0 cu ft) of oxygen at a pressure of 120 bars (1,700 psi) which was equipped with a control valve and connected to the breathing bag. Opening the cylinder's valve admitted oxygen to the bag at ambient pressure. The rig also included an emergency buoyancy bag on the front of to help keep the wearer afloat. The DSEA was adopted by the Royal Navy after further development by Davis in 1927.[10]		During the 1930s and all through World War II, the British, Italians and Germans developed and extensively used oxygen rebreathers to equip the first frogmen. The British adapted the Davis Submerged Escape Apparatus and the Germans adapted the Dräger submarine escape rebreathers,[11] for their frogmen during the war.		The Italians developed similar rebreathers for the combat swimmers of the Decima Flottiglia MAS, especially the Pirelli ARO.[12] In the U.S. Major Christian J. Lambertsen invented an underwater free-swimming oxygen rebreather in 1939, which was accepted by the Office of Strategic Services.[13] In 1952 he patented a modification of his apparatus, this time named SCUBA, which became the generic English word for autonomous breathing equipment for diving.[14] After World War II, military frogmen continued to use rebreathers since they do not make bubbles which would give away the presence of the divers. The high percentage of oxygen used by these early rebreather systems limited the depth at which they could be used.		Air Liquide started selling the Cousteau-Gagnan regulator commercially as of 1946 under the name of scaphandre Cousteau-Gagnan or CG45 ("C" for Cousteau, "G" for Gagnan and 45 for the 1945 patent). The same year Air Liquide created a division called La Spirotechnique, to develop and sell regulators and other diving equipment. To sell his regulator in English-speaking countries Cousteau registered the Aqua-Lung trademark, which was first licensed to the U.S. Divers company (the American division of Air Liquide) and later sold with La Spirotechnique and U.S. Divers to finally become the name of the company, Aqua-Lung/La Spirotechnique, currently located in Carros, near Nice.[15]		In 1948 the Cousteau-Gagnan patent was also licensed to Siebe Gorman of England,[16] when Siebe Gorman was directed by Robert Henry Davis.[17] Siebe Gorman was allowed to sell in Commonwealth countries, but had difficulty in meeting the demand and the U.S. patent prevented others from making the product. This patent was curcumvented by Ted Eldred of Melbourne, Australia, who had been developing a rebreather called the Porpoise. When a demonstration resulted in a diver passing out, he developed the single-hose open-circuit scuba system, which separates the first and second stages by a low-pressure hose, and releases exhaled gas at the second stage.[18] Eldred sold the first Porpoise Model CA single hose scuba early in 1952.		Early scuba sets were usually provided with a plain harness of shoulder straps and waist belt. The waist belt buckles were usually quick-release, and shoulder straps sometimes had adjustable or quick release buckles. Many harnesses did not have a backplate, and the cylinders rested directly against the diver's back.		Early scuba divers dived without a buoyancy aid.[19] In an emergency they had to jettison their weights. In the 1960s adjustable buoyancy life jackets (ABLJ) became available, which can be used to compensate for loss of buoyancy at depth due to compression of the neoprene wetsuit and as a lifejacket that will hold an unconscious diver face-upwards at the surface, and that can be quickly inflated. The first versions were inflated from a small disposable carbon dioxide cylinder, later with a small direct coupled air cylinder. A low-pressure feed from the regulator first-stage to an inflation/deflation valve unit lets the volume of the ABLJ be controlled as a buoyancy aid. In 1971 the stabilizer jacket was introduced by ScubaPro. This class of buoyancy aid is known as a buoyancy control device or buoyancy compensator.		Technical diving is recreational scuba diving that exceeds the generally accepted recreational limits. Technical diving may expose the diver to hazards beyond those normally associated with recreational diving, and to greater risks of serious injury or death. These risks may be reduced by appropriate skills, knowledge and experience, and by using suitable equipment and procedures. The equipment often involves breathing gases other than air or standard nitrox mixtures, multiple gas sources, and different equipment configurations.[20] Over time, several aspects of technical diving have become more widely accepted for recreational diving.		A backplate and wing is a type of scuba harness with an attached buoyancy compensation device (BCD) which establishes neutral buoyancy underwater and positive buoyancy on the surface. Unlike most BCDs, the backplate and wing is a modular system, in that it consists of separable components. The core components of this system are the backplate, usually made from metal, which is held against the diver’s back by the harness, and to which the diver’s primary cylinder or cylinders are attached, and inflatable buoyancy bladder known as a wing, sandwiched between the backplate and the cylinder(s), used for adjusting the buoyancy of the diver when in the water. This arrangement clears the front and sides of the diver for other equipment to be attached in the region where it is easily accessible. This additional equipment is usually suspended from the harness or carried in pockets on the exposure suit.[21][22]		Sidemount is a scuba diving equipment configuration which has scuba sets mounted alongside the diver, below the shoulders and along the hips, instead of on the back of the diver. It originated as a configuration for advanced cave diving, as it facilitates penetration of tight sections of cave, allows easy access to cylinder valves, provides easy and reliable gas redundancy, and tanks can be easily removed when necessary. These benefits for operating in confined spaces were also recognized by divers who conducted technical wreck diving penetrations.		Sidemount diving is now growing in popularity within the technical diving community for general decompression diving,[23] and is becoming an increasingly popular specialty training for recreational diving, with several diver certification agencies offering recreational and technical level sidemount training programs.[24][25][26]		The ready availability of oxygen sensing cells beginning in the late 1980s led to a resurgence of interest in rebreather diving. By accurately measuring the partial pressure of oxygen, it became possible to maintain a breathable gas mixture in the loop at any depth.		The term "SCUBA" (an acronym for "self-contained underwater breathing apparatus") originally referred to United States combat frogmen's oxygen rebreathers, developed during World War II by Christian J. Lambertsen for underwater warfare.[2][27][28]		"SCUBA" was originally an acronym, but is now generally used as a common noun or adjective, "scuba".[29] It has become acceptable to refer to "scuba equipment" or "scuba apparatus"—examples of the linguistic RAS syndrome.		Scuba diving may be performed for a number of reasons, both personal and professional. Recreational diving is done purely for enjoyment and has a number of technical disciplines to increase interest underwater, such as cave diving, wreck diving, ice diving and deep diving.[30][31][32]		Divers may be employed professionally to perform tasks underwater. Some of these tasks are suitable for scuba.[1][3][33]		There are divers who work, full or part-time, in the recreational diving community as instructors, assistant instructors, divemasters and dive guides. In some jurisdictions the professional nature, with particular reference to responsibility for health and safety of the clients, of recreational diver instruction, dive leadership for reward and dive guiding is recognised and regulated by national legislation.[33]		Other specialist areas of scuba diving include military diving, with a long history of military frogmen in various roles. Their roles include direct combat, infiltration behind enemy lines, placing mines or using a manned torpedo, bomb disposal or engineering operations. In civilian operations, many police forces operate police diving teams to perform "search and recovery" or "search and rescue" operations and to assist with the detection of crime which may involve bodies of water. In some cases diver rescue teams may also be part of a fire department, paramedical service or lifeguard unit, and may be classed as public service diving.[33]		Lastly, there are professional divers involved with underwater environment, such as underwater photographers or underwater videographers, who document the underwater world, or scientific diving, including marine biology, geology, hydrology, oceanography and underwater archaeology.[3][33]		The choice between scuba and surface-supplied diving equipment is based on both legal and logistical constraints. Where the diver requires mobility and a large range of movement, scuba is usually the choice if safety and legal constraints allow. Higher risk work, particularly in commercial diving, may be restricted to surface-supplied equipment by legislation and codes of practice.[34][33]		Diving activities commonly associated with scuba include:		The depth range applicable to scuba diving depends on the application and training. The major worldwide certification agencies consider 130 feet (40 m) to be the limit for recreation diving. British and European agencies, including BSAC and SAA, recommend a maximum depth of 50 metres (160 ft)[35] Shallower limits are recommended for divers who are youthful, inexperienced, or who have not taken training for deep dives. Technical diving extends these depth limits through changes to training, equipment, and the gas mix used. The maximum depth considered safe is controversial and varies among agencies and instructors, however, there are programs that train divers for dives to 100 metres (330 ft).		Professional diving usually limits the allowed planned decompression depending on the code of practice, operational directives, or statutory restrictions. Depth limits depend on the jurisdiction, and maximum depths allowed range from 30 metres (100 ft) to more than 50 metres (160 ft), depending on the breathing gas used and the availability of a decompression chamber nearby or on site.[34][33]		The defining equipment used by a scuba diver is the eponymous scuba, the self-contained underwater breathing apparatus which allows the diver to breathe while diving, and is transported by the diver.		As one descends, in addition to the normal atmospheric pressure at the surface, the water exerts increasing hydrostatic pressure of approximately 1 bar (14.7 pounds per square inch) for every 10 m (33 feet) of depth. The pressure of the inhaled breath must balance the surrounding or ambient pressure to allow inflation of the lungs. It becomes virtually impossible to breathe air at normal atmospheric pressure through a tube below three feet under the water.[2]		Most recreational scuba diving is done using a half mask which covers the diver's eyes and nose, and a mouthpiece to supply the breathing gas from the demand valve or rebreather. Inhaling from a regulator's mouthpiece becomes second nature very quickly. The other common arrangement is a full face mask which covers the eyes, nose and mouth, and often allows the diver to breathe through the nose. Professional scuba divers are more likely to use full face masks, which protect the diver's airway if the diver loses consciousness.[33]		Open circuit scuba has no provision for using the breathing gas more than once for respiration.[1] The gas inhaled from the scuba equipment is exhaled to the environment, or occasionally into another item of equipment for a special purpose, usually to increase buoyancy of a lifting device such as a buoyancy compensator, inflatable surface marker buoy or small lifting bag. The breathing gas is generally provided from a high-pressure diving cylinder through a scuba regulator. By always providing the appropriate breathing gas at ambient pressure, demand valve regulators ensure the diver can inhale and exhale naturally and without excessive effort, regardless of depth, as and when needed.[36]		The most commonly used scuba set uses a "single-hose" open circuit 2-stage demand regulator, connected to a single back-mounted high-pressure gas cylinder, with the first stage connected to the cylinder valve and the second stage at the mouthpiece.[1] This arrangement differs from Emile Gagnan's and Jacques Cousteau's original 1942 "twin-hose" design, known as the Aqua-lung, in which the cylinder pressure was reduced to ambient pressure in one or two stages which were all in the housing mounted to the cylinder valve or manifold.[36] The "single-hose" system has significant advantages over the original system for most applications.[37]		In the "single-hose" two-stage design, the first stage regulator reduces the cylinder pressure of up to about 300 bars (4,400 psi) to an intermediate pressure (IP) of about 8 to 10 bars (120 to 150 psi) above ambient pressure. The second stage demand valve regulator, supplied by a low-pressure hose from the first stage, delivers the breathing gas at ambient pressure to the diver's mouth. The exhaled gases are exhausted directly to the environment as waste through a non-return valve on the second stage housing. The first stage typically has at least one outlet port delivering gas at full tank pressure which is connected to the diver's submersible pressure gauge or dive computer, to show how much breathing gas remains in the cylinder.[37]		Less common are closed circuit (CCR) and semi-closed (SCR) rebreathers which unlike open-circuit sets that vent off all exhaled gases, process all or part of each exhaled breath for re-use by removing the carbon dioxide and replacing the oxygen used by the diver.[38]		Rebreathers release little or no gas bubbles into the water, and use much less stored gas volume, for an equivalent depth and time because exhaled oxygen is recovered; this has advantages for research, military,[1] photography, and other applications. Rebreathers are more complex and more expensive than open-circuit scuba, and special training and correct maintenance are required for them to be safely used, due to the larger variety of potential failure modes.[38]		In a closed-circuit rebreather the oxygen partial pressure in the rebreather is controlled, so it can be maintained at a safe continuous maximum, which reduces the inert gas (nitrogen and/or helium) partial pressure in the breathing loop. Minimising the inert gas loading of the diver's tissues for a given dive profile reduces the decompression obligation. This requires continuous monitoring of actual partial pressures with time and for maximum effectiveness requires real-time computer processing by the diver's decompression computer. Decompression can be much reduced compared to fixed ratio gas mixes used in other scuba systems and, as a result, divers can stay down longer or require less time to decompress. A semi-closed circuit rebreather injects a constant mass flow of a fixed breathing gas mixture into the breathing loop, or replaces a specific percentage of the respired volume, so the partial pressure of oxygen at any time during the dive depends on the diver's oxygen consumption and/or breathing rate. Planning decompression requirements requires a more conservative approach for a SCR than for a CCR, but decompression computers with a real time oxygen partial pressure input can optimise decompression for these systems.		Because rebreathers produce very few bubbles, they do not disturb marine life or make a diver's presence known at the surface; this is useful for underwater photography, and for covert work.		For some diving, gas mixtures other than normal atmospheric air (21% oxygen, 78% nitrogen, 1% trace gases) can be used,[1][2] so long as the diver is competent in their use. The most commonly used mixture is nitrox, also referred to as Enriched Air Nitrox (EAN), which is air with extra oxygen, often with 32% or 36% oxygen, and thus less nitrogen, reducing the risk of decompression sickness or allowing longer exposure to the same pressure for equal risk. The reduced nitrogen may also allow for no stops or shorter decompression stop times or a shorter surface interval between dives. A common misconception is that nitrox can reduce narcosis, but research has shown that oxygen is also narcotic.[39][2]:304		The increased partial pressure of oxygen due to the higher oxygen content of nitrox increases the risk of oxygen toxicity, which becomes unacceptable below the maximum operating depth of the mixture. To displace nitrogen without the increased oxygen concentration, other diluent gases can be used, usually helium, when the resultant three gas mixture is called trimix, and when the nitrogen is fully substituted by helium, heliox.[3]		For dives requiring long decompression stops, divers may carry cylinders containing different gas mixtures for the various phases of the dive, typically designated as Travel, Bottom, and Decompression gases. These different gas mixtures may be used to extend bottom time, reduce inert gas narcotic effects, and reduce decompression times.		To take advantage of the freedom of movement afforded by scuba equipment, the diver needs to be mobile underwater.		Personal mobility is enhanced by fins and optionally diver propulsion vehicles. Fins have a large blade area and use the more powerful leg muscles, so are much more efficient for propulsion and manoeuvring thrust than arm and hand movements, but require skill to provide fine control. Several types of fin are available, some of which may be more suited for maneuvering, alternative kick styles, speed, endurance, reduced effort or ruggedness.[3]		Streamlining dive gear will reduce drag and improve mobility. Balanced trim which allows the diver to align in any desired direction also improves streamlining by presenting the smallest section area to the direction of movement and allows propulsion thrust to be used more efficiently.[40]		Occasionally a diver may be towed using a "sled", an unpowered device towed behind a surface vessel which conserves the diver's energy and allows more distance to be covered for a given air consumption and bottom time. The depth is usually controlled by the diver by using diving planes or by tilting the whole sled.[41] Some sleds are faired to reduce drag on the diver.[42]		To dive safely, divers must control their rate of descent and ascent in the water[2] and be able to maintain a constant depth in midwater.[43] Ignoring other forces such as water currents and swimming, the diver's overall buoyancy determines whether they ascend or descend. Equipment such as diving weighting systems, diving suits (wet, dry or semi-dry suits are used depending on the water temperature) and buoyancy compensators can be used to adjust the overall buoyancy.[1] When divers want to remain at constant depth, they try to achieve neutral buoyancy. This minimises the effort of swimming to maintain depth and therefore reduces gas consumption.[43]		The buoyancy force on the diver is the weight of the volume of the liquid that they and their equipment displace minus the weight of the diver and their equipment; if the result is positive, that force is upwards. The buoyancy of any object immersed in water is also affected by the density of the water. The density of fresh water is about 3% less than that of ocean water.[44] Therefore, divers who are neutrally buoyant at one dive destination (e.g. a fresh water lake) will predictably be positively or negatively buoyant when using the same equipment at destinations with different water densities (e.g. a tropical coral reef).[43]		The removal ("ditching" or "shedding") of diver weighting systems can be used to reduce the diver's weight and cause a buoyant ascent in an emergency.[43]		Diving suits made of compressible materials decrease in volume as the diver descends, and expand again as the diver ascends, causing buoyancy changes. Diving in different environments also necessitates adjustments in the amount of weight carried to achieve neutral buoyancy. The diver can inject air into dry suits to counteract the compression effect and squeeze. Buoyancy compensators allow easy and fine adjustments in the diver's overall volume and therefore buoyancy. For open circuit divers, changes in the diver's average lung volume during a breathing cycle can be used to make fine adjustments of buoyancy.[43]		Neutral buoyancy in a diver is an unstable state. It is changed by small differences in ambient pressure caused by a change in depth, and the change has a positive feedback effect. A small descent will increase the pressure, which will compress the gas filled spaces and reduce the total volume of diver and equipment. This will further reduce the buoyancy, and unless counteracted, will result in sinking more rapidly. The equivalent effect applies to a small ascent, which will trigger an increased buoyancy and will result in accelerated ascent unless counteracted. The diver must continuously adjust buoyancy or depth in order to remain neutral. Fine control of buoyancy can be achieved by controlling the average lung volume in open circuit scuba, but this feature is not available to the closed circuit rebreather diver, as exhaled gas remains in the breathing loop. This is a skill which improves with practice until it becomes second nature.[43]		Buoyancy changes with depth variation are proportional to the compressible part of the volume of the diver and equipment, and to the proportional change in pressure, which is greater per unit of depth near the surface. Minimising the volume of gas required in the buoyancy compensator will minimise the buoyancy fluctuations with changes in depth. This can be achieved by accurate selection of ballast weight, which should be the minimum to allow neutral buoyancy with depleted gas supplies at the end of the dive unless there is an operational requirement for greater negative buoyancy during the dive.		Buoyancy and trim can significantly affect drag of a diver. The effect of swimming with a head up angle, of about 15° as is quite common in poorly trimmed divers, can be an increase in drag in the order of 50%.[40]		Water has a higher refractive index than air – similar to that of the cornea of the eye. Light entering the cornea from water is hardly refracted at all, leaving only the eye's crystalline lens to focus light. This leads to very severe hypermetropia. People with severe myopia, therefore, can see better underwater without a mask than normal-sighted people.		Diving masks and helmets solve this problem by providing an air space in front of the diver's eyes.[1] The refraction error created by the water is mostly corrected as the light travels from water to air through a flat lens, except that objects appear approximately 34% bigger and 25% closer in water than they actually are. Therefore, total field-of-view is significantly reduced and eye–hand coordination must be adjusted.		Divers who need corrective lenses to see clearly outside the water would normally need the same prescription while wearing a mask. Generic and custom corrective lenses are available for some two-window masks. Custom lenses can be bonded onto masks that have a single front window or two windows.[45]		Cylindrically curved faceplates such as those used for firefighting full-face masks produce severely distorted views underwater.[citation needed]		As a diver descends, they must periodically exhale through their nose to equalise the internal pressure of the mask with that of the surrounding water. Swimming goggles are not suitable for diving because they only cover the eyes and thus do not allow for equalisation. Failure to equalise the pressure inside the mask may lead to a form of barotrauma known as mask squeeze.[1][3]		Masks tend to fog when warm humid exhaled air condenses on the cold inside of the faceplate. To prevent fogging many divers spit into the dry mask before use, spread the saliva around the inside of the glass and rinse it out with a little water. The saliva residue allows condensation to wet the glass and form a continuous film, rather than tiny droplets. There are several commercial products that can be used as an alternative to saliva, some of which are more effective and last longer, but there is a risk of getting the anti-fog agent in the eyes.[46]		Water attenuates light by selective absorption.[47][48] Pure water preferentially absorbs red light, and to a lesser extent, yellow and green, so the colour that is least absorbed is blue light.[49] Dissolved materials may also selectively absorb colour in addition to the absorption by the water itself. In other words, as a diver goes deeper on a dive, more colour is absorbed by the water, and in clean water the colour becomes blue with depth. Colour vision is also affected by turbidity of the water which tends to reduce contrast. Artificial light is useful to provide light in the darkness, and to restore natural colour lost to absorption.[citation needed]		Protection from heat loss in cold water is usually provided by wet suits or dry suits. These also provide protection from sunburn, abrasion and stings from some marine organisms. Where thermal insulation is not important, lycra suits/diving skins may be sufficient.		A wetsuit is a garment, usually made of foamed neoprene, which provides thermal insulation, abrasion resistance and buoyancy. The insulation properties depend on bubbles of gas enclosed within the material, which reduce its ability to conduct heat. The bubbles also give the wetsuit a low density, providing buoyancy in water.		A good close fit and few zips helps the suit to remain waterproof and reduce flushing - the replacement of water trapped between suit and body by cold water from the outside. Improved seals at the neck, wrists and ankles and baffles under the entry zip produce a suit known as a "semi-dry".		Suits range from a thin (2 mm or less) "shortie", covering just the torso, to a full 8 mm semi-dry, usually complemented by neoprene boots, gloves and hood.		A dry suit provides thermal insulation to the wearer while immersed in water,[50][51][52][53] and normally protects the whole body except the head, hands, and sometimes the feet. In some configurations, these are also covered. Dry suits are usually used where the water temperature is below 15 °C (60 °F) or for extended immersion in water above 15 °C (60 °F), where a wet suit user would get cold, and with an integral helmet, boots, and gloves for personal protection when diving in contaminated water.[54]		Dry suits are designed to prevent water entering. This generally allows better insulation making them more suitable for use in cold water. They can be uncomfortably hot in warm or hot air, and are typically more expensive and more complex to don. For divers, they add some degree of complexity as the suit must be inflated and deflated with changes in depth in order to avoid "squeeze" on descent or uncontrolled rapid ascent due to over-buoyancy.[54]		Unless the maximum depth of the water is known, and is quite shallow, a diver must monitor the depth and duration of a dive to avoid decompression sickness. Traditionally this was done by using a depth gauge and a diving watch, but electronic dive computers are now in general use, as they are programmed to do real-time modelling of decompression requirements for the dive, and automatically allow for surface interval. Many can be set for the gas mixture to be used on the dive, and some can accept changes in the gas mix during the dive. Most dive computers provide a fairly conservative decompression model, and the level of conservatism may be selected by the user within limits. Most decompression computers can also be set for altitude compensation to some degree.		If the dive site and dive plan require the diver to navigate, a compass may be carried, and where retracing a route is critical, as in cave or wreck penetrations, a guide line is laid from a dive reel.		In less critical conditions, many divers simply navigate by landmarks and memory, a procedure also known as pilotage or natural navigation.		A scuba diver should always be aware of the remaining breathing gas supply, and the duration of diving time that this will safely support, taking into account the time required to surface safely and an allowance for foreseeable contingencies. This is usually monitored by using a submersible pressure gauge on each cylinder.		Cutting tools such as knives, line cutters or shears are often carried by divers to cut loose from entanglement in nets or lines. A surface marker buoy on a line held by the diver indicates the position of the diver to the surface personnel. This may be an inflatable marker deployed by the diver at the end of the dive, or a sealed float, towed for the whole dive. A surface marker also allows easy and accurate control of ascent rate and stop depth for safer decompression.		Various surface detection aids may be carried to help surface personnel spot the diver after ascent.		Divers may carry underwater photographic or video equipment, or tools for a specific application in addition to diving equipment.		The underwater environment is unfamiliar and hazardous, and to ensure diver safety, simple, yet necessary procedures must be followed. A certain minimum level of attention to detail and acceptance of responsibility for one's own safety and survival are required. Most of the procedures are simple and straightforward, and become second nature to the experienced diver, but must be learned, and take some practice to become automatic and faultless, just like the ability to walk or talk. Most of the safety procedures are intended to reduce the risk of drowning, and many of the rest are to reduce the risk of barotrauma and decompression sickness. In some applications getting lost is a serious hazard, and specific procedures to minimise the risk are followed.		The purpose of dive planning is to ensure that divers do not exceed their comfort zone or skill level, or the safe capacity of their equipment, and includes scuba gas planning to ensure that the amount of breathing gas to be carried is sufficient to allow for any reasonably foreseeable contingencies. Before starting a dive both the diver and their buddy do equipment checks to ensure everything is in good working order and available. Recreational divers are responsible for planning their own dives, unless in training, when the instructor is responsible.[55][56][57] Divemasters may provide useful information and suggestions to assist the divers, but are generally not responsible for the details unless specifically employed to do so. In professional diving teams all team members are usually expected to contribute to planning and to check the equipment they will use, but the overall responsibility for the safety of the team lies with the supervisor.[58][34][33][59]		Many repetitive dive tables, such as those based on the Bühlmann decompression algorithm and similar, use a Letter Group designation to model the amount of residual nitrogen in the diver's body after a dive. For example, in NAUI dive tables the letters range from A to L, where the letter A represents a small amount of nitrogen and the amount of nitrogen increases as the letters progress to L. At the end of a dive, a Letter Group read from the tables designates the net amount of excess nitrogen absorbed during the dive. As a diver spends time on the surface between dives, excess nitrogen is eliminated (known as "off-gassing"), and the residual nitrogen is assigned to a lower Letter Group.		At the start of a subsequent dive to a given depth, the current Letter Group determines a time penalty representing the residual nitrogen in the diver's body. This residual nitrogen time is subtracted from the time limit indicated by the tables for a dive that carried no penalty. This results in a shorter time limit for the repetitive dive. At the end of the repetitive dive, the residual nitrogen time is added to the actual dive time to determine the total equivalent nitrogen time, which is used to determine the new end-of-dive Letter Group.[60]		These include debriefing where appropriate, and equipment maintenance, to ensure that the equipment is kept in good condition for later use.		Buddy and team diving procedures are intended to ensure that a recreational scuba diver who gets into difficulty underwater is in the presence of a similarly equipped person who understands and can render assistance. Divers are trained to assist in those emergencies specified in the training standards for their certification, and are required to demonstrate competence in a set of prescribed buddy assist skills. The fundamentals of buddy/team safety are centred on diver communication, redundancy of gear and breathing gas by sharing with the buddy, and the added situational perspective of another diver.[61]		Solo divers take responsibility for their own safety and compensate for the absence of a buddy with skill, vigilance and appropriate equipment. Like buddy or team divers, properly equipped solo divers rely on the redundancy of critical articles of dive gear which may include at least two independent supplies of breathing gas and ensuring that there is always enough available to safely terminate the dive if any one supply fails. The difference between the two practices is that this redundancy is carried and managed by the solo diver instead of a buddy. Agencies that certify for solo diving require candidates to have a high level of dive experience - usually about 100 dives or more.[62][63]		Since the inception of scuba, there has been ongoing debate regarding the wisdom of solo diving with strong opinions on both sides of the issue. This debate is complicated by the fact that the line which separates a solo diver from a buddy/team diver is not always clear.[64] For example, should a scuba instructor (who supports the buddy system) be considered a solo diver if their students do not have the knowledge or experience to assist the instructor through an unforeseen scuba emergency? Should the buddy of an underwater photographer consider themselves as effectively diving alone since their buddy (the photographer) is giving most or all of their attention to the subject of the photograph? This debate has motivated some prominent scuba agencies such as Global Underwater Explorers (GUE) to stress that its members only dive in teams and "remain aware of team member location and safety at all time."[65] Other agencies such as Scuba Diving International (SDI) and Professional Association of Diving Instructors (PADI) have taken the position that divers might find themselves alone (by choice or by accident) and have created certification courses such as the "SDI Solo Diver Course" and the "PADI Self-Reliant Diver Course" in order to train divers to handle such possibilities.[66][67]		Divers cannot talk underwater unless they are wearing a full-face mask and electronic communications equipment, but they can communicate basic and emergency information using hand signals, light signals, and rope signals, and more complex messages can be written on waterproof slates.		The most urgent emergencies specific to scuba diving generally involve loss of breathing gas: Gas supply failures, situations where breathing air is likely to run out before the diver can surface, or inability to ascend, and uncontrolled ascents.		Controlled emergency ascents are almost always a consequence of loss of breathing gas, while uncontrolled ascents are usually the result of a buoyancy control failure.		The most urgent underwater emergencies usually involve a compromised breathing gas supply. Divers are trained in procedures for donating and receiving breathing gas from each other in an emergency, and may carry an alternative air source if they do not choose to rely on a buddy.		Divers may be trained in procedures which have been approved by the training agencies for recovery of an unresponsive diver to the surface, where it might be possible to administer first aid. Not all recreational divers have this training as some agencies do not include it in entry level training. Professional divers may be required by legislation or code of practice to have a standby diver at any diving operation, who is both competent and available to attempt rescue of a distressed diver.		Two basic types of entrapment are significant hazards for scuba divers: Inability to navigate out of an enclosed space, and physical entrapment which prevents the diver from leaving a location. The first case can usually be avoided by staying out of enclosed spaces, and when the objective of the dive includes penetration of enclosed spaces, taking precautions such as the use of lights and guidelines.[68] The most common form of physical entrapment is getting snagged on ropes, lines or nets, and use of a cutting implement is the standard method of dealing with the problem. The risk of entanglement can be reduced by careful configuration of equipment to minimise those parts which can easily be snagged, and allow easier disentanglement. Other forms of entrapment such as getting wedged into tight spaces can often be avoided, but must otherwise be dealt with as they happen. The assistance of a buddy may be helpful where possible.[21]		Scuba diving in relatively hazardous environments such as caves and wrecks, areas of strong water movement, relatively great depths, with decompression obligations, with equipment that has more complex failure modes, and with gases that are not safe to breathe at all depths of the dive require specialised safety and emergency procedures tailored to the specific hazards.		Divers face specific physical and health risks when they go underwater with scuba equipment.		The presence of a combination of several hazards simultaneously is common in diving, and the effect is generally increased risk to the diver, particularly where the occurrence of an incident due to one hazard triggers other hazards with a resulting cascade of incidents. Many diving fatalities are the result of a cascade of incidents overwhelming the diver, who should be able to manage any single reasonably foreseeable incident.[69]		Divers must avoid injuries caused by changes in pressure. The weight of the water column above the diver causes an increase in pressure in proportion to depth, in the same way that the weight of the column of atmospheric air above the surface causes a pressure of 101.3 kPa (14.7 pounds-force per square inch) at sea level. This variation of pressure with depth will cause compressible materials and gas filled spaces to tend to change volume, which can cause the surrounding material or tissues to be stressed, with the risk of injury if the stress gets too high. Pressure injuries are called barotrauma[2] and can be quite painful, even potentially fatal – in severe cases causing a ruptured lung, eardrum or damage to the sinuses. To avoid barotrauma, the diver equalises the pressure in all air spaces with the surrounding water pressure when changing depth. The middle ear and sinus are equalised using one or more of several techniques, which is referred to as clearing the ears.		The scuba mask (half-mask) is equalised during descent by periodically exhaling through the nose. During ascent it will automatically equalise by leaking excess air round the edges. A helmet or full face mask will automatically equalise as any pressure differential will either vent through the exhaust valve or open the demand valve and release air into the low-pressure space.		If a drysuit is worn, it must be equalised by inflation and deflation, much like a buoyancy compensator. Most dry suits are fitted with an auto-dump valve, which, if set correctly, and kept at the high point of the diver by good trim skills, will automatically release gas as it expands and retain a virtually constant volume during ascent. During descent the dry suit must be inflated manually.		Although there are many dangers involved in scuba diving, divers can decrease the risks through proper procedures and appropriate equipment. The requisite skills are acquired by training and education, and honed by practice. Open-water certification programmes highlight diving physiology, safe diving practices, and diving hazards, but do not provide the diver with sufficient practice to become truly adept.		The prolonged exposure to breathing gases at high partial pressure will result in increased amounts of non-metabolic gases, usually nitrogen and/or helium, (referred to in this context as inert gases) dissolving in the bloodstream as it passes through the alveolar capillaries, and thence carried to the other tissues of the body, where they will accumulate until saturated. This saturation process has very little immediate effect on the diver. However, when the pressure is reduced during ascent, the amount of dissolved inert gas that can be held in stable solution in the tissues is reduced. This effect is described by Henry's Law.[70]		As a consequence of the reducing partial pressure of inert gases in the lungs during ascent, the dissolved gas will be diffused back from the bloodstream to the gas in the lungs and exhaled. The reduced gas concentration in the blood has a similar effect when it passes through tissues carrying a higher concentration, and that gas will diffuse back into the bloodsteam, reducing the loading of the tissues.[70] As long as this process is gradual, the tissue gas loading in the diver will reduce by diffusion and perfusion until it eventually re-stabilises at the current saturation pressure. The problem arises when the pressure is reduced more quickly than the gas can be removed by this mechanism, and the level of supersaturation rises sufficiently to become unstable. At this point, bubbles may form and grow in the tissues, and may cause damage either by distending the tissue locally, or blocking small blood vessels, shutting off blood supply to the downstream side, and resulting in hypoxia of those tissues.[70]		This effect is called decompression sickness[2] or 'the bends', and must be avoided by reducing the pressure on the body slowly while ascending and allowing the inert gases dissolved in the tissues to be eliminated while still in solution. This process is known as "off-gassing", and is done by restricting the ascent (decompression) rate to one where the level of supersaturation is not sufficient for bubbles to form or grow. This is done by controlling the speed of ascent and making periodic stops to allow gases to be eliminated by respiration. The procedure of making stops is called staged decompression, and the stops are called decompression stops. Decompression stops that are not computed as strictly necessary are called safety stops, and reduce the risk of bubble formation further. Dive computers or decompression tables are used to determine a relatively safe ascent profile, but are not completely reliable. There remains a statistical possibility of decompression bubbles forming even when the guidance from tables or computer has been followed exactly.[70]		Decompression sickness must be treated as soon as practicable. Definitive treatment is usually recompression in a recompression chamber with hyperbaric oxygen treatment. Exact details will depend on severity and type of symptoms, response to treatment, and the dive history of the casualty. Administering enriched-oxygen breathing gas or pure oxygen to a decompression sickness stricken diver on the surface is the definitive form of first aid for decompression sickness, although death or permanent disability may still occur.[71]		Nitrogen narcosis or inert gas narcosis is a reversible alteration in consciousness producing a state similar to alcohol intoxication in divers who breathe high-pressure gas containing nitrogen at depth.[2] The mechanism is similar to that of nitrous oxide, or "laughing gas," administered as anaesthesia. Being "narced" can impair judgement and make diving very dangerous. Narcosis starts to affect some divers at about 66 feet (20 m) on air. At this depth, narcosis often manifests itself as a slight giddiness. The effects increase with an increase in depth. Almost all divers will notice the effects by 132 feet (40 m). At this depth divers may feel euphoria, anxiety, loss of coordination and/or lack of concentration. At extreme depths, a hallucinogenic reaction, tunnel vision or unconsciousness can occur. Jacques Cousteau famously described it as the "rapture of the deep".[6] Nitrogen narcosis occurs quickly and the symptoms typically disappear equally quickly during the ascent, so that divers often fail to realise they were ever affected. It affects individual divers at varying depths and conditions, and can even vary from dive to dive under identical conditions. Diving with trimix or heliox reduces the effects, which are proportional to the partial pressure of nitrogen in the breathing gas.		Oxygen toxicity occurs when the tissues are exposed to an excessive combination of partial pressure (PPO2) and duration.[2] In acute cases it affects the central nervous system and causes a seizure, which can result in the diver spitting out their regulator and drowning. While the exact limit is not reliably predictable, it is generally recognised that central nervous system oxygen toxicity is preventable if one does not exceed an oxygen partial pressure of 1.4 bar.[72] For deep dives—generally past 180 feet (55 m), divers use "hypoxic blends" containing a lower percentage of oxygen than atmospheric air. A less immediately threatening form known as pulmonary oxygen toxicity occurs after exposures to lower oxygen partial pressures for much longer periods than generally encountered in scuba diving.		The underwater environment presents a constant hazard of asphyxiation due to drowning. Breathing apparatus used for diving is life-support equipment, and failure can have fatal consequences - reliability of the equipment and the ability of the diver to deal with a single point of failure are essential for diver safety. Failure of other items of diving equipment is generally not as immediately threatening, as provided the diver is conscious and breathing, there may be time to deal with the situation, however an uncontrollable gain or loss of buoyancy can put the diver at severe risk of decompression sickness, or of sinking to a depth where nitrogen narcosis or oxygen toxicity may render the diver incapable of managing the situation, which may lead to drowning while breathing gas remains available.[73]		Water conducts heat from the diver 25 times[74] better than air, which can lead to hypothermia even in mild water temperatures.[2] Symptoms of hypothermia include impaired judgment and dexterity,[75] which can quickly become deadly in an aquatic environment. In all but the warmest waters, divers need the thermal insulation provided by wetsuits or drysuits.[1]		In the case of a wetsuit, the suit is designed to minimise heat loss. Wetsuits are usually made of foamed neoprene that has small closed bubbles, generally containing nitrogen, trapped in it during the manufacturing process. The poor thermal conductivity of this expanded cell neoprene means that wetsuits reduce loss of body heat by conduction to the surrounding water. The neoprene, and to a larger extent the nitrogen gas, function as an insulator. The effectiveness of the insulation is reduced when the suit is compressed due to depth, as the nitrogen filled bubbles are then smaller and the compressed gas conducts heat better. The second way in which wetsuits can reduce heat loss is to trap the water which leaks into the suit. Body heat then heats the trapped water, and provided the suit is reasonably well-sealed at all openings (neck, wrists, ankles, zippers and overlaps with other suit components), this water remains inside the suit and is not replaced by more cold water, which would also take up body heat, and this helps reduce the rate of heat loss. This principle is applied in the "Semi-Dry" wetsuit.		A dry suit functions by keeping the diver dry. The suit is waterproof and sealed so that water cannot penetrate the suit. Special purpose undergarments are usually worn under a dry suit to keep a layer of air between the diver and the suit for thermal insulation. Some divers carry an extra gas bottle dedicated to filling the dry suit, which may contain argon gas, because it is a better insulator than air.[76] Dry suits should not be inflated with gases containing helium as it is a good thermal conductor.		Drysuits fall into two main categories:		Diving suits also help prevent the diver's skin being damaged by rough or sharp underwater objects, marine animals, coral, or metal debris commonly found on shipwrecks.		Some marine animals can be hazardous to divers. In most cases this is a defensive reaction to contact with, or molestation by the diver.		Some physical and psychological conditions are known or suspected to increase the risk of injury or death in the underwater environment, or to increase the risk of a stressful incident developing into a serious incident culminating in injury or death. Conditions which significantly compromise the cardiovascular system, respiratory system or central nervous system may be considered absolute or relative contraindications for diving, as are psychological conditions which impair judgement or compromise the ability to deal calmly and systematically with deteriorating conditions which a competent diver should be able to manage.[82]		Safety of underwater diving operations can be improved by reducing the frequency of human error and the consequences when it does occur.[83] Human error can be defined as an individual's deviation from acceptable or desirable practice which culminates in undesirable or unexpected results.[84] Human error is inevitable and everyone makes mistakes at some time. The consequences of these errors are varied and depend on many factors. Most errors are minor and do not cause significant harm, but others can have catastrophic consequences. Human error and panic are considered to be the leading causes of dive accidents and fatalities.[83]		Some underwater tasks may present hazards related to the activity or the equipment used, In some cases it is the use of the equipment, in some cases transporting the equipment during the dive, and in some cases the additional task loading, or any combination of these that is the hazard.[86]		The risks of dying during recreational, scientific or commercial diving are small, and on scuba, deaths are usually associated with poor gas management, poor buoyancy control, equipment misuse, entrapment, rough water conditions and pre-existing health problems. Some fatalities are inevitable and caused by unforeseeable situations escalating out of control, but the majority of diving fatalities can be attributed to human error on the part of the victim.[87]		Equipment failure is rare in open circuit scuba, and while the cause of death is commonly recorded as drowning, this is mainly the consequence of an uncontrollable series of events taking place in water. Air embolism is also frequently cited as a cause of death, and it, too is the consequence of other factors leading to an uncontrolled and badly managed ascent, possibly aggravated by medical conditions. About a quarter of diving fatalities are associated with cardiac events, mostly in older divers. There is a fairly large body of data on diving fatalities, but in many cases the data is poor due to the standard of investigation and reporting. This hinders research which could improve diver safety.[87]		According to death certificates, over 80% of the deaths were ultimately attributed to drowning, but other factors usually combined to incapacitate the diver in a sequence of events culminating in drowning, which is more a consequence of the medium in which the accidents occurred than the actual accident. Often the drowning obscures the real cause of death. Scuba divers should not drown unless there are other contributory factors as they carry a supply of breathing gas and equipment designed to provide the gas on demand. Drowning occurs as a consequence of preceding problems, such as cardiac disease, pulmonary barotrauma, unmanageable stress, unconsciousness from any cause, water aspiration, trauma, equipment difficulties, environmental hazards, inappropriate response to an emergency or failure to manage the gas supply.[88]		Fatality rates are comparable with jogging (13 deaths per 100,000 persons per year) and are within the range where reduction is desirable by Health and Safety Executive (HSE) criteria,[89] The most frequent root cause for diving fatalities is running out of or low on gas. Other factors cited include buoyancy control, entanglement or entrapment, rough water, equipment misuse or problems and emergency ascent. The most common injuries and causes of death were drowning or asphyxia due to inhalation of water, air embolism and cardiac events. Risk of cardiac arrest is greater for older divers, and greater for men than women, although the risks are equal by age 65.[89]		Several plausible opinions have been put forward but have not yet been empirically validated. Suggested contributing factors included inexperience, infrequent diving, inadequate supervision, insufficient predive briefings, buddy separation and dive conditions beyond the diver's training, experience or physical capacity.[89]		Based on actual exposure time, according to a 1970 North American study, diving was 96 times more dangerous than driving an automobile.[90] and according to a 2000 Japanese study, 36 to 62 times riskier than driving.[91] A difference between the risks of driving and diving is that the diver is less at risk from fellow divers than the driver is from other drivers.		Decompression sickness and arterial gas embolism in recreational diving are associated with certain demographic, environmental, and dive style factors. A statistical study published in 2005 tested potential risk factors: age, gender, body mass index, smoking, asthma, diabetes, cardiovascular disease, previous decompression illness, years since certification, dives in last year, number of diving days, number of dives in a repetitive series, last dive depth, nitrox use, and drysuit use. No significant associations with decompression sickness or arterial gas embolism were found for asthma, diabetes, cardiovascular disease, smoking, or body mass index. Increased depth, previous DCI, days diving, and being male were associated with higher risk for decompression sickness and arterial gas embolism. Nitrox and drysuit use, greater frequency of diving in the past year, increasing age, and years since certification were associated with lower risk, possibly as indicators of more extensive training and experience.[92]		Risk management has three major aspects: Risk assessment, emergency planning and insurance cover. The risk assessment for a dive is primarily a planning activity, and may range in formality from a part of the pre-dive buddy check for recreational divers, to a safety file with professional risk assessment and detailed emergency plans for professional diving projects. Some form of pre-dive briefing is customary with organised recreational dives, and this generally includes a recitation by the divemaster of the known and predicted hazards, the risk associated with the significant ones, and the procedures to be followed in case of the reasonably foreseeable emergencies associated with them. Insurance cover for diving accidents may not be included in standard policies. There are a few organisations which focus specifically on diver safety and insurance cover, such as the international Divers Alert Network[93]		Recreational scuba diving does not have a centralised certifying or regulatory agency, and is mostly self regulated. There are, however, several large diving organisations that train and certify divers and dive instructors, and many diving related sales and rental outlets require proof of diver certification from one of these organisations prior to selling or renting certain diving products or services.		The following organisations publish standards for competence in recreational diving skills and knowledge:		Underwater diver training is normally given by a qualified instructor who is a member of one of many diving training agencies or is registered with a government agency.		Basic diver training entails the learning of skills required for the safe conduct of activities in an underwater environment, and includes procedures and skills for the use of diving equipment, safety, emergency self-help and rescue procedures, dive planning, and use of dive tables or a personal decompression computer.		Some of the scuba skills which an entry level diver will normally learn include:		Some knowledge of physiology and the physics of diving is considered necessary by most diver certification agencies, as the diving environment is alien and relatively hostile to humans. The physics and physiology knowledge required is fairly basic, and helps the diver to understand the effects of the diving environment so that informed acceptance of the associated risks is possible.		The physics mostly relates to gases under pressure, buoyancy, heat loss, and light underwater. The physiology relates the physics to the effects on the human body, to provide a basic understanding of the causes and risks of barotrauma, decompression sickness, gas toxicity, hypothermia, drowning and sensory variations.		More advanced training often involves first aid and rescue skills, skills related to specialised diving equipment, and underwater work skills.		The current record for the longest continuous submergence using SCUBA gear was set by Mike Stevens of Birmingham, England at the National Exhibition Centre, Birmingham, during the annual National Boat, Caravan and Leisure Show between February 14 and February 23, 1986. Mike Stevens was continuously submerged for 212.5 hours beating his own previous record of 121.5 hours. The record was ratified by the Guinness Book of Records.[94]		
Marco Polo (/ˈmɑːrkoʊ ˈpoʊloʊ/ ( listen); Italian: [ˈmarko ˈpɔːlo]; 1254 – January 8–9, 1324)[1] was a Venetian merchant traveller.[2][3][4][5][6] His travels are recorded in Livres des merveilles du monde (Book of the Marvels of the World, also known as The Travels of Marco Polo, c. 1300), a book that described to Europeans the wealth and great size of China, its capital Peking, and other Asian cities and countries.		He learned the mercantile trade from his father and uncle, Niccolò and Maffeo, who travelled through Asia and met Kublai Khan. In 1269, they returned to Venice to meet Marco for the first time. The three of them embarked on an epic journey to Asia, returning after 24 years to find Venice at war with Genoa; Marco was imprisoned and dictated his stories to a cellmate. He was released in 1299, became a wealthy merchant, married, and had three children. He died in 1324 and was buried in the church of San Lorenzo in Venice.		Marco Polo was not the first European to reach China (see Europeans in Medieval China), but he was the first to leave a detailed chronicle of his experience. This book inspired Christopher Columbus[7] and many other travellers. There is a substantial literature based on Polo's writings; he also influenced European cartography, leading to the introduction of the Fra Mauro map.						Marco Polo was born in 1254[8][nb 1] in the Republic of Venice.[9] His exact date and place of birth are archivally unknown.[10][11] Some historians mentioned that he was born on September 15[12][13] but that date is not endorsed by mainstream scholarship.[citation needed] Marco Polo's birthplace is generally considered Venice,[11][14] but also varies between Constantinople[15][11] and the island of Korčula.[16][11][17][18] There is dispute as to whether the Polo family is of Venetian origin, as Venetian historical sources considered them to be of Dalmatian origin.[8][11][19][20] The lack of evidence makes the Korčula theory (probably under Ramusio influence[21]) as a specific birthplace strongly disputed,[9] and even some Croatian scholars consider it justly invented.[22]		In 1168, his great-uncle, Marco Polo, borrowed money and commanded a ship in Constantinople.[23][24] His grandfather, Andrea Polo of the parish of San Felice, had three sons, Maffeo, yet another Marco, and the traveller's father Niccolò.[23] This genealogy, described by Ramusio, is not universally accepted as there is no additional evidence to support it.[25][26]		His father, Niccolò Polo, a merchant, traded with the Near East, becoming wealthy and achieving great prestige.[27][28] Niccolò and his brother Maffeo set off on a trading voyage before Marco's birth.[8][28] In 1260, Niccolò and Maffeo, while residing in Constantinople, then the capital of the Latin Empire, foresaw a political change; they liquidated their assets into jewels and moved away.[27] According to The Travels of Marco Polo, they passed through much of Asia, and met with Kublai Khan, a Mongol ruler and founder of the Yuan dynasty.[29] Their decision to leave Constantinople proved timely. In 1261 Michael VIII Palaiologos, the ruler of the Empire of Nicaea, took Constantinople, promptly burned the Venetian quarter and re-established the Eastern Roman Empire. Captured Venetian citizens were blinded,[30] while many of those who managed to escape perished aboard overloaded refugee ships fleeing to other Venetian colonies in the Aegean Sea.		Almost nothing is known about the childhood of Marco Polo until he was fifteen years old, excepting that he probably spent part of his childhood in Venice.[31][32][24] Meanwhile, Marco Polo's mother died, and an aunt and uncle raised him.[28] He received a good education, learning mercantile subjects including foreign currency, appraising, and the handling of cargo ships;[28] he learned little or no Latin.[27] His father later married Floradise Polo (née Trevisan).[26]		In 1269, Niccolò and Maffeo returned to their families in Venice, meeting young Marco for the first time.[31] In 1271, during the rule of Doge Lorenzo Tiepolo, Marco Polo (at seventeen years of age), his father, and his uncle set off for Asia on the series of adventures that Marco later documented in his book.[33] They returned to Venice in 1295, 24 years later, with many riches and treasures. They had travelled almost 15,000 miles (24,000 km).[28]		Marco Polo returned to Venice in 1295 with his fortune converted into gemstones. At this time, Venice was at war with the Republic of Genoa.[34] Polo armed a galley equipped with a trebuchet[35] to join the war. He was probably caught by Genoans in a skirmish in 1296, off the Anatolian coast between Adana and the Gulf of Alexandretta[36] and not during the battle of Curzola (September 1298), off the Dalmatian coast.[37] The latter claim is due to a later tradition (16th Century) recorded by Giovanni Battista Ramusio.[38][39]		He spent several months of his imprisonment dictating a detailed account of his travels to a fellow inmate, Rustichello da Pisa,[28] who incorporated tales of his own as well as other collected anecdotes and current affairs from China. The book soon spread throughout Europe in manuscript form, and became known as The Travels of Marco Polo. It depicts the Polos' journeys throughout Asia, giving Europeans their first comprehensive look into the inner workings of the Far East, including China, India, and Japan.[40]		Polo was finally released from captivity in August 1299,[28] and returned home to Venice, where his father and uncle in the meantime had purchased a large palazzo in the zone named contrada San Giovanni Crisostomo (Corte del Milion).[41] For such a venture, the Polo family probably invested profits from trading, and even many gemstones they brought from the East.[41] The company continued its activities and Marco soon became a wealthy merchant. Marco and his uncle Maffeo financed other expeditions, but likely never left Venetian provinces, nor returned to the Silk Road and Asia.[42] Sometime before 1300, his father Niccolò died.[42] In 1300, he married Donata Badoèr, the daughter of Vitale Badoèr, a merchant.[43] They had three daughters, Fantina (married Marco Bragadin), Bellela (married Bertuccio Querini), and Moreta.[44][45]		In 1305 he is mentioned in a Venetian document among local sea captains regarding the payment of taxes.[26] His relation with a certain Marco Polo, who in 1300 was mentioned with riots against the aristocratic government, and escaped the death penalty, as well as riots from 1310 led by Bajamonte Tiepolo (by mother side grandson of Trogir count Stjepko Šubić) and Marco Querini, among whose rebels were Jacobello and Francesco Polo from another family branch, is unclear.[26] Polo is clearly mentioned again after 1305 in Maffeo's testament from 1309–1310, in a 1319 document according to which he became owner of some estates of his deceased father, and in 1321, when he bought part of the family property of his wife Donata.[26]		In 1323, Polo was confined to bed, due to illness.[46] On January 8, 1324, despite physicians' efforts to treat him, Polo was on his deathbed.[47] To write and certify the will, his family requested Giovanni Giustiniani, a priest of San Procolo. His wife, Donata, and his three daughters were appointed by him as co-executrices.[47] The church was entitled by law to a portion of his estate; he approved of this and ordered that a further sum be paid to the convent of San Lorenzo, the place where he wished to be buried.[47] He also set free Peter, a Tartar servant, who may have accompanied him from Asia,[48] and to whom Polo bequeathed 100 lire of Venetian denari.[49]		He divided up the rest of his assets, including several properties, among individuals, religious institutions, and every guild and fraternity to which he belonged.[47] He also wrote-off multiple debts including 300 lire that his sister-in-law owed him, and others for the convent of San Giovanni, San Paolo of the Order of Preachers, and a cleric named Friar Benvenuto.[47] He ordered 220 soldi be paid to Giovanni Giustiniani for his work as a notary and his prayers.[50]		The will was not signed by Polo, but was validated by the then-relevant "signum manus" rule, by which the testator only had to touch the document to make it legally valid.[49][51] Due to the Venetian law stating that the day ends at sunset, the exact date of Marco Polo's death cannot be determined, but according to some scholars it was between the sunsets of January 8 and 9, 1324.[52] Biblioteca Marciana, which holds the original copy of his testament, dates the testament in January 9, 1323, and gives the date of his death at some time in June 1324.[51]		An authoritative version of Marco Polo's book does not and cannot exist, for the early manuscripts differ significantly. The published editions of his book either rely on single manuscripts, blend multiple versions together, or add notes to clarify, for example in the English translation by Henry Yule. The 1938 English translation by A.C. Moule and Paul Pelliot is based on a Latin manuscript found in the library of the Cathedral of Toledo in 1932, and is 50% longer than other versions.[53] Approximately 150 manuscript copies in various languages are known to exist, and before availability of the printing press discrepancies were inevitably introduced during copying and translation.[54] The popular translation published by Penguin Books in 1958 by R.E. Latham works several texts together to make a readable whole.[55]		Polo related his memoirs orally to Rustichello da Pisa while both were prisoners of the Genova Republic. Rustichello wrote Devisement du Monde in Langues d'Oil, a lingua franca of crusaders and western merchants in the Orient.[56] The idea probably was to create a handbook for merchants, essentially a text on weights, measures and distances.[57]		The book opens with a preface describing his father and uncle traveling to Bolghar where Prince Berke Khan lived. A year later, they went to Ukek[58] and continued to Bukhara. There, an envoy from the Levant invited them to meet Kublai Khan, who had never met Europeans.[59] In 1266, they reached the seat of Kublai Khan at Dadu, present day Beijing, China. Kublai received the brothers with hospitality and asked them many questions regarding the European legal and political system.[60] He also inquired about the Pope and Church in Rome.[61] After the brothers answered the questions he tasked them with delivering a letter to the Pope, requesting 100 Christians acquainted with the Seven Arts (grammar, rhetoric, logic, geometry, arithmetic, music and astronomy). Kublai Khan requested that an envoy bring him back oil of the lamp in Jerusalem.[62] The long sede vacante between the death of Pope Clement IV in 1268 and the election of his successor delayed the Polos in fulfilling Kublai's request. They followed the suggestion of Theobald Visconti, then papal legate for the realm of Egypt, and returned to Venice in 1269 or 1270 to await the nomination of the new Pope, which allowed Marco to see his father for the first time, at the age of fifteen or sixteen.[63]		In 1271, Niccolò, Maffeo and Marco Polo embarked on their voyage to fulfil Kublai's request. They sailed to Acre, and then rode on camels to the Persian port of Hormuz. The Polos wanted to sail straight into China, but the ships there were not seaworthy, so they continued overland through the Silk Road, until reaching Kublai's summer palace in Shangdu, near present-day Zhangjiakou. In one instance during their trip, the Polos joined a caravan of travelling merchants whom they crossed paths with. Unfortunately, the party was soon attacked by bandits, who used the cover of a sandstorm to ambush them. The Polos managed to fight and escape through a nearby town, but many members of the caravan were killed or enslaved.[64] Three and a half years after leaving Venice, when Marco was about 21 years old, the Polos were welcomed by Kublai into his palace.[28] The exact date of their arrival is unknown, but scholars estimate it to be between 1271 and 1275.[nb 2] On reaching the Yuan court, the Polos presented the sacred oil from Jerusalem and the papal letters to their patron.[27]		Marco knew four languages, and the family had accumulated a great deal of knowledge and experience that was useful to Kublai. It is possible that he became a government official;[28] he wrote about many imperial visits to China's southern and eastern provinces, the far south and Burma.[65] Highly respected and sought after in the Mongolian court, Kublai Khan decided to decline the Polos' requests to leave China. They became worried about returning home safely, believing that if Kublai died, his enemies might turn against them because of their close involvement with the ruler. In 1292, Kublai's great-nephew, then ruler of Persia, sent representatives to China in search of a potential wife, and they asked the Polos to accompany them, so they were permitted to return to Persia with the wedding party—which left that same year from Zaitun in southern China on a fleet of 14 junks. The party sailed to the port of Singapore,[66] travelled north to Sumatra,[67] sailed west to the Point Pedro port of Jaffna under Savakanmaindan and to Pandyan of Tamilakkam.[68] Eventually Polo crossed the Arabian Sea to Hormuz. The two-year voyage was a perilous one—of the six hundred people (not including the crew) in the convoy only eighteen had survived (including all three Polos).[69] The Polos left the wedding party after reaching Hormuz and travelled overland to the port of Trebizond on the Black Sea, the present day Trabzon.[28]		The British scholar Ronald Latham has pointed out that The Book of Marvels was in fact a collaboration written in 1298–1299 between Polo and a professional writer of romances, Rustichello of Pisa.[70] Latham also argued that Rustichello may have glamorised Polo's accounts, and added fantastic and romantic elements that made the book a bestseller.[70] The Italian scholar Luigi Foscolo Benedetto had previously demonstrated that the book was written in the same "leisurely, conversational style" that characterised Rustichello's other works, and that some passages in the book were taken verbatim or with minimal modifications from other writings by Rustichello. For example, the opening introduction in The Book of Marvels to "emperors and kings, dukes and marquises" was lifted straight out of an Arthurian romance Rustichello had written several years earlier, and the account of the second meeting between Polo and Kublai Khan at the latter's court is almost the same as that of the arrival of Tristan at the court of King Arthur at Camelot in that same book.[71] Latham believed that many elements of the book, such as legends of the Middle East and mentions of exotic marvels may have been the work of Rustichello who was giving what medieval European readers expected to find in a travel book.[72]		Since the book publication, some have viewed the book with skepticism.[73] Some in the Middle Ages regarded the book simply as a romance or fable, due largely to the sharp difference of its descriptions of a sophisticated civilisation in China to other early accounts by Giovanni da Pian del Carpine and William of Rubruck who portrayed the Mongols as 'barbarians' who appeared to belong to 'some other world'.[73] Doubts have also been raised in later centuries about Marco Polo's narrative of his travels in China, for example for his failure to mention the Great Wall of China, and in particular the difficulties in identifying many of the place names he used[74] (the great majority however have since been identified).[75] Many have questioned if he had visited the places he mentioned in his itinerary, if he had appropriated the accounts of his father and uncle or other travelers, and some doubted if he even reached China, or that if he did, perhaps never went beyond Khanbaliq (Beijing).[74][76]		It has however been pointed out that Polo's accounts of China are more accurate and detailed than other travelers' accounts of the periods. Polo had at times refuted the 'marvelous' fables and legends given in other European accounts, and despite some exaggerations and errors, Polo's accounts have relatively few of the descriptions of irrational marvels. In many cases where present (mostly given in the first part before he reached China, such as mentions of Christian miracles), he made a clear distinction that they are what he had heard rather than what he had seen. It is also largely free of the gross errors found in other accounts such as those given by the Moroccan traveler Ibn Battuta who had confused the Yellow River with the Grand Canal and other waterways, and believed that porcelain was made from coal.[77]		Modern studies have further shown that details given in Marco Polo's book, such as the currencies used, salt productions and revenues, are accurate and unique. Such detailed descriptions are not found in other non-Chinese sources, and their accuracy is supported by archaeological evidence as well as Chinese records compiled after Polo had left China, his accounts are therefore unlikely to have been obtained second hand.[78] Other accounts have also been verified; for example, when visiting Zhenjiang in Jiangsu, China, Marco Polo noted that a large number of Christian churches had been built there. His claim is confirmed by a Chinese text of the 14th century explaining how a Sogdian named Mar-Sargis from Samarkand founded six Nestorian Christian churches there in addition to one in Hangzhou during the second half of the 13th century.[79] His story of the princess Kököchin sent from China to Persia to marry the Īl-khān is also confirmed by independent sources in both Persia and China.[80]		Skeptics have long wondered if Marco Polo wrote his book based on hearsay, with some pointing to omissions about noteworthy practices and structures of China as well as the lack of details on some places in his book. While Polo describes paper money and the burning of coal, he fails to mention the Great Wall of China, tea, Chinese characters, chopsticks, or footbinding.[81] His failure to note the presence of the Great Wall of China was first raised in the middle of seventeenth century, and in the middle of eighteenth century, it was suggested that he might have never reached China.[74] Later scholars such as John W. Haeger argued the Marco Polo might not have visited Southern China due to the lack of details in his description of southern Chinese cities compared to northern ones, while Herbert Franke also raised the possibility that Marco Polo might not have been to China at all, and wondered if he might have based his accounts on Persian sources due to his use of Persian expressions.[76][82] This is taken further by Dr. Frances Wood who claimed in her 1995 book Did Marco Polo Go to China? that at best Polo never went farther east than Persia (modern Iran), and that there is nothing in The Book of Marvels about China that could not be obtained via reading Persian books.[83] Wood maintains that it is more probable that Polo only went to Constantinople (modern Istanbul, Turkey) and some of the Italian merchant colonies around the Black Sea, picking hearsay from those travellers who had been farther east.[83]		Supporters of the book's basic accuracy countered on the points raised by skeptics such as footbinding and the Great Wall of China. Historian Stephen G. Haw argued that the Great Walls were built to keep out northern invaders, whereas the ruling dynasty during Marco Polo's visit were those very northern invaders. They note that the Great Wall familiar to us today is a Ming structure built some two centuries after Marco Polo's travels; and that the Mongol rulers whom Polo served controlled territories both north and south of today's wall, and would have no reasons to maintain any fortifications that may have remained there from the earlier dynasties.[84] Other Europeans who travelled to Khanbaliq during the Yuan Dynasty, such as Giovanni de' Marignolli and Odoric of Pordenone, said nothing about the wall either. The Muslim traveler Ibn Batutta, who asked about the wall when he visited China during the Yuan Dynasty, could find no one who had either seen it or knew of anyone who had seen it, suggesting that while ruins of the wall constructed in the earlier periods might have existed, they were not significant or noteworthy at that time.[84]		Haw also argued that footbinding was not common even among Chinese during Polo's time and almost unknown among the Mongols. While the Italian missionary Odoric of Pordenone who visited Yuan China mentioned footbinding (it is however unclear whether he was merely relaying something he had heard as his description is inaccurate),[85] no other foreign visitors to Yuan China mentioned the practice, perhaps an indication that the footbinding was not widespread or was not practiced in an extreme form at that time.[86] Marco Polo himself noted (in the Toledo manuscript) the dainty walk of Chinese women who took very short steps.[84] It has also been noted by other scholars that many of the things not mentioned by Marco Polo such as tea and chopsticks weren't mentioned by other travelers as well.[87] Haw also pointed out that despite the few omissions, Marco Polo's account is more extensive, more accurate and more detailed than those of other foreign travelers to China in this period.[88]		Many scholars believe that Marco Polo exaggerated his importance in China. The British historian David Morgan thought that Polo had likely exaggerated and lied about his status in China,[89] while Ronald Latham believed that such exaggerations were embellishments by his ghost writer Rustichello da Pisa.[72] In The Book of Marvels, Polo claimed that he was a close friend and advisor to Kublai Khan and that he was the governor of the city of Yangzhou for three years – yet no Chinese source mentions him as either a friend of the Emperor or as the governor of Yangzhou – indeed no Chinese source mentions Marco Polo at all.[89] Herbert Franke noted that all occurrences of Po-lo or Bolod (an Altaic word meaning "steel") in Yuan texts were names of people of Mongol or Turkic extraction.[82] The sinologist Paul Pelliot thought that Polo might have served as an officer of the government salt monopoly in Yangzhou, which was a position of some significance that could explain the exaggeration.[89] Polo also claimed to have provided the Mongols with technical advice on building mangonels during the Siege of Xiangyang, a claim that cannot possibly be true as the siege was over before Polo had arrived in China.[89] The Mongol army that besieged Xiangyang did have foreign military engineers, but they were mentioned in Chinese sources as being from Baghdad and had Arabic names.[82]		Stephen G. Haw, however, challenges this idea that Polo exaggerated his own importance, writing that, "contrary to what has often been said...Marco does not claim any very exalted position for himself in the Yuan empire."[90] He points out that Marco never claimed to be a minister of high rank, a darughachi, a leader of a tumen (i.e. 10,000 men), not even the leader of 1,000 men, only that he was an emissary for the khan and held a position of some honor. Haw sees this as a reasonable claim if Marco was a keshig, who numbered some fourteen thousand at the time.[90] Haw explains how the earliest manuscripts of Polo's accounts provide contradicting information about his role in Yangzhou, with some stating he was just a simple resident, others stating he was a governor, and Ramusio's manuscript claiming he was simply holding that office as a temporary substitute for someone else, yet all the manuscripts concur that he worked as an esteemed emissary for the khan.[91] Haw also objected to the approach to finding mention of Marco Polo in Chinese texts, contending that contemporaneous Europeans had little regard for using surnames, and a direct Chinese transcription of the name "Marco" ignores the possibility of him taking on a Chinese or even Mongol name that had no bearing or similarity with his Latin name.[90]		A number of errors in Marco Polo's account have been noted: for example, he described the bridge later known as Marco Polo Bridge as having twenty-four arches instead of eleven or thirteen.[87] He also said that city wall of Khanbaliq had twelve gates when it had only eleven.[92] Archaeologists have also pointed out that Polo may have mixed up the details from the two attempted invasions of Japan by Kublai Khan in 1274 and 1281. Polo wrote of five-masted ships, when archaeological excavations found that the ships in fact had only three masts.[93]		Wood accused Marco Polo of taking other people's accounts in his book, retelling other stories as his own, or based his accounts on Persian guidebooks or other lost sources. For example, Sinologist Francis Woodman Cleaves noted that Polo's account of the voyage of the princess Kököchin from China to Persia to marry the Īl-khān in 1293 has been confirmed by a passage in the 15th-century Chinese work Yongle Encyclopedia and by the Persian historian Rashid-al-Din Hamadani in his work Jami' al-tawarikh. However neither of these accounts mentions Polo or indeed any European as part of the bridal party,[80] and Wood used the lack of mention of Polo in these works as an example of Polo's "retelling of a well-known tale". Morgan, in Polo's defence, noted that even the princess herself was not mentioned in the Chinese source, and that it would have been surprising if Polo had been mentioned by Rashid-al-Din.[94] Historian Igor de Rachewiltz argued that Marco Polo's account in fact allows the Persian and Chinese sources to be reconciled – by relaying the information that two of the three envoys sent (mentioned in the Chinese source and whose names accord with those given by Polo) had died during the voyage, it explains why only the third who survived, Coja/Khoja, was mentioned by Rashìd al-Dìn. Polo had therefore completed the story by providing information not found in either source. He also noted that the only Persian source that mentions the princess was not completed until 1310-11, therefore Marco Polo could not have learned the information from any Persian book. According to de Rachewiltz, the concordance of Polo's detailed account of the princess with other independent sources that gave only incomplete information is proof of the veracity of Polo's story and his presence in China.[87]		Morgan writes that since much of what The Book of Marvels has to say about China is "demonstrably correct" that to claim that Polo did not go to China "creates far more problems than it solves" and so that the "balance of probabilities" strongly suggests that Polo really did go to China, even if he exaggerated somewhat his importance in China.[95] Haw dismisses the various anachronistic criticisms of Polo's accounts that started in the 17th century, and highlights Polo's accuracy in great part of his accounts, for example on the lay of the land such as the Grand Canal of China.[96] "If Marco was a liar," Haw writes, "then he must have been an implausibly meticulous one."[97]		In 2012, the University of Tübingen Sinologist and historian Hans Ulrich Vogel released a detailed analysis of Polo's description of currencies, salt production and revenues, and argued that the evidence supports his presence in China because he included details which he could not have otherwise known.[78][98] Vogel noted that no other Western, Arab, or Persian sources have given such accurate and unique details about the currencies of China, for example, the shape and size of the paper, the use of seals, the various denominations of paper money as well as variations in currency usage in different regions of China, such as the use of cowry shells in Yunnan, details supported by archaeological evidence and Chinese sources compiled long after Polo's had left China.[99] His accounts of salt production and revenues from the salt monopoly are also accurate, and accord with Chinese documents of the Yuan era.[100] Economic historian Mark Elvin, in his preface to Vogel's 2013 monograph, concludes that Vogel "demonstrates by specific example after specific example the ultimately overwhelming probability of the broad authenticity" of Polo's account. Many problems were caused by the oral transmission of the original text and the proliferation of significantly different hand-copied manuscripts. For instance, did Polo exert "political authority" (seignora) in Yangzhou or merely "sojourn" (sejourna) there. Elvin concludes that "those who doubted, although mistaken, were not always being casual or foolish," but "the case as a whole had now been closed": the book is, "in essence, authentic, and, when used with care, in broad terms to be trusted as a serious though obviously not always final, witness."[101]		Other lesser-known European explorers had already travelled to China, such as Giovanni da Pian del Carpine, but Polo's book meant that his journey was the first to be widely known. Christopher Columbus was inspired enough by Polo's description of the Far East to want to visit those lands for himself; a copy of the book was among his belongings, with handwritten annotations.[7] Bento de Góis, inspired by Polo's writings of a Christian kingdom in the east, travelled 4,000 miles (6,400 km) in three years across Central Asia. He never found the kingdom but ended his travels at the Great Wall of China in 1605, proving that Cathay was what Matteo Ricci (1552–1610) called "China".[102]		Marco Polo's travels may have had some influence on the development of European cartography, ultimately leading to the European voyages of exploration a century later.[103] The 1453 Fra Mauro map was said by Giovanni Battista Ramusio (disputed by historian/cartographer Piero Falchetta, in whose work the quote appears) to have been partially based on the one brought from Cathay by Marco Polo:		That fine illuminated world map on parchment, which can still be seen in a large cabinet alongside the choir of their monastery (the Camaldolese monastery of San Michele di Murano) was by one of the brothers of the monastery, who took great delight in the study of cosmography, diligently drawn and copied from a most beautiful and very old nautical map and a world map that had been brought from Cathay by the most honourable Messer Marco Polo and his father.		Though Marco Polo never produced a map that illustrated his journey, his family drew several maps to the Far East based on the wayward's accounts. These collection of maps were signed by Polo's three daughters: Fantina, Bellela and Moreta.[104] Not only did it contain maps of his journey, but also sea routes to Japan, Siberia's Kamchatka Peninsula, the Bering Strait and even to the coastlines of Alaska, centuries before the rediscovery of the Americas by Europeans.		The Marco Polo sheep, a subspecies of Ovis ammon, is named after the explorer,[105] who described it during his crossing of Pamir (ancient Mount Imeon) in 1271.[nb 3]		In 1851, a three-masted Clipper built in Saint John, New Brunswick also took his name; the Marco Polo was the first ship to sail around the world in under six months.[106]		The airport in Venice is named Venice Marco Polo Airport.[107]		The frequent flyer programme of Hong Kong flag carrier Cathay Pacific is known as the "Marco Polo Club".[108]		The travels of Marco Polo are fictionalised in a number works, such as:		
Fear is a feeling induced by perceived danger or threat that occurs in certain types of organisms, which causes a change in metabolic and organ functions and ultimately a change in behavior, such as fleeing, hiding, or freezing from perceived traumatic events. Fear in human beings may occur in response to a specific stimulus occurring in the present, or in anticipation or expectation of a future threat perceived as a risk to body or life. The fear response arises from the perception of danger leading to confrontation with or escape from/avoiding the threat (also known as the fight-or-flight response), which in extreme cases of fear (horror and terror) can be a freeze response or paralysis.		In humans and animals, fear is modulated by the process of cognition and learning. Thus fear is judged as rational or appropriate and irrational or inappropriate. An irrational fear is called a phobia.		Psychologists such as John B. Watson, Robert Plutchik, and Paul Ekman have suggested that there is only a small set of basic or innate emotions and that fear is one of them. This hypothesized set includes such emotions as acute stress reaction, anger, angst, anxiety, fright, horror, joy, panic, and sadness. Fear is closely related to, but should be distinguished from, the emotion anxiety, which occurs as the result of threats that are perceived to be uncontrollable or unavoidable.[1] The fear response serves survival by generating appropriate behavioral responses, so it has been preserved throughout evolution.[2]						Many physiological changes in the body are associated with fear, summarized as the fight-or-flight response. An inborn response for coping with danger, it works by accelerating the breathing rate (hyperventilation), heart rate, constriction of the peripheral blood vessels leading to blushing and vasodilation of the central vessels (pooling), increasing muscle tension including the muscles attached to each hair follicle to contract and causing "goose bumps", or more clinically, piloerection (making a cold person warmer or a frightened animal look more impressive), sweating, increased blood glucose (hyperglycemia), increased serum calcium, increase in white blood cells called neutrophilic leukocytes, alertness leading to sleep disturbance and "butterflies in the stomach" (dyspepsia). This primitive mechanism may help an organism survive by either running away or fighting the danger.[3] With the series of physiological changes, the consciousness realizes an emotion of fear.		People develop specific fears as a result of learning. This has been studied in psychology as fear conditioning, beginning with John B. Watson's Little Albert experiment in 1920, which was inspired after observing a child with an irrational fear of dogs. In this study, an 11-month-old boy was conditioned to fear a white rat in the laboratory. The fear became generalized to include other white, furry objects, such as a rabbit, dog, and even a ball of cotton.		Fear can be learned by experiencing or watching a frightening traumatic accident. For example, if a child falls into a well and struggles to get out, he or she may develop a fear of wells, heights (acrophobia), enclosed spaces (claustrophobia), or water (aquaphobia). There are studies looking at areas of the brain that are affected in relation to fear. When looking at these areas (such as the amygdala), it was proposed that a person learns to fear regardless of whether they themselves have experienced trauma, or if they have observed the fear in others. In a study completed by Andreas Olsson, Katherine I. Nearing and Elizabeth A. Phelps the amygdala were affected both when subjects observed someone else being submitted to an aversive event, knowing that the same treatment awaited themselves, and when subjects were subsequently placed in a fear-provoking situation.[4] This suggests that fear can develop in both conditions, not just simply from personal history.		Fear is affected by cultural and historical context. For example, in the early 20th century, many Americans feared polio, a disease that can lead to paralysis.[5] There are consistent cross-cultural differences in how people respond to fear.[citation needed] Display rules affect how likely people are to show the facial expression of fear and other emotions.		Although many fears are learned, the capacity to fear is part of human nature. Many studies[citation needed] have found that certain fears (e.g. animals, heights) are much more common than others (e.g. flowers, clouds). These fears are also easier to induce in the laboratory. This phenomenon is known as preparedness. Because early humans that were quick to fear dangerous situations were more likely to survive and reproduce, preparedness is theorized to be a genetic effect that is the result of natural selection.[citation needed]		From an evolutionary psychology perspective, different fears may be different adaptations that have been useful in our evolutionary past. They may have developed during different time periods. Some fears, such as fear of heights, may be common to all mammals and developed during the mesozoic period. Other fears, such as fear of snakes, may be common to all simians and developed during the cenozoic time period. Still others, such as fear of mice and insects, may be unique to humans and developed during the paleolithic and neolithic time periods (when mice and insects become important carriers of infectious diseases and harmful for crops and stored foods).[6]		Fear is high only if the observed risk and seriousness both are high, and is low, if risk or seriousness is low.[7]		In a 2005 Gallup Poll (U.S.), a national sample of adolescents between the ages of 13 and 17 were asked what they feared the most. The question was open-ended and participants were able to say whatever they wanted. The top ten fears were, in order: terrorist attacks, spiders, death, failure, war, criminal or gang violence, being alone, the future, and nuclear war.[8]		In an estimate of what people fear the most, book author Bill Tancer analyzed the most frequent online queries that involved the phrase, "fear of..." following the assumption that people tend to seek information on the issues that concern them the most. His top ten list of fears published 2008 consisted of flying, heights, clowns, intimacy, death, rejection, people, snakes, failure, and driving.[9]		According to surveys, some of the most common fears are of demons and ghosts, the existence of evil powers, cockroaches, spiders, snakes, heights, water, enclosed spaces, tunnels, bridges, needles, social rejection, failure, examinations, and public speaking.[10][11][12]		Death anxiety is multidimensional; it covers "fears related to one's own death, the death of others, fear of the unknown after death, fear of obliteration, and fear of the dying process, which includes fear of a slow death and a painful death".[13]		The Yale philosopher Shelly Kagan examined fear of death in a 2007 Yale open course[14] by examining the following questions: Is fear of death a reasonable appropriate response? What conditions are required and what are appropriate conditions for feeling fear of death? What is meant by fear, and how much fear is appropriate? According to Kagan for fear in general to make sense, three conditions should be met: the object of fear needs to be "something bad", there needs to be a non-negligible chance that the bad state of affairs will happen, and there needs to be some uncertainty about the bad state of affairs. The amount of fear should be appropriate to the size of "the bad". If the 3 conditions aren't met, fear is an inappropriate emotion. He argues, that death does not meet the first two criteria, even if death is a "deprivation of good things" and even if one believes in a painful afterlife. Because death is certain, it also does not meet the third criterion, but he grants that the unpredictability of when one dies may be cause to a sense of fear.		In a 2003 study of 167 women and 121 men, aged 65–87, low self-efficacy predicted fear of the unknown after death and fear of dying for women and men better than demographics, social support, and physical health. Fear of death was measured by a "Multidimensional Fear of Death Scale" which included the 8 subscales Fear of Dying, Fear of the Dead, Fear of Being Destroyed, Fear for Significant Others, Fear of the Unknown, Fear of Conscious Death, Fear for the Body After Death, and Fear of Premature Death. In hierarchical multiple regression analysis the most potent predictors of death fears were low "spiritual health efficacy", defined as beliefs relating to one's perceived ability to generate spiritually based faith and inner strength, and low "instrumental efficacy", defined as beliefs relating to one's perceived ability to manage activities of daily living.[13]		Psychologists have tested the hypothesis that fear of death motivates religious commitment, and assurances about an afterlife alleviate the fear and empirical research on this topic has been equivocal.[citation needed] Religiosity can be related to fear of death when the afterlife is portrayed as time of punishment. "Intrinsic religiosity", as opposed to mere "formal religious involvement" has been found to be negatively correlated with death anxiety.[13] In a 1976 study people of various Christian denominations those most firm in their faith, attending religious services weekly were the least afraid of dying. The survey found a negative correlation between fear of death and "religious concern".[15][better source needed]		In a 2006 study of white, Christian men and women the hypothesis was tested that traditional, church-centered religiousness and de-institutionalized spiritual seeking are ways of approaching fear of death in old age. Both religiousness and spirituality were related to positive psychosocial functioning, but only church-centered religiousness protected subjects against the fear of death.[16][better source needed]		Fear of the unknown or irrational fear is caused by negative thinking (worry) which arises from anxiety accompanied with a subjective sense of apprehension or dread. Irrational fear like any other fears share common neural pathway that engages to mobilize bodily resources in the face of threat. Many people are scared of the "unknown". The irrational fear can branch out to many areas such as the hereafter, the next ten years, or even tomorrow. Chronic irrational fear has deleterious effects since the elicitor stimulus is commonly absent or perceived from delusions. In these cases specialists use False Evidence Appearing Real as a definition. Such fear can create comorbidity with the anxiety disorder umbrella.[17] Being scared may cause people to experience anticipatory fear of what may lie ahead rather than planning and evaluating for the same. E.g. Continuation of scholarly education, most educators perceive this as a risk that may cause them fear and stress[18] and they would rather teach things they've been taught than go and do research, this can lead to habits such as laziness and procrastination.[better source needed] The ambiguity of a situations that tend to be uncertain and unpredictable can cause anxiety, other psychological and physical problems in some populations; especially those who engage it constantly. E.g. War-ridden or Conflict places, Terrorism, Abuse ...etc. Poor parenting that instills fear can also debilitate children's psyche development or personality. E.g. Parents tell their children not to talk to strangers in order to protect them. In school they would be motivated to not show fear in talking with strangers, but to be assertive and also aware of the risks and the environment that it takes place. Ambiguous and mixed messages like this can affect their self-esteem and self-confidence. Researchers say talking to strangers isn't something to be thwarted but allowed in a parent's presence if required.[19] Developing a sense of equanimity to handle various situations is often advocated as an antidote to irrational fear and essential skill by a number of ancient philosophies.		Often laboratory studies with rats are conducted to examine the acquisition and extinction of conditioned fear responses.[20] In 2004, researchers conditioned rats (Rattus norvegicus) to fear a certain stimulus, through electric shock.[21] The researchers were able to then cause an extinction of this conditioned fear, to a point that no medications or drugs were able to further aid in the extinction process. However the rats did show signs of avoidance learning, not fear, but simply avoiding the area that brought pain to the tests rats. The avoidance learning of rats is seen as a conditioned response, and therefore the behavior can be unconditioned, as supported by the earlier research. Species-specific defense reactions (SSDRs) or avoidance learning in nature is the specific tendency to avoid certain threats or stimuli, it is how animals survive in the wild. Humans and animals both share these species-specific defense reactions, such as the flight, fight, which also include pseudo-aggression, fake or intimidating aggression, freeze response to threats, which is controlled by the sympathetic nervous system. These SSDRs are learned very quickly through social interactions between others of the same species, other species, and interaction with the environment.[22] These acquired sets of reactions or responses are not easily forgotten. The animal that survives is the animal that already knows what to fear and how to avoid this threat. An example in humans is the reaction to the sight of a snake, many jump backwards before cognitively realizing what they are jumping away from, and in some cases it is a stick rather than a snake.		As with many functions of the brain, there are various regions of the brain involved in deciphering fear in humans and other nonhuman species.[23] The amygdala communicates both directions between the prefrontal cortex, hypothalamus, the sensory cortex, the hippocampus, thalamus, septum, and the brainstem. The amygdala plays an important role in SSDR, such as the ventral amygdalofugal, which is essential for associative learning, and SSDRs are learned through interaction with the environment and others of the same species. An emotional response is created only after the signals have been relayed between the different regions of the brain, and activating the sympathetic nervous systems; which controls the flight, fight, freeze, fright, and faint response.[24][25] Often a damaged amygdala can cause impairment in the recognition of fear (like the human case of patient S.M.).[26] This impairment can cause different species to lack the sensation of fear, and often can become overly confident, confronting larger peers, or walking up to predatory creatures.		Robert C. Bolles (1970), a researcher at University of Washington, wanted to understand species-specific defense reactions and avoidance learning among animals, but found that the theories of avoidance learning and the tools that were used to measure this tendency were out of touch with the natural world.[27] He theorized the species-specific defense reaction (SSDR).[28] There are three forms of SSDRs: flight, fight (pseudo-aggression), or freeze. Even domesticated animals have SSDRs, and in those moments it is seen that animals revert to atavistic standards and become "wild" again. Dr. Bolles states that responses are often dependent on the reinforcement of a safety signal, and not the aversive conditioned stimuli. This safety signal can be a source of feedback or even stimulus change. Intrinsic feedback or information coming from within, muscle twitches, increased heart rate, is seen to be more important in SSRDs than extrinsic feedback, stimuli that comes from the external environment. Dr. Bolles found that most creatures have some intrinsic set of fears, to help assure survival of the species. Rats will run away from any shocking event, and pigeons will flap their wings harder when threatened, the wing flapping in pigeons and the scattered running of rats are considered a species-specific defense reaction or behavior. Bolles believed that SSDR are conditioned through pavlovian conditioning, and not operant conditioning; SSDR arise from the association between the environmental stimuli and adverse events.[29] Michael S. Fanselow conducted an experiment, to test some specific defense reactions, he observed that rats in two different shock situations responded differently, based on instinct or defensive topography, rather than contextual information.[30]		Species specific defense responses are created out of fear, and are essential for survival.[31] Rats that lack the gene stathmin show no avoidance learning, or a lack of fear, and will often walk directly up to cats and be eaten.[32] Animals use these SSDR to continue living, to help increase their chance of fitness, by surviving long enough to procreate. Humans and animals alike have created fear to know what should be avoided, and this fear can be learned through association with others in the community, or learned through personal experience with a creature, species, or situations that should be avoided. SSDRs are an evolutionary adaptation that has been seen in many species throughout the world including rats, chimpanzees, prairie dogs, and even humans, an adaptation created to help individual creatures survive in a hostile world.		Fear learning changes across the lifetime due to natural developmental changes in the brain.[33][34] This includes changes in the prefrontal cortex and the amygdala.[35]		The brain structure that is the center of most neurobiological events associated with fear is the amygdala, located behind the pituitary gland. The amygdala is part of a circuitry of fear learning.[2] It is essential for proper adaptation to stress and specific modulation of emotional learning memory. In the presence of a threatening stimulus, the amygdala generates the secretion of hormones that influence fear and aggression.[36] Once response to the stimulus in the form of fear or aggression commences, the amygdala may elicit the release of hormones into the body to put the person into a state of alertness, in which they are ready to move, run, fight, etc. This defensive response is generally referred to in physiology as the fight-or-flight response regulated by the hypothalamus, part of the limbic system.[37] Once the person is in safe mode, meaning that there are no longer any potential threats surrounding them, the amygdala will send this information to the medial prefrontal cortex (mPFC) where it is stored for similar future situations, which is known as memory consolidation.[38]		Some of the hormones involved during the state of fight-or-flight include epinephrine, which regulates heart rate and metabolism as well as dilating blood vessels and air passages, norepinephrine increasing heart rate, blood flow to skeletal muscles and the release of glucose from energy stores,[39] and cortisol which increases blood sugar, increases circulating neutrophilic leukocytes, calcium amongst other things.[40]		After a situation which incites fear occurs, the amygdala and hippocampus record the event through synaptic plasticity.[41] The stimulation to the hippocampus will cause the individual to remember many details surrounding the situation.[42] Plasticity and memory formation in the amygdala are generated by activation of the neurons in the region. Experimental data supports the notion that synaptic plasticity of the neurons leading to the lateral amygdala occurs with fear conditioning.[43] In some cases, this forms permanent fear responses such as posttraumatic stress disorder (PTSD) or a phobia.[44] MRI and fMRI scans have shown that the amygdala in individuals diagnosed with such disorders including bipolar or panic disorder is larger and wired for a higher level of fear.[45]		Pathogens can suppress amygdala activity. Rats infected with the toxoplasmosis parasite become less fearful of cats, sometimes even seeking out their urine-marked areas. This behavior often leads to them being eaten by cats. The parasite then reproduces within the body of the cat. There is evidence that the parasite concentrates itself in the amygdala of infected rats.[46] In a separate experiment, rats with lesions in the amygdala did not express fear or anxiety towards unwanted stimuli. These rats pulled on levers supplying food that sometimes sent out electrical shocks. While they learned to avoid pressing on them, they did not distance themselves from these shock-inducing levers.[47]		Several brain structures other than the amygdala have also been observed to be activated when individuals are presented with fearful vs. neutral faces, namely the occipitocerebellar regions including the fusiform gyrus and the inferior parietal / superior temporal gyri.[48] Interestingly, fearful eyes, brows and mouth seem to separately reproduce these brain responses.[48] Scientist from Zurich studies show that the hormone oxytocin related to stress and sex reduces activity in your brain fear center.[49]		In threatening situations insects, aquatic organisms, birds, reptiles, and mammals emit odorant substances, initially called alarm substances, which are chemical signals now called alarm pheromones ("Schreckstoff" in German). This is to defend themselves and at the same time to inform members of the same species of danger and leads to observable behavior change like freezing, defensive behavior, or dispersion depending on circumstances and species. For example, stressed rats release odorant cues that cause other rats to move away from the source of the signal. Pheromones are synthesized, emitted and perceived by all living organisms studied to date, with the exception of viruses and prions: i.e. in bacteria, prokaryotes, plants, plankton, parasites, insects, invertebrates and vertebrates (aquatic organisms, birds, reptiles, and mammals).		After the discovery of pheromones in 1959, alarm pheromones were first described in 1968 in ants[50] and earthworms,[51] and 4 years later also found in mammals, both mice and rats.[52] Over the next two decades identification and characterization of these pheromones proceeded in all manner of insects and sea animals, including fish, but it was not until 1990 that more insight into mammalian alarm pheromones was gleaned.		Early on, in 1985, a link between odors released by stressed rats and pain perception was discovered: unstressed rats exposed to these odors developed opioid-mediated analgesia.[53] In 1997, researchers found bees became less responsive to pain after they had been stimulated with isoamyl acetate, a chemical smelling of banana, and a component of bee alarm pheromone.[54] The experiment also showed that the bees' fear-induced pain tolerance was mediated by an endorphine.		By using the forced swimming test in rats as a model of fear-induction, the first mammalian "alarm substance" was found.[55]		In 1991, this "alarm substance" was shown to fulfill criteria for pheromones: well-defined behavioral effect, species specificity, minimal influence of experience and control for nonspecific arousal. Rat activity testing with alarm pheromone and their preference/avoidance for odors from cylinders containing the pheromone showed, that the pheromone had very low volatility.[56]		In 1993 a connection between alarm chemosignals in mice and their immune response was found.[57]		Pheromone production in mice was found to be associated with or mediated by the pituitary gland in 1994.[58]		It was not until 2011 that a link between severe pain, neuroinflammation and alarm pheromones release in rats was found: real time RT-PCR analysis of rat brain tissues indicated that shocking the footpad of a rat increased its production of proinflammatory cytokines in deep brain structures, namely of IL-1β, heteronuclear Corticotropin-releasing hormone and c-fos mRNA expressions in both the paraventricular nucleus and the bed nucleus of the stria terminalis, and it increased stress hormone levels in plasma (corticosterone).[59]		In 2004, it was demonstrated that rats’ alarm pheromones had different effects on the “recipient“ rat (the rat perceiving the pheromone) depending which body region they were released from: Pheromone production from the face modified behavior in the recipient rat, e.g. caused sniffing or movement, whereas pheromone secreted from the rat's anal area induced autonomic nervous system stress responses, like an increase in core body temperature.[60] Further experiments showed that when a rat perceived alarm pheromones, it increased its defensive and risk assessment behavior.[61] and its acoustic startle reflex was enhanced.		The neurocircuit for how rats perceive alarm pheromones was shown to be related to hypothalamus, brainstem, and amygdala, all of which are evolutionary ancient structures deep inside or in the case of the brainstem underneath the brain away from the cortex, and involved in the fight-or-flight response, as is the case in humans.[62]		Alarm pheromone-induced anxiety in rats has been used to evaluate the degree to which anxiolytics can alleviate anxiety in humans. For this the change in the acoustic startle reflex of rats with alarm pheromone-induced anxiety (i.e. reduction of defensiveness) has been measured. Pretreatment of rats with one of five anxiolytics used in clinical medicine was able to reduce their anxiety: namely midazolam, phenelzine (a nonselective monoamine oxidase (MAO) inhibitor), propranolol, a nonselective beta blocker, clonidine, an alpha 2 adrenergic agonist or CP-154,526, a corticotropin-releasing hormone antagonist.[63]		Faulty development of odor discrimination impairs the perception of pheromones and pheromone-related behavior, like aggressive behavior and mating in male rats: The enzyme Mitogen-activated protein kinase 7 (MAPK7) has been implicated in regulating the development of the olfactory bulb and odor discrimination and it is highly expressed in developing rat brains, but absent in most regions of adult rat brains. conditional deletion of the MAPK7gene in mouse neural stem cells impairs several pheromone-mediated behaviors, including aggression and mating in male mice. These behavior impairments were not caused by a reduction in the level of testosterone, by physical immobility, by heightened fear or anxiety or by depression. Using mouse urine as a natural pheromone-containing solution, it has been shown that the impairment was associated with defective detection of related pheromones, and with changes in their inborn preference for pheromones related to sexual and reproductive activities.[64]		Lastly, alleviation of an acute fear response because a friendly peer (or in biological language: an affiliative conspecific) tends and befriends is called "social buffering". The term is in analogy to the 1985 "buffering" hypothesis in psychology, where social support has been proven to mitigate the negative health effects of alarm pheromone mediated distress.[65] The role of a "social pheromone" is suggested by the recent discovery that olfactory signals are responsible in mediating the "social buffering" in male rats.[66] "Social buffering" was also observed to mitigate the conditioned fear responses of honeybees. A bee colony exposed to an environment of high threat of predation did not show increased aggression and aggressive-like gene expression patterns in individual bees, but decreased aggression. That the bees did not simply habituate to threats is suggested by the fact that the disturbed colonies also decreased their foraging.[67]		Biologists have proposed in 2012 that fear pheromones evolved as molecules of "keystone significance", a term coined in analogy to keystone species. Pheromones may determine species compositions, and affect rates of energy and material exchange in an ecological community. Thus pheromones generate structure in a trophic web and play critical roles in maintaining natural systems.[68]		Evidence of chemosensory alarm signals in humans has emerged slowly: Although alarm pheromones have not been physically isolated and their chemical structure has not been identified in man so far, there is evidence for their presence. Androstadienone, for example, a steroidal, endogenous odorant, is a pheromone candidate found in human sweat, axillary hair and plasma. The closely related compound androstenone is involved in communicating dominance, aggression or competition; sex hormone influences on androstenone perception in humans showed high testosterone level related to heightened androstenone sensitivity in men, a high testosterone level related to unhappiness in response to androstenone in men, and a high estradiol level related to disliking of androstenone in women.[69]		A German study from 2006 showed when anxiety-induced versus exercise-induced human sweat from a dozen people was pooled and offered to seven study participants, of five able to olfactorily distinguish exercise-induced sweat from room air, three could also distinguish exercise-induced sweat from anxiety induced sweat. The acoustic startle reflex response to a sound when sensing anxiety sweat was larger than when sensing exercise-induced sweat, as measured by electromyograph analysis of the orbital muscle, which is responsible for the eyeblink component. This showed for the first time that fear chemosignals can modulate the startle reflex in humans without emotional mediation; fear chemosignals primed the recipient's "defensive behavior" prior to the subjects' conscious attention on the acoustic startle reflex level.[70]		In analogy to the social buffering of rats and honeybees in response to chemosignals, induction of empathy by "smelling anxiety" of another person has been found in humans.[71]		A study from 2013 provided brain imaging evidence that human responses to fear chemosignals may be gender-specific. Researchers collected alarm-induced sweat and exercise-induced sweat from donors extracted it, pooled it and presented it to 16 unrelated people undergoing functional brain MRI. While stress-induced sweat from males produced a comparably strong emotional response in both females and males, stress-induced sweat from females produced a markedly stronger arousal in women than in men. Statistical tests pinpointed this gender-specificity to the right amygdala and strongest in the superficial nuclei. Since no significant differences were found in the olfactory bulb, the response to female fear-induced signals is likely based on processing the meaning, i.e. on the emotional level, rather than the strength of chemosensory cues from each gender, i.e. the perceptual level.[72]		An approach-avoidance task was set up where volunteers seeing either an angry or a happy cartoon face on a computer screen pushed away or pulled toward them a joystick as fast as possible. Volunteers smelling anandrostadienone, masked with clove oil scent responded faster, especially to angry faces, than those smelling clove oil only, which was interpreted as anandrostadienone-related activation of the fear system.[73] A potential mechanism of action is, that androstadienone alters the "emotional face processing". Androstadienone is known to influence activity of the fusiform gyrus which is relevant for face recognition.		A drug treatment for fear conditioning and phobias via the amygdala is the use of glucocorticoids.[74] In one study, glucocorticoid receptors in the central nucleus of the amygdala were disrupted in order to better understand the mechanisms of fear and fear conditioning. The glucocorticoid receptors were inhibited using lentiviral vectors containing Cre-recombinase injected into mice. Results showed that disruption of the glucocorticoid receptors prevented conditioned fear behavior. The mice were subjected to auditory cues which caused them to freeze normally. However, a reduction of freezing was observed in the mice that had inhibited glucocorticoid receptors.[75]		Cognitive behavioral therapy has been successful in helping people overcome fear. Because fear is more complex than just forgetting or deleting memories, an active and successful approach involves people repeatedly confronting their fears. By confronting their fears in a safe manner a person can suppress the fear-triggering memory or stimulus.[citation needed] Known as ‘exposure therapy’, this practice can help cure up to 90% of people, with specific phobias.[38]		The fear of the end of life and its existence is in other words the fear of death. The fear of death ritualized the lives of our ancestors. These rituals were designed to reduce that fear; they helped collect the cultural ideas that we now have in the present.[citation needed] These rituals also helped preserve the cultural ideas. The results and methods of human existence had been changing at the same time that social formation was changing. One can say[by whom?] that the formation of communities happened because people lived in fear. The result of this fear forced people to unite to fight dangers together rather than fight alone.[citation needed]		Religions are filled with different fears that humans have had throughout many centuries. The fears aren't just metaphysical (including the problems of life and death) but are also moral. Death is seen as a boundary to another world. That world would always be different depending on how each individual lived their lives. The origins of this intangible fear are not found in the present world. In a sense we can assume that fear was a big influence on things such as morality. This assumption, however, flies in the face of concepts such as moral absolutism and moral universalism – which would hold that our morals are rooted in either the divine or natural laws of the universe, and would not be generated by any human feeling, thought or emotion.[citation needed]		Fear may be politically and culturally manipulated to persuade citizenry of ideas which would otherwise be widely rejected or dissuade citizenry from ideas which would otherwise be wildly supported. In contexts of disasters, nation-states manage the fear not only to provide their citizens with an explanation about the event or blaming some minorities, but also to adjust their previous beliefs. The manipulation of fear is done by means of symbolic instruments as terror movies and the administration ideologies that lead to nationalism. After a disaster, the fear is re-channeled in a climate of euphoria based on patriotism.[76]		Fear is found and reflected in mythology and folklore as well as in works of fiction such as novels and films.		Works of dystopian and (post)apocalyptic fiction convey the fears and anxieties of societies.[77][78]		The fear of the world's end is about as old as civilization itself.[79] In a 1967 study Frank Kermode suggests that the failure of religious prophecies led to a shift in how society apprehends this ancient mode.[80] Scientific and critical thought supplanting religious and mythical thought as well as a public emancipation may be the cause of eschatology becoming replaced by more realistic scenarios. Such might constructively provoke discussion and steps to be taken to prevent depicted catastrophes.		The Story of the Youth Who Went Forth to Learn What Fear Was is a German fairy tale dealing with the topic of not knowing fear. Many stories also include characters who fear the antagonist of the plot. One important characteristic of historical and mythical heroes across cultures is to be fearless in the face of big and often lethal enemies.[citation needed]		People who have damage to the amygdala, such as from Urbach–Wiethe disease, are unable to experience fear. This is not debilitating, but a lack of fear can allow someone to get into a dangerous situation they otherwise would have avoided.[81]		
The Hero with a Thousand Faces (first published in 1949) is a work of comparative mythology by American mythologist Joseph Campbell. In this book, Campbell discusses his theory of the journey of the archetypal hero found in world mythologies.		Since publication of The Hero with a Thousand Faces, Campbell's theory has been consciously applied by a wide variety of modern writers and artists. The best known is perhaps George Lucas, who has acknowledged Campbell's influence on the Star Wars films.[1]		The Joseph Campbell Foundation and New World Library issued a new edition of The Hero with a Thousand Faces in July 2008 as part of the Collected Works of Joseph Campbell series of books, audio and video recordings. In 2011, Time placed the book in its list of the 100 best and most influential books written in English since the magazine was founded in 1923.[2]						Campbell explores the theory that important myths from around the world which have survived for thousands of years all share a fundamental structure, which Campbell called the monomyth. In a well-known quote from the introduction to The Hero with a Thousand Faces, Campbell summarized the monomyth:		A hero ventures forth from the world of common day into a region of supernatural wonder: fabulous forces are there encountered and a decisive victory is won: the hero comes back from this mysterious adventure with the power to bestow boons on his fellow man.[3]		In laying out the monomyth, Campbell describes a number of stages or steps along this journey. The hero starts in the ordinary world, and receives a call to enter an unusual world of strange powers and events (a call to adventure). If the hero accepts the call to enter this strange world, the hero must face tasks and trials (a road of trials), and may have to face these trials alone, or may have assistance. At its most intense, the hero must survive a severe challenge, often with help earned along the journey. If the hero survives, the hero may achieve a great gift (the goal or "boon"), which often results in the discovery of important self-knowledge. The hero must then decide whether to return with this boon (the return to the ordinary world), often facing challenges on the return journey. If the hero is successful in returning, the boon or gift may be used to improve the world (the application of the boon).		Very few myths contain all of these stages—some myths contain many of the stages, while others contain only a few; some myths may have as a focus only one of the stages, while other myths may deal with the stages in a somewhat different order. These stages may be organized in a number of ways, including division into three sections: Departure (sometimes called Separation), Initiation and Return. "Departure" deals with the hero venturing forth on the quest, "Initiation" deals with the hero's various adventures along the way, and "Return" deals with the hero's return home with knowledge and powers acquired on the journey.[citation needed]		The classic examples of the monomyth relied upon by Campbell and other scholars include the stories of Osiris, Prometheus, the Buddha, Moses, Mohammed, and Jesus, although Campbell cites many other classic myths from many cultures which rely upon this basic structure. The alleged similarities between these shared hero legends is one of the basic arguments of the Christ myth theory.		While Campbell offers a discussion of the hero's journey by using the Freudian concepts popular in the 1940s and 1950s, the monomythic structure is not tied to these concepts. Similarly, Campbell uses a mixture of Jungian archetypes, unconscious forces, and Arnold van Gennep's structuring of rites of passage rituals to provide some illumination.[4] However, this pattern of the hero's journey influences artists and intellectuals worldwide, suggesting a basic usefulness for Campbell's insights not tied to academic categories and mid-20th century forms of analysis.		Campbell used the work of early 20th century theorists to develop his model of the hero (see also structuralism), including Freud (particularly the Oedipus complex), Carl Jung (archetypal figures and the collective unconscious), and Arnold Van Gennep (the three stages of The Rites of Passage, translated by Campbell into Separation, Initiation and Return). Campbell also looked to the work of ethnographers James George Frazer and Franz Boas and psychologist Otto Rank.		Campbell called this journey of the hero the monomyth.[5] Campbell was a noted scholar of James Joyce (in 1944 he co-authored A Skeleton Key to Finnegans Wake with Henry Morton Robinson), and Campbell borrowed the term monomyth from Joyce's Finnegans Wake. In addition, Joyce's Ulysses was also highly influential in the structuring of The Hero with a Thousand Faces.		The book was originally published by the Bollingen Foundation through Pantheon Press as the seventeenth title in the Bollingen Series. This series was taken over by Princeton University Press, who published The Hero through 2006. Originally issued in 1949 and revised by Campbell in 1968, The Hero with a Thousand Faces has been reprinted a number of times. Reprints issued after the release of Star Wars in 1977 used the image of Mark Hamill as Luke Skywalker on the cover. Princeton University Press issued a commemorative printing of the second edition in 2004 on the occasion of the joint centennial of Campbell's birth and the Press's founding with an added foreword by Clarissa Pinkola Estés.		A third edition, compiled by the Joseph Campbell Foundation and published by New World Library, was released as the twelfth title in the Collected Works of Joseph Campbell series in July 2008.		The Hero with a Thousand Faces has been translated into over twenty languages, including Spanish, Portuguese, French, German, Italian, Japanese, Korean, Chinese (simplified and traditional), Turkish, Dutch, Greek, Danish, Norwegian, Persian, Polish, Czech, Croatian, Serbian, Slovenian, Russian, Hungarian, Bulgarian and Hebrew, and has sold well over a million copies worldwide.[6]		One of the criticisms that has been raised about the way that Campbell laid out the monomyth of the hero's journey in Hero with a Thousand Faces was that it focused on the masculine journey. Although this was not altogether true—the princess of the Grimms' "The Frog Prince" tale and the saga of the hero-goddess Inanna's descent into the underworld feature prominently in Campbell's schema—it was, nonetheless, a criticism that has been raised about the book since its publication.		Late in his life, Campbell had this to say:		In the Odyssey, you'll see three journeys. One is that of Telemachus, the son, going in quest of his father. The second is that of the father, Odysseus, becoming reconciled and related to the female principle in the sense of male-female relationship, rather than the male mastery of the female that was at the center of the Iliad. And the third is of Penelope herself, whose journey is [...] endurance. Out in Nantucket, you see all those cottages with the widow's walk up on the roof: when my husband comes back from the sea. Two journeys through space and one through time.[7]		In Pathways to Bliss: Mythology and Personal Transformation, a book drawn from Campbell's late lectures and workshops, he says about artists and the monomyth:		The artist is meant to put the objects of this world together in such a way that through them you will experience that light, that radiance which is the light of our consciousness and which all things both hide and, when properly looked upon, reveal. The hero journey is one of the universal patterns through which that radiance shows brightly. What I think is that a good life is one hero journey after another. Over and over again, you are called to the realm of adventure, you are called to new horizons. Each time, there is the same problem: do I dare? And then if you do dare, the dangers are there, and the help also, and the fulﬁllment or the ﬁasco. There's always the possibility of a ﬁasco. But there's also the possibility of bliss.		The Hero with a Thousand Faces has influenced a number of artists, filmmakers, musicians, and poets, such as Bob Dylan, George Lucas, and Jim Morrison. Additionally, Mickey Hart, Bob Weir, and Jerry Garcia of the Grateful Dead had long noted Campbell's influence and agreed to participate in a seminar with him in 1986, entitled "From Ritual to Rapture".[9]		Stanley Kubrick introduced Arthur C. Clarke to the book during the writing of 2001: A Space Odyssey.[10]		George Lucas' deliberate use of Campbell's theory of the monomyth in the making of the Star Wars movies is well documented. On the DVD release of the famous colloquy between Campbell and Bill Moyers, filmed at Lucas' Skywalker Ranch and broadcast in 1988 on PBS as The Power of Myth, Campbell and Moyers discussed Lucas's use of The Hero with a Thousand Faces in making his films.[11] Lucas himself discussed how Campbell's work affected his approach to storytelling and film-making.[12]		Jenova Chen, lead designer at thatgamecompany, also cites "The Hero's Journey" as the primary inspiration for the PlayStation 3 game Journey (2012).[13]		Mark Rosewater, head designer of the Magic: The Gathering trading card game, cites "The Hero's Journey" as a major inspiration for "The Weatherlight Saga", an epic storyarc that went from 1997 to 2001, and spanned multiple cardsets, comic books, and novels.[citation needed]		Christopher Vogler, a Hollywood film producer and writer, wrote a memo for Disney Studios on the use of The Hero with a Thousand Faces as a guide for scriptwriters; this memo influenced the creation of such films as Aladdin (1992), The Lion King (1994), and Beauty and the Beast (1991). Vogler later expanded the memo and published it as the book The Writer's Journey: Mythic Structure For Writers, which became the inspiration for a number of successful Hollywood films and is believed to have been used in the development of the Matrix series.		Novelist Richard Adams acknowledges a debt to Campbell's work, and specifically to the concept of the monomyth.[14] In his best known work, Watership Down, Adams uses extracts from The Hero with a Thousand Faces as chapter epigrams.[15]		Author Neil Gaiman, whose work is frequently seen as exemplifying the monomyth structure,[16] says that he started The Hero with a Thousand Faces but refused to finish it: "I think I got about half way through The Hero with a Thousand Faces and found myself thinking if this is true—I don't want to know. I really would rather not know this stuff. I’d rather do it because it's true and because I accidentally wind up creating something that falls into this pattern than be told what the pattern is."[17]		Many scholars and reviewers have noted how closely J. K. Rowling's popular Harry Potter books hewed to the monomyth schema.[18] To date, however, Rowling has neither confirmed that she used Campbell's work as an inspiration, nor denied that she ever read The Hero with a Thousand Faces.[citation needed]		Singer Janelle Monáe, in the liner notes of her album, The ArchAndroid (2010), cites Hero as one of her inspirations for the track "57821".[citation needed]		The Atlanta-based metal band Mastodon draw from The Hero With a Thousand Faces on their 2006 album Blood Mountain.[citation needed]		Dan Harmon, the creator of the TV show Community, has stated that he has used the monomyth as inspiration for his work.[19]		The sixth and final season of Lost recognizes Campbell's theories on the hero. During one of the bonus features, the makers of the series discuss the journey of the main characters and how each is a hero in their own way. Before each little segment of this particular feature, they quote Campbell and then expound on that particular quote by discussing the various characters.		
Adventure travel is a type of niche tourism, involving exploration of travel in an “unusual, exotic, remote, or wilderness destination.” (www.tru.ca). Travelers are highly engaged in involvement with activities that include perceived (and possibly actual) risk, and potentially requiring specialized skills and physical exertion. Adventure tourism has grown in recent decades, as tourists seek out-of-the-ordinary or "roads less traveled" types of vacations, but measurement of market size and growth is hampered by the lack of a clear operational definition. According to the U.S. based Adventure Travel Trade Association, adventure travel may be any tourist activity that includes the following three components: a physical activity, a cultural exchange and connection with nature.[1]		Thompson Rivers University describes adventure tourists as, “explorers of both an outer world, especially the unspoiled, exotic parts of our planet and an inner world of personal challenge, self perception and self mastery.” (www.tru.ca). Adventure tourists may be motivated to achieve mental states characterized as rush or flow,[2] resulting from stepping outside of their comfort zone. This may be from experiencing culture shock or through the performance of acts, that require significant effort and involve some degree of risk (real or perceived) and/or physical danger (See extreme sports). This may include activities such as mountaineering, trekking, bungee jumping, mountain biking, canoeing, scuba diving, rafting, kayaking, zip-lining, paragliding, hiking, exploring, sandboarding, caving and rock climbing.[3] Some obscure forms of adventure travel include disaster and ghetto tourism.[4] Other rising forms of adventure travel include jungle tourism.		Access to inexpensive consumer technology, with respect to Global Positioning Systems, flashpacking, social networking and photography, have increased the worldwide interest in adventure travel.[5] The interest in independent adventure travel has also increased as more specialist travel websites emerge offering previously niche locations and sports.						There is a trend for developing tourism specifically for the disabled. Adventure travel for the disabled has become a $13 billion USD a year industry in North America.[6] Some adventure travel destinations offer diverse programs and job opportunities developed specifically for the disabled.[7]		Cultural tourism is the act of travelling to a place to see that location's culture, including the lifestyle of the people in that area, the history of those people, their art, architecture, religions, and other factors that shaped their way of life.		Disaster tourism is the act of traveling to a disaster area as a matter of curiosity.[8] The behavior can be a nuisance if it hinders rescue, relief, and recovery operations. If not done because of pure curiosity, it can be cataloged as disaster learning.		Ecotourism is now defined as "responsible travel to natural areas that conserves the environment, sustains the well-being of the local people, and involves interpretation and education" (TIES, 2015). The objective of ecotourism is to protect the environment from detrimental impacts such as human traffic, and to provide educational information by promoting the unique qualities of the environment. Additionally, ecotourism, “should attempt to move Eco tourists from a passive role, where their recreation is simply based on the natural environment, to a more active role where their activities actually contribute to the health and viability of those environments.” (Orams pg. 5).		Ethno tourism refers to visiting a foreign location for the sake of observing the indigenous members of its society for the sake of non-scientific gain. Some extreme forms of this include attempting to make first contact with tribes that are protected from outside visitors.		Two controversial issues associated with ethno tourism include bringing natives into contact with diseases they do not have immunities for, and the possible degradation or destruction of a unique culture and/or language.[9]		Extreme tourism involves travel to dangerous (extreme) locations or participation in dangerous events or activities. This form of tourism can overlap with extreme sport.		Ghetto tourism includes all forms of entertainment — "gangsta rap," video games, movies, TV, and other forms that allow consumers to traffic in the inner city without leaving home.[4][10]		Jungle tourism is a rising subcategory of adventure travel defined by active multifaceted physical means of travel in the jungle regions of the earth. Although similar in many respects to adventure travel, jungle tourism pertains specifically to the context of region, culture and activity. According to the Glossary of Tourism Terms, jungle tours have become a major component of green tourism in tropical destinations and are a relatively recent phenomenon of Western international tourism.		Overland travel or overlanding refers to an "overland journey" - perhaps originating with Marco Polo's first overland expedition in the 13th century from Venice to the Mongolian court of Kublai Khan. Today overlanding is a form of extended adventure holiday, embarking on a long journey, often in a group. Overland companies provide a converted truck or a bus plus a tour leader, and the group travels together overland for a period of weeks or months.		Since the 1960s overlanding has been a popular means of travel between destinations across Africa, Europe, Asia (particularly India), the Americas and Australia. The "Hippie trail" of the 60s and 70s saw thousands of young westerners travelling through the Middle East to India and Nepal. Many of the older traditional routes are still active, along with newer routes like Iceland to South Africa overland and Central Asian post soviet states.		Urban exploration (often shortened as urbex or UE) is the examination of the normally unseen or off-limits parts of urban areas or industrial facilities. Urban exploration is also commonly referred to as infiltration, although some people consider infiltration to be more closely associated with the exploration of active or inhabited sites. It may also be referred to as "draining" (when exploring drains) "urban spelunking", "urban caving", or "building hacking".		The nature of this activity presents various risks, including both physical danger and the possibility of arrest and punishment. Many, but not all, of the activities associated with urban exploration could be considered trespassing or other violations of local or regional laws.		
Adventure films are a genre of film. They typically use their action scenes to display and explore exotic locations in an energetic way.[citation needed]						Subgenres of adventure films include swashbuckler films, survival films, Western films, pirate films, time travel films, disaster films, superhero films, road films and historical dramas.[citation needed] Main plot elements include quests for lost continents; a jungle, mountain, island, sea, wilderness, city, or desert setting; characters embarking on treasure and heroic journeys, travels and explorations for the unknown, usually also having to overcome an adversary.[citation needed] Adventure films are commonly set in a period background and may include adapted stories of historical or fictional adventure heroes within the historical context. Kings, monarchies, battles, empires, rebellion or piracy are commonly seen in adventure films.[1] Adventure films may also be combined with other movie genres such as action, science fiction, fantasy, horror or war.[citation needed]		Adventure film popularity peaked in the 1930s and 1940s, when films such as Captain Blood, The Adventures of Robin Hood and The Mark of Zorro were regularly made with major stars, notably Errol Flynn and Tyrone Power, who were closely associated with the genre.[citation needed] Saturday morning serials used many of the same thematic elements as high-budget adventure films.[citation needed]		In the early days of adventure films, the protagonists were mainly male. These heroes were courageous, often fighting suppression and facing tyrants. Recent adventure films have featured heroines, such as Lara Croft, as protagonists.[1]		Adventure films can contain stock characters and stereotypes. In some cases, this has been accused of going as far as implicit racism; claimed examples of this are Indiana Jones and the Temple of Doom, First Blood and James Bond "kicking third-world people around" in Dr. No.[2][page needed][3][page needed]		
Helen Adams Keller (June 27, 1880 – June 1, 1968) was an American author, political activist, and lecturer. She was the first deaf-blind person to earn a bachelor of arts degree. The story of how Keller's teacher, Anne Sullivan, broke through the isolation imposed by a near complete lack of language, allowing the girl to blossom as she learned to communicate, has become widely known through the dramatic depictions of the play and film The Miracle Worker. Her birthplace in West Tuscumbia, Alabama, is now a museum[1] and sponsors an annual "Helen Keller Day". Her birthday on June 27 is commemorated as Helen Keller Day in the U.S. state of Pennsylvania and was authorized at the federal level by presidential proclamation by President Jimmy Carter in 1980, the 100th anniversary of her birth.		A prolific author, Keller was well-traveled and outspoken in her convictions. A member of the Socialist Party of America and the Industrial Workers of the World, she campaigned for women's suffrage, labor rights, socialism, antimilitarism, and other similar causes. She was inducted into the Alabama Women's Hall of Fame in 1971[2] and was one of twelve inaugural inductees to the Alabama Writers Hall of Fame on June 8, 2015.[3] Keller proved to the world that deaf people could all learn to communicate and that they could survive in the hearing world. She also taught that deaf people are capable of doing things that hearing people can do. One of the most famous deaf people in history, she is an idol to many deaf people in the world.[4]						Helen Adams Keller was born on June 27, 1880, in Tuscumbia, Alabama.[5] Her family lived on a homestead, Ivy Green,[1] that Helen's grandfather had built decades earlier.[6] She had two siblings, Mildred Campbell and Phillip Brooks Keller, and two older half-brothers from her father's prior marriage, James and William Simpson Keller.[7][8]		Her father, Arthur H. Keller,[9] spent many years as an editor for the Tuscumbia North Alabamian, and had served as a captain for the Confederate Army.[5][6] Her paternal grandmother was second cousins with Robert E. Lee.[10] Her mother, Kate Adams,[11] was the daughter of Charles W. Adams, a Confederate general.[12] Though originally from Massachusetts, Charles Adams also fought for the Confederate Army during the American Civil War, earning the rank of colonel (and acting brigadier-general). Her paternal lineage was traced to Casper Keller, a native of Switzerland.[10][13] One of Helen's Swiss ancestors was the first teacher for the deaf in Zurich. Keller reflected on this coincidence in her first autobiography, stating "that there is no king who has not had a slave among his ancestors, and no slave who has not had a king among his."[10]		Helen Keller was born with the ability to see and hear. At 19 months old, she contracted an unknown illness described by doctors as "an acute congestion of the stomach and the brain",[14] which might have been scarlet fever or meningitis.[5][15] The illness left her both deaf and blind. At that time, she was able to communicate somewhat with Martha Washington, the six-year-old daughter of the family cook, who understood her signs;[16]:11 by the age of seven, Keller had more than 60 home signs to communicate with her family. Even though blind and deaf, Helen Keller had passed through many obstacles and she learned to live with her disabilities. She learned how to tell which person was walking by from the vibrations their footsteps would make. The sex and age of the person could be identified by how strong and continuous the steps were.[17]		In 1886, Keller's mother, inspired by an account in Charles Dickens' American Notes of the successful education of another deaf and blind woman, Laura Bridgman, dispatched the young Keller, accompanied by her father, to seek out physician J. Julian Chisolm, an eye, ear, nose, and throat specialist in Baltimore, for advice.[18] Chisholm referred the Kellers to Alexander Graham Bell, who was working with deaf children at the time. Bell advised them to contact the Perkins Institute for the Blind, the school where Bridgman had been educated, which was then located in South Boston. Michael Anagnos, the school's director, asked 20-year-old former student Anne Sullivan, herself visually impaired, to become Keller's instructor. It was the beginning of a 49-year-long relationship during which Sullivan evolved into Keller's governess and eventually her companion.[16]:Introduction, Key Figures		Sullivan arrived at Keller's house in March 1887, and immediately began to teach Helen to communicate by spelling words into her hand, beginning with "d-o-l-l" for the doll that she had brought Keller as a present. Keller was frustrated, at first, because she did not understand that every object had a word uniquely identifying it. In fact, when Sullivan was trying to teach Keller the word for "mug", Keller became so frustrated she broke the mug.[19] Keller's breakthrough in communication came the next month, when she realized that the motions her teacher was making on the palm of her hand, while running cool water over her other hand, symbolized the idea of "water"; she then nearly exhausted Sullivan demanding the names of all the other familiar objects in her world.		Helen Keller was viewed as isolated, but was very in touch with the outside world. She was able to enjoy music by feeling the beat and she was able to have a strong connection with animals through touch. She was delayed at picking up language, but that did not stop her from having a voice.[20]		In May 1888, Keller started attending the Perkins Institute for the Blind. In 1894, Keller and Sullivan moved to New York to attend the Wright-Humason School for the Deaf, and to learn from Sarah Fuller at the Horace Mann School for the Deaf. In 1896, they returned to Massachusetts, and Keller entered The Cambridge School for Young Ladies before gaining admittance, in 1900, to Radcliffe College,[21] where she lived in Briggs Hall, South House. Her admirer, Mark Twain, had introduced her to Standard Oil magnate Henry Huttleston Rogers, who, with his wife Abbie, paid for her education. In 1904, at the age of 24, Keller graduated from Radcliffe, becoming the first deaf blind person to earn a Bachelor of Arts degree. She maintained a correspondence with the Austrian philosopher and pedagogue Wilhelm Jerusalem, who was one of the first to discover her literary talent.[22]		Determined to communicate with others as conventionally as possible, Keller learned to speak, and spent much of her life giving speeches and lectures on aspects of her life. She learned to "hear" people's speech by reading their lips with her hands—her sense of touch had heightened. She became proficient at using braille[23] and reading sign language with her hands as well. Shortly before World War I, with the assistance of the Zoellner Quartet, she determined that by placing her fingertips on a resonant tabletop she could experience music played close by.[24]		On January 22, 1916, Keller and Sullivan traveled to the small town of Menomonie in western Wisconsin to deliver a lecture at the Mabel Tainter Memorial Building. Details of her talk were provided in the weekly Dunn County News on January 22, 1916:		A message of optimism, of hope, of good cheer, and of loving service was brought to Menomonie Saturday—a message that will linger long with those fortunate enough to have received it. This message came with the visit of Helen Keller and her teacher, Mrs. John Macy, and both had a hand in imparting it Saturday evening to a splendid audience that filled The Memorial. The wonderful girl who has so brilliantly triumphed over the triple afflictions of blindness, dumbness and deafness, gave a talk with her own lips on "Happiness," and it will be remembered always as a piece of inspired teaching by those who heard it.		When part of the account was reprinted in the January 20, 2016, edition of the paper under the heading "From the Files", the column compiler added		According to those who attended, Helen Keller spoke of the joy that life gave her. She was thankful for the faculties and abilities that she did possess and stated that the most productive pleasures she had were curiosity and imagination. Keller also spoke of the joy of service and the happiness that came from doing things for others ... Keller imparted that "helping your fellow men were one's only excuse for being in this world and in the doing of things to help one's fellows lay the secret of lasting happiness." She also told of the joys of loving work and accomplishment and the happiness of achievement. Although the entire lecture lasted only a little over an hour, the lecture had a profound impact on the audience.[25]		Anne Sullivan stayed as a companion to Helen Keller long after she taught her. Sullivan married John Macy in 1905, and her health started failing around 1914. Polly Thomson was hired to keep house. She was a young woman from Scotland who had no experience with deaf or blind people. She progressed to working as a secretary as well, and eventually became a constant companion to Keller.[26]		Keller moved to Forest Hills, Queens, together with Sullivan and Macy, and used the house as a base for her efforts on behalf of the American Foundation for the Blind.[27] "While in her thirties Helen had a love affair, became secretly engaged, and defied her teacher and family by attempting an elopement with the man she loved."[28] He was "Peter Fagan, a young Boston Herald reporter who was sent to Helen's home to act as her private secretary when lifelong companion, Anne, fell ill."[29]		Anne Sullivan died in 1936 after a coma as a result of coronary thrombosis,[30]:266 with Keller holding her hand.[31]:255 Keller and Thomson moved to Connecticut. They traveled worldwide and raised funds for the blind. Thomson had a stroke in 1957 from which she never fully recovered, and died in 1960. Winnie Corbally, a nurse whom they originally hired to care for Thomson in 1957, stayed on after her death and was Keller's companion for the rest of her life.[27]		Keller went on to become a world-famous speaker and author. She is remembered as an advocate for people with disabilities, amid numerous other causes. The Deaf community was widely impacted by her. She traveled to twenty-five different countries giving motivational speeches about Deaf people's conditions.[33] She was a suffragette, pacifist, radical socialist, birth control supporter, and opponent of Woodrow Wilson. In 1915 she and George A. Kessler founded the Helen Keller International (HKI) organization. This organization is devoted to research in vision, health and nutrition. In 1920, she helped to found the American Civil Liberties Union (ACLU). Keller traveled to over 40 countries with Sullivan, making several trips to Japan and becoming a favorite of the Japanese people. Keller met every U.S. President from Grover Cleveland to Lyndon B. Johnson and was friends with many famous figures, including Alexander Graham Bell, Charlie Chaplin and Mark Twain. Keller and Twain were both considered radicals at the beginning of the 20th century, and as a consequence, their political views have been forgotten or glossed over in the popular mind.[34]		Keller was a member of the Socialist Party and actively campaigned and wrote in support of the working class from 1909 to 1921. Many of her speeches and writings were about women's right to vote and the impacts of war. She had speech therapy in order to have her voice heard better by the public. When the Rockefeller-owned press refused to print her articles, she protested until her work was finally published.[30] She supported Socialist Party candidate Eugene V. Debs in each of his campaigns for the presidency. Before reading Progress and Poverty, Helen Keller was already a socialist who believed that Georgism was a good step in the right direction.[35] She later wrote of finding "in Henry George's philosophy a rare beauty and power of inspiration, and a splendid faith in the essential nobility of human nature."[36]		Keller claimed that newspaper columnists who had praised her courage and intelligence before she expressed her socialist views now called attention to her disabilities. The editor of the Brooklyn Eagle wrote that her "mistakes sprung out of the manifest limitations of her development." Keller responded to that editor, referring to having met him before he knew of her political views:		At that time the compliments he paid me were so generous that I blush to remember them. But now that I have come out for socialism he reminds me and the public that I am blind and deaf and especially liable to error. I must have shrunk in intelligence during the years since I met him. ... Oh, ridiculous Brooklyn Eagle! Socially blind and deaf, it defends an intolerable system, a system that is the cause of much of the physical blindness and deafness which we are trying to prevent.[37]		Keller joined the Industrial Workers of the World (the IWW, known as the Wobblies) in 1912,[34] saying that parliamentary socialism was "sinking in the political bog". She wrote for the IWW between 1916 and 1918. In Why I Became an IWW,[38] Keller explained that her motivation for activism came in part from her concern about blindness and other disabilities:		I was appointed on a commission to investigate the conditions of the blind. For the first time I, who had thought blindness a misfortune beyond human control, found that too much of it was traceable to wrong industrial conditions, often caused by the selfishness and greed of employers. And the social evil contributed its share. I found that poverty drove women to a life of shame that ended in blindness.		The last sentence refers to prostitution and syphilis, the former a frequent cause of the latter, and the latter a leading cause of blindness. In the same interview, Keller also cited the 1912 strike of textile workers in Lawrence, Massachusetts for instigating her support of socialism.		Like Alexander Graham Bell and others, Keller supported eugenics. In 1915 she wrote in favor of refusing life-saving medical procedures to infants with severe mental impairments or physical deformities, stating that their lives were not worthwhile and they would likely become criminals.[39][40] Keller also expressed concerns about human overpopulation.[41][42]		Keller wrote a total of 12 published books and several articles.		One of her earliest pieces of writing, at age 11, was The Frost King (1891). There were allegations that this story had been plagiarized from The Frost Fairies by Margaret Canby. An investigation into the matter revealed that Keller may have experienced a case of cryptomnesia, which was that she had Canby's story read to her but forgot about it, while the memory remained in her subconscious.[27]		At age 22, Keller published her autobiography, The Story of My Life (1903), with help from Sullivan and Sullivan's husband, John Macy. It recounts the story of her life up to age 21 and was written during her time in college.		Keller wrote The World I Live In in 1908, giving readers an insight into how she felt about the world.[43] Out of the Dark, a series of essays on socialism, was published in 1913.		When Keller was young, Anne Sullivan introduced her to Phillips Brooks, who introduced her to Christianity, Keller famously saying: "I always knew He was there, but I didn't know His name!"[44][45][46]		Her spiritual autobiography, My Religion,[47] was published in 1927 and then in 1994 extensively revised and re-issued under the title Light in My Darkness. It advocates the teachings of Emanuel Swedenborg, the Christian revelator and theologian who gives a spiritual interpretation of the teachings of the Bible and who claims that the second coming of Jesus Christ has already taken place. Adherents use several names to describe themselves, including Second Advent Christian, Swedenborgian, and New Church.		Keller described the progressive views of her belief in these words:		But in Swedenborg's teaching it [Divine Providence] is shown to be the government of God's Love and Wisdom and the creation of uses. Since His Life cannot be less in one being than another, or His Love manifested less fully in one thing than another, His Providence must needs be universal ... He has provided religion of some kind everywhere, and it does not matter to what race or creed anyone belongs if he is faithful to his ideals of right living.[47]		Keller suffered a series of strokes in 1961 and spent the last years of her life at her home.[27]		On September 14, 1964, President Lyndon B. Johnson awarded her the Presidential Medal of Freedom, one of the United States' two highest civilian honors. In 1965 she was elected to the National Women's Hall of Fame at the New York World's Fair.[27]		Keller devoted much of her later life to raising funds for the American Foundation for the Blind. She died in her sleep on June 1, 1968, at her home, Arcan Ridge, located in Easton, Connecticut, a few weeks short of her eighty-eighth birthday. A service was held in her honor at the National Cathedral in Washington, D.C., her body was cremated and her ashes were placed there next to her constant companions, Anne Sullivan and Polly Thomson. She was buried at the Washington National Cathedral.[48]		Keller's life has been interpreted many times. She appeared in a silent film, Deliverance (1919), which told her story in a melodramatic, allegorical style.[49]		She was also the subject of the documentaries Helen Keller in Her Story, narrated by Katharine Cornell, and The Story of Helen Keller, part of the Famous Americans series produced by Hearst Entertainment.		The Miracle Worker is a cycle of dramatic works ultimately derived from her autobiography, The Story of My Life. The various dramas each describe the relationship between Keller and Sullivan, depicting how the teacher led her from a state of almost feral wildness into education, activism, and intellectual celebrity. The common title of the cycle echoes Mark Twain's description of Sullivan as a "miracle worker." Its first realization was the 1957 Playhouse 90 teleplay of that title by William Gibson. He adapted it for a Broadway production in 1959 and an Oscar-winning feature film in 1962, starring Anne Bancroft and Patty Duke. It was remade for television in 1979 and 2000.		In 1984, Keller's life story was made into a TV movie called The Miracle Continues.[50] This film that entailed the semi-sequel to The Miracle Worker recounts her college years and her early adult life. None of the early movies hint at the social activism that would become the hallmark of Keller's later life, although a Disney version produced in 2000 states in the credits that she became an activist for social equality.		The Bollywood movie Black (2005) was largely based on Keller's story, from her childhood to her graduation.[51]		A documentary called Shining Soul: Helen Keller's Spiritual Life and Legacy was produced by the Swedenborg Foundation in the same year. The film focuses on the role played by Emanuel Swedenborg's spiritual theology in her life and how it inspired Keller's triumph over her triple disabilities of blindness, deafness and a severe speech impediment.[citation needed]		On March 6, 2008, the New England Historic Genealogical Society announced that a staff member had discovered a rare 1888 photograph showing Helen and Anne, which, although previously published, had escaped widespread attention.[52] Depicting Helen holding one of her many dolls, it is believed to be the earliest surviving photograph of Anne Sullivan Macy.[53]		Video footage showing Helen Keller learning to mimic speech sounds also exists.[54]		A biography of Helen Keller was written by the German Jewish author H.J.Kaeser.		A 10-by-7-foot painting titled The Advocate: Tribute to Helen Keller was created by three artists from Kerala as a tribute to Helen Keller. The Painting was created in association with a non-profit organization Art d'Hope Foundation, artists groups Palette People and XakBoX Design & Art Studio.[55] This painting was created for a fundraising event to help blind students in India [56] and was inaugurated by M. G. Rajamanikyam, IAS (District Collector Ernakulam) on Helen Keller day (June 27, 2016).[57] The painting depicts the major events of Helen Keller's life and is one of the biggest paintings done based on Helen Keller's life.		A preschool for the deaf and hard of hearing in Mysore, India, was originally named after Helen Keller by its founder, K. K. Srinivasan. In 1999, Keller was listed in Gallup's Most Widely Admired People of the 20th century.		In 2003, Alabama honored its native daughter on its state quarter.[58] The Alabama state quarter is the only circulating US coin to feature braille.[59]		The Helen Keller Hospital in Sheffield, Alabama, is dedicated to her.[60]		Streets are named after Helen Keller in Zürich, Switzerland, in the USA, in Getafe, Spain, in Lod, Israel,[61] in Lisbon, Portugal[62] and in Caen, France.		A stamp was issued in 1980 by the United States Postal Service depicting Keller and Sullivan, to mark the centennial of Keller's birth.		On October 7, 2009, a bronze statue of Helen Keller was added to the National Statuary Hall Collection, as a replacement for the State of Alabama's former 1908 statue of the education reformer Jabez Lamar Monroe Curry. It is displayed in the United States Capitol Visitor Center and depicts Keller as a seven-year-old child standing at a water pump. The statue represents the seminal moment in Keller's life when she understood her first word: W-A-T-E-R, as signed into her hand by teacher Anne Sullivan. The pedestal base bears a quotation in raised Latin and braille letters: "The best and most beautiful things in the world cannot be seen or even touched, they must be felt with the heart."[63] The statue is the first one of a person with a disability and of a child to be permanently displayed at the U.S. Capitol.[64][65][66]		Archival material of Helen Keller stored in New York was lost when the Twin Towers were destroyed in the September 11 attacks.[67][68][69]		The Helen Keller Archives are owned by the American Foundation for the Blind.[70]		
Thomas Kennerly "Tom" Wolfe Jr. (born March 2, 1931)[1] is an American author and journalist, best known for his association with and influence over the New Journalism literary movement, in which literary techniques are used extensively and traditional values of journalistic objectivity and evenhandedness are rejected. He began his career as a regional newspaper reporter in the 1950s, but achieved national prominence in the 1960s following the publication of such best-selling books as The Electric Kool-Aid Acid Test (a highly experimental account of Ken Kesey and the Merry Pranksters), and two collections of articles and essays, Radical Chic & Mau-Mauing the Flak Catchers and The Kandy-Kolored Tangerine-Flake Streamline Baby. His first novel, The Bonfire of the Vanities, published in 1987, was met with critical acclaim, became a commercial success, and was adapted as a major motion picture (directed by Brian De Palma).						Wolfe was born in Richmond, Virginia, the son of Louise (née Agnew), a landscape designer, and Thomas Kennerly Wolfe Sr., an agronomist.[2][3]		Wolfe grew up on Gloucester Road in the historic Richmond North Side neighborhood of Sherwood Park. He recounts some of his childhood memories of growing up there in a foreword to a book about the nearby historic Ginter Park neighborhood.		Wolfe was student council president, editor of the school newspaper and a star baseball player at St. Christopher's School, an Episcopal all-boys school in Richmond, Virginia.		Upon graduation in 1947, he turned down admission to Princeton University to attend Washington and Lee University, both all-male schools at the time; at Washington and Lee, Wolfe was a member of the Phi Kappa Sigma fraternity. Wolfe majored in English and practiced his writing outside the classroom as well. He was the sports editor of the college newspaper and helped found a literary magazine, Shenandoah. Of particular influence was his professor Marshall Fishwick, a teacher of American studies educated at Yale. More in the tradition of anthropology than literary scholarship, Fishwick taught his classes to look at the whole of a culture, including those elements considered profane. The very title of Wolfe's undergraduate thesis, "A Zoo Full of Zebras: Anti-Intellectualism in America," evinced his fondness for words and aspirations toward cultural criticism. Wolfe graduated cum laude in 1951.		Wolfe had continued playing baseball as a pitcher and had begun to play semi-professionally while still in college. In 1952 he earned a tryout with the New York Giants but was cut after three days, which Wolfe blamed on his inability to throw good fastballs. Wolfe abandoned baseball and instead followed his professor Fishwick's example, enrolling in Yale University's American studies doctoral program. His PhD thesis was titled The League of American Writers: Communist Organizational Activity Among American Writers, 1929–1942.[4] In the course of his research, Wolfe interviewed many writers, including Malcolm Cowley, Archibald MacLeish, and James T. Farrell.[5] A biographer remarked on the thesis: "Reading it, one sees what has been the most baleful influence of graduate education on many who have suffered through it: it deadens all sense of style."[6] His thesis was originally rejected but it was finally accepted after he rewrote it in an objective rather than a subjective style. Upon leaving Yale he wrote a friend explaining through expletives his personal opinions about his thesis.		Though Wolfe was offered teaching jobs in academia, he opted to work as a reporter. In 1956, while still preparing his thesis, Wolfe became a reporter for the Springfield Union in Springfield, Massachusetts. Wolfe finished his thesis in 1957 and in 1959 was hired by The Washington Post. Wolfe has said that part of the reason he was hired by the Post was his lack of interest in politics. The Post's city editor was "amazed that Wolfe preferred cityside to Capitol Hill, the beat every reporter wanted." He won an award from The Newspaper Guild for foreign reporting in Cuba in 1961 and also won the Guild's award for humor. While there, he experimented with fiction-writing techniques in feature stories.[7]		In 1962, Wolfe left Washington for New York City, taking a position with the New York Herald Tribune as a general assignment reporter and feature writer. The editors of the Herald Tribune, including Clay Felker of the Sunday section supplement New York magazine, encouraged their writers to break the conventions of newspaper writing.[8] During the 1962 New York City newspaper strike, Wolfe approached Esquire magazine about an article on the hot rod and custom car culture of Southern California. He struggled with the article until finally a desperate editor, Byron Dobell, suggested that Wolfe send him his notes so they could piece the story together.		Wolfe procrastinated until, on the evening before the article was due, he typed a letter to Dobell explaining what he wanted to say on the subject, ignoring all journalistic conventions. Dobell's response was to remove the salutation "Dear Byron" from the top of the letter and publish it intact as reportage. The result, published in 1963, was "There Goes (Varoom! Varoom!) That Kandy-Kolored Tangerine-Flake Streamline Baby." The article was widely discussed—loved by some, hated by others—and helped Wolfe publish his first book, The Kandy-Kolored Tangerine-Flake Streamline Baby, a collection of his writings in the Herald-Tribune, Esquire, and other publications.[9]		This was what Wolfe called New Journalism, in which some journalists and essayists experimented with a variety of literary techniques, mixing them with the traditional ideal of dispassionate, even-handed reporting. More specifically, Wolfe experimented with four literary devices not normally associated with feature writing—scene-by-scene construction, extensive dialogue, multiple points of view, and detailed description of one's status-life symbols (the materialistic choices one makes)—to produce this stylized form of journalism, which would later be commonly referred to as literary journalism.[10] Of status symbols, Wolfe has said, "I think every living moment of a human being’s life, unless the person is starving or in immediate danger of death in some other way, is controlled by a concern for status."[11]		Wolfe also championed what he called “saturation reporting,” a reportorial approach where the journalist “shadows” and observes the subject over an extended period of time. “To pull it off,” says Wolfe, “you casually have to stay with the people you are writing about for long stretches . . . long enough so that you are actually there when revealing scenes take place in their lives.”[12] Saturation reporting differs from “in-depth” and “investigative” reporting, which involve the direct interviewing of numerous sources and/or the extensive analyzing of external documents relating to the story. Saturation reporting, according to communication professor Richard Kallan, “entails a more complex set of relationships wherein the journalist becomes an involved, more fully reactive witness, no longer distanced and detached from the people and events reported.”[13]		One of the most striking examples of New Journalism is Wolfe's The Electric Kool-Aid Acid Test. The book, an account of the adventures of the Merry Pranksters, a famous sixties counter-culture group, was highly experimental in its use of onomatopoeia, free association, and eccentric punctuation—such as multiple exclamation marks and italics—to convey the manic ideas and personalities of Ken Kesey and his followers.		In addition to his own forays into this new style of journalism, Wolfe edited a collection of New Journalism with E.W. Johnson, published in 1973 and titled The New Journalism. This book brought together pieces from Truman Capote, Hunter S. Thompson, Norman Mailer, Gay Talese, Joan Didion, and several other well-known writers with the common theme of journalism that incorporated literary techniques and that could be considered literature.[14]		In 1965, a collection of his articles in this style was published under the title The Kandy-Kolored Tangerine-Flake Streamline Baby, and Wolfe's fame grew. A second volume of articles, The Pump House Gang, followed in 1968. Wolfe wrote on popular culture, architecture, politics, and other topics that underscored, among other things, how American life in the 1960s had been transformed by post-WWII economic prosperity. His defining work from this era is The Electric Kool-Aid Acid Test (published the same day as The Pump House Gang in 1968), which for many epitomized the 1960s. Although a conservative in many ways and certainly not a hippie (in 2008, he claimed never to have used LSD and to have tried marijuana only once[15]) Wolfe became one of the notable figures of the decade.		In 1970, he published two essays in book form as Radical Chic & Mau-Mauing the Flak Catchers: "Radical Chic," a biting account of a party given by Leonard Bernstein to raise money for the Black Panther Party, and "Mau-Mauing The Flak Catchers," about the practice of using racial intimidation ("mau-mauing") to extract funds from government welfare bureaucrats ("flak catchers"). The phrase "radical chic" soon became a popular derogatory term for upper-class leftism. Published in 1977, Mauve Gloves & Madmen, Clutter & Vine included one of Wolfe's more famous essays, "The Me Decade and the Third Great Awakening."		In 1979, Wolfe published The Right Stuff, an account of the pilots who became America's first astronauts. Famously following their training and unofficial, even foolhardy, exploits, he likened these heroes to "single combat champions" of a bygone era, going forth to battle in the space race on behalf of their country. In 1983, the book was adapted as a successful feature film.		In 2016 Wolfe published The Kingdom of Speech, which is a controversial[16] critique of Charles Darwin and Noam Chomsky.[17]		Wolfe also wrote two highly skeptical social histories of modern art and modern architecture, The Painted Word and From Bauhaus to Our House, in 1975 and 1981, respectively. The Painted Word mocked the excessive insularity of the art world and its dependence on what he saw as faddish critical theory, while From Bauhaus to Our House explored the negative effects of the Bauhaus style on the evolution of modern architecture.[18]		A fictional television movie appeared on PBS in 1977, "Tom Wolfe's Los Angeles", a suitably satirical story set in Los Angeles. Wolfe appears in the movie himself.[19][20]		Throughout his early career, Wolfe had planned to write a novel that would capture the wide spectrum of American society. Among his models was William Makepeace Thackeray's Vanity Fair, which described the society of 19th century England. Wolfe remained occupied writing nonfiction books and contributing to Harper's until 1981, when he ceased his other work to concentrate on the novel.		Wolfe began researching the novel by observing cases at the Manhattan Criminal Court and shadowing members of the Bronx homicide squad. While the research came easily, the writing did not immediately follow. To overcome his writer's block, Wolfe wrote to Jann Wenner, editor of Rolling Stone, to propose an idea drawn from Charles Dickens and Thackeray. The Victorian novelists that Wolfe viewed as his models had often written their novels in serial installments. Wenner offered Wolfe around $200,000 to serialize his work.[21] The deadline pressure gave him the motivation he had hoped for, and from July 1984 to August 1985 each biweekly issue of Rolling Stone contained a new installment. Wolfe was later not happy with his "very public first draft"[22] and thoroughly revised his work. Even Sherman McCoy, the novel's central character, changed: originally a writer, the book version cast McCoy as a bond salesman. Wolfe researched and revised for two years, and his The Bonfire of the Vanities was published in 1987. The book was a commercial and critical success, spending weeks on bestseller lists and earning praise from much of the literary establishment on which Wolfe had long heaped scorn.[23]		Because of the success of Wolfe's first novel, there was widespread interest in his second. This novel took him more than 11 years to complete; A Man in Full was published in 1998. The book's reception was not universally favorable, though it received glowing reviews in Time, Newsweek, The Wall Street Journal and elsewhere. An enormous initial printing of 1.2 million copies was announced and the book stayed at number one on the New York Times bestseller list for ten weeks. John Updike wrote a critical review for The New Yorker complaining that the novel "amounts to entertainment, not literature, even literature in a modest aspirant form." This touched off an intense war of words in the print and broadcast media between Wolfe and Updike, John Irving, and Norman Mailer. In 2001, Wolfe published an essay referring to these three authors as "My Three Stooges."		After publishing Hooking Up (a collection of short pieces, including the 1997 novella Ambush at Fort Bragg) in 2001, he followed up with his third novel, I Am Charlotte Simmons (2004), which chronicles the decline of a poor, bright scholarship student from Alleghany County, North Carolina, in the context of snobbery, materialism, institutionalised anti-intellectualism and sexual promiscuity she finds at a prestigious contemporary American university. The novel met with a mostly tepid response by critics but won praise from many social conservatives, who saw the book's account of college sexuality as revealing of a disturbing moral decline. The novel won a Bad Sex in Fiction Award from the London-based Literary Review, a prize established "to draw attention to the crude, tasteless, often perfunctory use of redundant passages of sexual description in the modern novel". Wolfe later explained that such sexual references were deliberately clinical.		Wolfe has written that his goal in writing fiction is to document contemporary society in the tradition of John Steinbeck, Charles Dickens, and Émile Zola.		In early 2008, it was announced that Wolfe was leaving his longtime publisher, Farrar, Straus and Giroux. His fourth novel, Back to Blood, was published in October 2012 by Little, Brown. According to The New York Times, Wolfe was paid close to US$7 million for the book.[24] According to the publisher, Back to Blood is about "class, family, wealth, race, crime, sex, corruption and ambition in Miami, the city where America's future has arrived first."[25]		Several themes are present in much of Wolfe's writing, including his novels. One such theme is male power-jockeying, which is a major part of The Bonfire of the Vanities, A Man in Full, and I Am Charlotte Simmons as well as several of his journalistic pieces. Male characters in his fiction often suffer from feelings of extreme inadequacy or hugely inflated egos, sometimes alternating between both. He satirizes racial politics, most commonly between whites and blacks; he also highlights class divisions between characters. Men's fashions often play a large part in his stories, being used to indicate economic status. Much of his recent work also addresses neuroscience, a subject which he admitted a fascination with in "Sorry, Your Soul Just Died," one of the essays in Hooking Up, and which played a large role in I Am Charlotte Simmons—the title character being a student of neuroscience, and characters' thought processes, such as fear, humiliation and lust, frequently being described in the terminology of brain chemistry. Wolfe also frequently gives detailed descriptions of various aspects of his characters' anatomies.[26]		Two of his novels (A Man in Full and I Am Charlotte Simmons) feature major characters (Conrad Hensley and Jojo Johanssen, respectively) who are set on paths to self-discovery by reading classical Roman and Greek philosophy.		Law and banking firms in Wolfe's writing often have satirical names formed by the surnames of the partners. "Dunning, Sponget and Leach" and "Curry, Goad and Pesterall" appear in The Bonfire of the Vanities, and "Wringer, Fleasom and Tick" in A Man in Full. Ambush at Fort Bragg contains a law firm called "Crotalus, Adder, Cobran and Krate" (all names or homophones of venomous snakes).		Some characters appear in multiple novels, creating a sense of a "universe" that is continuous throughout Wolfe's fiction. The character of Freddy Button, a lawyer from Bonfire of the Vanities, is mentioned briefly in I Am Charlotte Simmons. A character named Ronald Vine, an interior decorator who is mentioned in The Bonfire of the Vanities, reappears in A Man in Full as the designer of Charlie Croker's home.		A fictional sexual practice called "that thing with the cup" appears in several of his writings, including The Bonfire of the Vanities, A Man in Full and a (non-fiction) essay in Hooking Up.		The surname "Bolka" appears in three Wolfe novels—as the name of a rendering plant in A Man in Full, as a partner in an accounting firm in Bonfire of the Vanities, and as a college lacrosse player from the Balkans in I Am Charlotte Simmons.		Wolfe adopted the white suit as a trademark in 1962. He bought his first white suit planning to wear it in the summer in the style of Southern gentlemen. However, he found that the suit he purchased was too heavy for summer use, so he wore it in winter, which created a sensation.[27] Wolfe has maintained this uniform ever since, sometimes worn with a matching white tie, white homburg hat, and two-tone shoes. Wolfe has said that the outfit disarms the people he observes, making him, in their eyes, "a man from Mars, the man who didn't know anything and was eager to know."[28]		In 1989, Wolfe wrote an essay for Harper's Magazine titled "Stalking the Billion-Footed Beast", which criticized modern American novelists for failing to engage fully with their subjects, and suggested that modern literature could be saved by a greater reliance on journalistic technique. This attack on the mainstream literary establishment was interpreted as a boast that Wolfe's work was superior to more highly regarded authors.[29]		Wolfe was a supporter of George W. Bush and said he voted for him for president in 2004 because of what he called Bush's "great decisiveness and willingness to fight." (Bush apparently reciprocates the admiration, having read all of Wolfe's books, according to friends in 2005.[30]) After this fact emerged in a New York Times interview, Wolfe said that the reaction in the literary world was as if he had said, "I forgot to tell you—I'm a child molester." Because of this incident, he sometimes wears an American flag pin on his suit, which he compared to "holding up a cross to werewolves."[31]		Wolfe's views and choice of subject material, such as mocking left-wing intellectuals in Radical Chic and glorifying astronauts in The Right Stuff, have sometimes led to his being labeled conservative,[32] and his depiction of the Black Panther Party in Radical Chic led to a member of the party calling him a racist.[33] Wolfe rejects such labels; in a 2004 interview, he said that his "idol" in writing about society and culture is Émile Zola, who, in Wolfe's words, was "a man of the left" but "went out, and found a lot of ambitious, drunk, slothful and mean people out there. Zola simply could not—and was not interested in—telling a lie."[32]		Asked to comment by the Wall Street Journal on blogs in 2007 to mark the tenth anniversary of their advent, Wolfe wrote that "the universe of blogs is a universe of rumors" and that "blogs are an advance guard to the rear." He also took the opportunity to criticize Wikipedia, saying that "only a primitive would believe a word of" it. He noted a story about him in his Wikipedia entry at the time, which he said had never happened.[34]		Wolfe lives in New York City with his wife Sheila, who designs covers for Harper's magazine. They have two children, a daughter, Alexandra, and a son, Tommy.[35]		A writer for Examiner Magazine who interviewed Wolfe in 1998 said, "He has no computer and does not surf, or even know how to use, the Internet", adding, however, that Wolfe's novel A Man in Full does have a subplot involving "a muckraking cyber-gossip site, à la the Drudge Report or Salon."[35]		Wolfe is credited with introducing the terms "statusphere," "the right stuff," "radical chic," "the Me Decade," "social x-ray," and "pushing the envelope" into the English lexicon.[36][dubious – discuss] He is sometimes credited with inventing the term "trophy wife" as well, but this is incorrect: he described emaciated wives as "X-rays" in his novel The Bonfire of the Vanities but did not use the term "trophy wife".[37] According to journalism professor Ben Yagoda, Wolfe is also responsible for the use of the present tense in magazine profile pieces; before he began doing so in the early 1960s, profile articles had always been written in the past tense.[38]		
Gold digging is a type of transactional relationship[1] in which people engage in romantic relationships for money rather than love. When it turns into marriage it is a type of marriage of convenience.		Peggy Hopkins Joyce was in the 1920s considered the perfect example of a gold digger,[2] with some claims existing that the term was even coined to describe her.[3]		A popular association between chorus girls and gold diggers was established in 1919 by the "The Gold Diggers" play, association which was also present in the subsequent film four years later, The Gold Diggers.[4]		In 1920s and 1930s American cinema the "gold digger" was the type of femme fatale that gradually replaced the "vamp".[5] The character type would be featured for example in How to Marry a Millionaire, a 1953 film with Marilyn Monroe.		In the analysis of rap music it has been theorized that the "gold digger script" is one of a few prevalent sexual scripts present for young African American women.[6] One very famous song featuring it is "Gold Digger", by Kanye West.		The term is often used with a negative connotation. Society frowns upon it because it is most commonly the case where the attractive women have no genuine feelings for the successful man but lead him on to believe they do. In doing that they allow the man to display them with pride knowing that they are only in it for the money or social status. This type of affair usually results in a divorce in which the gold diggers will attempt to obtain the wealthy man's assets through legal separations.				
The International Standard Book Number (ISBN) is a unique[a][b] numeric commercial book identifier.		An ISBN is assigned to each edition and variation (except reprintings) of a book. For example, an e-book, a paperback and a hardcover edition of the same book would each have a different ISBN. The ISBN is 13 digits long if assigned on or after 1 January 2007, and 10 digits long if assigned before 2007. The method of assigning an ISBN is nation-based and varies from country to country, often depending on how large the publishing industry is within a country.		The initial ISBN configuration of recognition was generated in 1967 based upon the 9-digit Standard Book Numbering (SBN) created in 1966. The 10-digit ISBN format was developed by the International Organization for Standardization (ISO) and was published in 1970 as international standard ISO 2108 (the SBN code can be converted to a ten digit ISBN by prefixing it with a zero).		Occasionally, a book may appear without a printed ISBN if it is printed privately or the author does not follow the usual ISBN procedure; however, this can be rectified later.[1]		Another identifier, the International Standard Serial Number (ISSN), identifies periodical publications such as magazines; and the International Standard Music Number (ISMN) covers for musical scores.						The Standard Book Numbering (SBN) code is a 9-digit commercial book identifier system created by Gordon Foster, Emeritus Professor of Statistics at Trinity College, Dublin,[2] for the booksellers and stationers WHSmith and others in 1965.[3] The ISBN configuration of recognition was generated in 1967 in the United Kingdom by David Whitaker[4] (regarded as the "Father of the ISBN"[5]) and in 1968 in the US by Emery Koltay[4] (who later became director of the U.S. ISBN agency R.R. Bowker).[5][6][7]		The 10-digit ISBN format was developed by the International Organization for Standardization (ISO) and was published in 1970 as international standard ISO 2108.[3][4] The United Kingdom continued to use the 9-digit SBN code until 1974. ISO has appointed the International ISBN Agency as the registration authority for ISBN worldwide and the ISBN Standard is developed under the control of ISO Technical Committee 46/Subcommittee 9 TC 46/SC 9. The ISO on-line facility only refers back to 1978.[8]		An SBN may be converted to an ISBN by prefixing the digit "0". For example, the second edition of Mr. J. G. Reeder Returns, published by Hodder in 1965, has "SBN 340 01381 8" – 340 indicating the publisher, 01381 their serial number, and 8 being the check digit. This can be converted to ISBN 0-340-01381-8; the check digit does not need to be re-calculated.		Since 1 January 2007, ISBNs have contained 13 digits, a format that is compatible with "Bookland" European Article Number EAN-13s.[9]		An ISBN is assigned to each edition and variation (except reprintings) of a book. For example, an ebook, a paperback, and a hardcover edition of the same book would each have a different ISBN.[10] The ISBN is 13 digits long if assigned on or after 1 January 2007, and 10 digits long if assigned before 2007. An International Standard Book Number consists of 4 parts (if it is a 10 digit ISBN) or 5 parts (for a 13 digit ISBN):		A 13-digit ISBN can be separated into its parts (prefix element, registration group, registrant, publication and check digit), and when this is done it is customary to separate the parts with hyphens or spaces. Separating the parts (registration group, registrant, publication and check digit) of a 10-digit ISBN is also done with either hyphens or spaces. Figuring out how to correctly separate a given ISBN number is complicated, because most of the parts do not use a fixed number of digits.[13]		ISBN issuance is country-specific, in that ISBNs are issued by the ISBN registration agency that is responsible for that country or territory regardless of the publication language. The ranges of ISBNs assigned to any particular country are based on the publishing profile of the country concerned, and so the ranges will vary depending on the number of books and the number, type, and size of publishers that are active. Some ISBN registration agencies are based in national libraries or within ministries of culture and thus may receive direct funding from government to support their services. In other cases, the ISBN registration service is provided by organisations such as bibliographic data providers that are not government funded. In Canada, ISBNs are issued at no cost with the stated purpose of encouraging Canadian culture.[14] In the United Kingdom, United States, and some other countries, where the service is provided by non-government-funded organisations, the issuing of ISBNs requires payment of a fee.		Australia: ISBNs are issued by the commercial library services agency Thorpe-Bowker,[15] and prices range from $42 for a single ISBN (plus a $55 registration fee for new publishers) to $2,890 for a block of 1,000 ISBNs. Access is immediate when requested via their website.[16]		Brazil: National Library of Brazil, a government agency, is responsible for issuing ISBNs, and there is a cost of R$16 [17]		Canada: Library and Archives Canada, a government agency, is responsible for issuing ISBNs, and there is no cost. Works in French are issued an ISBN by the Bibliothèque et Archives nationales du Québec.		Colombia: Cámara Colombiana del Libro, a NGO, is responsible for issuing ISBNs. Cost of issuing an ISBN is about USD 20.		Hong Kong: The Books Registration Office (BRO), under the Hong Kong Public Libraries, issues ISBNs in Hong Kong. There is no fee.[18]		India: The Raja Rammohun Roy National Agency for ISBN (Book Promotion and Copyright Division), under Department of Higher Education, a constituent of the Ministry of Human Resource Development, is responsible for registration of Indian publishers, authors, universities, institutions, and government departments that are responsible for publishing books.[19] There is no fee associated in getting ISBN in India.[20]		Italy: The privately held company EDISER srl, owned by Associazione Italiana Editori (Italian Publishers Association) is responsible for issuing ISBNs.[21] The original national prefix 978-88 is reserved for publishing companies, starting at €49 for a ten-codes block[22] while a new prefix 979-12 is dedicated to self-publishing authors, at a fixed price of €25 for a single code.		Maldives: The National Bureau of Classification (NBC) is responsible for ISBN registrations for publishers who are publishing in the Maldives.[citation needed]		Malta: The National Book Council (Maltese: Il-Kunsill Nazzjonali tal-Ktieb) issues ISBN registrations in Malta.[23][24][25]		Morocco: The National Library of Morocco is responsible for ISBN registrations for publishing in Morocco and Moroccan-occupied portion of Western Sahara.		New Zealand: The National Library of New Zealand is responsible for ISBN registrations for publishers who are publishing in New Zealand.[26]		Pakistan: The National Library of Pakistan is responsible for ISBN registrations for Pakistani publishers, authors, universities, institutions, and government departments that are responsible for publishing books.		South Africa: The National Library of South Africa is responsible for ISBN issuance for South African publishing institutions and authors.		United Kingdom and Republic of Ireland: The privately held company Nielsen Book Services Ltd, part of Nielsen Holdings N.V., is responsible for issuing ISBNs in blocks of 10, 100 or 1000. Prices start from £120 (plus VAT) for the smallest block on a standard turnaround of ten days.[27]		United States: In the United States, the privately held company R.R. Bowker issues ISBNs.[4] There is a charge that varies depending upon the number of ISBNs purchased, with prices starting at $125.00 for a single number. Access is immediate when requested via their website.[28]		Publishers and authors in other countries obtain ISBNs from their respective national ISBN registration agency. A directory of ISBN agencies is available on the International ISBN Agency website.		The registration group identifier is a 1- to 5-digit number that is valid within a single prefix element (i.e. one of 978 or 979).[11] Registration group identifiers have primarily been allocated within the 978 prefix element.[29] The single-digit group identifiers within the 978 prefix element are: 0 or 1 for English-speaking countries; 2 for French-speaking countries; 3 for German-speaking countries; 4 for Japan; 5 for Russian-speaking countries; and 7 for People's Republic of China. An example 5-digit group identifier is 99936, for Bhutan. The allocated group IDs are: 0–5, 600–621, 7, 80–94, 950–989, 9926–9989, and 99901–99976.[30] Books published in rare languages typically have longer group identifiers.[31]		Within the 979 prefix element, the registration group identifier 0 is reserved for compatibility with International Standard Music Numbers (ISMNs), but such material is not actually assigned an ISBN.[11] The registration group identifiers within prefix element 979 that have been assigned are 10 for France, 11 for the Republic of Korea, and 12 for Italy.[32]		The original 9-digit standard book number (SBN) had no registration group identifier, but prefixing a zero (0) to a 9-digit SBN creates a valid 10-digit ISBN.		The national ISBN agency assigns the registrant element (cf. Category:ISBN agencies) and an accompanying series of ISBNs within that registrant element to the publisher; the publisher then allocates one of the ISBNs to each of its books. In most countries, a book publisher is not required by law to assign an ISBN; however, most bookstores only handle ISBN bearing publications.[citation needed]		A listing of more than 900,000 assigned publisher codes is published, and can be ordered in book form (€1399, US$1959). The web site of the ISBN agency does not offer any free method of looking up publisher codes.[33] Partial lists have been compiled (from library catalogs) for the English-language groups: identifier 0 and identifier 1.		Publishers receive blocks of ISBNs, with larger blocks allotted to publishers expecting to need them; a small publisher may receive ISBNs of one or more digits for the registration group identifier, several digits for the registrant, and a single digit for the publication element. Once that block of ISBNs is used, the publisher may receive another block of ISBNs, with a different registrant element. Consequently, a publisher may have different allotted registrant elements. There also may be more than one registration group identifier used in a country. This might occur once all the registrant elements from a particular registration group have been allocated to publishers.		By using variable block lengths, registration agencies are able to customise the allocations of ISBNs that they make to publishers. For example, a large publisher may be given a block of ISBNs where fewer digits are allocated for the registrant element and many digits are allocated for the publication element; likewise, countries publishing many titles have few allocated digits for the registration group identifier and many for the registrant and publication elements.[34] Here are some sample ISBN-10 codes, illustrating block length variations.		English-language registration group elements are 0 and 1 (2 of more than 220 registration group elements). These two registration group elements are divided into registrant elements in a systematic pattern, which allows their length to be determined, as follows:[35]		A check digit is a form of redundancy check used for error detection, the decimal equivalent of a binary check bit. It consists of a single digit computed from the other digits in the number. The method for the ten digit code is an extension of that for SBNs, the two systems are compatible, and SBN prefixed with "0" will give the same check-digit as without – the digit is base eleven, and can be 0-9 or X. The system for thirteen digit codes is not compatible and will, in general, give a different check digit from the corresponding 10 digit ISBN, and does not provide the same protection against transposition. This is because the thirteen digit code was required to be compatible with the EAN format, and hence could not contain an "X".		The 2001 edition of the official manual of the International ISBN Agency says that the ISBN-10 check digit[36] – which is the last digit of the ten-digit ISBN – must range from 0 to 10 (the symbol X is used for 10), and must be such that the sum of all the ten digits, each multiplied by its (integer) weight, descending from 10 to 1, is a multiple of 11.		For example, for an ISBN-10 of 0-306-40615-2:		Formally, using modular arithmetic, we can say:		It is also true for ISBN-10's that the sum of all the ten digits, each multiplied by its weight in ascending order from 1 to 10, is a multiple of 11. For this example:		Formally, we can say:		The two most common errors in handling an ISBN (e.g., typing or writing it) are a single altered digit or the transposition of adjacent digits. It can be proved that all possible valid ISBN-10's have at least two digits different from each other. It can also be proved that there are no pairs of valid ISBN-10's with eight identical digits and two transposed digits. (These are true only because the ISBN is less than 11 digits long, and because 11 is a prime number.) The ISBN check digit method therefore ensures that it will always be possible to detect these two most common types of error, i.e. if either of these types of error has occurred, the result will never be a valid ISBN – the sum of the digits multiplied by their weights will never be a multiple of 11. However, if the error occurs in the publishing house and goes undetected, the book will be issued with an invalid ISBN.[37]		In contrast, it is possible for other types of error, such as two altered non-transposed digits, or three altered digits, to result in a valid ISBN number (although it is still unlikely).		Modular arithmetic is convenient for calculating the check digit using modulus 11. Each of the first nine digits of the ten-digit ISBN—excluding the check digit itself—is multiplied by a number in a sequence from 10 to 2, and the remainder of the sum, with respect to 11, is computed. The resulting remainder, plus the check digit, must equal a multiple of 11 (either 0 or 11). Therefore, the check digit is (11 minus the remainder of the sum of the products modulo 11) modulo 11. Taking the remainder modulo 11 a second time accounts for the possibility that the first remainder is 0. Without the second modulo operation the calculation could end up with 11 – 0 = 11 which is invalid. (Strictly speaking the first "modulo 11" is unneeded, but it may be considered to simplify the calculation.)		For example, the check digit for an ISBN-10 of 0-306-40615-? is calculated as follows:		Thus the check digit is 2, and the complete sequence is ISBN 0-306-40615-2. The value x 10 {\displaystyle x_{10}} required to satisfy this condition might be 10; if so, an 'X' should be used.		It is possible to avoid the multiplications in a software implementation by using two accumulators. Repeatedly adding t into s computes the necessary multiples:		The modular reduction can be done once at the end, as shown above (in which case s could hold a value as large as 496, for the invalid ISBN 99999-999-9-X), or s and t could be reduced by a conditional subtract after each addition.		The 2005 edition of the International ISBN Agency's official manual[38] describes how the 13-digit ISBN check digit is calculated. The ISBN-13 check digit, which is the last digit of the ISBN, must range from 0 to 9 and must be such that the sum of all the thirteen digits, each multiplied by its (integer) weight, alternating between 1 and 3, is a multiple of 10.		Formally, using modular arithmetic, we can say:		The calculation of an ISBN-13 check digit begins with the first 12 digits of the thirteen-digit ISBN (thus excluding the check digit itself). Each digit, from left to right, is alternately multiplied by 1 or 3, then those products are summed modulo 10 to give a value ranging from 0 to 9. Subtracted from 10, that leaves a result from 1 to 10. A zero (0) replaces a ten (10), so, in all cases, a single check digit results.		For example, the ISBN-13 check digit of 978-0-306-40615-? is calculated as follows:		Thus, the check digit is 7, and the complete sequence is ISBN 978-0-306-40615-7.		In general, the ISBN-13 check digit is calculated as follows.		Let		Then		This check system – similar to the UPC check digit formula – does not catch all errors of adjacent digit transposition. Specifically, if the difference between two adjacent digits is 5, the check digit will not catch their transposition. For instance, the above example allows this situation with the 6 followed by a 1. The correct order contributes 3×6+1×1 = 19 to the sum; while, if the digits are transposed (1 followed by a 6), the contribution of those two digits will be 3×1+1×6 = 9. However, 19 and 9 are congruent modulo 10, and so produce the same, final result: both ISBNs will have a check digit of 7. The ISBN-10 formula uses the prime modulus 11 which avoids this blind spot, but requires more than the digits 0-9 to express the check digit.		Additionally, if the sum of the 2nd, 4th, 6th, 8th, 10th, and 12th digits is tripled then added to the remaining digits (1st, 3rd, 5th, 7th, 9th, 11th, and 13th), the total will always be divisible by 10 (i.e., end in 0).		The conversion is quite simple as one only needs to prefix "978" to the existing number and calculate the new checksum using the ISBN-13 algorithm.		Publishers and libraries have varied policies about the use of the ISBN check digit. Publishers sometimes fail to check the correspondence of a book title and its ISBN before publishing it; that failure causes book identification problems for libraries, booksellers, and readers.[39] For example, ISBN 0-590-76484-5 is shared by two books – Ninja gaiden® : a novel based on the best-selling game by Tecmo (1990) and Wacky Laws (1997), both published by Scholastic.		Most libraries and booksellers display the book record for an invalid ISBN issued by the publisher. The Library of Congress catalogue contains books published with invalid ISBNs, which it usually tags with the phrase "Cancelled ISBN".[40] However, book-ordering systems such as Amazon.com will not search for a book if an invalid ISBN is entered to its search engine.[citation needed] OCLC often indexes by invalid ISBNs, if the book is indexed in that way by a member library.		Only the term "ISBN" should be used; the terms "eISBN" and "e-ISBN" have historically been sources of confusion and should be avoided. If a book exists in one or more digital (e-book) formats, each of those formats must have its own ISBN. In other words, each of the three separate EPUB, Amazon Kindle, and PDF formats of a particular book will have its own specific ISBN. They should not share the ISBN of the paper version, and there is no generic "eISBN" which encompasses all the e-book formats for a title.[41]		Currently the barcodes on a book's back cover (or inside a mass-market paperback book's front cover) are EAN-13; they may have a separate barcode encoding five digits for the currency and the recommended retail price.[42] For 10 digit ISBNs, the number "978", the Bookland "country code", is prefixed to the ISBN in the barcode data, and the check digit is recalculated according to the EAN13 formula (modulo 10, 1x and 3x weighting on alternate digits).		Partly because of an expected shortage in certain ISBN categories, the International Organization for Standardization (ISO) decided to migrate to a thirteen-digit ISBN (ISBN-13). The process began 1 January 2005 and was planned to conclude 1 January 2007.[43] As of 2011, all the 13-digit ISBNs began with 978. As the 978 ISBN supply is exhausted, the 979 prefix was introduced. Part of the 979 prefix is reserved for use with the Musicland code for musical scores with an ISMN. 10 digit ISMN codes differed visually as they began with an "M" letter; the bar code represents the "M" as a zero (0), and for checksum purposes it counted as a 3. All ISMNs are now 13 digits commencing 979-0; 979-1 to 979-9 will be used by ISBN.		Publisher identification code numbers are unlikely to be the same in the 978 and 979 ISBNs, likewise, there is no guarantee that language area code numbers will be the same. Moreover, the ten-digit ISBN check digit generally is not the same as the thirteen-digit ISBN check digit. Because the GTIN-13 is part of the Global Trade Item Number (GTIN) system (that includes the GTIN-14, the GTIN-12, and the GTIN-8), the 13-digit ISBN falls within the 14-digit data field range.[44]		Barcode format compatibility is maintained, because (aside from the group breaks) the ISBN-13 barcode format is identical to the EAN barcode format of existing 10-digit ISBNs. So, migration to an EAN-based system allows booksellers the use of a single numbering system for both books and non-book products that is compatible with existing ISBN based data, with only minimal changes to information technology systems. Hence, many booksellers (e.g., Barnes & Noble) migrated to EAN barcodes as early as March 2005. Although many American and Canadian booksellers were able to read EAN-13 barcodes before 2005, most general retailers could not read them. The upgrading of the UPC barcode system to full EAN-13, in 2005, eased migration to the ISBN-13 in North America.		
